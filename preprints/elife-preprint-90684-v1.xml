<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">90684</article-id>
<article-id pub-id-type="doi">10.7554/eLife.90684</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.90684.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>On the computational principles underlying human exploration</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Fox</surname>
<given-names>Lior</given-names>
</name>
<email>lior.fox@mail.huji.ac.il</email>
<xref ref-type="aff" rid="a1">*</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Dan</surname>
<given-names>Ohad</given-names>
</name>
<email>ohad.dan@gmail.com</email>
<xref ref-type="aff" rid="a2">†</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Loewenstein</surname>
<given-names>Yonatan</given-names>
</name>
<email>yonatan@huji.ac.il</email>
<xref ref-type="aff" rid="a1">*</xref>
<xref ref-type="aff" rid="a3">‡</xref>
</contrib>
<aff id="a1"><label>*</label><institution>The Edmond and Lily Safra Center for Brain Sciences, The Hebrew University</institution>, <country>Jerusalem</country></aff>
<aff id="a2"><label>†</label><institution>Yale School of Medicine</institution></aff>
<aff id="a3"><label>‡</label><institution>Department of Cognitive Sciences, The Alexander Silberman Institute of Life Sciences, and The Federmann Center for the Study of Rationality</institution></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Verstynen</surname>
<given-names>Timothy</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Behrens</surname>
<given-names>Timothy E</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<pub-date pub-type="epub">
<day>18</day>
<month>07</month>
<year>2023</year>
</pub-date>
<pub-date date-type="original-publication" iso-8601-date="2023-10-10">
<day>10</day>
<month>10</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP90684</elocation-id>
<history><date date-type="sent-for-review" iso-8601-date="2023-07-16">
<day>16</day>
<month>07</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-07-18">
<day>18</day>
<month>07</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.31234/osf.io/s96b2"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Fox et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Fox et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-90684-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Adapting to new environments is a hallmark of animal and human cognition, and Reinforcement Learning (RL) models provide a powerful and general framework for studying such adaptation. A fundamental learning component identified by RL models is that in the absence of direct supervision, when learning is driven by trial-and-error, <italic>exploration</italic> is essential. The necessary ingredients of effective exploration have been studied extensively in machine learning. However, the relevance of some of these principles to humans’ exploration is still unknown. An important reason for this gap is the dominance of the Multi-Armed Bandit tasks in human exploration studies. In these tasks, the exploration component <italic>per se</italic> is simple, because local measures of uncertainty, most notably visit-counters, are sufficient to effectively direct exploration. By contrast, in more complex environments, actions have long-term exploratory consequences that should be accounted for when measuring their associated uncertainties. Here, we use a novel experimental task that goes beyond the bandit task to study human exploration. We show that when local measures of uncertainty are insufficient, humans use exploration strategies that propagate uncertainties over states and actions. Moreover, we show that the long-term exploration consequences are temporally-discounted, similar to the temporal discounting of rewards in standard RL tasks. Additionally, we show that human exploration is largely uncertainty-driven. Finally, we find that humans exhibit signatures of temporally-extended learning, rather than local, 1-step update rules which are commonly assumed in RL models. All these aspects of human exploration are well-captured by a computational model in which agents learn an exploration “value-function”, analogous to the standard (reward-based) value-function in RL.</p>
</abstract>

</article-meta>
</front>
<body>
<sec id="s1" sec-type="intro">
<title>Introduction</title>
<p>When encountered with a novel setting, animals and humans explore their environment. Such exploration is essential for learning which actions are beneficial for the organism and which should be avoided. The speed of learning, and even the learning outcome, crucially depends on the “quality” of that exploration: for example, if as a result of poor exploration some actions are never chosen, their effects are never observed, and hence cannot be learned. More generally, a fundamental difference between learning by trial and error and Supervised Learning scenarios is that in the latter, the distribution of examples is controlled by the “teacher”, whereas in the former, the distribution of examples that the agent gets to observe depends on the agent’s own behavioral policy. Therefore, in order to successfully learn a good policy by trial and error, agents need to take into account <italic>uncertainty</italic> when choosing actions, reflecting the fact that the observations collected so far might mis-represent the actual quality of the different actions.</p>
<p>Learning by trial and error is often abstracted in the framework of the computational problem of Reinforcement Learning (RL) (<xref ref-type="bibr" rid="CIT0048">Sutton and Barto, 2018</xref>): An agent makes sequential decisions in an unknown environment; at each time-step, it observes the current state of the environment, and chooses an action from a set of possible actions. In response to this action, the environment transfers the agent to the next state, and provides a reward signal (which can also be zero or negative). The ultimate goal of the agent is to learn how to choose actions – i.e, learn a <italic>policy</italic> – such as to maximize some performance metric, typically the expected cumulative reward.</p>
<p>Exploration algorithms in RL differ in the particular way they address uncertainties. <italic>Random exploration</italic>, in which a random component is added to the policy (e.g., a policy otherwise maximizing based on current estimates) is, arguably, the simplest way of incorporating exploration. By adding randomness, the agent is bound to eventually accumulate information about all states and actions. More sophisticated exploration methods, referred to as <italic>directed exploration</italic> (<xref ref-type="bibr" rid="CIT0050">Thrun, 1992</xref>), attempt to identify and actively choose the specific actions that will be more effective in reducing uncertainty. To do that, the agent needs to track and update some estimate or measures of uncertainty associated with different actions. For example, the agent can use visit-counters: keep track of the number of times each action was chosen in each state, and prioritize those actions that have previously been neglected (<xref ref-type="bibr" rid="CIT0001">Auer et al., 2002</xref>; <xref ref-type="bibr" rid="CIT0004">Bellemare et al., 2016</xref>; <xref ref-type="bibr" rid="CIT0049">Tang et al., 2017</xref>; <xref ref-type="bibr" rid="CIT0036">Ostrovski et al., 2017</xref>).</p>
<p>The intuition behind counter-based methods can be made precise in the important case of Multi-Armed Bandit problems (or bandit problems, for short). In a <italic>k</italic>-armed bandit, the environment is characterized by a single state and <italic>k</italic> actions (“arms”), each associated with a reward distribution. Because these distributions are unknown, and feedback (i.e., a sample from the distribution) is given only for the chosen arm at each trial, exploration is needed to guarantee that the best arm (i.e., the one associated with the highest expected reward) is identified. Bandit problems are theoretically well-understood, with various algorithms having optimality guarantees, under some statistical assumptions (for a comprehensive review see <xref ref-type="bibr" rid="CIT0025">Lattimore and Szepesvári, 2020</xref>). Particularly, counter-based methods (e.g., UCB, <xref ref-type="bibr" rid="CIT0001">Auer et al., 2002</xref>) can be shown to explore optimally in bandit tasks, in the online-learning sense of minimizing regret.</p>
<p>Human exploration has been studied extensively in bandit and bandit-like problems (<xref ref-type="bibr" rid="CIT0043">Shteingart et al., 2013</xref>; <xref ref-type="bibr" rid="CIT0055">Wilson et al., 2014</xref>; <xref ref-type="bibr" rid="CIT0029">Mehlhorn et al., 2015</xref>; <xref ref-type="bibr" rid="CIT0015">Gershman, 2018</xref>; <xref ref-type="bibr" rid="CIT0042">Schulz et al., 2020</xref>). Because these are arguably the simplest form of RL problems, they offer a clean and potentially well-controlled framework for experiments (<xref ref-type="bibr" rid="CIT0014">Fox et al., 2020</xref>). The strong theoretical foundations are another appeal for experimental work, because behavior can be compared with well-defined algorithms, and, potentially, also with an optimal solution.</p>
<p>However, generalizing conclusions about human exploration from behavior in bandit tasks to behavior in more complex environments is not trivial. In a bandit task, an action that was chosen less times is, everything else being equal, exploratory more valuable compared to one that was chosen more often. By contrast, visit-counters alone might be a poor measure of uncertainty in complex environments, because they completely ignore future consequences of the actions (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Indeed, the limitations of naive counter-based exploration in structured and complex environments have been discussed in the machine learning literature, and different exploration schemes that take into account the long-term exploratory consequences of actions have been proposed (<xref ref-type="bibr" rid="CIT0046">Storck et al., 1995</xref>; <xref ref-type="bibr" rid="CIT0030">Meuleau and Bourgine, 1999</xref>; <xref ref-type="bibr" rid="CIT0034">Osband et al., 2016a</xref>,<xref ref-type="bibr" rid="CIT0035">b</xref>; <xref ref-type="bibr" rid="CIT0007">Chen et al., 2017</xref>; <xref ref-type="bibr" rid="CIT0013">Fox et al., 2018</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1</label>
<caption><title>Directed exploration in complex environments.</title><p>(<bold>a</bold>) In a bandit problem (left), actions have no long-term consequences. In complex environments (right), actions have long-term consequences as particular actions might lead, in the future, to different parts of the state-space. In this example, these parts (shaded areas) are of different size. As a result, the local visit-counters are no longer a good measure of uncertainty. In this example, <italic>a</italic><sub>2</sub> should be, in general, chosen more often compared to <italic>a</italic><sub>1</sub> in order to exhaust the larger uncertainty associated with it. <bold>(b)</bold> Participants were instructed to navigate through a maze of rooms. Each room was identified by a unique background image and a title. To move to the next room, participants chose between the available doors by mouse-clicking. Background images and room titles (Armenian letters) were randomized between participants, and were devoid of any clear semantic or spatial structure. <bold>(c)</bold> The three maze structures in Experiment 1 (Top) have a root state <italic>S</italic> (highlighted in yellow) with two doors. They differ in the imbalance between the number of doors available in future rooms <italic>M<sub>R</sub></italic> and <italic>M<sub>L</sub></italic> (<italic>n<sub>R</sub></italic> : <italic>n<sub>L</sub></italic> – 4:3, 5:2, 6:1). Consistent with models of directed exploration that take into account long-term consequences of actions, and unlike counter-based models, participants exhibited bias towards room <italic>M<sub>R</sub></italic>, deviating from a uniform policy (Bottom, bars and error-bars denote mean and 95% confidence interval of <italic>p<sub>R</sub></italic>; number of participants: <italic>n</italic> = 161; 120; 137. Statistical significance, here and in following figures: * : p &lt; 0.05, ** : p &lt; 0.01; *** : p &lt; 0.001).</p></caption>
<graphic xlink:href="s96b2v2_fig1.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<p>Our goal here is to study the extent to which human exploration is sensitive to long-term consequences of actions, as opposed to counter-based exploration. Crucially, this question cannot be addressed in the common bandit problems paradigm, because general exploration algorithms are reduced to counter-based methods when they are faced with a bandit problem. Thus, even if humans do (approximately) use some general, beyond visit-counters, directed exploration strategies, they will likely manifest as counter-based strategies in bandit tasks. Therefore, we set out to study exploration in a novel task that addresses these issues. First, we show that humans take into account the long-term exploratory consequences of their actions when exploring complex environments (Experimental results). Next, we model this exploration using an RL-like algorithm, in which agents learn exploratory “action-values” and use these values to guide their exploration (Computational modeling).</p>
</sec>
<sec id="s2" sec-type="results">
<title>Results</title>
<sec id="s2-1">
<title>Experimental results</title>
<sec id="s2-1-1">
<title>Sensitivity to future consequences of actions</title>
<p>To test the hypothesis that human exploration is sensitive to the long-term consequences of actions, we conducted an experiment that formalizes the intuition presented in the Introduction (see <xref ref-type="fig" rid="fig1">Figure 1a</xref>). In the experiment (denoted as “Experiment 1”), participants were instructed to explore a novel environment, a maze of rooms, by navigating through the doors connecting those rooms (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Each room was identified by a unique background, a title, and the number of doors in that room. No reward was given in this task, but participants were instructed to “understand how the rooms are connected” (see Methods). Testing participants in a task devoid of clear goal and rewards is somewhat unorthodox. We go back to this point in the Discussion section.</p>
<p>Three groups of participants were tested, each in a different maze as is described in <xref ref-type="fig" rid="fig1">Figure 1c</xref> (top): In all mazes, there was a start room (<italic>S</italic>) with two doors, each leading to a different room. One of these rooms, a multi-action room (<italic>M<sub>R</sub></italic>) was endowed with <italic>n<sub>R</sub></italic> doors, while the other, denoted as <italic>M<sub>L</sub></italic>, was endowed with <italic>n<sub>L</sub></italic> doors. All three mazes were unbalanced, in the sense that <italic>n<sub>R</sub> &gt; n<sub>L</sub></italic>. Between the different mazes, we varied <italic>n<sub>R</sub> − n<sub>L</sub></italic>, while keeping <italic>n<sub>R</sub></italic> + <italic>n<sub>L</sub></italic> = 7 constant. The locations of the doors leading to <italic>M<sub>R</sub></italic> and <italic>M<sub>L</sub></italic> were counterbalanced across participants. For clarity of notation, we refer to them as “right” and “left”, respectively. All other remaining rooms were endowed with only a single door. After going through these single-door rooms, a participant would reach a common terminal room (<italic>T</italic>). There, they were informed that they reached the end of the maze and then they were transported back to <italic>S</italic>. Overall, each participant visited <italic>S</italic> (of the one particular environment they were assigned to) 20 times.</p>
<p>Since there was no reward, all choices in this task are exploratory. If participant’s exploration is driven by visit-counters, then we expect that the <italic>frequencies</italic> in which they choose each of the doors in <italic>S</italic>, denoted <italic>p<sub>R</sub></italic> and <italic>p<sub>L</sub></italic>, would be equal. By contrast, if they take into consideration the long-term consequences of their actions, then we would expect them to choose the right door more often (resulting in <italic>p<sub>R</sub> &gt; p<sub>L</sub></italic>). In line with the hypothesis that participants are sensitive to the long-term consequences of their actions, we found that averaged over all participants in the three conditions, <italic>p<sub>R</sub> &gt; p<sub>L</sub></italic> (<italic>p<sub>R</sub></italic> = 0.54, 95% confidence interval: <italic>p<sub>R</sub></italic> ∈ [0.518, 0.563]). Considering each group of participants separately, significant bias in favor of <italic>p<sub>R</sub></italic> was observed in the 6:1 (<italic>p<sub>R</sub></italic> = 0.572, <italic>n</italic> = 137, 95% CI: [0.528, 0.617]) and the 5:2 groups (<italic>p<sub>R</sub></italic> = 0.549, <italic>n</italic> = 120, 95% CI: [0.506, 0.592]), but not in the 4:3 group (<italic>p<sub>R</sub></italic> = 0.507, <italic>n</italic> = 161, 95% CI: [0.472, 0.541]).</p>
<p>We hypothesized that the larger the imbalance (<italic>n<sub>R</sub> − n<sub>L</sub></italic>), the stronger will be the bias towards <italic>M<sub>R</sub></italic> (larger <italic>p<sub>R</sub></italic>). To test this hypothesis, we compared the biases of participants in the different groups (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). As expected, the average <italic>p<sub>R</sub></italic> in the 5:2 and 6:1 groups was significantly larger than that of the 4:3 group (p &lt; 0.05 and p &lt; 0.01 respectively, permutation test, see Methods). The average <italic>p<sub>R</sub></italic> in the 6:1 group was larger than that of the 5:2 group. However, this difference was not statistically significant (p = 0.17).</p>
<p>The results depicted in <xref ref-type="fig" rid="fig1">Figure 1c</xref> indicate that on average, human participants are sensitive to the exploratory long-term consequences of their actions. Considering individual participants, however, there was substantial heterogeneity in the biases exhibited by the different participants. While some chose the right door almost exclusively, others favored the left door. We next asked whether some of this heterogeneity across participants reflects more general individual-differences in exploratory strategies, which would also manifest in their exploration in other states. To test this hypothesis, we focused on state <italic>M<sub>R</sub></italic>. In this state, exploration is also required because there are <italic>n<sub>R</sub></italic> different alternatives to choose from. However, unlike in state <italic>S</italic>, these alternatives do not, effectively, have long-term consequences. As such, choosing an action in <italic>M<sub>R</sub></italic> is a bandit-like task. Thereofre, directed exploration in <italic>M<sub>R</sub></italic> is expected to be driven by visit-counters, such that participants would equalize the number of times each door in <italic>M<sub>R</sub></italic> is selected. Note that this is not a strong prediction, because random exploration will, on average, also equalize the number of choices of each door. Yet, directed and random exploration have diverging predictions with respect to the temporal pattern of choices in <italic>M<sub>R</sub></italic>. Specifically, with pure directed exploration (that is driven by visit-counters), participants are expected to avoid choosing the same door that they chose the last time that they visited <italic>M<sub>R</sub></italic>. Consequently, the probability of repeating the same choice in consecutive visits of <italic>M<sub>R</sub></italic>, which we denote by <italic>p</italic><sub>repeat</sub>, is expected to vanish. By contrast, random exploration predicts that <italic>p</italic><sub>repeat</sub> = 1/<italic>n<sub>R</sub></italic>. <xref ref-type="fig" rid="fig2">Figure 2</xref> (Top) depicts the histograms (over participants) of <italic>p</italic><sub>repeat</sub> in the three experimental conditions, demonstrating that participants exhibited substantial variability in <italic>p</italic><sub>repeat</sub>. While for some participants <italic>p</italic><sub>repeat</sub> was close to 0, as predicted by pure directed exploration, for others it was similar to 1/<italic>n<sub>R</sub></italic>, as predicted by random exploration. Many other participants exhibited <italic>p</italic><sub>repeat</sub> that was even larger than 1/<italic>n<sub>R</sub></italic>, indicating that, potentially, choice bias and / or momentum also influenced choices in the task. Based on the predictions of directed and random exploration, we divided participants into two groups, depending on the quality of exploration in <italic>M<sub>R</sub></italic>: “good” directed explorers, in which <italic>p</italic><sub>repeat</sub> &lt; 1/<italic>n<sub>R</sub></italic>, and “poor” directed explorers, in which <italic>p</italic><sub>repeat</sub> ≥ 1/<italic>n<sub>R</sub></italic> (<xref ref-type="fig" rid="fig2">Figure 2</xref> Top, dots and diagonal stripes, respectively).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2</label>
<caption><title>Heterogeneity in exploration strategies.</title><p>Top: Histograms of <italic>p</italic><sub>repeat</sub> at state <italic>M<sub>R</sub></italic> (highlighted in yellow) for participants in the three conditions of Experiment 1 (left to right: <italic>n<sub>R</sub></italic> = 4, 5, 6). Dashed vertical line represents the value expected by chance, 1/<italic>n<sub>R</sub></italic>. Based on their <italic>p</italic><sub>repeat</sub> values, we divided participants into “good” and “poor” directed explorers (dotted and striped patterns, respectively; “good” explorers proportion: 40%, 44%, 51%). <bold>Bottom</bold>: Histograms of <italic>p<sub>R</sub></italic> at state <italic>S</italic> (highlighted in yellow), for the “good” and “poor” directed explorers groups.</p></caption>
<graphic xlink:href="s96b2v2_fig2.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<p>Is the quality of directed exploration in the bandit-like task of state <italic>M<sub>R</sub></italic> informative about directed exploration in <italic>S</italic>? To address this question, we computed the histograms of <italic>p<sub>R</sub></italic> separately for the “good” and “poor” directed explorers (<xref ref-type="fig" rid="fig2">Figure 2</xref> Bottom). Averaging within each group we found that indeed, <italic>p<sub>R</sub></italic> among the “poor” explorers was not significantly different from chance in any of the three conditions (<xref ref-type="fig" rid="fig3">Figure 3a</xref>), consistent with the predictions of random exploration. By contrast, among “good” explorers, there was a significant bias in the 5:2 (<italic>p<sub>R</sub></italic> = 0.597, <italic>n</italic> = 53, 95% CI: [0.537, 0.652]) and the 6:1 (<italic>p<sub>R</sub></italic> = 0.612, <italic>n</italic> = 71, 95% CI: [0.544, 0.678]) groups (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). These findings show that participants that avoid repetition in the bandit task are also more sensitive to the long-term exploratory consequences of their actions. We conclude that those participants who tend to perform good directed exploration in <italic>M<sub>R</sub></italic> also perform good directed exploration in <italic>S</italic>. Crucially, the <italic>implementation</italic> of directed exploration in the two states is rather different. In <italic>M<sub>R</sub></italic>, where different actions have no long-term consequences, “good” explorers rely on visit-counters that are the relevant measure of uncertainty, resulting in an overall uniform choice. By contrast in <italic>S</italic>, actions do have long-term consequences, and “good” explorers go beyond the visit-counters, biasing their choices in favor of the action associated with more future uncertainty.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3</label>
<caption><title>“Poor” and “good” directed explorers.</title><p>Choice biases at state <italic>S</italic> (<italic>p<sub>R</sub></italic>) analyzed separately for “poor” and “good” explorers (striped and dotted patterns; divided based on their exploration in <italic>M<sub>R</sub></italic>, see <xref ref-type="fig" rid="fig2">Figure 2</xref>) in the 3 conditions of Experiment 1. While behavior of the “poor” explorers was not significantly different from chance (consistent with the prediction of random exploration), “good” explorers in the <italic>n<sub>R</sub></italic> = 5, 6 conditions exhibited significant bias towards “right”. Bars and error bars denote mean and 95% confidence interval of <italic>p<sub>R</sub></italic>; number of participants <italic>n</italic> = 95; 66 67; 53, 66; 71 (“poor”; “good”).</p></caption>
<graphic xlink:href="s96b2v2_fig3.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
</sec>
<sec id="s2-1-2">
<title>Temporal discounting</title>
<p>In the previous section we showed that if the future exploratory consequences of the actions are one trial ahead, humans are sensitive to these consequences. It is well known that in humans and animals, the value of a reward is discounted with its delay (<xref ref-type="bibr" rid="CIT0052">Vanderveldt et al., 2016</xref>). We hypothesized that similar temporal discounting will manifest in evaluating the exploratory “usefulness” of actions. To test this prediction, we conducted Experiment 2 on a new set of participants. Similar to Experiment 1, Experiment 2 consisted of 3 different maze structures. The imbalance between the number of possible outcomes was kept fixed across 3 mazes, at <italic>n<sub>R</sub></italic> = 5 and <italic>n<sub>L</sub></italic> = 2. However, the <italic>depth</italic> at which these outcomes occur, relative to the root state <italic>S</italic>, varied between 1 (as in Experiment 1) to 3 (<xref ref-type="fig" rid="fig4">Figure 4</xref>, Top). The depth of <italic>M<sub>R</sub></italic> determines the delay between the choice made at <italic>S</italic> and its exploratory benefit. In the presence of temporal discounting of exploration, we therefore expect <italic>p<sub>R</sub></italic> to decrease with the depth of <italic>M<sub>R</sub></italic>.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4</label>
<caption><title>Temporal discounting of exploratory consequences.</title><p>The three mazes in Experiment 2 (Top) had the same imbalance (<italic>n<sub>R</sub></italic> = 5, <italic>n<sub>L</sub></italic> = 2), however we varied the depth of <italic>M<sub>R</sub></italic> (and <italic>M<sub>L</sub></italic>) relative to the root state <italic>S</italic> (left to right: depth = 1, 2, 3). “Poor” and “good” directed explorers (striped and dotted patterns, respectively) were divided by their <italic>p</italic><sub>repeat</sub> value at <italic>M<sub>R</sub></italic> (same as in Experiment 1, see <xref ref-type="fig" rid="fig2">Figure 2</xref>). Bars and error-bars denote mean and 95% confidence interval of <italic>p<sub>R</sub></italic>. Number of participants <italic>n</italic> = 99; 92, 121; 84, 153; 85 (“poor”; “good”).</p></caption>
<graphic xlink:href="s96b2v2_fig4.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<p>To test this prediction, we divided participants to “good” and “poor” directed explorers, as in Experiment 1, based on the degree of <italic>p</italic><sub>repeat</sub> in <italic>M<sub>R</sub></italic>. As depicted in <xref ref-type="fig" rid="fig4">Figure 4</xref>, both the “poor” and “good” explorers exhibited a bias in favor of “right” in <italic>S</italic>. For the “good” explorers, a larger delay was also associated with a smaller bias.</p>
</sec>
<sec id="s2-1-3">
<title>The dynamics of exploration</title>
<p>Insofar, we demonstrated that human participants exhibit directed exploration in which they take into their considerations the future exploratory consequences of their action. To better understand the computational principles underlying this directed exploration, we revisit the question of why explore in the first place. One possible answer to this question is that exploration is required for learning. According to this view, actions are favorable from an exploratory point of view when they are associated with, or lead to other actions associated with, high uncertainty, missing knowledge, and other related quantities (<xref ref-type="bibr" rid="CIT0040">Schmidhuber, 1991</xref>; <xref ref-type="bibr" rid="CIT0045">Still and Precup, 2012</xref>; <xref ref-type="bibr" rid="CIT0027">Little and Sommer, 2014</xref>; <xref ref-type="bibr" rid="CIT0022">Houthooft et al., 2016</xref>; <xref ref-type="bibr" rid="CIT0038">Pathak et al., 2017</xref>; <xref ref-type="bibr" rid="CIT0006">Burda et al., 2019</xref>). An alternative, that has received some attention in the machine learning literature, is that exploration could be driven by its own normative objective (<xref ref-type="bibr" rid="CIT0028">Machado and Bowling, 2016</xref>; <xref ref-type="bibr" rid="CIT0021">Hazan et al., 2019</xref>; <xref ref-type="bibr" rid="CIT0057">Zhang et al., 2020</xref>; <xref ref-type="bibr" rid="CIT0056">Zahavy et al., 2021</xref>). For example, such objective could be to maximize the entropy of the discounted distribution of visited states and chosen actions (<xref ref-type="bibr" rid="CIT0021">Hazan et al., 2019</xref>). Experimentally, the difference between the two approaches will be particularly pronounced towards the end of a long experiment. When all states and actions had been visited sufficiently many times, everything that can be learned has already been learned. Thus, if the goal of exploration is to facilitate learning, then exploratory behavior is expected to fade over time. By contrast, if exploration is driven by a normative objective, then we generally expect behavior to converge to a one that (approximately) maximizing this objective, and hence maintaining asymptotic exploratory behavior.</p>
<p>Specifically considering Experiment 1 and 2, we do not expect any bias in <italic>S</italic> (<italic>p<sub>R</sub></italic> = 0.5) in the beginning of the task, because participants are naive and are unaware of the different long-term consequences of the two actions. With time and learning, we expect participants to favor <italic>M<sub>R</sub></italic> over <italic>M<sub>L</sub></italic> (<italic>p<sub>R</sub></italic> &gt; 0.5). This prediction holds either if participants are driven by the goal of reducing the (long-term) uncertainty associated with <italic>M<sub>R</sub></italic>, or by the goal of optimizing some exploration objective, such as to match the choices per door in <italic>M<sub>R</sub></italic> and <italic>M<sub>L</sub></italic>. In other words, both approaches predict that with time, <italic>p<sub>R</sub></italic> will increase. With more time elapsing, however, the predictions of the two approaches diverge. As uncertainty decreases, uncertainty-driven exploration predicts a decay of <italic>p<sub>R</sub></italic> to its baseline value <inline-formula id="ID1"><alternatives><mml:math display="inline" id="I1"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="s96b2v2_ieq1.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives></inline-formula>. By contrast, the normative approach predicts that <italic>p<sub>R</sub></italic> will converge to a <inline-formula id="ID2"><alternatives><mml:math display="inline" id="I2"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="s96b2v2_ieq2.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives></inline-formula> steady-state.</p>
<p><xref ref-type="fig" rid="fig5">Figure 5</xref> depicts the temporal dynamics of <italic>p<sub>R</sub></italic> (<italic>t</italic>), as a function of the number of times <italic>t</italic> that <italic>S</italic> was visited (defined as “episodes”). The learning curves are shown separately for the “poor” (<xref ref-type="fig" rid="fig5">Figure 5a</xref>) and “good” (<xref ref-type="fig" rid="fig5">Figure 5b</xref>) explorers, averaged over all 6 conditions of Experiments 1 and 2. As expected, there was no preference in the first episodes. However, with time, the participants developed a bias in favor of <italic>M<sub>R</sub></italic>, which was more pronounced in the “good” directed explorers group. In this group, participants exhibited a significant bias, <italic>p<sub>R</sub></italic> (<italic>t</italic>) &gt; 0.5 from the 3<sup>rd</sup> episode. Notably, this increased bias was followed by a decrease to a steady state bias value (episodes 10 − 20). This steady state value was lower than its peak transient value (consistent with uncertainty-driven exploration), but was higher than baseline level before learning (consistent with a normative exploration objective).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5</label>
<caption><title>Learning dynamics.</title><p>Bias towards <italic>M<sub>R</sub></italic> as a function of training episode (<italic>p<sub>R</sub></italic> (<italic>t</italic>)), averaged over participants in all 6 conditions (Experiments 1 &amp; 2), shown for the “poor” (a) and “good” (b) groups. The “good” explorers exhibited a transient peak in <italic>p<sub>R</sub></italic> (<italic>t</italic>), consistent with models of uncertainty-driven exploration. However, the steady-state value <inline-formula id="ID3"><alternatives><mml:math display="inline" id="I3"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="s96b2v2_ieq3.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives></inline-formula> was still slightly larger than chance, consistent with an objective-driven exploration component. Dots and shaded areas denote mean and 95% confidence interval of <italic>p<sub>R</sub></italic> (<italic>t</italic>).</p></caption>
<graphic xlink:href="s96b2v2_fig5.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
</sec>
</sec>
<sec id="s2-2">
<title>Computational modeling</title>
<sec id="s2-2-1">
<title>The model</title>
<p>Together, the two experiments of the previous sections provide us with the following insights: (1) Humans exploration is affected by long-term consequences of actions (<xref ref-type="fig" rid="fig1">Figure 1c</xref>); (2) Both the number of future states and their depth affect this exploration (<xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig4">Figure 4</xref>); and finally, (3) Exploration dynamics peaks transiently and then decays, consistent with an uncertainty-driven exploration (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p>
<p>In theorizing about effective exploration we have alluded to concepts such as “exploratory value” or “usefulness” of particular actions, but did not provide a precise working definition for it. In this section we consider a specific computational model for directed exploration, and test this model in view of these experimental findings. The model is a general-purpose algorithm for directed exploration, which formalizes the intuition that the challenge of exploration in complex environments is analogous to the standard credit-assignment problem in RL (in the reward-maximization sense).</p>
<p>According to the model, the agent observes the current state of the environment <italic>s</italic> at each time-step and chooses an action <italic>a</italic> from the set of possible actions. In response to this action, the environment transfers the agent to the next state <italic>s′</italic>, at which the agent chooses action <italic>a′</italic>. Each state-action pair (<italic>s, a</italic>) is associated with an exploration value, denoted <italic>E</italic> (<italic>s, a</italic>) (<xref ref-type="bibr" rid="CIT0013">Fox et al., 2018</xref>). These exploration values represent a current estimate of “missing knowledge”, such that a high value indicates that further exploration of that action is beneficial. At the beginning of the process, <italic>E</italic>-values are initialized to a positive constant (specifically <italic>E</italic> = 1), representing the largest possible missing knowledge. Each transition from <italic>s, a</italic> to <italic>s′, a′</italic> triggers an update to <italic>E</italic> (<italic>s, a</italic>) according to the following update rule:
<disp-formula id="eqn1"><alternatives><mml:math display="block" id="M1"><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>←</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic xlink:href="s96b2v2_eqn1.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives><label>(1)</label></disp-formula></p>
<p>In words, the change in <italic>E</italic> (<italic>s, a</italic>) is a sum of two contributions. The first, −<italic>E</italic> (<italic>s, a</italic>), is the immediate reduction in the uncertainty regarding state <italic>s</italic> and action <italic>a</italic> due to the current visit of that state-action. The second, <italic>γE</italic> (<italic>s′, a′</italic>) represents future uncertainty propagating back to (<italic>s, a</italic>). This second part is weighted by a discount-factor parameter, 0 <italic>≤ γ ≤</italic> 1. The overall update magnitude is controlled by a learning-rate parameter 0 <italic>&lt; η</italic> &lt; 1. In the particular case that <italic>s′</italic> is a terminal state, its exploration value is always defined as 0.</p>
<p>To complete the model specification, we define the <italic>policy</italic> as derived directly from these exploration values. We use a standard softmax policy, in which the probability of choosing an action <italic>a</italic> in state <italic>s</italic> is given by:
<disp-formula id="eqn2"><alternatives><mml:math display="block" id="M2"><mml:mrow><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="s96b2v2_eqn2.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives><label>(2)</label></disp-formula>
where <italic>β</italic> ≥ 0 is a gain parameter. A gain value of <italic>β</italic> = 0 corresponds to random exploration, with all actions chosen at equal probability, while a positive gain corresponds to (stochastically) preferring actions associated with a larger <italic>E</italic>-value (and hence higher uncertainty).</p>
<p>Conceptually, this model is similar to standard RL algorithms (specifically the <sc>SARSA</sc> algroithm, <xref ref-type="bibr" rid="CIT0039">Rummery and Niranjan, 1994</xref>) that are used to account for operant learning in animals and humans. There, a similar update rule is used to learn the expected discounted sum of future rewards (and a similar rule is assumed for action-selection). Therefore, similar cognitive mechanisms that account for operant learning, can account for this type of directed exploration (at least to the extent that standard RL models are indeed a good descriptions of operant learning; see <xref ref-type="bibr" rid="CIT0032">Mongillo et al., 2014</xref>; <xref ref-type="bibr" rid="CIT0014">Fox et al., 2020</xref>).</p>
<p>To gain insight into the properties of the <italic>E</italic>-values, we consider first the case of “infinite” discounting, namely <italic>γ</italic> = 0. In that case, the update rule of <xref ref-type="disp-formula" rid="eqn1">Equation 1</xref> becomes:
<disp-formula id="eqn3"><alternatives><mml:math display="block" id="M3"><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>←</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic xlink:href="s96b2v2_eqn3.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives><label>(3)</label></disp-formula>
and hence, after <italic>n</italic> visits of (<italic>s, a</italic>), the associated <italic>E</italic>-value is <italic>E</italic> (<italic>s, a</italic>) = (1 − <italic>η</italic>)<sup><italic>n</italic></sup>, such that − log <italic>E ∝ n</italic>.<sup><xref ref-type="fn" rid="fn01">1</xref></sup> In other words, when <italic>γ</italic> = 0, and long-term consequences are completely ignored, the <italic>E</italic>-value is effectively a visit-counter.</p>
<p>When <italic>γ</italic> &gt; 0, the change in the value of <italic>E</italic> (<italic>s, a</italic>) following a visit of (<italic>s, a</italic>) is more complex. In addition to the decay term, a term that is proportional to <italic>E</italic> (<italic>s′, a′</italic>) is added to <italic>E</italic> (<italic>s, a</italic>). Notably, <italic>E</italic> (<italic>s′, a′</italic>) depends on the number of past visits of (<italic>s′, a′</italic>), (as well its <italic>own</italic> future states (<italic>s″, a″</italic>) and so on). Consequently, the number of <italic>actual</italic> visits that is required to reduce the <italic>E</italic>-values by a given amount is larger in state-actions leading to many future states than in state-actions leading to fewer future states. In that sense, the <italic>E</italic>-values are a <italic>generalization</italic> of visit-counters.</p>
<p>Finally (and regardless of the value of <italic>γ</italic>), the softmax policy of <xref ref-type="disp-formula" rid="eqn2">Equation 2</xref> favors actions associated with larger <italic>E</italic>-values. Because choosing these actions will generally lead to a reduction in their associated <italic>E</italic>-values, the result will be a policy that effectively attempts to equalize the <italic>E</italic>-values of all available actions (within a given state). In the case of <italic>γ</italic> = 0, this will result in a preference toward those actions that were chosen less often. In the case of <italic>γ</italic> &gt; 0, it will result in a preference that is also sensitive to (the number of) future potential states reachable through the different actions.</p>
<p>To conclude, the model therefore encapsulates the three principles identified in human behavior – it propagates information to track long-term uncertainties associated with individual state-actions, it temporally discounts future exploratory consequences, and it uses estimated uncertainties to derive a behavioral policy.</p>
</sec>
<sec id="s2-2-2">
<title>Directed-exploration in the maze task</title>
<p>We now return to the maze task and study the behavior of the model there. In state <italic>M<sub>R</sub></italic>, where the <italic>E</italic>-values correspond to visit-counters, the attempt to equalize the <italic>E</italic>-values will result in a bias against repeating the same action, yielding a low <italic>p</italic><sub>repeat</sub> value and on average, a uniform policy. To demonstrate this, we simulated behavior of the model in the 3 conditions of Experiments 1. Indeed, as depicted in <xref ref-type="fig" rid="fig6">Figure 6a</xref>, the values of <italic>p</italic><sub>repeat</sub> in the simulations were smaller than chance-level. Unlike the population of human participants, simulated agents are more homogeneous, as reflected in the narrower histograms of <italic>p</italic><sub>repeat</sub>. This is due to the fact that the model is designed to perform directed exploration, that is, to model the behavior of the “good” directed explorers. Nevertheless, the model can also produce random exploration if the gain parameter is set to <italic>β</italic> = 0 (see also Discussion).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6</label>
<caption><title>Simulations results.</title><p>Simulating behavior of the <italic>E</italic>-values model (<xref ref-type="disp-formula" rid="eqn1">Equation 1</xref>–<xref ref-type="disp-formula" rid="eqn2">2</xref>) reproduces the main findings of directed exploration in the maze task. <bold>(a)</bold> In <italic>M<sub>R</sub></italic>, the model exhibits directed exploration which manifests in low values of <italic>p</italic><sub>repeat</sub> (shown for the 3 conditions of Experiment 1; dashed line denote chance-level expected for random exploration, 1/<italic>n<sub>R</sub></italic>) <bold>(b)</bold> In the environments of Experiment 1, agents exhibited bias towards <italic>M<sub>R</sub></italic> that increased with imbalance of <italic>n<sub>R</sub></italic> : <italic>n<sub>L</sub></italic>, reflecting the propagation of long-term uncertainties over states. <bold>(c)</bold> In the environments of Experiment 2, the bias decreased with depth, reflecting temporal discounting. <bold>(d)</bold> Bias towards <italic>M<sub>R</sub></italic> peaks transiently, followed by a decay to baseline at steady-state, as expected from uncertainty-driven exploration (average results over all 6 environments). Results are based on 3,000 simulations in each environment. Bars and histograms in (a)-(c) are shown for the first 20 episodes for comparison with the behavioral experiments. Error bars are negligible and therefore are not shown. Model parameters: <italic>η</italic> = 0.9, <italic>β</italic> = 5, <italic>γ</italic> = 0.6.</p></caption>
<graphic xlink:href="s96b2v2_fig6.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<p>More interesting is the behavior of the model in state <italic>S</italic>. The larger <italic>n<sub>R</sub></italic>, the smaller will be the decay of <italic>E</italic> (<italic>s</italic> = <italic>S, a</italic> = right) per a single visit of (<italic>s</italic> = <italic>S, a</italic> = right). Therefore, the model will tend to choose “right” more often (<italic>p<sub>R</sub></italic> &gt; 0.5), a bias that is expected to increase with <italic>n<sub>R</sub></italic>. Indeed, similar to the behavior of the “good” human explorers, the simulated agents exhibited a preference towards “right” in <italic>S</italic>, a preference that increased with <italic>n<sub>R</sub> − n<sub>L</sub></italic> (<xref ref-type="fig" rid="fig6">Figure 6b</xref>).</p>
<p>The model is sensitive to long-term consequences because it propagates future uncertainty, from the next visited state-action back to the current state-action. This future uncertainty, however, is weighted by <italic>γ</italic> &lt; 1, such that the effect of further away states on <italic>E</italic> (<italic>s, a</italic>) is expected to decrease with distance. In the environments of experiment 2, where we manipulated the depth of <italic>M<sub>R</sub></italic> (relative to <italic>S</italic>), this will result in a decrease of the bias (<italic>p<sub>R</sub></italic>) at <italic>S</italic>, as demonstrated in <xref ref-type="fig" rid="fig6">Figure 6c</xref>.</p>
<p>Because the policy in the model is derived from the <italic>E</italic>-values, the temporal pattern of exploration is expected to be transient. In the first episodes, when <italic>E</italic> (<italic>s</italic> = <italic>S, a</italic> = right) = <italic>E</italic> (<italic>s</italic> = <italic>S, a</italic> = left), the result is <italic>p<sub>R</sub></italic> = 0.5. With sufficient learning, exploration values of all visited state-actions decay to 0 and in this limit, <italic>p<sub>R</sub></italic> = 0.5 as well. Therefore, we expect the learning dynamics to exhibit a transient increase in bias, followed by a decay back to chance level. This is demonstrated in <xref ref-type="fig" rid="fig6">Figure 6d</xref> where we plot <italic>p<sub>R</sub></italic> (<italic>t</italic>), averaged over the simulations of the model in all six conditions of Experiments 1 and 2.</p>
<p>Qualitatively, the transient dynamics resemble the experimental results (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). However, there are two important differences. First, while the human participants exhibited what seems like a steady-state bias even at the end of the experiment, <italic>p<sub>R</sub></italic> in the model decays to chance level. As discussed above, the decay to chance in the simulations is expected because exploration in the model is uncertainty-driven. In the framework of this model, steady-state exploration can be achieved if we assume that <italic>β</italic> is not stationary, but rather increases over episodes. However, we hypothesize that to capture this aspect of humans’ exploration, we may need to go beyond this class of uncertainty-driven models. Second, the transient dynamics of the model are longer than that of the human participants. While the learning speed in the model is largely controlled by the learning-rate parameter <italic>η</italic>, the value of <italic>η</italic> cannot by itself explain this gap. This is because in the model <italic>η</italic> &lt; 1, and the dynamics cannot be arbitrarily fast. Particularly, in the simulations of <xref ref-type="fig" rid="fig6">Figure 6d</xref> we have used a large learning-rate of <italic>η</italic> = 0.9, but learning was still considerably slower compared to human participants. We further discuss the issue of learning speed in the next section.</p>
</sec>
<sec id="s2-2-3">
<title>Learning dynamics: 1-step updates and trajectory-based updates</title>
<p>To learn to prefer “right” in <italic>S</italic>, the agent needs to learn that this action leads, in the future, to <italic>M<sub>R</sub></italic>, which from an exploratory point of view is superior to <italic>M<sub>L</sub></italic>. This kind of learning of delayed outcomes is typical of RL problems, in which the agent needs to learn that the value of a particular action stems from its consequences, which can be delayed. For example, an action may valuable because it leads to a large reward, even if this reward is delayed. In the RL literature this is known as the <italic>credit assignment</italic> problem, because during learning, upon observing a desired outcome (in “standard” RL, getting a large reward; here, arriving at <italic>M<sub>R</sub></italic>), the agent needs to properly assign credit for <italic>past</italic> actions that have led to this outcome.</p>
<p>RL algorithms typically address the credit assignment problem by propagating information about the reward backwards through sequences of visited states and actions (<xref ref-type="bibr" rid="CIT0047">Sutton, 1988</xref>; <xref ref-type="bibr" rid="CIT0053">Watkins and Dayan, 1992</xref>; <xref ref-type="bibr" rid="CIT0009">Dayan, 1992</xref>). According to some RL algorithms, the information about the reward propagates backwards one state at a time. By contrast, in other algorithms, a trace of the entire trajectory is maintained, allowing the information to “jump” backwards over a large number of states and actions. We refer to these alternatives as 1-step and trajectory-based updates, respectively.</p>
<p>The <italic>E</italic>-values model can be understood as an RL algorithm that propagates visitations information (rather than reward information). Specifically, it uses 1-step updates (<xref ref-type="disp-formula" rid="eqn1">Equation 1</xref>) such that with each observation (a transition of the form <italic>s, a, s′, a′</italic>) only immediate information, from (<italic>s′, a′</italic>), is used to update the exploration value of (<italic>s, a</italic>). With 1-step updates it takes time (episodes) for information from <italic>M<sub>R</sub></italic> to reach back to <italic>S</italic>. We hypothesized that this reliance on 1-step updates might be an important source for the difference in learning speed between the model and humans, who might use more temporally-extended learning rules. To test this, we considered an extension to the exploration model in which <italic>E</italic>-values are learned using a trajectory-based update rule. Technically, this corresponds to changing the TD algorithm of <xref ref-type="disp-formula" rid="eqn1">Equation 1</xref> to a TD (<italic>λ</italic>) algorithm (see Methods, <xref ref-type="boxed-text" rid="box1">Algorithm 1</xref>). Simulating this extended model we found that, similar to the original model, it reproduces the main experimental findings (<xref ref-type="supplementary-material" rid="data1">Figure S1</xref>, compare with <xref ref-type="fig" rid="fig6">Figure 6</xref>). Moreover, as predicted, learning is faster than that the learning in the original model (<xref ref-type="supplementary-material" rid="data1">Figure S1d</xref>, compare with <xref ref-type="fig" rid="fig6">Figure 6d</xref>). Nevertheless, even this faster learning is still slower than the rapid learning observed in human participants, suggesting further components of human learning that are not captured by either of the models (we get back to this point in the Discussion).</p>
<p>Another way of distinguishing between 1-step and trajectory-based updates is to consider the predictions they make in Experiment 2. Recall that the three conditions in Experiment 2 differ in the delay (in the sense of number of states) between <italic>S</italic> and <italic>M<sub>R</sub></italic>. If information (about the exploratory “value” of <italic>M<sub>R</sub></italic>) propagates one step at a time, then the time it takes to learn that “right” is preferable in <italic>S</italic> will increase with the delay: it will be shortest in Condition 1, in which <italic>M<sub>R</sub></italic> and <italic>M<sub>L</sub></italic> are merely one step ahead of <italic>S</italic>, and longest in Condition 3, in which <italic>M<sub>R</sub></italic> and <italic>M<sub>L</sub></italic> are three steps away from <italic>S</italic> (<xref ref-type="fig" rid="fig7">Figure 7</xref>, top left). By contrast, if information about <italic>M<sub>R</sub></italic> and <italic>M<sub>L</sub></italic> can “jump” directly to <italic>S</italic> within each episode, as in trajectory-based updates, learning speed will be comparable in all three conditions (<xref ref-type="fig" rid="fig7">Figure 7</xref>, top right). A more thorough analysis of the model dependence on the parameters <italic>γ</italic> and <italic>λ</italic> is depicted in <xref ref-type="supplementary-material" rid="data2">Figure S2</xref>. Finally, <xref ref-type="fig" rid="fig7">Figure 7</xref> (bottom) depicts the learning dynamics of the “good” human explorers, analyzed separately in the three conditions of Experiment 2. We did not find evidence supporting the hypothesis that learning time increases with depth. These results further support the hypothesis that human learning relies on more global, temporally-extended update rules in which information can “jump” backwards over several states and actions.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7</label>
<caption><title>1-step backups and trajectory-based updates.</title><p>Learning dynamics simulated by the <italic>E</italic>-values model using the 1-step backup learning rule of TD (0) (<xref ref-type="disp-formula" rid="eqn1">Equation 1</xref>–<xref ref-type="disp-formula" rid="eqn2">2</xref>; <bold>top left</bold>) and the trajectory-based learning rule TD (<italic>λ</italic>) (Methods, <xref ref-type="boxed-text" rid="box1">Algorithm 1</xref>; <bold>top right</bold>) in the 3 environments of Experiment 2. With TD (0), the depth of <italic>M<sub>R</sub></italic> relative to <italic>S</italic> (depth = 1, 2, 3) affects both the peak value of <italic>p<sub>R</sub></italic> (<italic>t</italic>) (due to temporal discounting) and the time it takes the model to learn (due to the longer sequence of states over which the information has to be propagated). By contrast, with TD (<italic>λ</italic>), different depths result in a different maximum bias (due to temporal discounting), but the learning time is comparable (because information is propagated over multiple steps in each update). For the same reason, learning is overall faster with TD (<italic>λ</italic>). In humans (<bold>bottom</bold>), peak bias decreased with depth (consistent with temporal-discounting), but there was no noticeable difference in learning speed (consistent with trajectory-based updates). Learning curves of human participants are shown with a moving-average of 3 episodes. Dots and shaded areas denote means and 70% confidence intervals of <italic>p<sub>R</sub></italic> (<italic>t</italic>). Model results are average over 30, 000 simulations; model parameters: <italic>η</italic> = 0.9, <italic>β</italic> = 5, <italic>γ</italic> = 0.6, and <italic>λ</italic> = 0.6 (for the TD (<italic>λ</italic>) model).</p></caption>
<graphic xlink:href="s96b2v2_fig7.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
</sec>
</sec>
</sec>
<sec id="s3" sec-type="discussion">
<title>Discussion</title>
<p>Exploration is a wide phenomenon that has been linked to different aspects of behavior, including foraging (<xref ref-type="bibr" rid="CIT0031">Mobbs et al., 2018</xref>; <xref ref-type="bibr" rid="CIT0023">Kolling and Akam, 2017</xref>), curiosity (<xref ref-type="bibr" rid="CIT0018">Gottlieb and Oudeyer, 2018</xref>), and creativity (<xref ref-type="bibr" rid="CIT0020">Hart et al., 2018</xref>). In this study, we focused on exploration as part of learning. For that, we use the framework of RL, in which exploration is an essential component. Particularly, we study the computational principles underlying human exploration in complex environments – sufficiently complex such that exploration <italic>per se</italic> requires learning, due to delayed and long-term consequences of actions. Our approach builds on the analogy between the challenges of learning to explore, and the challenges of learning to maximize reward – the latter being the standard RL scenario. In both cases, the agent needs to represent information, propagate it, and use it to choose actions. In the former case it is information about uncertainty and in the latter it is information about expected reward.</p>
<p>We found that while exploring in complex environments, humans are sensitive to long-term consequences of actions and not only to local measures of uncertainty. Moreover, such longterm exploratory consequences are temporally-discounted, similar to the discounting of future rewards. Finally, the dynamics of exploration is consistent with the predictions of uncertainty-driven exploration, in which directed exploratory behavior peaks transiently, and then decay to a more random exploration (supposedly when most of the uncertainty have been resolved). To account for these experimental results, we introduce a computational model that uses a RL-like learning rule implementing the aforementioned principles. In the model, information about state-action visits, rather than about reward as in standard RL algorithms, is being propagated (and discounted) over sequences visited state-actions. This results in a set of “exploration values” (analogous to reward-based values) which are then used to choose actions.</p>
<p><bold>Directed exploration beyond bandit tasks</bold>   Previous studies have identified some components of directed exploration in human behavior using bandit tasks (<xref ref-type="bibr" rid="CIT0055">Wilson et al., 2014</xref>; <xref ref-type="bibr" rid="CIT0015">Gershman, 2018</xref>, <xref ref-type="bibr" rid="CIT0016">2019</xref>), particularly, the use of counter-based methods such as Upper Confidence Bounds (UCB, <xref ref-type="bibr" rid="CIT0001">Auer et al., 2002</xref>). Going beyond the bandit, we were able to show that these counter-based strategies might be a special case implementation (appropriate for bandit tasks) of more general principles. To study and identify these principles, it is therefore necessary to test human exploration in environments that are more complex than the bandit task. Indeed a more recent study have shown that more general principles might underlie human exploration, both random and directed, in sequential tasks (<xref ref-type="bibr" rid="CIT0054">Wilson et al., 2020</xref>). However, unlike our experiments, in that study actions did not have long-term consequences in the sense of state transitions. Finally, the necessity of going beyond simple bandit tasks is not unique to the study of exploration alone. It is present also when studying other components of RL algorithms underlying operant learning. For example, it is impossible to distinguish in a bandit task between <italic>model-based</italic> and <italic>model-free</italic> RL, because there is no “model” to be learned in those tasks (<xref ref-type="bibr" rid="CIT0008">Daw et al., 2011</xref>).</p>
<p><bold>Non-stationary aspects of exploration</bold>   While the analogy between learning to explore and learning to maximize rewards is a useful one, there are some important differences. One difference is that while in RL, rewards (more precisely, the distribution thereof) are typically assumed to be Markovian and stationary, exploration has a fundamental non-stationary nature. This is due to the fact that if exploration is interpreted as part of the learning process, or is uncertainty driven, then the exploratory “reward” from a given state-action will <italic>decrease</italic> over time, because uncertainty will reduce with visits of that state-action. This non-stationarity poses a challenge for exploration algorithms. The <italic>E</italic>-values model circumvents that by assuming a stationary (and constant) zero fictitious “reward”, combined with an optimism bias at initialization (<xref ref-type="bibr" rid="CIT0013">Fox et al., 2018</xref>).</p>
<p>A different solution to the challenge of non-stationarity is to posit an exploration objective function which is by itself independent of learning. The predictions of the two classes of models differ with respect to the expected steady-state behavior. In the former, exploration will diminish over time while in the latter, it will be sustained. The observation that human participants maintain a preference (albeit relatively small) for “right” even at the end of the experiment suggests that human exploration is driven, at least in part, by more than just uncertainty. A more complete characterization of these two components will be an interesting topic for future work.</p>
<p><bold>Pure-exploration and the role of reward</bold>   It has been long argued that at least part of human and animal behavior is driven by intrinsic motivation, which is largely independent of external rewards (<xref ref-type="bibr" rid="CIT0037">Oudeyer and Kaplan, 2009</xref>; <xref ref-type="bibr" rid="CIT0002">Barto, 2013</xref>). Pure exploration tasks can be used to characterize aspects of such intrinsic motivation. In this study, the “desire” to visit less-visited states is one such intrinsic motivation factor. Additional factors that are based on information-theoretic quantities (<xref ref-type="bibr" rid="CIT0045">Still and Precup, 2012</xref>; <xref ref-type="bibr" rid="CIT0027">Little and Sommer, 2014</xref>; <xref ref-type="bibr" rid="CIT0022">Houthooft et al., 2016</xref>) or prediction errors of non-reward signals (<xref ref-type="bibr" rid="CIT0038">Pathak et al., 2017</xref>; <xref ref-type="bibr" rid="CIT0006">Burda et al., 2019</xref>) have also been proposed in the literature. While many of these will, in general, be correlated, and hence difficult to identify experimentally, we believe that future studies of pure-exploration in complex environments will allow to better relate these concepts, mostly discussed in the theoretical and computational literature, to the learning and behavior of humans and animals.</p>
<p>To dissect the exploratory component of behavior, we focused on a pure-exploration, reward-free task. This allowed us to neutralize the exploration-exploitation dilemma, focusing on the unique challenges for exploration itself. More generally, we expect the identified exploration principles to be relevant also in the reward maximization scenario. Indeed, it has been shown theoretically and empirically that the naive use of counter-based methods (or other “local” exploration techniques) can be highly sub-optimal for learning an optimal policy (in the reward maximization sense) in complex environments (<xref ref-type="bibr" rid="CIT0034">Osband et al., 2016a</xref>,<xref ref-type="bibr" rid="CIT0035">b</xref>; <xref ref-type="bibr" rid="CIT0007">Chen et al., 2017</xref>; <xref ref-type="bibr" rid="CIT0013">Fox et al., 2018</xref>; <xref ref-type="bibr" rid="CIT0033">Oh and Iyengar, 2018</xref>). How humans deal with the exploration-exploitation dilemma in complex environments is an important open question.</p>
<p><bold>Implications for neuroscience</bold>   Algorithms such as TD-learning hold considerable sway in neuroscience. For example, it is generally believed that dopaminergic neurons encode reward prediction errors, which are used for learning the “values” of states and actions (<xref ref-type="bibr" rid="CIT0041">Schultz et al., 1997</xref>; <xref ref-type="bibr" rid="CIT0017">Glimcher, 2011</xref>, but see also <xref ref-type="bibr" rid="CIT0011">Elber-Dorozko and Loewenstein, 2018</xref>). More recent studies suggest that in fact, the brain maintains a separate representation of different reward dimensions (<xref ref-type="bibr" rid="CIT0044">Smith et al., 2011</xref>; <xref ref-type="bibr" rid="CIT0019">Grove et al., 2022</xref>). Given that our formalism of uncertainty (<italic>E</italic>-values) is identical to that of other types of value, it would be interesting to test whether the representation of uncertainty in the brain is similar to that of other reward types. For example, whether dopaminergic neurons also represent the equivalent of <italic>E</italic>-values TD-error. Along the same lines, it would be interesting to check whether the finding that dopaminergic neurons encode what seems to be reward-independent features of the task (<xref ref-type="bibr" rid="CIT0012">Engelhard et al., 2019</xref>) can be better understood assuming that uncertainty is a reward-like measure.</p>
<p><bold>Heterogeneity</bold>   There was a substantial heterogeneity among participants in both Experiments 1 and 2. We used this heterogeneity to divide participants into “good” and “poor” explorers in terms of the “directedness” of their exploration. However, this division is somewhat crude. For example, while bias in favor of <italic>M<sub>R</sub></italic> was smaller in the “poor” explorers, it was still larger than the baseline level of 0.5 predicted by a true random exploration behavior (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). This separation can be understood as a first approximation, highlighting the more prominent source of exploratory behavior at the individual subject basis. Moreover, even within the “good” explorers, there was considerable variability. Heterogeneity in the parameters of the computational model can, perhaps, explain some of the heterogeneity, but parameters variability alone (within the <italic>E</italic>-values model) certainly cannot explain all of the heterogeneity in participants’ behavior. For example, consider again the division to “poor” and “good” directed explorers. In principle, such a division could be modeled through the gain parameter <italic>β</italic>, with random explorers having a value of <italic>β</italic> = 0 (and directed explorers a value of <italic>β</italic> &gt; 0). Even with random exploration, the model prediction for <italic>p</italic><sub>repeat</sub> is 1/<italic>n<sub>R</sub></italic>. By contrast, many participants exhibited values of <italic>p</italic><sub>repeat</sub> larger than this chance-level, all the way up to <italic>p</italic><sub>repeat</sub> = 1. Similarly, considering behavior at <italic>S</italic> as measured by <italic>p<sub>R</sub></italic>, no combination of model parameters predict <italic>p<sub>R</sub></italic> values which are smaller than 0.5. This is because even random exploration will result in <italic>p<sub>R</sub></italic> = 0.5. Values of <italic>p<sub>R</sub></italic> that are close to 1 are also impossible in the model, because they imply under-exploration of the left-hand-side of the maze. Yet some human participants exhibited extreme (close to 0 or 1) values of <italic>p<sub>R</sub></italic>. Other factors, such as (task-independent) choice bias (<xref ref-type="bibr" rid="CIT0003">Baum, 1974</xref>; <xref ref-type="bibr" rid="CIT0024">Laquitaine et al., 2013</xref>; <xref ref-type="bibr" rid="CIT0026">Lebovich et al., 2019</xref>) and tendency to repeat actions (<xref ref-type="bibr" rid="CIT0051">Urai et al., 2019</xref>) are likely to contribute to participants’ choices.</p>
<p><bold>Learning speed</bold>   Another limitation of the model is the gap between the learning speed of human participants and the learning speed of the model. Overall, humans learned considerably faster than the model, even with a large learning-rate. On average participants exhibited a bias as soon as the 3<sup>rd</sup> episode, which is faster than the theoretical limit possible for the TD(0) model in this task. While some of this discrepancy can be attributed to the model’s reliance on 1-step backups, it is noteworthy that even in comparison with TD(<italic>λ</italic>), humans’ learning is faster than the that of the model. The rapid learning in humans suggest mechanisms that go beyond simple <italic>model-free</italic> learning as implemented in our models. In our model, the fact that “right” is favorable can only be learned implicitly, by actually visiting more unique states following <italic>M<sub>R</sub></italic> (compared to <italic>M<sub>L</sub></italic>). This is because the only information that is available to the agent is the identity of states and actions. By contrast, a single visit of both <italic>M<sub>R</sub></italic> an <italic>M<sub>L</sub></italic> is likely sufficient for humans to learn that the number of doors in <italic>M<sub>R</sub></italic> is larger than in <italic>M<sub>L</sub></italic>, a fact which can by itself bias their following choices in favor of “right”. Indeed by using this (possibly salient) feature, of the number of doors, as an explicit part of the state representation, one could infer that <italic>M<sub>R</sub></italic> is more favorable over <italic>M<sub>L</sub></italic> already after 2 episodes even with model-free learning. While such strategy is not as <italic>general</italic> as the computational principles encapsulated by our models, in the <italic>specific</italic> task at hand it will be rather effective. The ability of humans to rapidly form and utilize such heuristics and generalizations is likely an important part of their ability to rapidly adapt and learn in novel situations. The interplay between basic, more general-purpose, computational principles, and heuristic, more ad-hoc, principles remains an important challenge for computational modeling in the cognitive sciences.</p>
<p><bold>Generalization, priors, and “natural” exploration</bold> The goal of this study was to identify computational principles underlying exploration in a “general” setting. To that goal, we used a task in which the semantic content attached to states was minimal, with no a-priori indication of any structure (temporal, geometric, spatial, etc.) of the state-space. The motivation behind this design was to de-emphasize, as much as possible, behavior components stemming from participants’ prior knowledge and generalization abilities, and focus on core exploratory strategies. This also justified the models that we used: general-purpose, simplistic, learning models that operate on an abstract notion of states and actions. On the other hand, the abstract design of the task limits its applicability to more realistic tasks and natural behavior. Indeed in complex environments, it has been demonstrated that humans rely largely on both priors and generalizations to achieve efficient learning and exploration (<xref ref-type="bibr" rid="CIT0010">Dubey et al., 2018</xref>; <xref ref-type="bibr" rid="CIT0042">Schulz et al., 2020</xref>). How such priors, semantic knowledge, and generalization interact with more abstract and general principles of exploration and decision-making is an important open question. Notably, we have found that humans are capable of performing directed exploration of complex environments even in the absence of a readily-available semantic structure to guide their exploration. This is in contrast to the recent work of <xref ref-type="bibr" rid="CIT0005">Brändle et al. (2022)</xref>, that demonstrated directed exploration (interpreted as driven by the information-theoretic quantity of <italic>empowerment</italic>) in complex environments with available semantic structure, that was not observed in a structurally identical task where the semantic structure has been masked.</p>
</sec>
<sec id="s4" sec-type="methods">
<title>Methods</title>
<sec id="s4-1">
<title>Online experiments and data collection</title>
<p>The study was approved by the Hebrew University Committee for the Use of Human Subjects in Research. Participants were recruited using the Amazon MechanicalTurk online platform, and were randomly assigned to one of the conditions in each experiment. Participants were instructed to “understand how the rooms are connected”, and were informed regarding the test phase: “At the end of the task, a test will check how quickly can you get from one specific room to a different one.”. The training phase of the experiment consisted of 120 trials, corresponding to 20 episodes. Between 20% to 30% of participants (depending on the experiment and condition) performed a longer experiment of 250 trials corresponding to 42 episodes, but for these participants only the first 20 episodes were analyzed. The end of each episode (reaching the terminal state <italic>T</italic>) was signaled by a message screen (“Youv’e reached a dead-end room, and will be moved back to the first room”). After the training episodes, there was a test phase in which participants were asked to navigate to a target room in the minimal number of steps possible, starting from a particular start room (which was not the initial state <italic>S</italic>). An online working copy of the experiment can be accessed at: <ext-link ext-link-type="uri" xlink:href="https://decision-making-lab.com/lf/eee_rep/Instructions.php">https://decision-making-lab.com/lf/eee_rep/Instructions.php</ext-link>.</p>
<p>For each participant, we recorded the sequence of visited rooms (states) and chosen doors (actions), in the train and test phases. No other details (including demographics details, questionnaire, or comments about the experiment) were collected from participants. Test performance was used as a criterion for filtering. Out of the total participants who finished the experiment (i.e., finished both training and test phases), we rejected those who did not finish the test phase in a number of steps smaller than expected by chance (e.g., the expected number of steps it would take to reach the target by random walk). We also rejected participants who, during training, did not choose both “right” and “left” at least twice. The test start and target rooms were identical for all participants, and were chosen as to maximize the difference between performance (i.e., number of steps) expected by chance to that of the optimal (shortest path) policy. The number of participants in each experiment is given in <xref ref-type="table" rid="tab1">Table 1</xref>, and their division into “Good” and “Poor” explorers is given in <xref ref-type="table" rid="tab2">Table 2</xref>.</p>
<table-wrap id="tab1">
<label>Table 1</label>
<caption><p>Number of participants in Experiments 1 and 2.</p></caption>
<alternatives>
<graphic xlink:href="s96b2v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
<table frame="hsides" rules="all">
<thead>
<tr>
<th align="left">Exp.</th>
<th align="center">Env.</th>
<th align="center">Completed</th>
<th align="center">Included</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="3">1</td>
<td align="center">3 : 4</td>
<td align="center">191</td>
<td align="center">161</td>
</tr>
<tr>
<td align="center">2 : 5</td>
<td align="center">174</td>
<td align="center">120</td>
</tr>
<tr>
<td align="center">1 : 6</td>
<td align="center">176</td>
<td align="center">137</td>
</tr>
<tr>
<td align="left" rowspan="3">2</td>
<td align="center"><italic>d</italic> = 1</td>
<td align="center">244</td>
<td align="center">191</td>
</tr>
<tr>
<td align="center"><italic>d</italic> = 2</td>
<td align="center">269</td>
<td align="center">205</td>
</tr>
<tr>
<td align="center"><italic>d</italic> = 3</td>
<td align="center">282</td>
<td align="center">238</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tab2">
<label>Table 2</label>
<caption><p>Participant groups in Experiments 1 and 2</p></caption>
<alternatives>
<graphic xlink:href="s96b2v2_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
<table frame="hsides" rules="all">
<thead>
<tr>
<th align="left">Exp.</th>
<th align="center">Env.</th>
<th align="center">“good” explorers</th>
<th align="center">“poor” explorers</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="3">1</td>
<td align="center">3 : 4</td>
<td align="center">66</td>
<td align="center">95</td>
</tr>
<tr>
<td align="center">2 : 5</td>
<td align="center">53</td>
<td align="center">67</td>
</tr>
<tr>
<td align="center">1 : 6</td>
<td align="center">71</td>
<td align="center">66</td>
</tr>
<tr>
<td align="left" rowspan="3">2</td>
<td align="center"><italic>d</italic> = 1</td>
<td align="center">92</td>
<td align="center">99</td>
</tr>
<tr>
<td align="center"><italic>d</italic> = 2</td>
<td align="center">84</td>
<td align="center">121</td>
</tr>
<tr>
<td align="center"><italic>d</italic> = 3</td>
<td align="center">85</td>
<td align="center">153</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="s4-2">
<title>Estimating policy from behavior</title>
<p>For the average results, we computed for each participant their <italic>p<sub>R</sub></italic> value as the number of “right” choices divided by the total (and fixed) number of visits to <italic>S</italic>. Similarly, <italic>p</italic><sub>repeat</sub> was calculated for individual participants as the number of visits to <italic>M<sub>R</sub></italic> in which the chosen action was identical to the one chosen in their previous visit of <italic>M<sub>R</sub></italic>, divided by the total visits of <italic>M<sub>R</sub></italic> minus one. Note that the total number of visits to <italic>M<sub>R</sub></italic> was different for different participants, as it depended on their policy at <italic>S</italic>. We have used the same measurements for the results of the model simulations for consistency. Note that, in principle, the model allows to measure the policy of individual agents (at individual time-points) directly, without the need to estimate it from behavior (i.e., the generated stochastic choices). To estimate learning dynamics, we can no longer estimate <italic>p<sub>R</sub></italic> (<italic>t</italic>) on an individual level, because each participant only made one binary choice at a given episode. Therefore, we computed <italic>p<sub>R</sub></italic> (<italic>t</italic>) at the population level, as the number of participants who chose “right” in the <italic>t</italic><sup>th</sup> episode divided by the total number of participants (possibly within a particular group, for example only “good” explorers). Alternatively, when considering specific experimental conditions, we have estimated <italic>p<sub>R</sub></italic> (<italic>t</italic>) for individual participants using a moving-average over a window of 3 consecutive episodes.</p>
</sec>
<sec id="s4-3">
<title>Statistical analysis</title>
<p>Confidence Intervals (CI) for <italic>p<sub>R</sub></italic> were computed using bootstrapping, by resampling participants and choices. Comparisons between different conditions were computed using a permutation test, by shuffling all participants of the two groups being compared, and resampling under the null hypothesis of no group difference. With this resampling we computed the distribution of <italic>p<sub>R</sub></italic> (A) − <italic>p<sub>R</sub></italic> (B) for two random shuffled groups of participants A and B. Reported p-value is the CDF of this distribution evaluated at the real (unshuffled) groups.</p>
</sec>
<sec id="s4.4">
<title>TD (<italic>λ</italic>) learning for <italic>E</italic>-values</title>
<p>We start by proving a short, non-technical description of the TD and TD (<italic>λ</italic>) value-learning algorithms. The <italic>value</italic> of a state-action (denoted <italic>Q</italic> (<italic>s, a</italic>)), is defined as the expected sum of (discounted) rewards achieved following that state-action. The goal of the algorithms is to learn these values. To that end, the agent maintains and updates estimates <inline-formula id="ID4"><alternatives><mml:math display="inline" id="I4"><mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="s96b2v2_ieq4.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives></inline-formula> of the true state-action values <italic>Q</italic> (<italic>s, a</italic>). In TD-learning, Upon observing a transition (<italic>s, a, r, s′, a′</italic>), the estimated value <inline-formula id="ID5"><alternatives><mml:math display="inline" id="I5"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>Q</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="s96b2v2_ieq5.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives></inline-formula> is updated towards <inline-formula id="ID6"><alternatives><mml:math display="inline" id="I6"><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mover accent="true"><mml:mi>Q</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="s96b2v2_ieq6.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives></inline-formula>. Crucially, <inline-formula id="ID7"><alternatives><mml:math display="inline" id="I7"><mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="s96b2v2_ieq7.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives></inline-formula> is also, on its own, an estimated value. This usage of (a part of) the current estimator to form the target for updating the same estimator is known as <italic>bootstrapping</italic>. TD learning therefore breaks the estimation of value – the sum of rewards – into two parts: the first reward, which is taken from the environment, and the rest of the sum, which is bootstrapped.</p>
<p>It is possible, however, to estimate the values while breaking the sum of rewards in other ways. For example one could sum the first two rewards based on observations, and bootstrap the rest, that is, from time-step 3 on-wards. Importantly, this would result in information (about the rewards) propagating backwards 2-steps in a single update, rather than 1-step. More generally, breaking the sum after <italic>n</italic> steps will result in an <italic>n</italic>-step backup learning rule. It is also possible to average multiple <italic>n</italic>-step backups in a single update. The TD (<italic>λ</italic>) algorithm is a particular popular scheme to do that: it can be understood as combining <italic>all</italic> possible <italic>n</italic>-step backups, with a weighting function that decays exponentially with <italic>n</italic> (i.e., the weight given to the <italic>n</italic>-step backup is <italic>λ</italic><sup><italic>n</italic>−1</sup>, where <italic>λ</italic> is a parameter). With <italic>λ</italic> = 0 the algorithm recovers the standard 1-step backup algorithm, or in other words, TD (0) is simply TD. A value of <italic>λ</italic> = 1 corresponds to no bootstrapping at all, relying instead on <italic>Monte Carlo</italic> estimates of the action value by collecting direct samples (sum of rewards over complete trajectories).<sup><xref ref-type="fn" rid="fn02">2</xref></sup></p>
<p><xref ref-type="disp-formula" rid="eqn1">Equation 1</xref> can be understood as a TD algorithm (specifically, using the sarsa algorithm (<xref ref-type="bibr" rid="CIT0039">Rummery and Niranjan, 1994</xref>; <xref ref-type="bibr" rid="CIT0048">Sutton and Barto, 2018</xref>)) in the particular case that all the rewards signals are assumed to be <italic>r</italic> = 0, and estimates are initialized at 1. The extended model (<xref ref-type="boxed-text" rid="box1">Algorithm 1</xref>) is a direct generalization of that correspondence to the TD (<italic>λ</italic>) case.</p>
<boxed-text id="box1">
<caption><p>Algorithm 1 TD (<italic>λ</italic>) learning for <italic>E</italic>-values</p></caption>
<alternatives>
    <graphic mime-subtype="tiff" xlink:href="algo1.tif" mimetype="image"/>
    <preformat preformat-type="algorithm">
        <bold>Require:</bold> Parameters <italic>η, λ, γ</italic>
        <bold>Initialize</bold> <italic>E</italic> (<italic>s, a</italic>) = 1 for all <italic>s, a</italic>
        <bold>for</bold> all episodes <bold>do</bold>
        <bold>set</bold> <italic>ε</italic> (<italic>s, a</italic>) = 0 for all <italic>s, a</italic>                                                                                                                                                                                                                                                                       ⊳ eligibility-traces
        <bold>set</bold> <italic>τ</italic> = {}                                                                                                                                                                                                                                                                                                                   ⊳ trajectory in this episode
        set <italic>s</italic> to the initial state and choose action <italic>a</italic>
        <bold>while</bold> <italic>s</italic> is not a terminal state <bold>do</bold>
        sample the next state and action <italic>s′, a′</italic>
        increment <italic>ε</italic> (<italic>s, a</italic>) ← <italic>ε</italic> (<italic>s, a</italic>) + 1 and concatenate (<italic>s, a</italic>) to <italic>τ</italic>
        <bold>for all</bold> (<italic>s<sub>t</sub>, a<sub>t</sub></italic>) in <italic>τ</italic> <bold>do</bold>
        <italic>E</italic> (<italic>s<sub>t</sub>, a<sub>t</sub></italic>) ← <italic>E</italic> (<italic>s<sub>t</sub>, a<sub>t</sub></italic>) + <italic>ηε</italic> (<italic>s<sub>t</sub>, a<sub>t</sub></italic>) (<italic>γE</italic> (<italic>s′, a′</italic>) − <italic>E</italic> (<italic>s, a</italic>))        ⊳ update <italic>E</italic>-value
        <italic>ε</italic> (<italic>s<sub>t</sub>, a<sub>t</sub></italic>) ← <italic>γλε</italic> (<italic>s<sub>t</sub>, a<sub>t</sub></italic>)                                                                                                                                                                    ⊳ decay eligibility-trace
        <bold>end for</bold>
        <italic>s ← s′, a ← a′</italic>
        <bold>end while</bold>
        <italic>E</italic> (<italic>s, a</italic>) ← (1 − <italic>η</italic>) <italic>E</italic> (<italic>s, a</italic>)                                                                                                                                                                             ⊳ update in terminal-state
        <bold>end for</bold>
    </preformat>
</alternatives>
</boxed-text>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="CIT0001"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Auer</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Cesa-Bianchi</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>and Fischer</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Finite-time analysis of the multiarmed bandit problem</article-title>. <source>Machine learning</source>, <volume>47</volume>(<issue>2-3</issue>):<fpage>235</fpage>–<lpage>256</lpage>.</mixed-citation></ref>
<ref id="CIT0002"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Barto</surname>, <given-names>A. G.</given-names></string-name></person-group> (<year>2013</year>). <chapter-title>Intrinsic motivation and reinforcement learning</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Baldassarre</surname>, <given-names>G.</given-names></string-name> and <string-name><surname>Mirolli</surname>, <given-names>M.</given-names></string-name></person-group>, editors, <source>Intrinsically motivated learning in natural and artificial systems</source>, pages <fpage>17</fpage>–<lpage>47</lpage>. <publisher-name>Springer</publisher-name>.</mixed-citation></ref>
<ref id="CIT0003"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baum</surname>, <given-names>W. M.</given-names></string-name></person-group> (<year>1974</year>). <article-title>On two types of deviation from the matching law: Bias and undermatching</article-title>. <source>Journal of the Experimental Analysis of Behavior</source>, <volume>22</volume>(<issue>1</issue>):<fpage>231</fpage>–<lpage>242</lpage>.</mixed-citation></ref>
<ref id="CIT0004"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Bellemare</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Srinivasan</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ostrovski</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Schaul</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Saxton</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Munos</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2016</year>). <chapter-title>Unifying count-based exploration and intrinsic motivation</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Lee</surname>, <given-names>D. D.</given-names></string-name>, <string-name><surname>Sugiyama</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Luxburg</surname>, <given-names>U. V.</given-names></string-name>, <string-name><surname>Guyon</surname>, <given-names>I.</given-names></string-name>, and <string-name><surname>Garnett</surname>, <given-names>R.</given-names></string-name></person-group>, editors, <source>Advances in Neural Information Processing Systems 29</source>, pages <fpage>1471</fpage>–<lpage>1479</lpage>. <publisher-name>Curran Associates, Inc</publisher-name>.</mixed-citation></ref>
<ref id="CIT0005"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Brändle</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Stocks</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Tenenbaum</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name>, and <string-name><surname>Schulz</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Intrinsically motivated exploration as empowerment</article-title>. <source>PsyArXiv Preprints</source></mixed-citation></ref>
<ref id="CIT0006"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Burda</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Edwards</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Storkey</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Klimov</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Exploration by random network distillation</article-title>. In <conf-name><italic>International Conference on Learning Representations</italic></conf-name>.</mixed-citation></ref>
    <ref id="CIT0007"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>R. Y.</given-names></string-name>, <string-name><surname>Sidor</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Abbeel</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Schulman</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Ucb exploration via q-ensembles</article-title>. <source>arXiv preprint</source> <pub-id pub-id-type="doi">10.48550/arXiv.1706.01502</pub-id>.</mixed-citation></ref>
<ref id="CIT0008"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Daw</surname>, <given-names>N. D.</given-names></string-name>, <string-name><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Seymour</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Dolan</surname>, <given-names>R. J.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Model-based influences on humans’ choices and striatal prediction errors</article-title>. <source>Neuron</source>, <volume>69</volume>(<issue>6</issue>):<fpage>1204</fpage>–<lpage>1215</lpage>.</mixed-citation></ref>
<ref id="CIT0009"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name></person-group> (<year>1992</year>). <article-title>The convergence of td (<italic>λ</italic>) for general <italic>λ</italic></article-title>. <source>Machine learning</source>, <volume>8</volume>:<fpage>341</fpage>–<lpage>362</lpage>.</mixed-citation></ref>
<ref id="CIT0010"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Dubey</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Agrawal</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Pathak</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Griffiths</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Efros</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Investigating human priors for playing video games</article-title>. In <person-group person-group-type="editor"><string-name><surname>Dy</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Krause</surname>, <given-names>A.</given-names></string-name></person-group>, editors, <conf-name><italic>Proceedings of the 35th International Conference on Machine Learning</italic>, volume 80 of <italic>Proceedings of Machine Learning Research</italic></conf-name>, pages <fpage>1349</fpage>–<lpage>1357</lpage>. <conf-loc>PMLR</conf-loc>.</mixed-citation></ref>
<ref id="CIT0011"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Elber-Dorozko</surname>, <given-names>L.</given-names></string-name> and <string-name><surname>Loewenstein</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Striatal action-value neurons reconsidered</article-title>. <source>eLife</source>, <volume>7</volume>:<fpage>e34248</fpage>.</mixed-citation></ref>
<ref id="CIT0012"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engelhard</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Finkelstein</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Cox</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Fleming</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Jang</surname>, <given-names>H. J.</given-names></string-name>, <string-name><surname>Ornelas</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Koay</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Thiberge</surname>, <given-names>S. Y.</given-names></string-name>, <string-name><surname>Daw</surname>, <given-names>N. D.</given-names></string-name>, <string-name><surname>Tank</surname>, <given-names>D. W.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2019</year>). <article-title>Specialized coding of sensory, motor and cognitive variables in vta dopamine neurons</article-title>. <source>Nature</source>, <volume>570</volume>(<issue>7762</issue>):<fpage>509</fpage>–<lpage>513</lpage>.</mixed-citation></ref>
<ref id="CIT0013"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Fox</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Choshen</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Loewenstein</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2018</year>). <article-title>DORA the explorer: Directed outreaching reinforcement action-selection</article-title>. In <conf-name><italic>International Conference on Learning Representations</italic></conf-name>.</mixed-citation></ref>
<ref id="CIT0014"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Fox</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Dan</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Elber-Dorozko</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Loewenstein</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2020</year>). <chapter-title>Exploration: from machines to humans</chapter-title>. <source>Current Opinion in Behavioral Sciences</source>, <volume>35</volume>:<fpage>104</fpage>–<lpage>111</lpage>. <publisher-name>Curiosity (Explore vs Exploit)</publisher-name>.</mixed-citation></ref>
<ref id="CIT0015"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Deconstructing the human algorithms for exploration</article-title>. <source>Cognition</source>, <volume>173</volume>:<fpage>34</fpage>–<lpage>42</lpage>.</mixed-citation></ref>
<ref id="CIT0016"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Uncertainty and exploration</article-title>. <source>Decision</source>, <volume>6</volume>(<issue>3</issue>):<fpage>277</fpage>.</mixed-citation></ref>
<ref id="CIT0017"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glimcher</surname>, <given-names>P. W.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Understanding dopamine and reinforcement learning: the dopamine reward prediction error hypothesis</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>108</volume>:<fpage>15647</fpage>–<lpage>15654</lpage>.</mixed-citation></ref>
<ref id="CIT0018"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gottlieb</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Oudeyer</surname>, <given-names>P.-Y.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Towards a neuroscience of active sampling and curiosity</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>19</volume>(<issue>12</issue>):<fpage>758</fpage>–<lpage>770</lpage>.</mixed-citation></ref>
<ref id="CIT0019"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grove</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Gray</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>La Santa Medina</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Sivakumar</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Ahn</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Corpuz</surname>, <given-names>T. V.</given-names></string-name>, <string-name><surname>Berke</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Kreitzer</surname>, <given-names>A. C.</given-names></string-name>, and <string-name><surname>Knight</surname>, <given-names>Z. A.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Dopamine subsystems that track internal states</article-title>. <source>Nature</source>, <volume>608</volume>(<issue>7922</issue>):<fpage>374</fpage>–<lpage>380</lpage>.</mixed-citation></ref>
<ref id="CIT0020"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hart</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Goldberg</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Striem-Amit</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Mayo</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Noy</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Alon</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Creative exploration as a scale-invariant search on a meaning landscape</article-title>. <source>Nature communications</source>, <volume>9</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="CIT0021"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Hazan</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Kakade</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Singh</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Van Soest</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Provably efficient maximum entropy exploration</article-title>. In <person-group person-group-type="editor"><string-name><surname>Chaudhuri</surname>, <given-names>K.</given-names></string-name> and <string-name><surname>Salakhutdinov</surname>, <given-names>R.</given-names></string-name></person-group>, editors, <conf-name><italic>Proceedings of the 36th International Conference on Machine Learning</italic>, volume 97 of <italic>Proceedings of Machine Learning Research</italic></conf-name>, pages <fpage>2681</fpage>–<lpage>2691</lpage>, <conf-loc>Long Beach, California, USA. PMLR</conf-loc>.</mixed-citation></ref>
<ref id="CIT0022"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Houthooft</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Duan</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Schulman</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>De Turck</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Abbeel</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Vime: Variational information maximizing exploration</article-title>. In <source>Advances in Neural Information Processing Systems</source>, pages <fpage>1109</fpage>–<lpage>1117</lpage>.</mixed-citation></ref>
<ref id="CIT0023"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kolling</surname>, <given-names>N.</given-names></string-name> and <string-name><surname>Akam</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2017</year>). <chapter-title>(reinforcement?) learning to forage optimally</chapter-title>. <source>Current Opinion in Neurobiology</source>, <volume>46</volume>:<fpage>162</fpage>–<lpage>169</lpage>. <publisher-name>Computational Neuroscience</publisher-name>.</mixed-citation></ref>
<ref id="CIT0024"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laquitaine</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Piron</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Abellanas</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Loewenstein</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Boraud</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Complex population response of dorsal putamen neurons predicts the ability to learn</article-title>. <source>PLOS ONE</source>, <volume>8</volume>(<issue>11</issue>):<comment>null</comment>.</mixed-citation></ref>
<ref id="CIT0025"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Lattimore</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Szepesvári</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2020</year>). <source>Bandit Algorithms</source>. <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation></ref>
<ref id="CIT0026"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lebovich</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Darshan</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Lavi</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hansel</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Loewenstein</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Idiosyncratic choice bias naturally emerges from intrinsic stochasticity in neuronal dynamics</article-title>. <source>Nature Human Behaviour</source>, <volume>3</volume>(<issue>11</issue>):<fpage>1190</fpage>–<lpage>1202</lpage>.</mixed-citation></ref>
<ref id="CIT0027"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Little</surname>, <given-names>D. Y.</given-names></string-name> and <string-name><surname>Sommer</surname>, <given-names>F. T.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Learning and exploration in action-perception loops</article-title>. <source>Closing the Loop Around Neural Systems</source>, page <fpage>295</fpage>.</mixed-citation></ref>
    <ref id="CIT0028"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Machado</surname>, <given-names>M. C.</given-names></string-name> and <string-name><surname>Bowling</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Learning purposeful behaviour in the absence of rewards</article-title>. <source>arXiv preprint</source>. <pub-id pub-id-type="doi">10.48550/arXiv.1605.07700</pub-id></mixed-citation></ref>
<ref id="CIT0029"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mehlhorn</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Newell</surname>, <given-names>B. R.</given-names></string-name>, <string-name><surname>Todd</surname>, <given-names>P. M.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Morgan</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Braithwaite</surname>, <given-names>V. A.</given-names></string-name>, <string-name><surname>Hausmann</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Fiedler</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Gonzalez</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Unpacking the exploration–exploitation tradeoff: A synthesis of human and animal literatures</article-title>. <source>Decision</source>, <volume>2</volume>(<issue>3</issue>):<fpage>191</fpage>.</mixed-citation></ref>
<ref id="CIT0030"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meuleau</surname>, <given-names>N.</given-names></string-name> and <string-name><surname>Bourgine</surname>, <given-names>P.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Exploration of multi-state environments: Local measures and back-propagation of uncertainty</article-title>. <source>Machine Learning</source>, <volume>35</volume>(<issue>2</issue>):<fpage>117</fpage>–<lpage>154</lpage>.</mixed-citation></ref>
<ref id="CIT0031"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mobbs</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Trimmer</surname>, <given-names>P. C.</given-names></string-name>, <string-name><surname>Blumstein</surname>, <given-names>D. T.</given-names></string-name>, and <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Foraging for foundations in decision neuroscience: insights from ethology</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>19</volume>(<issue>7</issue>):<fpage>419</fpage>–<lpage>427</lpage>.</mixed-citation></ref>
<ref id="CIT0032"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mongillo</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Shteingart</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Loewenstein</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2014</year>). <article-title>The misbehavior of reinforcement learning</article-title>. <source>Proceedings of the IEEE</source>, <volume>102</volume>(<issue>4</issue>):<fpage>528</fpage>–<lpage>541</lpage>.</mixed-citation></ref>
    <ref id="CIT0033"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Oh</surname>, <given-names>M.-h.</given-names></string-name> and <string-name><surname>Iyengar</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Directed exploration in pac model-free reinforcement learning</article-title>. <source>arXiv preprint</source>. <pub-id pub-id-type="doi">10.48550/arXiv.1808.10552</pub-id></mixed-citation></ref>
<ref id="CIT0034"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Osband</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Blundell</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Pritzel</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Van Roy</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2016a</year>). <article-title>Deep exploration via bootstrapped dqn</article-title>. In <source>Advances in neural information processing systems</source>, pages <fpage>4026</fpage>–<lpage>4034</lpage>.</mixed-citation></ref>
<ref id="CIT0035"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Osband</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Roy</surname>, <given-names>B. V.</given-names></string-name>, and <string-name><surname>Wen</surname>, <given-names>Z.</given-names></string-name></person-group> (<year>2016b</year>). <article-title>Generalization and exploration via randomized value functions</article-title>. In <person-group person-group-type="editor"><string-name><surname>Balcan</surname>, <given-names>M. F.</given-names></string-name> and <string-name><surname>Weinberger</surname>, <given-names>K. Q.</given-names></string-name></person-group>, editors, <conf-name><italic>Proceedings of The 33rd International Conference on Machine Learning</italic>, volume 48 of <italic>Proceedings of Machine Learning Research</italic></conf-name>, pages <fpage>2377</fpage>–<lpage>2386</lpage>, <conf-loc>New York, New York, USA. PMLR</conf-loc>.</mixed-citation></ref>
    <ref id="CIT0036"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Ostrovski</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Bellemare</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Oord</surname>, <given-names>A. v. d.</given-names></string-name>, and <string-name><surname>Munos</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Count-based exploration with neural density models</article-title>. <source>arXiv preprint</source>. <pub-id pub-id-type="doi">10.48550/arXiv.1703.01310</pub-id></mixed-citation></ref>
<ref id="CIT0037"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oudeyer</surname>, <given-names>P.-Y.</given-names></string-name> and <string-name><surname>Kaplan</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2009</year>). <article-title>What is intrinsic motivation? a typology of computational approaches</article-title>. <source>Frontiers in neurorobotics</source>, <volume>1</volume>:<fpage>6</fpage>.</mixed-citation></ref>
<ref id="CIT0038"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Pathak</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Agrawal</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Efros</surname>, <given-names>A. A.</given-names></string-name>, and <string-name><surname>Darrell</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Curiosity-driven exploration by self-supervised prediction</article-title>. In <source>ICML</source>.</mixed-citation></ref>
<ref id="CIT0039"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Rummery</surname>, <given-names>G. A.</given-names></string-name> and <string-name><surname>Niranjan</surname>, <given-names>M.</given-names></string-name></person-group> (<year>1994</year>). <source>On-line Q-learning using connectionist systems</source>. <publisher-name>University of Cambridge, Department of Engineering</publisher-name>.</mixed-citation></ref>
<ref id="CIT0040"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Schmidhuber</surname>, <given-names>J.</given-names></string-name></person-group> (<year>1991</year>). <article-title>Curious model-building control systems</article-title>. In <conf-name><italic>Neural Networks, 1991. 1991 IEEE International Joint Conference on</italic></conf-name>, pages <fpage>1458</fpage>–<lpage>1463</lpage>. <conf-loc>IEEE</conf-loc>.</mixed-citation></ref>
<ref id="CIT0041"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schultz</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Montague</surname>, <given-names>P. R.</given-names></string-name></person-group> (<year>1997</year>). <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source>, <volume>275</volume>(<issue>5306</issue>):<fpage>1593</fpage>–<lpage>1599</lpage>.</mixed-citation></ref>
<ref id="CIT0042"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schulz</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Franklin</surname>, <given-names>N. T.</given-names></string-name>, and <string-name><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Finding structure in multi-armed bandits</article-title>. <source>Cognitive Psychology</source>, <volume>119</volume>:<fpage>101261</fpage>.</mixed-citation></ref>
<ref id="CIT0043"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shteingart</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Neiman</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Loewenstein</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2013</year>). <article-title>The role of first impression in operant learning</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>142</volume>(<issue>2</issue>):<fpage>476</fpage>.</mixed-citation></ref>
<ref id="CIT0044"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>K. S.</given-names></string-name>, <string-name><surname>Berridge</surname>, <given-names>K. C.</given-names></string-name>, and <string-name><surname>Aldridge</surname>, <given-names>J. W.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Disentangling pleasure from incentive salience and learning signals in brain reward circuitry</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>108</volume>(<issue>27</issue>):<fpage>E255</fpage>–<lpage>E264</lpage>.</mixed-citation></ref>
<ref id="CIT0045"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Still</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Precup</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2012</year>). <article-title>An information-theoretic approach to curiosity-driven reinforcement learning</article-title>. <source>Theory in Biosciences</source>, <volume>131</volume>(<issue>3</issue>):<fpage>139</fpage>–<lpage>148</lpage>.</mixed-citation></ref>
<ref id="CIT0046"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Storck</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hochreiter</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Schmidhuber</surname>, <given-names>J.</given-names></string-name></person-group> (<year>1995</year>). <article-title>Reinforcement driven information acquisition in non-deterministic environments</article-title>. In <conf-name><italic>Proceedings of the international conference on artificial neural networks, Paris</italic></conf-name>, volume <volume>2</volume>, pages <fpage>159</fpage>–<lpage>164</lpage>. <conf-loc>Citeseer</conf-loc>.</mixed-citation></ref>
<ref id="CIT0047"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sutton</surname>, <given-names>R. S.</given-names></string-name></person-group> (<year>1988</year>). <article-title>Learning to predict by the methods of temporal differences</article-title>. <source>Machine learning</source>, <volume>3</volume>:<fpage>9</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
<ref id="CIT0048"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sutton</surname>, <given-names>R. S.</given-names></string-name> and <string-name><surname>Barto</surname>, <given-names>A. G.</given-names></string-name></person-group> (<year>2018</year>). <source>Reinforcement learning : an introduction</source>. <edition>Second edition</edition>.</mixed-citation></ref>
<ref id="CIT0049"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Houthooft</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Foote</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Stooke</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>O. X.</given-names></string-name>, <string-name><surname>Duan</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Schulman</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>DeTurck</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Abbeel</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2017</year>). <article-title># exploration: A study of count-based exploration for deep reinforcement learning</article-title>. In <source>Advances in Neural Information Processing Systems</source>, pages <fpage>2753</fpage>–<lpage>2762</lpage>.</mixed-citation></ref>
<ref id="CIT0050"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Thrun</surname>, <given-names>S. B.</given-names></string-name></person-group> (<year>1992</year>). <source>Efficient exploration in reinforcement learning</source>.</mixed-citation></ref>
<ref id="CIT0051"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Urai</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>de Gee</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Tsetsos</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Donner</surname>, <given-names>T. H.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Choice history biases subsequent evidence accumulation</article-title>. <source>eLife</source>, <volume>8</volume>:<fpage>e46331</fpage>.</mixed-citation></ref>
<ref id="CIT0052"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vanderveldt</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Oliveira</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Green</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Delay discounting: pigeon, rat, human—does it matter?</article-title> <source>Journal of Experimental Psychology: Animal learning and cognition</source>, <volume>42</volume>(<issue>2</issue>):<fpage>141</fpage>.</mixed-citation></ref>
<ref id="CIT0053"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Watkins</surname>, <given-names>C. J.</given-names></string-name> and <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name></person-group> (<year>1992</year>). <article-title>Q-learning</article-title>. <source>Machine learning</source>, <volume>8</volume>(<issue>3-4</issue>):<fpage>279</fpage>–<lpage>292</lpage>.</mixed-citation></ref>
<ref id="CIT0054"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Wilson</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sadeghiyeh</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Cohen</surname>, <given-names>J. D.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Deep exploration as a unifying account of explore-exploit behavior</article-title>. <source>PsyArXiv preprint</source>.</mixed-citation></ref>
<ref id="CIT0055"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilson</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Geana</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>White</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Ludvig</surname>, <given-names>E. A.</given-names></string-name>, and <string-name><surname>Cohen</surname>, <given-names>J. D.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Humans use directed and random exploration to solve the explore–exploit dilemma</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>143</volume>(<issue>6</issue>):<fpage>2074</fpage>.</mixed-citation></ref>
<ref id="CIT0056"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zahavy</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>O’ Donoghue</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Desjardins</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Singh</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2021</year>). <chapter-title>Reward is enough for convex mdps</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Ranzato</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Beygelzimer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dauphin</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Liang</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Vaughan</surname>, <given-names>J. W.</given-names></string-name></person-group>, editors, <source>Advances in Neural Information Processing Systems</source>, volume <volume>34</volume>, pages <fpage>25746</fpage>–<lpage>25759</lpage>. <publisher-name>Curran Associates, Inc</publisher-name>.</mixed-citation></ref>
<ref id="CIT0057"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Koppel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bedi</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Szepesvari</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Wang</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2020</year>). <chapter-title>Variational policy gradient method for reinforcement learning with general utilities</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Larochelle</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ranzato</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hadsell</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Balcan</surname>, <given-names>M. F.</given-names></string-name>, and <string-name><surname>Lin</surname>, <given-names>H.</given-names></string-name></person-group>, editors, <source>Advances in Neural Information Processing Systems</source>, volume <volume>33</volume>, pages <fpage>4572</fpage>–<lpage>4583</lpage>. <publisher-name>Curran Associates, Inc</publisher-name>.</mixed-citation></ref>
</ref-list>
<sec id="s5">
<title>Supplementary material</title>
<fig id="data1">
<label>Figure S1</label>
<caption><title>Simulations results of TD (<italic>λ</italic>).</title><p>Simulating behavior of the <italic>E</italic>-values model with the TD (<italic>λ</italic>) learning rule (Methods, <xref ref-type="boxed-text" rid="box1">Algorithm 1</xref>) reproduces the main findings of directed exploration in the maze task. <bold>(a)</bold> In <italic>M<sub>R</sub></italic>, the model exhibits directed exploration which manifests in low values of <italic>p</italic><sub>repeat</sub> (shown for the 3 conditions of Experiment 1; dashed line denote chance-level expected for random exploration, 1/<italic>n<sub>R</sub></italic>) <bold>(b)</bold> In the environments of Experiment 1, agents exhibited bias towards <italic>M<sub>R</sub></italic> that increased with imbalance of <italic>n<sub>R</sub></italic> : <italic>n<sub>L</sub></italic>, reflecting the propagation of long-term uncertainties over states. <bold>(c)</bold> In the environments of Experiment 2, the bias decreased with depth, reflecting temporal discounting. <bold>(d)</bold> Bias towards <italic>M<sub>R</sub></italic> peaks transiently, followed by a decay to baseline at steady-state, as expected from uncertainty-driven exploration (average results over all 6 environments). The learning dynamics is faster than that of the 1-step update model. Results are based on 3,000 simulations in each environment. Bars and histograms in (a)-(c) are shown for the first 20 episodes to match the behavioral experiments. Model parameters: <italic>η</italic> = 0.9, <italic>β</italic> = 5, <italic>γ</italic> = 0.6, <italic>λ</italic> = 0.6.</p></caption>
<graphic xlink:href="s96b2-data1.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<fig id="data2">
<label>Figure S2</label>
<caption><title>Model parameters.</title><p>Learning curves of the TD (<italic>λ</italic>) model in the 3 environments of Experiment 2 for different values of <italic>γ</italic>,<italic>λ</italic> (with fixed <italic>η</italic> = 0.9, <italic>β</italic> = 5). With infinite discounting (<italic>γ</italic> = 0), future consequences are neglected, resulting in a uniform (counter-based like) policy with no bias. With no discounting (<italic>γ</italic> = 1), information from the terminal state <italic>T</italic> dominates, resulting in a bias towards “right” (since there are more routes to the terminal states via the “right” branch) that is not dependent of the depth of <italic>M<sub>R</sub></italic>. For intermediate values of <italic>γ</italic>, transient exploration opportunities (i.e., in <italic>M<sub>R</sub></italic>) becomes important, resulting in a bias towards <italic>M<sub>R</sub></italic> that decreases with depth, reflecting temporal-discounting. In this regime, one-step backup learning rule (<italic>λ</italic> = 0) results in difference learning speed for different depths, while for trajectory-based learning rules (<italic>λ</italic> &gt; 0) learning speed is comparable for the different depths. Each learning curve is the average of 30, 000 simulations.</p></caption>
<graphic xlink:href="s96b2-data2.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
</sec>
<fn-group>
<fn id="fn01"><label>1</label><p>because <italic>η</italic> &lt; 1, we have that log (1 − <italic>η</italic>) &lt; 0.</p></fn>
<fn id="fn02"><label>2</label><p>Implicitly assuming an episodic setting, in which every episode terminates after a finite number of steps.</p></fn>
</fn-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90684.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Verstynen</surname>
<given-names>Timothy</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This well-written and well-reasoned manuscript describes a behavioral and computational modeling study designed to understand pure exploration, where action selection is driven by pure information seeking and not rewards. Using a novel task, the authors find that a subset of people use information value to drive their selection behavior, consistent with a simple information maximization model of reinforcement learning. The rest of the participants did not exhibit this behavior. This <bold>valuable</bold> work provides intriguing, yet somewhat <bold>incomplete</bold>, insights into understanding directed exploration and its computational form.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90684.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Fox, Dan, and Loewenstein investigated how people explored six maze-like environments. They show that roughly one-third of their participants make choices now that increase the potential for future information gain and also temporally discount potential information gain based on how far in the future potential gains might be. The authors argue that rather than valuing exploration in its own right, participant behavior is most consistent with using exploration as a way to reduce uncertainty. They then propose a reinforcement learning (RL) model in which agents estimate an &quot;exploration value&quot; (the expected cumulative information gained by taking a given action in a given state) using standard RL techniques for estimating value (expected cumulative reward). They find that this model exhibits several qualitative similarities with human behavior and that it best captures the temporal dynamics of human exploration when propagating information through the entire history of a behavioral episode (as opposed to merely propagating it in a single step as some of the simplest RL models do).</p>
<p>While the core insight and basic method of the paper are compelling, the way in which both the behavioral experiment and computational modeling were conducted raise concerns that mean that, in their present form, the results do not fully justify the conclusions. After resolving these issues, the work would demonstrate how human exploration is sensitive to long-range dependencies in information gain, as well as valuable insights about how best to characterize this behavior computationally. I am not particularly well-versed in the literature on exploration so cannot comment on novelty here.</p>
<p>Strengths:</p>
<p>
The entire paper is logically well-motivated. It builds on a valuable basic insight, namely that while bandit tasks are an ideally minimal platform for testing certain questions about decision-making and exploration, richer paradigms are needed to capture the long-range informational dependencies distinguishing between various approaches to exploration.</p>
<p>Even so, the maze navigation paradigm explored here remains simple. Participants navigate a maze with two main branches which are identical save for minimal, theoretically motivated differences. Moreover, the tested differences are designed to clearly and explicitly test well-identified questions. The task, and really the entire paper, is clearly organized, and each component is logically connected to a larger argument.</p>
<p>The proposed model is also simple, clearly presented, and a clever way of applying ideas typically used to reason about reward-motivated behavior to reason here about information-motivated behavior.</p>
<p>One other strength of this work is that it combines behavioral experiments with computational modeling. This approach pairs a detailed and objectively specified theory (i.e. the model) with novel data specifically designed to test that theory and thus in principle presents a particularly strong test of the authors' hypotheses.</p>
<p>Weaknesses:</p>
<p>
Despite many strengths in the underlying logic of the paper, the presented evidence does not provide compelling support for the conclusions. In particular:</p>
<p>- The main claims are based on the behavior of 452 participants classed as good explorers, out of 1,052 participants included in the analyses and 1,336 participants who completed the study. That is, the authors' broad claims about human exploration are based on a third of their total sample; the other two-thirds displayed very different behavior, including 20% who performed at or below chance levels. That is, while a significant sub-population may demonstrate the claimed abilities, it is far from clear that they are universal.</p>
<p>- While the experimental manipulations are elegant, the behavioral study seems underpowered. In each of the primary manipulations, key theoretical predictions are not statistically validated. For example, in Experiment 1, the preference for the right door increases from the 4:3 condition to the 5:2 condition, but not when moving from the 5:2 condition to the 6:1 condition, as predicted (Figure 1c). Similar results can be seen for other analyses in Figures 3b and 4b. Relatedly, the experiments comprised just 20 episodes, and it is unclear whether that was sufficiently long for participants to demonstrate asymptotic behavior (e.g. Figure 5b). Either more participants or greater differences between conditions (e.g. testing 9:8, 12:5, and 15:2 conditions in a revised Experiment 1), as well as running a greater number of total episodes, would be needed to resolve this concern.</p>
<p>- The model is presented after the behavioral results, giving the impression that it was perhaps constructed to fit the data. No attempt is made to fit the model to a subset of the data and then validate the rest or give any clear indication as to how the model parameters were set. Moreover, as noted, even where the model is successful, it only explains the behavior of a minority of the total participants. No modeling work is done to explain the behavior of the other two-thirds of the participants.</p>
<p>- The authors helpfully discuss several meaningful alternative models of exploration, such as visit-counting and incorporating an objective function sensitive to information gain. They do not, however, compare their model against these or any other meaningful baselines. Moreover, the comparison between model and human participants is qualitative rather than quantitative. These issues could be resolved by introducing a more rigorous analysis quantitatively comparing a variety of theoretically relevant models as quantitative explanations of the human data.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90684.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
In this article, the authors develop an algorithm for exploration inspired by the classic, state-action-reward-state-action (SARSA) reinforcement learning algorithm. Designed to account for exploration in multi-state environments, this algorithm computes the expected discounted return from selecting an action in a state and uses that value to update the cached value of taking a given action in a given state. The value represents the uncertainty in a given state, and the backed-up value is computed from the discounted future return plus the immediate reduction in uncertainty regarding the state.</p>
<p>Strengths:</p>
<p>
The article is ambitious and seeks to characterize human exploration in a novel task using zero rewards. That characterization is useful.</p>
<p>Weaknesses:</p>
<p>
The paper suffers from many problems. Here, I will mention three. First, the algorithm is very poorly motivated-exploration is central to many behaviors, but the algorithm computes the value of exploration independent of any long-run considerations of exploitation. Second, the article attempts to recover the observed exploratory behavior in two different multi-state choice tasks. But the algorithm does not explain that behavior, and there is no performance metric on the model, nor a comparison to other models. Third, the article frames the algorithm in terms of uncertainty, but there is no measure of uncertainty.</p>
<p>In short, in many ways this manuscript is 'half an article', and the authors have much work to do. They could decide to dive into the convergence proofs and other theoretical properties of the model. However, as far as I understand the model, it is literally an optimistic SARSA, whose characteristics are well-understood. Or, they could compare the model's performance to a number of other exploration models (UCB, Thompson sampling, infomax, infotaxis-there are so many!). However the authors need to choose one or the other. I urge the authors to properly compare their model to other models.</p>
<p>1. Motivation</p>
<p>
The algorithm is poorly motivated. Exploration is valuable for a time but quickly becomes less valued as more is learned about the environment. The algorithm attempts to account for this by the ad hoc nature of the backup: the immediate outcome is -E(s,a), which represents a reduction in uncertainty. So in the long run, the exploratory value will decrease to zero. But this is ad hoc; why not add E(s,a)? In addition, exploration values are set to 1. But this is also ad hoc; why should E(s,a) start at 1? They have cherry-picked their starting values and the nature of the back-up to yield exploratory behavior.</p>
<p>2. Performance</p>
<p>
The authors wish to compare the model's performance to observed exploration behavior. However, their model does a poor job of explaining the behavior. What's confusing is that the authors note the ways the model deviates. There are two principal deviations. First, the model exhibits an exploratory transient, but it is too wide to match the humans. Second, the model fails to exhibit the low-level persistent exploration characteristic of humans in their task.</p>
<p>The next natural step would be to augment the model in different ways to attempt to describe the behavior. The authors do attempt to import td-λ aspects into their exploration model. They determine that importation fails to capture the observed behavior. But why stop there? Why not continue? Why not follow through and change the model in a way that can capture the dynamics of exploration?</p>
<p>In addition, a natural complement would be to compare the model's ability to describe human performance to other models. This would require model fitting, recovery, and validation. However the authors don't engage in that model fitting exercise.</p>
<p>They note that a model-based learning strategy could account for the speed of learning in humans. However they don't comment generally on how model-based strategies could explain their findings nor how they relate to their model. They should comment on this. In particular, the participants are likely learning a model of their environment, and this can be done using non-parametric Bayesian inference (along the lines of Gershman or Collins's work). The authors should model their task using these models and compare this to their algorithm.</p>
<p>The authors state that there was no reward. Were subjects paid for their time? Also, the lack of a reward is unusual, and even if unconsciously, participants may have been engaged in reward-seeking. The authors should try to model the behavior with a pseudo-reward to see how that accounts for their findings. This is especially true from the perspective of computational RL. On that theory, the only object 'in' the agent is the policy; everything else is considered 'in' the environment. This means that rewards in RL need not be from environmental returns but could also be from inside the organism (even if modeled as 'outside' the agent in the RL framework). So they need to model the behavior using 'pseudorewards' to see if that can account for their findings. Finally, though trivially, a reward of 0 is technically a reward, and the model's exploratory drive comes from settling on the true values of the states (i.e., 0).</p>
<p>3. Uncertainty</p>
<p>
The authors frame their model in terms of uncertainty, but their model does not measure uncertainty at all. The model makes choices on the basis of optimistic initial Q-values and then searches on that basis, backing up the 0 rewards until the true values are more or less hit upon. But that is not a measure of uncertainty in any sense; rather, it is an optimistic Q-value bias that drives exploration. However, I may simply fail to understand their model.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90684.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
In this article, Fox and colleagues describe the results of a novel and innovative task, coupled with a modified computational model, to explore pure directed exploration (not quite a pun, but intended nonetheless). In their task, participants make a series of discrete choices, importantly with no reward feedback, to navigate a nested series of rooms in a virtual environment. The initial 2-door choice is used as the primary probe and the complexity of the series of rooms behind each choice is used as the critical independent variable. The authors find that, as the number of follow-up options behind a door increases, &quot;good&quot; participants are more likely to choose the door that leads to the more complex choices. As the depth of the search increased (i.e. the room with the most doors was presented &quot;farther&quot; down the search), these same participants were less likely to choose the door leading to the more complex route. Finally, these same &quot;good&quot; participants showed an initial boost in preference towards the more complex exploration option after a few learning episodes that settled down after about 10 episodes, with a modest reliable preference towards the more complex route. This reflected the fact that information value decays over time in stable situations. Using an adaptation of standard Q-learning, with a proxy of information value being substituted for reward value, the authors show how their model can qualitatively capture most of the observed experimental effects, although with some critical differences in the temporal dynamics of learning, suggesting that the memory horizon for humans is longer than in the adapted Q-learning model.</p>
<p>Strengths:</p>
<p>
1. Clever experimental design</p>
<p>
The novel task is really clever and gets around many of the limitations for understanding directed exploration that have plagued prior research (which typically involve some use of reward feedback). Finding a way to provide direct information that can be experimentally manipulated, without needing to provide any explicit reward feedback, makes this one of the few pure exploration tasks that I am aware of.</p>
<p>2. Compelling results</p>
<p>
The effect of manipulating choice complexity and depth on initial choice probability for &quot;good&quot; directed learners seems fairly strong, as do the learning dynamics. The heterogeneity in exploration style across participants is also interesting and brings up more questions that are useful for follow-up research.</p>
<p>3. Simple model</p>
<p>
The computational model used is a simple adaptation of standard reinforcement learning models, specifically Q-learning models. This is elegant as it doesn't require major changes in the dynamics of learning, simply a revision of the variables going into the update. The simplicity of this change, coupled with the ability to capture the results of the &quot;good&quot; directed explorers makes a strong case that information seeking and reward-seeking may share common underlying mechanisms (as shown previously by Kobayashi, K., &amp; Hsu, M. (2019). Common neural code for reward and information value. Proceedings of the National Academy of Sciences, 116(26), 13061-13066.).</p>
<p>Weaknesses:</p>
<p>1. &quot;Good&quot; vs. &quot;poor&quot;</p>
<p>
There is an odd circularity, and implicit value judgment, in the classification of participants into &quot;good&quot; and &quot;poor&quot; directed explorers. The logic, based on the visit-counter model of directed exploration, is that the probability of repeating a choice (at the initial decision trial) should be low for directed explorers vs. random explorers. Doing the median split on repetition probability seems intuitively fine here, but it does bring up two issues. First, the labels &quot;good&quot; vs. &quot;poor&quot; seem arbitrarily judgemental, after all random exploration is a viable exploration strategy in many contexts. Would &quot;directed&quot; vs. &quot;random&quot; be more appropriate labels based on how the decision was made to categorize participants? Second, how much of the &quot;good&quot; participant performance is driven by the extreme non-repeaters? For example, if a tertiary split was performed instead of a binary median split, would the middle group show a weaker version of the effects seen in the &quot;good&quot; group or appear more like the &quot;poor&quot; group?</p>
<p>2. Characterization of information value</p>
<p>
The authors discuss primarily methods that can be summarized by visit counters as a description for all directed exploration models. However, that doesn't seem to be a good summary of the overall literature in this space. There are also entropy-based approaches, that quantify information value based on the statistics of the feedback. For example, in machine learning methods like the KL divergence are often used to represent the information value of a channel. A few such papers are highlighted below. Now it is entirely possible that these approaches can be extrapolated to simple visit-count approaches, but I am unaware of anything showing this. I think it would be good to broaden the discussion on directed exploration models beyond visit-counter methods like UCB, highlighting the other methods used to promote directed exploration.</p>
<p>Houthooft, R., Chen, X., Duan, Y., Schulman, J., De Turck, F., &amp; Abbeel, P. (2016). Vime: Variational information maximizing exploration. Advances in neural information processing systems, 29.</p>
<p>Eysenbach, B., Gupta, A., Ibarz, J., &amp; Levine, S. (2018). Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070.</p>
<p>Hazan, E., Kakade, S., Singh, K., &amp; Van Soest, A. (2019, May). Provably efficient maximum entropy exploration. In International Conference on Machine Learning (pp. 2681-2691). PMLR.</p>
<p>3. Model vetting</p>
<p>
The model used to simulate the behavioral results is interesting and intuitive. However, there seem to be some things left on the table and unresolved. First, the definition of information value (E) that is maximized is assumed to satisfy the same constraints as typical reward does in the Bellman solution for reinforcement learning. This is the only way it can be substituted into the typical Q-learning method. Is that true here?</p>
<p>Second, the advantage of these simpler computational-level models is that they can be effectively fit to behavior. The model outlined in the paper has only a few free parameters (some of which can be fixed for convenience purposes). Was there an attempt to fit each participant's data into the model? This would be a powerful way of highlighting where exactly the differences between the &quot;good&quot; and &quot;bad&quot; participants arise.</p>
</body>
</sub-article>
</article>