<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">105302</article-id>
<article-id pub-id-type="doi">10.7554/eLife.105302</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.105302.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Immunology and Inflammation</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Celldetective: an AI-enhanced image analysis tool for unraveling dynamic cell interactions</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Torro</surname>
<given-names>Rémy</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>remy.torro@inserm.fr</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Díaz-Bello</surname>
<given-names>Beatriz</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
    <surname>El Arawi</surname>
<given-names>Dalia</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dervanova</surname>
<given-names>Ksenija</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ammer</surname>
<given-names>Lorna</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dupuy</surname>
<given-names>Florian</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6104-6286</contrib-id>
<name>
<surname>Chames</surname>
<given-names>Patrick</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1060-2713</contrib-id>
<name>
<surname>Sengupta</surname>
<given-names>Kheya</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9523-8086</contrib-id>
<name>
<surname>Limozin</surname>
<given-names>Laurent</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>laurent.limozin@inserm.fr</email>
</contrib>
    <aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/035xkbk20</institution-id><institution>Aix-Marseille Univ, CNRS, INSERM, Turing Centre for Living systems, LAI</institution></institution-wrap>, <city>Marseille</city>, <country country="FR">France</country>.</aff>
    <aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/035xkbk20</institution-id><institution>Aix-Marseille Univ, CNRS, Turing Centre for Living systems, CINAM</institution></institution-wrap>, <city>Marseille</city>, <country country="FR">France</country>.</aff>
    <aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/035xkbk20</institution-id><institution>Aix-Marseille Univ, CNRS, INSERM, Institut Paoli-Calmettes, CRCM</institution></institution-wrap>, <city>Marseille</city>, <country country="FR">France</country>.</aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Koo</surname>
<given-names>Peter</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Cold Spring Harbor Laboratory</institution>
</institution-wrap>
<city>Cold Spring Harbor</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Taniguchi</surname>
<given-names>Tadatsugu</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Tokyo</institution>
</institution-wrap>
<city>Tokyo</city>
<country>Japan</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-03-11">
<day>11</day>
<month>03</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP105302</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-12-12">
<day>12</day>
<month>12</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-11-13">
<day>13</day>
<month>11</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.03.15.585250"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Torro et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Torro et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-105302-v1.pdf"/>
<abstract>
<title>Abstract</title><p>A current challenge in bioimaging for immunology and immunotherapy research lies in analyzing multimodal and multidimensional data that capture dynamic interactions between diverse cell populations. Here, we introduce Celldetective, an open-source Python-based software designed for high-performance, end-to-end analysis of image-based <italic>in vitro</italic> immune and immunotherapy assays. Purpose-built for multicondition, 2D multichannel time-lapse microscopy of mixed cell populations, Celldetective is optimized for the needs of immunology assays. The software seamlessly integrates AI-based segmentation, Bayesian tracking, and automated single-cell event detection, all within an intuitive graphical interface that supports interactive visualization, annotation, and training capabilities. We demonstrate its utility with original data on immune effector cell interactions with an activating surface, mediated by bispecific antibodies, and further showcase its potential for analyzing extensive sets of pairwise interactions in antibody-dependent cell cytotoxicity events.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>bioimage analysis</kwd>
<kwd>multimodal microscopy</kwd>
<kwd>artificial intelligence</kwd>
<kwd>immune cell interactions</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>compact results and discussion and transfer to materials and methods</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>An overarching goal of post-genomic biomedical research is to decipher cell states and transitions as well as cell interactions in functional dynamic assays [<xref ref-type="bibr" rid="c1">1</xref>]. This requires high throughput and high resolution experimental methods supported by advanced and powerful analyses [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c3">3</xref>]. The field of immunology presents specific challenges, related to complex cell states characterized by transient and heterogeneous receptor expression, highly motile behavior and immune function relying on dynamic cell-cell interactions [<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c5">5</xref>]. These features are largely out of reach in ensemble biochemical measurements [<xref ref-type="bibr" rid="c6">6</xref>] as well as low time-resolution single cell multi-omic data [<xref ref-type="bibr" rid="c7">7</xref>].</p>
<p>Microscopy-based <italic>in-vitro</italic> assays can bridge temporal and spatial scales, contributing to decipher lymphocyte activation from molecular to cellular scales [<xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c10">10</xref>]. Combining high content and high spatial resolution, imaging flow cytometry was applied to reconstruct disease progression on single immune cell populations [<xref ref-type="bibr" rid="c11">11</xref>] or to functionally characterize therapeutic antibodies against liquid tumours [<xref ref-type="bibr" rid="c12">12</xref>]. The anlysis is however limited to single cell or cell doublets. Multichannel fluorescence microscopy was successfully applied to image-based profiling of human T lymphocytes and Natural Killer cells (NK) [<xref ref-type="bibr" rid="c13">13</xref>] or therapeutic targets in drug discovery [<xref ref-type="bibr" rid="c14">14</xref>], but it provided limited dynamic information on the system. In some cases, nanowell grids were used to organize cell contacts and facilitate image analysis [<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c16">16</xref>] at the expense of imposing less physiological conditions.</p>
<p>High content, multi-channel spatiotemporal imaging of living cells provides rich, dynamic and multiscale information, from molecular to cell population scale [<xref ref-type="bibr" rid="c17">17</xref>]. In single-cell image sequences, time-dependent signals contain functional information, traditionally collected or treated manually in immunological settings [<xref ref-type="bibr" rid="c18">18</xref>]. Recent methods performed profiling on time dependent data for cellular phenotyping, but without measuring single events in time [<xref ref-type="bibr" rid="c19">19</xref>]. Functional dynamic information like calcium imaging can also be extracted for immune cell classification [<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>] or to interpret the the brain’s neuronal activity [<xref ref-type="bibr" rid="c22">22</xref>]. Most recent works on cell dynamics, specially in the context of cancer immunotherapy, involve specialized microscopy techniques and complex analysis protocols [<xref ref-type="bibr" rid="c23">23</xref>–<xref ref-type="bibr" rid="c25">25</xref>], sometimes using proprietary software [<xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>].</p>
<p>In the last years, machine learning and deep learning (DL) have revolutionized the field of image analysis [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c28">28</xref>–<xref ref-type="bibr" rid="c32">32</xref>]. A critical step in the context of cell images is segmentation, for which DL tools are highly successful [<xref ref-type="bibr" rid="c33">33</xref>–<xref ref-type="bibr" rid="c38">38</xref>]. However, they often rely on pretrained models for which the original dataset is not always accessible or can be too remote from users’ data [<xref ref-type="bibr" rid="c39">39</xref>], limiting adaptability. For dynamic data, cell tracking is also crucial and can be performed by available tools [<xref ref-type="bibr" rid="c40">40</xref>–<xref ref-type="bibr" rid="c42">42</xref>]. Recent methods that integrate profiling with time-dependent information significantly enhance cell tracking performance [<xref ref-type="bibr" rid="c43">43</xref>–<xref ref-type="bibr" rid="c45">45</xref>]. However, current software associating the two tasks are mostly applied to the analysis of cell lineage [<xref ref-type="bibr" rid="c41">41</xref>, <xref ref-type="bibr" rid="c46">46</xref>–<xref ref-type="bibr" rid="c48">48</xref>]. Most popular free bioimage analysis software like Fiji [<xref ref-type="bibr" rid="c49">49</xref>], Icy [<xref ref-type="bibr" rid="c50">50</xref>] or Cellprofiler [<xref ref-type="bibr" rid="c51">51</xref>] were not originally designed for DL methods [<xref ref-type="bibr" rid="c52">52</xref>], relying on extensions and plugins to bring in these new methods, or were not conceived for time-dependent data [<xref ref-type="bibr" rid="c53">53</xref>]. Finally, end-to-end analysis tools accessible to experimentalists, adapted for both images and time series are currently lacking, preventing dataset curation and DL based event-detection. With the exception of MIA [<xref ref-type="bibr" rid="c54">54</xref>], existing partial solutions require coding skills [<xref ref-type="bibr" rid="c55">55</xref>] or proficiency in integrating/mastering disparate tools [<xref ref-type="bibr" rid="c56">56</xref>].</p>
<p>We introduce Celldetective, an open-source software designed to tackle the challenge of analyzing image-based <italic>in vitro</italic> immune and immunotherapy assays. It addresses the problem of cell interactions by extracting and interpreting single-cell behaviors for up to two populations of interest in a 2D co-culture, with a cell pair description linking the populations. The software adapts to varying imaging conditions and cell types, incorporating dataset curation and deep learning model training features directly within the graphical user interface (GUI). Celldetective organizes and automates complicated and time-consuming processes while providing a user-friendly, no-code GUI, making it accessible to experimental scientists without programming expertise. We tested the software on two applications in immunology and immunotherapy: i) measuring immune cell behavior in contact with an antigen-presenting surface using a label-free microscopy technique, and ii) assessing dynamic cell-cell interactions in a multiwell co-culture of immune and target cells, an established cytotoxic assay, using multicolor fluorescence microscopy.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Software analysis pipeline</title>
<p>Celldetective is implemented as a Python package with a PyQt5-based GUI, available through the PyPi package repository and regularly updated. The source code can be accessed on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/celldetective/celldetective">https://github.com/celldetective/celldetective</ext-link>), while all models, datasets, and demos are hosted in a public Zenodo repository (<ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/13907294">https://zenodo.org/records/13907294</ext-link>). Extensive documentation is available online (<ext-link ext-link-type="uri" xlink:href="https://celldetective.readthedocs.io">https://celldetective.readthedocs.io</ext-link>). The software is designed to run locally on a single computer equipped with either a graphics processing unit (GPU) or multiple processors. Image processing is conducted frame by frame with garbage collection to significantly reduce memory requirements, allowing it to operate on most laboratory computers. Celldetective is compatible with multi-channel time-lapse microscopy image stack files in TIF format, which are typically generated by acquisition software such as Micro-Manager [<xref ref-type="bibr" rid="c57">57</xref>].</p>
<p>As illustrated in <xref rid="fig1" ref-type="fig">Fig. 1</xref>, the software input consists of an experimental project that organizes raw data into a two-level folder structure emulating a multiwell plate, commonly used in <italic>in vitro</italic> immune assays (startup window in <xref rid="figs6" ref-type="fig">Fig. S6A</xref>). At the top level, folders represent wells (or biological conditions), while at the second level folders represent positions (a field of view associated with the biological condition). Experiment metadata – including channel names and order, spatiotemporal calibration, and biological condition labels – are stored in a configuration file created during project setup (<xref rid="figs6" ref-type="fig">Fig. S6B</xref>). This structure organizes the experimental data by biological conditions and field of views, facilitating the processing of large data volumes.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 1</label>
<caption><title>Software Overview.</title>
<p>An end-to-end GUI pipeline for studying interactions between target and effector cell pairs (from left to right). After loading an experiment project that mimics a multiwell plate structure, the user can apply preprocessing steps to the 2D time lapse microscopy images before segmentation. Target and effector cells are then segmented, tracked, and measured independently. Events are detected from the resulting time series, and the co-culture images are distilled into tables of single-cell measurements. The neighborhood module links cells in spatial proximity, and the cell-pair signal analysis framework facilitates the investigation of interactions between cell pairs. Eye and brush icons indicate steps where visual control and corrections are possible, with an appropriate viewer.</p><p><xref rid="figs6" ref-type="fig">Fig. S6</xref>. Main GUI windows.</p><p><xref rid="figs7" ref-type="fig">Fig. S7</xref>. Processing modules to extract single cells. <xref rid="figs8" ref-type="fig">Fig. S8</xref>. Background correction methods.</p><p>Tab. S2. Generalist deep learning segmentation models.</p></caption>
<graphic xlink:href="585250v3_fig1_rev.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Upon loading a project, a modular interface organizes all possible user actions (<xref rid="figs6" ref-type="fig">Fig. S6C</xref> with single-cell extraction modules illustrated in <xref rid="figs7" ref-type="fig">Fig. S7</xref>). Image preprocessing enables users to transform images prior to segmentation (<xref rid="figs8" ref-type="fig">Fig. S8</xref>). Two processing blocks drive the single-cell extraction for the effector and target populations, independently. Population-specific segmentation, optional tracking and measurements, reduce the image stacks into tables, where each row represents a single cell at a specific time point. With tracking information available, single-cell measurements are structured and interpreted as time series to detect events. Alternatively, users can classify cells at each time point from their measurements. A viewer is integrated at each step for quality control, error correction, or data annotation. The final module addresses cell interactions within or across populations, enabling users to describe and annotate cell pairs. Throughout the process, users can plot and compare single-cell or cell pair measurement distributions via a table exploration module designed for manipulating tracks. Additional analysis modules allow to perform survival studies and represent synchronized time series over cell populations.</p>
<p>Rather than reinventing efficient existing solutions, we incorporated established open-source deep learning segmentation methods StarDist [<xref ref-type="bibr" rid="c35">35</xref>] and Cellpose [<xref ref-type="bibr" rid="c36">36</xref>] (see model list in <xref rid="tbls2" ref-type="table">Tab. S2</xref>)—alongside the Bayesian tracker bTrack [<xref ref-type="bibr" rid="c41">41</xref>] into Celldetective. The software provides intuitive graphical options for tuning input and control parameters of these algorithms. The diverse software functionalities are listed and compared to existing software in <xref rid="tbl1" ref-type="table">Table 1</xref>. Notably, Celldetective stands out by offering training and transfer options in segmentation, enabling detection of a population of interest within a mixed population, as well as time series annotation and survival analysis, not currently integrated in any other software. These features will be illustrated in the two biological applications described after.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1</label>
    <caption><title>Comparative table of software functionalities with a selection of available solutions.</title>
    <p>A ✓ is attributed if the task can be carried without coding. The use of an integrated solution or plugin is indicated in parentheses.</p></caption>
<graphic xlink:href="585250v3_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s2b">
<title>High-throughput and time-resolved analysis of cell adhesion and spreading responses</title>
<p>In this first application, we investigated the dynamics of human primary Natural Killer (NK) immune cells as they sedimented and spread on cancer-cell-mimicking surfaces. Such assays offer rich spatiotemporal insights into interactions between lymphocytes and surrogate antigen-presenting cells [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c58">58</xref>, <xref ref-type="bibr" rid="c59">59</xref>]. The bispecific antibody (bsAb) studied here, with potential therapeutic applications, bridges the NK cell’s activating receptor CD16 and the surface-bound antigen Human Epidermal Growth Factor Receptor 2 (HER-2), a breast cancer marker. Our objective was to evaluate the bsAb’s effectiveness in promoting cell spreading, used here as a proxy for antibody-mediated cell-cell interaction [<xref ref-type="bibr" rid="c58">58</xref>, <xref ref-type="bibr" rid="c60">60</xref>]. We imaged cell behavior at the surface interface using reflection interference contrast microscopy (RICM), a label-free technique sensitive to submicrometer distances between the cell membrane and the surface [<xref ref-type="bibr" rid="c61">61</xref>] (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 2</label>
<caption><title>Functional response of immune cells in a spreading assay.</title>
<p>A) Schematic (top) and snapshot (bottom) of a spreading assay imaged by time-lapse RICM. Primary NK cells sediment on an HER2 coated surface in the presence of bispecific anti-CD16 <italic>×</italic> HER2 antibodies. Cells touch the surface in a desynchronized manner and may spread after a stochastic hovering duration. The contours of the cells are represented in orange for cells classified as spread on the surface and blue for hovering cells (corresponding colors are also used in the schematic). B) A traditional segmentation pipeline is used to generate a first instance segmentation of the cells imaged in RICM. Cell masks can be manually curated before training a new Cellpose [<xref ref-type="bibr" rid="c36">36</xref>] model starting from scratch or from an existing generalist model. Barplot comparing segmentation scores obtained for three different methods that can be called within the software. With the improved results yielded by the new model, the user can proceed to tracking and measurements. C) Intensity time series for a cell performing a contact and spreading sequence (see text for details). D) Single-cell intensity time series color-coded for three classes: 1) the cell spreads during the movie (’event’, orange), 2) the cell is not observed to spread during the movie (’no-event’, green) and 3) the cell is already spread at the beginning of the movie (’left-censored’, purple). E) Benchmarking of cell class automatic determination reported by a confusion matrix, and comparison of spreading times determined automatically. F) Individual cell time series (tonal, morphological, etc) are synchronized with respect to a characteristic time to measure the average population response at the event time. G,H) Functional response for the cell population as a function of bsAb concentration: hovering survival or distribution of cell decision time before spreading (G), spreading rate (H).</p><p><xref rid="figs9" ref-type="fig">Fig. S9</xref>. Designing a traditional segmentation pipeline.</p><p><xref rid="figs10" ref-type="fig">Fig. S10</xref>. Segmentation correction and annotation. <xref rid="figs11" ref-type="fig">Fig. S11</xref>. Overview of segmentation strategies.</p><p>Video: Supplementary video 1</p></caption>
<graphic xlink:href="585250v3_fig2_rev.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Cell segmentation in RICM images is challenging because the cells can be locally brighter or darker than the background, causing direct thresholding techniques to fail [<xref ref-type="bibr" rid="c61">61</xref>]. To address this, previous approaches have adopted special techniques, such as background correction and applying a variance filter before thresholding [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c62">62</xref>, <xref ref-type="bibr" rid="c63">63</xref>], which, however was often inadequate and required expert intervention. Our goal here was to train a DL model to segment lymphocytes spreading or hovering on the surface, without need for expert corrections. The complete strategy – background subtraction (<xref rid="figs8" ref-type="fig">Supplementary Fig. S8B</xref>), generating an initial set of labels with the traditional segmentation pipeline (<xref rid="figs9" ref-type="fig">Supplementary Fig. S9</xref>), correcting errors in the viewer to build a training dataset (<xref rid="figs10" ref-type="fig">Supplementary Fig. S10</xref>), and training a DL model from this dataset – is illustrated in <xref rid="fig2" ref-type="fig">Figure 2A–B</xref>. Moving away from this application, Celldetective provides a whole ecosystem to perform segmentation and adapt deep learning methods (see <xref rid="figs11" ref-type="fig">Supplementary Fig. S11</xref>). Here, we trained a new Cellpose model on the RICM dataset (both the dataset and model are available on Zenodo). We benchmarked this model against the traditional segmentation pipeline and the <italic>cyto3</italic> generalist Cellpose model [<xref ref-type="bibr" rid="c37">37</xref>] using a test set, quantifying lymphocyte detection accuracy and segmentation quality separately (see Materials and Methods). Hovering cells, which appeared as bright blobs with interference patterns and had an ill-defined area in RICM, were excluded from the segmentation quality score. Our model achieved a higher detection accuracy (0.86), compared to the traditional pipeline (0.62) or the generalist model (0.5). The traditional pipeline tended to produce many false-positive detections, while the generalist model struggled to simultaneously detect spread and hovering cells. Segmentation quality, however, was higher for the pipeline (0.93) than for our model (0.89) or the generalist model (0.88). Since most spread cell labels in the test set required little manual correction (except for contiguous cells), we were not surprised to observe the highest segmentation quality with the pipeline method. The generalist model segmented spread cells well when it could detect them. Overall, the new Cellpose model achieved the best average performance across both tasks (0.88), making it the most effective approach for segmenting cells in this assay.</p>
<p>Next, tracking of cells — which move while hovering and are quasi-stationary while spreading — was performed using a versatile motion model (see Materials and Methods), without feature inputs to prevent sharp morpho-tonal transitions from disrupting tracking. We enabled track extrapolation from the beginning of the movies, allowing intensity measurements to start before the cells reached the surface (<xref rid="fig2" ref-type="fig">Figure 2C-D</xref>). With track post-processing, we could measure intensities prior to cell arrival, with a position-centered average intensity measurement, in a user-defined disk (position-based), which compared well with the average over the cell mask, when available (mask-based). Notably, the position-based intensity showed a spike during the spreading phase, which was not observed in the mask-based intensity.</p>
<p>We defined two characteristic event times (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>) : contact time (<italic>t</italic><sub>contact</sub>)—the moment when the cell first appeared in RICM, with an average intensity greater than 1; and the onset of spreading (<italic>t</italic><sub>spread</sub>) when the average intensity dropped below 1. The hovering time (<italic>t</italic><sub>spread</sub> −<italic>t</italic><sub>contact</sub>), for spreading cells, is the time required to make a spreading decision. <italic>t</italic><sub>contact</sub> is automatically detected as the time of first segmentation. For <italic>t</italic><sub>spread</sub>, spreading events were identified through condition-based classification (mask-based RICM intensity <italic>&lt;</italic> 1) and an irreversible event hypothesis (see Materials and Methods). We benchmarked classification performance and hovering duration estimates against a manually annotated test set (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>). The set was annotated with a dedicated viewer (see Supplementary video 1). A confusion matrix indicates perfect agreement between automatic classification and ground truth, while a correlation plot comparing estimated hovering durations to the ground truth shows excellent agreement, with a mean absolute error of 0.35 minute (<xref rid="fig2" ref-type="fig">Fig. 2E</xref>).</p>
<p>After automatically extracting the event times, we examined average population response based on single-cell time series, by synchronizing each single-cell measurement to a specific event time to calculate the mean response across the population (<xref rid="fig2" ref-type="fig">Figure 2F</xref>). Next, we tested whether varying the concentration of bsAb on the surface impacts spreading. First, we estimated the “hovering survival” by plotting fraction of cells hovering at a given time (<xref rid="fig2" ref-type="fig">Figure 2G</xref>). We observe a higher fraction of spreading cells and shorter spreading decision times with increasing bsAb concentrations. The acceleration of the spreading decision time was particularly apparent in the 10-1000 pM bsAb concentration range, saturating beyond 100 pM. Next, Using the table exploration feature in Celldetective, we differentiated the area with respect to time for each cell track, extracting the spreading rate at the onset of spreading (<italic>t</italic><sub>spread</sub>), which increases in the 14-1400 pM range (<xref rid="fig2" ref-type="fig">Figure 2H</xref>). Overall, determining event times for individual cells, measured at high throughput, highlights Celldetective capabilities in evaluating cell-surface interactions.</p>
</sec>
<sec id="s2c">
<title>Cytotoxic response in a co-culture with overlapping channels</title>
<p>In the second application, we studied antibody-dependent cell cytotoxicity (ADCC) in a high-density co-culture of target tumor cells (MCF-7) and effector cells (primary NK) for various biological conditions (concentration of bsAb anti-CD16 × HER2, tumor cell HER2 expression). A multi-color composite representation of a single position is shown at endpoints in <xref rid="fig3" ref-type="fig">Figure 3A</xref>, illustrating how the number of dead (red) nuclei increases over time.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 3</label>
<caption><title>High throughput cytotoxic response of cancer cells in co-culture with immune cells.</title>
<p>A. Schematics side-view of target/NK cells co-culture assay for bispecific ADCC (top) and representative multimodal composite images, obtained at two different time points, with target nuclei labelled in blue, dying cells in red and NK cells in green (bottom). Corresponding colors are also used in the schematics. B. Decomposition of partly overlapping fluorescence channels and benchmark of segmentation DL models. Brightfield and fluorescence images at three different time points. Hoechst channel (target nucleus) is taken as input to the existing StarDist <italic>versatile fluo</italic> or Cellpose <italic>nuclei</italic> models (available directly in Celldetective). Two new models trained in Celldetective were benchmarked: a transfer of the StarDist <italic>versatile fluo</italic> model on our MCF-7 nuclei dataset, with Hoechst as input, and a StarDist multimodal modeln trained from scratch using 4 channels as its input. C. Time series of nuclear fluorescence intensities and nucleus apparent area for a set of non-dying target cells (left) and of dying cells (right); the reference time <italic>t</italic><sub>0</sub> is the death time for the dying cells. D. Benchmark of three methods for event classification and regression (Threshold method on PI, DL classifier on PI, DL classifier on PI and nuclear area). Top row: confusion matrices with 3 classes (fraction of predictions displayed). Bottom row: correlation plots. E. Survival curves of target cells without NK cells or in the presence of NK cells for different bsAb concentrations. F. Schematics for neighbors counting and histogram of target neighbors count. G. Survival curves for two subpopulations of targets at 100 pM bsAb, splitted as a function of the local target cell density. <xref rid="figs12" ref-type="fig">Fig. S12</xref>. Event detection model architectures.</p><p><xref rid="figs13" ref-type="fig">Fig. S13</xref>. Neighbor counting methods.</p><p>Tab. S3. MCF7 nuclei segmentation models.</p><p>Video: Supplementary video 2</p></caption>
<graphic xlink:href="585250v3_fig3_rev.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>First, we studied the lysis behavior of target cells over time, which entailed achieving a single-cell description for the target cells. Directly using generalist nuclear segmentation models (StarDist <italic>versatile fluorescence</italic> and Cellpose <italic>nuclei</italic> models) on the Hoechst channel, we could only achieve a poor accuracy, limited by effector cell screening of the underlying target cells. While it was possible to segment all cells and classify them either simultaneously [<xref ref-type="bibr" rid="c64">64</xref>] or in two steps [<xref ref-type="bibr" rid="c65">65</xref>], label contamination from overlapping populations remained an issue. We produced expert annotations of the MCF-7 nuclei using all available multi-channel and time information. Since the MCF-7 nuclei are larger than primary NK cell nuclei, we expected to be able to train an efficient model detecting the MCF-7 nuclei from the Hoechst channel only, failing only on blurry images, where the size separation was no longer obvious (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>). We used Celldetective to retrain the StarDist <italic>versatile fluorescence</italic> model on our dataset using the Hoechst channel as its input and considerably improved the performance, from 0.55 to 0.82. Last, we trained a completely multi-channel model on all four available channels to detect the MCF-7 nuclei and achieve a slightly higher performance (0.83). All of the models benchmarked are accessible directly in Celldetective (see <xref rid="tbls2" ref-type="table">Supplementary Tab. S2</xref> and <xref rid="tbls3" ref-type="table">S3</xref>). Target cell tracking is detailed in Materials and Methods.</p>
<p>An expert annotated death events defined by a sharp increase in PI signal at <italic>t</italic><sub>death</sub>. Single-cell time series were then grouped by event class and synchronized to <italic>t</italic><sub>death</sub> (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). The average PI intensity for cells undergoing death shows a distinctive step-like transition with low lateral variance, validating the annotation accuracy (<xref rid="fig3" ref-type="fig">Fig 3C</xref>, top-right). The PI signal in live cells, by contrast, rises slightly over time but remains well below the levels observed in dead cells (<xref rid="fig3" ref-type="fig">Fig 3C</xref>, top-left). Notably, the apparent nuclear area of dying cells contracts sharply at <italic>t</italic><sub>death</sub> (<xref rid="fig3" ref-type="fig">Fig 3C</xref>, bottom-right), while the nuclear area of non-dying cells remains constant (<xref rid="fig3" ref-type="fig">Fig 3C</xref>, bottom-left).</p>
<p>Event detection was evaluated for three methods on this dataset: (1) a condition-based classification using the irreversible event hypothesis on the PI intensity time series, (2) a DL model trained to detect death events from PI alone, and (3) a DL model trained to detect events using both the PI intensity and nuclear area time series. Detailed information on the training procedures for these models is provided in the Materials and Methods section and the DL architecture on <xref rid="figs12" ref-type="fig">Fig. S12</xref>. <xref rid="fig3" ref-type="fig">Figure 3D</xref> presents the confusion matrices for each method, showing agreement between predicted and ground-truth event classes, along with a correlation plot evaluating the accuracy of death time extraction. Method 1 incorrectly predicted about 15 % of the events as death events when none had occurred. This method also had the highest mean absolute error (MAE) for death time estimates (7.1 minutes). Method 2 performed better, with only 3 % of its predicted death events corresponding to an absence of events. The multivariate model (Method 3), incorporating both PI and nuclear area data, achieved the highest precision in death event detection (96 % of predicted death events were true events). However, this model showed a reduced accuracy in identifying cells as already dead, with 20 % of these cells (2 % of the total number of cells) observed to die later. As before, and in practice, quality check and minor manual corrections were performed with the event annotation viewer, using this time a multichannel composite of the images (Supplementary Video 2).</p>
<p>We next investigated whether the probability of tumor cell lysis depended on bsAb concentration. Using <italic>t</italic> = 0 min as the synchronization time, we excluded cells already dead at the start (always less than 5%) and disregarded cells born from mitosis (during tracking post-processing). We represented the MCF-7 survival curve using <italic>t</italic><sub>death</sub> as the event time (<xref rid="fig3" ref-type="fig">Figure 3E</xref>), which shows an increase in the fraction of dead MCF-7 cells (endpoint) occurring at a faster rate with rising bsAb concentrations.</p>
<p>Our single-cell approach permits also to link cell survival with local context. To do this, we calculated a target-target neighborhood within a 32 <italic>µ</italic>m radius (isotropic method, see Materials and Methods and FIg. <xref rid="figs13" ref-type="fig">Fig. S13</xref>) to count the local density experienced by each target cell until death (<xref rid="fig3" ref-type="fig">Figure 3F</xref>). We then applied a filter in Celldetective to compare survival outcomes for cells in low-density versus high-density environments (<xref rid="fig3" ref-type="fig">Fig 3G</xref>), observing a significant survival advantage for targets in high-density regions.</p>
</sec>
<sec id="s2d">
<title>From a single time point to a dynamic readout of effector-target interactions</title>
<p>After describing the ADCC assay from the targets’ perspective, we shifted our focus to the effectors. Effector cells were segmented using a custom multichannel Cellpose model that combines brightfield, Hoechst, and CFSE, a cytoplasmic fluorescence label (see <xref rid="tbls4" ref-type="table">Supplementary Tab. S4</xref>). Unlike targets, effectors exhibit high motility and can form dense clusters in the assay, limiting reliable tracking.</p>
<p>When effector tracking was unfeasible (e.g., at an effector-to-target ratio of 5:1 and frame interval greater than 2 minutes), we could still perform a multicondition analysis of the cells at each time point. We aimed to measure the signal of antibodies tagged with a single ATTO 647 dye. Specifically, we compared the CE4-21 bsAb with the CE4-X control antibody, which only binds to the HER2 antigen on targets, across two target cell lines with distinct antigen expression levels (WT and HER2+) [<xref ref-type="bibr" rid="c66">66</xref>]. <xref rid="fig4" ref-type="fig">Figure 4</xref> presents the mean bsAb fluorescence intensity under effector cell masks at 1 hour, broken down by target cell type, antibody, and contact state (in or off contact with targets, see Materials and Methods and Supplementary video 3). For WT targets (lowest HER2 expression), the cells exhibited a signal in the CE4-21 bsAb condition, regardless of the contact state (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). In contrast, no signal was measured with the CE4-X antibody condition, supporting that the bsAb-condition signal came from the effector cells (visual assessment in <xref rid="fig4" ref-type="fig">Figure 4B</xref>). With the HER2+ targets and CE4-X antibody, the signal measured off-contact was negligible, confirming that the antibody did not bind to the effectors (<xref rid="fig4" ref-type="fig">Fig 4A-B</xref>). The signal measured in contact did not originate from the effector cells but from the target cells below. With the CE4-21 bsAb, on the other hand, the signal measured in contact was a sum of contributions from both the target and effector cells.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 4</label>
<caption><title>Single time point analysis of effector-target interactions.</title>
<p>In the absence of effector tracking, fluorescent bsAbs were used to study the distribution of Ab on effectors and targets. A. bsAb intensity measured on effector cells on targets with high or low antigen expression. bsAb CE4-21 binds to both HER2 and CD16, while CE4-X control antibody binds only to the HER2 antigen on target cells. B. Representative snapshot of fluorescence channel for each condition of A. C. Empirical cumulative distribution functions (ECDFs) for the simulated (gray, sum of CE4-21/no contact + CE4-X/contact) and observed (orange, CE4-21/contact) distributions. The dotted black line represents the Kolmogorov-Smirnov statistic, <italic>i.e.</italic> the maximum separation between the two ECDFs.</p><p>Tab. S4. Primary NK segmentation models. <xref rid="figs14" ref-type="fig">Fig. S14</xref>. Effect size at each time point.</p><p>Video: Supplementary video 3</p></caption>
<graphic xlink:href="585250v3_fig4_rev.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Based on the measured signals, we sought to determine whether the added effector signal from the CE4-21/off-contact condition (without target contribution) and the effector signal from the CE4-X/in-contact condition (without effector contribution) could approximate the CE4-21/in-contact signal. Using the distributions generated by the software, we simulated the distribution of this summed signal with a Monte-Carlo approach, and compared it with the CE4-21/in-contact condition. <xref rid="fig4" ref-type="fig">Figure 4C</xref> reveals that the simulated signal distribution was significantly lower than that of the CE4-21/in-contact condition, with a small effect size (Cliff’s Delta <italic>&gt;</italic> 0.33). By calculating this effect size at each time point, we further confirmed the robustness of this finding, as the effect consistently maintained the same direction, with a medium effect size observed more often than not (Suppl. <xref rid="figs14" ref-type="fig">Fig. S14</xref>).</p>
<p>If effector tracking is achievable (e.g., at an effector-to-target ratio of 1:1 and frame interval lower than 1 minute), we can explore the measurement time series. We illustrate this software feature by adding a degranulation marker (fluorescent anti-LAMP1) to monitor the lytic activity of the effector cells in the assay. In this experiment, we compared two bsAbs, CE4-21 and CE4-28, formed of the same CE4 nanobody against HER2 and two different nanobodies against CD16 on the effector side.</p>
<p>First, we investigated whether LAMP1 expression was influenced by biological conditions, mirroring our approach with fluorescent antibodies. For each target cell type and antibody condition, we reported the fraction of LAMP1-positive effector, decomposed by contact state, across all time points (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>). The results indicated that, except for the no-antibody controls, the fraction of LAMP1-positive effector cells was consistently higher in-contact with target cells.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 5</label>
<caption><title>Effector-target dynamic interactions.</title>
<p>A. Fraction of LAMP1-positive effector cells for different bsAb conditions, decomposed by contact with the targets (over all time points). B. Average effector velocity for different bsAb conditions, decomposed by target contact (up to two estimates per effector). C. Examples of killer-victim identification with one (left) or two (right) killers. One target cell (center) is connected to effector neighbors via colored segments. Dynamic effector-target interaction monitoring includes PI signal of target cell, relative distance and relative velocity between target and effector, as well as LAMP1 signal in effector neighbors. D, E. Characteristic parameters for killer/victim and non-killer/victim pairs. Effector cells are manually annotated as potential killer or not. Each pair of points represents one victim target, with the average over all neighbors of the parameter decomposed by killer class. D. LAMP1 intensity. E. Relative effector-target velocity.</p><p>Video: Supplementary video 4</p></caption>
<graphic xlink:href="585250v3_fig5_rev.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>From this time-integrated approach, we shifted our focus to the dynamics of the effector cells, specifically investigating whether their motility was influenced by target cell proximity. Given that velocity estimators can be noisy, we computed a time-averaged velocity estimate for each cell track, decomposed by contact state. The resulting velocities for all conditions are presented in <xref rid="fig5" ref-type="fig">Figure 5B</xref>. In each condition, the contact velocities were consistently lower than off-contact, regardless of antibody presence or type. This difference was always significant, with a small effect size for WT targets (Cliff’s Delta <italic>&gt;</italic> 0.147) and a large effect size for HER2+ targets (Cliff’s Delta <italic>&gt;</italic> 0.474). The lowest contact velocities were observed in the presence of antibodies, showing a significant drop from the target-specific control, with medium effect sizes for the WT bsAbs and HER2+ CE4-28, and a large effect size for HER2+ CE4-21. However, we did not detect any difference between the two antibodies, separately for each cell type.</p>
<p>After computing effector-target neighborhoods, we measured features of cell pairs, including relative distance and angle, as well as their time derivatives. <xref rid="fig5" ref-type="fig">Figure 5C</xref> illustrates time series data for pairs, showing the PI intensity of a target cell that died at 5 minutes alongside the relative distance, velocity, and LAMP1 signal of neighboring effectors. Notably, only the selected neighbor (highlighted in red) exhibits a strong LAMP1 signal, which rises shortly after the target cell’s death and remains elevated until the observation concludes.</p>
<p>Using the pair annotation feature, we aimed to identify “killer-victim” pairs. An expert annotated each neighborhood containing a dead target cell, identifying the most probable effector cell killer to establish these pairs. This annotation was done via a cell pair viewer, similar to the viewer designed for single cells, which leveraged color cues for pre-determined target cell death events and immobilized effector cells (Supplementary video 4). The expert viewed only the brightfield movie, blinded to the LAMP1 signal, and identified effectors with prolonged, restricted movement in contact with the target cell. Effectors that arrived post-mortem were excluded, and in ambiguous cases, multiple pairs could be labeled as “killer-victim” within a neighborhood. Due to the complexity of this task, precise event times were not annotated. <xref rid="fig5" ref-type="fig">Figures 5D-E</xref> display average relative velocity and LAMP1 expression for each neighborhood, with values grouped by killer-victim status and shown in paired boxplots. Results indicate significantly higher LAMP1 expression and lower relative velocity in killer-victim pairs, with both metrics showing a large effect size (Cliff’s Delta <italic>&gt;</italic> 0.474).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Here we developed a new software to study multichannel time-lapse microscopy data at scales ranging from single-cells to cell populations. It integrates original methods for event detection and cell-pair analysis with established methods for cell segmentation and tracking [<xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c41">41</xref>]. Although the pipeline steps are primarily sequential (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>, S7) and could be performed independently with different tools and scripts, we demonstrate how integrating them within a unified, interactive software environment enhances data exploration and facilitates the analysis of dynamic cell interactions in large datasets.</p>
<p>Celldetective demonstrated its capacity to manage datasets of hundreds of gigabytes, generating high-quality plots that include hundreds of single-cell traces and data points. As a standalone application, it empowers biology experts to analyze single-cell data, annotate labels and events, and train models to automate tasks without requiring coding skills—similar to recent advances for fixed images [<xref ref-type="bibr" rid="c54">54</xref>]. Celldetective integrates the open-source annotation tool napari for label correction and introduces unique tools for event annotation and pair classification. The software combines traditional threshold-based image and signal analysis methods with deep learning-based approaches, which can be used complementarily for enhanced results.</p>
<p>We showed how recording multivariate time series may reveal multiple phenomena occuring during the same event. For example, cell death, monitored through an increase in PI fluorescence, was observed alongside apparent nuclear shrinkage (see <xref rid="fig3" ref-type="fig">Fig. 3C</xref>). We interpret this as cell de-adhesion upon death, releasing the pushing pressure on the nucleus and allowing its axial shape relaxation [<xref ref-type="bibr" rid="c67">67</xref>]. In immunotherapy assays involving human primary cells, we prioritized a fluorescence-based labeling strategy that avoids genetic modification. Due to the exchange of fluorescent dyes between cell populations and cell lysis during the assay, along with significant size disparities between effector and target cells, generalist deep learning segmentation models perform poorly. To address this, we trained custom models within the software, using StarDist for target nuclei segmentation (convex object) and Cellpose for effector cells (any shape). Performance improved slightly with the simultaneous use of multiple input channels (see <xref rid="fig3" ref-type="fig">Fig. 3B</xref>). Looking up close, the improvement was more noticeable on difficult data (blurry, noisy). This approach also benefited event detection: training a model on both PI and nuclear size time series enhanced the event classification performance on the two most important classes for survival studies (see <xref rid="fig3" ref-type="fig">Fig. 3D</xref>). Our spatially resolved immunotherapy assay documents the effect of heterogeneity in target cell density. Indeed, we observed how the high density of target cells reduces their ADCC killing rate, suggesting an apparent protective effect of close target neighbors (see <xref rid="fig3" ref-type="fig">Fig. 3G</xref>).</p>
<p>Immune cells leverage transient and weak contacts to take rapid decisions [<xref ref-type="bibr" rid="c9">9</xref>]. We demonstrate the power of high throughput imaging and analysis to explore such mechanisms. In a cell-surface assay, we measured by interference microscopy the decision time for NK cells to spread on an antigen surface in the presence of bispecific antibody. Celldetective provides a higher statistical power than in earlier works [<xref ref-type="bibr" rid="c59">59</xref>]. Thus, we observed differences in NK response to bispecific antibodies concentration in the minute range, which could not be resolved yet for other systems like TCR-pMHC [<xref ref-type="bibr" rid="c68">68</xref>]. Here, the detection of sharp cell morphology changes during adhesion and spreading could be further enhanced by combining RICM with brightfield imaging, since each imaging modality exhibit a contrast specific to each cell state [<xref ref-type="bibr" rid="c58">58</xref>].</p>
<p>Proximity between effector and target provides a spatial gating out of reach for traditional flow cytometry, showing some clear differences in LAMP1 signal and relative velocity in neighboorhoods of target. We introduce a classification scheme, which allows quantification of a subpopulation based on arbitrary thresholds on any measurement. Additionally, instantaneous classification can be complemented by a time series analysis, with tracks available. To go further into the analysis of effector-target dynamic interactions, we addressed the question of killer:victim pair identification. The strategy consisted in collecting relative measurements like velocity and manually annotating the potential killer for each target from the images. Our data are consistent with one or several killers. The most prominent parameters for killer identification are relative velocity and LAMP1 signal. Since cell-pairs were implemented like single-cells, Celldetective gives the possibility to train an event detection model from pair, target and effector time series to automate killer:victim pair identification.</p>
<p>As a perspective, further work is required to analyze automatically the dynamic signaling and provide more robust prediction. Celldetective offers in principle the complete environment for such an advanced analysis, as demonstrated with the death time automatic detection.</p>
<p>For its applicability, Celldetective does not require coding skills. It was tested by graduate students from our labs with diverse backgrounds in biophysics, cell biology and immunology. Its release is accompanied by examples and a complete documentation. Software maintenance and user assistance are offered via github platform and a professional development team (<ext-link ext-link-type="uri" xlink:href="https://github.com/centuri-engineering">https://github.com/centuri-engineering</ext-link>). One highlight of Celldetective is to offer alternative methods, in particular for cell segmentation, so that the user can maximize the outcome. In order to guide the user for software applicability and workflow, we propose a decision tree, visible as question marks in the interface, to assist in choosing between the different options.</p>
<p>The experimental systems analyzed here are largely two-dimensional, but at the cellular scale, exhibit a three-dimensional aspect. Future software expansions could incorporate 2.5D or 3D sequence analysis. Currently, to our knowledge, only proprietary solutions are offered and exploited in immunotherapy assays [<xref ref-type="bibr" rid="c26">26</xref>]. Notably, cell-interaction studies benefit from higher time resolution, which may limit compatibility with extensive 3D analysis.</p>
<p>The methodologies we demonstrate here should adapt well to commercial high-throughput automated microscopes for long-term cell co-culture studies [<xref ref-type="bibr" rid="c39">39</xref>], for which no efficient analysis solutions are available, to our knowledge. A distinctive aspect of our approach is the use of compact, efficient models that are user-trainable on a single GPU setup. This contrasts with approaches relying on large-scale databases [<xref ref-type="bibr" rid="c39">39</xref>] and extensive DL models [<xref ref-type="bibr" rid="c69">69</xref>]. Moreover, the software’s integrated visualization and annotation features encourage meticulous review of segmentation and tracking results, enhancing confidence in the analysis.</p>
<p>Convolutional models used by the software, such as U-Net and ResNet, are prone to forgetting previously learned tasks upon transfer [<xref ref-type="bibr" rid="c70">70</xref>]. To address this, Celldetective allows the addition of the original dataset during transfer from a model we trained. Future extensions could include foundation models like SAM [<xref ref-type="bibr" rid="c71">71</xref>] for broader adaptability.</p>
</sec>
<sec id="s4">
<title>Conclusion</title>
<p>In conclusion, Celldetective is a user-friendly software designed for non-specialists, providing advanced cell-scale analysis for large cell populations. Combining high time-resolution with robust statistical capabilities, we show the potential to uncover new fundamental parameters characterizing dynamic interactions between immune and tumor cells. We demonstrated the efficiency of Celldetective through two applications in the context of immunotherapy by isolating time-dependent cell-scale parameters and paired-interactions, in high cell-density environment. We believe that such approach holds great potential for enhancing the design and discovery of drugs in a more powerful and rational manner, while also finding broader applications in fundamental studies of cell-cell interactions in developmental biology, microbiology or neurosciences.</p>
</sec>
<sec id="s5">
<title>Materials and Methods</title>
<sec id="s5a">
<title>Software and hardware</title>
<p>The software, implemented as a Python package with a PyQt5 graphical user interface (GUI), incorporates Material Design for styling and integrates Matplotlib canvases for displaying images, animations, and plots. Each processing module—segmentation, tracking, measurement, event detection, and pair analysis—operates on a single movie at a time within an independent subprocess. Upon completion, CPU and GPU memory are fully released, enabling seamless iteration on the next movie. Where applicable, multi-threading has been implemented and can be customized within the software. Each module generates outputs (images or tables) saved directly to the experiment folder, ensuring data preservation and modularity. This design allows users to import data flexibly from external tools, including ImageJ and Cellpose UI. Detailed outputs include mask images from segmentation, trajectory tables from tracking, and additional columns for measurements and event detection. The neighborhood module further extends the trajectory tables, producing a pickle file with a nested-dictionary structure that logs neighbor information at each time point.</p>
<p>Development and testing were conducted on various hardware configurations: an Intel Core i9 CPU, NVIDIA RTX 3070 GPU, and 16 GB RAM desktop running Ubuntu 20.04 for primary development; extensive testing was completed on both Windows 11 (Intel Core i9 and Core i7 laptops) and an older Ubuntu 20.04 desktop with an Intel Core i5 CPU. Celldetective is routinely validated across Python versions 3.9 to 3.11 on both Windows and Linux, with comprehensive procedural and graphical unit tests.</p>
</sec>
<sec id="s5b">
<title>Antibody design and production</title>
<p>The bispecific antibody (bsAb) C7b-21 (or CE4-21, CE4-28) is a fusion of two single domain antibodies (sdAb, also called nanobodies), sdAb C7b (or CE4) targeting against Human epidermal growth factor receptor-2 (HER-2/neu or ErbB2) [<xref ref-type="bibr" rid="c72">72</xref>] and sdAb C21 (or C28) targeting human CD16 (Fc<italic>γ</italic>RIII), using the human CH1/Ck heterodimerization motif, corresponding to the bispecific Fab (bsFab) format [<xref ref-type="bibr" rid="c73">73</xref>]. bsFab CE4-28 was produced by co-transfection of two plasmids using the mammalian transient system Expi293 (Thermofisher) and purified as previously described [<xref ref-type="bibr" rid="c74">74</xref>]. For experiments in <xref rid="fig4" ref-type="fig">Fig. 4</xref> bsAb CE4-21 and control antibody CE4-X where specifically labelled with Atto647 dye using transglutaminase reaction (Zedira, Darmstadt, Germany).</p>
</sec>
<sec id="s5c">
<title>Cell lines culture, effector extraction and phenotyping</title>
<p>Modified MCF7-HER2+ cells were stably transfected from MCF7 cell line to overexpress HER2 receptors. Determination of HER-2 levels on the cells was performed by flow cytometry with Herceptin antibody and a secondary fluorescent anti-human antibody. Fluorescence intensity was determined and correlated with HER-2 levels. Cells were maintained in RPMI 1640 media (Gibco, Life Technologies), supplemented with 10% of Fetal Bovine Serum, FBS (Gibco, Life Technologies), 50 mg/mL of hygromycin as an antibiotic resistance selection. Cells were amplified three times per week and keep in the incubator at 37<italic><sup>◦</sup></italic>C under 5% CO<sub>2</sub> atmosphere.</p>
<p>Primary human NK cells. NK cells were isolated as described in [<xref ref-type="bibr" rid="c58">58</xref>]. Briefly, blood samples were obtained from the Etablissement Francais du Sang (Marseille, France), using the MACSxpress whole-blood human NK cell isolation kit (Miltenyi Biotec, Bergisch Gladbach, Germany) a negative selection was performed. The characterization of the sorted cells was determined by flow cytometry with anti-CD16, anti-CD3 and anti-CD56 antibodies. Cells were maintained in RPMI 1640 medium and 10% FBS at 37<italic><sup>◦</sup></italic>C, 5% CO<sub>2</sub> and used in the following 24 hours.</p>
</sec>
<sec id="s5d">
<title>Cell-surface assay</title>
<p>Primary human NK cells, freshly isolated and prepared at a concentration of 200,000 cells/mL, were introduced to an uncoated eight-well chambered polymer coverslip-bottom (Ibidi, 80821). For surface preparation, each well was initially rinsed with PBS, then incubated with 100 <italic>µ</italic>g/mL Biotin-labeled bovine serum albumin (BSA; Sigma A8549-10MG) in PBS for 30 minutes at room temperature (RT) with agitation. Non-adsorbed BSA-biotin was removed by four PBS rinses. Next, a 10 <italic>µ</italic>g/mL Neutravidin solution (Thermo Scientific, 31000) in PBS was added for 30 minutes at RT with agitation, followed by another four PBS rinses. The wells were then incubated with a 10 nM/mL solution of Her2/ERBB2 protein (Sinobiological, 10004-H27H-B) in 0.2% BSA for 30 minutes at RT with agitation and subsequently washed four times with PBS. After this surface preparation, the Ibidi chamber was placed under a microscope at 37<italic><sup>◦</sup></italic>C. Prior to NK cell introduction, the cells were pre-incubated with bsFab (C7b-21) at the desired concentration for 30 minutes at 37<italic><sup>◦</sup></italic>C. They were then injected into the prepared sample.</p>
<p>Cell-surface contact and spreading dynamics were monitored using reflection interference contrast microscopy (RICM), which is sensitive to variations in cell-surface distance. The RICM configuration, described in detail by Limozin et al. [<xref ref-type="bibr" rid="c61">61</xref>], employed an antiflex Zeiss objective (NA = 1.25) and a green LED light source (<italic>λ</italic> = 546 nm) with a 14-bit CCD detector (Andor iXonEM, Oxford Instruments). This setup allowed for live cell observation at 37<italic><sup>◦</sup></italic>C. Images were taken after cell deposition in the chamber, with sixteen fields of view selected per condition and monitored through cyclic imaging over a 10-minute interval. The motorized stage (Physik Instrumente, Germany) facilitated sequential imaging, with temporal intervals between images in each field ranging from 15 to 20 seconds, enabling analysis of decision time between cell landing and spreading as well as NK cell spreading kinetics.</p>
</sec>
<sec id="s5e">
<title>Cell-cell assay</title>
<p>MCF7-Mod cells, expressing HER2 at a surface density of approximately 500 molecules/<italic>µ</italic>m<sup>2</sup> [<xref ref-type="bibr" rid="c66">66</xref>], were used as targets. 80,000 cells per well were seeded into an 8-well chamber <italic>µ</italic>-Slide (Ibidi—Munich, Germany, polymer bottom, TC treated) and cultured overnight in RPMI complemented media under standard conditions (37<italic><sup>◦</sup></italic>C, 5% CO<sub>2</sub>) to reach exponential growth. The next day, media was removed, and cells were incubated with Hoechst 33342 (5 <italic>µ</italic>g/mL) in colorless RPMI at 37°C for 10 minutes. After three rinses in warm RPMI with 10% FBS, cells were returned to the incubator.</p>
<p>Primary NK cells were labeled with CFSE dye following manufacturer instructions (CFSE CellTrace, ThermoFisher). 2.5 million cells were centrifuged at 1500 rpm for 5 minutes, supernatant was discarded, and cells were incubated with 2.5 mL CFSE in PBS for 20 minutes at 37<italic><sup>◦</sup></italic>C. Following a brief incubation in RPMI with 10% FBS (12.5 mL added to the cells for 5 minutes), cells were centrifuged, resuspended in complete media, and incubated.</p>
<p>Dilutions of the bispecific antibody (bsAb) CE4-28 were prepared for final concentrations of 1, 10, and 100 pM. The bsAb solution (2 <italic>µ</italic>g/mL) was added to wells with Hoechst-labeled target cells, incubated at 37<italic><sup>◦</sup></italic>C for 20 minutes, followed by Propidium Iodide (PI; Sigma). NK cells were then added at an effector-to-target (E:T) ratio of 2.5, and the slide was placed on a Zeiss AxioObserver microscope with temperature control at 37<italic><sup>◦</sup></italic>C. Imaging was performed with a 20x/0.4 objective (pixel size 0.31 <italic>µ</italic>m) in transmitted light (brightfield) and epifluorescence channels (CFSE, Hoechst, PI), capturing 5-9 fields per well over 2-3 hours with 3-minute intervals between frames.</p>
<p>For NK cell tracking, observations were conducted using a 40x/1.3 Oil DIC (UV) objective (pixel size 0.157 <italic>µ</italic>m) with a 1.73-minute interval per frame, and the E:T ratio was adjusted to 1:1. To monitor degranulation, anti-LAMP1 APC-labeled antibodies (BioLegend, Clone H4A3, Cat. 328619) were added at 5 <italic>µ</italic>L per 1 million NK cells.</p>
</sec>
<sec id="s5f">
<title>Software methods</title>
<sec id="s5f1">
<title>Tracking tuning</title>
<p>Celldetective provides a graphical interface for the open-source Python package bTrack [<xref ref-type="bibr" rid="c41">41</xref>], integrating its Bayesian tracker, which, after tuning, proved well-suited for tracking immune and cancer cells in our applications. We developed a configuration interface that enables users to view and adjust the bTrack configuration file (defining the motion model and tracklet reconnection parameters) or to import configurations from the bTrack-napari plugin. Typical bTrack configuration files are provided for the applications we present. Additionally, users can incorporate mask-based measurements to enhance the Bayesian update, with feature normalization handled automatically by the software. Finally, users can set options for track post-processing (interpolation, extrapolation, filtering) to refine and correct the tracking data. A tracking quality control module visualizes the raw bTrack output (before post-processing) in napari, recoloring labels to preserve cell identity.</p>
</sec>
<sec id="s5f2">
<title>Segmentation model training</title>
<p>To train a model, users can select any combination of channels as the input. Each channel’s normalization can be independently adjusted using either percentile or absolute normalization. For RICM images, which are background-corrected with intensities centered around 1, an absolute normalization range of 0.75 to 1.25 was adopted. In contrast, fluorescence images—where light source power and exposure often vary across experiments—use percentile normalization, typically set between the 0.1-th and 99.99-th percentile for each image. During training, images in the training set are randomly augmented with operations such as flips and lateral shifts. Each input channel can undergo independent intensity adjustments, including blurring (random sigma between 0 and 4 px), physical noise addition (Gaussian, local variance, Poisson, and speckle), and random deactivation of channels (excluding the primary input channel), encouraging the model to learn redundancy across channels. StarDist models are trained for around 300 epochs with a learning rate of approximately 10<italic><sup>−</sup></italic><sup>4</sup> and a typical batch size of 8, while Cellpose models are trained over roughly 3000 epochs with a learning rate near 10<italic><sup>−</sup></italic><sup>2</sup> and a similar batch size.</p>
</sec>
<sec id="s5f3">
<title>Annotations for segmentation</title>
<p>To improve the annotation experience in napari, we developed a plugin that addresses common annotation errors identified with our expert annotators. The plugin detects repeated label values by comparing cell bounding boxes with mask areas, interpreting significant differences as multiple cells sharing the same label. Noncontiguous objects are automatically separated and relabeled, while small objects (less than 9 px<sup>2</sup>) are filtered out. The plugin also reassigns label values sequentially from 1 to the total number of objects, manages annotation naming and storage, and appends experiment metadata to each annotation, enabling precise tracking of annotation origin and context.</p>
</sec>
<sec id="s5f4">
<title>Classification and condition-based event detection</title>
<p>We developed a module to classify cells by writing conditions on the measurements with AND and OR logic gates. A flow-cytometry-like scatter plot showing the classification result at each time point for any combination of measurements assists the user in writing arbitrarily complicated conditions. This classification is instantaneous, as each cell from each time point is classified independently. With cell tracks available, we can perform a time-correlated analysis of the binary condition response for each cell. We implemented two particular descriptions: the first is that the cell has a unique state (condition always true or false): the median response determines the cell class; the second is the irreversible event, introduced to describe the cellular events in our applications. It implied three possible states for each cell: always false (no event observed), always true (the event happened before the beginning of observation), or transitioning from false to true (the event occurred during observation). For transitioning events, the software catches the event time by fitting a sigmoid over the binary response for each cell.</p>
</sec>
<sec id="s5f5">
<title>Deep learning event detection</title>
<p>To automate event detection, we implemented a deep learning approach with two convolutional models: (1) a classifier that assigns each cell to one of three event classes based on selected measurement time series, and (2) a regressor that estimates the event time for cells exhibiting the event, using the same time series input. Both models share a common backbone, a modified 1D ResNet, designed to process multivariate time series (<xref rid="figs12" ref-type="fig">Supplementary Fig. S12</xref>). To match the user-defined input tensor size, time series endpoints are extrapolated by repeating the edge values. In the training set, the time series are augmented with random noise and time shifts to balance the classes and generate artificially a quasi uniform event time distribution. Users can opt to use our pre-trained event-detection models for specific applications (<xref rid="tbls5" ref-type="table">Tab. S5</xref>) or easily train new models on their own datasets generated within the associated viewer, with only a few clicks. The event detection system comprises two sequential ResNet-like models with a shared backbone but distinct heads tailored to different tasks: classification and regression (see <xref rid="figs12" ref-type="fig">Fig. S12</xref>). Input time series are structured as tensors of shape (<italic>T</italic> × <italic>n</italic><sub>channels</sub>), where <italic>n</italic><sub>channels</sub> represents the number of coupled time series (or channels) and <italic>T</italic> is an arbitrary maximum time series length, typically set to 128 frames. The input tensor is initially processed by a 1D convolution layer with a (1,) kernel to expand it into 64 filters. It then passes through two 1D-ResBlocks, each with a (8,) kernel and 64 filters, followed by a max-pooling layer (size 2) that doubles the number of filters to 128. The tensor continues through two additional 1D-ResBlocks before undergoing a global average pooling operation. A dense layer with 512 neurons collects the convolutional information, followed by a dropout layer with a 50 % dropout rate. The final layers differ by model task: for classification, a layer with three neurons and softmax activation distinguishes between three classes; by convention “event observed”, “no event observed”, and “event left censored”; for regression, a single neuron with linear activation predicts <italic>t</italic><sub>event</sub>. The models are trained for 300 and 600 epochs, respectively, using the Adam optimizer with a user-controlled learning rate (typically 10<italic><sup>−</sup></italic><sup>4</sup>, unless mentioned otherwise). For classification, categorical cross-entropy loss is minimized, with class weights introduced to mitigate class imbalance. For regression, the mean squared error is minimized. The batch size, usually set to 64 or 128 depending on the dataset size, is user-controlled. Only cells classified as the first class (“event observed”) proceed to the regression model during both training and inference.</p>
</sec>
<sec id="s5f6">
<title>Neighborhoods</title>
<p>Single cells from two populations can be linked through a neighborhood scheme, which can either define relationships within a single cell population (e.g., targets-targets) or across two distinct populations (e.g., effectors-targets). To accommodate specific application requirements, we developed two distinct neighborhood methods. The mask contact method identifies touching cell masks at any time point, using mask dilation to allow a tolerance for cell contact. The isotropic method, by contrast, disregards cell masks and instead uses a circular region of a selected radius, centered on a reference cell’s centroid, to determine neighboring cells. This approach is particularly useful for quantifying cell co-presence in cases with nuclear segmentation or unreliable masks. Neighbor counts extend the single-cell data tables in the same way as other single-cell measurements (more details in <xref rid="figs13" ref-type="fig">Supplementary Fig. S13</xref>). The reference-neighbor pairs identified at least once are then measured (e.g., relative distance, angle, velocity) and stored as rows in a cell-pair table. Similar to single cells, these pairs can be classified and annotated through an interactive cell pair viewer, which will be demonstrated in the second application.</p>
</sec>
</sec>
<sec id="s5g">
<title>Details on the analyis of the cell-surface assay</title>
<sec id="s5g1">
<title>Image preprocessing</title>
<p>A RICM background for each well of the cell-surface assay was reconstructed using the model-free background estimate method in Celldetective (described in <xref rid="figs8" ref-type="fig">Supplementary Fig. S8B</xref>). The reconstructed backgrounds were optimized and divided to the RICM images, yielding float intensities centered around 1.</p>
</sec>
<sec id="s5g2">
<title>Single-cell extraction</title>
<p>The traditional segmentation pipeline consisted of two mathematical operations on the pixels (subtracting one and taking the absolute value) and a Gaussian filter (kernel of 0.8 pixels), followed by thresholding, watershed and automatic removal of non-lymphocyte objects. The pipeline was defined with the dedicated Celldetective module (<xref rid="figs9" ref-type="fig">Supplementary Fig. S9</xref>). To track the cells, the bTrack configuration was tuned to increase the isotropic spatial bin size to consider hypotheses (dist_thresh = 99.99), so that hypothesese could be generated for each tracklet. The branching, apoptosis and merging tracklet reconnection hypotheses were disabled and the segmentation miss rate reduced to 1 %. The position-based intensity measurements were performed in a circle of 2 <italic>µ</italic>m diameter centered around the cell positions.</p>
</sec>
<sec id="s5g3">
<title>Spreading rate</title>
<p>For each spread cell, the area was differentiated as a function of timeusing a sliding window of 5 frames (1 min 18 s) looking forward in time: from <italic>t<sub>i</sub></italic>to <italic>t<sub>i</sub></italic><sub>+4</sub> (this was configured graphically in the table exploration feature of Celldetective). The value at the onset of spreading <italic>t</italic><sub>spread</sub> was then extracted for each cell (with a track collapse option in the software) and plotted.</p>
</sec>
<sec id="s5g4">
<title>Segmentation benchmark</title>
<p>To use the Cellpose <italic>cyto3</italic> model, we passed a diameter value of 15 <italic>µ</italic>m, a cellprob_threshold of 0 and flow_threshold of 0.4. For each sample in the test set, the lymphocyte detection accuracy was defined as: <italic>DA</italic> = TP<italic>/</italic>(TP + FP + FN), with TP the number of lymphocytes predicted, FP the number of non-lymphocyte objects detected and FN the number of lymphocytes missed. A detection was accepted when the intersection over union (IoU) reached at least 0.1. The segmentation quality was defined as the IoU between the ground truth and predicted masks when they could be matched (TP detections). Since hovering cells had an ill-defined area, the segmentation quality was only computed for the subset of spread cells. The scores reported in <xref rid="fig2" ref-type="fig">Fig. 2</xref> are the average over all test samples of the lymphocyte detection accuracy and the segmentation quality. The average score was defined as the average of these two scores for each segmentation method. The benchmark was done in a Jupyter Notebook.</p>
</sec>
</sec>
<sec id="s5h">
<title>Details on the analyis of the cell-cell assay</title>
<sec id="s5h1">
<title>Image preprocessing</title>
<p>The fluorescent bsAb and LAMP1 channels were corrected for the background using the paraboloid fit technique described in <xref rid="figs8" ref-type="fig">Supplementary Fig. S8A</xref>. The fitted background for each image was subtracted to the original image, without clipping. The movies from the LAMP1 experiment were aligned prior to analysis with the SIFT multichannel registration plugin in Fiji (to be able to investigate effector cell velocities). The macro used to align the movies in a Celldetective experiment project is available in the documentation.</p>
</sec>
<sec id="s5h2">
<title>Tracking of target cells</title>
<p>The MCF-7 nuclei were tracked with a quasi-stationary motion model, allowing for small displacements and potentially long-time gaps. We passed the Hoechst and PI intensities and the nuclear apparent area to the tracker which helped in experiments where the live/dead transitions were not too sudden. Cells not present in the first frame of the movie were filtered out to limit the number of false positive detections and exclude daughter cells from our study. Tracks were extrapolated until the end of the observation to measure intensities at the last detected position, which worked well in this system since the nuclei were mostly stationary.</p>
</sec>
<sec id="s5h3">
<title>Effector cell extraction</title>
<p>For the dynamical study, some manual corrections of target and effector segmentation results had to be done to achieve an excellent tracking quality for both populations and study their interactions. Effector cell velocity was computed over a sliding window of 3 frames (5.19 min) centered on the current time (from <italic>t<sub>i</sub><sub>−</sub></italic><sub>1</sub> to <italic>t<sub>i</sub></italic><sub>+1</sub>).</p>
</sec>
<sec id="s5h4">
<title>bsAb fluorescence measurements</title>
<p>The mean bsAb fluorescence intensity over each NK cell mask was measured from the background corrected images.</p>
</sec>
<sec id="s5h5">
<title>LAMP1 measurement</title>
<p>The mean LAMP1 intensity over each NK cell mask was measured from the corrected images. We classified cells as LAMP1-positive or LAMP1-negative at each time point using an intensity threshold (<italic>I</italic><sub>LAMP1</sub> <italic>&gt;</italic> 5 a.u.).</p>
</sec>
<sec id="s5h6">
<title>Contact classification</title>
<p>Target cell contact with NK cells was inferred using a radial neighborhood calculation, as only target cell nuclei masks were available, and was further classified by the number of neighboring target cells: an NK cell in contact was defined as having at least one target neighbor (inclusive counting convention, see <xref rid="figs13" ref-type="fig">Supplementary Fig. S13</xref>). A quality check of this contact classification was performed in-situ on the images using a dedicated viewer that displays cells directly on microscopy images (Supplementary Video 3).</p>
</sec>
<sec id="s5h7">
<title>Simulation</title>
<p>The Monte-Carlo simulation for the sum of bsAb fluorescence signals and its comparison with the other conditions was performed in a Jupyter Notebook with in-house code and functions from the Celldetective package. This notebook allowed us to produce <xref rid="fig4" ref-type="fig">figures 4D</xref> and Figure Supp S14.</p>
</sec>
<sec id="s5h8">
<title>Segmentation benchmark</title>
<p>Nuclear detection accuracy was defined in the same way as the lymphocyte detection accuracy in the cell-surface assay. To use the Cellpose <italic>nuclei</italic> model, we set the diameter to 22 <italic>µ</italic>m, the cellprob_threshold parameter to 0.0 and the flow_threshold parameter to 0.4. The segmentation quality is computed over all nuclei. As before, the scores reported in <xref rid="fig3" ref-type="fig">Fig. 3</xref> are the average over all test samples of the detection accuracy and segmentation quality. The average score was defined as the average of these two scores for each segmentation method.</p>
</sec>
<sec id="s5h9">
<title>Death event detection benchmark</title>
<p>The event detection models we present were trained on the db-si-NucPI dataset available in Zenodo with a 20 % validation split and 10 % test split, randomized at each training. For each tested combination of batch size (BS ∈ [<xref ref-type="bibr" rid="c64">64</xref>, 128, 256]) and learning rate (<italic>α</italic> ∈ [10<italic><sup>−</sup></italic><sup>4</sup>, 10<italic><sup>−</sup></italic><sup>3</sup>, 10<italic><sup>−</sup></italic><sup>2</sup>, 10<italic><sup>−</sup></italic><sup>1</sup>]), we trained 10 event detection models for 300 epochs with the two different input approaches (PI time series alone or PI + nuclear area time series). The time series were normalized with a percentile normalization (0.1 % and 99.9 %) and clipping. The model input tensor length was set to 128 frames. For the hyperparameters that achieved the best average performance (here BS = 64 and <italic>α</italic> = 10<italic><sup>−</sup></italic><sup>4</sup>), we picked the best event detection model produced out of the 10 for the benchmark of <xref rid="fig3" ref-type="fig">Fig 3D</xref>. This benchmark was performed on a separate test set, identical for all methods. The annotations came from 3 movies, two of which belonged to experiments never seen by the models. The third is from a position adjacent to a position seen by the models. It was selected due to its over-representation of cells that are already dead from the beginning, or dying at the very beginning, which are very rare in most experiments. The benchmark was done in a Jupyter notebook.</p>
</sec>
</sec>
<sec id="s5i">
<title>Statistics</title>
<sec id="s5i1">
<title>Statistical tests</title>
<p>Since for all multi-condition comparisons at least one distribution was not normally distributed, we assessed statistical difference using the nonparametric two-sample Kolmogorov-Smirnov one-sided test (Python package scipy.stats.ks_2samp). P-values were converted into star notations following the GraphPad convention: P <italic>&gt;</italic> 0.05: ns, P <italic>&lt;</italic> 0.05: *, P <italic>&lt;</italic> 0.01: **, P <italic>&lt;</italic> 0.001: ***, P <italic>&lt;</italic> 0.0001: ****.</p>
</sec>
<sec id="s5i2">
<title>Effect sizes</title>
<p>For statistically significant differences, we computed Cliff’s Delta to report the effect size (Python package cliffs-delta). We followed the conventions of the package to convert the effect size value into a qualitative effect: <italic>δ &lt;</italic> 0.147: negligible, <italic>δ &lt;</italic> 0.33: small, <italic>δ &lt;</italic> 0.474: medium, <italic>δ &gt;</italic> 0.474: large.</p>
</sec>
<sec id="s5i3">
<title>Plots</title>
<p>Most plots presented in this article were generated within Celldetective using the matplotlib and seaborn packages. Notable exceptions include the benchmarks (segmentation scores, confusion matrix and correlation plots). Composition was done on Inkscape to add statistical tests and effect sizes or merge separate plot layers.</p>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="s6">
<title>Supplementary Tables</title>
<table-wrap id="tbls2" orientation="portrait" position="float">
<label>Table S2</label>
<caption><title>Generalist deep learning segmentation models.</title><p>This table lists the different generalist models (Cellpose or StarDist) which can be called natively in Celldetective. The sample images are cropped to (200 <italic>×</italic> 200) px and rescaled homogeneously to fit in the table.</p></caption>
<graphic xlink:href="585250v3_tbls2.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tbls3" orientation="portrait" position="float">
<label>Table S3</label>
<caption><title>MCF7 nuclei segmentation models in the presence of primary NK cells.</title>
    <p>Each model was trained on the same dataset of ADCC images, picking only the relevant channels.</p></caption>
<graphic xlink:href="585250v3_tbls3.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tbls4" orientation="portrait" position="float">
<label>Table S4</label>
    <caption><title>Primary NK segmentation models.</title>
    <p>The models have been trained on a dataset of annotated primary NKs in ADCC images (<italic>primary NKs w MCF7</italic>).</p></caption>
<graphic xlink:href="585250v3_tbls4.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tbls5" orientation="portrait" position="float">
<label>Table S5</label>
    <caption><title>Event detection models.</title>
    <p>We trained the following 1D DL models to classify and regress events of interest. The mean event response, centered at the event time is shown for each channel in the pattern column.</p></caption>
<graphic xlink:href="585250v3_tbls5.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s7">
<title>Supplementary Figures</title>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S6</label>
<caption><title>Main GUI windows:</title>
<p>A) Experiment project selection window. B) GUI to generate a new experiment, where the user provides the metadata. C) Main interface after loading a project. The process block for the effector population is unravelled, showing the 4 main steps detailed in <xref rid="fig1" ref-type="fig">Fig. 1</xref>.</p></caption>
<graphic xlink:href="585250v3_figs6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs7" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S7</label>
<caption><title>Processing modules to extract single cells:</title>
<p>Processing modules are shown on the left, with brief illustrations of the main options, while the output and visualization modules are represented on the right side, following Celldetective’s graphical structure. A typical pipeline features successively (from top to bottom): A) an instance segmentation (either with traditional thresholding method or a deep learning model, stored in a model zoo, using StarDist [<xref ref-type="bibr" rid="c35">35</xref>] or Cellpose [<xref ref-type="bibr" rid="c37">37</xref>]). The output data are masks, which can be visualized and annotated using napari [<xref ref-type="bibr" rid="c56">56</xref>], for correction or model retraining. B) Bayesian tracking associated with feature extraction (btrack [<xref ref-type="bibr" rid="c41">41</xref>]), the output data being trajectories, which can also be visualized with napari. C) Single cell measurements of intensity, morphology, texture (based on cell position or on mask), the output being tables of trajectories enriched with features.</p></caption>
<graphic xlink:href="585250v3_figs7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs8" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S8</label>
<caption><title>Background correction methods.</title>
<p>Screenshots of the background correction tabs in the software and illustration of the associated pipeline. Colored bounding boxes match graphical parameters to their role in the pipeline. The thick black arrows indicate the starting point of the pipelines. A) The steps for a fit-based correction involve selecting a channel of interest, setting a threshold on the standard-deviation filtered image to exclude the cells (done graphically with a viewer). Then a model (either plane or paraboloid) is fit on the background pixels for each image. The fitted background is either subtracted or divided, with or without clipping. B) With the model-free approach, a single background is reconstructed per well, following the same threshold approach but leveraging the multi-positional information to construct a median background. Instead of applying the background directly, it is amplified to minimize the difference with the current image’s background. As with A), the background is then either subtracted or divided, with or without clipping.</p></caption>
<graphic xlink:href="585250v3_figs8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs9" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S9</label>
<caption><title>Designing a traditional segmentation pipeline.</title>
<p>Screenshots of the traditional segmentation pipeline configuration interface applied to a normalized RICM image of spreading primary NK cells. Top: User-selected mathematical operations (subtraction, absolute value) and preprocessing filters (Gaussian blur) are applied to the image, with results displayed in real-time on the right. A binary mask, set using an interactive thresholding slider, is overlaid on the transformed image in purple. Spot detection is then performed on the Euclidean Distance Transform of the binary mask, using parameters for footprint size and minimum object distance; detected spots are shown as red scatter points. Bottom: After applying the watershed algorithm, the instance segmentation of cells is overlaid on the original normalized RICM image. The user can set filters on object features to exclude false positives; rejected cells are marked in red in both the feature scatter plot and the image.</p></caption>
<graphic xlink:href="585250v3_figs9.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs10" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S10</label>
<caption><title>Segmentation correction and annotation.</title>
<p>To view segmentation results, the user can click the “eye” button in the segmentation module, which opens images and labels in napari. Shown is a screenshot of a segmentation outcome for a normalized RICM image of spreading primary NK cells, with labels overlaid on the image. The user can correct the masks directly on the image, for example, to separate cells indicated by white arrows. New labels can be assigned and saved, and the full set of corrected masks can either be saved in place (top-right plugin) or exported as a training sample for a segmentation model (bottom-right plugin).</p></caption>
<graphic xlink:href="585250v3_figs10.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs11" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S11</label>
<caption><title>Overview of segmentation strategies.</title>
<p>The principle is illustrated for a mixture of two cell populations, effector and targets. Celldetective provides several entry points (black arrows) to perform segmentation, with the intent of segmenting specifically a cell population (left: effectors, right: targets). The generalist DL models are listed in Tab. S2. Specific DL models are listed in Tab. S3 for targets and Tab. S4 for effectors. The traditional pipeline refers to a thresholding method accessible through the GUI (see Fig. S9) and which can serve as a starting point to prepare a dataset for training a new DL model. The masks output from each segmentation technique can be visualized and manually corrected in napari. Exporting those corrections into a dataset of paired image/masks can be used either to fit a generalist model (transfer learning) or train a new model from scratch.</p></caption>
<graphic xlink:href="585250v3_figs11.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs12" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S12</label>
<caption><title>Event detection model architectures.</title>
<p>The event detection models consist of two CNN-based models called back to back during inference to 1) classify single-cell time series and 2) detect the event times from the same time series. The models share a similar architecture of a first 1D convolution, followed by ResBlocks encoding the time series into either three neurons with a softmax activation for the classification problem (three classes) or one neuron with a linear activation for the regression problem. As illustrated here, only the time series that were classified as “event” are sent to the regressor model.</p></caption>
<graphic xlink:href="585250v3_figs12.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs13" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S13</label>
<caption><title>Neighbor counting methods:</title>
<p>A) All neighboring cells in contact or within the isotropic neighborhood are linked to the reference cell (inclusive method). B) Each neighboring cell is linked exclusively to the closest reference cell (exclusive method). C) Neighboring cells in contact or within the isotropic neighborhood are linked to the reference cell, with weights inversely proportional to their number of neighbors.</p></caption>
<graphic xlink:href="585250v3_figs13.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs14" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S14</label>
<caption><title>Effect size at each time point:</title>
<p>Cliff’s Delta between the simulated sum of two distributions (CE4-21/off-contact/HER2+ + CE4-X/in-contact/HER2+) and CE4-21/in-contact/HER2+ computed at each timepoint, independently.</p></caption>
<graphic xlink:href="585250v3_figs14.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<ack>
<title>Acknowledgments</title>
<p>We thank Cristina Gonzalez and Maddy Messa N’Dong for preliminary experiments; Juliette Prothon for producing and labelling the bispecific antibodies; Adrien Aimard, Dominique Touchard, Martine Biarnes for excellent technical support; Pierre-Henri Puech and Thierry Galliano for fruitful discussions. The project leading to this publication has received funding from France 2030, the French Government program managed by the French National Research Agency (ANR-16-CONV-0001) and from Excellence Initiative of Aix-Marseille University - A*MIDEX AMX-21-IET-017, as well as AMIDEX Emergence Innovation (project ForSelecAntibody), Plan Cancer PhysCancer program (project ComPhysAb) and ANR-22-CE09-0014..</p>
</ack>
<sec id="suppd1e1049" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Supplementary video 1.</label>
<caption><title>Single timepoint contact.</title><p>The RICM channel is loaded in grayscale into the dynamic event annotation viewer, looping at a user-controlled frame rate. First, the user selects the &quot;firstdetection&quot; class, corresponding to the initial contact event of cells with the surface, and interacts with individual cells (represented by centroids marked on the image) to verify and adjust the detected t_contact times. The outer circle of each cell’s marker represents the event class: red for observed events, blue for no observed event. The inner cross indicates the current status within the class: blue for &quot;event not yet happened,&quot; and red for &quot;event happened.&quot; By clicking directly on a cell in the image, users can view the RICM intensity time series for that cell, displayed on the left side. Next, the user switches to the &quot;spread&quot; class to review and adjust spread event detection. The user can correct event classifications and timings, seamlessly switching between event types.</p></caption>
<media xlink:href="105302Video1.mp4"/>
</supplementary-material>
<supplementary-material id="supp2">
<label>Supplementary video 2.</label>
<caption><title>Interactive And Dynamic Event Representation.</title><p>A user-defined RGB composite of the PI, CFSE, and Hoechst channels from an ADCC movie stack is loaded into the dynamic event annotation viewer. The viewer plays the RGB movie in a loop at a user-controlled frame rate. Users can correct pre-generated death event classifications and timings, with updates reflected immediately in the time series and on the image.</p></caption>
<media xlink:href="105302Video2.mp4"/>
</supplementary-material>
<supplementary-material id="supp3">
<label>Supplementary video 3.</label>
<caption><title>Interactive Single-Cell Classification Representation.</title><p>Single-cell data from the effector cell population in an ADCC experiment is monitored in situ on the Brightfield images. Each cell’s outer circle encodes its contact state, indicating whether it has at least one neighboring cancer cell (red for contact, blue for no contact), following the protocol described in XX. This contact state can be manually corrected by the user. For a selected cell, its measurements are highlighted as a red dot on box plots displayed on the left, representing the distribution of all measurements in this movie. The blue strip plot shows measurements of all cells within the current frame. The user can normalize measurements on-the-fly to compare quantities with different dimensions side by side.</p></caption>
<media xlink:href="105302Video3.mp4"/>
</supplementary-material>
<supplementary-material id="supp4">
<label>Supplementary video 4.</label>
<caption><title>Cell Interaction Viewer.</title><p>Target and effector cell positions are overlaid on an animation of the Brightfield channel from an ADCC experiment. After selecting a neighborhood of interest and applying color coding to cells based on a precomputed attribute (e.g., classification or event type), the user can click on a reference cell to view all its neighbors at any time point. An interactive line segment represents each cell pair, allowing the user to annotate the pair and explore associated time series data. Available data includes measurements from the reference cell (here: PI intensity), the neighbor cell, or the cell pair itself (here: relative velocity and relative distance).</p></caption>
<media xlink:href="105302Video4.mp4"/>
</supplementary-material>
</sec>
<sec id="nt1">
<title>Note</title>
<p>This reviewed preprint has been updated to add the supplementary videos.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rafelski</surname>, <given-names>S. M.</given-names></string-name> &amp; <string-name><surname>Theriot</surname>, <given-names>J. A</given-names></string-name></person-group>. <article-title>Establishing a conceptual framework for holistic cell states and state transitions</article-title>. <source>Cell</source> <volume>187</volume>, <fpage>2633</fpage>–<lpage>2651</lpage> (<year>2024</year>).</mixed-citation></ref>
    <ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kramer</surname>, <given-names>B. A.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Sarabia del Castillo</surname></string-name>, <string-name><given-names>L.</given-names> &amp; <surname>Pelkmans</surname></string-name></person-group> <article-title>Multimodal perception links cellular state to decision-making in single cells</article-title>. <source>Science</source> <volume>377</volume>, <fpage>642</fpage>–<lpage>648</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bagheri</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Carpenter</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Lundberg</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Plant</surname>, <given-names>A. L.</given-names></string-name> &amp; <string-name><surname>Horwitz</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>The new era of quantitative cell imaging—challenges and opportunities</article-title>. <source>Molecular Cell</source> <volume>82</volume>, <fpage>241</fpage>–<lpage>247</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Malissen</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Bongrand</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>Early T cell activation: Integrating biochemical, structural, and biophysical cues</article-title>. <source>Annual review of immunology</source> <volume>33</volume>, <fpage>539</fpage>–<lpage>561</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Belardi</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Son</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Felce</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Dustin</surname>, <given-names>M. L.</given-names></string-name> &amp; <string-name><surname>Fletcher</surname>, <given-names>D. A</given-names></string-name></person-group>. <article-title>Cell–cell interfaces as specialized compartments directing cell function</article-title>. <source>Nature Reviews Molecular Cell Biology</source> <volume>21</volume>, <fpage>750</fpage>–<lpage>764</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kiesgen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Messinger</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Chintala</surname>, <given-names>N. K.</given-names></string-name>, <string-name><surname>Tano</surname>, <given-names>Z.</given-names></string-name> &amp; <string-name><surname>Adusumilli</surname>, <given-names>P. S</given-names></string-name></person-group>. <article-title>Comparative analysis of assays to measure CAR T-cell-mediated cytotoxicity</article-title>. <source>Nature Protocols</source> <volume>16</volume>, <fpage>1331</fpage>–<lpage>1342</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ginhoux</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Yalin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dutertre</surname>, <given-names>C. A.</given-names></string-name> &amp; <string-name><surname>Amit</surname>, <given-names>I</given-names></string-name></person-group>. <article-title>Single-cell immunology: Past, present, and future</article-title>. <source>Immunity</source> <volume>55</volume>, <fpage>393</fpage>–<lpage>404</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Limozin</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Puech</surname>, <given-names>P.-H</given-names></string-name></person-group>. <article-title>Membrane Organization and Physical Regulation of Lymphocyte Antigen Receptors: A Biophysicist’s Perspective</article-title>. <source>The Journal of Membrane Biology</source> <volume>252</volume>, <fpage>397</fpage>–<lpage>412</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bongrand</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>Understanding How Cells Probe the World: A Preliminary Step towards Modeling Cell Behavior?</article-title> <source>International Journal of Molecular Sciences</source> <volume>24</volume>, <fpage>2266</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sengupta</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Dillard</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Limozin</surname>, <given-names>L</given-names></string-name></person-group>. <article-title>Morphodynamics of T-lymphocytes: Scanning to spreading</article-title>. <source>Biophysical Journal</source> <fpage>S0006349524001577</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eulenberg</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Reconstructing cell cycle and disease progression using deep learning</article-title>. <source>Nature Communications</source> <volume>8</volume>, <fpage>463</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Shetab</given-names> <surname>Boushehri</surname></string-name>, S., <etal>et al.</etal></person-group> <article-title>Explainable machine learning for profiling the immunological synapse and functional characterization of therapeutic antibodies</article-title>. <source>Nature Communications</source> <volume>14</volume>, <fpage>7888</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>German</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Morphological profiling of human T and NK lymphocytes by high-content cell imaging</article-title>. <source>Cell Reports</source> <volume>36</volume>, <fpage>109318</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chandrasekaran</surname>, <given-names>S. N.</given-names></string-name>, <string-name><surname>Ceulemans</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Boyd</surname>, <given-names>J. D.</given-names></string-name> &amp; <string-name><surname>Carpenter</surname>, <given-names>A. E</given-names></string-name></person-group>. <article-title>Image-based profiling for drug discovery: Due for a machine-learning upgrade?</article-title> <source>Nature Reviews Drug Discovery</source> <volume>20</volume>, <fpage>145</fpage>–<lpage>159</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olofsson</surname>, <given-names>P. E.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Distinct Migration and Contact Dynamics of Resting and IL-2-Activated Human Natural Killer Cells</article-title>. <source>Frontiers in Immunology</source> <volume>5</volume> (<year>2014</year>).</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lu</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>TIMING 2.0: High-throughput single-cell profiling of dynamic cell–cell interactions by time-lapse imaging microscopy in nanowell grids</article-title>. <source>Bioinformatics</source> <volume>35</volume>, <fpage>706</fpage>–<lpage>708</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alieva</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wezenaar</surname>, <given-names>A. K. L.</given-names></string-name>, <string-name><surname>Wehrens</surname>, <given-names>E. J.</given-names></string-name> &amp; <string-name><surname>Rios</surname>, <given-names>A. C</given-names></string-name></person-group>. <article-title>Bridging live-cell imaging and next-generation cancer treatment</article-title>. <source>Nature Reviews Cancer</source> (<year>2023</year>).</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vanherberghen</surname>, <given-names>B.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Classification of human natural killer cells based on migration behavior and cytotoxic response</article-title>. <source>Blood</source> <volume>121</volume>, <fpage>1326</fpage>–<lpage>1334</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wiggins</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The CellPhe toolkit for cell phenotyping using time-lapse imaging and pattern recognition</article-title>. <source>Nature Communications</source> <volume>14</volume>, <fpage>1854</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Salles</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Barcoding T Cell Calcium Response Diversity with Methods for Automated and Accurate Analysis of Cell Signals (MAAACS)</article-title>. <source>PLoS Computational Biology</source> <volume>9</volume>, <fpage>e1003245</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>This</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Costantino</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Melichar</surname>, <given-names>H. J</given-names></string-name></person-group>. <article-title>Machine learning predictions of T cell antigen specificity from intracellular calcium dynamics</article-title>. <source>Science Advances</source> <volume>10</volume>, <fpage>eadk2298</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sità</surname>, <given-names>L.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>A deep-learning approach for online cell identification and trace extraction in functional two-photon calcium imaging</article-title>. <source>Nature Communications</source> <volume>13</volume>, <fpage>1529</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Deep-learning-based three-dimensional label-free tracking and analysis of immunological synapses of CAR-T cells</article-title>. <source>eLife</source> <volume>9</volume>, <elocation-id>e49023</elocation-id> (<year>2020</year>).</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ichise</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Functional visualization of NK cell-mediated killing of metastatic single tumor cells</article-title>. <source>eLife</source> <volume>11</volume>, <elocation-id>e76269</elocation-id> (<year>2022</year>).</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>B.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Deep Learning–Based 3D Single-Cell Imaging Analysis Pipeline Enables Quantification of Cell–Cell Interaction Dynamics in the Tumor Microenvironment</article-title>. <source>Cancer Research</source> <volume>84</volume>, <fpage>517</fpage>–<lpage>526</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dekkers</surname>, <given-names>J. F.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Uncovering the mode of action of engineered T cells in patient cancer organoids</article-title>. <source>Nature Biotechnology</source> <volume>41</volume>, <fpage>60</fpage>–<lpage>69</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roudot</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>U-track3D: Measuring, navigating, and validating dense particle trajectories in three dimensions</article-title>. <source>Cell Reports Methods</source> <volume>3</volume>, <fpage>100655</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Berg</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Ilastik: Interactive machine learning for (bio)image analysis</article-title>. <source>Nature Methods</source> <volume>16</volume>, <fpage>1226</fpage>–<lpage>1232</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moen</surname>, <given-names>E.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Deep learning for cellular image analysis</article-title>. <source>Nature Methods</source> <volume>16</volume>, <fpage>1233</fpage>–<lpage>1246</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arzt</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>LABKIT: Labeling and Segmentation Toolkit for Big Image Data</article-title>. <source>Frontiers in Computer Science</source> <volume>4</volume>, <fpage>777728</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jan</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Spangaro</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Lenartowicz</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Mattiazzi Usaj</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>From pixels to insights: Machine learning and deep learning for bioimage analysis</article-title>. <source>BioEssays</source> <volume>46</volume>, <fpage>2300114</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shroff</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Testa</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Jug</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Manley</surname>, <given-names>S</given-names></string-name></person-group>. <article-title>Live-cell imaging powered by computation</article-title>. <source>Nature Reviews Molecular Cell Biology</source> (<year>2024</year>).</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vicar</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Cell segmentation methods for label-free contrast microscopy: Review and comprehensive comparison</article-title>. <source>BMC Bioinformatics</source> <volume>20</volume>, <issue>360</issue> (<year>2019</year>).</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Falk</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>U-Net: Deep learning for cell counting, detection, and morphometry</article-title>. <source>Nature Methods</source> <volume>16</volume>, <fpage>67</fpage>–<lpage>70</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Weigert</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Haase</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sugawara</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Myers</surname>, <given-names>G.</given-names></string-name></person-group> <source>Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy</source>, <fpage>3655</fpage>–<lpage>3662</lpage> (<publisher-name>IEEE, Snowmass Village, CO</publisher-name>, <publisher-loc>USA</publisher-loc>, <year>2020</year>).</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stringer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Michaelos</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Pachitariu</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Cellpose: A generalist algorithm for cellular segmentation</article-title>. <source>Nature Methods</source> <volume>18</volume>, <fpage>100</fpage>–<lpage>106</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Stringer</surname>, <given-names>C</given-names></string-name></person-group>. <article-title>Cellpose 2.0: How to train your own model</article-title>. <source>Nature Methods</source> <volume>19</volume>, <fpage>1634</fpage>–<lpage>1641</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greenwald</surname>, <given-names>N. F.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning</article-title>. <source>Nature Biotechnology</source> <volume>40</volume>, <fpage>555</fpage>–<lpage>565</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Edlund</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>LIVECell—A large-scale dataset for label-free live cell segmentation</article-title>. <source>Nature Methods</source> <volume>18</volume>, <fpage>1038</fpage>–<lpage>1045</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wen</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>3DeeCellTracker, a deep learning-based pipeline for segmenting and tracking cells in 3D time lapse images</article-title>. <source>eLife</source> <volume>10</volume>, <elocation-id>e59187</elocation-id> (<year>2021</year>).</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ulicna</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Vallardi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Charras</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Lowe</surname>, <given-names>A. R</given-names></string-name></person-group>. <article-title>Automated deep lineage tree analysis using a Bayesian single cell tracking approach</article-title>. <source>Frontiers in Computer Science</source> <volume>3</volume>, <fpage>734559</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ershov</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>TrackMate 7: Integrating state-of-the-art segmentation algorithms into tracking pipelines</article-title>. <source>Nature Methods</source> (<year>2022</year>).</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cuny</surname>, <given-names>A. P.</given-names></string-name>, <string-name><surname>Ponti</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kündig</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Rudolf</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Stelling</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Cell region fingerprints enable highly precise single-cell tracking and lineage reconstruction</article-title>. <source>Nature Methods</source> <volume>19</volume>, <fpage>1276</fpage>–<lpage>1285</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Soelistyo</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Ulicna</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Lowe</surname>, <given-names>A. R</given-names></string-name></person-group>. <article-title>Machine learning enhanced cell tracking</article-title>. <source>Frontiers in Bioinformatics</source> <volume>3</volume>, <issue>1228989</issue> (<year>2023</year>).</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ollion</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Maliet</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Giuglaris</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Vacher</surname>, <given-names>É.</given-names></string-name> &amp; <string-name><surname>Deforet</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>DiSTNet2D: Leveraging Long-Range Temporal Information for Efficient Segmentation and Tracking</article-title>. <source>PRX Life</source> <volume>2</volume>, <fpage>023004</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lugagne</surname>, <given-names>J.-B.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Dunlop</surname>, <given-names>M. J.</given-names></string-name></person-group> <article-title>DeLTA: Automated cell segmentation, tracking, and lineage reconstruction using deep learning</article-title>. <source>PLOS Computational Biology</source> <volume>16</volume>, <fpage>e1007673</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Padovani</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Mairhörmann</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Falter-Braun</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Lengefeld</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Schmoller</surname>, <given-names>K. M.</given-names></string-name></person-group> <article-title>Segmentation, tracking and cell cycle analysis of live-cell imaging data with Cell-ACDC</article-title>. <source>BMC Biology</source> <volume>20</volume>, <fpage>174</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Schwartz</surname>, <given-names>M. S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Caliban: Accurate cell tracking and lineage construction in live-cell imaging experiments with deep learning</article-title> <source>bioRxiv</source> (<year>2023</year>).</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schindelin</surname>, <given-names>J.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Fiji: An open-source platform for biological-image analysis</article-title>. <source>Nature Methods</source> <volume>9</volume>, <fpage>676</fpage>–<lpage>682</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Chaumont</surname>, <given-names>F.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Icy: An open bioimage informatics platform for extended reproducible research</article-title>. <source>Nature Methods</source> <volume>9</volume>, <fpage>690</fpage>–<lpage>696</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McQuin</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>CellProfiler 3.0: Next-generation image processing for biology</article-title>. <source>PLOS Biology</source> <volume>16</volume>, <fpage>e2005970</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gómez-de-Mariscal</surname>, <given-names>E.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>DeepImageJ: A user-friendly environment to run deep learning models in ImageJ</article-title>. <source>Nature Methods</source> <volume>18</volume>, <fpage>1192</fpage>–<lpage>1195</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stirling</surname>, <given-names>D. R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>CellProfiler 4: Improvements in speed, utility and usability</article-title>. <source>BMC Bioinformatics</source> <volume>22</volume>, <fpage>433</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Körber</surname>, <given-names>N.</given-names></string-name></person-group> <article-title>MIA is an open-source standalone deep learning application for microscopic image analysis</article-title>. <source>Cell Reports Methods</source> <volume>3</volume>, <fpage>100517</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Windhager</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>An end-to-end workflow for multiplexed image processing and analysis</article-title>. <source>Nature Protocols</source> (<year>2023</year>).</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>von Chamier</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Democratising deep learning for microscopy with ZeroCostDL4Mic</article-title>. <source>Nature Communications</source> <volume>12</volume>, <fpage>2276</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Edelstein</surname>, <given-names>A. D.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Advanced methods of microscope control using <italic>µ</italic>Manager software</article-title>. <source>Journal of Biological Methods</source> <volume>1</volume>, <fpage>10</fpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gonzalez</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Nanobody-CD16 Catch Bond Reveals NK Cell Mechanosensitivity</article-title>. <source>Biophysical Journal</source> <volume>116</volume>, <fpage>1516</fpage>–<lpage>1526</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c59"><label>[59]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Robert</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Limozin</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Van Der Merwe</surname>, <given-names>P. A.</given-names></string-name> &amp; <string-name><surname>Bongrand</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>CD8 Co-Receptor Enhances T-Cell Activation without Any Effect on Initial Attachment</article-title>. <source>Cells</source> <volume>10</volume>, <fpage>429</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c60"><label>[60]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turini</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Chames</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Bruhns</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Baty</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Kerfelec</surname>, <given-names>B</given-names></string-name></person-group>. <article-title>A FcgammaRIII-engaging bispecific antibody expands the range of HER2-expressing breast tumors eligible to antibody therapy</article-title>. <source>Oncotarget</source> <volume>5</volume>, <fpage>5304</fpage>–<lpage>5319</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c61"><label>[61]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Limozin</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Sengupta</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>Quantitative Reflection Interference Contrast Microscopy (RICM) in Soft Matter and Cell Adhesion</article-title>. <source>ChemPhysChem</source> <volume>10</volume>, <fpage>2752</fpage>–<lpage>2768</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c62"><label>[62]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sengupta</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Aranda-Espinoza</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Janmey</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Hammer</surname>, <given-names>D</given-names></string-name></person-group>. <article-title>Spreading of Neutrophils: From Activation to Migration</article-title>. <source>Biophysical Journal</source> <volume>91</volume>, <fpage>4638</fpage>–<lpage>4648</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c63"><label>[63]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dillard</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Varma</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sengupta</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Limozin</surname>, <given-names>L</given-names></string-name></person-group>. <article-title>Ligand-Mediated Friction Determines Morphodynamics of Spreading T Cells</article-title>. <source>Biophysical Journal</source> <volume>107</volume>, <fpage>2629</fpage>–<lpage>2638</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c64"><label>[64]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Pang</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Roy</surname>, <given-names>T. K.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>X.</given-names></string-name> &amp; <string-name><surname>Tan</surname>, <given-names>K.</given-names></string-name></person-group> <article-title>CelloType: A Unified Model for Segmentation and Classification of Tissue Images</article-title> <source>bioRxiv</source> (<year>2024</year>).</mixed-citation></ref>
<ref id="c65"><label>[65]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Juppet</surname>, <given-names>Q.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Deep Learning Enables Individual Xenograft Cell Classification in Histological Images by Analysis of Contextual Features</article-title>. <source>Journal of Mammary Gland Biology and Neoplasia</source> <volume>26</volume>, <fpage>101</fpage>–<lpage>112</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c66"><label>[66]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Gonzalez Gutierrez</surname>, <given-names>C.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Decoupling individual host response and immune cell engager cytotoxic potency</article-title> <source>bioRxiv</source> (<year>2024</year>).</mixed-citation></ref>
<ref id="c67"><label>[67]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Versaevel</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Grevesse</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Gabriele</surname>, <given-names>S</given-names></string-name></person-group>. <article-title>Spatial coordination between cell and nuclear shape within micropatterned endothelial cells</article-title>. <source>Nature Communications</source> <volume>3</volume> (<year>2012</year>).</mixed-citation></ref>
<ref id="c68"><label>[68]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brodovitch</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>T lymphocytes need less than 3 min to discriminate between peptide MHCs with similar TCR-binding parameters: Cellular immune response</article-title>. <source>European Journal of Immunology</source> <volume>45</volume>, <fpage>1635</fpage>–<lpage>1642</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c69"><label>[69]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Israel</surname>, <given-names>U.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A Foundation Model for Cell Segmentation</article-title>, <source>bioRxiv</source> (<year>2023</year>).</mixed-citation></ref>
<ref id="c70"><label>[70]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ma</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The multimodality cell segmentation challenge: Toward universal solutions</article-title>. <source>Nature Methods</source> <volume>21</volume>, <fpage>1103</fpage>–<lpage>1113</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c71"><label>[71]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Ravi</surname>, <given-names>N.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>SAM 2: Segment Anything in Images and Videos</article-title> (<year>2024</year>). <source>arXiv</source>.</mixed-citation></ref>
<ref id="c72"><label>[72]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Even-Desrumeaux</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Masked Selection: A Straightforward and Flexible Approach for the Selection of Binders Against Specific Epitopes and Differentially Expressed Proteins by Phage Display</article-title>. <source>Molecular &amp; Cellular Proteomics</source> <volume>13</volume>, <fpage>653</fpage>– <lpage>665</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c73"><label>[73]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rozan</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Single-Domain Antibody-Based and Linker-Free Bispecific Antibodies Targeting Fc RIII Induce Potent Antitumor Activity without Recruiting Regulatory T Cells</article-title>. <source>Molecular Cancer Therapeutics</source> <volume>12</volume>, <fpage>1481</fpage>–<lpage>1491</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c74"><label>[74]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raynaud</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Anti-NKG2D single domain-based antibodies for the modulation of anti-tumor immune response</article-title>. <source>OncoImmunology</source> <volume>10</volume>, <fpage>1854529</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c75"><label>[75]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barshir</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The TissueNet database of human tissue protein–protein interactions</article-title>. <source>Nucleic Acids Research</source> <volume>41</volume>, <fpage>D841</fpage>–<lpage>D844</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c76"><label>[76]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Caicedo</surname>, <given-names>J. C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Nucleus segmentation across imaging experiments: The 2018 Data Science Bowl</article-title>. <source>Nature Methods</source> <volume>16</volume>, <fpage>1247</fpage>–<lpage>1253</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c77"><label>[77]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kumar</surname>, <given-names>N.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A Multi-Organ Nucleus Segmentation Challenge</article-title>. <source>IEEE Transactions on Medical Imaging</source> <volume>39</volume>, <fpage>1380</fpage>–<lpage>1391</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c78"><label>[78]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Naylor</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Lae</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Reyal</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Walter</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>Segmentation of Nuclei in Histopathology Images by Deep Regression of the Distance Map</article-title>. <source>IEEE Transactions on Medical Imaging</source> <volume>38</volume>, <fpage>448</fpage>–<lpage>459</lpage> (<year>2019</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105302.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Koo</surname>
<given-names>Peter</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Cold Spring Harbor Laboratory</institution>
</institution-wrap>
<city>Cold Spring Harbor</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>CellDetective is a <bold>useful</bold> software package for segmentation, tracking, and analysis of time‐lapse microscopy datasets, specifically designed to be accessible to researchers without coding expertise. The authors provide <bold>solid</bold> evidence of its capabilities through comprehensive validations and well‐executed comparisons across immunological assays. However, the current implementation is limited to 2D widefield imaging and presents technical challenges - including occasional crashes, restricted flexibility in defining multiple cell populations, and some interface issues that hinder the full user experience. Overall, this work will be of significant interest to the bioimaging community, especially those in immunology and cell biology, and promises to evolve into a more robust tool with further development.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105302.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this manuscript, Torro et al. presented CellDetective, an open-source software designed for a user-friendly execution of single-cell segmentation, tracking, and analysis of time-lapse microscopy data. The authors demonstrated the applications of the software by measuring NK cell spreading events acquired with reflection interference contrast microscopy (RICM), as well as detecting target cell death events and their interaction with neighboring NK cells in a multichannel widefield microscopy dataset.</p>
<p>Strengths:</p>
<p>The segmentation (StarDist, Cellpose) and tracking (bTrack) modules implemented were based on existing and published software packages. The authors added the event detection, classification, and analysis modules to enable an end-to-end time-lapse microscopy data processing and analysis pipeline, complete with a graphical user interface (GUI). This minimizes the coding experience required from the user. The documentation that accompanies CellDetective is also adequate.</p>
<p>Weaknesses:</p>
<p>Given that the software was designed to improve user experience, such an approach also limits its scope and functionality and is currently capable of handling very specific types of experiments. Additionally, this reviewer has also encountered many technical difficulties (see documented bugs/crashes below) that have prevented an extensive exploration of all the functionality of CellDetective.</p>
<p>Specifics:</p>
<p>(1) The software can only handle 2D 'widefield' time-lapse imaging datasets. It should be noted that many studies that examine cell-cell interactions in vitro also used confocal microscopy and acquired the time-lapse images in 3D z-stacks to enable the reconstruction of entire cell volumes from multiple optical sections along the z-axis.</p>
<p>Given that almost all of the implemented segmentation (StarDist, Cellpose) and tracking (bTrack) packages already support the handling of 3D datasets, it is unclear why CellDetective was designed to only work with 2D datasets.</p>
<p>As noted above, extending the support for 3D images would allow the scope and utility of this software to be further extended for imaging studies acquired in z-stacks. As an example, the dense clustering of effector cells in Figure 4 had prevented accurate segmentation due to the 2D nature of the experimental dataset. More importantly, support for a 3D dataset could also allow for the tracking of fluorescent protein-based sub-cellular as well as membrane protein localization during cell-cell interactions.</p>
<p>Furthermore, it also widens the potential applicability for analyzing datasets from 3D organoid imaging and perhaps even intravital two-photon microscopy.</p>
<p>(2) The software in its current form only allows the broad demarcation of the cells examined into two populations: targets and effectors. This limits the number of cell populations that can be examined for their interactions. It might be more useful to just allow multiple user-defined populations instead of restricting the populations to target and effector cells only.</p>
<p>(3) Similarly, subsetting of each of the populations could be made more intuitive. Although it is possible to define subsets of cells using the &quot;Custom classification&quot; function under the &quot;Measure&quot; module with user-defined parameters, visualization of multiple groups remains unintuitive and it appears that only one custom classified group can be selected and visualized at any given time in the Signal Annotator under Measurement instead of allowing visualization of multiple (custom defined) groups of cells in different colors. It is also unclear how, if possible at all, to visualize a custom group of cells in the Signal Annotator under the Detect Events module.</p>
<p>Software issues:</p>
<p>(4) When initially tested on v1.3.9, the Segment module could not be initiated (with the error message AttributeError: 'WindowsPath' object has no attribute 'endswith' when attempting to run segmentation).</p>
<p>
Update: this has been fixed in v1.3.9.post4 dated February 7th, 2025.</p>
<p>(5) Further testing was then performed by downgrading the software to v1.3.1. While testing the ADCC demo experiment (<ext-link ext-link-type="uri" xlink:href="https://celldetective.readthedocs.io/en/latest/adcc-example.html">https://celldetective.readthedocs.io/en/latest/adcc-example.html</ext-link>), the workflow was stuck at attempts to initiate the Detect Events step:</p>
<p>AssertionError: No signal matches with the requirements of the model ['dead_nuclei_channel_mean', 'area']. Please pass the signals manually with the argument selected_signals or add measurements. Abort.</p>
<p>(Update: fixed in the latest v1.3.9.post4 version dated February 7th, 2025)</p>
<p>(6) Random bugs causing the software to crash. Example: switching characteristic to 'status_color' in the Signal Annotator under Measurement caused the software to crash (v1.3.9.post4):</p>
<p>TypeError: ufunc 'isnan' is not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule 'safe'</p>
<p>(7) Overall, when exploring the functionality of the software, there have been multiple instances of software crashes when clicking/switching around to show different parameters, etc.</p>
<p>This reviewer understands the difficulties and time involved in bug fixing and hopes that the experience could have been much smoother and that the software behaves much more stably in order to maximize its useability.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105302.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Immune assays enable the analysis of immune responses in vitro. These assays generate time series image data across several experimental conditions. The imaging parameters such as the imaging modality and the number of channels can vary across experiments. A challenge in the field is the lack of (open source) tools to process and analyze these data. R. Torro, et. al. developed an open source end-to-end pipeline for the analysis of image data from these immune assays. The pipeline is designed with a GUI and is suited for experimental biologists with no coding experience. The authors have incorporated several existing methods and tools for individual tasks such as for segmentation and cell tracking, and incorporated them with custom methods where necessary such as for tracking cell state transitions.</p>
<p>Strengths:</p>
<p>(1) The tool is extremely well-documented and easy to install.</p>
<p>(2) Applicable to a wide variety of imaging modalities and analysis.</p>
<p>(3) There are several different options for each step, such as segmentation using traditional methods or deep learning methods, and all the analysis steps are integrated in one place with a GUI. The no-coding requirement makes this a very powerful tool for biologists and has the potential to enable a wide variety of analyses.</p>
<p>Weakness:</p>
<p>(1) It would be good to provide documentation on how to make the tool applicable for applications and analysis other than for immune profiling since most methods integrated here are applicable well beyond immune profiling. For example, a user might want to use the tool just for the segmentation of their IF microscopy-images.</p>
<p>(2) They applied Celldetective to two immune assays. The authors present the results from these assays and use the results to validate their assay. However, they have not included data that demonstrates results obtained via this pipeline are comparable to results obtained with other pipelines and/or if these results are consistent with what is expected in the literature.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105302.1.sa3</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Torro</surname>
<given-names>Rémy</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Díaz-Bello</surname>
<given-names>Beatriz</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>El Arawi</surname>
<given-names>Dalia</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dervanova</surname>
<given-names>Ksenija</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ammer</surname>
<given-names>Lorna</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dupuy</surname>
<given-names>Florian</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chames</surname>
<given-names>Patrick</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6104-6286</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Sengupta</surname>
<given-names>Kheya</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1060-2713</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Limozin</surname>
<given-names>Laurent</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9523-8086</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>In view of the suggestions of the referees, we wish to underline that a user can interact with <italic>celldetective</italic> at two levels: a non-coder can analyse data and train models without coding, but is necessarily offered pre-determined choices and flexibility. An advanced user however has practically limitless flexibility to extend the fully-open source <italic>celldetective,</italic> aided by its modularity and detailed manual.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Summary:</p>
<p>In this manuscript, Torro et al. presented CellDetective, an open-source software designed for a user-friendly execution of single-cell segmentation, tracking, and analysis of time-lapse microscopy data. The authors demonstrated the applications of the software by measuring NK cell spreading events acquired with reflection interference contrast microscopy (RICM), as well as detecting target cell death events and their interaction with neighboring NK cells in a multichannel widefield microscopy dataset.</p>
<p>Strengths:</p>
<p>The segmentation (StarDist, Cellpose) and tracking (bTrack) modules implemented were based on existing and published software packages. The authors added the event detection, classification, and analysis modules to enable an end-to-end time-lapse microscopy data processing and analysis pipeline, complete with a graphical user interface (GUI). This minimizes the coding experience required from the user. The documentation that accompanies CellDetective is also adequate.</p>
<p>Weaknesses:</p>
<p>Given that the software was designed to improve user experience, such an approach also limits its scope and functionality and is currently capable of handling very specific types of experiments. Additionally, this reviewer has also encountered many technical difficulties (see documented bugs/crashes below) that have prevented an extensive exploration of all the functionality of CellDetective.</p>
</disp-quote>
<p>We apologize for the technical difficulties and bugs; the ones mentioned have been already corrected. New users have also tested the installation and reported it to be bug-free.</p>
<p>We fully agree on the compromise that has to be found between user experience and versatility. We have already tested celldetective in other biological contexts, such as microbiology, but made a choice to showcase it in the article for immunological applications. We invite the reader to consult the software documentation and online examples to learn about more options.</p>
<disp-quote content-type="editor-comment">
<p>Specifics:</p>
<p>(1) The software can only handle 2D 'widefield' time-lapse imaging datasets. It should be noted that many studies that examine cell-cell interactions in vitro also used confocal microscopy and acquired the time-lapse images in 3D z-stacks to enable the reconstruction of entire cell volumes from multiple optical sections along the z-axis.</p>
<p>Given that almost all of the implemented segmentation (StarDist, Cellpose) and tracking (bTrack) packages already support the handling of 3D datasets, it is unclear why CellDetective was designed to only work with 2D datasets.</p>
<p>As noted above, extending the support for 3D images would allow the scope and utility of this software to be further extended for imaging studies acquired in z-stacks. As an example, the dense clustering of effector cells in Figure 4 had prevented accurate segmentation due to the 2D nature of the experimental dataset. More importantly, support for a 3D dataset could also allow for the tracking of fluorescent protein-based sub-cellular as well as membrane protein localization during cell-cell interactions.</p>
<p>Furthermore, it also widens the potential applicability for analyzing datasets from 3D organoid imaging and perhaps even intravital two-photon microscopy.</p>
</disp-quote>
<p>We thank the reviewer for this suggestion. Indeed, extension to 3-dimensions is a natural development, since we have chosen segmentation and tracking methods which are compatible with 3D. However, two important strengths of <italic>celldetective</italic> are: harnessing statistical power of cell populations together with multiplexing biological conditions, and dynamic analysis of fast events.</p>
<p>For both, 2D is advantageous. Our own focus is on analyzing cellular events with minute time resolution, relevant in immunology. By our estimate (experience and literature), 3D timelapse acquisition would reduce the time resolution, as well as throughput (in terms of events and conditions) to below acceptable level. While we don’t envisage this upgrade in the immediate future, we encourage advanced users to contribute to further develop the open-source code in this direction. As a mitigation solution, a 2.5D approach on a flat sample by combining two z planes (in order to address issues of cell superposition for example), could be readily implemented with minimal change.</p>
<disp-quote content-type="editor-comment">
<p>(2) The software in its current form only allows the broad demarcation of the cells examined into two populations: targets and effectors. This limits the number of cell populations that can be examined for their interactions. It might be more useful to just allow multiple user-defined populations instead of restricting the populations to target and effector cells only.</p>
</disp-quote>
<p>We thank the reviewer for this suggestion. There is little architectural limitation to its implementation; this will be proposed in the future version. This updated version will allow more than two user-defined populations, labelled directly by the user, which will also facilitate the natural extension to more varied biological applications. Three-way interactions are much more complex, and, to our knowledge, not currently addressed by biologists. The interactions will for the moment be limited to 2 populations interactions, as multipartite ones involve a higher level of code modifications, not immediately envisaged.</p>
<disp-quote content-type="editor-comment">
<p>(3) Similarly, subsetting of each of the populations could be made more intuitive. Although it is possible to define subsets of cells using the &quot;Custom classification&quot; function under the &quot;Measure&quot; module with user-defined parameters, visualization of multiple groups remains unintuitive and it appears that only one custom classified group can be selected and visualized at any given time in the Signal Annotator under Measurement instead of allowing visualization of multiple (custom defined) groups of cells in different colors. It is also unclear how, if possible at all, to visualize a custom group of cells in the Signal Annotator under the Detect Events module.</p>
</disp-quote>
<p>The simultaneous visualization of several classes poses problems in the choice of colors and symbols, and may render the tool difficult to use. The time propagation option in the classification tool allows to define event classes as opposed to groups, that are compatible with the Signal Annotator. For more complex classifications, a simple solution is to work with composite classifications, which are already supported by using logical AND/OR operators on the condition defining the class. We believe that this feature is sufficient to address this issue.</p>
<disp-quote content-type="editor-comment">
<p>Software issues:</p>
<p>(4) When initially tested on v1.3.9, the Segment module could not be initiated (with the error message AttributeError: 'WindowsPath' object has no attribute 'endswith' when attempting to run segmentation).</p>
<p>Update: this has been fixed in v1.3.9.post4 dated February 7th, 2025.</p>
<p>(5) Further testing was then performed by downgrading the software to v1.3.1. While testing the ADCC demo experiment (<ext-link ext-link-type="uri" xlink:href="https://celldetective.readthedocs.io/en/latest/adcc-example.html">https://celldetective.readthedocs.io/en/latest/adcc-example.html</ext-link>), the workflow was stuck at attempts to initiate the Detect Events step:</p>
<p>AssertionError: No signal matches with the requirements of the model ['dead_nuclei_channel_mean', 'area']. Please pass the signals manually with the argument selected_signals or add measurements. Abort.</p>
<p>(Update: fixed in the latest v1.3.9.post4 version dated February 7th, 2025)</p>
<p>(6) Random bugs causing the software to crash. Example: switching characteristic to 'status_color' in the Signal Annotator under Measurement caused the software to crash (v1.3.9.post4):</p>
<p>TypeError: ufunc 'isnan' is not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule 'safe'</p>
<p>(7) Overall, when exploring the functionality of the software, there have been multiple instances of software crashes when clicking/switching around to show different parameters, etc.</p>
<p>This reviewer understands the difficulties and time involved in bug fixing and hopes that the experience could have been much smoother and that the software behaves much more stably in order to maximize its useability.</p>
</disp-quote>
<p>We apologize again for the various technical issues encountered during the review process, and thank the reviewer for mentioning that several bugs were already fixed in the last software release. The open source and software maintenance protocol enabled by github should help to resolve any further emerging issue.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Summary:</p>
<p>Immune assays enable the analysis of immune responses in vitro. These assays generate time series image data across several experimental conditions. The imaging parameters such as the imaging modality and the number of channels can vary across experiments. A challenge in the field is the lack of (open source) tools to process and analyze these data. R. Torro, et. al. developed an open source end-to-end pipeline for the analysis of image data from these immune assays. The pipeline is designed with a GUI and is suited for experimental biologists with no coding experience. The authors have incorporated several existing methods and tools for individual tasks such as for segmentation and cell tracking, and incorporated them with custom methods where necessary such as for tracking cell state transitions.</p>
<p>Strengths:</p>
<p>(1) The tool is extremely well-documented and easy to install.</p>
<p>(2) Applicable to a wide variety of imaging modalities and analysis.</p>
<p>(3) There are several different options for each step, such as segmentation using traditional methods or deep learning methods, and all the analysis steps are integrated in one place with a GUI. The no-coding requirement makes this a very powerful tool for biologists and has the potential to enable a wide variety of analyses.</p>
<p>Weakness:</p>
<p>(1) It would be good to provide documentation on how to make the tool applicable for applications and analysis other than for immune profiling since most methods integrated here are applicable well beyond immune profiling. For example, a user might want to use the tool just for the segmentation of their IF microscopy-images.</p>
</disp-quote>
<p>This is an important suggestion that we will implement as short demonstrations using data from the public domain. These will be proposed as examples in the online documentation.</p>
<disp-quote content-type="editor-comment">
<p>(2) They applied Celldetective to two immune assays. The authors present the results from these assays and use the results to validate their assay. However, they have not included data that demonstrates results obtained via this pipeline are comparable to results obtained with other pipelines and/or if these results are consistent with what is expected in the literature.</p>
</disp-quote>
<p>In the final version of the article, we shall compare <italic>celldetective</italic> with existing literature, including our previous work, when possible. However, we emphasize that most of the presented data are original and don’t have any published equivalent in the literature. Concerning the immunotherapy assays, data presented already show expected trends (see for example Fig. 2 and Fig. 5). We reserve for future publications the systematic comparison with traditional (non microscopy-based) methods, as we consider it out-of-scope here. Additionally, there is, to our knowledge no existing open pipeline performing the full end-to-end analysis.</p>
</body>
</sub-article>
</article>