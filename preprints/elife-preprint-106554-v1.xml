<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">106554</article-id>
<article-id pub-id-type="doi">10.7554/eLife.106554</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106554.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Enhanced Tactile Coding in Rat Neocortex Under Darkness</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Yamashiro</surname>
<given-names>Kotaro</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Tanaka</surname>
<given-names>Shiyori</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Matsumoto</surname>
<given-names>Nobuyoshi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>nobuyoshi@matsumoto.ac</email>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6434-9469</contrib-id>
<name>
<surname>Ikegaya</surname>
<given-names>Yuji</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>yuji@ikegaya.jp</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/057zh3y96</institution-id><institution>Graduate School of Pharmaceutical Sciences, The University of Tokyo</institution></institution-wrap>, <city>Tokyo</city>, <country country="JP">Japan</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/057zh3y96</institution-id><institution>Institute for AI and Beyond, The University of Tokyo</institution></institution-wrap>, <city>Tokyo</city>, <country country="JP">Japan</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016bgq349</institution-id><institution>Center for Information and Neural Networks, National Institute of Information and Communications Technology</institution></institution-wrap>, <city>Suita City</city>, <country country="JP">Japan</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Gjorgjieva</surname>
<given-names>Julijana</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Technical University of Munich</institution>
</institution-wrap>
<city>Freising</city>
<country country="DE">Germany</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Moore</surname>
<given-names>Tirin</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University, Howard Hughes Medical Institute</institution>
</institution-wrap>
<city>Stanford</city>
<country country="US">United States</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-08-05">
<day>05</day>
<month>08</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP106554</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-04-11">
<day>11</day>
<month>04</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-04-16">
<day>16</day>
<month>04</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.04.11.648316"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Yamashiro et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Yamashiro et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-106554-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Sensory systems are known for their adaptability, responding dynamically to changes in environmental conditions. A key example of this adaptability is the enhancement of tactile perception in the absence of visual input. Despite behavioral studies showing visual deprivation can improve tactile discrimination, the underlying neural mechanisms, particularly how tactile neural representations are reorganized during visual deprivation, remain unclear. In this study, we explore how the absence of visual input alters tactile neural encoding in the rat somatosensory cortex (S1). Rats were trained on a custom-designed treadmill with distinct tactile textures (rough and smooth), and local field potentials (LFPs) were recorded from S1 under light and dark conditions. Machine learning techniques, specifically a convolutional neural network, were used to decode the high-dimensional LFP signals. We found that the neural representations of tactile stimuli became more distinct in the dark, indicating a reorganization of sensory processing in S1 when visual input was removed. Notably, conventional amplitude-based analyses failed to capture these changes, highlighting the power of machine learning in uncovering subtle neural patterns. These findings offer new insights into how the brain rapidly adapts tactile processing in response to the loss of visual input, with implications for multisensory integration and potential strategies for sensory rehabilitation.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The primary somatosensory cortex (S1) is integral to the encoding of tactile information, processing sensory inputs from various regions of the body (<xref ref-type="bibr" rid="c10">Delhaye et al., 2018</xref>; <xref ref-type="bibr" rid="c46">Serino, 2019</xref>; <xref ref-type="bibr" rid="c12">Di Plinio et al., 2020</xref>; <xref ref-type="bibr" rid="c37">Piras et al., 2020</xref>). This cortical area is crucial for our sense of touch, facilitating the perception and interpretation of sensations such as pressure, vibration, temperature, and pain (<xref ref-type="bibr" rid="c8">Bushnell et al., 1999</xref>; <xref ref-type="bibr" rid="c28">Luna et al., 2005</xref>; <xref ref-type="bibr" rid="c30">Moulton et al., 2012</xref>). These external stimuli are represented as neural activity in S1, enabling animals to differentiate among distinct sensory experiences (<xref ref-type="bibr" rid="c24">Koch and Fuster, 1989</xref>; <xref ref-type="bibr" rid="c42">Salinas et al., 2000</xref>; <xref ref-type="bibr" rid="c19">Goodwin and Wheat, 2004</xref>; <xref ref-type="bibr" rid="c2">Bensmaia et al., 2008</xref>). However, the role of S1 extends beyond the mere reception of tactile signals. The neural representations within S1 are characterized by dynamic and flexible properties, shaped not only by feedforward mechanisms but also by factors such as attention, motivation, and cross-modal influences (<xref ref-type="bibr" rid="c13">Driver and Spence, 2000</xref>; <xref ref-type="bibr" rid="c14">Eimer and Forster, 2003</xref>; <xref ref-type="bibr" rid="c45">Schürmann et al., 2004</xref>; <xref ref-type="bibr" rid="c9">Butler et al., 2012</xref>; <xref ref-type="bibr" rid="c64">Ziegler et al., 2023</xref>). Such plasticity allows for adaptive responses to an ever-changing environment, which is critical for survival (<xref ref-type="bibr" rid="c11">Dijkerman and de Haan, 2007</xref>; <xref ref-type="bibr" rid="c1">Abraira and Ginty, 2013</xref>). A key manifestation of this adaptability is cross-modal processing, wherein inputs from other sensory modalities, such as visual or auditory information, can modulate tactile perception (<xref ref-type="bibr" rid="c21">Hopkins et al., 2017</xref>; <xref ref-type="bibr" rid="c31">Nikbakht et al., 2018</xref>; <xref ref-type="bibr" rid="c49">Sugiyama et al., 2019</xref>; <xref ref-type="bibr" rid="c6">Bulusu and Lazar, 2024</xref>). These findings suggest that S1 is not exclusively dedicated to tactile processing, but instead integrates and adapts sensory information from multiple modalities.</p>
<p>An example of cross-modal processing between S1 and other sensory modalities is the enhancement of tactile abilities in the absence or reduction of visual input. Blind individuals, particularly those who have been blind from an early age, consistently outperform sighted individuals on tasks requiring tactile discrimination, such as two-point discrimination and texture identification (<xref ref-type="bibr" rid="c54">Van Boven et al., 2000</xref>; <xref ref-type="bibr" rid="c18">Goldreich and Kanics, 2003</xref>; <xref ref-type="bibr" rid="c32">Norman and Bartholomew, 2011</xref>; <xref ref-type="bibr" rid="c55">Wong et al., 2011</xref>). This phenomenon is caused by repurposing of cortical areas normally devoted to vision or other senses to enhance processing of touch and other modalities (<xref ref-type="bibr" rid="c39">Rauschecker et al., 1992</xref>; <xref ref-type="bibr" rid="c34">Pascual-Leone and Torres, 1993</xref>; <xref ref-type="bibr" rid="c41">Sadato et al., 1996</xref>; <xref ref-type="bibr" rid="c7">Burton, 2003</xref>; <xref ref-type="bibr" rid="c43">Sathian and Stilla, 2010</xref>). Research indicates that during critical developmental periods, the visual cortex undergoes functional reorganization, and regions ordinarily involved in visual processing are repurposed to process stimuli from other sensory modalities, including tactile input (<xref ref-type="bibr" rid="c41">Sadato et al., 1996</xref>; <xref ref-type="bibr" rid="c7">Burton, 2003</xref>; <xref ref-type="bibr" rid="c22">Karlen et al., 2006</xref>). This reorganization is not immediate but rather occurs over extended periods (<xref ref-type="bibr" rid="c22">Karlen et al., 2006</xref>; <xref ref-type="bibr" rid="c36">Piché et al., 2007</xref>). This capacity for tactile enhancement results from both structural and functional plasticity, underscoring the brain’s remarkable ability to adapt and optimize sensory processing when one modality is lost or altered.</p>
<p>Interestingly, even sighted individuals can experience improvements in tactile performance when visual cues are removed or minimized, as evidenced by increased accuracy in tasks such as Braille reading under low-visibility conditions (<xref ref-type="bibr" rid="c33">Pascual-Leone and Hamilton, 2001</xref>; <xref ref-type="bibr" rid="c23">Kauffman et al., 2002</xref>; <xref ref-type="bibr" rid="c17">Facchini and Aglioti, 2003</xref>). These findings suggest that limiting or removing vision may trigger compensatory mechanisms that enhance the sense of touch, even in those with typical vision (<xref ref-type="bibr" rid="c5">Boroojerdi et al., 2000</xref>; <xref ref-type="bibr" rid="c29">Merabet et al., 2008</xref>; <xref ref-type="bibr" rid="c3">Bola et al., 2017</xref>). While much of the research on blindness focuses on the extended plasticity that occurs in response to sensory deprivation—such as the reorganization of somatosensory maps and the recruitment of visual regions for tactile processing—these changes typically unfold over long durations or during critical developmental periods. Thus, although the brain’s capacity to reorganize tactile representation in the absence of visual input is observed in both blind and sighted individuals, the underlying mechanisms behind long-term and short-term cross-modal processing are likely distinct. In blind individuals, these processes may involve more profound, lasting changes in neural circuitry, whereas in sighted individuals, the adaptations may be more transient and context-dependent. This suggests that different neural processes drive the change in somatosensory processing seen in response to visual deprivation, with the brain’s reorganization in sighted individuals occurring more rapidly and reversibly, while in blind individuals, it reflects more sustained and structural changes over time.</p>
<p>While behavioral evidence clearly enhanced tactile abilities in visually deprived individuals, the underlying neural mechanisms, particularly those occurring over short timescales in sighted individuals, remain poorly understood. How the somatosensory cortex might reorganize to enhance tactile perception within minutes or hours following visual deprivation remains unknown. fMRI studies have shown changes in cortical activity in short-term blindfolded individuals (<xref ref-type="bibr" rid="c29">Merabet et al., 2008</xref>; <xref ref-type="bibr" rid="c48">Siuda-Krzywicka et al., 2016</xref>; <xref ref-type="bibr" rid="c3">Bola et al., 2017</xref>), but how the neural representation of specific tactile stimuli is altered remains unclear. This gap in understanding arises because tactile representations in S1 are distributed across high-dimensional neural population codes, making it challenging for conventional methods to detect the subtle, stimulus-specific features embedded in these signals (<xref ref-type="bibr" rid="c40">Romo and Salinas, 2001</xref>; <xref ref-type="bibr" rid="c35">Pei et al., 2011</xref>; <xref ref-type="bibr" rid="c27">Lieber and Bensmaia, 2019</xref>). This highlights a gap in our understanding of whether—and how—S1’s population-level coding of tactile information may shift acutely when visual input is removed. Thus, more refined techniques and models are needed to investigate the immediate neural mechanisms underlying this form of cross-modal plasticity.</p>
<p>To address this gap, we focus on adult rats as a model for studying short-term cross-modal influences in S1. Rodents have well-defined and accessible somatosensory maps, enabling precise electrode placement for invasive recordings. In this study, we use local field potentials (LFPs) recorded from S1, which capture network-level dynamics not always apparent in single-unit spiking data or other imaging modalities. LFPs provide a rich, high-dimensional signal that can reflect both synchronous and asynchronous activity across cortical populations. By leveraging a machine learning framework, we can decode subtle differences in these signals—differences that might remain undetected through conventional amplitude-based analyses. Furthermore, to dissociate the effects of attention in the perception of the somatosensory stimulus (<xref ref-type="bibr" rid="c53">Tamè and Holmes, 2016</xref>), we created a custom behavioral paradigm that delivers the somatosensory stimuli to rats in the most naturalistic manner, while diverting the rat’s attention away from the stimuli (<xref ref-type="bibr" rid="c57">Yamashiro et al., 2024</xref>). This custom-designed treadmill paradigm allows us to manipulate both tactile (textured floors) and visual (light vs. dark) inputs independently, precisely controlling and isolating the contribution of visual cues to S1’s tactile coding.</p>
<p>We hypothesized that visual deprivation acutely alters the neural representation of tactile stimuli in S1, making texture-specific signals more distinct in the absence of visual input. To test this hypothesis, we analyzed high-dimensional LFP data using a convolutional neural network, a type of deep learning model, to examine how S1 encodes different floor textures under light and dark conditions. Our results reveal subtle, yet systematic shifts in neural coding that emerge in the absence of visual input. By demonstrating this rapid reorganization of tactile representations, we provide new insights into the mechanisms of cross-modal processing at short timescales. These findings hold broader implications for sensory rehabilitation and brain–machine interfaces, suggesting that leveraging the dynamic interplay between sensory modalities could lead to improved outcomes for individuals with sensory impairments.</p>
</sec>
<sec id="s2">
<title>Materials and Methods</title>
<sec id="s2a" sec-type="ethics-statement">
<title>Animal ethics</title>
<p>Animal experiments were performed with the approval of the Animal Experiment Ethics Committee at the University of Tokyo (approval numbers: P29–7 and P4–15) and according to the University of Tokyo guidelines for the care and use of laboratory animals. These experimental protocols were carried out following the Fundamental Guidelines for the Proper Conduct of Animal Experiments and Related Activities of the Academic Research Institutions (Ministry of Education, Culture, Sports, Science and Technology, Notice No. 71 of 2006), the Standards for Breeding and Housing of and Pain Alleviation for Experimental Animals (Ministry of the Environment, Notice No. 88 of 2006) and the Guidelines on the Method of Animal Disposal (Prime Minister’s Office, Notice No. 40 of 1995). While our experimental protocols have a mandate to humanely euthanize animals if they exhibit any signs of pain, prominent lethargy, and discomfort, such symptoms were not observed in any of the rats tested in this study. All efforts were made to minimize the animals’ suffering.</p>
</sec>
<sec id="s2b">
<title>Behavioral paradigm</title>
<p>To record LFPs during natural locomotion, we employed a custom-designed, disk-shaped treadmill with a diameter of 90 cm (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). The treadmill’s running surface was divided into two halves, each featuring a distinct texture: one side was coated with coarse sandpaper (grain #80) and the other with fine sandpaper (grain #1000). In this setup, the rat was placed on the treadmill and secured with a fabric vest (<xref rid="fig1" ref-type="fig">Figure 1B</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Behavioral paradigm and limb movement assessment with concurrent LFP recordings.</title>
<p>(<bold>A</bold>) A diagram of the disk-shaped treadmill used in the experiment. One half of the disk is covered with #80 sandpaper, and the other half with #2000 sandpaper. (<bold>B</bold>) A frame from the video capturing a walking rat from a left-side perspective. (<bold>C</bold>) The motivation scheme. The rats were water-deprived prior to the experiments. A water port was coupled with the movement of the treadmill so that when the rat walked on the treadmill, the water would come out. This way, the rats were always motivated to walk during the whole session. (<bold>D</bold>) The experimental protocol, where each rat walked for 10 minutes in light (50 lx) and then for 10 minutes in darkness (0 lx). (<bold>E</bold>) An example trajectory of the elbow and wrist joints from one session, plotted with the shoulder joint fixed in the coordinate space. (<bold>F</bold>) A schematic illustrating the forelimb and hindlimb subregions of S1. (<bold>G</bold>) A custom 32-channel electrode array used to record LFPs from these subregions.</p></caption>
<graphic xlink:href="648316v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Prior to the experiment, rats were water-restricted to ensure motivation. A waterspout was positioned in front of each rat, and its output was synchronized with the treadmill’s movement (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). Specifically, once the treadmill began to move, 30 µL of water was dispensed from the spout, encouraging the rat to continue walking to receive its water reward.</p>
<p>Each rat was deprived of water until its body weight reached 85% of its baseline weight, then it was trained to walk on the treadmill for four days (one hour per day), with 30 minutes in a light environment (50 lx) followed by 30 minutes in darkness (0 lx). After successful training, the rats were allowed free access to food and water to regain their original body weight prior to electrode implantation surgery. Following surgical recovery, the rats were again water-restricted and placed on the treadmill to assess cross-modal interactions between the visual and tactile systems. In a single session, LFP recordings were obtained for 10 minutes in the light environment trial, followed immediately by 10 minutes in the dark environment trial (<xref rid="fig1" ref-type="fig">Figure 1D</xref>). Recording was performed once a day and performed daily for 4-14 days.</p>
<p>During the trials, the trajectory of the forelimb and onsets when the forelimb came in contact with the floor were automatically identified using a previously developed deep-learning–based method (<xref rid="fig1" ref-type="fig">Figure 1E</xref>) (<xref ref-type="bibr" rid="c57">Yamashiro et al., 2024</xref>).</p>
</sec>
<sec id="s2c">
<title>Animal preparation and surgical procedures</title>
<p>LFPs were recorded from eleven 9-to 10-week-old Long-Evans rats (Japan SLC, Shizuoka, Japan) using a custom-designed, 32-channel electrode assembly. This assembly, fabricated from nichrome wires (761500, A-M Systems, WA, USA), targeted the right somatosensory cortex (S1) regions corresponding to the forelimb and hindlimb representations (<xref rid="fig1" ref-type="fig">Figure 1F</xref>). Specifically, 18 and 14 electrodes were placed in the forelimb and the hindlimb subregion respectively (<xref rid="fig1" ref-type="fig">Figure 1G</xref>). Each electrode tip was platinum-coated to reduce impedance to below 200 kΩ using a nanoZ tester (Plexon, TX, USA).</p>
<p>At the start of the surgical procedure, each rat was anesthetized with 2–3% isoflurane gas. A square craniotomy (2–6 mm posterior and 1–5 mm lateral to bregma) was then created using a dental drill. The electrode assembly was gently lowered through the cranial window to a depth of approximately 1.5 mm beneath the dura, targeting layer IV of S1. Additionally, two stainless steel screws were implanted in the bone above the cerebellum to serve as ground and reference electrodes. The recording device and electrodes were secured to the skull using stainless steel screws and dental cement. Following the surgery, each rat was housed individually in a transparent Plexiglas cage with ad libitum access to food and water for one week to ensure proper recovery.</p>
<p>LFP recordings from S1</p>
<p>LFPs were referenced to ground, digitized at 30 kHz using the OpenEphys recording system (<ext-link ext-link-type="uri" xlink:href="http://open-ephys.org">http://open-ephys.org</ext-link>) and an RHD 32-channel headstage (C3314, Intan Technologies, CA, USA), then downsampled to 2 kHz for subsequent analyses (<xref ref-type="bibr" rid="c56">Yamashiro et al., 2020</xref>). Concurrently, video footage was acquired at 60 Hz using a USB camera module (MCM-303NIR, Gazo, Niigata, Japan), capturing a lateral view of the rat. Each video frame was synchronized with the neural recordings via strobe signals.</p>
</sec>
<sec id="s2d">
<title>Data analysis</title>
<p>Data was analyzed offline using custom-made scripts in Python3. For box plots, the centerline shows the median, the box limits show the upper and lower quartiles, and the whiskers cover the 10−90% percentiles. <italic>P</italic> &lt; 0.05 was considered statistically significant. All statistical tests were two-sided.</p>
</sec>
<sec id="s2e">
<title>LFP analysis</title>
<p>To see if there were any differences in LFPs, the LFPs were aligned to the foot strike onsets detected using deep-learning assisted methods. The aligned LFP were then categorized by the trial (light <italic>vs.</italic> dark) and the floor texture (smooth <italic>vs.</italic> rough). To analyze the amplitude of the event-related response from the foot strike, a mean trace of the aligned LFP was calculated for each channel from 32 electrodes for each condition.</p>
</sec>
<sec id="s2f">
<title>A deep neural network for joint decoding of trial conditions and floor texture</title>
<p>A custom deep neural network (DNN) model was implemented to predict both the trial condition (<italic>e.g.</italic>, light <italic>vs.</italic> dark) and floor texture (<italic>e.g.</italic>, smooth <italic>vs</italic>. rough) from time-aligned, one-dimensional LFP segments, using the PyTorch framework. Our model architecture was inspired by one-dimensional ResNet-like structures and incorporated multiheaded outputs for simultaneous prediction of two distinct variables(<xref ref-type="bibr" rid="c20">He et al., 2015</xref>).</p>
<p>The input to the model consisted of one-dimensional LFP signals, arranged as a tensor with multiple channels (<italic>e.g.</italic>, 32 input channels) over time. The network began by splitting the input into two parallel convolutional pathways. The first pathway (“left” branch) applied sequential convolutional and pooling operations with relatively smaller kernel sizes and strides to incrementally reduce the dimensionality of the signal and extract fine-grained temporal features. Specifically, the model employed a two-stage convolutional process that first passed the input through a one-dimentional (1D) convolution layer with a kernel size of 7, stride of 2, and batch normalization, followed by a max pooling and an additional convolution layer. Both convolutional layers in the left branch used ReLU nonlinearities to facilitate stable and efficient feature extraction.</p>
<p>In contrast, the second pathway (“right” branch) processed the input through a single 1D convolutional layer with a larger kernel size (<italic>e.g.</italic>, 41) and a more aggressive stride (<italic>e.g.</italic>, stride of 8). This pathway captured broader temporal contexts from the input signals. Similar to the left branch, the right branch output was batch-normalized and passed through a ReLU activation function. After these two parallel extractions, the outputs of the left and right branches were concatenated along the channel dimension, forming a combined feature representation that integrated both fine- and coarse-grained temporal information.</p>
<p>The concatenated output was then passed through a max pooling operation, followed by two residual layers that employed 1D convolutional blocks (ResidualBlock) to refine feature representations. These residual layers allowed the network to learn more complex feature hierarchies by facilitating the flow of gradients during training and improving convergence, while also maintaining temporal resolution appropriate for downstream decoding.</p>
<p>Subsequently, the processed features were passed through an average pooling layer to summarize temporal information into a low-dimensional feature vector. This vector was flattened into a one-dimensional representation and then fed into two separate fully connected “heads” (fc_head1 and fc_head2). Each head was a simple linear layer that provided a scalar output value. By concatenating these outputs, the final layer jointly predicted two target variables from the same underlying features.</p>
<p>In summary, our model combined parallel convolutional branches for initial feature extraction, residual layers for robust representation learning, and multiheaded outputs to facilitate joint prediction of trial conditions and floor textures. The model was implemented in Python using PyTorch, and all parameters were optimized via standard stochastic gradient–based methods. This architecture allowed efficient and robust decoding of environmental conditions from LFP signals in both time and frequency domains.</p>
</sec>
<sec id="s2g">
<title>Training and evaluating the deep neural network</title>
<p>Of the 11 recorded rats, data from the initial two rats were utilized to determine the optimal model architecture. Once the architecture was established, data from the remaining 9 rats were used for training and evaluation. The raw LFP signals were downsampled to 10,000 Hz and segmented into 800 ms windows centered on the footstrike onset (<italic>e.g.</italic>, 400 ms before and 400 ms after). Each LFP segment was assigned two labels: one for the trial condition (<italic>e.g.</italic>, light <italic>vs</italic>. dark) and one for the floor texture (<italic>e.g.</italic>, smooth <italic>vs</italic>. rough).</p>
<p>For each rat, the dataset was shuffled, normalized, and then split into training (80%) and evaluation (20%) subsets. Using the training dataset, model training was performed for 80 epochs using a batch size of 128 and a learning rate of 10<sup>-6</sup>. Binary cross entropy with logits loss (BCEWithLogitsLoss) served as the objective function. These parameters were selected to ensure stable convergence and to reduce overfitting. After the training, the model performance was evaluated using the evaluation dataset. The confusion matrix was calculated as the number of true positives, false positives, false negatives, and true negatives, aggregated across all predictions in the evaluation set.</p>
</sec>
<sec id="s2h">
<title>Explainability analysis using occlusion and integrated gradients</title>
<p>To elucidate the internal decision-making processes of the trained deep neural network (DNN), we employed two established explainability techniques: occlusion and integrated gradients.</p>
<p>Occlusion: Occlusion analysis involves systematically masking specific input features to determine their relative contribution to the model’s output (<xref ref-type="bibr" rid="c60">Zeiler and Fergus, 2013</xref>). In the present study, one channel was selectively occluded at a time from the 32-channel LFP input and the sensitivity of each channel was calculated. Sensitivity was defined as the corresponding change in model performance when the specified channel was occluded. By conducting these analyses for each of the nine rats, channel-specific importance scores were obtained and subsequently normalized (z-scored) to facilitate cross-subject comparisons. Channels whose removal yielded a more pronounced decrease in model performance were considered more critical for accurate prediction.</p>
<p>Integrated gradients: Integrated gradients is an attribution method that quantifies the importance of each input feature by integrating the gradient of the model’s output with respect to the input, transitioning from a baseline input to the actual input (<xref ref-type="bibr" rid="c50">Sundararajan et al., 2017</xref>). This approach produces class activation maps, enabling the visualization of features most influential for the model’s output. Here, integrated gradients were applied to the LFP segments for each rat, and the resulting class activation maps were averaged across subjects and trial conditions. These maps allowed us to identify salient input regions associated with both trial conditions and floor texture.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Stable locomotion across light and dark conditions</title>
<p>To investigate how visual input influences tactile processing in S1, we devised an experimental paradigm where both tactile and visual inputs were independently manipulated. Rats were placed on a disk-shaped treadmill with two distinct sandpaper textures, and LFPs were recorded from walking rats. Each rat walked for 10 min in a light environment (50 lx) and then for 10 minutes in total darkness (0 lx). To make sure that rat’s trajectory was stable across different floor textures and environmental conditions, gait parameters were extracted from the trajectories using deep-learning– based analysis (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). From the trajectories, swing duration, stance duration, stride length, and footstrike speed were extracted. All parameters were calculated for each floor textures and environmental conditions. Comparison of all conditions revealed that none of these metrics differed significantly between floor textures or environmental conditions, indicating that overall locomotion remained stable (<xref rid="fig2" ref-type="fig">Figure 2B-E</xref>, <italic>P</italic> &gt; 0.05, one-way analysis of variance (ANOVA) followed by Tukey‒Kramer <italic>post hoc</italic> test, <italic>n</italic> = 149, 149, 107 and 107 trials for smooth-light, rough-light, smooth-dark, and rough-dark, respectively). Thus, any differences in neural activity under these conditions are unlikely to be driven by altered motor behavior.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Comparison of gait parameters across textures and environmental conditions.</title>
<p>(<bold>A</bold>) Swing phase <italic>vs</italic>. stance phase, illustrated with video frames (left: swing, right: stance). (<bold>B</bold>) Normalized swing duration measured for each rat under different textures (smooth <italic>vs</italic>. rough) and environmental condition (light <italic>vs</italic>. dark). There were no significant differences among trial conditions. (<bold>C–E</bold>) Stance duration, stride length, and footstrike speed, respectively, under the same conditions as in B. None of these parameters differed significantly across texture types or lighting conditions.</p></caption>
<graphic xlink:href="648316v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3b">
<title>Characteristic negative deflections in S1 LFP upon forelimb contact</title>
<p>We next analyzed LFPs from S1 using a custom 32-channel electrode array targeting the forelimb and hindlimb subregions (<xref rid="fig1" ref-type="fig">Figure 1F</xref>, G). LFP traces were first aligned to forelimb contacts with the disk surface. On inspection of a single LFP trace, no apparent event-related response could be observed (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). However, aligning each LFP at forelimb contact with the floor, the average trace showed a clear response after the onset (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). Across conditions, we observed a clear negative deflection in LFPs that was most pronounced in the forelimb subregion (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). The amplitude of the response was significantly larger in all rats, consistent with topographic specificity of S1 (<xref rid="fig3" ref-type="fig">Figure 3D</xref>, <italic>P</italic> = 1.89×10<sup>-3</sup>, <italic>t</italic><sub>10</sub> = -2.2, paired <italic>t</italic>-test, <italic>n</italic> = 11 rats).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>LFP recordings in rat S1 during walking.</title>
<p>(<bold>A</bold>) A single representative LFP trace aligned to a forelimb contact. (<bold>B</bold>) An example of an averaged LFP trace from one session, aligned to forelimb contact. (<bold>C</bold>) The electrode montage and averaged LFP at each electrode. Left: The electrode montage showing all 32 recording sites. Right: Averaged LFP signals aligned to forelimb contacts with the floor, shown for each electrode depicted in the left panel. (<bold>D</bold>) Comparison of amplitudes between hindlimb and forelimb subregions, aggregated across all 11 rats. <italic>P</italic> = 1.89×10<sup>-3</sup>, <italic>t</italic><sub>10</sub> = -2.2, paired t-test, <italic>n</italic> = 11 rats. (<bold>E</bold>) Comparison of averaged amplitudes within the same trial for different floor textures and environmental conditions. <italic>P</italic> &gt; 0.05, one-way analysis of variance (ANOVA) followed by Tukey‒Kramer post hoc test, <italic>n</italic> = 149, 149, 107 and 107 trials for smooth-light, rough-light, smooth-dark, and rough-dark, respectively. (<bold>F</bold>) Cumulative probability distributions of mean amplitude from each session, compared across different textures. <italic>P</italic> = 4.84×10<sup>-1</sup>, 8.35×10<sup>-1</sup>, <italic>D</italic> = 1.03×10<sup>-1</sup> and 7.54×10<sup>-2</sup> for light and dark environments respectively, two-sample Kolmogorov‒ Smirnov test, <italic>n</italic> = 149 and 107 trials from 11 rats for light and dark, respectively. (<bold>G</bold>) Cumulative probability distributions of mean amplitude from each session, compared across light and dark environments. <italic>P</italic> = 1.05×10<sup>-1</sup>, 1.83×10<sup>-1</sup>, <italic>D</italic> = 0.14 and 0.149 for smooth and rough textures respectively, two-sample Kolmogorov‒Smirnov test, <italic>n</italic> = 149 and 107 trials from 11 rats for light and dark, respectively. Abbreviations: ERP, event-related potential; LFP, local field potential; S1, primary somatosensory cortex.</p></caption>
<graphic xlink:href="648316v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To see if the event related responses were affected by the change in floor texture or the environmental condition, the amplitudes were compared. Despite the prominent negative deflection in LFPs at the forelimb contact with the floor, analyses showed no substantial differences in signal amplitude (<xref rid="fig3" ref-type="fig">Figure 3E</xref>, <italic>P</italic> &gt; 0.05, one-way analysis of variance (ANOVA) followed by Tukey‒Kramer <italic>post hoc</italic> test, <italic>n</italic> = 149, 149, 107 and 107 trials for smooth-light, rough-light, smooth-dark, and rough-dark, respectively) when comparing rough <italic>vs.</italic> smooth textures (<xref rid="fig3" ref-type="fig">Figure 3F</xref>, <italic>P</italic> = 0.48 and 0.84, <italic>D</italic> = 1.03×10<sup>-</sup> <sup>1</sup> and 7.54×10<sup>-2</sup> for light and dark environments respectively, two-sample Kolmogorov‒Smirnov test, <italic>n</italic> = 149 and 107 trials from 11 rats for light and dark, respectively) and light <italic>vs.</italic> dark environments (<xref rid="fig5" ref-type="fig">Figure 5G</xref>, <italic>P</italic> = 0.11, 0.18, <italic>D</italic> = 0.14 and 0.149 for smooth and rough textures respectively, two-sample Kolmogorov‒Smirnov test, <italic>n</italic> = 149 and 107 trials from 11 rats for light and dark, respectively). These findings suggest that simple amplitude metrics may fail to capture more subtle or higher-dimensional differences in the underlying neuronal population activity.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Model-based prediction of texture and environmental conditions from LFP.</title>
<p>(<bold>A</bold>) The deep learning model architecture. The LFP input is processed through two parallel pathways for macro- and micro-scale feature extraction, followed by residual blocks that feed into two output heads for floor texture (Smooth <italic>vs.</italic> Rough) and the environmental condition (Light <italic>vs.</italic> Dark). (<bold>B</bold>) Training performance for a single representative rat. The left graph shows accuracy curves for texture (blue) and lighting (yellow), and the right graph shows the loss curves. (<bold>C</bold>) Testing performance for the same rat. The model exhibits good generalization, as accuracy increases and loss decreases on held-out data. (<bold>D</bold>) Confusion matrix for texture classification for all rats. Values above chance on the diagonal indicate successful texture prediction. Note that all values in the same row add up to 1. (<bold>E</bold>) Same as D, but for environmental conditions. (<bold>F</bold>) Combined confusion matrix for texture and trial predictions. The model performs well on both tasks across all rats. Abbreviations: avgpool, average pooling layer; conv, convolutional layer; maxpool, max pooling layer; LFP, local field potential</p></caption>
<graphic xlink:href="648316v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Neural representations become distinct in dark environments than in light.</title>
<p><bold>(A)</bold> A 912-dimensional feature vector is extracted from the layer preceding the final output. (<bold>B</bold>) A scatter plot of these features from one rat shows individual LFP segments (aligned to forelimb contact). Light orange and dark orange correspond to the light conditions (smooth, rough), while light blue and dark blue correspond to the dark conditions (smooth, rough). (<bold>C</bold>) Silhouette scores across all nine rats, showing that the dark condition yields higher scores and thus more distinct neural representations. <italic>P</italic> = 4.83×10<sup>-2</sup>, <italic>t</italic><sub>8</sub> = -2.6, paired <italic>t</italic>-test, <italic>n</italic> = 9 rats. (<bold>D</bold>) A pseudo-color map based on occlusion analysis, illustrating the contribution of each electrode in the forelimb and hindlimb subregions. Hotter regions indicate higher importance for the model’s predictions. (<bold>E</bold>) Forelimb channels exhibit higher occlusion sensitivity than hindlimb channels, highlighting the forelimb’s dominant role when the foot contacts the floor. <italic>P</italic> = 4.53×10<sup>-</sup> <sup>6</sup>, <italic>t</italic><sub>16</sub> = -5.57, Student’s <italic>t</italic>-test, <italic>n</italic> = 9 rats. (<bold>F</bold>) Activation maps generated via integrated gradients highlight key input features responsible for accurate model predictions of texture and environmental conditions. Activation scores show each feature’s impact on the model’s output relative to a reference baseline: high positive scores denote features that strongly affect the predicted class. The onset of forelimb contact is aligned to time zero. (<bold>G</bold>) Activation scores averaged over forelimb electrodes for floor texture (left) and the environmental conditions (right). A temporal lag in the dark condition suggests an extended processing window for floor texture when visual cues are absent.</p></caption>
<graphic xlink:href="648316v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3c">
<title>Machine learning–based decoding of tactile and visual information</title>
<p>Given the lack of clear differences in averaged LFP features, we applied a convolutional neural network (CNN) to decode both the type of floor texture (smooth <italic>vs</italic>. rough) and the presence or absence of visual cues (light <italic>vs</italic>. dark). The model architecture incorporated parallel convolutional pathways to capture both macro- and micro-scale temporal features, followed by residual blocks, and then split into dual output layers to jointly predict texture and lighting conditions (<xref rid="fig4" ref-type="fig">Figure 4A</xref>).</p>
<p>When trained on 80% of the LFP data (segmented around footstrikes), the model exhibited stable learning curves (<xref rid="fig4" ref-type="fig">Figure 4B</xref>) and generalized well to held-out test data (<xref rid="fig4" ref-type="fig">Figure 4C</xref>). Confusion matrices for both texture and lighting classifications were above chance along the diagonal, demonstrating that the model reliably extracted neural representations of the floor textures from the LFPs (<xref rid="fig4" ref-type="fig">Figure 4D–F</xref>).</p>
</sec>
<sec id="s3d">
<title>Neural representations become more distinct in darkness</title>
<p>To understand how absence of the visual cue might refine tactile processing, we performed explainability analyses on the model’s learned representations (<xref rid="fig5" ref-type="fig">Figure 5</xref>). We extracted a 912-dimensional feature vector from the layer preceding the final outputs, then visualized these high-dimensional embeddings with scatter plots (<xref rid="fig5" ref-type="fig">Figure 5A</xref>, B). Clustering analyses using silhouette scores showed that representations of texture were more separated in the dark environment, suggesting that reduced visual input enhances the distinctness of neural codes for tactile stimuli (<xref rid="fig5" ref-type="fig">Figure 5C</xref>, <italic>P</italic> = 4.83×10<sup>-2</sup>, <italic>t</italic><sub>8</sub> = -2.6, paired <italic>t</italic>-test, <italic>n</italic> = 9 rats).</p>
<p>To elucidate which electrodes held most information about the floor textures, we performed occlusion analysis. The results revealed that electrodes in the forelimb subregion contributed more to successful texture and lighting predictions (<xref rid="fig5" ref-type="fig">Figure 5D</xref>, E, <italic>P</italic> = 4.53×10<sup>-6</sup>, <italic>t</italic><sub>8</sub> = -5.57, Student’s <italic>t</italic>-test, <italic>n</italic> = 9 rats), which aligns with our observation that negative deflections in LFPs were largest in forelimb-targeting channels (<xref rid="fig2" ref-type="fig">Figure 2E</xref>). Further analysis using feature maps showed spatiotemporal patterns of salient features unique to each experimental condition (<xref rid="fig5" ref-type="fig">Figure 5F</xref>). In particular, the average activation in the forelimb subregion displayed a temporal shift in the dark condition, implying an extended processing window for texture information when visual cues are unavailable (<xref rid="fig5" ref-type="fig">Figure 5G</xref>).</p>
<p>Collectively, these results indicate that visual deprivation modifies population-level activity in S1, yielding more distinctive representations of tactile stimuli. Such reorganization could provide a neural substrate for enhanced tactile perception under conditions of reduced or absent visual input.</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>This study provides evidence that S1 undergoes rapid and dynamic reorganization of tactile representations when visual input is removed, even over a short period of minutes. By combining high-density LFP recordings with advanced machine learning techniques, we revealed that the neural encoding of tactile stimuli becomes more distinct under conditions of visual deprivation. Notably, when rats walked in the dark condition, texture representations in S1 were more distinguishable than when visual inputs were present. These findings highlight the adaptability of S1 and underscore the brain’s ability to quickly adjust its tactile representations in response to changing sensory contexts.</p>
<p>Interestingly, our data indicated that forelimb-targeting electrodes played a crucial role in decoding both tactile textures and lighting conditions. First, the evoked potential aligned at forelimb contact with the floor was larger at electrodes in forelimb subregion of the S1. This finding is consistent with the topographic specificity of S1, where distinct cortical regions are dedicated to processing sensory information from different parts of the body (<xref ref-type="bibr" rid="c51">Sur et al., 1980</xref>; <xref ref-type="bibr" rid="c16">Ewert et al., 2008</xref>; <xref ref-type="bibr" rid="c38">Prsa et al., 2019</xref>). Moreover, the occlusion analysis revealed that these forelimb regions were particularly critical for distinguishing texture and lighting conditions, suggesting that the forelimb subregion of S1 plays a dominant role in encoding tactile information in this context. Although this result is intuitive, since LFPs were aligned at forelimb onset, this is the first to reveal that a distinct cortical region encodes “tactile information” at the specific part of the body. Visualization of the key channel and temporal features using integrated gradients revealed results similar to those of the occlusion analysis. Specifically, the forelimb channels were found to be particularly influential in encoding texture. Additionally, the temporal window immediately following forelimb contact proved critical for accurate predictions, with this temporal window being significantly longer in the dark condition compared to the light condition.</p>
<p>The observed temporal shift in the dark condition, where texture-specific processing extended over a longer time window, suggests that the absence of visual input may enhance the retention of tactile information in S1. Previous studies have shown that neurons in S1 are involved in the short-term retention of information (<xref ref-type="bibr" rid="c61">Zhou and Fuster, 1996</xref>, <xref ref-type="bibr" rid="c62">1997</xref>, <xref ref-type="bibr" rid="c63">2000</xref>), and higher-order sensory areas exhibit prolonged neuronal firing (<xref ref-type="bibr" rid="c25">Leavitt et al., 2017</xref>; <xref ref-type="bibr" rid="c15">Esmaeili and Diamond, 2019</xref>). This sustained activity may reflect enhanced feedback loops within S1 and also from higher cortical regions. The temporal shift observed in our study could thus indicate sustained neural representations, possibly as a result of visual deprivation leading to a more prolonged representation of tactile stimuli. Whether this shift reflects a compensatory mechanism for the lack of visual input or directly correlates with heightened tactile sensitivity requires further investigation.</p>
<p>The key point of this study is the use of a custom-designed behavioral paradigm and CNN model to decode high-dimensional LFP data. Traditional analyses of neural signals, such as amplitude-based methods or event-related potentials, often overlook subtle, higher-dimensional features that are embedded in the data (<xref ref-type="bibr" rid="c58">Yamins and DiCarlo, 2016a</xref>, <xref ref-type="bibr" rid="c59">2016b</xref>; <xref ref-type="bibr" rid="c44">Saxena and Cunningham, 2019</xref>). By leveraging machine learning, we were able to extract these fine-grained temporal and spatial patterns, which revealed that texture-specific neural signals in S1 became more distinct when rats walked in the dark. These results highlight the power of advanced computational methods in uncovering previously undetectable shifts in sensory coding, paving the way for similar techniques to be applied to other forms of high-dimensional neural data, such as multi-electrode array recordings in freely behaving animals.</p>
<p>Previous studies on tactile enhancement in short-term visually deprived subjects have shown improvements in tactile acuity and corresponding plastic changes in fMRI BOLD signals (<xref ref-type="bibr" rid="c33">Pascual-Leone and Hamilton, 2001</xref>; <xref ref-type="bibr" rid="c23">Kauffman et al., 2002</xref>; <xref ref-type="bibr" rid="c17">Facchini and Aglioti, 2003</xref>). However, due to the low temporal and spatial resolution of fMRI, these studies could not capture detailed changes in tactile representation within the brain. In contrast, our study successfully captured population-level activity from S1 using LFP recordings. Although future research is needed to assess whether rats can more accurately discriminate between textures in the dark, our findings suggest an underlying mechanism driving tactile perception enhancement under short-term visual deprivation.</p>
<p>From an evolutionary standpoint, the enhanced tactile sensitivity observed in the dark could offer a significant advantage. Many species, including rodents, rely on somatosensation for navigation and foraging in low-visibility environments. The ability to quickly enhance tactile processing in such conditions could aid in efficient navigation, resource acquisition, and predator detection. Notably, changes in arousal levels correlate with tactile sensitivity (<xref ref-type="bibr" rid="c47">Shimaoka et al., 2018</xref>; <xref ref-type="bibr" rid="c26">Lee et al., 2020</xref>), and environmental lighting strongly influences arousal (<xref ref-type="bibr" rid="c52">Tamayo et al., 2023</xref>). This mechanism may thus underlie the computational shifts observed in S1, further emphasizing the dynamic nature of sensory processing. Rather than being a passive receptor of tactile input, S1 appears to adapt its encoding strategies depending on the sensory context, optimizing its function based on available visual or other sensory cues.</p>
<p>In addition to advancing our understanding of cross-modal processing, these findings have potential applications for sensory rehabilitation. Our results suggest that visual deprivation can rapidly enhance the brain’s processing of tactile stimuli, a principle that could be leveraged in individuals with tactile impairments to improve performance. This concept could be explored in the context of training protocols such as those used for Braille reading or haptic feedback devices, where manipulating sensory input might improve tactile sensitivity (<xref ref-type="bibr" rid="c4">Bolognini et al., 2009</xref>). Moreover, the use of machine learning techniques to decode neural signals holds promise for advancing brain–machine interfaces and improving assistive technologies for individuals with sensory deficits.</p>
<p>While this study contributes to our understanding of cross-modal processing, several limitations must be considered. Our results are correlational, and future research could incorporate causal manipulations, such as pharmacological interventions or optogenetic techniques, to better elucidate the underlying mechanisms driving these changes. Additionally, while rats are a powerful model system for studying sensory processing, their neural architecture and behaviors may not fully reflect the complexities of human perception. Further studies that explore the short-term effects of visual deprivation on sensory encoding, or extend this paradigm to include behavioral readouts of tactile acuity, would help clarify how changes in neural coding relate to perceptual performance.</p>
<p>In conclusion, this study provides novel insights into the brain’s capacity for cross-modal adaptation, showing that visual deprivation rapidly reorganizes tactile processing in S1. The enhanced distinction of tactile stimuli under conditions of visual deprivation suggests that S1 can flexibly adjust its neural representations in response to environmental changes. By combining behavioral paradigms, LFP recordings, and machine learning techniques, this work highlights the value of advanced computational approaches in understanding the dynamic and adaptive nature of sensory coding. These findings open the door for future research on sensory rehabilitation and multisensory interactions, and they underscore the brain’s remarkable ability to reorganize itself in response to changes in sensory input.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>This work was supported by JST ERATO (JPMJER1801), AMED-CREST (24wm0625401h0001; 24wm0625502s0501; 24wm0625207s0101; 24gm1510002s0104), the Institute for AI and Beyond of the University of Tokyo, JSPS Grants-in-Aid for Scientific Research (18H05525, 20K15926, 22K21353), KOSÉ Cosmetology Research Foundation, the Public Foundation of Chubu Science and Technology Center, and Konica Minolta Science and Technology Foundation.</p>
</ack>
<sec id="s8" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5" sec-type="data-availability">
<title>Data availability</title>
<p>The data that support the findings of this study are available from the corresponding author upon reasonable request.</p>
</sec>
<sec id="s6">
<title>Code availability</title>
<p>The codes that support the findings of this study are available from the corresponding author upon reasonable request.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abraira</surname> <given-names>VE</given-names></string-name>, <string-name><surname>Ginty</surname> <given-names>DD</given-names></string-name></person-group> (<year>2013</year>) <article-title>The sensory neurons of touch</article-title>. <source>Neuron</source> <volume>79</volume>:<fpage>618</fpage>–<lpage>639</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bensmaia</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Denchev</surname> <given-names>PV</given-names></string-name>, <string-name><surname>Dammann</surname> <given-names>JF</given-names></string-name> 3rd, <string-name><surname>Craig</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Hsiao</surname> <given-names>SS</given-names></string-name></person-group> (<year>2008</year>) <article-title>The representation of stimulus orientation in the early stages of somatosensory processing</article-title>. <source>J Neurosci</source> <volume>28</volume>:<fpage>776</fpage>–<lpage>786</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bola</surname> <given-names>Ł</given-names></string-name>, <string-name><surname>Siuda-Krzywicka</surname> <given-names>K</given-names></string-name>, <string-name><surname>Paplińska</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sumera</surname> <given-names>E</given-names></string-name>, <string-name><surname>Zimmermann</surname> <given-names>M</given-names></string-name>, <string-name><surname>Jednoróg</surname> <given-names>K</given-names></string-name>, <string-name><surname>Marchewka</surname> <given-names>A</given-names></string-name>, <string-name><surname>Szwed</surname> <given-names>M</given-names></string-name></person-group> (<year>2017</year>) <article-title>Structural reorganization of the early visual cortex following Braille training in sighted adults</article-title>. <source>Sci Rep</source> <volume>7</volume>:<fpage>17448</fpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bolognini</surname> <given-names>N</given-names></string-name>, <string-name><surname>Pascual-Leone</surname> <given-names>A</given-names></string-name>, <string-name><surname>Fregni</surname> <given-names>F</given-names></string-name></person-group> (<year>2009</year>) <article-title>Using non-invasive brain stimulation to augment motor training-induced plasticity</article-title>. <source>J Neuroeng Rehabil</source> <volume>6</volume>:<fpage>8</fpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boroojerdi</surname> <given-names>B</given-names></string-name>, <string-name><surname>Bushara</surname> <given-names>KO</given-names></string-name>, <string-name><surname>Corwell</surname> <given-names>B</given-names></string-name>, <string-name><surname>Immisch</surname> <given-names>I</given-names></string-name>, <string-name><surname>Battaglia</surname> <given-names>F</given-names></string-name>, <string-name><surname>Muellbacher</surname> <given-names>W</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>LG</given-names></string-name></person-group> (<year>2000</year>) <article-title>Enhanced excitability of the human visual cortex induced by short-term light deprivation</article-title>. <source>Cereb Cortex</source> <volume>10</volume>:<fpage>529</fpage>–<lpage>534</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bulusu</surname> <given-names>V</given-names></string-name>, <string-name><surname>Lazar</surname> <given-names>L</given-names></string-name></person-group> (<year>2024</year>) <article-title>Crossmodal associations between naturally occurring tactile and sound textures</article-title>. <source>Perception</source> <volume>53</volume>:<fpage>219</fpage>–<lpage>239</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burton</surname> <given-names>H</given-names></string-name></person-group> (<year>2003</year>) <article-title>Visual cortex activity in early and late blind people</article-title>. <source>J Neurosci</source> <volume>23</volume>:<fpage>4005</fpage>–<lpage>4011</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bushnell</surname> <given-names>MC</given-names></string-name>, <string-name><surname>Duncan</surname> <given-names>GH</given-names></string-name>, <string-name><surname>Hofbauer</surname> <given-names>RK</given-names></string-name>, <string-name><surname>Ha</surname> <given-names>B</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>JI</given-names></string-name>, <string-name><surname>Carrier</surname> <given-names>B</given-names></string-name></person-group> (<year>1999</year>) <article-title>Pain perception: is there a role for primary somatosensory cortex?</article-title> <source>Proc Natl Acad Sci U S A</source> <volume>96</volume>:<fpage>7705</fpage>–<lpage>7709</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Butler</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Foxe</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Fiebelkorn</surname> <given-names>IC</given-names></string-name>, <string-name><surname>Mercier</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Molholm</surname> <given-names>S</given-names></string-name></person-group> (<year>2012</year>) <article-title>Multisensory representation of frequency across audition and touch: high density electrical mapping reveals early sensory-perceptual coupling</article-title>. <source>J Neurosci</source> <volume>32</volume>:<fpage>15338</fpage>–<lpage>15344</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Delhaye</surname> <given-names>BP</given-names></string-name>, <string-name><surname>Long</surname> <given-names>KH</given-names></string-name>, <string-name><surname>Bensmaia</surname> <given-names>SJ</given-names></string-name></person-group> (<year>2018</year>) <article-title>Neural basis of touch and proprioception in primate cortex</article-title>. <source>Compr Physiol</source> <volume>8</volume>:<fpage>1575</fpage>–<lpage>1602</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dijkerman</surname> <given-names>HC</given-names></string-name>, <string-name><surname>de Haan</surname> <given-names>EHF</given-names></string-name></person-group> (<year>2007</year>) <article-title>Somatosensory processing subserving perception and action: Dissociations, interactions, and integration</article-title>. <source>Behav Brain Sci</source> <volume>30</volume>:<fpage>224</fpage>–<lpage>230</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di Plinio</surname> <given-names>S</given-names></string-name>, <string-name><surname>Perrucci</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Aleman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ebisch</surname> <given-names>SJH</given-names></string-name></person-group> (<year>2020</year>) <article-title>I am Me: Brain systems integrate and segregate to establish a multidimensional sense of self</article-title>. <source>Neuroimage</source> <volume>205</volume>:<fpage>116284</fpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Driver</surname> <given-names>J</given-names></string-name>, <string-name><surname>Spence</surname> <given-names>C</given-names></string-name></person-group> (<year>2000</year>) <article-title>Multisensory perception: beyond modularity and convergence</article-title>. <source>Curr Biol</source> <volume>10</volume>:<fpage>R731</fpage>–<lpage>R735</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eimer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Forster</surname> <given-names>B</given-names></string-name></person-group> (<year>2003</year>) <article-title>Modulations of early somatosensory ERP components by transient and sustained spatial attention</article-title>. <source>Exp Brain Res</source> <volume>151</volume>:<fpage>24</fpage>–<lpage>31</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esmaeili</surname> <given-names>V</given-names></string-name>, <string-name><surname>Diamond</surname> <given-names>ME</given-names></string-name></person-group> (<year>2019</year>) <article-title>Neuronal correlates of tactile working memory in prefrontal and vibrissal somatosensory cortex</article-title>. <source>Cell Rep</source> <volume>27</volume>:<fpage>3167</fpage>–<lpage>3181.</lpage> </mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ewert</surname> <given-names>TAS</given-names></string-name>, <string-name><surname>Vahle-Hinz</surname> <given-names>C</given-names></string-name>, <string-name><surname>Engel</surname> <given-names>AK</given-names></string-name></person-group> (<year>2008</year>) <article-title>High-frequency whisker vibration is encoded by phase-locked responses of neurons in the rat’s barrel cortex</article-title>. <source>J Neurosci</source> <volume>28</volume>:<fpage>5359</fpage>–<lpage>5368</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Facchini</surname> <given-names>S</given-names></string-name>, <string-name><surname>Aglioti</surname> <given-names>SM</given-names></string-name></person-group> (<year>2003</year>) <article-title>Short term light deprivation increases tactile spatial acuity in humans</article-title>. <source>Neurology</source> <volume>60</volume>:<fpage>1998</fpage>–<lpage>1999</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goldreich</surname> <given-names>D</given-names></string-name>, <string-name><surname>Kanics</surname> <given-names>IM</given-names></string-name></person-group> (<year>2003</year>) <article-title>Tactile acuity is enhanced in blindness</article-title>. <source>J Neurosci</source> <volume>23</volume>:<fpage>3439</fpage>– <lpage>3445</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goodwin</surname> <given-names>AW</given-names></string-name>, <string-name><surname>Wheat</surname> <given-names>HE</given-names></string-name></person-group> (<year>2004</year>) <article-title>Sensory signals in neural populations underlying tactile perception and manipulation</article-title>. <source>Annu Rev Neurosci</source> <volume>27</volume>:<fpage>53</fpage>–<lpage>77</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>He</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Ren</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sun</surname> <given-names>J</given-names></string-name></person-group> (<year>2015</year>) <article-title>Deep residual learning for image recognition</article-title>. arXiv [csCV] Available at: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</ext-link>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hopkins</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kass</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Blalock</surname> <given-names>LD</given-names></string-name>, <string-name><surname>Brill</surname> <given-names>JC</given-names></string-name></person-group> (<year>2017</year>) <article-title>Effectiveness of auditory and tactile crossmodal cues in a dual-task visual and auditory scenario</article-title>. <source>Ergonomics</source> <volume>60</volume>:<fpage>692</fpage>–<lpage>700</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Karlen</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Kahn</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Krubitzer</surname> <given-names>L</given-names></string-name></person-group> (<year>2006</year>) <article-title>Early blindness results in abnormal corticocortical and thalamocortical connections</article-title>. <source>Neuroscience</source> <volume>142</volume>:<fpage>843</fpage>–<lpage>858</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kauffman</surname> <given-names>T</given-names></string-name>, <string-name><surname>Théoret</surname> <given-names>H</given-names></string-name>, <string-name><surname>Pascual-Leone</surname> <given-names>A</given-names></string-name></person-group> (<year>2002</year>) <article-title>Braille character discrimination in blindfolded human subjects</article-title>. <source>Neuroreport</source> <volume>13</volume>:<fpage>571</fpage>–<lpage>574</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koch</surname> <given-names>KW</given-names></string-name>, <string-name><surname>Fuster</surname> <given-names>JM</given-names></string-name></person-group> (<year>1989</year>) <article-title>Unit activity in monkey parietal cortex related to haptic perception and temporary memory</article-title>. <source>Exp Brain Res</source> <volume>76</volume>:<fpage>292</fpage>–<lpage>306</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leavitt</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Mendoza-Halliday</surname> <given-names>D</given-names></string-name>, <string-name><surname>Martinez-Trujillo</surname> <given-names>JC</given-names></string-name></person-group> (<year>2017</year>) <article-title>Sustained activity encoding working memories: Not fully distributed</article-title>. <source>Trends Neurosci</source> <volume>40</volume>:<fpage>328</fpage>–<lpage>346</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname> <given-names>CCY</given-names></string-name>, <string-name><surname>Kheradpezhouh</surname> <given-names>E</given-names></string-name>, <string-name><surname>Diamond</surname> <given-names>ME</given-names></string-name>, <string-name><surname>Arabzadeh</surname> <given-names>E</given-names></string-name></person-group> (<year>2020</year>) <article-title>State-dependent changes in perception and coding in the mouse somatosensory cortex</article-title>. <source>Cell Rep</source> <volume>32</volume>:<fpage>108197</fpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lieber</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Bensmaia</surname> <given-names>SJ</given-names></string-name></person-group> (<year>2019</year>) <article-title>High-dimensional representation of texture in somatosensory cortex of primates</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>116</volume>:<fpage>3268</fpage>–<lpage>3277</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luna</surname> <given-names>R</given-names></string-name>, <string-name><surname>Hernández</surname> <given-names>A</given-names></string-name>, <string-name><surname>Brody</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Romo</surname> <given-names>R</given-names></string-name></person-group> (<year>2005</year>) <article-title>Neural codes for perceptual discrimination in primary somatosensory cortex</article-title>. <source>Nat Neurosci</source> <volume>8</volume>:<fpage>1210</fpage>–<lpage>1219</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Merabet</surname> <given-names>LB</given-names></string-name>, <string-name><surname>Hamilton</surname> <given-names>R</given-names></string-name>, <string-name><surname>Schlaug</surname> <given-names>G</given-names></string-name>, <string-name><surname>Swisher</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Kiriakopoulos</surname> <given-names>ET</given-names></string-name>, <string-name><surname>Pitskel</surname> <given-names>NB</given-names></string-name>, <string-name><surname>Kauffman</surname> <given-names>T</given-names></string-name>, <string-name><surname>Pascual-Leone</surname> <given-names>A</given-names></string-name></person-group> (<year>2008</year>) <article-title>Rapid and reversible recruitment of early visual cortex for touch</article-title>. <source>PLoS One</source> <volume>3</volume>:<fpage>e3046</fpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moulton</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Pendse</surname> <given-names>G</given-names></string-name>, <string-name><surname>Becerra</surname> <given-names>LR</given-names></string-name>, <string-name><surname>Borsook</surname> <given-names>D</given-names></string-name></person-group> (<year>2012</year>) <article-title>BOLD responses in somatosensory cortices better reflect heat sensation than pain</article-title>. <source>J Neurosci</source> <volume>32</volume>:<fpage>6024</fpage>–<lpage>6031</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nikbakht</surname> <given-names>N</given-names></string-name>, <string-name><surname>Tafreshiha</surname> <given-names>A</given-names></string-name>, <string-name><surname>Zoccolan</surname> <given-names>D</given-names></string-name>, <string-name><surname>Diamond</surname> <given-names>ME</given-names></string-name></person-group> (<year>2018</year>) <article-title>Supralinear and Supramodal Integration of Visual and Tactile Signals in Rats: Psychophysics and Neuronal Mechanisms</article-title>. <source>Neuron</source> <volume>97</volume>:<fpage>626</fpage>–<lpage>639.</lpage> </mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Norman</surname> <given-names>JF</given-names></string-name>, <string-name><surname>Bartholomew</surname> <given-names>AN</given-names></string-name></person-group> (<year>2011</year>) <article-title>Blindness enhances tactile acuity and haptic 3-D shape discrimination</article-title>. <source>Atten Percept Psychophys</source> <volume>73</volume>:<fpage>2323</fpage>–<lpage>2331</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pascual-Leone</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hamilton</surname> <given-names>R</given-names></string-name></person-group> (<year>2001</year>) <article-title>The metamodal organization of the brain</article-title>. <source>Prog Brain Res</source> <volume>134</volume>:<fpage>427</fpage>–<lpage>445</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pascual-Leone</surname> <given-names>A</given-names></string-name>, <string-name><surname>Torres</surname> <given-names>F</given-names></string-name></person-group> (<year>1993</year>) <article-title>Plasticity of the sensorimotor cortex representation of the reading finger in Braille readers</article-title>. <source>Brain</source> <volume>116</volume> (<issue>Pt 1</issue>):<fpage>39</fpage>–<lpage>52</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pei</surname> <given-names>Y-C</given-names></string-name>, <string-name><surname>Hsiao</surname> <given-names>SS</given-names></string-name>, <string-name><surname>Craig</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Bensmaia</surname> <given-names>SJ</given-names></string-name></person-group> (<year>2011</year>) <article-title>Neural mechanisms of tactile motion integration in somatosensory cortex</article-title>. <source>Neuron</source> <volume>69</volume>:<fpage>536</fpage>–<lpage>547</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Piché</surname> <given-names>M</given-names></string-name>, <string-name><surname>Chabot</surname> <given-names>N</given-names></string-name>, <string-name><surname>Bronchti</surname> <given-names>G</given-names></string-name>, <string-name><surname>Miceli</surname> <given-names>D</given-names></string-name>, <string-name><surname>Lepore</surname> <given-names>F</given-names></string-name>, <string-name><surname>Guillemot</surname> <given-names>J-P</given-names></string-name></person-group> (<year>2007</year>) <article-title>Auditory responses in the visual cortex of neonatally enucleated rats</article-title>. <source>Neuroscience</source> <volume>145</volume>:<fpage>1144</fpage>–<lpage>1156</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Piras</surname> <given-names>F</given-names></string-name>, <string-name><surname>Vecchio</surname> <given-names>D</given-names></string-name>, <string-name><surname>Ciullo</surname> <given-names>V</given-names></string-name>, <string-name><surname>Gili</surname> <given-names>T</given-names></string-name>, <string-name><surname>Banaj</surname> <given-names>N</given-names></string-name>, <string-name><surname>Piras</surname> <given-names>F</given-names></string-name>, <string-name><surname>Spalletta</surname> <given-names>G</given-names></string-name></person-group> (<year>2020</year>) <article-title>Sense of external agency is sustained by multisensory functional integration in the somatosensory cortex</article-title>. <source>Hum Brain Mapp</source> <volume>41</volume>:<fpage>4024</fpage>–<lpage>4040</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Prsa</surname> <given-names>M</given-names></string-name>, <string-name><surname>Morandell</surname> <given-names>K</given-names></string-name>, <string-name><surname>Cuenu</surname> <given-names>G</given-names></string-name>, <string-name><surname>Huber</surname> <given-names>D</given-names></string-name></person-group> (<year>2019</year>) <article-title>Feature-selective encoding of substrate vibrations in the forelimb somatosensory cortex</article-title>. <source>Nature</source> <volume>567</volume>:<fpage>384</fpage>–<lpage>388</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rauschecker</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Tian</surname> <given-names>B</given-names></string-name>, <string-name><surname>Korte</surname> <given-names>M</given-names></string-name>, <string-name><surname>Egert</surname> <given-names>U</given-names></string-name></person-group> (<year>1992</year>) <article-title>Crossmodal changes in the somatosensory vibrissa/barrel system of visually deprived animals</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>89</volume>:<fpage>5063</fpage>– <lpage>5067</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Romo</surname> <given-names>R</given-names></string-name>, <string-name><surname>Salinas</surname> <given-names>E</given-names></string-name></person-group> (<year>2001</year>) <article-title>Touch and go: decision-making mechanisms in somatosensation</article-title>. <source>Annu Rev Neurosci</source> <volume>24</volume>:<fpage>107</fpage>–<lpage>137</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sadato</surname> <given-names>N</given-names></string-name>, <string-name><surname>Pascual-Leone</surname> <given-names>A</given-names></string-name>, <string-name><surname>Grafman</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ibañez</surname> <given-names>V</given-names></string-name>, <string-name><surname>Deiber</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Dold</surname> <given-names>G</given-names></string-name>, <string-name><surname>Hallett</surname> <given-names>M</given-names></string-name></person-group> (<year>1996</year>) <article-title>Activation of the primary visual cortex by Braille reading in blind subjects</article-title>. <source>Nature</source> <volume>380</volume>:<fpage>526</fpage>–<lpage>528</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Salinas</surname> <given-names>E</given-names></string-name>, <string-name><surname>Hernandez</surname> <given-names>A</given-names></string-name>, <string-name><surname>Zainos</surname> <given-names>A</given-names></string-name>, <string-name><surname>Romo</surname> <given-names>R</given-names></string-name></person-group> (<year>2000</year>) <article-title>Periodicity and firing rate as candidate neural codes for the frequency of vibrotactile stimuli</article-title>. <source>J Neurosci</source> <volume>20</volume>:<fpage>5503</fpage>–<lpage>5515</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sathian</surname> <given-names>K</given-names></string-name>, <string-name><surname>Stilla</surname> <given-names>R</given-names></string-name></person-group> (<year>2010</year>) <article-title>Cross-modal plasticity of tactile perception in blindness</article-title>. <source>Restor Neurol Neurosci</source> <volume>28</volume>:<fpage>271</fpage>–<lpage>281</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saxena</surname> <given-names>S</given-names></string-name>, <string-name><surname>Cunningham</surname> <given-names>JP</given-names></string-name></person-group> (<year>2019</year>) <article-title>Towards the neural population doctrine</article-title>. <source>Curr Opin Neurobiol</source> <volume>55</volume>:<fpage>103</fpage>–<lpage>111</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schürmann</surname> <given-names>M</given-names></string-name>, <string-name><surname>Caetano</surname> <given-names>G</given-names></string-name>, <string-name><surname>Jousmäki</surname> <given-names>V</given-names></string-name>, <string-name><surname>Hari</surname> <given-names>R</given-names></string-name></person-group> (<year>2004</year>) <article-title>Hands help hearing: facilitatory audiotactile interaction at low sound-intensity levels</article-title>. <source>J Acoust Soc Am</source> <volume>115</volume>:<fpage>830</fpage>–<lpage>832</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Serino</surname> <given-names>A</given-names></string-name></person-group> (<year>2019</year>) <article-title>Peripersonal space (PPS) as a multisensory interface between the individual and the environment, defining the space of the self</article-title>. <source>Neurosci Biobehav Rev</source> <volume>99</volume>:<fpage>138</fpage>–<lpage>159</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shimaoka</surname> <given-names>D</given-names></string-name>, <string-name><surname>Harris</surname> <given-names>KD</given-names></string-name>, <string-name><surname>Carandini</surname> <given-names>M</given-names></string-name></person-group> (<year>2018</year>) <article-title>Effects of arousal on mouse sensory cortex depend on modality</article-title>. <source>Cell Rep</source> <volume>22</volume>:<fpage>3160</fpage>–<lpage>3167</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Siuda-Krzywicka</surname> <given-names>K</given-names></string-name>, <string-name><surname>Bola</surname> <given-names>Ł</given-names></string-name>, <string-name><surname>Paplińska</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sumera</surname> <given-names>E</given-names></string-name>, <string-name><surname>Jednoróg</surname> <given-names>K</given-names></string-name>, <string-name><surname>Marchewka</surname> <given-names>A</given-names></string-name>, <string-name><surname>Śliwińska</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Amedi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Szwed</surname> <given-names>M</given-names></string-name></person-group> (<year>2016</year>) <article-title>Massive cortical reorganization in sighted Braille readers</article-title>. <source>eLife</source>. <volume>5</volume>: <elocation-id>e10762</elocation-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sugiyama</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kinukawa</surname> <given-names>T</given-names></string-name>, <string-name><surname>Takeuchi</surname> <given-names>N</given-names></string-name>, <string-name><surname>Nishihara</surname> <given-names>M</given-names></string-name>, <string-name><surname>Shioiri</surname> <given-names>T</given-names></string-name>, <string-name><surname>Inui</surname> <given-names>K</given-names></string-name></person-group> (<year>2019</year>) <article-title>Tactile Cross-Modal Acceleration Effects on Auditory Steady-State Response</article-title>. <source>Front Integr Neurosci</source> <volume>13</volume>:<fpage>496317</fpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Sundararajan</surname> <given-names>M</given-names></string-name>, <string-name><surname>Taly</surname> <given-names>A</given-names></string-name>, <string-name><surname>Yan</surname> <given-names>Q</given-names></string-name></person-group> (<year>2017</year>) <source>xiomatic attribution for deep networks. arXiv [csLG]</source> Available at: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1703.01365">http://arxiv.org/abs/1703.01365</ext-link>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sur</surname> <given-names>M</given-names></string-name>, <string-name><surname>Merzenich</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Kaas</surname> <given-names>JH</given-names></string-name></person-group> (<year>1980</year>) <article-title>Magnification, receptive-field area, and “hypercolumn” size in areas 3b and 1 of somatosensory cortex in owl monkeys</article-title>. <source>J Neurophysiol</source> <volume>44</volume>:<fpage>295</fpage>– <lpage>311</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tamayo</surname> <given-names>E</given-names></string-name>, <string-name><surname>Mouland</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Lucas</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Brown</surname> <given-names>TM</given-names></string-name></person-group> (<year>2023</year>) <article-title>Regulation of mouse exploratory behaviour by irradiance and cone-opponent signals</article-title>. <source>BMC Biol</source> <volume>21</volume>:<fpage>178</fpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tamè</surname> <given-names>L</given-names></string-name>, <string-name><surname>Holmes</surname> <given-names>NP</given-names></string-name></person-group> (<year>2016</year>) <article-title>Involvement of human primary somatosensory cortex in vibrotactile detection depends on task demand</article-title>. <source>Neuroimage</source> <volume>138</volume>:<fpage>184</fpage>–<lpage>196</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Boven</surname> <given-names>RW</given-names></string-name>, <string-name><surname>Hamilton</surname> <given-names>RH</given-names></string-name>, <string-name><surname>Kauffman</surname> <given-names>T</given-names></string-name>, <string-name><surname>Keenan</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Pascual-Leone</surname> <given-names>A</given-names></string-name></person-group> (<year>2000</year>) <article-title>Tactile spatial resolution in blind braille readers</article-title>. <source>Neurology</source> <volume>54</volume>:<fpage>2230</fpage>–<lpage>2236</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wong</surname> <given-names>M</given-names></string-name>, <string-name><surname>Gnanakumaran</surname> <given-names>V</given-names></string-name>, <string-name><surname>Goldreich</surname> <given-names>D</given-names></string-name></person-group> (<year>2011</year>) <article-title>Tactile spatial acuity enhancement in blindness: evidence for experience-dependent mechanisms</article-title>. <source>J Neurosci</source> <volume>31</volume>:<fpage>7028</fpage>–<lpage>7037</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamashiro</surname> <given-names>K</given-names></string-name>, <string-name><surname>Aoki</surname> <given-names>M</given-names></string-name>, <string-name><surname>Matsumoto</surname> <given-names>N</given-names></string-name>, <string-name><surname>Ikegaya</surname> <given-names>Y</given-names></string-name></person-group> (<year>2020</year>) <article-title>Polyherbal formulation enhancing cerebral slow waves in sleeping rats</article-title>. <source>Biol Pharm Bull</source> <volume>43</volume>:<fpage>1356</fpage>–<lpage>1360</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamashiro</surname> <given-names>K</given-names></string-name>, <string-name><surname>Ikegaya</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Matsumoto</surname> <given-names>N</given-names></string-name></person-group> (<year>2024</year>) <article-title>Automatic detection of foot-strike onsets in a rhythmic forelimb movement</article-title>. <source>Neurosci Res</source> <volume>206</volume>:<fpage>41</fpage>–<lpage>50</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamins</surname> <given-names>DL</given-names></string-name>, <string-name><surname>DiCarlo</surname> <given-names>JJ</given-names></string-name></person-group> (<year>2016a</year>) <article-title>Eight open questions in the computational modeling of higher sensory cortex</article-title>. <source>Curr Opin Neurobiol</source> <volume>37</volume>:<fpage>114</fpage>–<lpage>120</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamins</surname> <given-names>DLK</given-names></string-name>, <string-name><surname>DiCarlo</surname> <given-names>JJ</given-names></string-name></person-group> (<year>2016b</year>) <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title>. <source>Nat Neurosci</source> <volume>19</volume>:<fpage>356</fpage>–<lpage>365</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Zeiler</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Fergus</surname> <given-names>R</given-names></string-name></person-group> (<year>2013</year>) <source>Visualizing and understanding convolutional networks. arXiv [csCV]</source> Available at: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1311.2901">http://arxiv.org/abs/1311.2901</ext-link>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname> <given-names>YD</given-names></string-name>, <string-name><surname>Fuster</surname> <given-names>JM</given-names></string-name></person-group> (<year>1996</year>) <article-title>Mnemonic neuronal activity in somatosensory cortex</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>93</volume>:<fpage>10533</fpage>–<lpage>10537</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname> <given-names>YD</given-names></string-name>, <string-name><surname>Fuster</surname> <given-names>JM</given-names></string-name></person-group> (<year>1997</year>) <article-title>Neuronal activity of somatosensory cortex in a cross-modal (visuo-haptic) memory task</article-title>. <source>Exp Brain Res</source> <volume>116</volume>:<fpage>551</fpage>–<lpage>555</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname> <given-names>YD</given-names></string-name>, <string-name><surname>Fuster</surname> <given-names>JM</given-names></string-name></person-group> (<year>2000</year>) <article-title>Visuo-tactile cross-modal associations in cortical somatosensory cells</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>97</volume>:<fpage>9777</fpage>–<lpage>9782</lpage>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ziegler</surname> <given-names>K</given-names></string-name>, <string-name><surname>Folkard</surname> <given-names>R</given-names></string-name>, <string-name><surname>Gonzalez</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Burghardt</surname> <given-names>J</given-names></string-name>, <string-name><surname>Antharvedi-Goda</surname> <given-names>S</given-names></string-name>, <string-name><surname>Martin-Cortecero</surname> <given-names>J</given-names></string-name>, <string-name><surname>Isaías-Camacho</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kaushalya</surname> <given-names>S</given-names></string-name>, <string-name><surname>Tan</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Kuner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Acuna</surname> <given-names>C</given-names></string-name>, <string-name><surname>Kuner</surname> <given-names>R</given-names></string-name>, <string-name><surname>Mease</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Groh</surname> <given-names>A</given-names></string-name></person-group> (<year>2023</year>) <article-title>Primary somatosensory cortex bidirectionally modulates sensory gain and nociceptive behavior in a layer-specific manner</article-title>. <source>Nat Commun</source> <volume>14</volume>:<fpage>2999</fpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106554.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Gjorgjieva</surname>
<given-names>Julijana</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Technical University of Munich</institution>
</institution-wrap>
<city>Freising</city>
<country>Germany</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents <bold>useful</bold> findings on how the transient absence of visual input (i.e., darkness) affects tactile neural encoding in the somatosensory cortex. The evidence supporting the authors' claims is <bold>incomplete</bold>, as key conclusions rely on subtle differences in surface roughness discriminability between sensory conditions, whose physiological underpinnings remain unclear. Potential methodological confounds are also not fully addressed. With additional analyses and methodological clarifications, this work could substantially inform neuroscientists studying cross-modal interactions.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106554.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors aimed to investigate how short-term visual deprivation influences tactile processing in the primary somatosensory cortex (S1) of sighted rats. They justify the study based on previous studies that have shown that long-term blindness can enhance tactile perception, and aim to investigate the neural mechanisms underlying rapid, short-term cross-modal plasticity. The authors recorded local field potentials from S1 as rats encountered different tactile textures (smooth and rough sandpaper) under light and dark conditions. They used deep learning techniques to decode the neural signals and assess how tactile representations changed across the four different conditions. Their goal was to uncover whether the absence of visual cues leads to a rapid reorganization of tactile encoding in the brain.</p>
<p>Strengths:</p>
<p>The study effectively integrates high-density local field potential (LFP) recordings with convolutional neural network (CNN) analysis. This combination allows for decoding high-dimensional population-level signals, revealing changes in neural representations that traditional analyses (e.g., amplitude measures) failed to detect. The custom treadmill paradigm permits independent manipulation of visual and tactile inputs under stable locomotion conditions. Gait analysis confirms that motor behavior was consistent across conditions, strengthening the conclusion that neural changes are due to sensory input rather than movement artifacts.</p>
<p>Weaknesses:</p>
<p>(1) While the study interprets the emergence of more distinct texture representations in the dark as evidence of rapid cross-modal plasticity, the claim rests on correlational data from a short-term manipulation and decoding analysis. The authors show that CNN-derived feature embeddings cluster more clearly by texture in the dark, but this does not directly demonstrate plasticity in the classical sense (e.g., synaptic or circuit-level reorganization).</p>
<p>(2) Although gait was controlled, changes in arousal or exploratory behavior in light versus dark conditions might contribute to the observed neural differences. These factors are acknowledged but not directly measured (e.g., via pupillometry or cortical state indicators).</p>
<p>(3) Moreover, the time course of the observed changes (within 10 minutes) is quite rapid, and while intriguing, the study does not include direct evidence that the underlying circuits were reorganized - only that population-level signals become more discriminable. As such, the term &quot;plasticity&quot; may overstate the conclusions and should be interpreted with caution unless validated by additional causal or longitudinal data.</p>
<p>(4) The study highlights the forelimb region of S1 and a post-contact temporal window as particularly important for decoding texture, based on occlusion and integrated gradient analyses. However, this finding may be somewhat circular: The LFPs were aligned to forelimb contact, and the floor textures were sensed primarily via the forelimbs, making it unsurprising that forelimb electrodes were most informative. The observed temporal window corresponds directly to the event-aligned epoch, and while it may shift slightly in duration in the dark, this could reflect general differences in sensory gain or arousal, rather than changes in stimulus-specific encoding. Thus, while these findings are consistent with somatotopy and context-dependent dynamics, they do not provide strong independent evidence for novel spatial or temporal organization.</p>
<p>(5) While the neural data suggest enhanced tactile representations, the study does not assess whether rats' actual tactile perception improved. Without a behavioral readout (e.g., discrimination accuracy), claims about perceptual enhancement remain speculative.</p>
<p>(6) In addition to point 4, the authors discuss implications for sensory rehabilitation, including Braille training and haptic feedback enhancement. However, the lack of actual chronic or even more acute pathological sensory deprivation, behavioral data, or subsequent intervention in this study limits the ability to draw translational conclusions. It remains unknown whether the more distinct neural representations observed actually translate into better tactile performance, discriminability, or perception. Additionally, extrapolating from rats walking on sandpaper in the dark to human rehabilitative contexts is speculative without a clearer behavioral or mechanistic bridge. The potential is certainly there, but the claim is currently aspirational rather than empirically grounded.</p>
<p>(7) While the CNN showed good performance, details on generalization robustness and validation (e.g., cross-validation folds, variance across animals) are not deeply discussed. Also, while explainability tools were used, interpretability of CNNs remains limited, and more transparent models (e.g., linear classifiers or dimensionality reduction) could offer complementary insights.</p>
<p>Therefore, while the authors raise interesting hypotheses around rapid plasticity, somatotopic dynamics, and rehabilitation, the evidence for each is indirect. Stronger claims would require causal experiments, behavioral readouts, and mechanistic specificity beyond what the current data can provide.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106554.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Yamashiro et al. investigated how the transient absence of visual input (i.e., darkness) impacts tactile neural encoding in the rat primary somatosensory cortex (S1). They recorded local field potentials (LFPs) using a 32-channel array implanted in forelimb and hindlimb primary somatosensory cortex while rats walked on smooth or rough textures under illuminated and dark conditions. Employing a convolutional neural network (CNN), they successfully decoded both texture and lighting conditions from the LFPs. The authors conclude that the subtle differences in LFP patterns underlie tactile representation of surface roughness and become more distinct in darkness, suggesting a rapid cross-modal reorganization of the neural code for this sensory feature.</p>
<p>Strengths:</p>
<p>(1) The manuscript addresses a valuable question regarding how sensory cortices adapt dynamically to changes in sensory context.</p>
<p>(2) Utilization of machine learning (CNNs) allowed the authors to go beyond conventional amplitude-based analyses, potentially uncovering a subtle but interesting phenomenon.</p>
<p>Weaknesses:</p>
<p>(1) Despite applying explainability techniques to the CNN-based decoder, the study does not clearly demonstrate the precise &quot;subtle, high-dimensional patterns&quot; exploited by the CNN for surface roughness decoding, limiting the physiological interpretability of the results. Additional analyses (e.g., detailed waveform morphology analysis on grand averages, time-frequency decompositions, or further use of explainability methods) are necessary to clarify the exact nature of the discriminative activity features enabling the CNN to decode surface roughness and how these change with the sensory context (i.e., in light or darkness).</p>
<p>(2) The claim regarding cross-modal representation reorganization heavily relies on a silhouette analysis (Figure 5C), which shows a modest effect size and borderline statistical significance (p≈0.05 with n=9+2). More rigorous statistical quantification, such as permutation tests and reporting underlying cluster distances for all animals, would strengthen confidence in this finding.</p>
<p>(3) While the authors recorded in the somatosensory cortex, primarily known for its tactile responsivity, I would be cautious not to rule out a priori the presence of crossmodal (visual) responses in the area. In this case, the stronger texture separation in darkness might be explained by the absence of some visually-evoked potentials (VEPs) rather than genuine cross-modal reorganization. Clarification is needed to rule out visual interference and this would strengthen the claim.</p>
<p>(4) Behavioural controls are limited to gross gait parameters; more detailed analyses of locomotor behavior and additional metrics (e.g., pupil size or locomotor variance) would robustly rule out potential arousal or motor confounds.</p>
<p>(5) The consistent ordering of trials (10 minutes of light then 10 minutes of dark) could introduce confounds such as fatigue or satiation (and also related arousal state), which should be controlled by analyzing sessions with reversed condition ordering.</p>
<p>(6) The focus on forelimb-aligned LFP analyses raises the possibility that hindlimb-aligned data might yield different conclusions, suggesting alignment effects might bias the results.</p>
<p>(7) The authors' dismissal of amplitude-based metrics as ineffective is inadequately substantiated. A clearer demonstration (e.g., event-related waveforms averaged by conditions, presented both spatially and temporally) would support this claim.</p>
<p>(8) Wording ambiguity regarding &quot;attribution score&quot; versus &quot;activation amplitude&quot; (Figure 5) complicates the interpretation of key findings. This distinction must be clarified for proper assessment of the results.</p>
<p>(9) Generalization across animals remains unaddressed. The current within-subject decoding setup limits conclusions regarding shared neural representations across individuals. Adopting cross-validation strategies and exploring between-animal analyses would add significant value to the manuscript.</p>
</body>
</sub-article>
</article>