<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">95160</article-id>
<article-id pub-id-type="doi">10.7554/eLife.95160</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95160.3</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Sensory-memory interactions via modular structure explain errors in visual working memory</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Yang</surname>
<given-names>Jun</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Hanqi</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9936-5293</contrib-id>
<name>
<surname>Lim</surname>
<given-names>Sukbin</given-names>
</name>
<xref ref-type="corresp" rid="cor1">*</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Weiyang College, Tsinghua University</institution>, Beijing, 100084, People’s <country>Republic of China</country></aff>
<aff id="a2"><label>2</label><institution>Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning, NYU Shanghai</institution>, 567 West Yangsi Road, Shanghai, 200126, People’s <country>Republic of China</country></aff>
<aff id="a3"><label>3</label><institution>Neural Science, NYU Shanghai</institution>, 567 West Yangsi Road, Shanghai, 200126, People’s <country>Republic of China</country></aff>
<aff id="a4"><label>4</label><institution>NYU-ECNU Institute of Brain and Cognitive Science at NYU Shanghai</institution>, 3663 Zhongshan Road North, Shanghai, 200062, People’s <country>Republic of China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Wei</surname>
<given-names>Xue-Xin</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>UT Austin</institution>
</institution-wrap>
<city>Austin</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Correspondence: <email>sukbin.lim@nyu.edu</email></corresp>
<fn id="n1" fn-type="present-address"><label>†</label><p>Interdisciplinary Graduate Program in Quantitative Biosciences, Georgia Institute of Technology, Atlanta, GA 30332; School of Mathematics, Georgia Institute of Technology, Atlanta, GA 30332</p></fn>
<fn fn-type="con"><p><bold>Author Contributions</bold> J. Y., H. Z., and S.L. designed and performed research. All authors prepared the figures and wrote the manuscript.</p></fn>
<fn fn-type="others"><p><bold>Competing Interests</bold> The authors declare no competing financial interests.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-04-04">
<day>04</day>
<month>04</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2024-09-11">
<day>11</day>
<month>09</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP95160</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-01-03">
<day>03</day>
<month>01</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-01-03">
<day>03</day>
<month>01</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.11.09.566396"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-04-04">
<day>04</day>
<month>04</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95160.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.95160.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95160.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95160.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95160.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
<event>
<event-desc>Reviewed preprint v2</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-08-21">
<day>21</day>
<month>08</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95160.2"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.95160.2.sa4">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95160.2.sa3">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95160.2.sa2">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95160.2.sa1">Reviewer #3 (Public Review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.95160.2.sa0">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Yang et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Yang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-95160-v3.pdf"/>
<abstract>
<title>Abstract</title>
<p>Errors in stimulus estimation reveal how stimulus representation changes during cognitive processes. Repulsive bias and minimum variance observed near cardinal axes are well-known error patterns typically associated with visual orientation perception. Recent experiments suggest that these errors continuously evolve during working memory, posing a challenge that neither static sensory models nor traditional memory models can address. Here, we demonstrate that these evolving errors, maintaining characteristic shapes, require network interaction between two distinct modules. Each module fulfills efficient sensory encoding and memory maintenance, which cannot be achieved simultaneously in a single-module network. The sensory module exhibits heterogeneous tuning with strong inhibitory modulation reflecting natural orientation statistics. While the memory module, operating alone, supports homogeneous representation via continuous attractor dynamics, the fully connected network forms discrete attractors with moderate drift speed and nonuniform diffusion processes. Together, our work underscores the significance of sensory-memory interaction in continuously shaping stimulus representation during working memory.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Minor changes in the expression and addition of current affiliation</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The brain does not faithfully represent external stimuli. Even for low-level features like orientation, spatial frequency, or color of visual stimuli, their internal representations are thought to be modified by a range of cognitive processes, including perception, memory, and decision (<xref ref-type="bibr" rid="c18">Geisler 2008</xref>, <xref ref-type="bibr" rid="c55">Webster 2015</xref>, <xref rid="c3" ref-type="bibr">Bays, Schneegans et al. 2022</xref>). Experimental studies quantified such modification by analyzing behavior data or decoding neural activities. For instance, biases of errors, the systematic deviation from the original stimuli, observed in estimation tasks have been used as indirect evidence to infer changes in the internal representations of stimuli (<xref ref-type="bibr" rid="c57">Wei and Stocker 2017</xref>).</p>
<p>One important source of biases is adaptation to environmental statistics, such as the nonuniform stimulus distribution found in nature or the limited range in specific settings. Cardinal repulsion, which refers to the systematic shift away from the horizontal and vertical orientations observed in many perceptual tasks, is one of the examples (<xref rid="c12" ref-type="bibr">de Gardelle, Kouider et al. 2010</xref>). Theoretical works suggest that such a bias pattern reflects the prevalence of the cardinal orientations in natural scenes (<xref rid="c20" ref-type="bibr">Girshick, Landy et al. 2011</xref>). Similarly, the variance of errors for orientation stimuli was found to be inversely proportional to the stimulus statistics, minimum at cardinal and maximum at oblique orientations (<xref rid="c50" ref-type="bibr">van Bergen, Ma et al. 2015</xref>). It was postulated that the dependence of biases and variance of errors on natural statistics results from sensory encoding optimized to enhance precision around the most common stimuli (<xref ref-type="bibr" rid="c17">Ganguli and Simoncelli 2014</xref>, <xref ref-type="bibr" rid="c56">Wei and Stocker 2015</xref>, <xref ref-type="bibr" rid="c57">Wei and Stocker 2017</xref>).</p>
<p>On the other hand, there is a growing body of evidence indicating that error patterns are not solely influenced by sensory encoding but are also shaped by memory processes. In delayed estimation tasks, where participants are presented with stimuli followed by a delay period during which they rely on their working memory for estimation, it has been observed that representations of orientation or color stimuli undergo gradual and continuous modifications throughout the delay period (<xref rid="c37" ref-type="bibr">Panichello, DePasquale et al. 2019</xref>, <xref ref-type="bibr" rid="c2">Bae 2021</xref>, <xref rid="c21" ref-type="bibr">Gu, Lee et al. 2023</xref>). Such dynamic error patterns are inconsistent with sensory encoding models, most of which only establish a static relationship between stimuli and internal representations. Traditional working memory models are not suitable either. Most of them are constructed to faithfully maintain information about stimuli during the delay period, and thus, the memory representation has a similar geometry as that of the stimuli (<xref ref-type="bibr" rid="c54">Wang 2001</xref>, <xref ref-type="bibr" rid="c29">Khona and Fiete 2022</xref>). For continuous stimuli such as orientation, location, direction, or color, all stimuli are equally maintained in ring-like memory activities, predicting no biases (<xref ref-type="bibr" rid="c62">Zhang 1996</xref>, <xref rid="c10" ref-type="bibr">Compte, Brunel et al. 2000</xref>, <xref ref-type="bibr" rid="c6">Burak and Fiete 2009</xref>).</p>
<p>How can we explain error patterns in working memory tasks that are similar to those observed in perception tasks? Here, we claim that not a single module but a two-module network with recursive interaction is required. Each module has a distinct role — sensory encoding and memory maintenance. To illustrate this, we use orientation stimuli and examine how their representations change during the delayed estimation tasks. We employ two approaches to find solutions for generating correct error patterns. The first extends previously suggested sensory encoding models, while the second modifies low-dimensional memory models based on attractor dynamics. These approaches are integrated into the network models, which link network connectivity to neuronal tuning properties and behavioral error patterns and reveal the attractor dynamics through low-dimensional projection. Our results show that the sensory-memory interacting networks outperform single-module networks with better control over the shapes and evolution of dynamic error patterns. Furthermore, our network models emphasize the importance of inhibitory tuning in sensory circuits for generating correct error patterns under typical associative learning of natural statistics. Finally, we provide testable predictions regarding the effect of perturbations in sensory-memory interactions on error patterns in delayed estimation tasks.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Low-dimensional attractor models</title>
<p>In natural images, cardinal orientations are the most prevalent (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). Error patterns in estimation tasks show dependence on such natural statistics, such as biases away from cardinal orientations where the variance of errors is nonetheless minimal (<xref rid="fig1" ref-type="fig">Figure 1B,C</xref>). In delayed estimation tasks, such a bias pattern is consolidated in time (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). Also, experimental data suggested that estimation errors increase with a longer delay (<xref rid="c59" ref-type="bibr">Wimmer, Nykamp et al. 2014</xref>, <xref ref-type="bibr" rid="c43">Schneegans and Bays 2018</xref>), while the precision is still highest at cardinal orientations (<xref rid="c51" ref-type="bibr">van den Berg, Shin et al. 2012</xref>, <xref ref-type="bibr" rid="c4">Bays 2014</xref>, <xref rid="c50" ref-type="bibr">van Bergen, Ma et al. 2015</xref>). Thus, we assumed that the variance of errors increases as keeping its characteristic shape (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). To explain these errors across orientations and over time, we first explored the underlying working memory mechanism. We considered low-dimensional attractor models with input noise that describe the drift and diffusion of the memory states. Here, we show that two prominent classes of previously suggested models are inconsistent with experimental observations and examine what modification to the models is required.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Error patterns of orientation stimuli in delayed-estimation tasks and low-dimensional attractor models. (<bold>A-C</bold>) Characteristic patterns of natural statistics of orientation stimuli <italic>θ</italic> (<bold>A</bold>), bias (<bold>B</bold>), and standard deviation (SD; <bold>C</bold>) during the delay period observed experimentally. Cardinal orientations are predominant in natural images (<bold>A</bold>). Bias and SD increase during the delay period, keeping patterns of repulsive bias (<bold>B</bold>) and minimum variance (<bold>C</bold>) around cardinal orientations. These characteristic patterns are visualized using trigonometric functions, and the range is normalized by their maximum values. Red vertical lines correspond to representative cardinal and oblique orientations, and with a periodicity of the error patterns, we only show the grey-shaded range in the remaining panels. (<bold>D-L</bold>) Comparison of different attractor models. (<bold>D-F</bold>) Continuous attractors with constant noise. Energy potential is flat (<bold>D</bold>), resulting in no bias (<bold>E</bold>) and uniform SD with uniform noise (<bold>F</bold>). (<bold>G-L</bold>) Discrete attractors with constant (<bold>G-I</bold>) and nonuniform noise (<bold>J-L</bold>). The discrete attractor models have potential hills and wells at cardinal and oblique orientations, respectively (<bold>G</bold>,<bold>J</bold>). While the bias patterns depend only on the energy landscape (<bold>H</bold>,<bold>K</bold>), SD representing variability also depends on noise (<bold>I</bold>,<bold>L</bold>). For the correct SD pattern (<bold>L</bold>), uneven noise with its maxima at the obliques (<bold>J</bold>) is required. Bias and SD patterns in the attractor models were obtained by running one-dimensional drift-diffusion models (see <bold>Methods</bold>).</p></caption>
<graphic xlink:href="566396v4_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The most widely accepted model for working memory of orientation stimuli has continuous attractor dynamics, which assumes that all orientations are equally encoded and maintained (<xref rid="fig1" ref-type="fig">Figure 1D-F</xref>). Each attractor corresponds to the memory state for different stimuli and forms a continuous ring following the geometry of orientation stimuli. The dynamics along continuous attractors are conceptually represented as movement along a flat energy landscape (<xref rid="fig1" ref-type="fig">Figure 1D</xref>). Without external input, there is no systematic shift of mean activity, that is, no drift during the delay period (<xref rid="fig1" ref-type="fig">Figure 1E</xref>). Also, under the assumption of equal influence of noise for all orientations, the variance of errors is spatially flat with constant diffusion along the ring, while the overall magnitude increases over time due to the accumulation of noise (<xref rid="fig1" ref-type="fig">Figure 1F</xref>).</p>
<p>While such continuous attractor models have been considered suitable for memory storage of continuous stimuli, they cannot capture drift dynamics observed during the delay period. Instead, discrete attractor models with uneven energy landscapes have been suggested with the energy wells corresponding to discrete attractors (<xref rid="fig1" ref-type="fig">Figure 1G-I</xref>). As evolution toward a few discrete attractors creates drift dynamics, the bias increases during the delay (<xref rid="fig1" ref-type="fig">Figure 1H</xref>). Also, discrete attractor models naturally produce nonuniform variance patterns. Even with constant noise along the ring, variance becomes minimum/maximum at the attractors/repellers due to the drift dynamics (<xref rid="fig1" ref-type="fig">Figure 1I</xref>). However, discrete attractor models with constant noise yield inconsistent results when inferring the locus of attractors from the bias and variance patterns observed in the data. Cardinal orientations should be the repeller to account for cardinal repulsion. In contrast, the minimum variance observed at the cardinal orientations suggests they should be the attractors.</p>
<p>How can such inconsistency be resolved? One possible solution is discrete attractor models with nonuniform noise amplitude (<xref rid="fig1" ref-type="fig">Figure 1J</xref>). Let’s consider that attractors are formed at oblique orientations to generate correct bias patterns (<xref rid="fig1" ref-type="fig">Figure 1K</xref>). Additionally, we assumed that noise has the highest amplitude at the obliques. When the difference in the noise amplitude is large enough to overcome the attraction toward the obliques, the models can produce correct variance patterns, maximum at the obliques and minimum at cardinal orientations (<xref rid="fig1" ref-type="fig">Figure 1L</xref>). In sum, unlike two prominent memory models, continuous attractors or discrete attractors with constant noise, discrete attractors with maximum noise at the obliques could reproduce experimentally observed error patterns of orientation stimuli. Note that these attractor models often simplify the full network dynamics. Namely, the drift and diffusion terms are derived by projecting network dynamics onto low-dimensional memory states (<xref ref-type="bibr" rid="c7">Burak and Fiete 2012</xref>, <xref ref-type="bibr" rid="c11">Darshan and Rivkind 2022</xref>). Thus, it is still in question whether there exist memory networks that can implement attractor dynamics with correct drift and diffusion terms.</p>
</sec>
<sec id="s2b">
<title>Bayesian sensory model and extension</title>
<p>Before exploring full memory network models, we note that previous theoretical works for sensory processing suggested that Bayesian inference with efficient coding could generate the repulsive bias and the lowest variance at cardinal orientations (<xref ref-type="bibr" rid="c56">Wei and Stocker 2015</xref>, <xref ref-type="bibr" rid="c57">Wei and Stocker 2017</xref>). Efficient coding theory suggests the sensory system should enhance the sensitivity around more common stimuli. For orientation stimuli, precision should be highest around cardinal directions, which could be achieved by sharpening the likelihood functions. Equipped with Bayesian optimal readout, such a sensory system could reproduce correct error patterns observed in perceptual tasks for various visual stimuli, including orientations (<xref rid="fig2" ref-type="fig">Figure 2</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Extension of Bayesian sensory models. (<bold>A</bold>) Schematics of extension to memory processing. We adapted the previous Bayesian models (<xref ref-type="bibr" rid="c56">Wei and Stocker 2015</xref>) for sensory encoding where <italic>θ</italic> and <inline-formula><inline-graphic xlink:href="566396v4_inline32.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are the input and output of sensory modules. We added a memory module where it maintains <inline-formula><inline-graphic xlink:href="566396v4_inline33.gif" mime-subtype="gif" mimetype="image"/></inline-formula> with the addition of memory noise <italic>ξ</italic>. The output of the memory module, <inline-formula><inline-graphic xlink:href="566396v4_inline34.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, is fed back to the sensory module as the input for the next iteration. (<bold>B</bold>) Illustration of the first iteration of sensory-memory interaction. Prior distribution follows the natural statistics (top), resulting in a sharper likelihood function near cardinal orientations (middle). Combining prior and likelihood functions leads to the posterior distribution of decoded <inline-formula><inline-graphic xlink:href="566396v4_inline35.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (light colors at the bottom), which is broadened with the addition of memory noise (dark colors at the bottom). Different curves correspond to different initial <italic>θ</italic>. (<bold>C</bold>) Bias (top) and SD (bottom) patterns obtained from decoded <inline-formula><inline-graphic xlink:href="566396v4_inline36.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for the 1<sup>st</sup>, 2<sup>nd</sup>, and 3<sup>rd</sup> iterations.</p></caption>
<graphic xlink:href="566396v4_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>However, such models only account for the relationship between external and perceived stimuli during sensory processing, resulting in static error patterns. Here, we extended the framework so that the system can maintain information about the stimulus after its offset while bias and variance of errors grow in time (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). We added a memory stage to Bayesian sensory models such that the memory stage receives the output of the sensory stage and returns it as the input after the maintenance. For instance, let’s denote the external orientation stimulus given during the stimulus period as <italic>θ</italic><sub>1</sub>. The sensory stage receives <italic>θ</italic><sub>1</sub> as input and generates the perceived orientation, <inline-formula><inline-graphic xlink:href="566396v4_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, which varies from trial to trial with sensory noise (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). Through the memory stage, <inline-formula><inline-graphic xlink:href="566396v4_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is returned as the input to the sensory stage for the next iteration with the addition of memory noise <italic>ξ</italic>1.</p>
<p>Such a recursive process mimics interactions between sensory and memory systems where the sensory system implements efficient coding and Bayesian inference, and the memory system faithfully maintains information. As the recursive process iterates, the distribution of the internal representation of orientation broadens due to the accumulation of noise from the sensory and memory systems. This leads to an increase in bias and variance at each step while keeping their characteristic shapes (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). Thus, recurrent interaction between sensory and memory systems during the delay period, each of which meets different demands, successfully reproduces correct error patterns observed in memory tasks.</p>
</sec>
<sec id="s2c">
<title>Network models with sensory and memory modules</title>
<p>Next, we construct network models capturing the sensory-memory interactions formalized under the Bayesian framework. We consider two-module networks where each module corresponds to the sensory and memory systems. To generate orientation selectivity, both modules have a columnar architecture where neurons in each column have a similar preference for orientation (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). However, their connectivity structures are different (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). The memory module in isolation resembles the traditional ring attractor network with a strong and homogeneous recurrent connection. This enables the memory module in isolation to maintain information about all orientations equally during the delay period (<xref rid="fig3" ref-type="fig">Figure 3B-F</xref>, right). Conversely, the recurrent connectivity strengths in the sensory module are relatively weak, such that without connection to the memory module, the activities during the delay period decay back to the baseline levels (<xref rid="fig3" ref-type="fig">Figure 3B</xref>, left). Furthermore, the connectivity strengths across columns are heterogeneous, particularly stronger at the obliques. As a result, the tuning curves near cardinal orientations can be sharper and denser, consistent with experimental observations showing a larger number of cardinally tuned neurons (<xref rid="c34" ref-type="bibr">Li, Peterson et al. 2003</xref>, <xref rid="c46" ref-type="bibr">Shen, Tao et al. 2014</xref>) and their narrower tuning (<xref rid="c34" ref-type="bibr">Li, Peterson et al. 2003</xref>, <xref rid="c30" ref-type="bibr">Kreile, Bonhoeffer et al. 2011</xref>) (<xref rid="fig3" ref-type="fig">Figure 3C-F</xref>, left). Different response activities of the two modules in isolation are demonstrated in their response manifolds as more dispersed representations around cardinal orientations in the sensory module, compared to the ring-like geometry of the memory module (<xref rid="fig3" ref-type="fig">Figure 3F</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Network models of sensory and memory circuits in isolation, implementing efficient coding and ring-attractor dynamics, respectively. (<bold>A</bold>) Schematics of columnar architecture for orientation selectivity. Neurons in the same column have similar preferred orientations, and recurrent connections are a combination of local excitation and global inhibition, represented as triangles and circles, respectively. (<bold>B</bold>-<bold>F</bold>) Connectivity and tuning properties of the sensory network (left column) and memory network (right column). (<bold>B</bold>) Example connectivity strengths. We indexed neurons by <italic>ψ</italic> ranging uniformly from 0° to 180°. The connectivity strengths depend only on <italic>ψ</italic>’s of the presynaptic and postsynaptic neurons. Each curve shows the connectivity strengths from presynaptic neuron <italic>ψ</italic> to an example postsynaptic neuron. Unlike the homogeneous connectivity in the memory network (right), the sensory connectivity is heterogeneous, and its degree is denoted by <italic>α</italic>. (<bold>C</bold>) Heterogeneous tuning curves for different stimulus <italic>θ</italic> in the sensory network in the stimulus period (left) and homogeneous ones in the memory network in the delay period (right). The memory network can sustain persistent activity in isolation, while the sensory network cannot. (<bold>D</bold>) Histograms of the preferred orientations. We measured the maximum of the tuning curve of each neuron, denoted as <inline-formula><inline-graphic xlink:href="566396v4_inline37.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<bold>Methods</bold>). The heterogeneous sensory network has more cardinally tuned neurons. (<bold>E</bold>) Widths of tuning curves measured at the half maximum of the tuning curves (<bold>Methods</bold>). The sensory tuning curves sharpen around cardinal orientations. Each neuron is labeled with its index <italic>ψ</italic> as in (<bold>B</bold>). (<bold>F</bold>) Neural manifolds projected onto the first two principal components of activities during the stimulus period (left) and during the delay period (right). The neural manifold of the sensory network resembles a curved ellipsoid, while the manifold corresponding to the homogeneous memory network is a perfect ring.</p></caption>
<graphic xlink:href="566396v4_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>For sensory-memory interacting networks, we connected the two modules with inter-module connections set to be stronger between neurons with similar orientation selectivity (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). Activity profiles in both modules follow that of the sensory module — heterogeneous with narrower and denser tuning curves around cardinal orientations, leading to higher sensitivity (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). Such activity pattern is maintained even during the delay period when recurrent connections in the memory module support activities of both sensory and memory modules (<xref rid="fig4" ref-type="fig">Figure 4B</xref>, right). Note that while sensory activities convey stimulus information during the delay period, their overall firing rates are much lower than those during the stimulus period with weak interconnection strengths. Such low firing rates may lead to both positive and negative evidence of sustained activity in early sensory areas (<xref rid="c33" ref-type="bibr">Leavitt, Mendoza-Halliday et al. 2017</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Network model with interacting sensory and memory modules generates correct error patterns in delayed estimation tasks. (<bold>A</bold>) Schematic of two-module architecture. The sensory and memory modules are connected via feedforward and feedback connectivity to form a closed loop. The sensory module receives external input with orientation <italic>θ</italic> while internal representation is decoded from the memory module, denoted as <inline-formula><inline-graphic xlink:href="566396v4_inline38.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. (<bold>B</bold>) Tuning curves of sensory (upper panels) and memory (lower panels) modules at the end of the stimulus epoch (i.e., the beginning of the delay epoch; left panels) and during the delay period (right panels). Note that while both modules can sustain persistent activity in the delay period, the firing rates of the sensory module are significantly lower than those in the stimulus period (upper right). (<bold>C-E</bold>) Bias (<bold>C</bold>), standard deviation (SD; <bold>D</bold>), and Fisher information (FI; <bold>E</bold>) patterns. Error patterns evaluated at 1, 2.5, and 4 seconds into the delay are consistent with the characteristic patterns observed experimentally in delayed estimation tasks (<xref rid="fig1" ref-type="fig">Figure 1A-C</xref>). However, the low SD right after the stimulus offset in (<bold>D</bold>) deviates from error patterns seen in perception tasks (see <bold>Discussion</bold>). While FI decays due to noise accumulation, it is largest around cardinal orientations, corresponding to a smaller discrimination threshold (<bold>E</bold>). In (<bold>C</bold>) and (<bold>D</bold>), shaded areas mark the ±s.e.m. of 1000 realizations.</p></caption>
<graphic xlink:href="566396v4_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>When the internal representation of the orientation stimulus is read from the memory module using a population vector decoder mimicking Bayesian optimal readout (<xref ref-type="bibr" rid="c15">Fischer 2010</xref>), the sensory-memory interacting network exhibits repulsive bias and minimum variance at cardinal orientations, inheriting from efficient sensory coding (<xref rid="fig4" ref-type="fig">Figure 4C</xref>,<bold>D</bold>). Similar error patterns were observed when decoded from activities of the sensory module (<xref rid="fig4_S1" ref-type="fig">Figure 4 – Figure supplement 1</xref>). Such bias increases during the delay period with increasing asymmetry of tuning widths despite lower firing rates than the stimulus period (<xref rid="fig4_S2" ref-type="fig">Figure 4 – Figure supplement 2</xref>). At the same time, errors gradually increase due to noise accumulation in time, as in typical memory networks (<xref rid="c10" ref-type="bibr">Compte, Brunel et al. 2000</xref>, <xref ref-type="bibr" rid="c7">Burak and Fiete 2012</xref>). Note that the variance of errors is negligible during stimulus presentation when the external input overwhelms internal noise, which may not fully account for the variability observed during perception tasks (see <bold>Discussion</bold>). We obtained Fisher information measuring sensitivity at each orientation from the neural responses (see <bold>Methods</bold>). Opposite to the variance of errors, Fisher information is highest at cardinal orientations, while it decreases during the delay period (<xref rid="fig4" ref-type="fig">Figure 4E</xref>). Thus, the sensory-memory interacting network model that mechanistically embodies the extension of the Bayesian sensory model correctly reproduces the error patterns observed in delayed estimation tasks.</p>
</sec>
<sec id="s2d">
<title>Analysis of low-dimensional memory states</title>
<p>To further understand the mechanisms of generating the correct error patterns in sensory-memory interacting networks, we analyzed the network dynamics during the delay period. For this, we identified the low-dimensional manifold that has slow dynamics during the delay period, which corresponds to the memory states (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). We projected the dynamics along this manifold to obtain the drift and diffusion terms (<xref rid="fig5" ref-type="fig">Figure 5A-C</xref>; <xref rid="fig5_S1" ref-type="fig">Figure 5 – Figure supplement 1</xref>). The drift term shows similar patterns to cardinal repulsion (<xref rid="fig5" ref-type="fig">Figure 5B,E</xref>). Integrating this drift for orientation yields the energy function, which is minimum at the obliques (<xref rid="fig5" ref-type="fig">Figure 5D</xref>). This suggests that the network implements discrete attractor dynamics with attractors formed at the obliques. The diffusion term is also uneven — the noise amplitude is maximum at the obliques so that despite attraction toward them, the variance of errors can be maximum (<xref rid="fig5" ref-type="fig">Figure 5C,F</xref>). Note that while we use Poisson noise in all units to replicate neuronal spike variability, the pattern of noise coefficients remains unchanged even with constant Gaussian noise (<xref rid="fig5_S2" ref-type="fig">Figure 5– Figure supplement 2</xref>). This lower variance near cardinal orientations arises from more dispersed representations of stimuli, as the noise coefficient is inversely proportional to the distance between stimulus representations (<xref ref-type="disp-formula" rid="eqn21">Eq. 21</xref>). Thus, the nonuniform characteristics of both drift and diffusion processes stem from the heterogeneous connections within the sensory module and align with the solution identified in low-dimensional memory models (<xref rid="fig1" ref-type="fig">Figure 1J-L</xref>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Low-dimensional dynamics along memory manifold and their dependence on heterogeneity degrees in the sensory module. (<bold>A</bold>) Low-dimensional projection along the memory states. Left panel: The memory manifold projected to the first two PCs associated with the vector fields. Right panel: Example drift-diffusion trajectories along the memory manifold starting at <italic>θ</italic> = 112.5<sup>°</sup>. (<bold>B</bold>,<bold>C</bold>) Velocity (<bold>B</bold>) and noise coefficients (<bold>C</bold>) corresponding to drift and diffusion processes. Different grey scales represent different heterogeneity degrees in the sensory module, <italic>α</italic> in <xref rid="fig3" ref-type="fig">Figure 3B</xref>. The velocity with which the remembered orientation drifts to the obliques in a noise-free network (<bold>B</bold>). A larger noise coefficient around the obliques overcomes the underlying drift dynamics and causes the standard deviation pattern to reach its maxima at the obliques (<bold>C</bold>). (<bold>D</bold>) Equivalent one-dimensional energy potential derived from the velocity in (<bold>B</bold>). (<bold>E</bold>,<bold>F</bold>) Example bias (<bold>E</bold>) and standard deviation (<bold>F</bold>) patterns at 4s into the delay. The shaded areas mark the ±s.e.m. of 1000 realizations.</p></caption>
<graphic xlink:href="566396v4_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Next, we examined how heterogeneity of connectivity in the sensory module affects the dynamics along the memory states. The magnitude of heterogeneity is denoted as α, and larger α represents a larger asymmetry of connectivity strengths at cardinal and oblique orientations (<xref rid="fig3" ref-type="fig">Figure 3B</xref>, left). When α increases, the asymmetry of drift and energy levels becomes more prominent, leading to a more rapid increase in bias (<xref rid="fig5" ref-type="fig">Figure 5B,D,E</xref>). The diffusion term is also more asymmetric, compensating for stronger attraction to the obliques (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). Thus, for larger α, the variability of errors is still higher at the obliques (<xref rid="fig5" ref-type="fig">Figure 5F</xref>). Another important parameter influencing error patterns is the intermodal connectivity strengths (<xref rid="fig6" ref-type="fig">Figure 6</xref>). Similar to the effect of increasing α, increases in feedforward or feedback strengths cause the energy levels to become more asymmetrical (<xref rid="fig6" ref-type="fig">Figure 6A,E</xref>), leading to a larger bias (<xref rid="fig6" ref-type="fig">Figure 6B,F</xref>). Conversely, the noise coefficient is less affected (<xref rid="fig6" ref-type="fig">Figure 6C,G</xref>), and the variance of errors decreases as the drift force becomes stronger (<xref rid="fig6" ref-type="fig">Figure 6D,H</xref>). Note that bias and variance patterns depend on the product of feedforward and feedback connections, denoted as γ, such that for a fixed γ, the error patterns remain similar (<xref rid="fig6" ref-type="fig">Figure 6I,J</xref>). In sum, the bias and variability of errors are determined by the degree of heterogeneity in the sensory module (α) and intermodal connectivity strengths (γ) as both α and γ affect the asymmetry of drift term similarly, while the asymmetry of diffusion term is more strongly influenced by α (<xref rid="fig6" ref-type="fig">Figure 6K,L</xref>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><p>Error patterns and low-dimensional dynamics for different intermodal connectivity strengths. (<bold>A-J</bold>) Low-dimensional dynamics and error patterns with varying feedforward and feedback connection strengths, denoted by J<sub>f</sub> and J<sub>b</sub>. (<bold>K</bold>,<bold>L</bold>) Potential differences and noise coefficient indices comparing low-dimensional dynamics at cardinal and oblique orientations for changing Jb and heterogeneity degree, <italic>α</italic>. Increasing both feedforward (<bold>A-D</bold>) and feedback (<bold>E-H</bold>) connection strengths deepens the potential difference (<bold>A</bold>,<bold>E</bold>,<bold>K</bold>) and increases the bias (<bold>B</bold>,<bold>F</bold>), similar to the effects of <italic>α</italic> increases in <xref rid="fig5" ref-type="fig">Figure 5D,E</xref>. In contrast, the profile of noise coefficients is less affected (<bold>C</bold>,<bold>G</bold>,<bold>L</bold>) and the SD pattern gets flattened with stronger drift (<bold>D</bold>,<bold>H</bold>). Bias and SD patterns depend on the product of feedforward and feedback connection strengths (<bold>I</bold>,<bold>J</bold>). Bias and SD are estimated at 4s (<bold>B</bold>,<bold>D</bold>,<bold>F</bold>,<bold>H</bold>) or 1s (<bold>I</bold>,<bold>J</bold>) into the delay and shaded areas mark the ±s.e.m. of 1000 realizations.</p></caption>
<graphic xlink:href="566396v4_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2e">
<title>Importance of heterogeneously tuned inhibition</title>
<p>We showed that network models realizing sensory-memory interactions reproduce correct error patterns, where each module has a different connectivity structure. Previous work suggested that such a heterogeneous connection of the sensory system may arise from experience-dependent synaptic modification (<xref ref-type="bibr" rid="c36">Olshausen and Field 1996</xref>, <xref rid="c63" ref-type="bibr">Zylberberg, Murphy et al. 2011</xref>). For example, typical Hebbian learning is thought to potentiate connectivity strengths between neurons whose preferred stimuli are more frequently encountered. For orientations, cardinal directions are predominant in natural scenes. Thus, if experience-dependent learning occurs mainly at the excitatory synapses, the excitatory connections near cardinal orientations become stronger in the sensory module. This is opposite to the previously discussed case where the sensory module has the strongest connection at the obliques. With the strongest excitatory connections at cardinal orientations, the error patterns are reversed, resulting in cardinal attraction instead of repulsion, and the lowest variance occurs at the obliques.</p>
<p>Inhibitory synaptic connections can also be modified through learning (<xref rid="c53" ref-type="bibr">Vogels, Froemke et al. 2013</xref>, <xref rid="c28" ref-type="bibr">Khan, Poort et al. 2018</xref>, <xref rid="c32" ref-type="bibr">Larisch, Gonner et al. 2021</xref>). Here, we considered that experience-dependent learning exists in both excitatory and inhibitory pathways and similarly shapes their connectivity (<xref rid="fig7" ref-type="fig">Figure 7A</xref>). We assumed that excitatory and inhibitory connections are segregated and stronger near cardinal orientations (<xref rid="fig7" ref-type="fig">Figure 7B</xref>). We modulated the heterogeneity degree of both excitatory and inhibitory connections, denoted as α and β, respectively (<xref rid="fig7" ref-type="fig">Figure 7B-D</xref>). The ratio between α and β determines the direction and magnitude of bias and variance patterns (<xref rid="fig7" ref-type="fig">Figure 7C,D</xref>). For relatively larger α, the network shows cardinal attraction and minimum variance of errors at the obliques (<xref rid="fig7" ref-type="fig">Figure 7E</xref>). Reversely, for relatively larger β with stronger modulation in inhibitory connections, the network reproduced cardinal repulsion and minimum variance of errors at cardinal orientations, consistent with experiments (<xref rid="fig7" ref-type="fig">Figure 7F</xref>). With a larger difference between α and β, such patterns of bias and variance are potentiated and minimum Fisher information across orientations decreases, corresponding to memory loss (<xref rid="fig7" ref-type="fig">Figure 7C,D</xref>; <xref rid="fig7_S1" ref-type="fig">Figure 7 – Figure supplement 1</xref>). Thus, this emphasizes the important role of heterogeneously tuned inhibition in shaping the sensory response for higher precision at cardinal orientations and enabling the sensory-memory interacting network to generate correct error patterns.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><p>Stronger inhibitory synaptic modulation is required for correct error patterns. (<bold>A</bold>) Segregation of excitatory (blue) and inhibitory (red) synaptic pathways. (<bold>B</bold>) Example excitatory (left) and inhibitory (right) connectivity strengths of the sensory module. The heterogeneity degrees of excitatory and inhibitory connections are denoted by <italic>α</italic> and <italic>β</italic>, respectively. Unlike combined excitation and inhibition in <xref rid="fig3" ref-type="fig">Figure 3B</xref>, the connectivity strengths are maximal around cardinal orientations. (<bold>C</bold>,<bold>D</bold>) Bias with stimulus at 22.5<sup>°</sup> (<bold>C</bold>) and standard deviation (SD) index (<bold>D</bold>) estimated at 1s into the delay for different values of <italic>α</italic> and <italic>β</italic>. SD index compares the SD at the cardinal and oblique orientations (<bold>Methods</bold>). (<bold>E</bold>,<bold>F</bold>) Example bias (left) and SD (right) patterns when excitatory modulation overwhelms inhibitory modulation (<italic>α</italic> = 0.068, <italic>β</italic> = 0.04; <bold>E</bold>) and when inhibitory modulation is stronger (<italic>α</italic> = 0.03, <italic>β</italic> = 0.08; <bold>F</bold>). In (<bold>C</bold>) and (<bold>D</bold>), green (yellow) pentagrams mark the parameters used in (<bold>E</bold>) and (<bold>F</bold>). Stronger inhibitory modulation is required for correct bias and variance patterns (<bold>F</bold> and green regions in <bold>C</bold> and <bold>D</bold>). In (<bold>E</bold>) and (<bold>F</bold>), shaded areas mark the ±s.e.m. of 1000 realizations.</p></caption>
<graphic xlink:href="566396v4_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2f">
<title>Comparison to alternative circuit structures</title>
<p>So far, we have shown the sufficiency of sensory-memory interacting networks with different connectivity structures featuring heterogeneous-homogeneous recurrent connections within each module. Here, we explore whether such architecture is necessary by comparing its performance with alternative circuit structures for sensory-memory interactions. One candidate mechanism involves having the heterogeneous sensory network maintain memory with a long intrinsic time constant, similar to having autapses (Seung, Lee et al. 2000). However, this model fails to replicate the evolution of error patterns during the delay period as a long intrinsic time constant slows down the overall dynamics, thus hindering the evolution of error patterns (<xref rid="fig7_S2" ref-type="fig">Figure 7 – Figure Supplement 2</xref>). Alternatively, we focused on a two-module network with variations in connectivity structure. We assumed that sensory and memory modules still serve their distinctive functions, namely, sensory encoding and memory maintenance, with weak/strong recurrent connections in sensory/memory modules. On the other hand, the heterogeneity of connections in other circuits might differ as homogeneous-homogeneous, homogeneous-heterogeneous, and heterogeneous-heterogeneous connections for sensory-memory modules.</p>
<p>Circuits with homogeneous connections in both sensory and memory modules are similar to previous continuous attractor models for working memory, such that the energy landscape and noise amplitude are uniform for all orientations (<xref rid="fig1" ref-type="fig">Figure 1D-F</xref>). Such architecture is not suitable as it generates no bias in errors and flat variance patterns. This leaves the latter two types of configurations, which require heterogeneous connections within the memory module. With a strong recurrent connection within the memory module, its heterogeneous activity pattern dominates overall activities in sensory-memory interacting networks, which makes it analogous to an isolated memory module. Thus, we examined the property of the memory module alone, which can maintain memory while generating heterogeneous responses without connection to the sensory module (<xref rid="fig8" ref-type="fig">Figure 8</xref>).</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><p>Network model with memory module only cannot reproduce correct error patterns. (<bold>A</bold>) Schematics of one-module network with heterogeneous and strong recurrent connections that enable both efficient coding and memory maintenance. (<bold>B</bold>) Example tuning curves at the end of the stimulus epoch (left) and at 4s into the delay epoch (right). (<bold>C</bold>,<bold>D</bold>) Bias with stimulus at 22.5<sup>°</sup> (<bold>C</bold>) and standard deviation (SD) index (<bold>D</bold>) estimated at 1s into the delay for different heterogeneity degrees of excitatory and inhibitory connections, denoted by <italic>α</italic> and <italic>β</italic>. For the parameters that generate reasonable bias patterns, the SD index is always negative, which indicates that the SD pattern is inconsistent with experimental findings. (<bold>E</bold>) Bias (left), and SD (right) patterns in the delay. While the bias pattern is correct, the SD reaches maxima around cardinal orientations, unlike the experiments. In (<bold>C</bold>) and (<bold>D</bold>), the yellow pentagram marks the parameters used in (<bold>E</bold>).</p></caption>
<graphic xlink:href="566396v4_fig8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To generate the correct bias pattern, we assumed that excitatory and inhibitory pathways in the memory module are stronger near cardinal orientations, as we previously considered for the sensory module in the sensory-memory interacting network (<xref rid="fig8" ref-type="fig">Figure 8A,B</xref>). However, memory circuits with heterogeneous connections have problems in maintaining the information and reproducing correct error patterns (<xref rid="fig8" ref-type="fig">Figure 8C-E</xref>). First, memory circuits alone require fine-tuning of heterogeneity whose range generating a moderate drift speed is at least one order of magnitude smaller than that of the two-module network (<xref rid="fig8" ref-type="fig">Figure 8C,D</xref>). Deviation from this range results in a fast drift toward oblique orientations, leading to rapid loss of information during the delay period (<xref rid="fig7_S1" ref-type="fig">Figure 7 – Figure supplement 1</xref>). Second, despite the correct bias direction, the variance pattern is reversed such that the variance of errors is minimal at the oblique orientations (<xref rid="fig8" ref-type="fig">Figure 8E</xref>). Varying the heterogeneity in excitatory and inhibitory connections shows that such rapid drift and reversed error patterns are prevalent across different parameters (<xref rid="fig8" ref-type="fig">Figure 8C,D</xref>).</p>
<p>To understand why a heterogeneous memory circuit alone fails to reproduce correct error patterns, we compared its low-dimensional dynamics along the memory states to that of the sensory-memory interacting networks. For the network with a similar range of bias and variance on average, we compared their energy landscape and noise amplitude, which vary similarly in both networks with minimum energy level and maximum noise at the oblique orientations (<xref rid="fig9" ref-type="fig">Figure 9A-F</xref>). However, the energy difference between cardinal and oblique orientations in a single memory-circuit model is bigger than that in a sensory-memory interacting network (<xref rid="fig9" ref-type="fig">Figure 9C</xref>, left in <bold>G</bold>,<bold>H</bold>). In contrast, the difference in noise amplitude is smaller (<xref rid="fig9" ref-type="fig">Figure 9D-F</xref>, right in <bold>G</bold>,<bold>H</bold>). The attraction at the obliques is much stronger, leading to the correct bias patterns, but too rapid an increase. Also, smaller differences in noise amplitude cannot overcome strong drift dynamics, leading to the minimum variance of errors at the obliques and reversed variance patterns. Even for different types or levels of noise, such as Gaussian noise with varying amplitude, distinctive error patterns in one-module and two-module networks are maintained (<xref rid="fig9_S1" ref-type="fig">Figure 9 – Figure supplement 1</xref>).</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9.</label>
<caption><p>Comparison of low-dimensional dynamics between two-module and one-module network models. (<bold>A</bold>,<bold>B</bold>) Bias and standard deviation (SD) patterns of two-module (<bold>A</bold>) and one-module (<bold>B</bold>) networks, adapted from <xref rid="fig7" ref-type="fig">Figure 7F</xref> and <xref rid="fig8" ref-type="fig">Figure 8E</xref>, respectively. The averages of bias and SD over different <italic>θ</italic> at 4s into the delay are similar in the two networks. (<bold>C-F</bold>) Low-dimensional dynamics of two-module (black) and one-module (red) networks. In both networks, the energy potential (<bold>C</bold>), the distance between stimulus representation, <inline-formula><inline-graphic xlink:href="566396v4_inline39.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and its inverse determining noise coefficients (<bold>D</bold>,<bold>E</bold>; <xref ref-type="disp-formula" rid="eqn21">Eq. (21)</xref>), and the noise coefficients (<bold>F</bold>) exhibit similar profiles. However, the two-module network has a shallower potential (<bold>C</bold>) but larger heterogeneity in <inline-formula><inline-graphic xlink:href="566396v4_inline40.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and the noise coefficient profile (<bold>D-F</bold>). These differences make it possible for the SD to become smaller around cardinal orientations in the two-module network (right in <bold>A</bold>), while drift dynamics overwhelm and the SD pattern is opposite to that of the noise coefficient in the one-module network (right in <bold>B</bold>). (<bold>G</bold>,<bold>H</bold>) Potential difference (left) and index of noise coefficients (right) comparing low-dimensional dynamics at the cardinal and oblique orientations in two-module (<bold>G</bold>) and one-module (<bold>H</bold>) networks. The two-module network shows a smaller potential difference and more heterogeneous noise coefficients over a broad range of heterogeneity (see the color bars in <bold>G</bold> and <bold>H</bold>).</p></caption>
<graphic xlink:href="566396v4_fig9.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>For an intuitive understanding of how connectivity heterogeneity affects the degrees of asymmetry in drift and diffusion differently in one-module and two-module networks, consider a simple case where only the excitatory connection exhibits heterogeneity, the degree of which is denoted by α. For memory maintenance, the overall recurrent connections need to be strong enough to overcome intrinsic decay, simplified to w = 1. In the one-module network, α in the memory module causes deviations from perfect tuning, creating potential differences at cardinal and oblique orientations as 1± α. In the two-module network, with w = 1 fulfilled by the memory module, α in the sensory module acts as a perturbation. The effect of α is modulated by the intermodal connectivity strengths, denoted by γ, and potential differences at cardinal and oblique orientations can be represented as 1± γα. Thus, while a relatively large α leads to too fast drift in the one-module network, the drift speed in the two-module network could remain modest with small γ&lt;1. Conversely, even with small γ, the asymmetry of noise coefficients can be large enough to produce correct variance patterns because the noise coefficient is more strongly influenced by α in the two-module network (<xref rid="fig6" ref-type="fig">Figure 6</xref>). In sum, compared to a heterogeneous memory circuit alone, interactions between heterogeneous sensory and homogeneous memory modules are advantageous due to an additional degree of freedom, intermodal connectivity strengths, which allows better control of energy and noise difference at cardinal and oblique orientations.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>While higher association areas have long been considered as a locus of working memory (<xref rid="c42" ref-type="bibr">Roussy, Mendoza-Halliday et al. 2021</xref>, <xref ref-type="bibr" rid="c35">Mejias and Wang 2022</xref>), recent human studies found memory signals in early sensory areas, prompting a re-evaluation of their role in working memory (<xref ref-type="bibr" rid="c60">Xu 2020</xref>, <xref rid="c1" ref-type="bibr">Adam, Rademaker et al. 2022</xref>). Our work extends the traditional memory models (<xref ref-type="bibr" rid="c54">Wang 2001</xref>, <xref ref-type="bibr" rid="c29">Khona and Fiete 2022</xref>) with novel insights into the significance of stimulus-specific sensory areas. We showed how sensory-memory interactions can elucidate changes in the internal representation of orientation stimuli and their behavioral readout during memory tasks. The observed error patterns suggest that the network meets two demands simultaneously: efficient encoding that reflects natural statistics and memory maintenance for successful retrieval of stimuli after a delay. Achieving both demands for orientation stimuli conflicts in a one-module network. Efficient encoding necessitates asymmetrical connections, resulting in inconsistent bias and variance patterns and overly rapid drift in the one-module network unless fine-tuned. In contrast, connecting sensory and memory modules can generate error patterns correctly and with less need for fine-tuning heterogeneity for slow drift. Efficient coding of natural statistics in the sensory module underscores the role of inhibitory plasticity. Low-dimensional projection onto memory states reveals that drift and diffusion processes governing working memory dynamics closely resemble the bias and variance patterns derived under Bayesian sensory models. It also elucidates how the magnitudes of bias and variance change depending on the heterogeneity of sensory connections and intermodal connectivity strengths.</p>
<p>Our model makes testable predictions to differentiate two-module and one-module networks using perturbation, such as transcranial magnetic stimulation (TMS). Many studies have found that during the delay period, TMS can intervene with the feedforward signal from sensory areas through which working memory is consolidated (<xref rid="c51" ref-type="bibr">van de Ven, Jacobs et al. 2012</xref>) (but see (<xref rid="c1" ref-type="bibr">Adam, Rademaker et al. 2022</xref>) for mixed effects of TMS and related debate). Under such perturbations, the ability to maintain information in the memory module will not be affected due to strong recurrent connections in both two-module and one-module networks. However, we expect different effects on bias patterns — in the two-module network, the bias will stop systematically drifting towards the obliques, reducing systematic repulsion (<xref rid="fig10" ref-type="fig">Figure 10</xref>). This accompanies the nonincreasing heterogeneity of tuning curves after the disruption, marked by their tuning width indices (see <bold>Methods</bold>). In contrast, in the one-module network, perturbation does not incur changes in error patterns as memory activities are less dependent on the sensory module during the delay period. Thus, perturbation studies can be used to reveal the role of the sensory module in shaping the error patterns during working memory. Note that our model cannot predict the effects of distractors during working memory, as such effects do not experimentally lead to changes in error patterns (<xref rid="c40" ref-type="bibr">Rademaker, Chunharas et al. 2019</xref>). The effect of distractors and direct intervention in the inter-module connections may differ due to potential differences in the encoding of distractors compared to task-relevant stimuli. More advanced models are required to comprehensively understand the influence of distractors and the processing of ongoing visual stimuli or the storage of multiple stimuli.</p>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10.</label>
<caption><p>Effect of perturbations in sensory-memory interaction on error patterns. (<bold>A</bold>,<bold>B</bold>) Example bias (<bold>A</bold>) and standard deviation (<bold>B</bold>) patterns when we assumed that TMS is applied to interrupt the feedforward signal from 2.5s into the delay. Shaded areas mark the ±s.e.m. of 1000 realizations. (<bold>C</bold>,<bold>D</bold>) Evolution of bias with example cue orientation at <italic>θ</italic> = 18<sup>°</sup> (<bold>C</bold>) and the tuning width indices in the memory network (WI; <bold>C</bold>) representing the asymmetry of tuning widths at cardinal and oblique orientations (<bold>Methods</bold>). Two vertical dashed lines mark the end of the stimulus epoch and the beginning of TMS disruption, respectively. Solid and dashed curves correspond to with and without perturbations, respectively. Both bias (<bold>C</bold>) and WI (<bold>D</bold>) stop increasing when TMS is on (<bold>C</bold>,<bold>D</bold>).</p></caption>
<graphic xlink:href="566396v4_fig10.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Our work suggests biologically plausible network mechanisms for the previously postulated efficient coding and Bayesian inference principles, relating network connectivity to tuning properties and error patterns. Previous normative explanations for systematic bias observed in perception tasks also suggested possible neural substrates for efficient coding, such as asymmetrical gain, width, or density of tuning curves across stimulus features (<xref ref-type="bibr" rid="c17">Ganguli and Simoncelli 2014</xref>, <xref ref-type="bibr" rid="c56">Wei and Stocker 2015</xref>). Our work narrowed the mechanism to denser and narrower tuning curves at cardinal orientations, consistent with neurophysiological recordings in the visual cortex (<xref rid="c34" ref-type="bibr">Li, Peterson et al. 2003</xref>, <xref rid="c30" ref-type="bibr">Kreile, Bonhoeffer et al. 2011</xref>, <xref rid="c46" ref-type="bibr">Shen, Tao et al. 2014</xref>). We implemented a population vector decoder reflecting neuronal preferred orientations, which approximates Bayesian optimal readout (<xref ref-type="bibr" rid="c15">Fischer 2010</xref>). Compared to a previous work adapting efficient coding theories with static tuning curves to account for error patterns in working memory tasks (<xref ref-type="bibr" rid="c48">Taylor and Bays 2018</xref>), our extension to memory processes demonstrated how neural activities and behavior readout change dynamically during the delay period. Notably, recent work combined dynamic change of signal amplitude with static tuning curves to capture different time courses of estimation precision during sensory encoding and memory maintenance (<xref ref-type="bibr" rid="c49">Tomić and Bays 2023</xref>). Our network models embody such phenomenological models as the networks exhibit changes in overall firing rates after the stimulus offset.</p>
<p>Like our study, a few recent studies have employed attractor dynamics to explain dynamic error patterns observed for visual color memory (<xref rid="c37" ref-type="bibr">Panichello, DePasquale et al. 2019</xref>, <xref ref-type="bibr" rid="c38">Pollock and Jazayeri 2020</xref>, <xref ref-type="bibr" rid="c14">Eissa and Kilpatrick 2023</xref>). Behavior studies showed attractive bias and minimum variance around the prevalent colors, which one-module discrete attractor models could reproduce. However, these models cannot be generalized to other visual stimuli, such as orientations, spatial locations, or directions, of which the responses show repulsive bias away from the common stimuli (<xref ref-type="bibr" rid="c57">Wei and Stocker 2017</xref>). Also, a one-module network storing color memory requires fine-tuned heterogeneity for moderate drift speed. While the desired low-dimensional manifold and drift dynamics can be engineered in the one-module network (<xref ref-type="bibr" rid="c38">Pollock and Jazayeri 2020</xref>), its biological mechanism needs further investigation. The two-module network considered in our study also requires fine-tuning of homogeneity in the memory module and heterogeneity in the sensory module. However, the condition of asymmetrical connections in the sensory module is less stringent as they have a weaker influence on the entire dynamics than those in the memory module. Fine-tuning of homogeneous connections in the memory module can be mediated through activity-dependent plasticity, such as short-term facilitation (<xref rid="c27" ref-type="bibr">Itskov, Hansel et al. 2011</xref>, <xref ref-type="bibr" rid="c24">Hansel and Mato 2013</xref>, <xref rid="c44" ref-type="bibr">Seeholzer, Deger et al. 2019</xref>) or long-term plasticity (Renart, Song et al. 2003, <xref ref-type="bibr" rid="c22">Gu and Lim 2022</xref>). Also, recent work showed that continuous attractors formed under unstructured, heterogeneous connections are robust against synaptic perturbations (<xref ref-type="bibr" rid="c11">Darshan and Rivkind 2022</xref>). Thus, the two-module networks can control the drift speed better with possible additional mechanisms that promote homogeneous memory states. It needs further exploration whether they can be generalized to other stimuli like color, possibly involving additional categorical structures (<xref rid="c26" ref-type="bibr">Hardman, Vergauwe et al. 2017</xref>, <xref rid="c39" ref-type="bibr">Pratte, Park et al. 2017</xref>).</p>
<p>Our current study is limited to the dynamic evolution of memory representation for a single orientation stimulus and its associated error patterns, which does not capture nuanced error patterns in broader experimental settings (<xref ref-type="bibr" rid="c23">Hahn and Wei 2024</xref>). For instance, while shorter stimulus presentations with no explicit delay led to larger biases experimentally, our current model, which starts activities from a flat baseline, shows an increase in bias throughout the stimulus presentation (<xref rid="c12" ref-type="bibr">de Gardelle, Kouider et al. 2010</xref>). Additionally, the error variance during stimulus presentation is almost negligible compared to that during the delay period, as the external input overwhelms the internal noise. These mismatches during stimulus presentation have minimal impact on activities during the delay period when the internal dynamics dominate. Nonetheless, the model needs further refinement to accurately reproduce activities during stimulus presentation, possibly by incorporating more biologically plausible baseline activities. Also, a recent Bayesian perception model suggested different types of noise like external noise or variations in loss functions that adjust tolerance to small errors may help explain various error patterns observed across different modalities (<xref ref-type="bibr" rid="c23">Hahn and Wei 2024</xref>). Even for memories involving multiple items, noise can be critical in determining error patterns, as encoding more items might cause higher noise for each individual item (<xref rid="c8" ref-type="bibr">Chunharas, Rademaker et al. 2022</xref>).</p>
<p>The modularity structure in the brain is thought to be advantageous for fast adaptation to changing environments (<xref ref-type="bibr" rid="c47">Simon 1995</xref>, <xref rid="c9" ref-type="bibr">Cole, Reynolds et al. 2013</xref>, <xref ref-type="bibr" rid="c16">Frankland and Greene 2020</xref>). Recent works showed that recurrent neural networks trained for multiple cognitive tasks form clustered neural activities and modular dynamic motifs to repurpose shared functions for flexible computation (<xref rid="c61" ref-type="bibr">Yang, Joglekar et al. 2019</xref>, <xref rid="c13" ref-type="bibr">Driscoll, Shenoy et al. 2022</xref>). Resonant with these computational findings, an fMRI study showed that shared representation across distinct visual stimuli emerges during the delay period (<xref ref-type="bibr" rid="c31">Kwak and Curtis 2022</xref>). Although our work focuses on a single task, it highlights the necessity of having dedicated sensory and memory modules, and a memory module with ring geometry can be repurposed for various visual stimuli such as motion, spatial location, and color. It is reminiscent of the flexible working memory model, which proposes connections between multiple sensory modules and a control module (<xref ref-type="bibr" rid="c5">Bouchacourt and Buschman 2019</xref>). However, a key distinction lies in the role of the control module. Unlike the flexible working memory model that loses memory without sensory-control interactions, our work suggests that the memory module can independently maintain memory, while interaction with the sensory module continuously shapes the internal representation, potentially consolidating prior beliefs regarding natural statistics. The sensory-memory interaction and network architecture derived from dynamic changes of single stimulus representation can be a cornerstone for future studies in more complex conditions, such as under the stream of visual inputs (<xref ref-type="bibr" rid="c60">Xu 2020</xref>, <xref rid="c1" ref-type="bibr">Adam, Rademaker et al. 2022</xref>) or with high or noisy memory loads (<xref rid="c3" ref-type="bibr">Bays, Schneegans et al. 2022</xref>).</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Low-dimensional attractor models</title>
<p>To illustrate error patterns in different low-dimensional attractor models shown in <xref rid="fig1" ref-type="fig">Figure 1</xref>, we considered a one-dimensional stochastic differential equation given as
<disp-formula id="eqn1">
<graphic xlink:href="566396v4_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>θ</italic><sub><italic>t</italic></sub> and <italic>W</italic><sub><italic>t</italic></sub> are orientation and standard Brownian motion at time t. We assumed that the drift and noise coefficients <italic>μ</italic> and <italic>σ</italic> only depend on <italic>θ</italic><sub><italic>t</italic></sub> where <inline-formula><inline-graphic xlink:href="566396v4_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> with diffusion coefficient 𝒟.</p>
<p>For continuous attractor models in <xref rid="fig1" ref-type="fig">Figure 1D-F</xref>, <italic>μ</italic> and <italic>σ</italic> were set to be constant as <italic>μ</italic> = 0 and <italic>σ</italic> = 2°. For discrete attractor models in <xref rid="fig1" ref-type="fig">Figure 1G-L</xref>, we assumed that the energy function <italic>U</italic>(<italic>θ</italic><sub><italic>t</italic></sub>) is proportional to cos(4<italic>θ</italic><sub><italic>t</italic></sub>) (<xref rid="fig1" ref-type="fig">Figure 1G,J</xref>) so that the drift term <italic>μ</italic>(<italic>θ</italic><sub><italic>t</italic></sub>) = sin(4<italic>θ</italic><sub><italic>t</italic></sub>) with <inline-formula><inline-graphic xlink:href="566396v4_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. In these attractor models, the constant noise in <xref rid="fig1" ref-type="fig">Figure 1G-I</xref> is <italic>σ</italic> = 2° and the nonuniform noise in <xref rid="fig1" ref-type="fig">Figure 1J-L</xref> is <italic>σ</italic> = 2°(1 − cos(4<italic>θ</italic><sub><italic>t</italic></sub>)). The biases and standard deviation (SD) of errors were plotted at T = 1, 2, and 3 with 50,000 iterations. For the numerical simulation, dt = 0.01.</p>
</sec>
<sec id="s4b">
<title>Bayesian sensory models and extension</title>
<p>In <xref rid="fig2" ref-type="fig">Figure 2</xref>, we first constructed the sensory inference process, which receives orientation input <italic>θ</italic>, forms a corresponding noisy sensory representation <italic>m</italic> given <italic>θ</italic>, and then infers <inline-formula><inline-graphic xlink:href="566396v4_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula> as an estimate of the input orientation from the encoded representation <italic>m</italic>. This inference is made in a Bayesian manner based on likelihood function <italic>p</italic>(<italic>m</italic>|<italic>θ</italic>) and orientation prior <italic>q</italic>(<italic>θ</italic>).</p>
<p>To construct <italic>p</italic>(<italic>m</italic>|<italic>θ</italic>), we followed the procedure given in (<xref ref-type="bibr" rid="c56">Wei and Stocker 2015</xref>), and the summary is as follows. We started from the sensory space of <inline-formula><inline-graphic xlink:href="566396v4_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where both discriminability and Fisher Information <inline-formula><inline-graphic xlink:href="566396v4_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are uniform, and all likelihood functions <inline-formula><inline-graphic xlink:href="566396v4_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are homogeneous von Mises functions. And since <italic>J</italic>(<italic>θ</italic>) ∝ (<italic>q</italic>(<italic>θ</italic>))<sup>2</sup> under the efficient coding condition, the sensory space of <inline-formula><inline-graphic xlink:href="566396v4_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and the stimulus space of <italic>θ</italic> can be mapped by forward and backward mappings <italic>F</italic>(<italic>θ</italic>) and <inline-formula><inline-graphic xlink:href="566396v4_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>F</italic>(<italic>θ</italic>) is the cumulative distribution function of prior<italic>q</italic>(<italic>θ</italic>). Thus, likelihood functions <italic>p</italic>(<italic>m</italic>|<italic>θ</italic>) can be obtained by taking homogeneous von Mises likelihoods in the sensory space and transforming them back to the stimulus space using <italic>F</italic><sup>−1</sup>. To sum up the upper half of the procedural diagram in <xref rid="fig2" ref-type="fig">Figure 2A</xref>, the sensory module receives <italic>θ</italic>, encodes it in <italic>m</italic> following <italic>p</italic>(<italic>m</italic>|<italic>θ</italic>), and decodes <inline-formula><inline-graphic xlink:href="566396v4_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula> using likelihood functions and prior <italic>q</italic>(<italic>θ</italic>).</p>
<p>As an extension to include a memory process, the decoded <inline-formula><inline-graphic xlink:href="566396v4_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is passed on to the memory module, where <inline-formula><inline-graphic xlink:href="566396v4_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is maintained with the addition of memory noise <italic>ξ</italic>. The output of the memory module, <inline-formula><inline-graphic xlink:href="566396v4_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, is fed back to the sensory module as the new input. This completes one iteration of sensory-memory interaction. The whole process is then repeated recursively, resulting in increased biases and standard deviations in the <italic>θ</italic> statistics at subsequent iterations (call them <italic>θi</italic> for the input of iteration <italic>i</italic>).</p>
<p>For <xref rid="fig2" ref-type="fig">Figure 2B-C</xref>, we set the von Mises sensory-space likelihoods to be <inline-formula><inline-graphic xlink:href="566396v4_inline15.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, with <italic>km</italic> = 250. These likelihood functions are transformed by <inline-formula><inline-graphic xlink:href="566396v4_inline16.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>q</italic>(<italic>θ</italic>) = 3 + cos (4<italic>θ</italic>). Each internal representation <italic>mm</italic> is sampled from <italic>p</italic>(<italic>m</italic>|<italic>θ</italic>), after which <inline-formula><inline-graphic xlink:href="566396v4_inline17.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is estimated as the mean of the posterior <italic>p</italic>(<italic>θ</italic>|<italic>m</italic>)<italic>q</italic>(<italic>θ</italic>). With the parameters chosen above, the inferred samples of <inline-formula><inline-graphic xlink:href="566396v4_inline18.gif" mime-subtype="gif" mimetype="image"/></inline-formula> after the first sensory iteration have a circular standard deviation of <italic>σ</italic><sub><italic>θ</italic></sub> ≈ 1.3° at cardinal orientations. To have comparable memory and sensory noise levels, we set the memory noise as <italic>ξ</italic>∼<italic>𝒩</italic>(0, (1.3°)<sup>2</sup>) which is added on top of the sensory outputs. Thus, the memory outputs of the first iteration <inline-formula><inline-graphic xlink:href="566396v4_inline19.gif" mime-subtype="gif" mimetype="image"/></inline-formula> have a standard deviation of 1.84o at the cardinals. The first three iterations’ memory output statistics are plotted in <xref rid="fig2" ref-type="fig">Figure 2C</xref>, i.e., bias(<italic>θ</italic><sub>1</sub>), bias(<italic>θ</italic><sub>2</sub>), bias(<italic>θ</italic><sub>3</sub>), and SD(<italic>θ</italic><sub>1</sub>), SD(<italic>θ</italic><sub>2</sub>), SD(<italic>θ</italic><sub>3</sub>). The statistics were computed from 10,000 iterations of the simulation. The magnitude of biases and standard deviations vary for different sensory or memory noise levels, while the overall patterns and the increasing temporal trend are unchanged (not shown).</p>
</sec>
<sec id="s4c">
<title>Firing rate models</title>
<p>For network models, we considered sensory circuits with heterogeneous connections (<xref rid="fig3" ref-type="fig">Figure 3</xref>), memory circuits with homogeneous connections (<xref rid="fig3" ref-type="fig">Figure 3</xref>) and heterogeneous connections (<xref rid="fig8" ref-type="fig">Figure 8</xref>,<xref rid="fig9" ref-type="fig">9</xref>), and sensory-memory interacting circuits (<xref rid="fig4" ref-type="fig">Figure 4</xref>-<xref rid="fig7" ref-type="fig">7</xref>, <xref rid="fig9" ref-type="fig">9</xref>, <xref rid="fig10" ref-type="fig">10</xref>). In all cases, the activities of neurons are described by their firing rates and synaptic states, denoted by <italic>r</italic> and <italic>s</italic>. For columnar structure encoding orientation stimuli, we indexed the neurons by uniformly assigning them indices <inline-formula><inline-graphic xlink:href="566396v4_inline20.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for <italic>i</italic> from 1 to <italic>N</italic> where <italic>N</italic> is the number of neurons in each population. For sensory or memory networks alone, the dynamics of neuron <italic>i</italic> are described by the following equations,
<disp-formula id="eqn2">
<graphic xlink:href="566396v4_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the superscripts <italic>i</italic> and <italic>j</italic> are the neuronal indices, and the subscript <italic>K</italic> is either s or m, representing sensory or memory circuits. For the sensory-memory interacting network, the dynamics are given as
<disp-formula id="eqn3">
<graphic xlink:href="566396v4_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where activities and synaptic inputs are represented in the vector and matrix multiplication form, shown in bold cases. The additional subscripts f and b represent feedforward and backward connections between sensory and memory modules.</p>
<p>In both <xref ref-type="disp-formula" rid="eqn2">Eqs. (2)</xref> and <xref ref-type="disp-formula" rid="eqn3">(3)</xref>, <italic>s</italic>(<italic>t</italic>) is the low pass filtered <italic>r</italic>(<italic>t</italic>) with synaptic time constant τ and with the addition of <italic>ξ</italic> approximating Poisson noise. We modeled <italic>ξ</italic> as the Gaussian process with covariance <italic>ξ</italic><sup><italic>i</italic></sup>(<italic>t</italic>)<italic>ξ</italic><sup><italic>i</italic></sup>(<italic>t′</italic>) = <italic>r</italic><sup><italic>i</italic></sup>(<italic>t</italic>)<italic>δi δ</italic>(<italic>t</italic> − <italic>t′</italic>), following (<xref ref-type="bibr" rid="c7">Burak and Fiete 2012</xref>). We assumed that the rate dynamics are relatively fast such that <italic>r</italic>(<italic>t</italic>) equals the input current-output rate transfer function <italic>f</italic>. The input current is the sum of external input <italic>I</italic>ext and the synaptic currents from other neurons in the network, which are the postsynaptic states <italic>s</italic><sup><italic>j</italic></sup> weighted by synaptic strengths <italic>W</italic><sup><italic>ij</italic></sup>. The transfer function <italic>f</italic> has the Naka–Rushton form (<xref ref-type="bibr" rid="c58">Wilson 1999</xref>) given as
<disp-formula id="eqn4">
<graphic xlink:href="566396v4_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where [·]<sub>+</sub> denotes the linear rectification function. The transfer functions differ in the sensory and memory modules, denoted as <italic>f</italic><sub>s</sub> and <italic>f</italic><sub>m</sub>, respectively.</p>
</sec>
<sec id="s4d">
<title>Synaptic inputs in network models</title>
<p>Note that for all network models, we only considered excitatory neurons under the assumption that the inhibitory synaptic pathways have relatively fast dynamics. Thus, recurrent connectivity strengths, <italic>W</italic><sub>s</sub> and <italic>W</italic><sub>m</sub> within sensory and memory modules, reflect summed excitation and inhibition, and thus, can have either positive or negative signs. On the other hand, we assumed that intermodal interactions, <italic>W</italic><sub>f</sub> and <italic>W</italic><sub>b</sub>, are dominantly excitatory and, thus, can be only positive.</p>
<p>All <italic>W</italic>’s can be defined using neuronal indices of post- and presynaptic neurons as
<disp-formula id="eqn5">
<graphic xlink:href="566396v4_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
For <italic>W</italic><sub>s</sub> without segregating excitation and inhibition in <xref rid="fig3" ref-type="fig">Figure 3</xref>-<xref rid="fig6" ref-type="fig">6</xref>, <italic>N</italic> is the population size of sensory module, <italic>N</italic><sub>s</sub>, and <italic>J</italic><sub>s</sub> is the sum of a constant global inhibition and a short-range excitatory connection as
<disp-formula id="eqn6">
<graphic xlink:href="566396v4_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>where <italic>α</italic> &gt; 0 represents the heterogeneity degree of excitatory connectivity, and <italic>λ</italic><sub>E</sub> is the width of local excitatory connections.</p>
<p>When we segregated excitation and inhibition and considered the heterogeneity of inhibitory connection in <xref rid="fig7" ref-type="fig">Figure 7</xref>-<xref rid="fig10" ref-type="fig">10</xref>, <xref ref-type="disp-formula" rid="eqn6">Eq. (6)</xref> is replaced with
<disp-formula id="eqn7">
<graphic xlink:href="566396v4_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>β</italic> &gt; 0 is the degree of heterogeneity of inhibitory connections. Note the signs of modulation change in <xref ref-type="disp-formula" rid="eqn6">Eqs. (6)</xref> and <xref ref-type="disp-formula" rid="eqn7">(7)</xref> such that when only excitation is modulated in <xref ref-type="disp-formula" rid="eqn6">Eq. (6)</xref>, the connectivity strengths near the obliques are strong. In contrast, when excitation and inhibition are both modulated in <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>, the connectivity strengths near cardinal orientations are strong.</p>
<p>For the memory module, <italic>N</italic> is the population size of the memory module, <italic>N</italic><sub>m</sub> in <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>. Without heterogeneity in <xref rid="fig3" ref-type="fig">Figure 3</xref>-<xref rid="fig7" ref-type="fig">7</xref> and <xref rid="fig10" ref-type="fig">10</xref>, <italic>J</italic><sub>m</sub> is defined as
<disp-formula id="eqn8">
<graphic xlink:href="566396v4_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
In contrast, for the one-module network model in <xref rid="fig8" ref-type="fig">Figure 8</xref>, the connectivity of the memory module is heterogeneous, as in the sensory module in <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>, and is defined as
<disp-formula id="eqn9">
<graphic xlink:href="566396v4_eqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The feedforward and feedback connectivity are similarly defined as
<disp-formula id="eqn10">
<graphic xlink:href="566396v4_eqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Note the connectivity strength is normalized by the size of the presynaptic population so that the total synaptic current remains the same for different population sizes.</p>
<p>For the external inputs with orientation <italic>θ, I</italic><sub>ext,s</sub> in the sensory module is modeled as
<disp-formula id="eqn11">
<graphic xlink:href="566396v4_eqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>ε</italic> ϵ (0,0.5] determines the stimulus tuning of the input, <italic>λ</italic><sub>ext,s</sub> determines the width, and <italic>C</italic> describes the contrast (<xref ref-type="bibr" rid="c25">Hansel and Sompolinsky 1998</xref>).</p>
<p>For the memory network not connected to the sensory module in <xref rid="fig3" ref-type="fig">Figure 3</xref> and <xref rid="fig8" ref-type="fig">8</xref>, we assumed stimulus-specific input as
<disp-formula id="eqn12">
<graphic xlink:href="566396v4_eqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>Ic</italic>,m is a constant background input. When the memory module receives the inputs from the sensory population in <xref rid="fig4" ref-type="fig">Figure 4</xref>-<xref ref-type="disp-formula" rid="eqn7">7</xref> and <xref ref-type="disp-formula" rid="eqn10">10</xref>, we assumed <inline-formula><inline-graphic xlink:href="566396v4_inline21.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is constant as <italic>Ic</italic>,m.</p>
</sec>
<sec id="s4e">
<title>Analysis of network activities</title>
<p>We used population vector decoding to extract the internal representation of orientation and quantified how such representation deviated from the original stimulus. We also examined how tuning properties and Fisher information change during the delay period.</p>
<p>Note that while we indexed neurons uniformly with <italic>ψ</italic><sub><italic>i</italic></sub> between 0<sup>°</sup> and 180<sup>°</sup>, the maximum of the tuning curve of neuron <italic>ψ</italic><sub><italic>i</italic></sub> can change dynamically and differ from <italic>ψ</italic><sub><italic>i</italic></sub>. We defined the preferred feature (PF) of neuron <italic>i</italic> as the maximum of its tuning curve when the tuning curve reaches a steady state in the presence of external input. For numerical estimation, we set the stimulus-present encoding epoch to 5 seconds to obtain the steady states of tuning curves. The tuning width is given as the full width at half maximum (FWHM) of the tuning curve. To estimate PF and FWHM, we did a cubic spline interpolation to increase the number of sample orientations to 1000. The tuning width index (WI) is given as
<disp-formula id="eqn13">
<graphic xlink:href="566396v4_eqn13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
To estimate the internal representation of orientation in the network models, denoted as <inline-formula><inline-graphic xlink:href="566396v4_inline22.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, we utilized the population vector decoder (PVD) (Georgopoulos, Schwartz et al. 1986)
<disp-formula id="eqn14">
<graphic xlink:href="566396v4_eqn14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>N</italic> denotes the number of neurons and <inline-formula><inline-graphic xlink:href="566396v4_inline23.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denotes the PF of neuron <italic>j</italic>. The orientation is always decoded from the memory network tuning curves <italic>r</italic><sub>m</sub>(<italic>t</italic>) except for <xref rid="fig10" ref-type="fig">Figure 10A</xref>. The estimation bias <inline-formula><inline-graphic xlink:href="566396v4_inline24.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Since the bias is typically small enough, we computed the estimation standard deviation (SD) as the SD of bias using linear statistics. The SD index is defined as
<disp-formula id="eqn15">
<graphic xlink:href="566396v4_eqn15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The Fisher information (FI) is estimated by assuming that the probability density function <italic>p</italic>(<italic>r</italic> ∣ <italic>θ</italic>) is Gaussian as
<disp-formula id="eqn16">
<graphic xlink:href="566396v4_eqn16.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Where <inline-formula><inline-graphic xlink:href="566396v4_inline25.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denotes the variance of the firing rate of memory neuron <italic>i</italic>. Thus, we can estimate the FI of memory neuron <italic>i</italic> based on the empirical mean and variance of the firing rate at time <italic>t</italic> as
<disp-formula id="eqn17">
<graphic xlink:href="566396v4_eqn17.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>and the total FI is the summation of the FI of all memory neurons, given as FI(<italic>t</italic>) = Σ<italic>i</italic> FI (<italic>ψ</italic><sub>i</sub>, <italic>t</italic>).</p>
</sec>
<sec id="s4f">
<title>Drift and diffusivity in network models</title>
<p>Although the modulation breaks the continuity of the ring attractor and forms two discrete attractors at the obliques, there is still a one-dimensional trajectory <inline-formula><inline-graphic xlink:href="566396v4_inline26.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to which the noise-free dynamics quickly converge. We can linearize the system in the vicinity of this trajectory if the noise is small (<xref ref-type="bibr" rid="c7">Burak and Fiete 2012</xref>). Note that the dynamics of the synaptic variables in <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref> can be put into the following form
<disp-formula id="eqn18">
<graphic xlink:href="566396v4_eqn18.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and by linearizing around the stable trajectory <inline-formula><inline-graphic xlink:href="566396v4_inline27.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, we get
<disp-formula id="eqn19">
<graphic xlink:href="566396v4_eqn19.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we have ignored the zeroth-and higher-order terms. The drift velocity <italic>μ</italic>(<italic>θ</italic>) is estimated by projecting the noise-free dynamics along the normalized right eigenvector <italic>u</italic> of <italic>K</italic> with the largest real part of the eigenvalue
<disp-formula id="eqn20">
<graphic xlink:href="566396v4_eqn20.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The coefficient of diffusion can be obtained in the same way
<disp-formula id="eqn21">
<graphic xlink:href="566396v4_eqn21.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The noise coefficient is given as <inline-formula><inline-graphic xlink:href="566396v4_inline28.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Hence, we have reduced the high-dimensional dynamics to a simple one-dimensional stochastic differential equation as in <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref> as
<disp-formula id="ueqn1">
<graphic xlink:href="566396v4_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>and the potential <italic>U</italic>(<italic>θ</italic>) is obtained by the relation <inline-formula><inline-graphic xlink:href="566396v4_inline29.gif" mime-subtype="gif" mimetype="image"/></inline-formula> heterogeneity of noise coefficient across different orientations, we define the noise coefficient index as follows:
<disp-formula id="eqn22">
<graphic xlink:href="566396v4_eqn22.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s4g">
<title>Network parameters and simulations</title>
<p>Unless otherwise specified, <italic>N</italic><sub>s</sub> = <italic>N</italic><sub>m</sub> = 300, τ = 10 ms. The connectivity parameters are <italic>J</italic><sub>E,s</sub> = 0.6, <italic>J</italic><sub>I,s</sub> = 0.35, <italic>J</italic><sub>E,m</sub> = 1, <italic>J</italic><sub>I,m</sub> = 0.17, <italic>J</italic><sub>f</sub> = 0.1, <italic>J</italic><sub>b</sub> = 0.25, <italic>λ</italic><sub>E,s</sub> = 0.36<italic>π, λ</italic><sub>I,s</sub> = 1.1<italic>π, λ</italic><sub>E,m</sub> = 0.2<italic>π</italic>, λ<sub>I,m</sub> = 0.6<italic>π, λ</italic><sub>f</sub> = <italic>λ</italic><sub>b</sub> = 0.17<italic>π</italic>. For the external input, we set <italic>C</italic> = 4, <italic>ε</italic> = 0.2, and <italic>λ</italic><sub>ext,s</sub> = 0.3<italic>π</italic>. For the modulation of the sensory network, unless otherwise specified, we set <italic>α</italic> = 0.04 when only the excitatory plasticity is considered, and <italic>α</italic> = 0.03, <italic>β</italic> = 0.08 when the inhibitory plasticity is added. As for the modulation of the single-layer memory network, we set <italic>α</italic> = 5 × 10<sup>−4</sup>, <italic>β</italic> = 2.4 × 10<sup>−3</sup>. For the transfer function, <italic>f</italic><sub>max</sub> = 100, <italic>T</italic> = 0.1, <italic>q</italic> = 2, <italic>w</italic> = 6 for sensory <italic>f</italic><sub>s</sub>, and <italic>f</italic><sub>max</sub> = 100, <italic>T</italic> = 0.1, <italic>q</italic> = 1.5, <italic>w</italic> = 6.6 for memory <italic>f</italic><sub>m</sub>.</p>
<p>We uniformly sampled 50 cue orientations in [0<sup>°</sup>, 180<sup>°</sup>]. The visual cue lasts for 0.5 seconds except for the estimation of the PFs. In the grid parameter search figures, the delay epochs last for 1 s. In <xref rid="fig3" ref-type="fig">Figure 3</xref>, we set <italic>α</italic> = 0.07. In <xref rid="fig5" ref-type="fig">Figure 5A</xref>, the manifold corresponds to the synaptic variables at 4s into the delay with <italic>α</italic> = 0.05. We uniformly sampled 100 cue orientations for the manifold.</p>
<p>To compute the drift velocity and noise coefficient in <xref rid="fig5" ref-type="fig">Figure 5</xref>, <xref rid="fig6" ref-type="fig">6</xref>, and <xref rid="fig9" ref-type="fig">9</xref>, we use the stable trajectory <inline-formula><inline-graphic xlink:href="566396v4_inline30.gif" mime-subtype="gif" mimetype="image"/></inline-formula> at 1s into the delay to ensure the fast transient dynamics induced by stimulus offset fully decays. The stable trajectory is parameterized by the 50 cue orientations to numerically compute <inline-formula><inline-graphic xlink:href="566396v4_inline31.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p>All simulations of ordinary or stochastic differential equations of the network models were done using the Euler method with d<italic>t</italic> = 1 ms. We checked that similar results hold for smaller d<italic>t</italic>. Example bias and standard deviation patterns were estimated from 1000 independent realizations. The Fisher information patterns were estimated from 3000 independent realizations. The grid search of maximum bias at <italic>θ</italic> = 22.5<sup>°</sup> and standard deviation index were computed from 3000 realizations.</p>
<p>All simulations were run in Matlab. The code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/KYang-N/Cardinal-Repulsion.git">https://github.com/KYang-N/Cardinal-Repulsion.git</ext-link>.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We appreciate X. Wei for sharing the code for Bayesian inference models. J. Y. was supported by the NYU Shanghai Summer Undergraduate Research Program (SURP). S. L. received STI2030-Major Projects, No.2021ZD0203700/2021ZD0203705. H. Z. and S. L. also acknowledge the support of the Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning and the NYU-ECNU Institute of Brain and Cognitive Science at NYU Shanghai.</p>
</ack>
<sec id="s5">
<title>Competing Interests</title>
<p>The authors declare no competing financial interests.</p>
</sec>
<sec id="s6">
<title>Figure supplements</title>
<fig id="fig4_S1" position="float" fig-type="figure">
<label>Figure 4 – Figure supplement 1.</label>
<caption><p>Bias (<bold>A</bold>) and SD (<bold>B</bold>) patterns decoded from activities of sensory module. All parameters are the same as in <xref rid="fig4" ref-type="fig">Figure 4</xref>.</p></caption>
<graphic xlink:href="566396v4_fig4_S1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig4_S2" position="float" fig-type="figure">
<label>Figure 4 – Figure supplement 2.</label>
<caption><p>Dynamics of bias and tuning properties of sensory-memory interacting network models. (<bold>A</bold>) Maximum firing rate of <italic>ψ</italic> = 22.2<sup>°</sup> for all stimulus orientations. The vertical grey line represents the end of the stimulus presentation. Both sensory and memory modules show lower but sustained activities during the delay period. (<bold>B</bold>) Bias evolution to input orientation <italic>θ</italic> = 18<sup>°</sup>. The bias increases both in the stimulus and delay periods, while its increasing speed is reduced during the delay period. (<bold>C</bold>) Tuning width indices (WI) measuring the asymmetry of tuning widths at cardinal and oblique orientations (<bold>Methods</bold>). WI also increases in the whole process, indicating the tuning curves of the neural population become more heterogeneous. All parameters are the same as in <xref rid="fig4" ref-type="fig">Figure 4</xref>.</p></caption>
<graphic xlink:href="566396v4_fig4_S2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig5_S1" position="float" fig-type="figure">
<label>Figure 5 – Figure supplement 1.</label>
<caption><p>Comparison between bias and standard deviation (SD) patterns of the full network model (orangish) and low-dimensional projection (bluish curves). From top to bottom, each row corresponds to sensory-memory interacting networks in <xref rid="fig5" ref-type="fig">Figure 5</xref> with <italic>α</italic> = 0.03 (<bold>A</bold>,<bold>D</bold>), 0.04 (<bold>B</bold>,<bold>E</bold>), and 0.05 (<bold>C</bold>,<bold>F</bold>), respectively. We projected the dynamics onto the left (<bold>A-C</bold>) and right (<bold>D-F</bold>) eigenvectors of the Jacobian matrix obtained from local dynamics along the memory states (<bold>Methods</bold>). The initial orientations of the low-dimensional model were set to be the orientations decoded from the full model at 1s into the delay. We compared the increase of bias from then on, i.e., at 1.2s, 1.5s, and 2s into the delay for the full model, but 0.2s, 0.5s, and 1s for the low-dimensional model. Low-dimensional projection captures characteristic patterns well despite relatively larger deviation in the SD compared to bias, and we found that projecting to the right eigenvector (<bold>D-F</bold>) generally yields better predictions than projecting to the left eigenvector (<bold>A-C</bold>). All parameters are the same as in <xref rid="fig5" ref-type="fig">Figure 5</xref> except for <italic>α</italic>. Shaded areas (too narrow to be seen) mark the ±s.e.m. of 3000 realizations.</p></caption>
<graphic xlink:href="566396v4_fig5_S1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig5_S2" position="float" fig-type="figure">
<label>Figure 5 – Figure supplement 2.</label>
<caption><p>SD pattern remains consistent under different noise types. When Poisson noise is replaced by uniform Gaussian noise with <bold><italic>ξ</italic></bold>∼<italic>N</italic>(0, <italic>σ</italic><sub><italic>ξ</italic></sub><sup>2</sup>) in <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>, the diffusion term and variability of errors are affected (<bold>A-C</bold>), but the drift and bias patterns remain unchanged (not shown). The distance between stimulus representation <inline-formula><inline-graphic xlink:href="566396v4_inline41.gif" mime-subtype="gif" mimetype="image"/></inline-formula> determining noise coefficients (<bold>A</bold>; <xref ref-type="disp-formula" rid="eqn21">Eq. (21)</xref>), noise coefficients (<bold>B</bold>), and SD (<bold>C</bold>) exhibit similar profiles and dependence on <italic>α</italic> as those under Poisson noise (see <xref rid="fig5" ref-type="fig">Fig. 5C,F</xref>). <italic>σ</italic><sub><italic>ξ</italic></sub> = 0.2 and all remaining parameters are the same as in <xref rid="fig5" ref-type="fig">Figure 5</xref>.</p></caption>
<graphic xlink:href="566396v4_fig5_S2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig7_S1" position="float" fig-type="figure">
<label>Figure 7 – Figure supplement 1.</label>
<caption><p>Relationship between drift speed and memory loss in two-module (<bold>A-C</bold>) and one-module (<bold>D-F</bold>) networks. (<bold>A</bold>,<bold>D</bold>) Drift speed for different heterogeneity degrees, <italic>α</italic> and <italic>β</italic>. (<bold>B</bold>,<bold>E</bold>) Minimum Fisher information (FI). The upper panels were obtained with a coarse parameter grid, and lower panels were obtained with a fine grid but only along parameters for the smallest bias increase (red circle) and the orthogonal direction (red triangle). When bias speed is large with unbalanced excitation and inhibition strengths (triangle), the minimum FI decreases quickly in both two-module and one-module networks, suggesting memory loss. On the other hand, along the direction with the smallest bias increase (circle), the minimum FI is relatively high. The FI was estimated at 4s into the delay epoch using 1000 realizations. (<bold>C</bold>,<bold>F</bold>) Negative correlation between minimum FI and drift speed.</p></caption>
<graphic xlink:href="566396v4_fig7_S1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig7_S2" position="float" fig-type="figure">
<label>Figure 7 – Figure supplement 2.</label>
<caption><p>Error patterns in sensory networks with long intrinsic time constants. In the sensory module alone (<italic>J</italic><sub>f</sub> = <italic>J</italic><sub>b</sub> = 0), with an intrinsic time constant τ=1s in <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>, the bias (<bold>A</bold>) and SD (<bold>B</bold>) remain nearly unchanged during a 5-second delay period. Preferred orientations are estimated 10s after stimulus onset, and all other parameters are the same as those in <xref rid="fig7" ref-type="fig">Fig. 7F</xref>.</p></caption>
<graphic xlink:href="566396v4_fig7_S2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig9_S1" position="float" fig-type="figure">
<label>Figure 9 – Figure supplement 1.</label>
<caption><p>Error patterns remain unchanged under different levels of noise. Bias (left) and SD (right) in two-module (<bold>A</bold>,<bold>B</bold>) and one-module (<bold>C</bold>,<bold>D</bold>) networks with uniform Gaussian noise <bold><italic>ξ</italic></bold>∼<italic>N</italic>(0, <italic>σ</italic><sub><italic>ξ</italic></sub><sup>2</sup>) at varying <italic>σ</italic><sub><italic>ξ</italic></sub> levels. For each heterogeneity level α, the error patterns are unchanged across different <italic>σ</italic><sub><italic>ξ</italic></sub> values. Bias and SD are estimated 1 second into the delay period from 6000 realizations. All other parameters are the same as in <xref rid="fig7" ref-type="fig">Figure 7</xref> and <xref rid="fig8" ref-type="fig">8</xref>.</p></caption>
<graphic xlink:href="566396v4_fig9_S1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<ref-list>
<ref id="c1"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Adam</surname>, <given-names>K. C. S.</given-names></string-name>, <string-name><given-names>R. L.</given-names> <surname>Rademaker</surname></string-name> and <string-name><given-names>J. T.</given-names> <surname>Serences</surname></string-name></person-group> (<year>2022</year>). <chapter-title>Evidence for, and challenges to, sensory recruitment models of visual working memory</chapter-title>. <source>Visual Memory</source>. <person-group person-group-type="editor"><string-name><given-names>T. F.</given-names> <surname>Brady</surname></string-name> and <string-name><given-names>W. A.</given-names> <surname>Bainbridge</surname></string-name></person-group>, <publisher-loc>Routledge</publisher-loc>: <fpage>5</fpage>–<lpage>25</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bae</surname>, <given-names>G. Y.</given-names></string-name></person-group> (<year>2021</year>). “<article-title>Neural evidence for categorical biases in location and orientation representations in a working memory task</article-title>.” <source>Neuroimage</source> <volume>240</volume>: <fpage>118366</fpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Bays</surname>, <given-names>P.</given-names></string-name>, <string-name><given-names>S.</given-names> <surname>Schneegans</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Ma</surname></string-name> and <string-name><given-names>T. F.</given-names> <surname>Brady</surname></string-name></person-group> (<year>2022</year>). <article-title>Representation and computation in working memory</article-title>, <source>PsyArXiv</source>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bays</surname>, <given-names>P. M.</given-names></string-name></person-group> (<year>2014</year>). “<article-title>Noise in neural populations accounts for errors in working memory</article-title>.” <source>J Neurosci</source> <volume>34</volume>(<issue>10</issue>): <fpage>3632</fpage>–<lpage>3645</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bouchacourt</surname>, <given-names>F.</given-names></string-name> and <string-name><given-names>T. J.</given-names> <surname>Buschman</surname></string-name></person-group> (<year>2019</year>). “<article-title>A Flexible Model of Working Memory</article-title>.” <source>Neuron</source> <volume>103</volume>(<issue>1</issue>): <fpage>147</fpage>–<lpage>160 e148</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burak</surname>, <given-names>Y.</given-names></string-name> and <string-name><given-names>I. R.</given-names> <surname>Fiete</surname></string-name></person-group> (<year>2009</year>). “<article-title>Accurate path integration in continuous attractor network models of grid cells</article-title>.” <source>PLoS Comput Biol</source> <volume>5</volume>(<issue>2</issue>): <fpage>e1000291</fpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burak</surname>, <given-names>Y.</given-names></string-name> and <string-name><given-names>I. R.</given-names> <surname>Fiete</surname></string-name></person-group> (<year>2012</year>). “<article-title>Fundamental limits on persistent activity in networks of noisy neurons</article-title>.” <source>Proc Natl Acad Sci U S A</source> <volume>109</volume>(<issue>43</issue>): <fpage>17645</fpage>–<lpage>17650</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chunharas</surname>, <given-names>C.</given-names></string-name>, <string-name><given-names>R. L.</given-names> <surname>Rademaker</surname></string-name>, <string-name><given-names>T. F.</given-names> <surname>Brady</surname></string-name> and <string-name><given-names>J. T.</given-names> <surname>Serences</surname></string-name></person-group> (<year>2022</year>). “<article-title>An adaptive perspective on visual working memory distortions</article-title>.” <source>J Exp Psychol Gen</source> <volume>151</volume>(<issue>10</issue>): <fpage>2300</fpage>–<lpage>2323</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cole</surname>, <given-names>M. W.</given-names></string-name>, <string-name><given-names>J. R.</given-names> <surname>Reynolds</surname></string-name>, <string-name><given-names>J. D.</given-names> <surname>Power</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Repovs</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Anticevic</surname></string-name> and <string-name><given-names>T. S.</given-names> <surname>Braver</surname></string-name></person-group> (<year>2013</year>). “<article-title>Multi-task connectivity reveals flexible hubs for adaptive task control</article-title>.” <source>Nat Neurosci</source> <volume>16</volume>(<issue>9</issue>): <fpage>1348</fpage>–<lpage>1355</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Compte</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>N.</given-names> <surname>Brunel</surname></string-name>, <string-name><given-names>P. S.</given-names> <surname>Goldman-Rakic</surname></string-name> and <string-name><given-names>X. J.</given-names> <surname>Wang</surname></string-name></person-group> (<year>2000</year>). “<article-title>Synaptic mechanisms and network dynamics underlying spatial working memory in a cortical network model</article-title>.” <source>Cereb Cortex</source> <volume>10</volume>(<issue>9</issue>): <fpage>910</fpage>–<lpage>923</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Darshan</surname>, <given-names>R.</given-names></string-name> and <string-name><given-names>A.</given-names> <surname>Rivkind</surname></string-name></person-group> (<year>2022</year>). “<article-title>Learning to represent continuous variables in heterogeneous neural networks</article-title>.” <source>Cell Rep</source> <volume>39</volume>(<issue>1</issue>): <fpage>110612</fpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>de Gardelle</surname>, <given-names>V.</given-names></string-name>, <string-name><given-names>S.</given-names> <surname>Kouider</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Sackur</surname></string-name></person-group> (<year>2010</year>). “<article-title>An oblique illusion modulated by visibility: nonmonotonic sensory integration in orientation processing</article-title>.” <source>J Vis</source> <volume>10</volume>(<issue>10</issue>): <fpage>6</fpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Driscoll</surname>, <given-names>L.</given-names></string-name>, <string-name><given-names>K.</given-names> <surname>Shenoy</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Sussillo</surname></string-name></person-group> (<year>2022</year>). “<article-title>Flexible multitask computation in recurrent networks utilizes shared dynamical motifs</article-title>.” <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eissa</surname>, <given-names>T. L.</given-names></string-name> and <string-name><given-names>Z. P.</given-names> <surname>Kilpatrick</surname></string-name></person-group> (<year>2023</year>). “<article-title>Learning efficient representations of environmental priors in working memory</article-title>.” <source>PLoS Comput Biol</source> <volume>19</volume>(<issue>11</issue>): <fpage>e1011622</fpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Fischer</surname>, <given-names>B. J.</given-names></string-name></person-group> (<year>2010</year>). <chapter-title>Bayesian estimates from heterogeneous population codes</chapter-title>. <source>The 2010 International Joint Conference on Neural Networks (IJCNN)</source>, <publisher-loc>Barcelona, Spain</publisher-loc>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frankland</surname>, <given-names>S. M.</given-names></string-name> and <string-name><given-names>J. D.</given-names> <surname>Greene</surname></string-name></person-group> (<year>2020</year>). “<article-title>Concepts and Compositionality: In Search of the Brain’s Language of Thought</article-title>.” <source>Annu Rev Psychol</source> <volume>71</volume>: <fpage>273</fpage>–<lpage>303</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ganguli</surname>, <given-names>D.</given-names></string-name> and <string-name><given-names>E. P.</given-names> <surname>Simoncelli</surname></string-name></person-group> (<year>2014</year>). “<article-title>Efficient sensory encoding and Bayesian inference with heterogeneous neural populations</article-title>.” <source>Neural Comput</source> <volume>26</volume>(<issue>10</issue>): <fpage>2103</fpage>–<lpage>2134</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Geisler</surname>, <given-names>W. S.</given-names></string-name></person-group> (<year>2008</year>). “<article-title>Visual perception and the statistical properties of natural scenes</article-title>.” <source>Annu Rev Psychol</source> <volume>59</volume>: <fpage>167</fpage>–<lpage>192</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Georgopoulos</surname>, <given-names>A. P.</given-names></string-name>, <string-name><given-names>A. B.</given-names> <surname>Schwartz</surname></string-name> and <string-name><given-names>R. E.</given-names> <surname>Kettner</surname></string-name></person-group> (<year>1986</year>). “<article-title>Neuronal population coding of movement direction</article-title>.” <source>Science</source> <volume>233</volume>(<issue>4771</issue>): <fpage>1416</fpage>–<lpage>1419</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Girshick</surname>, <given-names>A. R.</given-names></string-name>, <string-name><given-names>M. S.</given-names> <surname>Landy</surname></string-name> and <string-name><given-names>E. P.</given-names> <surname>Simoncelli</surname></string-name></person-group> (<year>2011</year>). “<article-title>Cardinal rules: visual orientation perception reflects knowledge of environmental statistics</article-title>.” <source>Nat Neurosci</source> <volume>14</volume>(<issue>7</issue>): <fpage>926</fpage>–<lpage>932</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Gu</surname>, <given-names>H.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Lim</surname></string-name>, <string-name><given-names>H.-J.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Choe</surname></string-name>, <string-name><given-names>D.-G.</given-names> <surname>Yoo</surname></string-name>, <string-name><given-names>J. H.</given-names> <surname>Ryu</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Lim</surname></string-name> and <string-name><given-names>S.-H.</given-names> <surname>Lee</surname></string-name></person-group> (<year>2023</year>). “<article-title>Decision-consistent bias mediated by drift dynamics of human visual working memory</article-title>.” <source>bioRxiv</source>: 2023.2006.2028.546818.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gu</surname>, <given-names>J.</given-names></string-name> and <string-name><given-names>S.</given-names> <surname>Lim</surname></string-name></person-group> (<year>2022</year>). “<article-title>Unsupervised learning for robust working memory</article-title>.” <source>PLoS Comput Biol</source> <volume>18</volume>(<issue>5</issue>): <fpage>e1009083</fpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hahn</surname>, <given-names>M.</given-names></string-name> and <string-name><given-names>X. X.</given-names> <surname>Wei</surname></string-name></person-group> (<year>2024</year>). “<article-title>A unifying theory explains seemingly contradictory biases in perceptual estimation</article-title>.” <source>Nat Neurosci</source> <volume>27</volume>(<issue>4</issue>): <fpage>793</fpage>–<lpage>804</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hansel</surname>, <given-names>D.</given-names></string-name> and <string-name><given-names>G.</given-names> <surname>Mato</surname></string-name></person-group> (<year>2013</year>). “<article-title>Short-term plasticity explains irregular persistent activity in working memory tasks</article-title>.” <source>J Neurosci</source> <volume>33</volume>(<issue>1</issue>): <fpage>133</fpage>–<lpage>149</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hansel</surname>, <given-names>D.</given-names></string-name> and <string-name><given-names>H.</given-names> <surname>Sompolinsky</surname></string-name></person-group> (<year>1998</year>). <chapter-title>Modeling Feature Selectivity in Local Cortical Circuits</chapter-title>. <source>Methods in Neuronal Modeling: From Ions to Networks</source>. <person-group person-group-type="editor"><string-name><given-names>C.</given-names> <surname>Koch</surname></string-name> and <string-name><given-names>I.</given-names> <surname>Segev</surname></string-name></person-group>, <publisher-name>MIT Press</publisher-name>: <fpage>499</fpage>–<lpage>567</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hardman</surname>, <given-names>K. O.</given-names></string-name>, <string-name><given-names>E.</given-names> <surname>Vergauwe</surname></string-name> and <string-name><given-names>T. J.</given-names> <surname>Ricker</surname></string-name></person-group> (<year>2017</year>). “<article-title>Categorical working memory representations are used in delayed estimation of continuous colors</article-title>.” <source>J Exp Psychol Hum Percept Perform</source> <volume>43</volume>(<issue>1</issue>): <fpage>30</fpage>–<lpage>54</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Itskov</surname>, <given-names>V.</given-names></string-name>, <string-name><given-names>D.</given-names> <surname>Hansel</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Tsodyks</surname></string-name></person-group> (<year>2011</year>). “<article-title>Short-Term Facilitation may Stabilize Parametric Working Memory Trace</article-title>.” <source>Front Comput Neurosci</source> <volume>5</volume>: <fpage>40</fpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khan</surname>, <given-names>A. G.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Poort</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Chadwick</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Blot</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Sahani</surname></string-name>, <string-name><given-names>T. D.</given-names> <surname>Mrsic-Flogel</surname></string-name> and <string-name><given-names>S. B.</given-names> <surname>Hofer</surname></string-name></person-group> (<year>2018</year>). “<article-title>Distinct learning-induced changes in stimulus selectivity and interactions of GABAergic interneuron classes in visual cortex</article-title>.” <source>Nat Neurosci</source> <volume>21</volume>(<issue>6</issue>): <fpage>851</fpage>–<lpage>859</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khona</surname>, <given-names>M.</given-names></string-name> and <string-name><given-names>I. R.</given-names> <surname>Fiete</surname></string-name></person-group> (<year>2022</year>). “<article-title>Attractor and integrator networks in the brain</article-title>.” <source>Nat Rev Neurosci</source> <volume>23</volume>(<issue>12</issue>): <fpage>744</fpage>–<lpage>766</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kreile</surname>, <given-names>A. K.</given-names></string-name>, <string-name><given-names>T.</given-names> <surname>Bonhoeffer</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Hubener</surname></string-name></person-group> (<year>2011</year>). “<article-title>Altered visual experience induces instructive changes of orientation preference in mouse visual cortex</article-title>.” <source>J Neurosci</source> <volume>31</volume>(<issue>39</issue>): <fpage>13911</fpage>–<lpage>13920</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kwak</surname>, <given-names>Y.</given-names></string-name> and <string-name><given-names>C. E.</given-names> <surname>Curtis</surname></string-name></person-group> (<year>2022</year>). “<article-title>Unveiling the abstract format of mnemonic representations</article-title>.” <source>Neuron</source> <volume>110</volume>(<issue>11</issue>): <fpage>1822</fpage>–<lpage>1828 e1825</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Larisch</surname>, <given-names>R.</given-names></string-name>, <string-name><given-names>L.</given-names> <surname>Gonner</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Teichmann</surname></string-name> and <string-name><given-names>F. H.</given-names> <surname>Hamker</surname></string-name></person-group> (<year>2021</year>). “<article-title>Sensory coding and contrast invariance emerge from the control of plastic inhibition over emergent selectivity</article-title>.” <source>PLoS Comput Biol</source> <volume>17</volume>(<issue>11</issue>): <fpage>e1009566</fpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leavitt</surname>, <given-names>M. L.</given-names></string-name>, <string-name><given-names>D.</given-names> <surname>Mendoza-Halliday</surname></string-name> and <string-name><given-names>J. C.</given-names> <surname>Martinez-Trujillo</surname></string-name></person-group> (<year>2017</year>). “<article-title>Sustained Activity Encoding Working Memories: Not Fully Distributed</article-title>.” <source>Trends Neurosci</source> <volume>40</volume>(<issue>6</issue>): <fpage>328</fpage>–<lpage>346</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>B.</given-names></string-name>, <string-name><given-names>M. R.</given-names> <surname>Peterson</surname></string-name> and <string-name><given-names>R. D.</given-names> <surname>Freeman</surname></string-name></person-group> (<year>2003</year>). “<article-title>Oblique effect: a neural basis in the visual cortex</article-title>.” <source>J Neurophysiol</source> <volume>90</volume>(<issue>1</issue>): <fpage>204</fpage>–<lpage>217</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mejias</surname>, <given-names>J. F.</given-names></string-name> and <string-name><given-names>X. J.</given-names> <surname>Wang</surname></string-name></person-group> (<year>2022</year>). “<article-title>Mechanisms of distributed working memory in a large-scale network of macaque neocortex</article-title>.” <source>Elife</source> <volume>11</volume>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name> and <string-name><given-names>D. J.</given-names> <surname>Field</surname></string-name></person-group> (<year>1996</year>). “<article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>.” <source>Nature</source> <volume>381</volume>(<issue>6583</issue>): <fpage>607</fpage>–<lpage>609</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Panichello</surname>, <given-names>M. F.</given-names></string-name>, <string-name><given-names>B.</given-names> <surname>DePasquale</surname></string-name>, <string-name><given-names>J. W.</given-names> <surname>Pillow</surname></string-name> and <string-name><given-names>T. J.</given-names> <surname>Buschman</surname></string-name></person-group> (<year>2019</year>). “<article-title>Error-correcting dynamics in visual working memory</article-title>.” <source>Nat Commun</source> <volume>10</volume>(<issue>1</issue>): <fpage>3366</fpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pollock</surname>, <given-names>E.</given-names></string-name> and <string-name><given-names>M.</given-names> <surname>Jazayeri</surname></string-name></person-group> (<year>2020</year>). “<article-title>Engineering recurrent neural networks from task-relevant manifolds and dynamics</article-title>.” <source>PLoS Comput Biol</source> <volume>16</volume>(<issue>8</issue>): <fpage>e1008128</fpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pratte</surname>, <given-names>M. S.</given-names></string-name>, <string-name><given-names>Y. E.</given-names> <surname>Park</surname></string-name>, <string-name><given-names>R. L.</given-names> <surname>Rademaker</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Tong</surname></string-name></person-group> (<year>2017</year>). “<article-title>Accounting for stimulus-specific variation in precision reveals a discrete capacity limit in visual working memory</article-title>.” <source>J Exp Psychol Hum Percept Perform</source> <volume>43</volume>(<issue>1</issue>): <fpage>6</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rademaker</surname>, <given-names>R. L.</given-names></string-name>, <string-name><given-names>C.</given-names> <surname>Chunharas</surname></string-name> and <string-name><given-names>J. T.</given-names> <surname>Serences</surname></string-name></person-group> (<year>2019</year>). “<article-title>Coexisting representations of sensory and mnemonic information in human visual cortex</article-title>.” <source>Nat Neurosci</source> <volume>22</volume>(<issue>8</issue>): <fpage>1336</fpage>–<lpage>1344</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Renart</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>P.</given-names> <surname>Song</surname></string-name> and <string-name><given-names>X. J.</given-names> <surname>Wang</surname></string-name></person-group> (<year>2003</year>). “<article-title>Robust spatial working memory through homeostatic synaptic scaling in heterogeneous cortical networks</article-title>.” <source>Neuron</source> <volume>38</volume>(<issue>3</issue>): <fpage>473</fpage>–<lpage>485</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roussy</surname>, <given-names>M.</given-names></string-name>, <string-name><given-names>D.</given-names> <surname>Mendoza-Halliday</surname></string-name> and <string-name><given-names>J. C.</given-names> <surname>Martinez-Trujillo</surname></string-name></person-group> (<year>2021</year>). “<article-title>Neural Substrates of Visual Perception and Working Memory: Two Sides of the Same Coin or Two Different Coins?</article-title>” <source>Front Neural Circuits</source> <volume>15</volume>: <fpage>764177</fpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schneegans</surname>, <given-names>S.</given-names></string-name> and <string-name><given-names>P. M.</given-names> <surname>Bays</surname></string-name></person-group> (<year>2018</year>). “<article-title>Drift in Neural Population Activity Causes Working Memory to Deteriorate Over Time</article-title>.” <source>J Neurosci</source> <volume>38</volume>(<issue>21</issue>): <fpage>4859</fpage>–<lpage>4869</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seeholzer</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>M.</given-names> <surname>Deger</surname></string-name> and <string-name><given-names>W.</given-names> <surname>Gerstner</surname></string-name></person-group> (<year>2019</year>). “<article-title>Stability of working memory in continuous attractor networks under the control of short-term plasticity</article-title>.” <source>PLoS Comput Biol</source> <volume>15</volume>(<issue>4</issue>): <fpage>e1006928</fpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seung</surname>, <given-names>H. S.</given-names></string-name>, <string-name><given-names>D. D.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>B. Y.</given-names> <surname>Reis</surname></string-name> and <string-name><given-names>D. W.</given-names> <surname>Tank</surname></string-name></person-group> (<year>2000</year>). “<article-title>The autapse: a simple illustration of short-term analog memory storage by tuned synaptic feedback</article-title>.” <source>J Comput Neurosci</source> <volume>9</volume>(<issue>2</issue>): <fpage>171</fpage>–<lpage>185</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname>, <given-names>G.</given-names></string-name>, <string-name><given-names>X.</given-names> <surname>Tao</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>E. L.</given-names> <surname>Smith</surname></string-name>, 3rd and <string-name><given-names>Y. M.</given-names> <surname>Chino</surname></string-name></person-group> (<year>2014</year>). “<article-title>Oblique effect in visual area 2 of macaque monkeys</article-title>.” <source>J Vis</source> <volume>14</volume>(<issue>2</issue>).</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Simon</surname>, <given-names>H. A.</given-names></string-name></person-group> (<year>1995</year>). <chapter-title>Near-decomposability and complexity: How a mind resides in a brain</chapter-title>. <source>The Mind, the Brain, and Complex Adaptive Systems</source>. <person-group person-group-type="editor"><string-name><given-names>H.</given-names> <surname>Morowitz</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Singer</surname></string-name></person-group>, <publisher-name>Addison-Wesley</publisher-name>: <fpage>25</fpage>–<lpage>43</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Taylor</surname>, <given-names>R.</given-names></string-name> and <string-name><given-names>P. M.</given-names> <surname>Bays</surname></string-name></person-group> (<year>2018</year>). “<article-title>Efficient Coding in Visual Working Memory Accounts for Stimulus-Specific Variations in Recall</article-title>.” <source>J Neurosci</source> <volume>38</volume>(<issue>32</issue>): <fpage>7132</fpage>–<lpage>7142</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Tomić</surname>, <given-names>I.</given-names></string-name> and <string-name><given-names>P. M.</given-names> <surname>Bays</surname></string-name></person-group> (<year>2023</year>). “<article-title>A dynamic neural resource model bridges sensory and working memory</article-title>.” <source>bioRxiv</source>: 2023.2003.2027.534406.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Bergen</surname>, <given-names>R. S.</given-names></string-name>, <string-name><given-names>W. J.</given-names> <surname>Ma</surname></string-name>, <string-name><given-names>M. S.</given-names> <surname>Pratte</surname></string-name> and <string-name><given-names>J. F.</given-names> <surname>Jehee</surname></string-name></person-group> (<year>2015</year>). “<article-title>Sensory uncertainty decoded from visual cortex predicts behavior</article-title>.” <source>Nat Neurosci</source> <volume>18</volume>(<issue>12</issue>): <fpage>1728</fpage>–<lpage>1730</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van de Ven</surname>, <given-names>V.</given-names></string-name>, <string-name><given-names>C.</given-names> <surname>Jacobs</surname></string-name> and <string-name><given-names>A. T.</given-names> <surname>Sack</surname></string-name></person-group> (<year>2012</year>). “<article-title>Topographic contribution of early visual cortex to short-term memory consolidation: a transcranial magnetic stimulation study</article-title>.” <source>J Neurosci</source> <volume>32</volume>(<issue>1</issue>): <fpage>4</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van den Berg</surname>, <given-names>R.</given-names></string-name>, <string-name><given-names>H.</given-names> <surname>Shin</surname></string-name>, <string-name><given-names>W. C.</given-names> <surname>Chou</surname></string-name>, <string-name><given-names>R.</given-names> <surname>George</surname></string-name> and <string-name><given-names>W. J.</given-names> <surname>Ma</surname></string-name></person-group> (<year>2012</year>). “<article-title>Variability in encoding precision accounts for visual short-term memory limitations</article-title>.” <source>Proc Natl Acad Sci U S A</source> <volume>109</volume>(<issue>22</issue>): <fpage>8780</fpage>–<lpage>8785</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vogels</surname>, <given-names>T. P.</given-names></string-name>, <string-name><given-names>R. C.</given-names> <surname>Froemke</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Doyon</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Gilson</surname></string-name>, <string-name><given-names>J. S.</given-names> <surname>Haas</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Maffei</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Miller</surname></string-name>, <string-name><given-names>C. J.</given-names> <surname>Wierenga</surname></string-name>, <string-name><given-names>M. A.</given-names> <surname>Woodin</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Zenke</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Sprekeler</surname></string-name></person-group> (<year>2013</year>). “<article-title>Inhibitory synaptic plasticity: spike timing-dependence and putative network function</article-title>.” <source>Front Neural Circuits</source> <volume>7</volume>: <fpage>119</fpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>X. J.</given-names></string-name></person-group> (<year>2001</year>). “<article-title>Synaptic reverberation underlying mnemonic persistent activity</article-title>.” <source>Trends Neurosci</source> <volume>24</volume>(<issue>8</issue>): <fpage>455</fpage>–<lpage>463</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Webster</surname>, <given-names>M. A.</given-names></string-name></person-group> (<year>2015</year>). “<article-title>Visual Adaptation</article-title>.” <source>Annu Rev Vis Sci</source> <volume>1</volume>: <fpage>547</fpage>–<lpage>567</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname>, <given-names>X. X.</given-names></string-name> and <string-name><given-names>A. A.</given-names> <surname>Stocker</surname></string-name></person-group> (<year>2015</year>). “<article-title>A Bayesian observer model constrained by efficient coding can explain ‘anti-Bayesian’ percepts</article-title>.” <source>Nat Neurosci</source> <volume>18</volume>(<issue>10</issue>): <fpage>1509</fpage>–<lpage>1517</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname>, <given-names>X. X.</given-names></string-name> and <string-name><given-names>A. A.</given-names> <surname>Stocker</surname></string-name></person-group> (<year>2017</year>). “<article-title>Lawful relation between perceptual bias and discriminability</article-title>.” <source>Proc Natl Acad Sci U S A</source> <volume>114</volume>(<issue>38</issue>): <fpage>10244</fpage>–<lpage>10249</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Wilson</surname>, <given-names>H. R.</given-names></string-name></person-group> (<year>1999</year>). <source>Spikes, decisions, and actions : the dynamical foundations of neuroscience</source>. <publisher-loc>Oxford</publisher-loc>, <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wimmer</surname>, <given-names>K.</given-names></string-name>, <string-name><given-names>D. Q.</given-names> <surname>Nykamp</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Constantinidis</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Compte</surname></string-name></person-group> (<year>2014</year>). “<article-title>Bump attractor dynamics in prefrontal cortex explains behavioral precision in spatial working memory</article-title>.” <source>Nat Neurosci</source> <volume>17</volume>(<issue>3</issue>): <fpage>431</fpage>–<lpage>439</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2020</year>). “<article-title>Revisit once more the sensory storage account of visual working memory</article-title>.” <source>Vis cogn</source> <volume>28</volume>(<issue>5-8</issue>): <fpage>433</fpage>–<lpage>446</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname>, <given-names>G. R.</given-names></string-name>, <string-name><given-names>M. R.</given-names> <surname>Joglekar</surname></string-name>, <string-name><given-names>H. F.</given-names> <surname>Song</surname></string-name>, <string-name><given-names>W. T.</given-names> <surname>Newsome</surname></string-name> and <string-name><given-names>X. J.</given-names> <surname>Wang</surname></string-name></person-group> (<year>2019</year>). “<article-title>Task representations in neural networks trained to perform many cognitive tasks</article-title>.” <source>Nat Neurosci</source> <volume>22</volume>(<issue>2</issue>): <fpage>297</fpage>–<lpage>306</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>K.</given-names></string-name></person-group> (<year>1996</year>). “<article-title>Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory</article-title>.” <source>J Neurosci</source> <volume>16</volume>(<issue>6</issue>): <fpage>2112</fpage>–<lpage>2126</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zylberberg</surname>, <given-names>J.</given-names></string-name>, <string-name><given-names>J. T.</given-names> <surname>Murphy</surname></string-name> and <string-name><given-names>M. R.</given-names> <surname>DeWeese</surname></string-name></person-group> (<year>2011</year>). “<article-title>A sparse coding model with synaptically local plasticity and spiking neurons can account for the diverse shapes of V1 simple cell receptive fields</article-title>.” <source>PLoS Comput Biol</source> <volume>7</volume>(<issue>10</issue>): <fpage>e1002250</fpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95160.3.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Wei</surname>
<given-names>Xue-Xin</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>UT Austin</institution>
</institution-wrap>
<city>Austin</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> computational study provides new insights into how neural dynamics may lead to time-evolving behavioral errors as observed in certain working-memory tasks. By combining ideas from efficient coding and attractor neural networks, the authors construct a two-module network model to capture the sensory-memory interactions and the distributed nature of working memory representations. They provide <bold>convincing</bold> evidence supporting that their two-module network, although none of the alternative circuit structures they considered can account for error patterns reported in orientation-estimation tasks with delays.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95160.3.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Working memory is imperfect - memories accrue error over time and are biased towards certain identities. For example, previous work has shown memory for orientation is more accurate near the cardinal directions (i.e., variance in responses is smaller for horizontal and vertical stimuli) while being biased towards diagonal orientations (i.e., there is a repulsive bias away from horizontal and vertical stimuli). The magnitude of errors and biases increase the longer an item is held in working memory and when more items are held in working memory (i.e., working memory load is higher). Previous work has argued that biases and errors could be explained by increased perceptual acuity at cardinal directions. However, these models are constrained to sensory perception and do not explain how biases and errors increase over time in memory. The current manuscript builds on this work to show how a two-layer neural network could integrate errors and biases over a memory delay. In brief, the model includes a 'sensory' layer with heterogenous connections that lead to the repulsive bias and decreased error at the cardinal directions. This layer is then reciprocally connected with a classic ring attractor layer. Through their reciprocal interactions, the biases in the sensory layer are constantly integrated into the representation in memory. In this way, the model captures the distribution of biases and errors for different orientations that has been seen in behavior and their increasing magnitude with time. The authors compare the two-layer network to a simpler one-network model, showing that the one model network is harder to tune and shows an attractive bias for memories that have lower error (which is incompatible with empirical results).</p>
<p>Strengths:</p>
<p>The manuscript provides a nice review of the dynamics of items in working memory, showing how errors and biases differ across stimulus space. The two-layer neural network model is able to capture the behavioral effects as well as relate to neurophysiological observations that memory representations are distributed across sensory cortex and prefrontal cortex.</p>
<p>The authors use multiple approaches to understand how the network produces the observed results. For example, analyzing the dynamics of memories in the low-dimensional representational space of the networks provides the reader with an intuition for the observed effects.</p>
<p>As a point of comparison with the two-layer network, the authors construct a heterogenous one-layer network (analogous to a single memory network with embedded biases). They argue that such a network is incapable of capturing the observed behavioral effects but could potentially explain biases and noise levels in other sensory domains where attractive biases have lower errors (e.g., color).</p>
<p>The authors show how changes in the strength of Hebbian learning of excitatory and inhibitory synapses can change network behavior. This argues for relatively stronger learning in inhibitory synapses, an interesting prediction.</p>
<p>The manuscript is well-written. In particular, the figures are well done and nicely schematize the model and the results.</p>
<p>Weaknesses:</p>
<p>Despite its strengths, the manuscript does have some weaknesses. These weaknesses are adequately discussed in the manuscript and motivate future research.</p>
<p>One weakness is that the model is not directly fit to behavioral data, but rather compared to a schematic of behavioral data. As noted above, the model provides insight into the general phenomenon of biases in working memory. However, because the models are not fit directly to data, they may miss some aspects of the data.</p>
<p>In addition, directly fitting the models to behavioral data could allow for a broader exploration of parameter space for both the one-layer and two-layer models (and their alternatives). Such an approach would provide stronger support for the papers claims (such as &quot;....these evolving errors...require network interaction between two distinct modules.&quot;). That being said, the manuscript does explore several alternative models and also acknowledges the limitation of not directly fitting behavior, due to difficulties in fitting complex neural network models to data.</p>
<p>One important behavioral observation is that both diffusive noise and biases increase with the number of items in working memory. The current model does not capture these effects and it isn't clear how the model architecture could be extended to capture these effects. That being said, the authors note this limitation in the Discussion and present it as a future direction.</p>
<p>Overall:</p>
<p>Overall, the manuscript was successful in building a model that captured the biases and noise observed in working memory. This work complements previous studies that have viewed these effects through the lens of optimal coding, extending these models to explain the effects of time in memory. In addition, the two-layer network architecture extends previous work with similar architectures, adding further support to the distributed nature of working memory representations.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95160.3.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this manuscript, Yang et al. present a modeling framework to understand the pattern of response biases and variance observed in delayed-response orientation estimation tasks. They combine a series of modeling approaches to show that coupled sensory-memory networks are in a better position than single-area models to support experimentally observed delay-dependent response bias and variance in cardinal compared to oblique orientations. These errors can emerge from a population-code approach that implements efficient coding and Bayesian inference principles and is coupled to a memory module that introduces random maintenance errors. A biological implementation of such operation is found when coupling two neural network modules, a sensory module with connectivity inhomogeneities that reflect environment priors, and a memory module with strong homogeneous connectivity that sustains continuous ring attractor function. Comparison with single-network solutions that combine both connectivity inhomogeneities and memory attractors shows that two-area models can more easily reproduce the patterns of errors observed experimentally.</p>
<p>Strengths:</p>
<p>The model provides an integration of two modeling approaches to the computational bases of behavioral biases: one based on Bayesian and efficient coding principles, and one based on attractor dynamics. These two perspectives are not usually integrated consistently in existing studies, which this manuscript beautifully achieves. This is a conceptual advancement, especially because it brings together the perceptual and memory components of common laboratory tasks.</p>
<p>The proposed two-area model provides a biologically plausible implementation of efficient coding and Bayesian inference principles, which interact seamlessly with a memory buffer to produce a complex pattern of delay-dependent response errors. No previous model had achieved this.</p>
<p>Weaknesses:</p>
<p>The correspondence between the various computational models is not clearly shown. It is not easy to see clearly this correspondence because network function is illustrated with different representations for different models. In particular, the Bayesian model of Figure 2 is illustrated with population responses for different stimuli and delays, while the attractor models of Figure 3 and 4 are illustrated with neuronal tuning curves but not population activity.</p>
<p>The proposed model has stronger feedback than feedforward connections between the sensory and memory modules (J_f = 0.1 and J_b = 0.25). This is not the common assumption when thinking about hierarchical processing in the brain. The manuscript argues that error patterns remain similar as long as the product of J_f and J_b is constant, so it is unclear why the authors preferred this network example as opposed to one with J_b = 0.1 and J_f = 0.25.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95160.3.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The present study proposes a neural circuit model consisting of coupled sensory and memory networks to explain the circuit mechanism of the cardinal effect in orientation perception which is characterized by the bias towards the oblique orientation and the largest variance at the oblique orientation.</p>
<p>Strengths:</p>
<p>The authors have done numerical simulations and preliminary analysis of the neural circuit model to show the model successfully reproduces the cardinal effect. And the paper is well-written overall. As far as I know, most of the studies on the cardinal effect are at the level of statistical models, and the current study provides one possibility of how neural circuit models reproduce such an effect.</p>
<p>Weaknesses:</p>
<p>There are no major weaknesses and flaws in the present study, although I suggest the author conduct further analysis to deepen our understanding of the circuit mechanism of the cardinal effects.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95160.3.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Yang</surname>
<given-names>Jun</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Hanqi</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lim</surname>
<given-names>Sukbin</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9936-5293</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the previous reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3:</bold></p>
<p>I appreciate the revisions made by the author which address all of my concerns.</p>
<p>Nevertheless, I have some new questions when I read the paper again. These questions are not necessarily criticisms of the paper, which may reflect the gap in my understanding. Meanwhile, it also reflects the writing might be improved further.</p>
<p>- Fig. 1:</p>
<p>I understand that a critical assumption for generating the required result is that the oblique orientation has lower &quot;energy&quot; than the cardinal orientation (Fig. 1G). Meanwhile, I always have a concept that typically the energy is defined as the negative of log probability. If we take the log probability plotted in Fig. 1A, that will generate an energy landscape that is upside down compared with current Fig. 1G. How should I understand this discrepancy?</p>
</disp-quote>
<p>As the reviewer pointed out, a higher prior distribution near cardinal orientations causes cardinal attraction in typical Bayesian models, which can correspond to lower energy around these orientations. Additionally, in the context of learning natural statistics, Hebbian plasticity in excitatory connections strengthens recurrent connections and drives attraction toward more prevalent stimuli within neural circuits.</p>
<p>However, as demonstrated by Wei and Stocker (2015), Bayesian inference model can also produce cardinal repulsion when optimizing encoding efficiency. In our network, this efficient encoding is achieved through heterogeneous lateral connections and inhibitory Hebbian plasticity in the sensory module, resulting in lower energy near oblique orientations. Thus, the shape of prior distribution does not have a direct one-to-one correspondence with the bias pattern or the dynamic energy landscape.</p>
<disp-quote content-type="editor-comment">
<p>- Fig. 3 and its corresponding text.</p>
<p>I understand and agree the Fig. 3B&amp;C that neurons near cardinal orientations are shaper and denser. But why the stimulus representation around cardinal orientations are sparser compared with the oblique orientation? Isn't more neurons around cardinal orientation implying a less sparser representation?</p>
</disp-quote>
<p>Indeed, with sharper tuning curves, having more neurons can result in a sparser representation. Consider an extreme case where each orientation, discretized by 1°, is represented by only one active neuron with a tuning width of 1°. While this would require more neurons to represent overall stimuli compared to cases with wider tuning curves, each stimulus would be represented by fewer neurons, aligning with the traditional concept of sparse coding.</p>
<p>However, in Fig. 3 and corresponding text, we did not measure the sparseness of active neurons for each orientation. Instead, we used the term ‘sparser representation’ to describe the increased distance between representations of different stimuli near the cardinal orientations. Although this increased distance can be consistent with the traditional concept of sparse coding, to avoid any confusion, we have revised the term ‘sparser representation’ to ‘more dispersed representation’ in the 3rd paragraph in pg. 5 and the 3rd paragraph in pg. 6.</p>
</body>
</sub-article>
</article>