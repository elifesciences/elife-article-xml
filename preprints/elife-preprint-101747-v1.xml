<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">101747</article-id>
<article-id pub-id-type="doi">10.7554/eLife.101747</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.101747.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Dynamics of striatal action selection and reinforcement learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0930-7327</contrib-id>
<name>
<surname>Lindsey</surname>
<given-names>Jack</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2362-1937</contrib-id>
<name>
<surname>Markowitz</surname>
<given-names>Jeffrey E</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1659-8639</contrib-id>
<name>
<surname>Gillis</surname>
<given-names>Winthrop F</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8068-3862</contrib-id>
<name>
<surname>Datta</surname>
<given-names>Sandeep Robert</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2422-6576</contrib-id>
<name>
<surname>Litwin-Kumar</surname>
<given-names>Ashok</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>a.litwin-kumar@columbia.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Zuckerman Mind Brain Behavior Institute, Columbia University</institution></institution-wrap>, <city>New York</city>, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02j15s898</institution-id><institution>Wallace H. Coulter Department of Biomedical Engineering, Georgia Institute of Technology and Emory University</institution></institution-wrap>, <city>Atlanta</city>, <country>USA</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Department of Neurobiology, Harvard Medical School</institution></institution-wrap>, <city>Boston</city>, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Langdon</surname>
<given-names>Angela</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>National Institute of Mental Health</institution>
</institution-wrap>
<city>Bethesda</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="original-publication" iso-8601-date="2024-12-06">
<day>06</day>
<month>12</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP101747</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-08-16">
<day>16</day>
<month>08</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-07-25">
<day>25</day>
<month>07</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.02.14.580408"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Lindsey et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Lindsey et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-101747-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Spiny projection neurons (SPNs) in dorsal striatum are often proposed as a locus of reinforcement learning in the basal ganglia. Here, we identify and resolve a fundamental inconsistency between striatal reinforcement learning models and known SPN synaptic plasticity rules. Direct-pathway (dSPN) and indirect-pathway (iSPN) neurons, which promote and suppress actions, respectively, exhibit synaptic plasticity that reinforces activity associated with elevated or suppressed dopamine release. We show that iSPN plasticity prevents successful learning, as it reinforces activity patterns associated with negative outcomes. However, this pathological behavior is reversed if functionally opponent dSPNs and iSPNs, which promote and suppress the current behavior, are simultaneously activated by efferent input following action selection. This prediction is supported by striatal recordings and contrasts with prior models of SPN representations. In our model, learning and action selection signals can be multiplexed without interference, enabling learning algorithms beyond those of standard temporal difference models.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>S.R.D. sits on the scientific advisory boards of Neumora and Gilgamesh Therapeutics, which have licensed or sub-licensed the MoSeq technology.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Additional model simulations were added (Figure 4). Updates to the text were made for clarity and to discuss additional relevant work.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Numerous studies have proposed that the basal ganglia is a reinforcement learning system (<xref ref-type="bibr" rid="c32">Joel et al., 2002</xref>; <xref ref-type="bibr" rid="c47">Niv, 2009</xref>; <xref ref-type="bibr" rid="c30">Ito and Doya, 2011</xref>). Reinforcement learning algorithms use experienced and predicted rewards to learn to predict the expected future reward associated with an organism’s current state and the action to select in order to maximize this reward (<xref ref-type="bibr" rid="c61">Sutton and Barto, 2018</xref>). Spiny projection neurons (SPNs) in the striatum are well-positioned to take part in such an algorithm, as they receive diverse contextual information from the cerebral cortex and are involved in both action selection (in dorsal striatum; <xref ref-type="bibr" rid="c49">Packard and Knowlton, 2002</xref>; <xref ref-type="bibr" rid="c56">Seo et al., 2012</xref>; <xref ref-type="bibr" rid="c3">Balleine et al., 2007</xref>) and value prediction (in ventral striatum; <xref ref-type="bibr" rid="c9">Cardinal et al., 2002</xref>; <xref ref-type="bibr" rid="c46">Montague et al., 1996</xref>; <xref ref-type="bibr" rid="c48">O’Doherty et al., 2004</xref>). Moreover, plasticity of SPN input synapses is modulated by midbrain dopamine release (<xref ref-type="bibr" rid="c64">Wickens et al., 1996</xref>; <xref ref-type="bibr" rid="c8">Calabresi et al., 2000</xref>; <xref ref-type="bibr" rid="c12">Contreras-Vidal and Schultz, 1999</xref>). A variety of studies support the view that this dopamine release reflects reward prediction error (<xref ref-type="bibr" rid="c55">Schultz et al., 1997</xref>; <xref ref-type="bibr" rid="c46">Montague et al., 1996</xref>; <xref ref-type="bibr" rid="c27">Houk and Adams, 1995</xref>), which in many reinforcement learning algorithms is the key quantity used to modulate learning (<xref ref-type="bibr" rid="c61">Sutton and Barto, 2018</xref>; <xref ref-type="bibr" rid="c47">Niv, 2009</xref>).</p>
<p>Despite these links, several aspects of striatal physiology are difficult to reconcile with reinforcement learning models. SPNs are classified in two main types – direct-pathway (dSPNs) and indirect-pathway (iSPNs). These two classes of SPNs exert opponent effects on action based on perturbation data (<xref ref-type="bibr" rid="c36">Kravitz et al., 2010</xref>; <xref ref-type="bibr" rid="c25">Freeze et al., 2013</xref>; <xref ref-type="bibr" rid="c37">Lee and Sabatini, 2021</xref>), but also exhibit highly correlated activity (<xref ref-type="bibr" rid="c14">Cui et al., 2013</xref>). Moreover, dSPNs and iSPNs express different dopamine receptors (D1-type and D2-type) and thus undergo synaptic plasticity according to different rules. In particular, dSPN inputs are potentiated when coincident pre- and post-synaptic activity is followed by above-baseline dopamine activity, while iSPN inputs are potentiated when coincident pre- and post-synaptic activity is followed by dopamine suppression (<xref ref-type="bibr" rid="c57">Shen et al., 2008</xref>; <xref ref-type="bibr" rid="c24">Frank, 2005</xref>; <xref ref-type="bibr" rid="c29">Iino et al., 2020</xref>).</p>
<p>Prior studies have proposed that dSPNs learn from positive reinforcement to promote actions, and iSPNs learn from negative reinforcement to suppress actions (<xref ref-type="bibr" rid="c13">Cruz et al., 2022</xref>; <xref ref-type="bibr" rid="c11">Collins and Frank, 2014</xref>; <xref ref-type="bibr" rid="c31">Jaskir and Frank, 2023</xref>; <xref ref-type="bibr" rid="c63">Varin et al., 2023</xref>; <xref ref-type="bibr" rid="c43">Mikhael and Bogacz, 2016</xref>; <xref ref-type="bibr" rid="c17">Dunovan et al., 2019</xref>). However, we will show that a straightforward implementation of such a model fails to yield a functional reinforcement learning algorithm, as the iSPN learning rule assigns blame for negative outcomes to the wrong actions. Correct learning in this scenario requires a mechanism to selectively update corticostriatal weights corresponding to the chosen action, which is absent in prior models (see Discussion).</p>
<p>In this work, we begin by rectifying this inconsistency between standard reinforcement learning models of the striatum and known SPN plasticity rules. The iSPN learning rule reported in the literature reinforces patterns of iSPN activity that are associated with dopamine suppression, increasing the likelihood of repeating decisions that previously led to negative outcomes. We show that this pathological behavior is reversed if, after action selection, opponent dSPNs and iSPNs receive correlated efferent input encoding the animal’s selected action. A central contribution of our model is a decomposition of SPN activity into separate modes of activity for action selection and for learning, the latter driven by this efferent input. This decomposition provides an explanation for the apparent paradox that the activities of dSPNs and iSPNs are positively correlated despite their opponent causal functions (<xref ref-type="bibr" rid="c14">Cui et al., 2013</xref>), and provides a solution to the problem of multiplexing signals related to behavioral execution and learning. The model also makes predictions about the time course of SPN activity, including that dSPNs and iSPNs that are responsible for regulating the same behavior (promoting and suppressing it, respectively) should be coactive following action selection. This somewhat counterintuitive prediction contrasts with prior proposals that dSPNs that promote an action are coactive with iSPNs that suppress different actions (<xref ref-type="bibr" rid="c44">Mink, 1996</xref>; Red-grave et al., 1999). We find support for this prediction in experimental recordings of dSPNs and iSPNs during spontaneous behavior.</p>
<p>Next, we show that the nonuniformity of dSPN and iSPN plasticity rules enables more sophisticated learning algorithms than can be achieved in models with a single plasticity rule. In particular, it enables the striatum to implement so-called <italic>off-policy</italic> reinforcement learning algorithms, in which the cortico-striatal pathway learns from the the outcomes of actions that are driven by other neural pathways. Off-policy algorithms are commonly used in state-of-the-art machine learning models, as they dramatically improve learning efficiency by facilitating learning from expert demonstrations, mixture-of-experts models, and replayed experiences (<xref ref-type="bibr" rid="c1">Arulkumaran et al., 2017</xref>). Following the implications of this model further, we show that off-policy algorithms require a dopaminergic signal in dorsal striatum that combines classic state-based reward prediction error with a form of action prediction error. We confirm a key signature of this prediction in recent dopamine data collected from dorsolateral striatum during spontaneous behavior.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>In line with previous experimental (<xref ref-type="bibr" rid="c64">Wickens et al., 1996</xref>; <xref ref-type="bibr" rid="c8">Calabresi et al., 2000</xref>; <xref ref-type="bibr" rid="c12">Contreras-Vidal and Schultz, 1999</xref>) and modeling (<xref ref-type="bibr" rid="c61">Sutton and Barto, 2018</xref>; <xref ref-type="bibr" rid="c47">Niv, 2009</xref>) studies, we model plasticity of corticostriatal synapses using a three-factor learning rule, dependent on coincident presynaptic activity, postsynaptic activity, and dopamine release (<xref rid="fig1" ref-type="fig">Fig 1A,B</xref>). Concretely, we model plasticity of the weight <italic>w</italic> of a synapse from a cortical neuron with activity <italic>x</italic> onto a dSPN or iSPN with activity <italic>y</italic> as
<disp-formula id="eqn1">
<graphic xlink:href="580408v2_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn2">
<graphic xlink:href="580408v2_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>δ</italic> represents dopamine release relative to baseline, and the functions <italic>f</italic> <sup>dSPN</sup>(<italic>δ</italic>) and <italic>f</italic> <sup>iSPN</sup>(<italic>δ</italic>) model the dependence of the two plasticity rules on dopamine concentration.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Corticostriatal action selection circuits and plasticity rules. <bold>A</bold>. Left, diagram of cortical inputs to striatal populations. Right, illustration of action selection architecture. Populations of dSPNs (blue) and iSPNs (red) in DLS are responsible for promoting and suppressing specific actions, respectively. Active neurons (shaded circles) illustrate a pattern of activity consistent with typical models of striatal action selection, in which dSPNs that promote a chosen action and iSPNs that suppress other actions are active. <bold>B</bold>. Illustration of three-factor plasticity rules at SPN input synapses, in which adjustments to corticostriatal synaptic weights depend on presynaptic cortical activity, SPN activity, and dopamine release. <bold>C</bold>. Illustration of different models of the dopamine-dependent factor <italic>f</italic> (<italic>δ</italic>) in dSPN (blue) and iSPN (red) plasticity rules.</p></caption>
<graphic xlink:href="580408v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>For dSPNs, the propensity of input synapses to potentiate increases with increasing dopamine concentration, while for iSPNs the opposite is true. This observation is corroborated by converging evidence from observations of dendritic spine volume, intracellular PKA measurements, and spiketiming dependent plasticity protocols (<xref ref-type="bibr" rid="c57">Shen et al., 2008</xref>; <xref ref-type="bibr" rid="c26">Gurney et al., 2015</xref>; <xref ref-type="bibr" rid="c29">Iino et al., 2020</xref>; <xref ref-type="bibr" rid="c38">Lee et al., 2021</xref>). For the three-factor plasticity rule above, these findings imply that <italic>f</italic> <sup>dSPN</sup> is an increasing function of <italic>δ</italic> while <italic>f</italic> <sup>iSPN</sup> is a decreasing function. Prior modeling studies have proposed specific plasticity rules that correspond to different choices of <italic>f</italic> <sup>dSPN</sup> and <italic>f</italic> <sup>iSPN</sup>, some examples of which are shown in <xref rid="fig1" ref-type="fig">Fig. 1C</xref>.</p>
<sec id="s2a">
<title>iSPN plasticity rule impedes successful reinforcement learning</title>
<p>Prior work has proposed that dSPNs activate when actions are performed and iSPNs activate when actions are suppressed (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>). When an animal selects among multiple actions, subpopulations of dSPNs are thought to promote the selected action, while other subpopulations of iSPNs inhibit the unchosen actions (<xref ref-type="bibr" rid="c44">Mink, 1996</xref>; <xref ref-type="bibr" rid="c53">Redgrave et al., 1999</xref>). We refer to this general description as the “canonical action selection model” of SPN activity and show that this model, when combined with the plasticity rules above, fails to produce a functional reinforcement learning algorithm. This failure is specifically due to the iSPN plasticity rule. Later, we also show that the SPN representation predicted by the canonical action selection model is inconsistent with recordings of identified dSPNs and iSPNs. We begin by analyzing a toy model of an action selection task with two actions, one of which is rewarded. In the model, the probability of selecting an action is increased when the dSPN corresponding to that action is active and decreased when the corresponding iSPN is active. After an action is taken, dopamine activity reports the reward prediction error, increasing when reward is obtained and decreasing when it is not.</p>
<p>It is easy to see that the dSPN plasticity rule in <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref> is consistent with successful reinforcement learning (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>). Suppose action 1 is selected, leading to reward (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>, center). The resulting dopamine increase potentiates inputs to the action 1 dSPN from cortical neurons that are active during the task, making action 1 more likely to be selected in the future (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>, right).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p>Consequences of the canonical action selection model of SPN activity. <bold>A</bold>. Example in which dSPN plasticity produces correct learning. Left: cortical inputs to the dSPN and iSPN are equal prior to learning. Middle: action 1 is selected, corresponding to elevated activity in the dSPN that promotes action 1 and the iSPN that suppresses action 2. In this example, action 1 leads to reward and increased DA activity, which potentiates the input synapse to the action 1-promoting dSPN and (depending on the learning rule, see <xref ref-type="fig" rid="fig1">Fig. 1</xref>) depresses the input to the action 2-suppressing iSPN. Right: in a subsequent trial, cortical input to the action 1-promoting dSPN is stronger, increasing the likelihood of selecting action 1. Here, the dSPN-mediated effect of increasing action 1’s probability overcomes the iSPN-mediated effect of decreasing action 2’s probability. <bold>B</bold>. Example in which iSPN plasticity produces incorrect learning. Same as A, but in a scenario in which action 2 is selected leading to punishment and a corresponding decrease in DA activity. As a result, the input synapse to the action 2-promoting dSPN is (depending on the learning rule) depressed, and the input to the action 1-suppressing iSPN is potentiated. On a subsequent trial, the probability of selecting action 2 rather than action 1 is greater, despite action 2 being punished. Note that the dSPN input corresponding to action 2 is (potentially) weakened, which correctly decreases the probability of selecting action 2, but this effect is not sufficient to overcome the strengthened action 1 iSPN activity. <bold>C</bold>. Performance of a simulated striatal reinforcement learning system in go/no-go tasks with different reward contingencies. <bold>D</bold>. Same as C, but for action selection tasks with two cortical input states, two available actions, and one correct action per state, under different reward protocols.</p></caption>
<graphic xlink:href="580408v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>At first glance, it may seem that a similar logic would apply to iSPNs, since their suppressive effect on behavior and reversed dependence on dopamine concentration are both opposite to dSPNs. However, a more careful examination reveals that the iSPN plasticity rule in <xref ref-type="disp-formula" rid="eqn2">Eq. (2)</xref> does not promote successful learning. In the canonical action selection model, dSPNs promoting a selected action and iSPNs inhibiting unselected actions are active. If a negative outcome is encountered leading to a dopamine decrease, <xref ref-type="disp-formula" rid="eqn2">Eq. (2)</xref> predicts that inputs to iSPNs corresponding to unselected actions are strengthened (LTP in <xref rid="fig2" ref-type="fig">Fig. 2B</xref>, center). This makes the action that led to the negative outcome <italic>more</italic> rather than less likely to be taken when the same cortical inputs are active in the future (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>, right). More generally, the model demonstrates that, while the plasticity rule of <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref> correctly reinforces dSPN activity patterns that lead to positive outcomes, it incorrectly reinforces iSPN activity patterns that lead to negative outcomes. The function of iSPNs in inhibiting action does not change the fact that such reinforcement is undesirable.</p>
<p>We note that, depending on the learning rule (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>), inputs to dSPNs that promote the selected action may be weakened (LTD in <xref rid="fig2" ref-type="fig">Fig. 2B</xref>, left), which correctly disincentivizes the action that led to a negative outcome. However, this dSPN effect competes with the pathological behavior of the iSPNs and is often unable to overcome it. We also note that, if dopamine increases lead to depression of iSPN inputs (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>, center, right), positive outcomes will lead to actions that were correctly being inhibited by iSPNs to be less inhibited in the future. Thus, both positive and negative outcomes may cause incorrect iSPN learning. Some sources suggest that while dopamine suppression increases D2 receptor activation, dopamine increase has little effect on D2 receptors (<xref ref-type="bibr" rid="c16">Dreyer et al., 2010</xref>), corresponding to the rectified model of <italic>f</italic> (<italic>δ</italic>) (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>, left). In this case, pathological iSPN plasticity behavior still manifests when dopamine activity is suppressed (as in the examples of <xref rid="fig2" ref-type="fig">Fig. 2B</xref>).</p>
<p>We simulated learning of multiple tasks with the three-factor plasticity rules above, with dopamine activity modeled as reward prediction error obtained using a temporal difference learning rule. In a go/no-go task with one cue in which the “go” action is rewarded (<xref ref-type="fig" rid="figS1">Supp. Fig. 1</xref>), the system learns the wrong behavior when negative performance feedback is provided on no-go trials, and thus iSPN plasticity is the main driver of learning (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>). We also simulated a two-alternative forced choice task in which there are two cues (corresponding to different cortical input patterns), each with a corresponding target action. When performance feedback consists of rewards for correct actions, the system learns the task, as dSPNs primarily drive the learning. However, when instead performance feedback consists of giving punishments for incorrect actions, the system does not learn the task, as iSPNs primarily drive the learning (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>). We note that, in principle, this problem could be avoided if the learning rate of iSPNs were very small compared to that of dSPNs, ensuring that reinforcement learning is always primarily driven by the dSPN pathway (leaving iSPNs to potentially perform a different function). However, this alternative would be inconsistent with prior studies indicating a significant role for the indirect pathway in reinforcement learning (<xref ref-type="bibr" rid="c51">Peak et al., 2020</xref>; <xref ref-type="bibr" rid="c37">Lee and Sabatini, 2021</xref>). The model we introduce below makes use of contributions to learning from both pathways.</p>
</sec>
<sec id="s2b">
<title>Efferent activity in SPNs enables successful reinforcement learning</title>
<p>We have shown that the canonical action selection model, when paired with <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref>, produces incorrect learning. What pattern of SPN activity would produce correct learning? In the model, the probability of selecting an action is determined by the “difference mode” <italic>y</italic><sup>dSPN</sup>−<italic>y</italic><sup>iSPN</sup>, where <italic>y</italic><sup>dSPN</sup> and <italic>y</italic><sup>iSPN</sup> are the activities of dSPN and iSPN neurons associated with that action. We analyzed how the plasticity rule of <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref> determines changes to this difference mode. In the simplest case in which the SPN firing rate is a linear function of cortical input (that is, <italic>y</italic><sup>d<italic>/</italic>iSPN</sup> = <bold>w</bold><sup>d<italic>/</italic>iSPN</sup> · <bold>x</bold>) and plasticity’s dependence on dopamine concentration is also linear (that is, <italic>f</italic> <sup>d<italic>/</italic>iSPN</sup>(<italic>δ</italic>) ∝ <italic>±δ</italic>; <xref rid="fig1" ref-type="fig">Fig. 1C</xref>, center), the change in the probability of selecting an action due to learning is
<disp-formula id="eqn3">
<graphic xlink:href="580408v2_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Changes to the “difference mode” <italic>y</italic><sup>dSPN</sup> − <italic>y</italic><sup>iSPN</sup> are therefore driven by the “sum mode” <italic>y</italic><sup>dSPN</sup> + <italic>y</italic><sup>iSPN</sup>. This implies that the activity pattern that leads to correct learning about an action’s outcome is different from the activity pattern that selects the action. To promote or inhibit, respectively, an action that leads to a dopamine increase or decrease, this analysis predicts that both dSPNs that promote and iSPNs that inhibit the action should be co-active. A more general argument applies for other learning rules and firing rate nonlinearities: as long as <italic>y</italic><sup>d<italic>/</italic>iSPN</sup> is an increasing function of total input current, <italic>f</italic> <sup>dSPN</sup>(<italic>δ</italic>) has positive slope, and <italic>f</italic> <sup>iSPN</sup>(<italic>δ</italic>) has negative slope, changes in difference mode activity will be positively correlated with sum mode activity (see Supplemental Information).</p>
<p>The key insight of the above argument is that the pattern of SPN activity needed for learning involves simultaneous excitation of dSPNs that promote the current behavior and iSPNs that inhibit it. This differs from the pattern of activity needed to drive selection of that behavior in the first place. We therefore propose a model in which SPN activity contains a substantial <italic>efferent</italic> component that follows action selection and promotes learning, but has no causal impact on behavior. In the model, feedforward cortico-striatal inputs initially produce SPN activity whose difference mode causally influences action selection, consistent with the canonical model (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>, left). When an action is performed, both dSPNs and iSPNs responsible for promoting or inhibiting that action receive efferent excitatory input, producing sum-mode activity. Following this step, SPN activity reflects both contributions (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>, center). The presence of sum-mode activity leads to correct synaptic plasticity and learning (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>, right). Unlike the canonical action selection model (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>), this model thus predicts an SPN representation in which, after an action is selected, the most highly active neurons are those responsible for regulating that behavior and not other behaviors.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><p>The efference model of SPN activity. <bold>A</bold>. Illustration of the efference model in an action selection task. Left: feedforward SPN activity driven by cortical inputs. Center: once action 2 is selected, efferent inputs excite the dSPN and iSPN responsible for promoting and suppressing action 2. Efferent activity is combined with feedforward activity, such that the action 2-associated dSPNs and iSPNs are both more active than the action 1 dSPNs and iSPNs, but the relative dSPN and iSPN activity for each action remains unchanged. This produces strong LTD and LTP in the action 2-associated dSPNs and iSPNs upon a reduction in dopamine activity. Right: In a subsequent trial, this plasticity correctly reduces the likelihood of selecting action 2. <bold>B</bold>. The activity levels of the dSPN and iSPN populations that promote and suppress a given action can be plotted in a two-dimensional space. The difference mode influences the probabiility of taking that action, while activity in the sum mode drives future changes to activity in the difference mode via plasticity. Efferent activity excites the sum mode. <bold>C</bold>. Performance of a striatal RL system using the efference model on the tasks of <xref ref-type="fig" rid="fig2">Fig. 2C</xref>. <bold>D</bold>. Performance of a striatal RL system using the efference model on the tasks of <xref ref-type="fig" rid="fig2">Fig. 2D</xref>.</p></caption>
<graphic xlink:href="580408v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In SPN activity space, the sum and difference modes are orthogonal to one another. This orthogonality has two consequences. First, it implies that encoding the action in the difference mode (as in the canonical action selection model) produces synaptic weight changes that do not promote learning, consistent with the competing effects of dSPN and iSPN plasticity that we previously described. Second, it implies that adding efferent activity along the sum mode, which produces correct learning, has no effect on action selection. The model thus provides a solution to the problem of interference between “forward pass” (action selection) and “backward pass” (learning) activity, a common issue in models of biologically plausible learning algorithms (see Discussion).</p>
<p>In simulations, we confirm that unlike the canonical action selection model, this efference model solves go/no-go (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>) and action selection (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>) tasks regardless of the reward protocol. Although the derivation above assumes linear SPN responses and linear dependences of plasticity on dopamine concentration, our model enables successful learning even using a nonlinear model of SPN responses and a variety of plasticity rules (<xref rid="fig3" ref-type="fig">Fig. 3C,D</xref>; see Supplemental Information for a derivation that explains this general success). Finally, we also confirmed that our results apply to cases in which actions are associated with distributed modes of dSPN and iSPN activity, and with a larger action space (<xref ref-type="fig" rid="figS2">Supp. Fig. 2</xref>).</p>
</sec>
<sec id="s2c">
<title>Temporal dynamics of the efference model</title>
<p>We simulated a two-alternative forced choice task using a firing rate model of SPN activity. This allowed us to directly visualize dynamics in the sum and difference modes and verify that the efference model prevents interference between them. In each trial of the forced choice task, one of two stimuli is presented and one of two actions is subsequently selected (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>, top row). The selected action is determined by the difference mode activity of action-encoding SPNs during the first half of the stimulus presentation period. The sum mode is activated by efferent input during the second half of this period. Reward is obtained if the correct action is selected in a trial, and each stimulus has a different corresponding correct action. Plasticity of cortical weights encoding stimulus identity onto SPNs is governed by Eqs. (1), (2).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><p>Temporal dynamics of the efference model in a two-alternative forced choice task. <bold>A</bold>. Top row: In each trial, either stimulus 1 (magenta) or stimulus 2 (green) is presented for 2 s. After 1 s, either action 1 (magenta) or action 2 (green) is selected based on SPN activity. A correct trial is one in which action 1 (resp. 2) is selected after stimulus 1 (resp. 2) is presented. Second row: Firing rates of four SPNs. Dark and light colors denote SPNs that represent action 1 and action 2, respectively. Third and fourth rows: Projection of SPN activity onto difference and sum modes for actions 1 and 2. <bold>B</bold>. Same as A, but illustrating the first trial, in which stimulus 2 is presented and action 1 is incorrectly selected. <bold>C</bold>. Same as B, but illustrating the last trial, in which stimulus 1 is presented and action 1 is correctly selected.</p></caption>
<graphic xlink:href="580408v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The model learned the correct policy in about 10 trials. Early in learning, difference mode activity is small and primarily driven by noise, leading to random action selection (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>). However, sum mode activity is strongly driven after an action is selected (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>, bottom). As learning progresses, the magnitude of the difference mode activity evoked by the stimulus increases (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>, third row). Late in learning, dSPN and iSPN firing rates are more separable during stimulus presentation, leading to correct action selection (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>, second row). Both difference and sum mode activity is evident late in learning, with the former leading the latter (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>, bottom two rows).</p>
<p>Throughout the learning process, difference and sum mode activity for the two actions are separable and non-interfering, even when both are present simultaneously. As a result, action selection is not disrupted by efferent feedback. We conclude that the efference model multiplexes action selection and learning signals without separate learning phases or gated plasticity rules. While we illustrated this in a task with sequential trials for visualization purposes, this non-interference enables learning based on delayed reward and efferent feedback from past actions even as the selection of subsequent actions unfolds.</p>
</sec>
<sec id="s2d">
<title>Efference model predicts properties of SPN activity</title>
<p>Thus far, we have provided theoretical arguments and model simulations that suggest that simultaneous efferent input to opponent dSPNs and iSPNs is necessary for reinforcement learning, given known plasticity rules. We next sought to test this prediction in neural data. We predict these dynamics to be particularly important in scenarios where the action space is large and actions are selected continuously, without a clear trial structure. We therefore used data from a recent study which recorded bulk and cellular dSPN and iSPN activity in spontaneously behaving mice (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>; <xref ref-type="bibr" rid="c41">Markowitz et al., 2018</xref>). As no explicit rewards or task structure were provided during recording sessions, we adopted a modeling approach that makes minimal assumptions about the inputs to SPNs besides the core prediction of efferent activity. Specifically, we used a network model in which (1) populations of dSPNs and iSPNs promote or suppress different actions, (2) the feedforward inputs to all SPNs are random, (3) actions are sampled with log-likelihoods scaling according to the associated dSPN and iSPN difference mode, and (4) efferent activity excites the sum mode corresponding to the chosen action.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><p>Comparisons of model predictions about bulk dSPN and iSPN activity to experimental data. <bold>A</bold>. Schematic of experimental setup, taken from <xref ref-type="bibr" rid="c41">Markowitz et al. (2018)</xref>. Neural activity and kinematics of spontaneously behaving mice are recorded, and behavior is segmented into stereotyped “behavioral syllables” using the MoSeq pipeline. <bold>B</bold>. In simulation of efference model with random feedforward cortical inputs, cross-correlation of total dSPN and iSPN activity. <bold>C</bold>. Cross-correlation between fiber photometry recordings of bulk dSPN and iSPN activity in freely behaving mice, using the data from <xref ref-type="bibr" rid="c41">Markowitz et al. (2018)</xref>. Line thickness indicates standard error of the mean.</p></caption>
<graphic xlink:href="580408v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In this model, difference mode dSPN and iSPN activity drives behaviors, and those behaviors cause efferent activation of the corresponding sum mode. As a result, on average, dSPN activity tends to lead to increased future iSPN activity, while iSPN activity leads to decreased future dSPN activity. Consequently, the temporal cross-correlation between total dSPN activity and iSPN activity is asymmetric, with present dSPN activity correlating more strongly with future iSPN activity than with past iSPN activity (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>). Such asymmetry is not predicted by the canonical action selection model, or models that assume dSPNs and iSPNs are co-active. Computing the temporal cross-correlation in the bulk two-color photometry recordings of dSPN and iSPN activity, we find a very similar skewed relationship in the data (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>). We confirmed this result is not an artifact of the use of different indicators for dSPN and iSPN activity by repeating the analysis on data from mice where the indicators were reversed and finding the same result (<xref ref-type="fig" rid="figS3">Supp. Fig. 3</xref>).</p>
<p>Our model makes even stronger predictions about SPN population activity and its relationship to action selection. First, it predicts that both dSPNs and iSPNS exhibit similar selectivity in their tuning to actions. This contrasts with implementations of the canonical action selection model in which iSPNs are active whenever their associated action is not being performed and thus are more broadly tuned than dSPNs (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>). Second, it also predicts that efferent activity excites dSPNs that promote the currently performed action and iSPNs that suppress the currently performed action. As a result, dSPNs whose activity increases during the performance of a given action should tend to be above baseline shortly prior to the performance of that action. By contrast, iSPNs whose activity increases during an action should tend to be below baseline during the same time interval (<xref rid="fig6" ref-type="fig">Fig. 6A</xref>, left; <xref rid="fig4" ref-type="fig">Fig. 4C</xref>). Moreover, this effect should be action-specific: the dSPNs and iSPNs whose activity increases during a given action should display negligible average fluctuations around the onset of other actions (<xref rid="fig6" ref-type="fig">Fig. 6A</xref>, right). These predictions can also be reinterpreted in terms of the sum and difference modes. The difference mode activity associated with an action is elevated prior to selection of that action, while the sum mode activity is excited following action selection (<xref rid="fig6" ref-type="fig">Fig. 6B</xref>; <xref rid="fig4" ref-type="fig">Fig. 4C</xref>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><p>Comparisons of model predictions about action-tuned SPN subpopulations to experimental data. <bold>A</bold>. Activity of dSPNs (blue) and iSPNs (red) around the onset of their associated action (left) or other actions (right) in the simulation from <xref ref-type="fig" rid="fig5">Fig. 5</xref>. <bold>B</bold>. Same information as A, but plotting activity of the sum (dSPN + iSPN) and difference (dSPN - iSPN) modes. <bold>C</bold>. For an example experimental session, dSPN activity modes associated with each of the behavioral syllables, in z-scored firing rate units. <bold>D</bold>. Correlation between identified dSPN and iSPN activity modes in two random subsamples of the data, for shuffled (left, circles) and real (right, x’s) data. <bold>E</bold>. Projection of dSPN (blue) and iSPN (red) activity onto the syllable-associated modes identified in panel C, around the onset of the associated syllable (left panel) or other syllables (right panel) averaged across all syllables. Error bars indicate standard error of the mean across syllables. <bold>F</bold>. Same as panel E, restricting the analysis to mice in which dSPNs and iSPNs were simultaneously recorded. <bold>G</bold>. Same data as panel F, but plotting activity of the sum (dSPN + iSPN) and difference (dSPN - iSPN) modes.</p></caption>
<graphic xlink:href="580408v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To test these hypotheses, we used calcium imaging data collected during spontaneous mouse behavior (<xref ref-type="bibr" rid="c41">Markowitz et al., 2018</xref>). The behavior of the mice was segmented into consistent, stereotyped kinematic motifs referred to as “behavioral syllables,” as in previous studies (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>). We regard these behavioral syllables as the analogs of actions in our model. First, we examined the tuning of dSPNs and iSPNs to different actions and found that, broadly consistent with what our model predicts, both subpopulations exhibit similar selectivities (<xref ref-type="fig" rid="figS4">Supp. Fig. 4</xref>). Next, to test our predictions about dynamics before and after action selection (<xref rid="fig6" ref-type="fig">Fig. 6A,B</xref>), we identified, for each syllable, dSPN and iSPN population activity vectors (“modes”) that increased the most during performance of that syllable (<xref rid="fig6" ref-type="fig">Fig. 6C</xref>). We confirmed that these modes are meaningful by checking that modes identified using two disjoint subsets of the data are correlated (<xref rid="fig6" ref-type="fig">Fig. 6D</xref>). We then plotted the activity of these modes around the time of onset of the corresponding syllable, and averaged the result across the choice of syllables (<xref rid="fig6" ref-type="fig">Fig. 6E</xref>). The result displays remarkable agreement with the model prediction in <xref rid="fig6" ref-type="fig">Fig. 6A</xref>.</p>
<p>The majority of the above data consisted of recordings of either dSPNs or iSPNs from a given mouse. However, in a small subset (n=4) of mice, dSPNs and iSPNs were simultaneously recorded and identified. We repeated the analysis above on these sessions, and found the same qualitative results (<xref rid="fig6" ref-type="fig">Fig. 6F</xref>). The simultaneous recordings further allowed us to visualize the sum and difference mode activity (<xref rid="fig6" ref-type="fig">Fig. 6G</xref>), which also agrees with the predictions of our model (<xref rid="fig6" ref-type="fig">Fig. 6B</xref>).</p>
</sec>
<sec id="s2e">
<title>Efference model enables off-policy reinforcement learning</title>
<p>Prior studies have argued for the importance of motor efference copies during basal ganglia learning, in particular when action selection is influenced by other brain regions (<xref ref-type="bibr" rid="c20">Fee, 2014</xref>; <xref ref-type="bibr" rid="c39">Lindsey and Litwin-Kumar, 2022</xref>). Indeed, areas such as the motor cortex and cerebellum drive behavior independent of the basal ganglia (<xref ref-type="bibr" rid="c18">Exner et al., 2002</xref>; <xref ref-type="bibr" rid="c65">Wildgruber et al., 2001</xref>; <xref ref-type="bibr" rid="c2">Ashby et al., 2010</xref>; <xref ref-type="bibr" rid="c59">Silveri, 2021</xref>; <xref ref-type="bibr" rid="c6">Bostan and Strick, 2018</xref>). Actions taken by an animal may therefore at times differ from those most likely to be selected by striatal outputs (<xref rid="fig7" ref-type="fig">Fig. 7A</xref>), and it may be desirable for cortico-striatal synapses to learn about the consequences of these actions.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7:</label>
<caption><p>The efference model enables off-policy reinforcement learning. <bold>A</bold>. Illustration of the efference model when the striatum shares control of behavior with other pathways. In this example, striatal activity biases the action selection toward choosing action 2, but other neural pathways override the striatum and cause action 1 to be selected instead (left). Following action selection, efferent activity excites the dSPN and iSPN associated with action 1. However, the outputs of the striatal population remain unchanged. <bold>B</bold>. Performance of RL models in a simulated action selection task (10 cortical states, 10 available actions, in each state one of the actions results in a reward of 1 and the others result in zero reward). Control is shared between the striatal RL circuit and anothere pathway that biases action selection toward the correct action. Different lines indicate different strength of striatal control relative to the strength of the other pathway. Line style (dashed or solid) indicates the efference model: off-policy efference excites SPNs associated with the selected action, while on-policy efference excites SPNs associated with the action most favored by the striatum. <bold>C</bold>. Schematic of different reinforcement learning models of dopamine activity. The standard TD error models predicts that dopamine activity is sensitive to reward, the predicted value of the current state, and the predicted value of the previous state. The Q-learning error model predicts sensitivity to reward, the predicted value of the current state, and the predicted value of the previous state-action pair. <bold>D</bold>. In the task of panel B using the off-policy efference model, comparison between different models of dopamine activity as striatal control is varied (the Q-learning error model was used in panel B). <bold>E</bold>. Correlation between predicted and actual syllable-to-syllable transition matrix. Predictions were made according to different models of the relationship between dopamine activity and behavior, using observed average dopamine activity associated with syllable transitions in the data of <xref ref-type="bibr" rid="c42">Markowitz et al. (2023)</xref>. Each dot indicates a different experimental session.</p></caption>
<graphic xlink:href="580408v2_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In the reinforcement learning literature, this kind of learning is known as an “off-policy” algorithm, as the reinforcement learning system (in our model, the striatum) learns from actions that follow a different policy than its own. Off-policy learning has been observed experimentally, for instance in the consolidation of cortically driven behaviors into subcortical circuits including dorsolateral striatum (<xref ref-type="bibr" rid="c33">Kawai et al., 2015</xref>; <xref ref-type="bibr" rid="c28">Hwang et al., 2019</xref>; <xref ref-type="bibr" rid="c45">Mizes et al., 2023</xref>). Such learning requires efferent activity in SPNs that reflects the actions being performed, rather than the action that would be performed based on the striatum’s influence alone.</p>
<p>We modeled this scenario by assuming that action selection is driven by weighted contributions from both the striatum and other motor pathways and that the ultimately selected action drives efferent activity (<xref rid="fig7" ref-type="fig">Fig. 7A</xref>; see Methods). We found that when action selection is not fully determined by the striatum, such efferent activity is critical for successful learning (<xref rid="fig7" ref-type="fig">Fig. 7B</xref>). Notably, in our model, efferent activity has no effect on striatal action selection, due to the orthogonality of the sum and difference modes (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>). In a hypothetical alternative model in which the iSPN plasticity rule is the same as that of dSPNs, the efferent activity needed for learning is not orthogonal to the output of the striatum, impairing off-policy learning (<xref ref-type="fig" rid="figS5">Supp. Fig. 5</xref>). Thus, efferent excitation of opponent dSPNs/iSPNs is necessary both to implement correct learning updates given dSPN and iSPN plasticity rules, and to enable off-policy reinforcement learning.</p>
</sec>
<sec id="s2f">
<title>Off-policy reinforcement learning predicts relationship between dopamine activity and behavior</title>
<p>We next asked whether other properties of striatal dynamics are consistent with off-policy reinforcement learning. We focused on the dynamics of dopamine release, as off-policy learning makes specific predictions about this signal. Standard temporal difference (TD) learning models of dopamine activity (<xref rid="fig7" ref-type="fig">Fig. 7C</xref>, top) determine the expected future reward (or “value”) <italic>V</italic> (<italic>s</italic>) associated with each state <italic>s</italic> using the following algorithm:
<disp-formula id="eqn4">
<graphic xlink:href="580408v2_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5">
<graphic xlink:href="580408v2_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>s</italic><sub><italic>t</italic></sub> and <italic>s</italic><sub><italic>t</italic>−1</sub> indicate current and previous states, <italic>r</italic><sub><italic>t</italic></sub> indicates the currently received reward, <italic>α</italic> is a learning rate factor, and <italic>δ</italic><sub><italic>t</italic></sub> is the TD error thought to be reflected in phasic dopamine responses. These dopaminergic responses can be used as the learning signal for a updating action selection in dorsal striatum (<xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>, <xref ref-type="disp-formula" rid="eqn2">2</xref>), an arrangement commonly referred to as an”actor-critic” architecture (<xref ref-type="bibr" rid="c47">Niv, 2009</xref>).</p>
<p>TD learning is an on-policy algorithm, in that the values <italic>V</italic> (<italic>s</italic>) associated with each state are calculated under the assumption that the system’s future actions will be similar to those taken during learning. Hence, TD learning is poorly suited to training an action selection policy in the striatum in situations where the striatum does not fully control behavior, as the values <italic>V</italic> (<italic>s</italic>) will not reflect the expected future reward associated with a state if the striatum were to dictate behavior on its own. Off-policy algorithms such as Q-learning solve this issue by learning an action-dependent value function <italic>Q</italic>(<italic>s, a</italic>), which indicates the expected reward associated with taking action <italic>a</italic> in action <italic>s</italic> (<xref rid="fig7" ref-type="fig">Fig. 7C</xref>, bottom), via the following algorithm:
<disp-formula id="eqn6">
<graphic xlink:href="580408v2_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn7">
<graphic xlink:href="580408v2_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
This algorithm predicts that the dopamine response <italic>δ</italic><sub><italic>t</italic></sub> is action-dependent. The significance of on-policy vs. off-policy learning algorithms can be demonstrated in simulations of operant conditioning tasks in which control of action selection is shared between the striatum and another “tutor” pathway that biases responses toward the correct action. When the striatal contribution to decision-making is weak, it is unable to learn the appropriate response when dopamine activity is modeled as a TD error (<xref rid="fig7" ref-type="fig">Fig. 7D</xref>). On the other hand, a Q-learning model of dopamine activity enables efficient striatal learning even when control is shared with another pathway.</p>
<p>For the spontaneous behavior paradigm we analyzed previously (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>), Q-learning but not TD learning predicts sensitivity of dopamine responses to the likelihood of the previous syllable-to-syllable transition. Using recordings of dopamine activity in the dorsolateral striatum in this paradigm (<xref ref-type="bibr" rid="c42">Markowitz et al., 2023</xref>), we tested whether a Q-learning model could predict the relationship between dopamine activity and behavioral statistics, comparing it to TD learning and other alternatives (see Supplemental Information). The Q-learning model matches the data significantly better than alternatives (<xref rid="fig7" ref-type="fig">Fig. 7E</xref>), providing support for a model of dorsal striatum as an off-policy reinforcement learning system.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We have presented a model of reinforcement learning in the dorsal striatum in which efferent activity excites dSPNs and iSPNs that promote and suppress, respectively, the currently selected action. Thus, following action selection, iSPN activity counteruintively represents the action that is inhibited by the currently active iSPN population. This behavior contrasts with previous pro-posals in which iSPN activity reflects actions being inhibited. This model produces updates to corticostriatal synaptic weights given the known opposite-sign plasticity rules in dSPNs and iSPNs that correctly implement a form of reinforcement learning (<xref rid="fig3" ref-type="fig">Fig. 3</xref>), which in the absence of such efferent activity produce incorrect weight updates (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). The model makes several novel predictions about SPN activity which we confirmed in experimental data (<xref rid="fig5" ref-type="fig">Figs. 5</xref>, <xref rid="fig6" ref-type="fig">6</xref>). It also enables multiplexing of action selection signals and learning signals without interference. This facilitates more sophisticated learning algorithms such as off-policy reinforcement learning, which allows the striatum to learn from actions that were driven by other neural circuits. Off-policy reinforcement learning requires dopamine to signal action-sensitive reward predictions errors, which agrees better with experimental recordings of striatal dopamine activity than alternative models (<xref rid="fig7" ref-type="fig">Fig. 7</xref>).</p>
<sec id="s3a">
<title>Other models of striatal action selection</title>
<p>Prior models have modeled the opponent effects of dopamine on dSPN and iSPN plasticity (<xref ref-type="bibr" rid="c24">Frank, 2005</xref>; <xref ref-type="bibr" rid="c11">Collins and Frank, 2014</xref>; <xref ref-type="bibr" rid="c31">Jaskir and Frank, 2023</xref>). In these models, dSPNs come to represent the positive outcomes and iSPNs the negative outcomes associated with a stimulus-action pair. Such models can also represent uncertainty in reward estimates (<xref ref-type="bibr" rid="c43">Mikhael and Bogacz, 2016</xref>). Appropriate credit assignment in these models requires that only corticostriatal weights associated with SPNs encoding the chosen action are updated. Our model clarifies how the neural activity required for such selective weight updates can be multiplexed with the neural activity required for action selection, without requiring separate phases for action selection and learning.</p>
<p><xref ref-type="bibr" rid="c5">Bariselli et al. (2019)</xref> also argue against the canonical action selection model and propose a competitive role for dSPNs and iSPNs that is consistent with our model. However, the role of efferent activity and distinctions between action- and learning-related signals are not discussed.</p>
<p>Our model is related to these prior proposals but identifies motor efference as key for appropriate credit assignment across corticostriatal synapses. It also provides predictions concerning the temporal dynamics of such signals (<xref rid="fig4" ref-type="fig">Fig. 4</xref>) and a verification of these using physiological data (<xref rid="fig7" ref-type="fig">Fig. 7</xref>).</p>
</sec>
<sec id="s3b">
<title>Other models of efferent inputs to the striatum</title>
<p>Prior work has pointed out the need for efference copies of decisions to be represented in the striatum, particularly for actions driven by other circuits (<xref ref-type="bibr" rid="c20">Fee, 2014</xref>). <xref ref-type="bibr" rid="c24">Frank (2005)</xref> propose a model in which premotor cortex outputs collateral signals to the striatum that represent the actions under consideration, with the striatum potentially biasing the decision based on prior learning. Through bidirectional feedback (premotor cortex projecting to striatum, and striatum projecting to premotor cortex indirectly through the thalamus) a decision is collectively made by the combined circuit, and the selected action is represented in striatal activity, facilitating learning about the outcome of the action. While similar to our proposal in some ways, this model implicitly assumes that the striatal activity necessary for decision-making is also what is needed to facilitate learning. As we point out in this work, due to the opponent plasticity rules in dSPNs and iSPNs, a post-hoc efferent signal that is not causally relevant to the decision-making process is necessary for appropriate learning.</p>
<p>Other authors have proposed models in which efferent activity is used for learning. In the context of vocal learning in songbirds, <xref ref-type="bibr" rid="c21">Fee and Goldberg (2011)</xref> proposed that the variability-generating area LMAN, which projects to the song motor pathway, sends collateral projections to Area X, which undergoes dopamine-modulated plasticity. In this model, the efferent inputs to Area X allow it to learn which motor commands are associated with better song performance (signaled by dopamine). Similar to our model, this architecture implements off-policy reinforcement learning in Area X, with HVC inputs to Area X being analogous to corticostriatal projections in our model. However, in our work, the difference in plasticity rules between dSPNs and iSPNs is key to avoiding interference between efferent learning-related activity and feedforward action selection-related activity. A similar architecture was proposed in <xref ref-type="bibr" rid="c19">Fee (2012)</xref> in the context of oculomotor learning, in which oculomotor striatum receives efferent collaterals from the superior colliculus and/or cortical areas which generate exploratory variability. <xref ref-type="bibr" rid="c40">Lisman (2014)</xref> also propose a high-level model of striatal efferent inputs similar to ours, and also point out the issue with the iSPN plasticity rule assigning credit to inappropriate actions without efferent inputs. <xref ref-type="bibr" rid="c54">Rubin et al. (2021)</xref> argue that sustained efferent input is necessary for temporal credit assignment when reward is delayed relative to action selection.</p>
<p>Our model is consistent with these prior proposals, but describes how efferent input must be targeted to opponent SPNs. In our work, the distinction between dSPN and iSPN plasticity rules is key to enable multiplexing of action-selection and efferent learning signals without interference. Previous authors have proposed other mechanisms to avoid interference. For instance, <xref ref-type="bibr" rid="c20">Fee (2014)</xref> propose that efferent inputs might influence plasticity without driving SPN spiking by synapsing preferentially onto dendritic shafts rather than spines. To avoid action selection-related spikes interfering with learning, the system may employ spike timing-dependent plasticity rules that are tuned to match the latency at which efferent inputs excite SPNs. While these hypotheses are not mutually exclusive to ours, our model requires no additional circuitry or assumptions beyond the presence of appropriately tuned efferent input (see below) and opposite-sign plasticity rules in dSPNs and iSPNs, due to the orthogonality of the sum and difference modes. An important capability enabled by our model is that action selection and efferent inputs can be multiplexed simultaneously, unlike the works cited above, which posit the existence of temporally segregated action-selection and learning phases of SPN activity.</p>
</sec>
<sec id="s3c">
<title>Biological substrates of striatal efferent inputs</title>
<p>Efferent inputs to the striatum must satisfy two important conditions for our model to learn correctly. First, they must be appropriately targeted: when an action is performed, dSPNs and iSPNs associated with that action must be excited, but other dSPNs and iSPNs must not be. The striatum receives topographically organized inputs from cortex (<xref ref-type="bibr" rid="c52">Peters et al., 2021</xref>) and thalamus (<xref ref-type="bibr" rid="c60">Smith et al., 2004</xref>), and SPNs tuned to the same behavior tend to be located nearby in space (<xref ref-type="bibr" rid="c4">Barbera et al., 2016</xref>; <xref ref-type="bibr" rid="c58">Shin et al., 2020</xref>; <xref ref-type="bibr" rid="c34">Klaus et al., 2017</xref>). This anatomical organization could enable action-specific efferent inputs. Another possibility is that targeting of efferent inputs could be tuned via plasticity during development. For instance, if a dSPN promotes a particular action, reward-independent Hebbian plasticity of its efferent inputs would potentiate those inputs that en-code the promoted action. Reward-independent anti-Hebbian plasticity would serve an analogous function for iSPNs. Alternatively, if efferent inputs are fixed, plasticity downstream of striatum could adapt the causal effect of SPNs to match their corresponding efferent input.</p>
<p>A second key requirement of our model is that efferent input synapses should not be adjusted according to the same reward-modulated plasticity rules as the feedforward corticostriatal inputs, as these rules would disrupt the targeting of efferent inputs to the corresponding SPNs. This may be achieved in multiple ways. One possibility is that efferent inputs project from different subregions or cell types than feedforward inputs and are subject to different forms of plasticity. Alternatively, efferent input synapses may have been sufficiently reinforced that they exist in a less labile, “consolidated” synaptic state. A third possibility is that the system may take advantage of latency in efferent activity. Spike timing dependence in SPN input plasticity has been observed in several studies (<xref ref-type="bibr" rid="c57">Shen et al., 2008</xref>; <xref ref-type="bibr" rid="c22">Fino et al., 2005</xref>; <xref ref-type="bibr" rid="c50">Pawlak and Kerr, 2008</xref>; <xref ref-type="bibr" rid="c23">Fisher et al., 2017</xref>). This timing dependence could make plasticity sensitive to paired activity in state inputs and SPNs while being insensitive to paired activity in efferent inputs and SPNs. Investigating the source of efferent inputs to SPNs and how it is differentiated from other inputs is an important direction for future work.</p>
</sec>
<sec id="s3d">
<title>Extensions and future work</title>
<p>We have assumed that the striatum selects among a finite set of actions, each of which corresponds to mutually uncorrelated patterns of SPN activity. In reality, there is evidence that the striatal code for action is organized such that kinematically similar behaviors are encoded by similar SPN activity patterns (<xref ref-type="bibr" rid="c34">Klaus et al., 2017</xref>; <xref ref-type="bibr" rid="c41">Markowitz et al., 2018</xref>). Other work has shown that the dorsolateral striatum can exert influence over detailed kinematics of learned motor behaviors, rather than simply select among categorically distinct actions (<xref ref-type="bibr" rid="c15">Dhawale et al., 2021</xref>). A more continuous, structured code for action in dorsolateral striatum is useful in allowing reinforcement learning to generalize between related actions. The ability afforded by our model to multiplex arbitrary action selection and learning signals may facilitate these more sophisticated coding schemes. For instance, reinforcement learning in continuous-valued action spaces requires a three-factor learning rule in which the postsynaptic activity factor represents the discrepancy between the selected action and the action typically selected in the current behavioral state (<xref ref-type="bibr" rid="c39">Lindsey and Litwin-Kumar, 2022</xref>), which in our model would be represented by efferent activity in SPNs. Investigating such extensions to our model and their consequences for SPN tuning is an interesting future direction.</p>
<p>In this work we find strong empirical evidence for our model of efferent activity in SPNs and show that in principle it enables off-policy reinforcement learning capabilities. A convincing experimental demonstration of off-policy learning capabilities would require a way of identifying the causal contribution of SPN activity to action selection, in order to distinguish between actions that are consistent (on-policy) or inconsistent (off-policy) with SPN outputs. This could be achieved through targeted stimulation of SPN populations, or by recording SPN activity during behaviors that are known to be independent of striatal influence (<xref ref-type="bibr" rid="c45">Mizes et al., 2023</xref>). Simultaneous recordings in SPNs and other brain regions would also facilitate distinguishing between actions driven by striatum from those driven by other pathways. Our model predicts that the relative strength of fluctuations in difference mode versus sum mode activity should be greatest during striatum-driven actions. Such experimental design would also enable a stronger test of the Q-learning model of dopamine activity: actions driven by other regions should lead to increased dopamine activity, as they will be predicted according to the striatum’s learned action-values to have low value.</p>
<p>In our model, the difference between dSPN and iSPN plasticity rules is key to enabling multiplexing of action-selection and learning-related activity without interference. Observed plasticity rules elsewhere in the brain are also heterogeneous; for instance, both Hebbian and anti-Hebbian behavior are observed in cortico-cortical connections (<xref ref-type="bibr" rid="c35">Koch et al., 2013</xref>; <xref ref-type="bibr" rid="c10">Chindemi et al., 2022</xref>). It is an interesting question whether a similar strategy may be employed outside the striatum, and in other contexts besides reinforcement learning, to allow simultaneous encoding of behavior and learning-related signals without interference.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We thank Jaeeon Lee for providing the initial inspiration for this project, Sean Escola for fruitful discussions, and Steven A. Siegelbaum for comments on the manuscript. J.L. is supported by the Mathers Foundation and the Gatsby Charitable Foundation. J.M. is supported by a Career Award at the Scientific Interface from the Burroughs Wellcome Fund, a fellowship from the Sloan Foundation, and a fellowship from the David and Lucille Packard Foundation. S.R.D. is supported by NIH grants RF1AG073625, R01NS114020, U24NS109520, the Simons Foundation Autism Research Initiative, and the Simons Collaboration on Plasticity and the Aging Brain. A.L.-K. is supported by the Mathers Foundation, the Burroughs Wellcome Foundation, the McKnight Endowment Fund, and the Gatsby Charitable Foundation.</p>
</ack>
<sec id="s4">
<title>Declaration of interests</title>
<p>S.R.D. sits on the scientific advisory boards of Neumora and Gilgamesh Therapeutics, which have licensed or sub-licensed the MoSeq technology.</p>
</sec>
<sec id="s5">
<title>Methods</title>
<sec id="s5a">
<title>Numerical simulations</title>
<p>Coding implementing the model will be made available upon publication.</p>
</sec>
<sec id="s5b">
<title>Basic model architecture</title>
<p>In our simulated learning tasks, we used networks with the following architecture.</p>
<p>SPNs receive inputs from cortical neurons. In our simulated go/no-go tasks, there is a single cortical input neuron (representing a task cue) with activity equal to 1 on each trial. In simulated tasks with multiple different task cues (such as the two-alternative forced choice task), there is a population of cortical input neurons, each of which is active with activity 1 when the corresponding task cue is presented and 0 otherwise. The task cue is randomly chosen with uniform probability each trial.</p>
<p>For each of the <italic>A</italic> actions available to the model, there is an assigned dSPN and iSPN. We choose to use a single neuron per action for simplicity of the model, but our model could easily be generalized to use population activity to encode actions. The activities of the dSPN and iSPN associated with action <italic>a</italic> are denoted as <inline-formula><inline-graphic xlink:href="580408v2_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and<inline-formula><inline-graphic xlink:href="580408v2_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, respectively. Each dSPN and iSPN receives inputs from <italic>M</italic> cortical neurons, and the synaptic input weights from cortical neuron <italic>j</italic> to dSPN or iSPN associated with action <italic>a</italic> are denoted as <inline-formula><inline-graphic xlink:href="580408v2_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> or <inline-formula><inline-graphic xlink:href="580408v2_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Feedforward SPN activity is given by
<disp-formula id="eqn8">
<graphic xlink:href="580408v2_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn9">
<graphic xlink:href="580408v2_eqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>ϕ</italic> is a nonlinear activation function. We choose <italic>ϕ</italic> to be the rectified linear function: <italic>ϕ</italic>(<italic>h</italic>) = max(0, <italic>h</italic>).</p>
<p>Action selecton depends on SPN activity in the following manner. The log-likelihood of an action <italic>a</italic> being performed is proportional to <inline-formula><inline-graphic xlink:href="580408v2_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. That is, dSPN activity increases the likelihood of taking the action and iSPN activity decreases the likelihood of taking the action.</p>
<p>Concretely, the probability of action <italic>a</italic> being taken is:
<disp-formula id="eqn10">
<graphic xlink:href="580408v2_eqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>β</italic> is a parameter controlling the degree of stochasticity in action selection (higher <italic>β</italic> corresponds to more deterministic choices), and <italic>c</italic> controls the probability that no action is taken. In the simulated go/no-go tasks we choose <italic>c</italic><sub>no−go</sub> = 1 and in the tasks involving selection among multiple actions we choose <italic>c</italic><sub>no−go</sub> = 0. Except where otherwise noted we used <italic>β</italic> = 10.0 in all task simulations.</p>
</sec>
<sec id="s5c">
<title>Models of SPN activity following action selection</title>
<p>In the “canonical action selection model” (<xref rid="fig1" ref-type="fig">Fig. 1</xref>), following action selection, the activity of the dSPN associated with the selected action and the activity of all iSPNs associated with unselected actions are set to 1. Biologically, this activity pattern can be implemented via effective mutual inhibition between SPNs with opponent functions (dSPNs tuned to different actions, iSPNs tuned to different actions, and dSPN/iSPN pairs tuned to the same action) and mutual excitation between SPNs with complementary functions (dSPNs tuned to one action and iSPNs to another) (<xref ref-type="bibr" rid="c7">Burke et al., 2017</xref>).</p>
<p>In the proposed efference model, following selection of an action <italic>a</italic><sup>∗</sup>, activity of the SPNs associated with action <italic>a</italic><sup>∗</sup> is updated as follows:
<disp-formula id="eqn11">
<graphic xlink:href="580408v2_eqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn12">
<graphic xlink:href="580408v2_eqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn13">
<graphic xlink:href="580408v2_eqn13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where 1[<italic>a</italic> = <italic>a</italic><sup>∗</sup>] equals 1 for <italic>a</italic> = <italic>a</italic><sup>∗</sup> and 0 otherwise. The parameter <italic>c</italic> controls the strength of efferent excitation.</p>
</sec>
<sec id="s5d">
<title>Learning rules</title>
<p>In all models, SPN input weights are initialized at 1 and weight updates proceed according to the plasticity rules given below:
<disp-formula id="eqn14">
<graphic xlink:href="580408v2_eqn14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn15">
<graphic xlink:href="580408v2_eqn15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>α</italic> is a learning rate, set to 0.05 throughout all learning simulations (except the tutoring simulations of <xref rid="fig7" ref-type="fig">Fig. 7</xref> where it is set to 0.01). In the paper we experiment with various choices of <italic>f</italic> dSPN and <italic>f</italic> iSPN.
<disp-formula id="eqn16">
<graphic xlink:href="580408v2_eqn16.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn17">
<graphic xlink:href="580408v2_eqn17.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn18">
<graphic xlink:href="580408v2_eqn18.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with the offset s igmoid p arameters chosen a s <italic>a</italic> = −3.5, <italic>b</italic> = 11.5, <italic>c</italic> = 0.9, <italic>d</italic> = 1 (taken f rom <xref ref-type="bibr" rid="c13">Cruz et al. (2022)</xref>). The quantity <italic>δ</italic> indicates an estimate of reward prediction error. In our experiments in <xref rid="fig2" ref-type="fig">Fig 2</xref> and <xref rid="fig3" ref-type="fig">Fig. 3</xref> we use temporal difference learing to compute <italic>δ</italic>:
<disp-formula id="eqn19">
<graphic xlink:href="580408v2_eqn19.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn20">
<graphic xlink:href="580408v2_eqn20.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>α</italic><sub><italic>V</italic></sub> is a learning rate, set to 0.05 throughout all learning simulations (except the tutoring simulations of <xref rid="fig7" ref-type="fig">Fig. 7</xref> where it is set to 0.25) and <italic>s</italic> indicates the cortical input state (indicating which cue is being presented). <italic>V</italic> (<italic>s</italic>) is initialized at 0.</p>
<p>In our experiments in <xref rid="fig7" ref-type="fig">Fig. 7</xref> we use Q-learning to enable off-policy learning, corresponding to the following value for <italic>δ</italic>:
<disp-formula id="eqn21">
<graphic xlink:href="580408v2_eqn21.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>a</italic> indicates the action that was just taken in response to state <italic>s</italic>, and <italic>Q</italic>(<italic>s, a</italic>) is taken to be equal to the striatal output <inline-formula><inline-graphic xlink:href="580408v2_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in response to the state <italic>s</italic>.</p>
</sec>
<sec id="s5e">
<title>Firing rate simulations</title>
<p>In each trial of the two-alternative forced choice task (<xref rid="fig4" ref-type="fig">Fig. 4</xref>), one of two stimuli is presented for 2 s. Cortical activity <bold>x</bold> representing the stimulus is encoded in a one-hot vector. Four SPNs are modeled, one dSPN and one iSPN for each of two actions. The dynamics of SPN <italic>i</italic> follows:
<disp-formula id="eqn22">
<graphic xlink:href="580408v2_eqn22.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here, <italic>τ</italic> = 100 ms, [·]<sub>+</sub> denotes positive rectification, <italic>w</italic><sub><italic>ij</italic></sub> represent corticostriatal weights initialized following a Gaussian distribution with mean 0 and standard deviation 1 Hz, <italic>η</italic><sub><italic>i</italic></sub>(<italic>t</italic>) is an Ornstein-Uhlenbeck noise process with time constant 600 ms and variance 1/60 Hz<sup>2</sup>, <italic>e</italic><sub><italic>i</italic></sub>(<italic>t</italic>) denotes efferent input, and <italic>b</italic> = 5 Hz is a bias term. Simulations were performed with <italic>dt</italic> = 20 ms.</p>
<p>On each trial, an action is selected based on the average difference-mode activity for the two actions during the first 1 s of stimulus presentation. In the second half of the stimulus presentation period, efferent input is provided to the dSPN and iSPN corresponding to the chosen action by setting <italic>e</italic><sub><italic>i</italic></sub>(<italic>t</italic>) = 7.5 Hz for these neurons. Learning proceeds according to
<disp-formula id="eqn23">
<graphic xlink:href="580408v2_eqn23.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where in the second half of the stimulus presentation period <italic>f</italic><sub><italic>i</italic></sub>(<italic>δ</italic>) = 1 for dSPNs after a correct action is taken and iSPNs after an incorrect action is taken, and -1 otherwise, and <italic>η</italic> = 5 <italic>×</italic> 10<sup>−4</sup> ms<sup>−1</sup>.</p>
</sec>
<sec id="s5f">
<title>Experimental prediction simulations</title>
<p>For the model predictions of <xref rid="fig5" ref-type="fig">Fig. 5</xref> and <xref rid="fig6" ref-type="fig">Fig. 6</xref>, we used the following parameters: <italic>A</italic> = 50, <italic>β</italic> = 100, <italic>c</italic><sub>efference</sub> = 1.5 and set <italic>c</italic><sub>no−go</sub> such that the no-action option was chosen 50% of the time. Feedforward SPN activity was generated from a Gaussian process with kernel <inline-formula><inline-graphic xlink:href="580408v2_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (exponentially decaying autocorrelation with a time constant of 10 timesteps). Efference activity also decayed exponentially with a time constant of 10 timesteps. Action selection occured every 10 timesteps based on the SPN activity at the preceding timestep.</p>
</sec>
<sec id="s5g">
<title>Neural data analysis</title>
<p>For our analysis of SPN data we used recordings previously described by <xref ref-type="bibr" rid="c41">Markowitz et al. (2018)</xref>. For our analysis of dopamine data we used the recordings described in <xref ref-type="bibr" rid="c42">Markowitz et al. (2023)</xref>.</p>
</sec>
<sec id="s5h">
<title>Fiber photometry data</title>
<p><italic>A</italic>deno-associated viruses (AAVs) expressing Cre-On jRCaMP1b and Cre-Off GCaMP6s were injected into the dorsolateral striatum (DLS) of <italic>n</italic> = 10 <italic>Drd1a-Cre</italic> mice to measure bulk dSPN (red) and iSPN (green) activity via multicolor photometry. Activity of each indicator was recorded at a rate of 30Hz using an optical fiber implanted in the right DLS. Data was collected during spontaneous behavior in a circular open field, for 5-6 sessions of 20 minutes each for each mouse. In the reversed indicator experiments of <xref ref-type="fig" rid="figS3">Supp. Fig. 3</xref>, <italic>A2a-Cre</italic> mice were injected with a mixture of the same AAVs, labeling iSPNs with jRCaMP1b (red) and dSPNs with GCaMP6s (green). More details are reported in <xref ref-type="bibr" rid="c41">Markowitz et al. (2018)</xref>.</p>
<p>In our data analyses in <xref rid="fig5" ref-type="fig">Fig. 5C</xref> and <xref ref-type="fig" rid="figS3">Supp. Fig 3</xref>, for each session (<italic>n</italic> = 48 and <italic>n</italic> = 8, respectively) we computed the autocorrelation and cross-correlation of the dSPN and iSPN indicator activity across the entire session.</p>
</sec>
<sec id="s5i">
<title>Miniscope data</title>
<p><italic>Drd1a-Cre</italic> AAVs expressing GCaMP6f were injected into the right DLS of <italic>n</italic> = 4 Drd1a-Cre mice (to label dSPNs) and <italic>n</italic> = 6 <italic>A2a-Cre</italic> mice (to label iSPNs). A head-mounted single-photon microscope was coupled to a gradient index lens implanted into the dorsal striatum above the injection site.</p>
<p>Recordings were made, as for the photometry data, during spontaneous behavior in a circular open field. Calcium activity was recorded from a total of 653 dSPNs and 794 iSPNs for these mice, with the number of neurons per mouse ranging from 27–336. To enable simultaneous recording of dSPNs and iSPNs in the same mice, a different protocol was used: <italic>Drd1a-Cre</italic> mice were injected with an AAV mixture which labeled both dSPNs and iSPNS with GCaMP6s, but additionally selectively labeled dSPNS with nuclear-localized dTomato. This procedure enabled (in <italic>n</italic> = 4 mice) cell-type identification of dSPNs vs. iSPNs with a two-photon microscope which was cross-referenced with the single-photon microscope recordings. More details are given in <xref ref-type="bibr" rid="c41">Markowitz et al. (2018)</xref>. In our analyses, these data were used for the simultaneous-recording analyses in <xref rid="fig6" ref-type="fig">Fig. 6L</xref>,<xref rid="fig6" ref-type="fig">M</xref>,<xref rid="fig6" ref-type="fig">N</xref>,<xref rid="fig6" ref-type="fig">O</xref> and were also combined with the appropriate single-pathway data in the analyses of <xref rid="fig6" ref-type="fig">Fig. 6J</xref>,<xref rid="fig6" ref-type="fig">K</xref>.</p>
</sec>
<sec id="s5j">
<title>Behavioral data</title>
<p>Mouse behavior in the circular open field was recorded as follows: 3D pose information was recorded using a depth camera at a rate of 30Hz. The videos were preprocessed to center the mouse and align the nose-to-tail axis across frames and remove occluding objects. The videos were then fed through PCA to reduce the dimensinoality of the data and fed into the MoSeq algorithm (<xref ref-type="bibr" rid="c67">Wiltschko et al., 2015</xref>) which fits a generative model to the video data that automatically infers a set of behavioral “syllables” (repeated, stereotyped behavioral kinematics) and assigns each frame of the video to one of these syllables. More details on MoSeq are given in <xref ref-type="bibr" rid="c67">Wiltschko et al. (2015)</xref> and more details on its application to this dataset are given in <xref ref-type="bibr" rid="c41">Markowitz et al. (2018)</xref>. There were 89 syllables identified by MoSeq that appear across all the sessions. We restricted our analysis to the set of 62 syllables that appear at least 5 times in each behavioral session.</p>
</sec>
<sec id="s5k">
<title>Syllable-tuned SPN activity mode analysis</title>
<p>In our analysis, we first z-scored the activity of each neuron across the data collected for each mouse. We divided the data by the boundaries of behavioral syllables and split it into two equally sized halves (based on whether the timestamp, rounded to the nearest second, of the behavioral syllable was even or odd). To compute the activity modes associated with each behavioral syllable, we first computed the average change in activity for each neuron during each syllable and fit a linear regression model to predict this increase from a one-hot vector indicating the syllable identity. The resulting coefficients of this regression indicate the directions (“modes”) in activity space that increase the most during performance of each of the behavioral syllables. We linearly time-warped the data in each session based on the boundaries of each MoSeq-identified behavioral syllable, such that in the new time coordinates each behavioral syllable lasted 10 timesteps. The time course of the projection of SPN activity along the modes associated with each behavioral syllable was then computed around the onset of that syllable, or around all other sllables. As a way of crossvalidating the analysis, we performed the regression on one half of the data and plotted the average mode activity on the other half of the data (in both directions, and averaged the results). We averaged the resulting time courses of mode activity across all choices of behavioral syllables. This analysis was performed for each mouse and the results in <xref rid="fig6" ref-type="fig">Fig. 6J</xref>,<xref rid="fig6" ref-type="fig">K</xref>,<xref rid="fig6" ref-type="fig">L</xref>,<xref rid="fig6" ref-type="fig">M</xref>,<xref rid="fig6" ref-type="fig">N</xref>,<xref rid="fig6" ref-type="fig">O</xref> show means and standard errors across mice.</p>
</sec>
<sec id="s5l">
<title>Dopamine activity data and analysis</title>
<p>For 7E we used data from <xref ref-type="bibr" rid="c42">Markowitz et al. (2023)</xref>. Mice (<italic>n</italic> = 14) virally expressing the dopamine reporter dLight1.1 in the DLS were recorded with a fiber cannula implanted above the injection site. Mice were placed in a circular open field for 30 minute sessions and allowed to behave freely while spontaneous dLight activity was recorded. MoSeq (described above) was used to infer a set of <italic>S</italic>57 behavioral syllables observed across all sessions. As in <xref ref-type="bibr" rid="c42">Markowitz et al. (2023)</xref>, the data were preprocessed by computing the maximum dLight value during each behavioral syllable. These per-syllable dopamine values were z-scored across each session and used as our measure of dopamine activity during each syllable. We then computed an <italic>S × S</italic> table of the average dopamine activity during each syllable <italic>s</italic><sub><italic>t</italic></sub> conditioned on the previous syllable having been syllable <italic>s</italic><sub><italic>t</italic>−1</sub>, denoted as <italic>D</italic>(<italic>s</italic><sub><italic>t</italic>−1</sub>, <italic>s</italic><sub><italic>t</italic></sub>). We also computed the <italic>S × X</italic> table of probabilities of transitioning from syllable <italic>s</italic><sup><italic>′</italic></sup> to syllable <italic>s</italic> across the dataset, denoted as <italic>P</italic> (<italic>s</italic><sub><italic>t</italic>−1</sub>, <italic>s</italic><sub><italic>t</italic></sub>). These tables were computed separately for each mouse. In <xref rid="fig7" ref-type="fig">Fig. 7E</xref> we report the pearson correlation coefficient between the predicted and actual values of <italic>P</italic> (<italic>s</italic><sub><italic>t</italic>−1</sub>, <italic>s</italic><sub><italic>t</italic></sub>). We then experimented with several alternative models (see Supplemental Information) that predict <italic>P</italic> (<italic>s</italic><sub><italic>t</italic>−1</sub>, <italic>s</italic><sub><italic>t</italic></sub>) based on <italic>D</italic>(<italic>s</italic><sub><italic>t</italic>−1</sub>, <italic>s</italic><sub><italic>t</italic></sub>). In <xref rid="fig7" ref-type="fig">Fig. 7E</xref> we report the pearson correlation coefficient between the predicted and actual values of <italic>P</italic> (<italic>s</italic><sub><italic>t</italic>−1</sub>, <italic>s</italic><sub><italic>t</italic></sub>).</p>
</sec>
</sec>
<sec id="s6">
<title>Supplemental information</title>
<sec id="s6a">
<title>Model of go/no-go task</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Supplemental Fig. 1:</label>
<caption><p>Go/no-go task. <bold>A</bold>. Example in which dSPN plasticity produces correct learning behavior in a go/no-go task. Left: cortical inputs to the dSPN and iSPN are equal prior to learning. Middle: the “go” response is selected, corresponding to elevated dSPN activity. In this example, the “go” response is rewarded, leading to elevated DA activity and thus potentiation of the dSPN input synapse. Right: in a subsequent trial, cortical input to the dSPN is stronger, increasing the likelihood of selecting the “go” response. <bold>B</bold>. Example in which iSPN plasticity produces incorrect learning behavior in a go/no-go task. Left: same as panel B. Middle: the “no go” response is selected, corresponding to elevated iSPN activity. In this example, the “no-go” response is punished, leading to decreased DA activity and thus potentiation of the iSPN input synapse. Right: in a subsequent trial, cortical input to the iSPN is stronger, decreasing the likelihood of selecting the “go” response. <bold>C</bold>. Illustration of the efference model in a go/no-go task. Left: feedforward SPN activity driven by cortical inputs. Right: once the “go” response is selected, the dSPN and iSPN are both excited by efferent input, which is combined with their original input. As a result, both the dSPN and iSPN are more active than prior to action selection, but the dSPN is still more active than the iSPN.</p></caption>
<graphic xlink:href="580408v2_figS1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s6b">
<title>Relationship between sum mode activity and future difference mode activity</title>
<p>In the main text we provided an argument for why sum mode activity drives changes to future difference mode activity, assuming a linear <italic>f</italic> <sup>d<italic>/</italic>iSPN</sup>(<italic>δ</italic>) and linear neural activation functions. Here we generalize this argument to more general learning rules and activation functions <italic>ϕ</italic>, assuming only that <italic>f</italic> <sup>dSPN</sup>(<italic>δ</italic>) is monotonically increasing, <italic>f</italic> <sup>iSPN</sup>(<italic>δ</italic>) is monotonically increasing, and <italic>ϕ</italic>(·) is monotonically increasing. We have that <italic>y</italic><sup>d<italic>/</italic>iSPN</sup> = <italic>ϕ</italic>(<bold>w</bold><sup>d<italic>/</italic>iSPN</sup> · <bold>x</bold>), and <italic>δ</italic><bold>w</bold><sup>d<italic>/</italic>iSPN</sup> = (<italic>f</italic> <sup>d<italic>/</italic>iSPN</sup>(<italic>δ</italic>) · <italic>y</italic><sup>d<italic>/</italic>iSPN</sup>)<bold>x</bold>. Thus, in the limit of small small weight updates, we can write:
<disp-formula id="eqn24">
<graphic xlink:href="580408v2_eqn24.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>c</italic><sup>dSPN</sup> and <italic>c</italic><sup>iSPN</sup> are nonnegative because <italic>ϕ</italic><sup><italic>′</italic></sup> is always nonnegative by assumption. Snce by assumption <italic>f</italic> <sup>d<italic>/</italic>iSPN</sup> are increasing/decreasing, respectively, the first term of the above sum has nonnegative correlation with <italic>δy</italic><sup>dSPN</sup> and the second term has nonnegative correlation with <italic>δy</italic><sup>iSPN</sup>. Thus, changes Δ(<italic>y</italic><sup>dSPN</sup> − <italic>y</italic><sup>iSPN</sup>) to difference mode activity are always nonnegatively correlated with sum mode activity. If we assume that efferent excitation is always sufficiently strong that <italic>c</italic><sup>dSPN</sup> = <italic>ϕ</italic><sup><italic>′</italic></sup>(<bold>w</bold><sup>dSPN</sup> · <bold>x</bold>) and <italic>c</italic><sup>iSPN</sup> = <italic>ϕ</italic><sup><italic>′</italic></sup>(<bold>w</bold><sup>iSPN</sup> · <bold>x</bold>) are positive, and that there are no values of <italic>δ</italic> for which <italic>f</italic> <sup>d<italic>/</italic>iSPN</sup>(<italic>δ</italic>) both have zero derivative, we can further guarantee that changes to difference mode activity will always be <italic>positively</italic> correlated with sum mode activity.</p>
</sec>
<sec id="s6c">
<title>Generalizing the model to a distributed code for actions</title>
<p>In our model simulations in the main text we assumed for convenience that there is a single dSPN and iSPN that promote and suppress each available action, respectively. It is more realistic to model the code for action as distributed among many SPNs. Our model generalizes easily to this case; all that is necessary is for the efferent activity following action selection to excite the vectors (for both dSPNs and iSPNs) in population activity space corresponding to that action. To demonstrate this, we conducted a simulation with <italic>N</italic> = 1000 dSPNs and iSPNs each, <italic>S</italic> = 10 input cues (one-hot input vectors), and <italic>A</italic> = 10 actions, with one correct action for each input state. Feedforward SPN activity is given by
<disp-formula id="eqn25">
<graphic xlink:href="580408v2_eqn25.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn26">
<graphic xlink:href="580408v2_eqn26.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The log-likelihood of an action <italic>a</italic> being performed is proportional to
<disp-formula id="eqn27">
<graphic xlink:href="580408v2_eqn27.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="580408v2_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="580408v2_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are randomly sampled uniformly in the interval [0, 1] and then normalized so that each vector <inline-formula><inline-graphic xlink:href="580408v2_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="580408v2_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula> has norm 1. Thus, the contribution of each dSPN/iSPN to the promotion/suppression of each action is randomly distributed.</p>
<p>In the efference model, following selection of an action <italic>a</italic><sup>∗</sup>, activity of the SPNs associated with action <italic>a</italic>∗ is updated as follows, so that efference activity excites the modes <inline-formula><inline-graphic xlink:href="580408v2_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="580408v2_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula> associated with the selected action:
<disp-formula id="eqn28">
<graphic xlink:href="580408v2_eqn28.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn29">
<graphic xlink:href="580408v2_eqn29.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn30">
<graphic xlink:href="580408v2_eqn30.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We also experiment with a generalization of the canonical action selection model to this distributed action tuning architecture, in which following action selection, SPN activity is set to
<disp-formula id="eqn31">
<graphic xlink:href="580408v2_eqn31.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn32">
<graphic xlink:href="580408v2_eqn32.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn33">
<graphic xlink:href="580408v2_eqn33.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
In this model, dSPNs are excited in proportion to their contribution to the currently selected action and iSPNs are suppressed in proportion to their degree of inhibition of the currently selected action.</p>
<p>The plasticity rules used are the same as in the main text.</p>
<p>We find that the results of the main text – that the canonical action selection model fails to learn from negative rewards, while the efference model successully learns from both reward protocols – is replicated (<xref ref-type="fig" rid="figS2">Supp. Fig. 2</xref>).</p>
<fig id="figS2" position="float" fig-type="figure">
<label>Supplemental Fig. 2:</label>
<caption><p>Performance of striatal RL models with a distributed code for actions on a task with 10 cortical input states, 10 available actions, and one correct action for each input state.</p></caption>
<graphic xlink:href="580408v2_figS2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s6d">
<title>Photometry analysis with reversed indicators</title>
<fig id="figS3" position="float" fig-type="figure">
<label>Supplemental Fig. 3:</label>
<caption><p>Same as <xref rid="fig5" ref-type="fig">Fig. 5C</xref>, but performing the analysis on subjects with reversed assignment of indicators to SPN types.</p></caption>
<graphic xlink:href="580408v2_figS3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s6e">
<title>Comparison of selectivity of dSPNs and iSPNs</title>
<fig id="figS4" position="float" fig-type="figure">
<label>Supplemental Fig. 4:</label>
<caption><p>Comparison of dSPN and iSPN tuning selectivity. Violin plots indicate the distribution of selectivity values across all neurons computed using <xref ref-type="disp-formula" rid="eqn34">Eq. 34</xref>, using either unsigned (left) or rectified (right) z-scored activity as the raw measure of a neuron’s tuning to a behavioral syllable. Horizontal lines indicate the 0, 25, 50, 75, 100 percentile values of the distribution.</p></caption>
<graphic xlink:href="580408v2_figS4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To test whether dSPNs or iSPNs exhibit greater or less specificity in their tuning to behaviors, we computed the selectivity of each neuron in the imaging data of <xref rid="fig6" ref-type="fig">Fig. 6</xref>. For each neuron, we computed its average z-scored activity <italic>a</italic><sub><italic>i</italic></sub> in response to each of the behavioral syllables <italic>i</italic> ∈ {1, …, <italic>A</italic>} in the dataset. Common measures of selectivity require a nonnegative measurement of a neuron’s tuning to a given condition. Thus, we conducted the analysis in two ways, using either the unsigned activity |<italic>a</italic><sub><italic>i</italic></sub>| or the rectified activity max(<italic>a</italic><sub><italic>i</italic></sub>, 0) as the measure of the neuron’s tuning <italic>t</italic><sub><italic>i</italic></sub> to syllable <italic>i</italic>. The selectivity was then computed using the following expression introduced in prior work (<xref ref-type="bibr" rid="c62">Treves and Rolls, 1991</xref>; <xref ref-type="bibr" rid="c66">Willmore and Tolhurst, 2001</xref>):
<disp-formula id="eqn34">
<graphic xlink:href="580408v2_eqn34.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
This value ranges from 0 to 1, and higher value indicates that fluctuations in a neuron’s activity are driven primaril by one or a few behavioral syllables. The results are shown in <xref ref-type="fig" rid="figS4">Supp. Fig. 4</xref>. The selectivity values are fairly modest (consistent with a distributed code for actions) and comparable between dSPNs andn iSPNs.</p>
</sec>
<sec id="s6f">
<title>Alternative model with shared plasticity rule among all SPNs</title>
<fig id="figS5" position="float" fig-type="figure">
<label>Supplemental Fig. 5:</label>
<caption><p>Comparison to counterfactual model in which iSPNs use the same plasticity rule as dSPNs. A. Left: performance of simulated striatal RL system using efference model with the opponent dSPN/iSPN plasticity rules used elsewhere in the paper (black, same as <xref rid="fig3" ref-type="fig">Fig. 3E</xref>), and a system using the canonical action selection model and identical dSPN and iSPN plasticity rules (green). Right: same as left panel, but in an off-policy setting in which another pathway controls behavior during and always chooses the correct action, and the performance of the striatal RL system is evaluated over time. Here the Q-learning model of dopamine activity is used. B. In the counterfactual model in which iSPNs use the same plasticity rule as dSPNs, activity in the difference mode (dSPN - iSPN) influences (via plasticity) changes in future difference mode activity that affect decision-making.</p></caption>
<graphic xlink:href="580408v2_figS5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The issues identified in <xref rid="fig2" ref-type="fig">Fig. 2</xref> with the canonical action selection model are a consequence of the iSPN plasticity rule. From a normative perspective is interesting to consider why the empirically observed iSPN plasticity rule might be advantageous, compared to an alternative model in which iSPNs share the same plasticity rule as dSPNs. For instance, this alternative model can solve the two-alternative forced choice task of <xref rid="fig2" ref-type="fig">Fig. 2</xref> with both positive and negative reward protocols (<xref ref-type="fig" rid="figS5">Supp. Fig. 5A</xref>, left). However, the limitations of this alternative model are revealed in the off-policy learning setting, where the Q-learning algorithm is required. In this case, SPN activity must encode Q-values associated with each action, but in the canonical action selection model, these values are disrupted by the updates to SPN activity following action selection. This is because the activity updates in the canonical action selection model modify difference mode activity, which (when dSPN and iSPN plasticity rules are the same) is needed for learning (<xref rid="figS5" ref-type="fig">Supp. Fig. 5B</xref>). As a result, the predicted Q-values are inaccurate, and the model has difficulty learning the true value of each action. We demonstrate this in the two-alternative forced task in an off-policy learning protocol where an oracle chooses the correct action on each trial, and the striatal pathway’s ability to solve the task independently is evaluated. The efference activity model has no issue due to the orthogonality of the efferent activity and difference modes as described above, but the canonical action selection model fails to solve the task (<xref ref-type="fig" rid="figS5">Supp. Fig. 5A</xref>, right).</p>
<p>We note that non-orthogonality of the activity mode used for learning and behavior could cause other problems besides impairing the system’s ability to implement off-policy learning algorithms; for instance, even in an on-policy setting it could interfere with sequential action selection at rapid timescales.</p>
</sec>
<sec id="s6g">
<title>Models used for dopamine analysis</title>
<p>We experimented with models that predict transition probabilities <italic>P</italic> (<italic>s</italic><sub><italic>t</italic>−1</sub>, <italic>s</italic><sub><italic>t</italic></sub>) based on average dopamine activity <italic>D</italic>(<italic>s</italic><sub><italic>t</italic>−1</sub>, <italic>s</italic><sub><italic>t</italic></sub>) associated with each transition.</p>
<sec id="s6g1">
<title>Q-learning model</title>
<p>In the Q-learning model, the mouse maintains an internal estimate of the value <italic>Q</italic>(<italic>s</italic><sub><italic>t</italic>−1</sub>, <italic>s</italic><sub><italic>t</italic></sub>) of each transition between syllables. In the absence of explicit rewards, the dopamine activity associated with a syllable transition is predicted to be: <italic>D</italic>(<italic>s</italic><sub><italic>t</italic>−1</sub>, <italic>s</italic><sub><italic>t</italic></sub>) = max<sub><italic>s</italic></sub><italic>′ Q</italic>(<italic>s</italic><sub><italic>t</italic></sub>, <italic>s</italic><sup><italic>′</italic></sup>) − <italic>Q</italic>(<italic>s</italic><sub><italic>t</italic>−1</sub>, <italic>s</italic><sub><italic>t</italic></sub>). We inferred a set of Q-values by initializing a Q-table with all zero values and running gradient descent on the Q-table to minimize the mean squared error between the predicted and empirical values of <italic>D</italic>(<italic>s</italic><sub><italic>t</italic>−1</sub>, <italic>s</italic><sub><italic>t</italic></sub>). These inferred Q-values were used to predict behavioral transition probabilities according to:<inline-formula><inline-graphic xlink:href="580408v2_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We did not fit the value of <italic>β</italic>(<italic>s</italic><sub><italic>t</italic></sub> <italic>− 1</italic>) but rather chose it to be the reciprocal of the standard deviation of <italic>Q</italic>(<italic>s</italic><sub><italic>t</italic>−1</sub>, <italic>s</italic><sup><italic>′</italic></sup>) across all <italic>s</italic><sup><italic>′</italic></sup>, to ensure a reasonable dynamic range in predicted transition probabilities.</p>
</sec>
<sec id="s6g2">
<title>TD learning model</title>
<p>In this model, the mouse maintains an internal estimate of the value <italic>V</italic> (<italic>s</italic>) of each syllable, and the predicted dopamine activity at each transition is <italic>D</italic>(<italic>s</italic><sub><italic>t</italic>−1</sub>, <italic>s</italic><sub><italic>t</italic></sub>) = <italic>V</italic> (<italic>s</italic><sub><italic>t</italic></sub>) − <italic>V</italic> (<italic>s</italic><sub><italic>t</italic>−1</sub>). We fit the vector of values <italic>V</italic> (<italic>s</italic>) to minimize the mean squared error of predicted and empirical <italic>D</italic>(<italic>s</italic><sub><italic>t</italic>−1</sub>, <italic>s</italic><sub><italic>t</italic></sub>). The predicted transition probabilities in this modl (which are independent of the previous syllable <italic>s</italic> <sub><italic>t</italic>−1</sub>) are: <disp-formula><graphic xlink:href="580408v2_inline15.gif" mime-subtype="gif" mimetype="image"/></disp-formula> with <italic>β</italic> chosen to normalize the <italic>V</italic> (<italic>s</italic><sup><italic>′</italic></sup>) to have standard deviation 1, as in the previous models.</p>
</sec>
<sec id="s6g3">
<title>Action value model</title>
<p>In this model, we assume that dopamine activity simply reflects the probability of each transition rather than encoding a prediction error; that is, we assume <inline-formula><inline-graphic xlink:href="580408v2_inline16.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
</sec>
<sec id="s6g4">
<title>State value model</title>
<p>In this model, we assume that dopamine activity simply reflects the probability of each behavioral syllable being chosen and is independent of the previous syllable. That is, we compute the average dopamine activity <italic>D</italic>(<italic>s</italic>) associated with each syllable <italic>s</italic>, and predict <inline-formula><inline-graphic xlink:href="580408v2_inline17.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
</sec>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arulkumaran</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Deisenroth</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Brundage</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Bharath</surname>, <given-names>A. A.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Deep reinforcement learning: A brief survey</article-title>. <source>IEEE Signal Processing Magazine</source>, <volume>34</volume>(<issue>6</issue>):<fpage>26</fpage>–<lpage>38</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ashby</surname>, <given-names>F. G.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>B. O.</given-names></string-name>, and <string-name><surname>Horvitz</surname>, <given-names>J. C.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Cortical and basal ganglia contributions to habit learning and automaticity</article-title>. <source>Trends in cognitive sciences</source>, <volume>14</volume>(<issue>5</issue>):<fpage>208</fpage>–<lpage>215</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Balleine</surname>, <given-names>B. W.</given-names></string-name>, <string-name><surname>Delgado</surname>, <given-names>M. R.</given-names></string-name>, and <string-name><surname>Hikosaka</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2007</year>). <article-title>The role of the dorsal striatum in reward and decision-making</article-title>. <source>Journal of Neuroscience</source>, <volume>27</volume>(<issue>31</issue>):<fpage>8161</fpage>–<lpage>8165</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barbera</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Liang</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Gerfen</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Culurciello</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Lin</surname>, <given-names>D.-T.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Spatially compact neural clusters in the dorsal striatum encode locomotion relevant information</article-title>. <source>Neuron</source>, <volume>92</volume>(<issue>1</issue>):<fpage>202</fpage>–<lpage>213</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bariselli</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fobbs</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Creed</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Kravitz</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2019</year>). <article-title>A competitive model for striatal action selection</article-title>. <source>Brain research</source>, <volume>1713</volume>:<fpage>70</fpage>–<lpage>79</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bostan</surname>, <given-names>A. C.</given-names></string-name> and <string-name><surname>Strick</surname>, <given-names>P. L.</given-names></string-name></person-group> (<year>2018</year>). <article-title>The basal ganglia and the cerebellum: nodes in an integrated network</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>19</volume>(<issue>6</issue>):<fpage>338</fpage>–<lpage>350</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burke</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Rotstein</surname>, <given-names>H. G.</given-names></string-name>, and <string-name><surname>Alvarez</surname>, <given-names>V. A.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Striatal local circuitry: a new framework for lateral inhibition</article-title>. <source>Neuron</source>, <volume>96</volume>(<issue>2</issue>):<fpage>267</fpage>–<lpage>284</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calabresi</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Gubellini</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Centonze</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Picconi</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Bernardi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Chergui</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Svenningsson</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Fienberg</surname>, <given-names>A. A.</given-names></string-name>, and <string-name><surname>Greengard</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Dopamine and camp-regulated phosphoprotein 32 kda controls both striatal long-term depression and long-term potentiation, opposing forms of synaptic plasticity</article-title>. <source>Journal of Neuroscience</source>, <volume>20</volume>(<issue>22</issue>):<fpage>8443</fpage>–<lpage>8451</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cardinal</surname>, <given-names>R. N.</given-names></string-name>, <string-name><surname>Parkinson</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Hall</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Everitt</surname>, <given-names>B. J.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Emotion and motivation: the role of the amygdala, ventral striatum, and prefrontal cortex</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, <volume>26</volume>(<issue>3</issue>):<fpage>321</fpage>–<lpage>352</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chindemi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Abdellah</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Amsalem</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Benavides-Piccione</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Delattre</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Doron</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ecker</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jaquier</surname>, <given-names>A. T.</given-names></string-name>, <string-name><surname>King</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kumbhar</surname>, <given-names>P.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2022</year>). <article-title>A calcium-based plasticity model for predicting long-term potentiation and depression in the neocortex</article-title>. <source>Nature Communications</source>, <volume>13</volume>(<issue>1</issue>):<fpage>3038</fpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Collins</surname>, <given-names>A. G.</given-names></string-name> and <string-name><surname>Frank</surname>, <given-names>M. J.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Opponent actor learning (opal): modeling interactive effects of striatal dopamine on reinforcement learning and choice incentive</article-title>. <source>Psychological review</source>, <volume>121</volume>(<issue>3</issue>):<fpage>337</fpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Contreras-Vidal</surname>, <given-names>J. L.</given-names></string-name> and <string-name><surname>Schultz</surname>, <given-names>W.</given-names></string-name></person-group> (<year>1999</year>). <article-title>A predictive reinforcement model of dopamine neurons for learning approach behavior</article-title>. <source>Journal of computational neuroscience</source>, <volume>6</volume>(<issue>3</issue>):<fpage>191</fpage>–<lpage>214</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cruz</surname>, <given-names>B. F.</given-names></string-name>, <string-name><surname>Guiomar</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Soares</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Motiwala</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Machens</surname>, <given-names>C. K.</given-names></string-name>, and <string-name><surname>Paton</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Action suppression reveals opponent parallel control via striatal circuits</article-title>. <source>Nature</source>, <volume>607</volume>(<issue>7919</issue>):<fpage>521</fpage>–<lpage>526</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cui</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Jun</surname>, <given-names>S. B.</given-names></string-name>, <string-name><surname>Jin</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Pham</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Vogel</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Lovinger</surname>, <given-names>D. M.</given-names></string-name>, and <string-name><surname>Costa</surname>, <given-names>R. M.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Concurrent activation of striatal direct and indirect pathways during action initiation</article-title>. <source>Nature</source>, <volume>494</volume>(<issue>7436</issue>):<fpage>238</fpage>–<lpage>242</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dhawale</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Wolff</surname>, <given-names>S. B.</given-names></string-name>, <string-name><surname>Ko</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Ölveczky</surname>, <given-names>B. P.</given-names></string-name></person-group> (<year>2021</year>). <article-title>The basal ganglia control the detailed kinematics of learned motor skills</article-title>. <source>Nature neuroscience</source>, <volume>24</volume>(<issue>9</issue>):<fpage>1256</fpage>–<lpage>1269</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dreyer</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Herrik</surname>, <given-names>K. F.</given-names></string-name>, <string-name><surname>Berg</surname>, <given-names>R. W.</given-names></string-name>, and <string-name><surname>Hounsgaard</surname>, <given-names>J. D.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Influence of phasic and tonic dopamine release on receptor activation</article-title>. <source>Journal of Neuroscience</source>, <volume>30</volume>(<issue>42</issue>):<fpage>14273</fpage>–<lpage>14283</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dunovan</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Vich</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Clapp</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Verstynen</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Rubin</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Reward-driven changes in striatal pathway competition shape evidence evaluation in decision-making</article-title>. <source>PLoS computational biology</source>, <volume>15</volume>(<issue>5</issue>):<fpage>e1006998</fpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Exner</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Koschack</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Irle</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2002</year>). <article-title>The differential role of premotor frontal cortex and basal ganglia in motor sequence learning: evidence from focal basal ganglia lesions</article-title>. <source>Learning &amp; Memory</source>, <volume>9</volume>(<issue>6</issue>):<fpage>376</fpage>–<lpage>386</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fee</surname>, <given-names>M. S.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Oculomotor learning revisited: a model of reinforcement learning in the basal ganglia incorporating an efference copy of motor actions</article-title>. <source>Frontiers in neural circuits</source>, <volume>6</volume>:<fpage>38</fpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fee</surname>, <given-names>M. S.</given-names></string-name></person-group> (<year>2014</year>). <article-title>The role of efference copy in striatal learning</article-title>. <source>Current opinion in neurobiology</source>, <volume>25</volume>:<fpage>194</fpage>–<lpage>200</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fee</surname>, <given-names>M. S.</given-names></string-name> and <string-name><surname>Goldberg</surname>, <given-names>J. H.</given-names></string-name></person-group> (<year>2011</year>). <article-title>A hypothesis for basal ganglia-dependent reinforcement learning in the songbird</article-title>. <source>Neuroscience</source>, <volume>198</volume>:<fpage>152</fpage>–<lpage>170</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fino</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Glowinski</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Venance</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Bidirectional activity-dependent plasticity at corticostriatal synapses</article-title>. <source>Journal of Neuroscience</source>, <volume>25</volume>(<issue>49</issue>):<fpage>11279</fpage>–<lpage>11287</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fisher</surname>, <given-names>S. D.</given-names></string-name>, <string-name><surname>Robertson</surname>, <given-names>P. B.</given-names></string-name>, <string-name><surname>Black</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Redgrave</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Sagar</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Abraham</surname>, <given-names>W. C.</given-names></string-name>, and <string-name><surname>Reynolds</surname>, <given-names>J. N.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Reinforcement determines the timing dependence of corticostriatal synaptic plasticity in vivo</article-title>. <source>Nature communications</source>, <volume>8</volume>(<issue>1</issue>):<fpage>334</fpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frank</surname>, <given-names>M. J.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Dynamic dopamine modulation in the basal ganglia: a neurocomputational account of cognitive deficits in medicated and nonmedicated parkinsonism</article-title>. <source>Journal of cognitive neuroscience</source>, <volume>17</volume>(<issue>1</issue>):<fpage>51</fpage>–<lpage>72</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Freeze</surname>, <given-names>B. S.</given-names></string-name>, <string-name><surname>Kravitz</surname>, <given-names>A. V.</given-names></string-name>, <string-name><surname>Hammack</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Berke</surname>, <given-names>J. D.</given-names></string-name>, and <string-name><surname>Kreitzer</surname>, <given-names>A. C.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Control of basal ganglia output by direct and indirect pathway projection neurons</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume>(<issue>47</issue>):<fpage>18531</fpage>–<lpage>18539</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gurney</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Humphries</surname>, <given-names>M. D.</given-names></string-name>, and <string-name><surname>Redgrave</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2015</year>). <article-title>A new framework for cortico-striatal plasticity: behavioural theory meets in vitro data at the reinforcement-action interface</article-title>. <source>PLoS biology</source>, <volume>13</volume>(<issue>1</issue>):<fpage>e1002034</fpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Houk</surname>, <given-names>J. C.</given-names></string-name> and <string-name><surname>Adams</surname>, <given-names>J. L.</given-names></string-name></person-group> (<year>1995</year>). <chapter-title>A model of how the basal ganglia generate and use neural signals that predict reinforcement</chapter-title>. <person-group person-group-type="editor"><string-name><surname>Houk</surname>, <given-names>J. C.</given-names></string-name> and <string-name><surname>Davis</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Beiser</surname>, <given-names>D. G.</given-names></string-name></person-group> <source>Models of information processing in the basal ganglia</source>, page <fpage>249</fpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hwang</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Dahlen</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>Y. Y.</given-names></string-name>, <string-name><surname>Aguilar</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Mukundan</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Mitani</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Komiyama</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Disengagement of motor cortex from movement control during long-term learning</article-title>. <source>Science advances</source>, <volume>5</volume>(<issue>10</issue>):<fpage>eaay0001</fpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Iino</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Sawada</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Yamaguchi</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Tajiri</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ishii</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kasai</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Yagishita</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Dopamine d2 receptors in discrimination learning and spine enlargement</article-title>. <source>Nature</source>, <volume>579</volume>(<issue>7800</issue>):<fpage>555</fpage>– <lpage>560</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ito</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Doya</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Multiple representations and algorithms for reinforcement learning in the cortico-basal ganglia circuit</article-title>. <source>Current opinion in neurobiology</source>, <volume>21</volume>(<issue>3</issue>):<fpage>368</fpage>–<lpage>373</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jaskir</surname>, <given-names>A.</given-names></string-name> and <string-name><surname>Frank</surname>, <given-names>M. J.</given-names></string-name></person-group> (<year>2023</year>). <article-title>On the normative advantages of dopamine and striatal opponency for learning and choice</article-title>. <source>Elife</source>, <volume>12</volume>:<elocation-id>e85107</elocation-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Joel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Niv</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Ruppin</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Actor–critic models of the basal ganglia: New anatomical and computational perspectives</article-title>. <source>Neural networks</source>, <volume>15</volume>(<issue>4-6</issue>):<fpage>535</fpage>–<lpage>547</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kawai</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Markman</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Poddar</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Ko</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fantana</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Dhawale</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Kampff</surname>, <given-names>A. R.</given-names></string-name>, and <string-name><surname>Ölveczky</surname>, <given-names>B. P.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Motor cortex is required for learning but not for executing a motor skill</article-title>. <source>Neuron</source>, <volume>86</volume>(<issue>3</issue>):<fpage>800</fpage>–<lpage>812</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klaus</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Martins</surname>, <given-names>G. J.</given-names></string-name>, <string-name><surname>Paixao</surname>, <given-names>V. B.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Paninski</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Costa</surname>, <given-names>R. M.</given-names></string-name></person-group> (<year>2017</year>). <article-title>The spatiotemporal organization of the striatum encodes action space</article-title>. <source>Neuron</source>, <volume>95</volume>(<issue>5</issue>):<fpage>1171</fpage>–<lpage>1180</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koch</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Ponzo</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Di Lorenzo</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Caltagirone</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Veniero</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Hebbian and anti-hebbian spike-timing-dependent plasticity of human cortico-cortical connections</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume>(<issue>23</issue>):<fpage>9725</fpage>–<lpage>9733</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kravitz</surname>, <given-names>A. V.</given-names></string-name>, <string-name><surname>Freeze</surname>, <given-names>B. S.</given-names></string-name>, <string-name><surname>Parker</surname>, <given-names>P. R.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Thwin</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Deisseroth</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Kreitzer</surname>, <given-names>A. C.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Regulation of parkinsonian motor behaviours by optogenetic control of basal ganglia circuitry</article-title>. <source>Nature</source>, <volume>466</volume>(<issue>7306</issue>):<fpage>622</fpage>–<lpage>626</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Sabatini</surname>, <given-names>B. L.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Striatal indirect pathway mediates exploration via collicular competition</article-title>. <source>Nature</source>, <volume>599</volume>(<issue>7886</issue>):<fpage>645</fpage>–<lpage>649</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Lodder</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Patriarchi</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Tian</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Sabatini</surname>, <given-names>B. L.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Cell-type-specific asynchronous modulation of pka by dopamine in learning</article-title>. <source>Nature</source>, <volume>590</volume>(<issue>7846</issue>):<fpage>451</fpage>–<lpage>456</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lindsey</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Litwin-Kumar</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Action-modulated midbrain dopamine activity arises from distributed control policies</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>35</volume>:<fpage>5535</fpage>– <lpage>5548</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lisman</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Two-phase model of the basal ganglia: implications for discontinuous control of the motor system</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>369</volume>(<issue>1655</issue>):<fpage>20130489</fpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Markowitz</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Gillis</surname>, <given-names>W. F.</given-names></string-name>, <string-name><surname>Beron</surname>, <given-names>C. C.</given-names></string-name>, <string-name><surname>Neufeld</surname>, <given-names>S. Q.</given-names></string-name>, <string-name><surname>Robertson</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Bhagat</surname>, <given-names>N. D.</given-names></string-name>, <string-name><surname>Peterson</surname>, <given-names>R. E.</given-names></string-name>, <string-name><surname>Peterson</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Hyun</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Linderman</surname>, <given-names>S. W.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2018</year>). <article-title>The striatum organizes 3d behavior via moment-to-moment action selection</article-title>. <source>Cell</source>, <volume>174</volume>(<issue>1</issue>):<fpage>44</fpage>–<lpage>58</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Markowitz</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Gillis</surname>, <given-names>W. F.</given-names></string-name>, <string-name><surname>Jay</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wood</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Harris</surname>, <given-names>R. W.</given-names></string-name>, <string-name><surname>Cieszkowski</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Scott</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Brann</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Koveal</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kula</surname>, <given-names>T.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2023</year>). <article-title>Spontaneous behaviour is structured by reinforcement without explicit reward</article-title>. <source>Nature</source>, <volume>614</volume>(<issue>7946</issue>):<fpage>108</fpage>–<lpage>117</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mikhael</surname>, <given-names>J. G.</given-names></string-name> and <string-name><surname>Bogacz</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Learning reward uncertainty in the basal ganglia</article-title>. <source>PLoS computational biology</source>, <volume>12</volume>(<issue>9</issue>):<fpage>e1005062</fpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mink</surname>, <given-names>J. W.</given-names></string-name></person-group> (<year>1996</year>). <article-title>The basal ganglia: focused selection and inhibition of competing motor programs</article-title>. <source>Progress in neurobiology</source>, <volume>50</volume>(<issue>4</issue>):<fpage>381</fpage>–<lpage>425</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mizes</surname>, <given-names>K. G.</given-names></string-name>, <string-name><surname>Lindsey</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Escola</surname>, <given-names>G. S.</given-names></string-name>, and <string-name><surname>Ölveczky</surname>, <given-names>B. P.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Dissociating the contributions of sensorimotor striatum to automatic and visually guided motor sequences</article-title>. <source>Nature Neuroscience</source>, pages <fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Montague</surname>, <given-names>P. R.</given-names></string-name>, <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Sejnowski</surname>, <given-names>T. J.</given-names></string-name></person-group> (<year>1996</year>). <article-title>A framework for mesencephalic dopamine systems based on predictive hebbian learning</article-title>. <source>Journal of neuroscience</source>, <volume>16</volume>(<issue>5</issue>):<fpage>1936</fpage>–<lpage>1947</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niv</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Reinforcement learning in the brain</article-title>. <source>Journal of Mathematical Psychology</source>, <volume>53</volume>(<issue>3</issue>):<fpage>139</fpage>–<lpage>154</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Doherty</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Schultz</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Deichmann</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Dolan</surname>, <given-names>R. J.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Dissociable roles of ventral and dorsal striatum in instrumental conditioning</article-title>. <source>science</source>, <volume>304</volume>(<issue>5669</issue>):<fpage>452</fpage>–<lpage>454</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Packard</surname>, <given-names>M. G.</given-names></string-name> and <string-name><surname>Knowlton</surname>, <given-names>B. J.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Learning and memory functions of the basal ganglia</article-title>. <source>Annual review of neuroscience</source>, <volume>25</volume>(<issue>1</issue>):<fpage>563</fpage>–<lpage>593</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pawlak</surname>, <given-names>V.</given-names></string-name> and <string-name><surname>Kerr</surname>, <given-names>J. N.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Dopamine receptor activation is required for corticostriatal spike-timing-dependent plasticity</article-title>. <source>Journal of Neuroscience</source>, <volume>28</volume>(<issue>10</issue>):<fpage>2435</fpage>–<lpage>2446</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peak</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chieng</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Hart</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Balleine</surname>, <given-names>B. W.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Striatal direct and indirect pathway neurons differentially control the encoding and updating of goal-directed learning</article-title>. <source>Elife</source>, <volume>9</volume>:<elocation-id>e58544</elocation-id>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peters</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Fabre</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name>, and <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Striatal activity topographically reflects cortical activity</article-title>. <source>Nature</source>, <volume>591</volume>(<issue>7850</issue>):<fpage>420</fpage>–<lpage>425</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Redgrave</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Prescott</surname>, <given-names>T. J.</given-names></string-name>, and <string-name><surname>Gurney</surname>, <given-names>K.</given-names></string-name></person-group> (<year>1999</year>). <article-title>The basal ganglia: a vertebrate solution to the selection problem?</article-title> <source>Neuroscience</source>, <volume>89</volume>(<issue>4</issue>):<fpage>1009</fpage>–<lpage>1023</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rubin</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Vich</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Clapp</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Noneman</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Verstynen</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2021</year>). <article-title>The credit assignment problem in cortico-basal ganglia-thalamic networks: A review, a problem and a possible solution</article-title>. <source>European Journal of Neuroscience</source>, <volume>53</volume>(<issue>7</issue>):<fpage>2234</fpage>–<lpage>2253</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schultz</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Montague</surname>, <given-names>P. R.</given-names></string-name></person-group> (<year>1997</year>). <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source>, <volume>275</volume>(<issue>5306</issue>):<fpage>1593</fpage>–<lpage>1599</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seo</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Averbeck</surname>, <given-names>B. B.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Action selection and action value in frontal-striatal circuits</article-title>. <source>Neuron</source>, <volume>74</volume>(<issue>5</issue>):<fpage>947</fpage>–<lpage>960</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Flajolet</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Greengard</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Surmeier</surname>, <given-names>D. J.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Dichotomous dopaminergic control of striatal synaptic plasticity</article-title>. <source>Science</source>, <volume>321</volume>(<issue>5890</issue>):<fpage>848</fpage>–<lpage>851</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shin</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Song</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Paik</surname>, <given-names>S.-B.</given-names></string-name>, and <string-name><surname>Jung</surname>, <given-names>M. W.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Spatial organization of functional clusters representing reward and movement information in the striatal direct and indirect pathways</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>117</volume>(<issue>43</issue>):<fpage>27004</fpage>–<lpage>27015</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Silveri</surname>, <given-names>M. C.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Contribution of the cerebellum and the basal ganglia to language production: Speech, word fluency, and sentence construction—evidence from pathology</article-title>. <source>The Cerebellum</source>, <volume>20</volume>(<issue>2</issue>):<fpage>282</fpage>–<lpage>294</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Raju</surname>, <given-names>D. V.</given-names></string-name>, <string-name><surname>Pare</surname>, <given-names>J.-F.</given-names></string-name>, and <string-name><surname>Sidibe</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2004</year>). <article-title>The thalamostriatal system: a highly specific network of the basal ganglia circuitry</article-title>. <source>Trends in neurosciences</source>, <volume>27</volume>(<issue>9</issue>):<fpage>520</fpage>–<lpage>527</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sutton</surname>, <given-names>R. S.</given-names></string-name> and <string-name><surname>Barto</surname>, <given-names>A. G.</given-names></string-name></person-group> (<year>2018</year>). <source>Reinforcement learning: An introduction</source>. <publisher-name>MIT press</publisher-name>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Treves</surname>, <given-names>A.</given-names></string-name> and <string-name><surname>Rolls</surname>, <given-names>E. T.</given-names></string-name></person-group> (<year>1991</year>). <article-title>What determines the capacity of autoassociative memories in the brain?</article-title> <source>Network: Computation in Neural Systems</source>, <volume>2</volume>(<issue>4</issue>):<fpage>371</fpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Varin</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Cornil</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Houtteman</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bonnavion</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>de Kerchove d’Exaerde</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>The respective activation and silencing of striatal direct and indirect pathway neurons support behavior encoding</article-title>. <source>Nature communications</source>, <volume>14</volume>(<issue>1</issue>):<fpage>4982</fpage>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wickens</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Begg</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Arbuthnott</surname>, <given-names>G.</given-names></string-name></person-group> (<year>1996</year>). <article-title>Dopamine reverses the depression of rat corticostriatal synapses which normally follows high-frequency stimulation of cortex in vitro</article-title>. <source>Neuroscience</source>, <volume>70</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>5</lpage>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wildgruber</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ackermann</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Grodd</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Differential contributions of motor cortex, basal ganglia, and cerebellum to speech motor control: effects of syllable repetition rate evaluated by fmri</article-title>. <source>Neuroimage</source>, <volume>13</volume>(<issue>1</issue>):<fpage>101</fpage>–<lpage>109</lpage>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Willmore</surname>, <given-names>B.</given-names></string-name> and <string-name><surname>Tolhurst</surname>, <given-names>D. J.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Characterizing the sparseness of neural codes</article-title>. <source>Network: Computation in Neural Systems</source>, <volume>12</volume>(<issue>3</issue>):<fpage>255</fpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wiltschko</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Iurilli</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Peterson</surname>, <given-names>R. E.</given-names></string-name>, <string-name><surname>Katon</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Pashkovski</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Abraira</surname>, <given-names>V. E.</given-names></string-name>, <string-name><surname>Adams</surname>, <given-names>R. P.</given-names></string-name>, and <string-name><surname>Datta</surname>, <given-names>S. R.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Mapping sub-second structure in mouse behavior</article-title>. <source>Neuron</source>, <volume>88</volume>(<issue>6</issue>):<fpage>1121</fpage>–<lpage>1135</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101747.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Langdon</surname>
<given-names>Angela</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>National Institute of Mental Health</institution>
</institution-wrap>
<city>Bethesda</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Fundamental</kwd>
</kwd-group>
</front-stub>
<body>
<p>The authors present a biologically plausible framework for action selection and learning in the striatum that is a <bold>fundamental</bold> advance in our understanding of possible neural implementations of reinforcement learning in the basal ganglia. They provide <bold>compelling</bold> evidence that their model can reconcile realistic neural plasticity rules with the distinct functional roles of the direct and indirect spiny projection neurons of the striatum, recapitulating experimental findings regarding the activity profiles of these distinct neural populations and explaining a key aspect of striatal function.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101747.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors propose a new model of biologically realistic reinforcement learning in the direct and indirect pathway spiny projection neurons in the striatum. These pathways are widely considered to provide a neural substrate for reinforcement learning in the brain. However, we do not yet have a full understanding of mechanistic learning rules that would allow successful reinforcement learning like computations in these circuits. The authors outline some key limitations of current models and propose an interesting solution by leveraging learning with efferent inputs of selected actions. They show that the model simulations are able to recapitulate experimental findings about the activity profile in these populations of mice during spontaneous behavior. They also show how their model is able to implement off-policy reinforcement learning.</p>
<p>Strengths:</p>
<p>The manuscript has been very clearly written and the results have been presented in a readily digestible manner. The limitations of existing models, that motivate the presented work, have been clearly presented and the proposed solution seems very interesting. The novel contribution of the proposed model is the idea that different patterns of activity drive current action selection and learning. Not only does this allow the model is able to implement reinforcement learning computations well, but this suggestion may have interesting implications regarding why some processes selectively affect ongoing behavior and others affect learning. The model is able to recapitulate some interesting experimental findings about various activity characteristics of dSPN and iSPN pathway neuronal populations in spontaneously behaving mice. The authors also show that their proposed model can implement off-policy reinforcement learning algorithms with biologically realistic learning rules. This is interesting since off-policy learning provides some unique computational benefits and it is very likely that learning in neural circuits may, at least to some extent, implement such computations.</p>
<p>Weaknesses:</p>
<p>A weakness in this work is that it isn't clear how a key component in the model - an efferent copy of selected actions - would be accessible to these striatal populations. The authors propose several plausible candidates, but future work may clarify the feasibility of this proposal.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101747.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The basal ganglia is often understood within a reinforcement learning (RL) framework, where dopamine neurons convey a reward prediction error that modulates cortico-striatal connections onto spiny projection neurons (SPNS) in the striatum. However, current models of plasticity rules are inconsistent with learning in a reinforcement learning framework.</p>
<p>This paper proposes a new model that describes how distinct learning rules in direct and indirect pathway striatal neurons allow them to implement reinforcement learning models. It proposes that two distinct components of striatal activity affect action selection and learning. They show that the proposed implementation allows learning in simple tasks and is consistent with experimental data from calcium imaging data in direct and indirect SPNs in freely moving mice.</p>
<p>Strengths:</p>
<p>Despite the success of reward prediction errors at characterizing the responses of dopamine neurons as the temporal difference error within an RL framework, the implementation of RL algorithms in the rest of the basal ganglia has been unclear. A key missing aspect has been the lack of a RL implementation that is consistent with the distinction of direct- and indirect SPNs. This paper proposes a new model that is able to learn successfully in simple RL tasks and explains recent experimental results.</p>
<p>The author shows that their proposed model, unlike previous implementations, this model can perform well in RL tasks. The new model allows them to make experimental predictions. They test some of these predictions and show that the dynamics of dSPNs and iSPNs correspond to model predictions.</p>
<p>More generally, this new model can be used to understand striatal dynamics across direct and indirect SPNs in future experiments.</p>
<p>Weaknesses:</p>
<p>The authors could characterize better the reliability of their experimental predictions and the description of the parameters of some of the simulations</p>
<p>The authors propose some ideas about how the specificity of the striatal efferent inputs but should highlight better that this is a key feature of the model whose anatomical implementation has yet to be resolved.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101747.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper points out an inconsistency of the roles of the striatal spiny neurons projecting to the indirect pathway (iSPN) and the synaptic plasticity rule of those neurons expressing dopamine D2 receptors and proposes a novel, intriguing mechanisms that iSPNs are activated by the efference copy of the chosen action that they are supposed to inhibit.</p>
<p>The proposed model was supported by simulations and analysis of the neural recording data during spontaneous behaviors.</p>
<p>Strengths:</p>
<p>Previous models suggested that the striatal neurons learn action-value functions, but how the information about the chosen action is fed back to the striatum for learning was not clear. The author pointed out that this is a fundamental problem for iSPNs that are supposed to inhibit specific actions and its synaptic inputs are potentiated with dopamine dips.</p>
<p>The authors propose a novel hypothesis that iSPNs are activated by efference copy of the selected action which they are supposed to inhibit during action selection. Even though intriguing and seemingly unnatural, the authors demonstrated that the model based on the hypothesis can circumvent the problem of iSPNs learning to disinhibit the actions associated with negative reward errors. They further showed by analyzing the cell-type specific neural recording data by Markowitz et al. (2018) that iSPN activities tend to be anti-correlated before and after action selection.</p>
<p>Weaknesses:</p>
<p>(1) It is not correct to call the action value learning using the externally-selected action as &quot;off-policy.&quot; Both off-policy algorithm Q-learning and on-policy algorithm SARSA update the action value of the chosen action, which can be different from the greedy action implicated by the present action values. In standard reinforcement learning terminology, on-policy or off-policy is regarding the actions in the subsequent state, whether to use the next action value of (to be) chosen action or that of greedy choice as in equation (7).</p>
<p>It is worth noting that this paper suggested that dopamine neurons encode on-policy TD errors:</p>
<p>
Morris G, Nevet A, Arkadir D, Vaadia E, Bergman H (2006). Midbrain dopamine neurons encode decisions for future action. Nat Neurosci, 9, 1057-63. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1743">https://doi.org/10.1038/nn1743</ext-link></p>
<p>(2) It is also confusing to contract TD learning and Q-learning, as the latter is considered as one type of TD learning. In the TD error signal by state value function (6) is dependent on the chosen action a_{t-1} implicitly in r_t and s_t based on the reward and state transition function.</p>
<p>(3) It is not clear why interferences of the activities for action selection and learning can be avoided, especially when actions are taken with short intervals or even temporal overlaps. How can the efference copy activation for the previous action be dissociated with the sensory cued activation for the next action selection?</p>
<p>(4) Although it may be difficult to single out the neural pathway that carries the efference copy signal to the striatum, it is desired to consider their requirements and difference possibilities. A major issue is that the time delay from actions to reward feedback can be highly variable.</p>
<p>An interesting candidate is the long-latency neurons in the CM thalamus projecting to striatal cholinergic interneurons, which are activated following low-reward actions:</p>
<p>
Minamimoto T, Hori Y, Kimura M (2005). Complementary process to response bias in the centromedian nucleus of the thalamus. Science, 308, 1798-801. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1109154">https://doi.org/10.1126/science.1109154</ext-link></p>
<p>(5) In the paragraph before Eq. (3), Eq. (1) should be Eq. (2) for the iSPN.</p>
</body>
</sub-article>
</article>