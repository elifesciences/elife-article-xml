<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">101036</article-id>
<article-id pub-id-type="doi">10.7554/eLife.101036</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.101036.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Age-dependent predictors of effective reinforcement motor learning across childhood</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9710-0291</contrib-id>
<name>
<surname>Hill</surname>
<given-names>Nayo M</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>hilln@kennedykrieger.org</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9697-1546</contrib-id>
<name>
<surname>Tripp</surname>
<given-names>Haley M</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2011-2790</contrib-id>
<name>
<surname>Wolpert</surname>
<given-names>Daniel M</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9836-822X</contrib-id>
<name>
<surname>Malone</surname>
<given-names>Laura A</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6079-0997</contrib-id>
<name>
<surname>Bastian</surname>
<given-names>Amy J</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
<email>bastian@kennedykrieger.org</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05q6tgt32</institution-id><institution>Kennedy Krieger Institute</institution></institution-wrap>, <city>Baltimore</city>, <country country="US">United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/037zgn354</institution-id><institution>Department of Neuroscience, Johns Hopkins School of Medicine</institution></institution-wrap>, <city>Baltimore</city>, <country country="US">United States</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University</institution></institution-wrap>, <city>New York</city>, <country country="US">United States</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Department of Neuroscience, Columbia University</institution></institution-wrap>, <city>New York</city>, <country country="US">United States</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Kavli Institute for Brain Science, Columbia University</institution></institution-wrap>, <city>New York</city>, <country country="US">United States</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/037zgn354</institution-id><institution>Department of Neurology, Johns Hopkins School of Medicine</institution></institution-wrap>, <city>Baltimore</city>, <country country="US">United States</country></aff>
<aff id="a7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/037zgn354</institution-id><institution>Department of Physical Medicine and Rehabilitation, Johns Hopkins School of Medicine</institution></institution-wrap>, <city>Baltimore</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Dekker</surname>
<given-names>Tessa</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Makin</surname>
<given-names>Tamar R</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Cambridge</institution>
</institution-wrap>
<city>Cambridge</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>†</label><p>These authors contributed equally to this work</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: DMW is a consultant to CTRL-Labs Inc., in the Reality Labs Division of Meta however this entity did not support or influence this work.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-09-30">
<day>30</day>
<month>09</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-06-10">
<day>10</day>
<month>06</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP101036</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-07-09">
<day>09</day>
<month>07</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-07-09">
<day>09</day>
<month>07</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.07.09.602665"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-09-30">
<day>30</day>
<month>09</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.101036.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.101036.1.sa3">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.101036.1.sa2">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.101036.1.sa1">Reviewer #2 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.101036.1.sa0">Reviewer #3 (Public review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Hill et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Hill et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-101036-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Across development, children must learn motor skills such as eating with a spoon and drawing with a crayon. Reinforcement learning, driven by success and failure, is fundamental to such sensori-motor learning. It typically requires a child to explore movement options along a continuum (grip location on a crayon) and learn from probabilistic rewards (whether the crayon draws or breaks). Here, we studied the development of reinforcement motor learning using online motor tasks to engage children aged 3 to 17 years and adults (cross-sectional sample, N=385). Participants moved a cartoon penguin across a scene and were rewarded (animated cartoon clip) based on their final movement position. Learning followed a clear developmental trajectory when participants could choose to move anywhere along a continuum and the reward probability depended on the final movement position. Learning was incomplete or absent in 3 to 8-year-olds and gradually improved to adult-like levels by adolescence. A reinforcement learning model fit to each participant identified two age-dependent factors underlying improvement across development: an increasing amount of exploration after a failed movement and a decreasing level of motor noise. We predicted, and confirmed, that switching to discrete targets and deterministic reward would improve 3 to 8-year-olds’ learning to adult-like levels by increasing exploration after failed movements. Overall, we show a robust developmental trajectory of reinforcement motor learning abilities under ecologically relevant conditions i.e., continuous movement options mapped to probabilistic reward. This learning may be limited by immature spatial processing and probabilistic reasoning abilities in young children and can be rescued by reducing task demands.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Summary of major changes:
1) We expanded the discussion of the relevant literature in children and adults.
2) We improved the contextualization of our experimental design within previous reinforcement studies in both cognitive and motor domains highlighting the interplay between the two.
3) We reorganized the primary and supplementary results to better communicate the findings of the studies.
4) The modeling has been significantly revised and extended. We now formally compare 31 noise-based models and one value-based model and this led to a different model from the original being the preferred model. This has to a large extent cleaned up the modeling results. The preferred model is a special case (with no exploration after success) of the model proposed in Therrien et al. (2018). We also provide examples of individual fits of the model, fit all four tasks and show group fits for all, examine fits vs. data for the clamp phases by age, provide measures of relative and absolute goodness of fit, and examine how the optimal level of exploration varies with motor noise.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>In the game of Poohsticks, invented by A. A. Milne and described in <italic>The House at Pooh Corner</italic>, two children each drop a stick into a stream from the upstream side of a bridge (<xref ref-type="bibr" rid="c27">Milne and Shepard, 1928</xref>). They then race to the downstream side to see whose stick appears first, with the winner scoring a point. The game is repeated with each child trying to find the sweet spot to drop their stick in to win. Given the capricious nature of streams with their turbulent flow, dropping both sticks in exactly the same spot on two successive games can lead to different outcomes. To be an expert, a child must use probabilistic success and failure feedback to select a location along a continuous and infinite set of possible options (anywhere along the span of the bridge) to drop their stick to maximize reward. Despite the complexity of this probabilistic reinforcement task, the current world champion is 9 years old. Here, we examine how children develop the ability to learn such tasks from reinforcement feedback alone.</p>
<p>Reinforcement motor learning is essential for many sensorimotor tasks and is typically driven by (scalar) reward feedback in contrast to vector errors used in adaptation learning. Reward feed-back can be simple binary feedback, such as success or failure in hitting the space bar on a keyboard, or continuous, such as the height achieved on a swing. Critically, the learner is not told what to do but must discover which movements produce reward by trying different options (<xref ref-type="bibr" rid="c44">Sutton and Barto, 2018</xref>). Therefore, a key component of reinforcement learning is exploring and evaluating feedback to maximize reward.</p>
<p>The basic capacity for reinforcement motor learning emerges early in life. For example, a nine-week-old infant will increase kicking frequency when a ribbon connects their foot to an overhead mobile that moves with their kicks (<xref ref-type="bibr" rid="c34">Rovee and Rovee, 1969</xref>). Three-month-olds can even learn to produce a specific kick height to move a mobile (<xref ref-type="bibr" rid="c35">Sargent et al., 2014</xref>). Both tasks have a deterministic relationship between the action and the outcome; a correct kick is always rewarded. In a more complex probabilistic reaching task, children aged 3-to 9-years old showed different sensitives to reward probability (<xref ref-type="bibr" rid="c43">Stevenson and Weir, 1959</xref>). The youngest aged children were more likely to stick on a rewarded target even if the reward rate was low (e.g., 33%), whereas older children explored other targets. This suggests that younger children are less likely to explore new options and are more willing to accept lower reward rates.</p>
<p>Reinforcement learning has also been studied in cognitive tasks that require children to select from discrete options to receive reward. In a relatively simple task, two-year old children could accurately select a reinforced visual image from two choices (<xref ref-type="bibr" rid="c42">Minto de Sousa et al., 2015</xref>). More complex tasks have been studied in older children and adolescents identifying age-related changes in learning ability (<xref ref-type="bibr" rid="c52">Xia et al., 2021</xref>; <xref ref-type="bibr" rid="c5">Cohen et al., 2020</xref>; <xref ref-type="bibr" rid="c6">Decker et al., 2016</xref>; <xref ref-type="bibr" rid="c23">Mayor-Dubois et al., 2016</xref>; <xref ref-type="bibr" rid="c21">Master et al., 2020</xref>). In one task, participants selected from options in a probabilistic reward environment with hidden factors that could change the outcome. Although children (7-12 years old) and adolescents (13-17 years old) could identify the factors in the environment that changed the outcome, children were unable to use this information to optimize their choices. Similarly, <xref ref-type="bibr" rid="c6">Decker et al. (2016)</xref> used a sequential learning task (8–25 year olds) to show that probabilistic reasoning improves with age. As a whole, this previous work identifies differences between younger and older children on choice-based selection tasks that require integration of reward feedback to learn successfully.</p>
<p>Sensorimotor tasks involve different combinations of motor and cognitive mechanisms (<xref ref-type="bibr" rid="c4">Chen et al., 2017</xref>; <xref ref-type="bibr" rid="c24">McDougle et al., 2016</xref>), and there are both implicit and explicit (cognitive) contributions to reinforcement motor learning (<xref ref-type="bibr" rid="c13">Holland et al., 2018</xref>; <xref ref-type="bibr" rid="c22">van Mastrigt et al., 2023</xref>). Cognitive tasks tend to be discrete with arbitrary assignment of reward probability to the explicit choice options (cf. <xref ref-type="bibr" rid="c10">Giron et al., 2023</xref>). In contrast, motor learning tasks typically have features that are not present in cognitive tasks. Movement choice options can be continuous (akin to the choice of the direction to kick a soccer ball), corrupted by motor noise, and have a spatial gradient of reward (akin to the probability of scoring as a function of kick direction).</p>
<p>Here, we examine motor learning under reinforcement in which we control two key experimental factors. First, rewards can either be deterministic, the same action leads to the same outcome (e.g., pressing the space bar on a keyboard), or probabilistic, the outcome is stochastic (e.g., the path of a soccer ball depends on the wind and surface on which it is kicked). Second, action options can be discrete (e.g., the space or shift key on a keyboard) or continuous (e.g., the direction of a soccer kick). We report on a series of experiments in which we control both factors —reinforcement feedback (deterministic vs. probabilistic) and the action options (discrete vs. continuous targets) to examine the development of reinforcement learning across childhood. These factors reflect the natural variations in tasks that children have to learn during everyday life and are important factors for rehabilitation interventions. The goal of these tasks is to understand how children at different ages adjust their movements in response to reward feedback. Our study builds on previous work in healthy adults examining center out reaching movements under binary reward feedback (<xref ref-type="bibr" rid="c3">Cashaback et al., 2019</xref>; <xref ref-type="bibr" rid="c46">Therrien et al., 2016</xref>; <xref ref-type="bibr" rid="c13">Holland et al., 2018</xref>; <xref ref-type="bibr" rid="c15">van der Kooij and Overvliet, 2016</xref>).</p>
<p>We developed a remote video game paradigm for a cross-sectional study of 298 children aged 3 to 17 years and 87 adults from across the USA (locations and demographics shown in <xref rid="figS1" ref-type="fig">Figure 1— figure Supplement 1</xref>, <xref rid="tbl1" ref-type="table">Table 1</xref>, and <xref rid="tbl2" ref-type="table">Table 2</xref>). We hypothesized that children’s reinforcement learning abilities would improve with age, and depend on the developmental trajectory of exploration variability, learning rate (how much people adjust their reach after success), and motor noise (here defined as all sources of noise associated with movement, including sensory noise, memory noise, and motor noise). We think that these factors depend on the developmental progression of neural circuits that contribute to reinforcement learning abilities (<xref ref-type="bibr" rid="c32">Raznahan et al., 2014</xref>; <xref ref-type="bibr" rid="c28">Nelson et al., 2000</xref>; <xref ref-type="bibr" rid="c36">Schultz, 1998</xref>). We found that younger children (3 to 8 years) failed to learn with a continuous target and probabilistic reward feedback. Reinforcement learning improved with age enabling older children to find the optimal reward region. Using a mechanistic model, we show that this improvement is due to a developmental gradient of increasing exploration after failure and reducing motor noise with age. Importantly, we then show that use of deterministic feedback and discrete targets can dramatically improve learning abilities of younger children.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Participant Demographics</title></caption>
<graphic xlink:href="602665v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>Participant Ethnicity and Race</title></caption>
<graphic xlink:href="602665v2_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Game Environment.</title>
<p><bold>a.</bold> Screenshot of game environment and sample movement path (large text, arrows, and movement path were not displayed to participants). During the learning block, participants either experienced a continuous target (continuous groups) or seven discrete targets (discrete groups). <bold>b</bold>. Reward landscape for the learning blocks for the different task paradigms. The x-axis represents normalized units based on the participant’s computer setup. Continuous probabilistic: continuous target with position-based reward probability gradient; discrete probabilistic: discrete targets with target specific reward probabilities; continuous deterministic: continuous target with a single 100% rewarded zone; discrete deterministic: discrete targets with a single target giving 100% reward. <bold>c</bold>. Outcome feedback for continuous probabilistic task. Success (top), movie clip (different for each trial) and pleasant sound plays, and video screen is outlined in blue. Failure (bottom), movie clip does not play, the penguin falls over and red text “You slipped!” appears with a sad face image.</p>
<p><xref rid="figS1" ref-type="fig">Figure 1—figure supplement 1</xref>. Map of participant locations.</p></caption>
<graphic xlink:href="602665v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2">
<title>Results</title>
<p>Different groups of participants performed one of four tasks: a continuous probabilistic, a discrete probabilistic, a continuous deterministic, and a discrete deterministic task. We first focus on the continuous probabilistic task and its associated reinforcement learning model, as this is the most complex environment, before reporting the results of the other three tasks.</p>
<sec id="s2a">
<title>Continuous probabilistic task</title>
<p>In the continuous probabilistic task, we studied 111 children and adolescents, aged 3 to 17 years, and 33 adults as they attempted to learn to produce movements that maximized reward. For the key learning phase of the experiment, we created a probabilistic reward landscape in which the probability of reward after each movement depended on its endpoint. To implement this in a task that was engaging to children, we created a computer game that was played at home. Participants were required to move a cartoon penguin from a starting position to join a group of penguins arranged horizontally across the screen (<xref rid="fig1" ref-type="fig">Fig. 1a</xref> - continuous). Participants were told that there was a slippery ice patch just before the group (dark blue area, <xref rid="fig1" ref-type="fig">Fig. 1a</xref> - continuous) and that they should try to make the penguin cross at the location where they would not slip. In the instructions, participants were told that there was a location in the ice where they would never slip. The reward landscape (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> top left) determined the probability of success on each trial and was not known to the participant. There was a small 100% reward zone where the penguin would never slip with the reward probability decreasing away from this zone. Successful trials led to a short Disney cartoon clip playing whereas a failure trial led to the penguin falling over and the appearance of a static image of a sad penguin (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>).</p>
<p>Participants performed 5 blocks of trials (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). <xref rid="fig2" ref-type="fig">Fig. 2b</xref> shows examples from different aged participants or endpoint position over the experiment. In block 1, participants made 20 movements to a series of discrete targets, which were the same width as the 100% reward zone. The targets appeared at different locations across the scene and participants received reinforcement feedback at the end of each trial (sample paths are shown in <xref rid="figS2" ref-type="fig">Figure 2—figure Supplement 1</xref> and <xref rid="figS2a" ref-type="fig">Figure 2—figure Supplement 2</xref>). The targets were presented in a randomized order, but each participant received the same 20 target locations. This baseline block allowed participants to experience reaching to many locations in the workspace and allowed us to assess accuracy and precision in hitting discrete targets. All participants but one performed well in the baseline block of the task, with endpoint position on average within the target zone (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>). This shows that participants could accurately hit discrete targets.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Paradigm, example behavior, and target accuracy in the continuous probabilistic task.</title>
<p><bold>a.</bold> Experimental design with baseline: single discrete target presented in randomized locations across the screen; learning: learning block with reward determined by endpoint position; success clamp: feedback clamped to 100% success independent of endpoint position; fail clamp: feedback clamped to 100% failure independent of endpoint position; single target: single discrete target presented in the middle of the screen. <bold>b</bold>. Representative endpoint time series from various aged participants. Gray shaded zones indicate positions in the workspace where a reward is given 100% of the time (thin gray lines in first and last blocks are for the discrete targets). Green filled circles indicate rewarded trials while open circles indicate unrewarded trials. The horizontal colored bar on the x-axis indicates the trials corresponding with the experimental blocks outlined in <bold>a</bold>. In the learning block (trials 21–120), rewards were given based on the continuous probabilistic landscape. <bold>c</bold>. Mean baseline accuracy (average reach deviation from the discrete targets) by age. Adult data are averaged and plotted to the right with standard error of the mean. The gray region shows the width of a discrete target. <bold>d</bold>. Same as <bold>c</bold> for the single target in block 5. In <bold>c</bold> and <bold>d</bold>, participants who completed the task in person (in lab) are indicated in white circles.</p>
<p><xref rid="figS2" ref-type="fig">Figure 2—figure supplement 1</xref>. Example baseline paths for participants ages 3 to 11 years old.</p>
<p><xref rid="figS2a" ref-type="fig">Figure 2—figure supplement 2</xref>. Example baseline paths for participants ages 12 to 17 years old and adults.</p>
<p><xref rid="figS2b" ref-type="fig">Figure 2—figure supplement 3</xref>. Path length ratios.</p>
<p><xref rid="figS2c" ref-type="fig">Figure 2—figure supplement 4</xref>. Timing information.</p></caption>
<graphic xlink:href="602665v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Participants then began the learning block, where they could move to any endpoint location on the continuous target. Note that the reach location for the first learning trial was not affected by (correlated with) the target position on the last baseline trial (p &gt; 0.3 for both children and adults, separately). <xref rid="fig2" ref-type="fig">Fig. 2b</xref> shows the 5- and 7-year-old children moved to many endpoint locations (i.e., showed exploration) receiving reward based on the probability landscape. Interestingly, their exploration appeared to show little sensitivity to previous failure (open circles) versus success (filled circles) and endpoints did not converge on the 100% reward zone (gray area) by the end of the block. The older children, aged 11, 12, and 15, performed similarly to the adult, exploring early in the block, and then focusing movements on the unseen 100% reward zone. This indicates that they were able to explore successfully after failures and exploit the 100% reward zone. These patterns were also observed in group data (<xref rid="fig3" ref-type="fig">Fig. 3</xref>, binned across 6 age groups) where, on average, the 3 to 5 and 6 to 8-year-olds did not find the 100% reward zone, but the older age groups did.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Continuous probabilistic task learning block time series.</title>
<p>Data and model (red, smooth curves) for each trial of the learning block grouped into age ranges. Data shows mean (solid line) and standard error of mean (shading) of participants’ endpoint. Model in red shows mean (dashed lines) and standard error (shading) from the model simulations. The gray region shows 100% reward zone.</p></caption>
<graphic xlink:href="602665v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>After learning, participants transitioned seamlessly (no break or change in instructions) into two short blocks with clamped feedback to examine behavior in response to repeated success or repeated failure. The success clamp always rewarded movements and the fail clamp never rewarded movements. The 5- and 7-year-olds explored similarly in both clamped blocks whereas the older children showed greater exploration in the fail clamp compared to the success clamp (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>).</p>
<p>In the final block, participants moved to a single discrete target in the center of the workspace to examine whether the precision changed over the course of the experiment (examples <xref rid="fig2" ref-type="fig">Fig. 2b</xref>; group data <xref rid="fig2" ref-type="fig">Fig. 2d</xref>). For participants aged 3 to 17 years, an ANOVA of accuracy (distance from discrete target center) by block (first vs. last) and age (5 bins) shows no effect of block (<italic>F</italic><sub>1,106</sub> = 0.282, p = 0.597) and no interaction between age and block (<italic>F</italic><sub>4,106</sub> = 1.219, p = 0.307), indicating that accuracy of movement to discrete targets was maintained throughout the experiment even for the younger children.</p>
<p>Note that participants were instructed to reach to the continuous target; there were no instructions regarding the path nor the precise timing of the movements. In younger children, movements tended to be more curved compared to older children, as indicated by larger path length ratios (<xref rid="figS2b" ref-type="fig">Figure 2—figure Supplement 3</xref>). Movement times were similar for all children, (<xref rid="figS2c" ref-type="fig">Figure 2—figure Supplement 4</xref>) but younger children took longer to initiate a trial by clicking on the ball (longer Reaction Time) and start moving after initiating (longer Stationary Time). Since these elements of movement were not instructed and did not prevent participants from hitting discrete targets, we focus on the endpoint of the movement in our analyses.</p>
<sec id="s2a1">
<title>Measures associated with learning in the continuous probabilistic task</title>
<p>We assessed how key features of behavior changed across childhood (<xref rid="fig4" ref-type="fig">Fig. 4</xref>, with the average of adult performance for comparison). In the Baseline block, the endpoint variability relative to target center <italic>(Baseline precision)</italic> decreased significantly with age (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>; regression <italic>R</italic><sup>2</sup> = 0.153, <italic>F</italic><sub>1,109</sub> = 19.7, p &lt; 0.001). This is consistent with known developmental changes in motor variability (<xref ref-type="bibr" rid="c45">Takahashi et al., 2003</xref>; <xref ref-type="bibr" rid="c7">Deutsch and Newell, 2005</xref>) and may represent one component of motor noise that is distinct from exploration.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Variability and learning in the continuous probabilistic task.</title>
<p><bold>a.</bold> Baseline precision by age. Average adult variability shown for comparison. <bold>b</bold>. Learning block performance by age. Learning measure is the distance from the target, measured as the absolute distance from the 100% reward zone. <bold>c</bold>. Endpoint variability in the success clamp by age. <bold>d</bold>. Endpoint variability in the failure clamp by age. Regression line with 95% confidence interval shown for children and error bars show standard error of the mean for adults. Participants who completed the task in person (in lab) are indicated in white symbols.</p></caption>
<graphic xlink:href="602665v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Children also improved learning as a function of age by reducing their endpoint distance from the 100% reward zone (<italic>distance from target</italic>). The distance from target, averaged over the last 10 reaches, significantly reduced with age (<xref rid="fig4" ref-type="fig">Fig. 4b</xref>; regression <italic>R</italic><sup>2</sup> = 0.256, <italic>F</italic><sub>1,109</sub> = 37.4, p &lt; 0.001). Younger children rarely reached the 100% reward zone (gray region) whereas children over 9 years old often did.</p>
<p>We used the clamp blocks to determine how participants responded to success versus failure, and if this could contribute to developing adult-like learning patterns. We would expect participants to explore more after failure versus success. To assess this, we calculated the standard deviation of movement endpoints in each clamp condition as a measure of movement variability (<xref rid="fig4" ref-type="fig">Fig. 4c</xref> &amp; d). Adults showed high variability after failure and low variability after success, as expected. In children, variability after failure was low in younger children and increased significantly with age (regression <italic>R</italic><sup>2</sup> = 0.17, <italic>F</italic><sub>1,109</sub> = 22.3, p &lt; 0.001), doubling across the age range to reach adult levels (<xref rid="fig4" ref-type="fig">Fig. 4d</xref>). In contrast, variability after success was relatively stable across age (regression <italic>R</italic><sup>2</sup> = 0.012, <italic>F</italic><sub>1,109</sub> = 1.34, p = 0.249). Overall, these results suggest that younger children do not explore as much as older children after failures, which is essential to finding the reward zone.</p>
</sec>
<sec id="s2a2">
<title>Comparing reinforcement learning models of the task</title>
<p>To understand how participants update their reach endpoint based on binary feedback, we developed a set of reinforcement learning models of the task. Several reinforcement models have been proposed for tasks similar to ours (<xref ref-type="bibr" rid="c47">Therrien et al., 2018</xref>, <xref ref-type="bibr" rid="c46">2016</xref>; <xref ref-type="bibr" rid="c3">Cashaback et al., 2019</xref>; <xref ref-type="bibr" rid="c33">Roth et al., 2023</xref>). In the full model, a participant maintains an estimate of their desired reach location (<italic>x</italic><sub><italic>t</italic></sub>) which can be updated across trials. On each trial the actual reach location (<italic>s</italic><sub><italic>t</italic></sub>) is the desired reach with the addition of (possibly) exploration variability (<italic>e</italic><sub><italic>t</italic></sub>), planning variability (<italic>p</italic><sub><italic>t</italic></sub>) and motor noise (<italic>m</italic><sub><italic>t</italic></sub>). Motor noise, as we use it here, can include other sources of noise that do not benefit movement (e.g., sensory and memory noise). The distinction between exploration and planning variability is that exploration is only added if the last trial was unsuccessful (reward <italic>r</italic><sub><italic>t</italic></sub> = 1 for successful trials, and 0 for unsuccessful trials) whereas planning variability is added on all trials:
<disp-formula id="eqn1">
<graphic xlink:href="602665v2_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the sources of variability are all drawn from zero-mean Gaussians with standard deviations given by σ<sub><italic>e</italic></sub>, σ<sub><italic>p</italic></sub> and σ<sub><italic>m</italic></sub>.</p>
<p>The probability <italic>p</italic> of receiving a reward, <italic>r</italic><sub><italic>t</italic></sub>, depends on the actual reach endpoint and the particular reward regime used such that the <italic>reward</italic> equation is
<disp-formula id="eqn2">
<graphic xlink:href="602665v2_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>f</italic> () can represent different functions such as the continuous or discrete probabilistic and deterministic reward regimes. The desired reach can then be updated by both planning noise and any exploration noise but not by motor noise:
<disp-formula id="eqn3">
<graphic xlink:href="602665v2_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>η</italic><sub><italic>p</italic></sub> and <italic>η</italic><sub><italic>e</italic></sub> are the learning rates controlling exploration and planning contribution to the update. In the full model <inline-formula><inline-graphic xlink:href="602665v2_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the reward on that trial <italic>r</italic> so that both planning and exploration are used to update only after success. In some model variants <inline-formula><inline-graphic xlink:href="602665v2_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> so that the desired reach is always updated by planning variability. By allowing different sources of variability, learning rates and update rules, there are 30 reduced variants of the full model allowing us to fit all 31 models to the data (See Methods and <xref rid="figS5" ref-type="fig">Figure 5—figure Supplement 1</xref>)</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Reinforcement learning model for the continuous probabilistic task.</title>
<p><bold>a.</bold> Model schematic. The participant maintains an estimate of the desired reach which they can update across the experiment. The actual reach on the current trial (pink box) depends on whether the previous trial (yellow box) was a failure (top) or success (bottom). After failure (top) the actual reach is the desired reach with the addition of exploration variability and motor noise (draws from zero mean Gaussian distributions with standard deviations σ<sub><italic>e</italic></sub> and σ<sub><italic>m</italic></sub>, respectively). In contrast, if the previous trial was a success (bottom), the participant does not explore so that the actual reach is the desired reach with only motor noise. The actual reach determines the probability of whether the current trial is a failure or a success. If the current trial is a success, the desired reach is updated for the next trial (blue box) by the exploration (if any). <bold>b</bold>. Examples of model fits to three participants. The data are shown as circles, with success trials filled green and unsuccessful trials filled white. The estimated desired reach is shown as a thick black line and the estimated exploration variability (orange line) and motor noise (blue line) connect the desired reach to the data. The simulation of the participant with the fit parameters are shown in the pink line with shading showing one standard deviation across the simulations. <bold>c</bold>. Model fit parameters {σ<sub><italic>m</italic></sub>, σ<sub><italic>e</italic></sub>}, by age for the continuous probabilistic group. The line is a regression fit (with 95% confidence interval in shading) to the data for participants younger than 18 years old. The correlation and p-value for each regression are shown in the bottom left corner of each plot (and exclude the adult data). Average adult parameters are shown on the right with standard error of the mean.</p>
<p><xref rid="figS5" ref-type="fig">Figure 5—figure supplement 1</xref>. Model comparison for the Continuous and Discrete Probabilistic tasks.</p>
<p><xref rid="figS5a" ref-type="fig">Figure 5—figure supplement 2</xref>. Model parameter recovery.</p>
<p><xref rid="figS5b" ref-type="fig">Figure 5—figure supplement 3</xref>. Example fits of the model to the Continuous Probabilistic task.</p>
<p><xref rid="figS5c" ref-type="fig">Figure 5—figure supplement 4</xref>. Fits to the success and failure clamp phases for the Continuous Probabilistic task.</p></caption>
<graphic xlink:href="602665v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To identify the best model variant that could account for the data we performed model selection. We fit the model to the 100 learning trials for each child (see Methods for details), so as not to contaminate model selection by behavior in the clamp blocks or of the adults. We performed BIC comparison combining the BICs from the continuous and discrete probabilistic tasks (which had the largest datasets). Model 11 was the preferred model with a ΔBIC=216 to the next best model (<xref rid="figS5" ref-type="fig">Figure 5—figure Supplement 1</xref>). The same model was preferred when we only used BICs from each of the two tasks separately for either the children alone or including the adults. We also considered a value based model which tries to learn the value of each location. This model is based on the model by <xref ref-type="bibr" rid="c10">Giron et al. (2023)</xref> (See Methods). This model was not preferred to Model 11 (ΔBIC = 3,018) when fit the children in the Continuous probabilistic task.</p>
</sec>
<sec id="s2a3">
<title>Reinforcement learning model with exploration variability and motor noise</title>
<p>In the preferred model (Model 11 which, from now on, we will refer to simply as the model) there was no planning noise and the learning rate for exploration variability was unity. This model is a special case (with no exploration after success) of the model proposed in <xref ref-type="bibr" rid="c47">Therrien et al. (2018)</xref> and has the following output and update processes:
<disp-formula id="eqn4">
<graphic xlink:href="602665v2_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn5">
<graphic xlink:href="602665v2_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In this model, if the previous trial was a failure (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>, top), the current reach is the desired reach with the addition of exploration variability and motor noise. The desired reach for the next trial is updated only if the current reach is successful. In contrast, if the previous reach was a success (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>, bottom) the participant does not explore so that the current reach is the desired reach with motor noise. As there is no exploration, the desired reach will remain the same for the next trial whatever the outcome.</p>
<p>To gain more power for parameter estimates, we then used this model to fit the 120 trials in the experiment (learning and clamp phases). Having done this, we then examined how well our model fitting procedure could recover parameters from synthetic data. This analysis showed that both parameters (σ<sub><italic>e</italic></sub>, σ<sub><italic>m</italic></sub>) were well recovered with correlations with the true parameters of at least 0.97 (<xref rid="figS5a" ref-type="fig">Figure 5—figure Supplement 2</xref>).</p>
<p>Example fits of the model are shown in <xref rid="fig5" ref-type="fig">Fig. 5b</xref> for three participants. The first participant (top) is an example of a 5 year-old non-learner with minimal exploration variability and large motor noise. The desired reach (black line) is connected to the data (filled and hollow circles for rewarded and unrewarded trials) by the estimates of motor noise (blue vertical lines). The other participants (bottom) show learning. Here the desired reach is connected to the data by either motor noise after a successful reach, or by both motor noise and exploration variability (orange vertical lines) after an unsuccessful reach. The desired reach only changes after a successful trial that was preceded by a failure trial (hence exploration). The pink lines and shading show the median feedforward simulation and standard deviation. <xref rid="figS5b" ref-type="fig">Figure 5—figure Supplement 3</xref> shows individual fits to the same participants as in <xref rid="fig2" ref-type="fig">Fig. 2</xref>.</p>
<p>We examined how the two parameters fit to the data varied with age (<xref rid="fig5" ref-type="fig">Fig. 5c</xref>). This showed that both parameters varied significantly with age for the children (both p &lt; 0.001). Motor noise decreased by about 40% within the age range tested. Exploration variability increased with age, with an almost four-fold increase on average from the youngest to the oldest child. While this change is in the same direction to the variability in the fail clamp, the model allows us to examine exploration variability in the learning setting and separate it from motor noise. Overall, this model analysis suggests that the increased motor noise and reduced exploration limits the ability of the younger children to learn efficiently.</p>
<p>The simulations of the model with the fit parameters accounted for the differences in learning across the age group bins (<xref rid="fig3" ref-type="fig">Fig. 3</xref> red line and shading show model simulations). While the BIC analysis with the other model variants provides a relative goodness of fit, it is not straightforward to provide an absolute goodness of fit for probabilistic models such as the one we use here (as explained in the Methods). To provide an overall goodness of fit, we examined the traditional <italic>R</italic><sup>2</sup> between the average of all the children’s data during learning and the average simulation of the model (repeated 4000 time) for each child so as to reduce the stochastic variation. This analysis gave an <italic>R</italic><sup>2</sup> = 0.41.</p>
<p>We compared the model fits for the reach variability with the empirical data in the two clamp blocks. <xref rid="figS5c" ref-type="fig">Figure 5—figure Supplement 4a &amp; b</xref> show the variability expected from simulations of the model (red) and the measured movement variability of the children (blue) for the success and fail clamps as a function of age. We also examined the model fits against the empirical data (<xref rid="figS5c" ref-type="fig">Figure 5—figure Supplement 4c &amp; d</xref>) which showed we could explain 16% and 42% of the variance for the success and fail clamp phases (note that we only have 9 trials to calculate each clamp block standard deviation).</p>
<p>Overall, the results from the continuous probabilistic task model results suggest that through development, children’s motor noise decreases, exploration variability increases, and upon success, the desired reach is fully updated by exploration by any exploration present.</p>
</sec>
</sec>
<sec id="s2b">
<title>Discrete probabilistic task</title>
<p>Thus far, we identified features that limit learning in young children within a continuous probabilistic landscape. In a second experiment, we asked if segmenting the continuous target into seven discrete targets could improve learning (<xref rid="fig1" ref-type="fig">Fig. 1a</xref> – discrete). In this new task, participants were not able to move between targets but were required to definitively reach to a single target on each trial. We predicted that discrete targets could increase exploration by encouraging children to move to a different target after failure. We studied a new cohort of 106 children and 33 adults as they learned the discrete probabilistic task (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> top right).</p>
<p>Individual example data show that the older children tend to move to a new target after failure, whereas the 3 year-old often chose the same target after failure (<xref rid="figS6" ref-type="fig">Figure 6—figure Supplement 1</xref>). Discretizing the target appeared to improve the learning for the two youngest age groups (<xref rid="fig6" ref-type="fig">Fig. 6a</xref>). Learning, baseline precision, and fail clamp variability had similar trends across age as the continuous probabilistic group (<xref rid="figS6a" ref-type="fig">Figure 6—figure Supplement 2</xref>). That is, distance from target decreased with age (<italic>R</italic><sup>2</sup> = 0.074, <italic>F</italic><sub>1,104</sub> = 8.31, p = 0.0048), baseline precision improved with age (<italic>R</italic><sup>2</sup> = 0.150, <italic>F</italic><sub>1,104</sub> = 18.4, p &lt; 0.001) and fail clamp variability increased with age (<italic>R</italic><sup>2</sup> = 0.162, <italic>F</italic><sub>1,104</sub> = 20.1, p &lt; 0.001). In addition, success clamp variability decreased with age (<italic>R</italic><sup>2</sup> = 0.048, <italic>F</italic><sub>1,104</sub> = 5.25, p = 0.024).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Discrete probabilistic task learning block time series and parameter fits.</title>
<p><bold>a</bold> Same format as Fig. 3. <bold>b</bold> Panels in same format as Fig. 5c with the regressions for the continuous probabilistic task overlaid in blue.</p>
<p><xref rid="figS6" ref-type="fig">Figure 6—figure supplement 1</xref>. Example behavior, and discrete target performance in the discrete probabilistic task.</p>
<p><xref rid="figS6a" ref-type="fig">Figure 6—figure supplement 2</xref>. Variability and learning in the discrete probabilistic task.</p>
<p><xref rid="figS6b" ref-type="fig">Figure 6—figure supplement 3</xref>. Example fits of the model to the Discrete Probabilistic task.</p>
<p><xref rid="figS6c" ref-type="fig">Figure 6—figure supplement 4</xref>. Fits to the success and fail clamp phases for the Discrete Probabilistic task.</p></caption>
<graphic xlink:href="602665v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We assessed if there were differences in these measures between the discrete and continuous probabilistic groups by comparing regressions as function of age, with the intercept set to the age of the youngest child (<xref rid="figS6a" ref-type="fig">Figure 6—figure Supplement 2</xref> vs. <xref rid="fig4" ref-type="fig">Fig. 4</xref>). This showed there was no significant difference in baseline precision or variability after success. However, the variability after failure had a higher intercept (increase of 41%; p = 0.048) for the discrete probabilistic group, consistent with increasing exploration in the younger children. In addition, the intercept was lower for the distance to target (decrease of 41%; p = 0.007) consistent with better learning in the younger children with discrete targets.</p>
<sec id="s2b1">
<title>Modeling the discrete probabilistic task</title>
<p>We fit the model to the discrete probabilistic task. Exploration increased significantly with age (p &lt; 0.001; <xref rid="fig6" ref-type="fig">Fig. 6b</xref>) but the motor noise change with age was not significant. To evaluate the fit parameters by age between the two tasks, we compared the linear regressions (<xref rid="fig6" ref-type="fig">Fig. 6b</xref> blue line is continuous regression replotted on the discrete parameters). This showed that neither slope nor intercept (set to the youngest child’s age) were significantly different between the groups. In sum, the empirical data showed a significant increase in variability after failure for the younger children. The exploration parameter of the model changed in the expected direction but did not reach significance.</p>
</sec>
</sec>
<sec id="s2c">
<title>Deterministic reward tasks</title>
<p>In both experiments considered so far, the reward was probabilistic. While this provides gradient information that should guide the participant to the target, younger children may not use the same strategy for this reward landscape. In the final two tasks, we studied new cohorts of 3 to 8-year-old children since they showed poorest learning in the continuous and discrete probabilistic tasks. We assessed the effect of a deterministic reward landscape (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>, bottom row) on learning with the continuous and discrete targets.</p>
<p><xref rid="fig7" ref-type="fig">Fig. 7a</xref> compares the time courses for the 3 to 8 year olds across all four tasks. There was no significant difference in mean age between the four tasks (<italic>F</italic><sub>3,168</sub> = 1.072, p = 0.362) nor was there a significant interaction between task and sex on learning (<italic>F</italic><sub>3,164</sub> = 0.97, p = 0.409). An ANOVA of final learning (<xref rid="fig7" ref-type="fig">Fig. 7b</xref> left panel) showed a significant effect of Target type (discrete better than continuous; <italic>F</italic><sub>1,168</sub> = 12.87, p &lt; 0.001) and Reward landscape (deterministic better than probabilistic; <italic>F</italic><sub>1,168</sub> = 43.66, p &lt; 0.001) with no interaction (<italic>F</italic><sub>1,168</sub> = 0.24, p = 0.628). Learning was worst under the continuous probabilistic task, followed by the discrete probabilistic task and continuous deterministic task. The 3 to 8 year olds performed best on the discrete deterministic task. This shows that making the task discrete or deterministic improves learning and that these factors were additive.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Comparison of the four tasks for the three-to eight-year-old children.</title>
<p><bold>a.</bold> Learning block performance for the continuous probabilistic, discrete probabilistic, continuous deterministic, and discrete deterministic tasks in the same format as Fig. 3. <bold>b</bold>. Comparative performance between tasks for precision in baseline, learning distance from target, and variability in the success and fail clamp. Learning significantly improved with discrete targets and deterministic reward feedback. Precision in baseline was not statistically different between tasks. <bold>c</bold>. Estimates of motor noise and exploration variability standard deviation from the model. Statistically significant pairwise comparisons indicated as follows: * = p &lt; 0.05, + = p &lt; 0.01, and Δ = p &lt; Abbreviations: CD = Continuous Deterministic; CP = Continuous Probabilistic; DD = Discrete Deterministic; DP = Discrete Probabilistic</p>
<p><xref rid="figS7" ref-type="fig">Figure 7—figure supplement 1</xref>. Model fit parameters by age for the deterministic tasks.</p>
<p><xref rid="figS7a" ref-type="fig">Figure 7—figure supplement 2</xref>. Expected distance as a function of model parameters.</p></caption>
<graphic xlink:href="602665v2_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>This improvement in learning was not due to significant differences in baseline precision (<xref rid="fig7" ref-type="fig">Fig. 7b</xref> 2nd panel; ANOVA gave Target type: <italic>F</italic><sub>1,168</sub> = 0.16, p = 0.69, Reward landscape: <italic>F</italic><sub>1,168</sub> = 0.56, p = 0.455, interaction: <italic>F</italic><sub>1,168</sub> = 1.71, p = 0.192). For the success clamp block, there was no main effect of Target type (<italic>F</italic><sub>1,168</sub> = 0.07, p = 0.79), but a main effect of Reward landscape (<italic>F</italic><sub>1,168</sub> = 6.71, p = 0.01; less variability with a deterministic landscape) with no interaction (<italic>F</italic><sub>1,168</sub> = 3.58, p = 0.06). This is consistent with there being no advantage to explore after success under a deterministic reward landscape, whereas there is under a probabilistic landscape (exploration can lead to locations that have higher probability of reward). For the fail clamp block there was a main effect of both Target type (<italic>F</italic><sub>1,168</sub> = 29.93, p &lt; 0.001; greater variability for discrete targets) and Reward landscape (<italic>F</italic><sub>1,168</sub> = 9.26, p = 0.003; greater variability with a deterministic landscape) with no interaction (<italic>F</italic><sub>1,168</sub> = 1.01, p = 0.316). The increased variability for discrete targets is likely because participants must move to a new target to explore resulting in a larger position change. Increased variability after failure in the deterministic landscape is likely because a failure at one location predicts there will never be success at that location (in contrast to the probabilistic tasks) thereby encouraging exploration. These show that even young children choose their exploration in a rational way based on tasks features.</p>
<sec id="s2c1">
<title>Modeling the four tasks and the optimal model</title>
<p>Model fits to the 3 to 8 year olds are shown in <xref rid="fig7" ref-type="fig">Fig. 7a</xref> (red lines and shading). Examination of the fit parameter for these groups (<xref rid="fig7" ref-type="fig">Fig. 7c</xref>) showed that motor noise was significantly affected by Reward landscape (<italic>F</italic><sub>1,168</sub> = 35.5, p &lt; 0.001) but not by Target type. Exploration variability was significantly affected by Reward landscape (<italic>F</italic><sub>1,168</sub> = 60.1, p &lt; 0.001) and Target type (<italic>F</italic><sub>1,168</sub> = 16.6, p &lt; 0.001).</p>
<p>We can use the model to understand the optimal amount of exploration variability for different amounts of motor noise. To do this we used the model to estimate the average distance to target across the learning phase for different levels of motor noise and exploration variability for the four tasks (<xref rid="figS7a" ref-type="fig">Figure 7—figure Supplement 2</xref> colormap). These simulations showed that as motor noise increases, the optimal amount of exploration variability decreases (white lines; see Appendix 1 for a proof of this relation). Although the same trend is seen in the model fits to the children’s behavior (gray bars), in general the children did not produce the optimal amount of exploration.</p>
</sec>
<sec id="s2c2">
<title>Task comprehension</title>
<p>Poor or absent learning in the youngest age group could arise from these participants not understanding the task. However, we can rule out task comprehension as an explanation for lack of learning, at least for the majority of younger participants. First, 16 participants completed the continuous probabilistic task in the lab, and this sample included 5 participants in the 3-to 8-year-old age range. The task was administered in the same way as the online version, without additional instructions or interaction with the experimenter during the task. However, after completion, the experimenter asked these children to explain what they did during the task as an informal comprehension check and it was clear that this group understood the task. These in lab participants were able to accurately describe the goal of the task. Performance for all participants tested in the lab was not significantly different from online completion and these data points are indicated in <xref rid="fig2" ref-type="fig">Fig. 2</xref> and <xref rid="fig4" ref-type="fig">Fig. 4</xref> with white symbols. Wilcoxon signed-rank tests showed no significant difference between in lab and online participants in baseline mean (Z = 1.238, p = 0.216), baseline precision (Z = –0.626, p = 0.532), distance from target (Z = –0.348, p = 0.728), success clamp variability (Z = –1.952, p = 0.051), fail clamp variability (Z = 0.281, p = 0.779), or single target mean (Z = –0.399, p = 0.690).</p>
<p>Second, we used the reinforcement learning model to assess whether an individual’s performance is consistent with understanding the task. In the case of a child who does not understand the task, we expect that they simply have motor noise on their reach, and crucially, that they would not explore more after failure, nor update their reach after success. Therefore, we used a likelihood ratio test to examine whether the preferred model was significantly better at explaining each participant’s data compared to the model variant which had only motor noise (Model 1). Focusing on only the youngest children (age 3–5), this analysis showed that that 43, 59, 65 and 86% of children (out of N = 21, 22, 20 and 21) for the continuous probabilistic, discrete probabilistic, continuous deterministic, and discrete deterministic conditions, respectively, were better fit with the preferred model, indicating non-zero exploration after failure.</p>
<p>In the 3–5 year old group for the discrete deterministic condition, 18 out of 21 had performance better fit by the preferred model, suggesting this age group understands the basic task of moving in different direction to find a rewarding location. The reduced numbers fit by the preferred model for the other conditions likely reflects differences in the task conditions (continuous and/or probabilistic) rather than a lack of understanding of the goal of the task.</p>
</sec>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We studied a large, cross-sectional cohort of children and adolescents aged 3 to 17-years-old and adults performing reinforcement-based motor learning tasks. Binary feedback—success or failure—was the only learning signal provided as participants moved to continuous or discrete targets under probabilistic or deterministic reward landscapes. The continuous target gave them unlimited movement choices across the target zone, whereas discrete targets constrained choices. The movement endpoint position was mapped to reward probability, with the probabilistic landscape varying from 25% to a small 100% reward region, and the deterministic landscapes had a single, small 100% reward zone. We found a developmental trajectory of reinforcement learning in the continuous probabilistic task, with older children learning more than younger children. This was paralleled by age-dependent increases in exploration after a failed trial which is essential for learning, and decreases in baseline movement precision. A mechanistic model of the task revealed that in addition to the increase in exploration, there was a reduction in motor noise with age.</p>
<p>In contrast to the children in the continuous probabilistic group, younger children learned bet ter when the task had discrete targets or deterministic reward. These effects appeared to be additive—the 3 to 8-year-old children learned best with discrete targets <italic>and</italic> in a deterministic reward landscape. Thus, the youngest children had the fundamental mechanisms for interpreting binary reward signals to drive reinforcement learning of movement—this is not surprising given that this ability has been shown in infants (<xref ref-type="bibr" rid="c34">Rovee and Rovee, 1969</xref>). However, children aged 3 to 8 did not effectively utilize reward signals to learn in situations where they had to respond to probabilistic environments or where there were no spatial cues specifying distinct targets.</p>
<p>Our data suggest that the developmental trajectory we identified was not due to poor motor accuracy or lack of understanding of the task in the younger children. We designed the baseline block to ensure that children could accurately hit individual targets presented in different locations across the screen. The width of these targets was the same as that of the hidden 100% reward zone within the learning block and children of all ages could hit these targets accurately. The youngest children could also learn similarly to adults in the discrete deterministic task. This shows that children were able to understand the concept of the task and how to be successful.</p>
<p>In reinforcement learning tasks, different sources of movement variability have been proposed, together with a range of models of how these sources are used to update the desired reach for future movements. For example, motor noise has been proposed in several models of control (e.g. <xref ref-type="bibr" rid="c11">Harris and Wolpert, 1998</xref>; <xref ref-type="bibr" rid="c48">Todorov and Jordan, 2002</xref>; <xref ref-type="bibr" rid="c49">Trommershauser et al., 2003</xref>). Such noise is thought to be unintentional and unavoidable and studies show that humans tend to plan movements to minimize the bad consequences of motor noise. Other sources of variability are typically considered to be beneficial as they can be used to update reaching behavior. For example, in some models, planning variability (<xref ref-type="bibr" rid="c2">van Beers, 2009</xref>) is always added to the movement independent of whether previous trials were successful or not. A third possible source of variability is exploration that is distinguished from planning variability in that it is only present after an unsuccessful movement. Several models have been proposed with different combinations of motor noise, planning, and exploration variability (e.g. <xref ref-type="bibr" rid="c46">Therrien et al., 2016</xref>, <xref ref-type="bibr" rid="c47">2018</xref>; <xref ref-type="bibr" rid="c33">Roth et al., 2023</xref>). We were able to unify these previous models as special cases of a more general model. This allowed us to compare 31 model variants and we found that the preferred model was a special case (with no exploration after success) of the model proposed in <xref ref-type="bibr" rid="c47">Therrien et al. (2018)</xref>. This suggests that variability in our task is due to motor noise as well as active exploration after failure. The desired reach is updated by the exploration present on a successful trial which was preceded by a failure trial (the necessary condition for exploration). The model differs from the preferred model in <xref ref-type="bibr" rid="c33">Roth et al. (2023)</xref> which also includes a learning rate for the update so that only part of exploration is incorporated into the next reach.</p>
<p>One major finding from this study is that exploration variability increases with age. Some other studies of development have shown that exploration can decrease with age indicating that adults explore less compared to children (<xref ref-type="bibr" rid="c37">Schulz et al., 2019</xref>; <xref ref-type="bibr" rid="c25">Meder et al., 2021</xref>; <xref ref-type="bibr" rid="c10">Giron et al., 2023</xref>). We believe the divergence between our work and these previous findings is largely due to the experimental design of our study and the role of motor noise. In the paradigm used initially by <xref ref-type="bibr" rid="c37">Schulz et al. (2019)</xref> and replicated in different age groups by <xref ref-type="bibr" rid="c25">Meder et al. (2021)</xref> and <xref ref-type="bibr" rid="c10">Giron et al. (2023)</xref>, participants push buttons on a two-dimensional grid to reveal continuous-valued rewards that are spatially correlated. Participants are unaware that there is a maximum reward available and therefore children may continue to explore to reduce uncertainty if they have difficulty evaluating whether they have reached a maxima. In our task by contrast, participants are given binary reward and told that there is a region in which reaches will always be rewarded. Motor noise is an additional factor which plays a key role in our reaching task but minimal if any role in the discretized grid task. As we show in simulations of our task, as motor noise goes down (as it is known to do through development) the optimal amount of exploration goes up (see <xref rid="figS7a" ref-type="fig">Figure 7—figure Supplement 2</xref> and Appendix 1). Therefore, the behavior of our participants is rational in terms of increasing exploration as motor noise decreases.</p>
<p>A key result is that exploration in our task reflects sensitivity to failure. Older children make larger adjustments after failure compared to younger children to find the highly rewarded zone more quickly. <xref ref-type="bibr" rid="c8">Dhawale et al. (2017)</xref> discuss the different contexts in which a participant may explore versus exploit (i.e., stick at the same position). Exploration is beneficial when reward is low as this indicates that the current solution is no longer ideal, and the participant should search for a better solution. <xref ref-type="bibr" rid="c14">Konrad et al. (2025)</xref> have recently shown this behavior in a real-world throwing task where 6 to 12 year old children increased throwing variability after missed trials and minimized variability after successful trials. This has also been shown in a postural motor control task where participants were more variable after non-rewarded trials compared to rewarded trials (<xref ref-type="bibr" rid="c51">Van Mastrigt et al., 2020</xref>). In general, these studies suggest that the optimal amount of exploration is dependent on the specifics of the task.</p>
<p>We found that turning a continuous target into discrete buttons improved learning performance. In our tasks, a child needs to remember their previous endpoint position and whether it was rewarded to decide where to move at the start of the next trial. We suspect that the continuous target task relies more on spatial working memory which is not fully developed (<xref ref-type="bibr" rid="c1">van Asselen et al., 2006</xref>), particularly in those under 10 years of age (<xref ref-type="bibr" rid="c12">Hitch et al., 1988</xref>; <xref ref-type="bibr" rid="c30">Pickering, 2001</xref>; <xref ref-type="bibr" rid="c50">Tsujii et al., 2009</xref>). Younger children may benefit from clear markers in the environment to differentiate various spatial positions along the target. This is consistent with tasks that have assessed children’s preferences for reporting information; they find it easier to select from discrete choices on a Likert scale versus using a continuous visual analog scale (<xref ref-type="bibr" rid="c16">van Laerhoven et al., 2004</xref>; <xref ref-type="bibr" rid="c40">Shields et al., 2003</xref>). Finally, it is also possible that older children had the ability to use different working memory encoding mechanisms. It is known that phonological encoding using verbal labels develops later than visual encoding and both can be used to hold information in working memory (e.g., my last movement was near the left edge of the screen) (<xref ref-type="bibr" rid="c30">Pickering, 2001</xref>; <xref ref-type="bibr" rid="c12">Hitch et al., 1988</xref>). Future work could explore whether providing an explicit verbal strategy to younger children could improve their ability to explore more effectively with a continuous target, highlighting the interplay of cognitive and motor domains in reinforcement learning.</p>
<p>We also found that deterministic reward feedback improved learning. The deterministic land-scape is less ambiguous than the probabilistic landscape by design—participants always fail if they are outside the 100% reward zone. The need to track and evaluate the reward frequency across target zones is eliminated, making the task less complex. Younger children have been reported to take longer to choose a reward maximization strategy in a probabilistic environment where all choices have the possibility of being rewarded. <xref ref-type="bibr" rid="c31">Plate et al. (2018)</xref> showed that when children and adults choose between eight options, they initially probability match (i.e., the frequency of selection closely matches the frequency of reward on the various task options). However, adults switch over to a maximizing strategy (i.e., choosing the highest reward option) more quickly than children (<xref ref-type="bibr" rid="c31">Plate et al., 2018</xref>). In a deterministic landscape, probability matching would result in the same behavior as reward maximizing and therefore the younger children’s behavior would appear nearly the same as adults. Young children’s behavior may also stem from a fundamental need to prioritize hypothesis testing and gathering information about the world (<xref ref-type="bibr" rid="c37">Schulz et al., 2019</xref>; <xref ref-type="bibr" rid="c18">Liquin and Gopnik, 2022</xref>; <xref ref-type="bibr" rid="c29">Nussenbaum and Hartley, 2019</xref>), a discovery process that facilitates increased knowledge about the environment’s causal structure (<xref ref-type="bibr" rid="c41">Sobel and Sommerville, 2010</xref>).</p>
<p>Our interpretation is that poorer learning performance in tasks besides discrete deterministic was due to an inability to effectively utilize probabilistic reward signals or find the high reward location without clearly delineated spatial cues. This has significant ramifications as most objects in the world do not have delineated targets on them; we learn which location on an object leads to reward by exploring different options (e.g., the best location to grab a mug or push a heavy door open). The world is also not deterministic, as it is rare that the same action will always give the same result. Movement outcomes are probabilistic due to both environmental variation and motor noise (e.g., the direction of a soccer ball when kicked on different fields or the location of a thrown dart on a dartboard). Eventually, children must learn how to move successfully to interact with their environments using such probabilistic signals.</p>
<p>The differential ability to incorporate reward signals into changes in behavior across childhood may stem from maturation of the reward centers in the brain. Structures important for these processes, such as the basal ganglia reward centers, dorsal lateral prefrontal cortex, posterior parietal cortex, and the anterior cingulate cortex develop throughout childhood (<xref ref-type="bibr" rid="c32">Raznahan et al., 2014</xref>; <xref ref-type="bibr" rid="c28">Nelson et al., 2000</xref>; <xref ref-type="bibr" rid="c36">Schultz, 1998</xref>). As a main underlying neural structure of reinforcement learning, the pallidum reaches peak volume at 9.5 years for females and 7.7 years for males while it takes the thalamus until 13.8 years for females and 17.4 years for males to reach peak volume (<xref ref-type="bibr" rid="c32">Raznahan et al., 2014</xref>). Older children have more mature brain circuits and may be better able to take advantage of strategies in probabilistic environments that younger children cannot (<xref ref-type="bibr" rid="c37">Schulz et al., 2019</xref>; <xref ref-type="bibr" rid="c18">Liquin and Gopnik, 2022</xref>). For example, older children might know to avoid a location as soon as a failure occurs, even if that location was previously rewarded. Younger children might continue sampling those locations, perhaps due to immature signaling in the brain. Indeed, it has been shown that brain activity in younger children can be similar after positive and negative rewards whereas in older children and adults it is more distinct (<xref ref-type="bibr" rid="c19">Mai et al., 2011</xref>; <xref ref-type="bibr" rid="c9">Eppinger et al., 2009</xref>; <xref ref-type="bibr" rid="c26">van Meel et al., 2005</xref>).</p>
<p>Online studies such as ours inevitably have some limitations. Given that this task was conducted remotely, we did not have control of the computer operating quality, internet speed, or testing environment for our participants (see Methods for exclusion criteria). As such, we were not able to control or analyze timing parameters of movement in detail, which can be done more easily in in-person experimentation. However, our key analyses compare across large groups which likely factor out these uncontrolled variables. Our results (comparison of in lab vs. online and fit of a noise model vs. full reinforcement learning model) also confirm that even the youngest age group understood the task. We also recognize that other processes, such as memory and motivation, could affect performance on these tasks however our study was not designed to test these processes directly and future work would benefit from exploring these other components more explicitly.</p>
<p>Our findings in typical development lay a foundation for better understanding of behavior during childhood and could help inform what function may be lost or impaired after injury. This underscores the need to consider not only disease process for interventions but also age as there are developmental differences in motor learning capacity in individuals at different ages. Knowing how the sensorimotor system works at different ages can guide decisions on how to intervene or alter an environment and give children the best opportunity to use reinforcement learning mechanisms for rehabilitation outcomes.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>Children and adults without neurological impairment or developmental delay were recruited to one of four experiments as outlined in <xref rid="tbl1" ref-type="table">Table 1</xref> and <xref rid="tbl2" ref-type="table">Table 2</xref>. A total of 385 complete datasets were included in the analysis. The continuous probabilistic task was designed first and the other three tasks, discrete probabilistic, continuous deterministic, and discrete deterministic, were conducted to expand upon the results of the first task and further identify factors contributing to learning by reinforcement across the developmental spectrum. Participants were recruited from the Johns Hopkins University community through an online announcement portal, the greater Baltimore Maryland area through Research Match, and nationwide through the online platform Lookit which merged with Children Helping Science in 2023 (<xref ref-type="bibr" rid="c39">Sheskin et al., 2020</xref>; <xref ref-type="bibr" rid="c38">Scott and Schulz, 2017</xref>). Our sample includes participants from 38 states out of the 50 United States of America (<xref rid="figS1" ref-type="fig">Figure 1—figure Supplement 1</xref>). Each participant was screened to rule out neurological impairment, developmental delay, and other developmental disorders that could affect motor and cognitive development. This study was approved by the Johns Hopkins School of Medicine Institutional Review Board and all participants, or their parent/legal guardian, provided informed consent prior to participation.</p>
</sec>
<sec id="s4b">
<title>Task Platform</title>
<p>All four tasks were completed on a web-based platform built with Javascript as previously reported by (<xref ref-type="bibr" rid="c20">Malone et al., 2023</xref>) with modifications to the game environment and feedback type. This platform allowed creativity in the game environment design to be kid friendly and engaging to young participants to foster sustained attention and increase likelihood of completing the full task. The task platform allowed remote completion of the experiment by participants on their home computer or tablet. A small subset of participants in the continuous probabilistic task (n = 16; 3 to 5yo n = 2, 6 to 8yo n = 3, 9 to 11yo n = 5, 12 to 14yo n = 5, 15 to 17yo n = 1) completed the task in person in the research laboratory and the remainder of participants completed the task remotely. Participants used a mouse, trackpad, or touchscreen input to control a cartoon penguin game piece and move across the game environment. Movement trajectories were sampled at the polling rate of the selected input device and data from each trial were uploaded and saved to Google Firebase Realtime Database at the end of each trial. Due to the remote data collection nature of this experiment, we were not able to control data sampling rates. Each trial was manually inspected and sampling rates of input devices had a mean of 37.363 ± 4.184 Hz.</p>
</sec>
<sec id="s4c">
<title>Procedure</title>
<p>The game environment is an immersive icy landscape with penguins. The overall goal of the task was to move a penguin from the starting position on one side of the ice (at the bottom of the computer or tablet screen closest to the participant) to a distance of 24 game units (GU) into the game (at the far edge of the screen away from the participant). If successful, a pleasant sound would play, the video screen above the ice would be outlined in blue, and a Disney video clip (different on each trial; gifs hosted on <ext-link ext-link-type="uri" xlink:href="https://giphy.com">https://giphy.com</ext-link>) would play. Multiple signals of reward were provided to ensure that participants associated their behavior with the feedback provided and could clearly differentiate between a successful trial and a failure trial. The penguin game piece started as a blue color to indicate that it was not active. To activate the penguin, the participant had to click the left mouse button (mouse or trackpad) or touch and hold the penguin and it would turn white to indicate that it was movable. Then the participant made a reaching movement to move the penguin across the ice. The trial would end when the Y position of the penguin exceeded 24 GU. To account for variability in input device sampling rates, the final X position was computed as an interpolation between the data points immediately before and after Y = 24 GU such that every trial had a calculated X position at Y = 24 GU.</p>
<p>Rewards were determined based upon the interpolated final X position of the game piece at the back edge of the ice and a task specific reward landscape. All tasks included five blocks as outlined in <xref rid="fig2" ref-type="fig">Fig. 2a</xref>. Baseline (20 trials): a single, discrete target (image of three dancing penguins) was presented at a randomized X position at the far edge of the ice. Participants were instructed to move accurately to the target. Learning (100 trials): participants were exposed to one of four learning environments with variations in the target type (continuous vs. discrete, <xref rid="fig1" ref-type="fig">Fig. 1a</xref>) and the reward feedback (probabilistic vs. deterministic, <xref rid="fig1" ref-type="fig">Fig. 1b</xref>). In the continuous conditions, participants could choose to move the penguin to any location on a continuous horizontal target (<xref rid="fig1" ref-type="fig">Fig. 1a</xref> – continuous). In the discrete conditions, participants could move to one of seven targets spread horizontally (<xref rid="fig1" ref-type="fig">Fig. 1a</xref> – discrete). For probabilistic feedback, the reward was determined by an unseen position-based probability gradient with a small 100% reward zone away from which reward probabilities decreased linearly to a baseline (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> – continuous probabilistic and discrete probabilistic). For deterministic feedback, reward was always given within a reward zone but not elsewhere (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> – continuous deterministic and discrete deterministic). Success Clamp (10 trials): every trial in this block was rewarded regardless of the final X position of the game piece. Fail Clamp (10 trials): no trials in this block were rewarded. Single Target (10 trials): the same single, discrete target as in baseline was presented in the center of the far edge of the ice and participants were cued to move accurately towards it. A break for a new set of instructions was provided between Baseline and Learning as well as Fail Clamp and Single Target. Participants were unaware of the reward criteria transitions between Learning and Success Clamp and Success Clamp and Fail Clamp. The ideal/mature behavior in these tasks was to explore at the beginning of the learning block to find the area of most frequent reward and then exploit this behavior to maximize reward for the remaining trials of the block. Moreover, if the 100% reward zone has been found successfully, continuing to move to this reinforced position during the success clamp and then exploring again in the fail clamp are indicators of mature sensitivity to reward and failure.</p>
</sec>
<sec id="s4d">
<title>Continuous Probabilistic</title>
<p>In the Learning block for the continuous probabilistic task, participants were presented with a continuous target and probabilistic reward feedback (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> – continuous probabilistic). The probability reward landscape is defined by setting reward percentage probabilities at 5 specific <italic>x</italic> locations with reward between these locations being linearly interpolated (and constant outside the range). We set X values of {−24, −14.5, −9.5, 1.1875} to reward probabilities {33, 100, 100, 25}.</p>
<p>Participants were warned that some of the ice was starting to melt which would cause the penguin to slip and were told that in some places they would slip a lot of the time, some places they would slip some of the time, and in some places they would never slip. They were instructed to move the penguin across the ice without slipping to get the movie clip to play as often as possible. If they were not successful, the penguin would fall over, and they would see a sad face penguin image before the game reset for the next trial <xref rid="fig1" ref-type="fig">Fig. 1c</xref>. This task design builds from <xref ref-type="bibr" rid="c3">Cashaback et al. (2019)</xref> where participants were asked to reach to any location on a continuous line, with the probability of being rewarded dependent on the reach end location. In their task, there was a small (hidden) 100% reward zone with reward probability decreasing on either side away from this zone. They found that adult participants could learn from a probabilistic reward landscape and find the rewarding zone (<xref ref-type="bibr" rid="c3">Cashaback et al., 2019</xref>). We explored a similar task design in participants across development.</p>
</sec>
<sec id="s4e">
<title>Discrete Probabilistic</title>
<p>In the Learning block for the discrete probabilistic task, participants were presented with a set of targets, each associated with a specific probability of reward (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> – discrete probabilistic). We set the center of seven the targets at X values of {−18, −12, −6, 0, 6, 12, 18} with target width of 5 and with reward percentage probabilities of the {66, 100, 66, 33, 25, 25, 25} (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> bottom left).</p>
<p>The discrete targets were visually the same as those used in the Baseline and Single Target blocks, however all seven were presented at the same time in equally spaced positions across the edge of the ice. Participants were instructed to find the group of penguins that made the video clip play all the time by moving their penguin game piece across the ice. They were told to move to one group of penguins on each trial and that some penguins would make the movie clip play some of the time but there was a group of penguins that played the clip all the time. If they were not successful, they would see a red X on the video screen and the video clip would not play before the game reset for the next trial. To ensure that participants could accurately distinguish the feedback associated with each target, there was a visible space between each target. If the participant moved between targets, the participant would receive a message to try again, and the trial would reset until one target was accurately hit.</p>
</sec>
<sec id="s4f">
<title>Continuous Deterministic</title>
<p>In the Learning block for the continuous deterministic task, participants were presented with a continuous target and deterministic reward feedback (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> – continuous deterministic). Participants were warned that some of the ice was starting to melt which would cause the penguin to slip and were told that in some places they would slip all of the time and in some places they would never slip. They were instructed to move the penguin across the ice without slipping to get the movie clip to play as often as possible. If they were not successful, the penguin would fall over, and they would see a sad face penguin image before the game reset for the next trial. This task was completed by a subset of participants aged three to eight years and adults.</p>
</sec>
<sec id="s4g">
<title>Discrete Deterministic</title>
<p>In the Learning block for the discrete deterministic task, participants were presented with a set of seven discrete targets and deterministic reward feedback (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> – discrete deterministic). Participants were instructed to move across the ice to one of the groups of penguins on each trial to get the movie clip to play. They were told that one group of penguins would make the video clip play all of the time. If they were not successful, they would see a red X on the video screen and the video clip would not play before the game reset for the next trial. As in the discrete probabilistic task, to ensure that participants could accurately distinguish the feedback associated with each target, there was a space between each target. If the participant moved between targets, the trial would reset until one target was accurately hit. This task was completed by a subset of participants aged three to eight years and adults.</p>
</sec>
<sec id="s4h">
<title>Demo Versions of Tasks</title>
<p>Shortened versions of each task without data collection are provided at the following links. In these versions, there are 5 trials in baseline, 10 trials in learning, 2 trials in each clamp, and 3 trials of single target. To proceed beyond the information input screen, use an arbitrary 6-digit code for the subjectID and participant information to sample the game environment.</p>
<p>Continuous Probabilistic:</p>
<p><ext-link ext-link-type="uri" xlink:href="https://kidmotorlearning.github.io/PenguinsDemo_Continuous-Probabilistic/">https://kidmotorlearning.github.io/PenguinsDemo_Continuous-Probabilistic/</ext-link></p>
<p>Discrete Probabilistic:</p>
<p><ext-link ext-link-type="uri" xlink:href="https://kidmotorlearning.github.io/PenguinsDemo_Discrete-Probabilistic/">https://kidmotorlearning.github.io/PenguinsDemo_Discrete-Probabilistic/</ext-link></p>
<p>Continuous Deterministic:</p>
<p><ext-link ext-link-type="uri" xlink:href="https://kidmotorlearning.github.io/PenguinsDemo_Continuous-Deterministic/">https://kidmotorlearning.github.io/PenguinsDemo_Continuous-Deterministic/</ext-link></p>
<p>Discrete Deterministic:</p>
<p><ext-link ext-link-type="uri" xlink:href="https://kidmotorlearning.github.io/PenguinsDemo_Discrete-Deterministic/">https://kidmotorlearning.github.io/PenguinsDemo_Discrete-Deterministic/</ext-link></p>
</sec>
<sec id="s4i">
<title>Measurements and Analysis</title>
<p>We used several metrics to analyze performance and variability in different blocks and evaluate reinforcement learning over childhood. Baseline performance was defined as the signed distance from the target center averaged over each of the 20 baseline trials. Baseline precision was defined as the standard deviation of the baseline performance. Importantly, the width of these baseline targets was the same as the 100% reward zone in the learning block for the continuous target tasks. This allowed determination of both success and precision with moving to targets the width of the goal. Learning performance (<italic>Distance from target</italic>) is defined as the absolute value of the interpolated X position distance from the center of 100% reward zone (located at X = -12 in all tasks) averaged over the last 10 trials of the learning block. A <italic>Distance from target</italic> value closer to 0 indicates better learning. To quantify variability after success, we used the standard deviation of the interpolated X position in trials 2 through 10 of the success clamp block. To quantify variability after failure, we used the standard deviation of the interpolated X position in trials 2 through 10 of the fail clamp block.</p>
<p>Participant characteristics (sex and game play handedness) and game play parameters (device and browser) were analyzed in one way ANOVAs with dependent variables of <italic>Precision</italic> and <italic>Distance from target</italic> to determine whether these parameters influenced learning ability (Supplementary Table 1). To determine whether there was a differential effect of sex, device, or handedness on learning performance in the four different tasks, additional two-way ANOVAs with dependent variable of <italic>Distance from target</italic> were used. There was not a significant interaction between task and sex (reported in Results), task and device (<italic>F</italic><sub>3,168</sub> = 0.62, p = 0.717), or task and handedness (<italic>F</italic><sub>3,168</sub> = 1.2, p = 0.312). Each trial was divided into three phases to describe the reaction, stationary, and movement times. Reaction time is defined as the time from the appearance of the penguin (start of the trial) to the time the penguin is clicked and activated. Stationary time is defined as the time from penguin click to movement onset (first non-zero position). Movement time is defined as movement onset to end of the trial when the penguin moves across the back edge of the ice. Trial timings for each phase of movement were extracted and averaged for each participant across the whole experiment and then averaged within individual age bins to evaluate across ages. Total game play time was also extracted and averaged by age bins. Path length ratios were calculated as the actual path length from the start to end of the trajectory divided by the ideal straight-line path from the first position in the trajectory to the last position in the trajectory. The path length ratio for all trials were averaged for each participant and then averaged within age bins for comparison between ages.</p>
<p>We used linear regression and one and two-way ANOVAs to evaluate effects of age and other outcome variables on learning as well as compare performance between tasks. Significance level of 0.05 was used for all statistical tests. All raw data and statistical analyses were completed using custom scripts in MATLAB (version R2023a).</p>
</sec>
<sec id="s4j">
<title>Reinforcement Learning Model</title>
<p>Several reinforcement models have been proposed for tasks similar to ours (<xref ref-type="bibr" rid="c47">Therrien et al., 2018</xref>, <xref ref-type="bibr" rid="c46">2016</xref>; <xref ref-type="bibr" rid="c33">Roth et al., 2023</xref>). With some reparameterizing, all these models can be considered as special cases of a ‘full’ model. We fit this full model, as well as all 30 possible variants of the full model. These models can be considered mechanistic models as defined by <xref ref-type="bibr" rid="c17">Levenstein et al. (2023)</xref>.</p>
<p>In these models, a participant maintains an estimate of their desired reach location (<italic>x</italic><sub><italic>t</italic></sub>; which reflects the estimated target location) which can be updated across trials. On each trial the actual real location (<italic>s</italic><sub><italic>t</italic></sub>) is the desired reach with the addition of (possibly) exploration variability (<italic>e</italic><sub><italic>t</italic></sub>), planning variability (<italic>p</italic><sub><italic>t</italic></sub>) and motor noise (<italic>m</italic><sub><italic>t</italic></sub>). The distinction between exploration and planning variability/motor noise is that exploration variability is only added if the last trial was unsuccessful (reward <italic>r</italic><sub><italic>t</italic></sub> = 1 for successful trials, and 0 for unsuccessful trials) whereas planning variability and motor noise is added on all trials:
<disp-formula id="eqn6">
<graphic xlink:href="602665v2_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the sources of variability are all zero mean Gaussian with standard deviations given by σ<sub><italic>e</italic></sub>, σ<sub><italic>p</italic></sub> and σ<sub><italic>m</italic></sub>. There can be other sources of noise (sensory, memory etc.) and we regard them as being included as part of motor noise (since they are never used to update the reach). The probability <italic>p</italic> of reward received, <italic>r</italic><sub><italic>t</italic></sub>, depends on the reach location and the particular reward regime used such that the <italic>reward</italic> equation is
<disp-formula id="eqn7">
<graphic xlink:href="602665v2_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>f</italic> () can represent different functions such as the continuous or discrete probabilistic and deterministic reward regimes. The desired reach can then be updated by both planning variability and any exploration noise but not by motor noise (this is the key distinction between planning variability and motor noise).
<disp-formula id="eqn8">
<graphic xlink:href="602665v2_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>η</italic><sub><italic>p</italic></sub> and <italic>η</italic><sub><italic>e</italic></sub> are the learning rates controlling exploration and planning variability contribution to the update. In the full model <inline-formula><inline-graphic xlink:href="602665v2_inline2a.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the reward on that trial <italic>r</italic><sub><italic>t</italic></sub> so that both exploration (if present) and planning variability are used to update after success. In some model variants <inline-formula><inline-graphic xlink:href="602665v2_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> so that the desired reach is always update by planning variability.</p>
<p>The variants of the model determine whether (i) each source of variability (<italic>e</italic><sub><italic>t</italic></sub>, <italic>p</italic><sub><italic>t</italic></sub>, <italic>m</italic><sub><italic>t</italic></sub>) are present or not, (ii) the settings of the learning rates (<italic>η</italic><sub><italic>e</italic></sub> and <italic>η</italic><sub><italic>p</italic></sub> independent, identical, <italic>η</italic><sub><italic>e</italic></sub> or <italic>η</italic><sub><italic>p</italic></sub> or both set to unity) and (iii) whether <inline-formula><inline-graphic xlink:href="602665v2_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>equals <italic>r</italic> or 1.</p>
<p>This leads to 80 variants: exploration (2) × planning (2) × motor (2) × learning <inline-formula><inline-graphic xlink:href="602665v2_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.However, many of these variants can be excluded leaving 31 model variants. The exclusions are (i) models with no planning variability and no motor noise (as they show no variability in success clamp for example), (ii) models with no exploration variability but a fit or non-zero <italic>η</italic><sub><italic>e</italic></sub>, (iii) models with no planning variability but a fit or non-zero <italic>η</italic>, (iv) models with no planning variability but <inline-formula><inline-graphic xlink:href="602665v2_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> set to <italic>r</italic><sub><italic>t</italic></sub>.</p>
<p>The model variants are shown in <xref rid="figS5" ref-type="fig">Figure 5—figure Supplement 1</xref> and some of these variants corresponds to previously proposed models (after allowing renaming of variables; for example a model with exploration variability present on every trial would be regarded as planning variability). Models 23, 22, 30, 10 &amp; 6 are models 1–5, respectively from (<xref ref-type="bibr" rid="c33">Roth et al., 2023</xref>). Model 8 is the model from (<xref ref-type="bibr" rid="c46">Therrien et al., 2016</xref>) and Model 28 is the model in (<xref ref-type="bibr" rid="c47">Therrien et al., 2018</xref>). Model 20 is the model from (<xref ref-type="bibr" rid="c3">Cashaback et al., 2019</xref>).</p>
<sec id="s4j1">
<title>Model fitting</title>
<p>To fit these stochastic models to each participant’s data, we used a Kalman smoother. The full model has 5 parameters: θ = {σ<sub><italic>m</italic></sub>, σ<sub><italic>e</italic></sub>, σ<sub><italic>p</italic></sub>, <italic>η</italic><sub><italic>e</italic></sub>, <italic>η</italic><sub><italic>p</italic></sub>}. We can reformulate the equations above into a standard Kalman update and output equations. The update equation is given by
<disp-formula id="eqn9">
<graphic xlink:href="602665v2_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="602665v2_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.The output equation is given by
<disp-formula id="eqn10">
<graphic xlink:href="602665v2_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We initialized the states as all zeros and the state covariance as
<disp-formula id="eqn11">
<graphic xlink:href="602665v2_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the wide prior on the initial reach direction allows the Kalman smoother to fit each participant’s initial reach. We used Matlab fminsearchbnd to fit the parameters to maximize the likelihood.</p>
</sec>
<sec id="s4j2">
<title>Model selection</title>
<p>We first fit each model to maximize the likelihood over the 100 learning trials for each child participant in the continuous probabilistic task. This ensured that our model selection was not contaminated by behavior in the clamp block. We used BIC to select the preferred model. The same model (model 11) was preferred if we only considered the children or also included the adults. Moreover, the same model was preferred for both the children in the discrete probabilistic task (or again if we included the adults). We therefore, focus on Model 11 which is a special case (with no exploration after success) of the model proposed in <xref ref-type="bibr" rid="c47">Therrien et al. (2018)</xref>.</p>
</sec>
<sec id="s4j3">
<title>Preferred model</title>
<p>In the preferred model, the produced reach is the desired reach with the addition of the variability from motor noise and exploration (if any), given by the <italic>output</italic> equation:
<disp-formula id="eqn12">
<graphic xlink:href="602665v2_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
After each reach, the participant updates their desired reach location only if they were successful. They update the reach to incorporate all of the exploration (if any) so that the <italic>update</italic> equation is
<disp-formula id="eqn13">
<graphic xlink:href="602665v2_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We examined parameter recovery for the preferred model by generating simulated data (for each participant using their best fit parameters for the preferred model) and compared the fit values with the true values used to simulate the data. This showed correlations were above 0.96 for both parameters.</p>
<p>Having found the preferred model, we used that to fit the 120 trials for each participant in each of the four conditions so as to constrain the parameter fits using all the data. For the discrete tasks, we fit the data in the same way as the continuous (so that the model does not know about the discrete nature of the task).</p>
</sec>
<sec id="s4j4">
<title>Model simulations</title>
<p>To simulate a participant, we used the best fit parameter to generate Monte Carlo simulations. We used the estimated posterior of the initial state from the Kalman smoother to initialize both the desired and actual reach by sampling from the appropriate posterior 100,000 times and used these to generated the same number of Monte Carlo simulations. We ensured that the desired and actual reaches were constrained to the workspace by clipping values outside the range. We calculate the median (to be robust) of these runs as well as the standard deviation across runs for plotting.</p>
<p>The model simulations also allowed us to simulate the two clamp phases and we calculated the average standard deviation of actual reaches in each clamp phase across simulations.</p>
</sec>
<sec id="s4j5">
<title>Goodness of fit</title>
<p>While the BIC analysis with the other model variants provides a relative goodness of fit, it is not straightforward to provide an absolute goodness of fit such as standard <italic>R</italic><sup>2</sup>. There are two problems. First, there is no single model output. Each time the model is simulated with the fit parameters it produces a different output (due to motor noise, exploration variability, and reward stochasticity). Second, the model is not meant to reproduce the actual motor noise, exploration variability, and reward stochasticity of a trial. For example, the model could fit pure Gaussian motor noise across trials (for a poor learner) by accurately fitting the standard deviation of motor noise, but would not be expected to actually match each data point and would therefore have a traditional <italic>R</italic><sup>2</sup> of 0.</p>
<p>To provide an overall goodness of fit, we have to reduce the noise component and to do so we examined the traditional <italic>R</italic><sup>2</sup> between the average of all the children’s data and the average simulation of the model (repeated 1000 times per participant) so as to reduce the stochastic variation.</p>
</sec>
<sec id="s4j6">
<title>Optimal exploration variability as function of motor noise</title>
<p>We used model simulations over a 50 × 50 grid of exploration variability and motor noise to assess the expected learning distance for the 4 tasks. To do this, for each grid location we simulated each child participant 4000 times (as above, selecting the initial state stochastically for each child) and calculated the expected learning distance over the 100 learning trials across the children. This showed the optimal exploration variability increasing as a function of decreasing motor noise (as also observed with age in the data). For an intuition and proof of this relation see Appendix 1.</p>
</sec>
<sec id="s4j7">
<title>Alternative value-based model</title>
<p>We considered a value based model which tries to learn the value of each location. This model is based on the model by <xref ref-type="bibr" rid="c10">Giron et al. (2023)</xref>. In this model, the participant represents the value of each location on each trial <italic>v</italic><sub><italic>t</italic></sub>(<italic>x</italic>). Given the reach locations and rewards received up until trials T, {<italic>x</italic><sub>1</sub>, <italic>r</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, <italic>r</italic><sub>2</sub> · <italic>x</italic><sub><italic>T</italic></sub>, <italic>r</italic><sub><italic>T</italic></sub>} the value function is fit using a Gaussian process with squared exponential kernel and logistic likelihood to give <italic>v</italic><sub><italic>T</italic></sub> (<italic>x</italic>). The kernel has two parameters, a length scale σ<sub><italic>l</italic></sub> which determines the generalization of learning to other locations and a strength σ<sub><italic>s</italic></sub> that controls the amount of learning (similar to a learning rate). The value function is passed through the softmax function with parameter <italic>b</italic> that controls the exploration
<disp-formula id="eqn14">
<graphic xlink:href="602665v2_eqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>P</italic><sub><italic>t</italic></sub>(<italic>x</italic>) is normalized so as to be the probability of moving to locations <italic>x</italic>.</p>
<p>To fit the models three parameters {σ<sub><italic>l</italic></sub>, σ<sub><italic>s</italic></sub>, <italic>b</italic>} we discretized <italic>x</italic> into 500 points and used fmincon to maximize the likelihood of the data. We found these optimized parameters were very sensitive to the initial parameter, so we chose 50 random initializations for each participant and selected the one with the highest likelihood. Although we were able to fit the data, unlike the noise-based model, often the parameters were not easily identifiable. For example, there are many ways a participant could fail to learn such as a large σ<sub><italic>l</italic></sub> or small σ<sub><italic>s</italic></sub>. Since this was not the preferred model, we chose not to examine the parameters for the fits.</p>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="s5" sec-type="data-availability">
<title>Data availability</title>
<p>Data are available upon reasonable request to the corresponding author.</p>
</sec>
<ack>
<title>Acknowledgments</title>
<p>This work was supported by the following funding sources: grants from the National Institutes of Health T32 HD007414 and R35 NS122266 to AJB. DMW is a consultant to CTRL-Labs Inc., in the Reality Labs Division of Meta. This entity did not support or influence this work. NMH gained skills in open science and reproducibility as a learner in the 2024-2025 cohort of Reproducible Rehabilitation funded by NIH NICDH/NCMRR R25 HD105583.</p>
</ack>
<ref-list>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Asselen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kessels</surname> <given-names>RPC</given-names></string-name>, <string-name><surname>Neggers</surname> <given-names>SFW</given-names></string-name>, <string-name><surname>Kappelle</surname> <given-names>LJ</given-names></string-name>, <string-name><surname>Frijns</surname> <given-names>CJM</given-names></string-name>, <string-name><surname>Postma</surname> <given-names>A.</given-names></string-name></person-group> <article-title>Brain areas involved in spatial working memory</article-title>. <source>Neuropsychologia</source>. <year>2006</year>; <volume>44</volume>(<issue>7</issue>):<fpage>1185</fpage>–<lpage>1194</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.10.005</pub-id>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Beers</surname> <given-names>RJ</given-names></string-name></person-group>. <article-title>Motor learning is optimally tuned to the properties of motor noise</article-title>. <source>Neuron</source>. <year>2009</year>; <volume>63</volume>(<issue>3</issue>):<fpage>406</fpage>–<lpage>417</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2009.06.025</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cashaback</surname> <given-names>JGA</given-names></string-name>, <string-name><surname>Lao</surname> <given-names>CK</given-names></string-name>, <string-name><surname>Palidis</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Coltman</surname> <given-names>SK</given-names></string-name>, <string-name><surname>McGregor</surname> <given-names>HR</given-names></string-name>, <string-name><surname>Gribble</surname> <given-names>PL</given-names></string-name></person-group>. <article-title>The gradient of the reinforcement landscape influences sensorimotor learning</article-title>. <source>PLOS Computational Biology</source>. <year>2019</year>; <volume>15</volume>(<issue>3</issue>):<fpage>e1006839</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006839</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname> <given-names>X</given-names></string-name>, <string-name><surname>Mohr</surname> <given-names>K</given-names></string-name>, <string-name><surname>Galea</surname> <given-names>JM</given-names></string-name></person-group>. <article-title>Predicting explorative motor learning using decision-making and motor noise</article-title>. <source>PLOS Computational Biology</source>. <year>2017</year>; <volume>13</volume>(<issue>4</issue>):<fpage>e1005503</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005503</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname> <given-names>AO</given-names></string-name>, <string-name><surname>Nussenbaum</surname> <given-names>K</given-names></string-name>, <string-name><surname>Dorfman</surname> <given-names>HM</given-names></string-name>, <string-name><surname>Gershman</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Hartley</surname> <given-names>CA</given-names></string-name></person-group>. <article-title>The rational use of causal inference to guide reinforcement learning strengthens with age</article-title>. <source>npj Science of Learning</source>. <year>2020</year>; <volume>5</volume>(<issue>1</issue>):<fpage>16</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41539-020-00075-3</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Decker</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Otto</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name><surname>Hartley</surname> <given-names>CA</given-names></string-name></person-group>. <article-title>From Creatures of Habit to Goal-Directed Learners: Tracking the Devel-opmental Emergence of Model-Based Reinforcement Learning</article-title>. <source>Psychological Science</source>. <year>2016</year>; <volume>27</volume>(<issue>6</issue>):<fpage>848</fpage>–<lpage>858</lpage>. doi: <pub-id pub-id-type="doi">10.1177/0956797616639301</pub-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deutsch</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Newell</surname> <given-names>KM</given-names></string-name></person-group>. <article-title>Noise, variability, and the development of children’s perceptual-motor skills</article-title>. <source>Developmental Review</source>. <year>2005</year>; <volume>25</volume>(<issue>2</issue>):<fpage>155</fpage>–<lpage>180</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.dr.2004.09.001</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dhawale</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>MA</given-names></string-name>, Ö<string-name><surname>lveczky</surname> <given-names>BP</given-names></string-name></person-group>. <article-title>The Role of Variability in Motor Learning</article-title>. <source>Annual Review of Neuroscience</source>. <year>2017</year>; <volume>40</volume>:<fpage>479</fpage>–<lpage>498</lpage>. doi: <pub-id pub-id-type="doi">10.1146/annurev-neuro-072116-031548</pub-id>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eppinger</surname> <given-names>B</given-names></string-name>, <string-name><surname>Mock</surname> <given-names>B</given-names></string-name>, <string-name><surname>Kray</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Developmental differences in learning and error processing: evidence from ERPs</article-title>. <source>Psychophysiology</source>. <year>2009</year>; <volume>46</volume>(<issue>5</issue>):<fpage>1043</fpage>–<lpage>1053</lpage>. doi: <pub-id pub-id-type="doi">10.1111/j.1469-8986.2009.00838.x</pub-id>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giron</surname> <given-names>AP</given-names></string-name>, <string-name><surname>Ciranka</surname> <given-names>S</given-names></string-name>, <string-name><surname>Schulz</surname> <given-names>E</given-names></string-name>, <string-name><surname>van den Bos</surname> <given-names>W</given-names></string-name>, <string-name><surname>Ruggeri</surname> <given-names>A</given-names></string-name>, <string-name><surname>Meder</surname> <given-names>B</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>CM</given-names></string-name></person-group>. <article-title>Developmental changes in exploration resemble stochastic optimization</article-title>. <source>Nature Human Behaviour</source>. <year>2023</year>; <volume>7</volume>(<issue>11</issue>):<fpage>1955</fpage>–<lpage>1967</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41562-023-01662-1</pub-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harris</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Wolpert</surname> <given-names>DM</given-names></string-name></person-group>. <article-title>Signal-dependent noise determines motor planning</article-title>. <source>Nature</source>. <year>1998</year>; <volume>394</volume>(<issue>6695</issue>):<fpage>780</fpage>–<lpage>784</lpage>. doi: <pub-id pub-id-type="doi">10.1038/29528</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hitch</surname> <given-names>GJ</given-names></string-name>, <string-name><surname>Halliday</surname> <given-names>S</given-names></string-name>, <string-name><surname>Schaafstal</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Schraagen</surname> <given-names>JMC.</given-names></string-name></person-group> <article-title>Visual working memory in young children</article-title>. <source>Memory &amp; Cognition</source>. <year>1988</year>; <volume>16</volume>(<issue>2</issue>):<fpage>120</fpage>–<lpage>132</lpage>. doi: <pub-id pub-id-type="doi">10.3758/BF03213479</pub-id>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holland</surname> <given-names>P</given-names></string-name>, <string-name><surname>Codol</surname> <given-names>O</given-names></string-name>, <string-name><surname>Galea</surname> <given-names>JM</given-names></string-name></person-group>. <article-title>Contribution of explicit processes to reinforcement-based motor learning</article-title>. <source>Journal of Neurophysiology</source>. <year>2018</year>; <volume>119</volume>(<issue>6</issue>):<fpage>2241</fpage>–<lpage>2255</lpage>. doi: <pub-id pub-id-type="doi">10.1152/jn.00901.2017</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Konrad</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Lohse</surname> <given-names>KR</given-names></string-name>, <string-name><surname>Marrus</surname> <given-names>N</given-names></string-name>, <string-name><surname>Lang</surname> <given-names>CE</given-names></string-name></person-group>. <article-title>Trial-to-trial motor behavior during a reinforcement learning task in children ages 6 to 12</article-title>. <source>Human Movement Science</source>. <year>2025</year>; <volume>99</volume>:<fpage>103317</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.humov.2024.103317</pub-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van der Kooij</surname> <given-names>K</given-names></string-name>, <string-name><surname>Overvliet</surname> <given-names>KE</given-names></string-name></person-group>. <article-title>Rewarding imperfect motor performance reduces adaptive changes</article-title>. <source>Experimental Brain Research</source>. <year>2016</year>; <volume>234</volume>(<issue>6</issue>):<fpage>1441</fpage>–<lpage>1450</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s00221-015-4540-1</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Laerhoven</surname> <given-names>H</given-names></string-name>, <string-name><surname>van der Zaag-Loonen</surname> <given-names>H</given-names></string-name>, <string-name><surname>Derkx</surname> <given-names>B.</given-names></string-name></person-group> <article-title>A comparison of Likert scale and visual analogue scales as response options in children’s questionnaires</article-title>. <source>Acta Paediatrica</source>. <year>2004</year>; <volume>93</volume>(<issue>6</issue>):<fpage>830</fpage>–<lpage>835</lpage>. doi: <pub-id pub-id-type="doi">10.1111/j.1651-2227.2004.tb03026.x</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Levenstein</surname> <given-names>D</given-names></string-name>, <string-name><surname>Alvarez</surname> <given-names>VA</given-names></string-name>, <string-name><surname>Amarasingham</surname> <given-names>A</given-names></string-name>, <string-name><surname>Azab</surname> <given-names>H</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>ZS</given-names></string-name>, <string-name><surname>Gerkin</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Hasenstaub</surname> <given-names>A</given-names></string-name>, <string-name><surname>Iyer</surname> <given-names>R</given-names></string-name>, <string-name><surname>Jolivet</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Marzen</surname> <given-names>S</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>On the role of theory and modeling in neuroscience</article-title>. <source>Journal of Neuroscience</source>. <year>2023</year>; <volume>43</volume>(<issue>7</issue>):<fpage>1074</fpage>–<lpage>1088</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1179-22.2022</pub-id>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liquin</surname> <given-names>EG</given-names></string-name>, <string-name><surname>Gopnik</surname> <given-names>A.</given-names></string-name></person-group> <article-title>Children are more exploratory and learn more than adults in an approach-avoid task</article-title>. <source>Cognition</source>. <year>2022</year>; <volume>218</volume>:<fpage>104940</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cognition.2021.104940</pub-id>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mai</surname> <given-names>X</given-names></string-name>, <string-name><surname>Tardif</surname> <given-names>T</given-names></string-name>, <string-name><surname>Doan</surname> <given-names>SN</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>C</given-names></string-name>, <string-name><surname>Gehring</surname> <given-names>WJ</given-names></string-name>, <string-name><surname>Luo</surname> <given-names>YJ</given-names></string-name></person-group>. <article-title>Brain Activity Elicited by Positive and Negative Feedback in Preschool-Aged Children</article-title>. <source>PLoS ONE</source>. <year>2011</year>; <volume>6</volume>(<issue>4</issue>):<fpage>e18774</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0018774</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Malone</surname> <given-names>LA</given-names></string-name>, <string-name><surname>Hill</surname> <given-names>NM</given-names></string-name>, <string-name><surname>Tripp</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wolpert</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Bastian</surname> <given-names>AJ</given-names></string-name></person-group>. <article-title>A novel video game for remote studies of motor adaptation in children</article-title>. <source>Physiological Reports</source>. <year>2023</year>; <volume>11</volume>(<issue>13</issue>):<fpage>e15764</fpage>. doi: <pub-id pub-id-type="doi">10.14814/phy2.15764</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Master</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Eckstein</surname> <given-names>MK</given-names></string-name>, <string-name><surname>Gotlieb</surname> <given-names>N</given-names></string-name>, <string-name><surname>Dahl</surname> <given-names>R</given-names></string-name>, <string-name><surname>Wilbrecht</surname> <given-names>L</given-names></string-name>, <string-name><surname>Collins</surname> <given-names>AGE</given-names></string-name></person-group>. <article-title>Disentangling the systems contributing to changes in learning during adolescence</article-title>. <source>Developmental Cognitive Neuroscience</source>. <year>2020</year>; <volume>41</volume>:<fpage>100732</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.dcn.2019.100732</pub-id>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Mastrigt</surname> <given-names>NM</given-names></string-name>, <string-name><surname>Tsay</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>T</given-names></string-name>, <string-name><surname>Avraham</surname> <given-names>G</given-names></string-name>, <string-name><surname>Abram</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>van der Kooij</surname> <given-names>K</given-names></string-name>, <string-name><surname>Smeets</surname> <given-names>JBJ</given-names></string-name>, <string-name><surname>Ivry</surname> <given-names>RB</given-names></string-name></person-group>. <article-title>Implicit reward-based motor learning</article-title>. <source>Experimental Brain Research</source>. <year>2023</year>; <volume>241</volume>(<issue>9</issue>):<fpage>2287</fpage>–<lpage>2298</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s00221-023-06683-w</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mayor-Dubois</surname> <given-names>C</given-names></string-name>, <string-name><surname>Zesiger</surname> <given-names>P</given-names></string-name>, <string-name><surname>Van der Linden</surname> <given-names>M</given-names></string-name>, <string-name><surname>Roulet-Perez</surname> <given-names>E.</given-names></string-name></person-group> <article-title>Procedural learning: A developmental study of motor sequence learning and probabilistic classification learning in school-aged children</article-title>. <source>Child Neuropsy-chology</source>. <year>2016</year>; <volume>22</volume>(<issue>6</issue>):<fpage>718</fpage>–<lpage>734</lpage>. doi: <pub-id pub-id-type="doi">10.1080/09297049.2015.1058347</pub-id>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDougle</surname> <given-names>SD</given-names></string-name>, <string-name><surname>Ivry</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Taylor</surname> <given-names>JA</given-names></string-name></person-group>. <article-title>Taking Aim at the Cognitive Side of Learning in Sensorimotor Adaptation Tasks</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2016</year>; <volume>20</volume>(<issue>7</issue>):<fpage>535</fpage>–<lpage>544</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tics.2016.05.002</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meder</surname> <given-names>B</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Schulz</surname> <given-names>E</given-names></string-name>, <string-name><surname>Ruggeri</surname> <given-names>A.</given-names></string-name></person-group> <article-title>Development of directed and random exploration in children</article-title>. <source>Developmental Science</source>. <year>2021</year>; <volume>24</volume>(<issue>4</issue>):<fpage>e13095</fpage>. doi: <pub-id pub-id-type="doi">10.1111/desc.13095</pub-id>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Meel</surname> <given-names>CS</given-names></string-name>, <string-name><surname>Oosterlaan</surname> <given-names>J</given-names></string-name>, <string-name><surname>Heslenfeld</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Sergeant</surname> <given-names>JA</given-names></string-name></person-group>. <article-title>Telling good from bad news: ADHD differentially affects processing of positive and negative feedback during guessing</article-title>. <source>Neuropsychologia</source>. <year>2005</year>; <volume>43</volume>(<issue>13</issue>):<fpage>1946</fpage>–<lpage>1954</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.03.018</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Milne</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Shepard</surname> <given-names>EH</given-names></string-name></person-group>. <source>The House at Pooh Corner</source>. <publisher-name>E.P. Dutton &amp; Company</publisher-name>; <year>1928</year>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nelson</surname> <given-names>CA</given-names></string-name>, <string-name><surname>Monk</surname> <given-names>CS</given-names></string-name>, <string-name><surname>Lin</surname> <given-names>J</given-names></string-name>, <string-name><surname>Carver</surname> <given-names>LJ</given-names></string-name>, <string-name><surname>Thomas</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Truwit</surname> <given-names>CL</given-names></string-name></person-group>. <article-title>Functional neuroanatomy of spatial working memory in children</article-title>. <source>Developmental Psychology</source>. <year>2000</year>; <volume>36</volume>(<issue>1</issue>):<fpage>109</fpage>–<lpage>116</lpage>. doi: <pub-id pub-id-type="doi">10.1037/0012-1649.36.1.109</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nussenbaum</surname> <given-names>K</given-names></string-name>, <string-name><surname>Hartley</surname> <given-names>CA</given-names></string-name></person-group>. <article-title>Reinforcement learning across development: What insights can we draw from a decade of research?</article-title> <source>Developmental Cognitive Neuroscience</source>. <year>2019</year>; <volume>40</volume>:<fpage>100733</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.dcn.2019.100733</pub-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pickering</surname> <given-names>SJ</given-names></string-name></person-group>. <article-title>The development of visuo-spatial working memory</article-title>. <source>Memory</source>. <year>2001</year>; <volume>9</volume>(<issue>4</issue>):<fpage>423</fpage>–<lpage>432</lpage>. doi: <pub-id pub-id-type="doi">10.1080/09658210143000182</pub-id>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Plate</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Fulvio</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Shutts</surname> <given-names>K</given-names></string-name>, <string-name><surname>Green</surname> <given-names>CS</given-names></string-name>, <string-name><surname>Pollak</surname> <given-names>SD</given-names></string-name></person-group>. <article-title>Probability Learning: Changes in Behavior Across Time and Development</article-title>. <source>Child Development</source>. <year>2018</year>; <volume>89</volume>(<issue>1</issue>):<fpage>205</fpage>–<lpage>218</lpage>. doi: <pub-id pub-id-type="doi">10.1111/cdev.12718</pub-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raznahan</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shaw</surname> <given-names>PW</given-names></string-name>, <string-name><surname>Lerch</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Clasen</surname> <given-names>LS</given-names></string-name>, <string-name><surname>Greenstein</surname> <given-names>D</given-names></string-name>, <string-name><surname>Berman</surname> <given-names>R</given-names></string-name>, <string-name><surname>Pipitone</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chakravarty</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Giedd</surname> <given-names>JN</given-names></string-name></person-group>. <article-title>Longitudinal four-dimensional mapping of subcortical anatomy in human development</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2014</year>; <volume>111</volume>(<issue>4</issue>):<fpage>1592</fpage>–<lpage>1597</lpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1316911111</pub-id>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roth</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Calalo</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Lokesh</surname> <given-names>R</given-names></string-name>, <string-name><surname>Sullivan</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Grill</surname> <given-names>S</given-names></string-name>, <string-name><surname>Jeka</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>van der Kooij</surname> <given-names>K</given-names></string-name>, <string-name><surname>Carter</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Cashaback</surname> <given-names>JGA</given-names></string-name></person-group>. <article-title>Reinforcement-based processes actively regulate motor exploration along redundant solution manifolds</article-title>. <source>Proceedings Biological Sciences</source>. <year>2023</year>; <volume>290</volume>(<issue>2009</issue>):<fpage>20231475</fpage>. doi: <pub-id pub-id-type="doi">10.1098/rspb.2023.1475</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rovee</surname> <given-names>CK</given-names></string-name>, <string-name><surname>Rovee</surname> <given-names>DT</given-names></string-name></person-group>. <article-title>Conjugate reinforcement of infant exploratory behavior</article-title>. <source>Journal of Experimental Child Psychology</source>. <year>1969</year>; <volume>8</volume>(<issue>1</issue>):<fpage>33</fpage>–<lpage>39</lpage>. doi: <pub-id pub-id-type="doi">10.1016/0022-0965(69)90025-3</pub-id>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sargent</surname> <given-names>B</given-names></string-name>, <string-name><surname>Schweighofer</surname> <given-names>N</given-names></string-name>, <string-name><surname>Kubo</surname> <given-names>M</given-names></string-name>, <string-name><surname>Fetters</surname> <given-names>L.</given-names></string-name></person-group> <article-title>Infant Exploratory Learning: Influence on Leg Joint Coordination</article-title>. <source>PLoS ONE</source>. <year>2014</year>; <volume>9</volume>(<issue>3</issue>):<fpage>e91500</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0091500</pub-id>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schultz</surname> <given-names>W.</given-names></string-name></person-group> <article-title>Predictive Reward Signal of Dopamine Neurons</article-title>. <source>Journal of Neurophysiology</source>. <year>1998</year>; <volume>80</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>27</lpage>. doi: <pub-id pub-id-type="doi">10.1152/jn.1998.80.1.1</pub-id>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schulz</surname> <given-names>E</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Ruggeri</surname> <given-names>A</given-names></string-name>, <string-name><surname>Meder</surname> <given-names>B.</given-names></string-name></person-group> <article-title>Searching for Rewards Like a Child Means Less Generalization and More Directed Exploration</article-title>. <source>Psychological Science</source>. <year>2019</year>; <volume>30</volume>(<issue>11</issue>):<fpage>1561</fpage>–<lpage>1572</lpage>. doi: <pub-id pub-id-type="doi">10.1177/0956797619863663</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scott</surname> <given-names>K</given-names></string-name>, <string-name><surname>Schulz</surname> <given-names>L.</given-names></string-name></person-group> <article-title>Lookit (Part 1): A New Online Platform for Developmental Research</article-title>. <source>Open Mind</source>. <year>2017</year>; <volume>1</volume>(<issue>1</issue>):<fpage>4</fpage>–<lpage>14</lpage>. doi: <pub-id pub-id-type="doi">10.1162/OPMI_a_00002</pub-id>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sheskin</surname> <given-names>M</given-names></string-name>, <string-name><surname>Scott</surname> <given-names>K</given-names></string-name>, <string-name><surname>Mills</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Bergelson</surname> <given-names>E</given-names></string-name>, <string-name><surname>Bonawitz</surname> <given-names>E</given-names></string-name>, <string-name><surname>Spelke</surname> <given-names>ES</given-names></string-name>, <string-name><surname>Fei-Fei</surname> <given-names>L</given-names></string-name>, <string-name><surname>Keil</surname> <given-names>FC</given-names></string-name>, <string-name><surname>Gweon</surname> <given-names>H</given-names></string-name>, <string-name><surname>Tenenbaum</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Jara-Ettinger</surname> <given-names>J</given-names></string-name>, <string-name><surname>Adolph</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Rhodes</surname> <given-names>M</given-names></string-name>, <string-name><surname>Frank</surname> <given-names>MC</given-names></string-name>, <string-name><surname>Mehr</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Schulz</surname> <given-names>L.</given-names></string-name></person-group> <article-title>Online Developmental Science to Foster Innovation, Access, and Impact</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2020</year>; <volume>24</volume>(<issue>9</issue>):<fpage>675</fpage>–<lpage>678</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tics.2020.06.004</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shields</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Palermo</surname> <given-names>TM</given-names></string-name>, <string-name><surname>Powers</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Grewe</surname> <given-names>SD</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>GA</given-names></string-name></person-group>. <article-title>Predictors of a child’s ability to use a visual analogue scale</article-title>. <source>Child: Care, Health and Development</source>. <year>2003</year>; <volume>29</volume>(<issue>4</issue>):<fpage>281</fpage>–<lpage>290</lpage>. doi: <pub-id pub-id-type="doi">10.1046/j.1365-2214.2003.00343.x</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sobel</surname> <given-names>D</given-names></string-name>, <string-name><surname>Sommerville</surname> <given-names>J.</given-names></string-name></person-group> <article-title>The Importance of Discovery in Children’s Causal Learning from Interventions</article-title>. <source>Frontiers in Psychology</source>. <year>2010</year>; <volume>1</volume>. doi: <pub-id pub-id-type="doi">10.3389/fpsyg.2010.00176</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Minto de Sousa</surname> <given-names>N</given-names></string-name>, <string-name><given-names>Gil</given-names> <surname>MSCdA</surname></string-name>, <string-name><surname>McIlvane</surname> <given-names>WJ</given-names></string-name></person-group>. <article-title>Discrimination and Reversal Learning by Toddlers Aged 15–23 Months</article-title>. <source>The Psychological Record</source>. <year>2015</year>; <volume>65</volume>(<issue>1</issue>):<fpage>41</fpage>–<lpage>47</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s40732-014-0084-1</pub-id>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stevenson</surname> <given-names>HW</given-names></string-name>, <string-name><surname>Weir</surname> <given-names>MW</given-names></string-name></person-group>. <article-title>Variables affecting children’s performance in a probability learning task</article-title>. <source>Journal of experimental psychology</source>. <year>1959</year>; <volume>57</volume>(<issue>6</issue>):<fpage>403</fpage>–<lpage>412</lpage>. doi: <pub-id pub-id-type="doi">10.1037/h0039258</pub-id>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sutton</surname> <given-names>RS</given-names></string-name>, <string-name><surname>Barto</surname> <given-names>AG</given-names></string-name></person-group>. <source>Reinforcement learning: an introduction</source>. <publisher-name>The MIT Press</publisher-name>; <year>2018</year>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Takahashi</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Nemet</surname> <given-names>D</given-names></string-name>, <string-name><surname>Rose-Gottron</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Larson</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Cooper</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Reinkensmeyer</surname> <given-names>DJ</given-names></string-name></person-group>. <article-title>Neuromotor Noise Limits Motor Performance, But Not Motor Adaptation, in Children</article-title>. <source>Journal of Neurophysiology</source>. <year>2003</year>; <volume>90</volume>(<issue>2</issue>):<fpage>703</fpage>–<lpage>711</lpage>. doi: <pub-id pub-id-type="doi">10.1152/jn.01173.2002</pub-id>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Therrien</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Wolpert</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Bastian</surname> <given-names>AJ</given-names></string-name></person-group>. <article-title>Effective reinforcement learning following cerebellar damage requires a balance between exploration and motor noise</article-title>. <source>Brain</source>. <year>2016</year>; <volume>139</volume>(<issue>1</issue>):<fpage>101</fpage>–<lpage>114</lpage>. doi: <pub-id pub-id-type="doi">10.1093/brain/awv329</pub-id>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Therrien</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Wolpert</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Bastian</surname> <given-names>AJ</given-names></string-name></person-group>. <article-title>Increasing Motor Noise Impairs Reinforcement Learning in Healthy Individuals</article-title>. <source>eNeuro</source>. <year>2018</year>; <volume>5</volume>(<issue>3</issue>):<elocation-id>ENEURO.0050–18.2018</elocation-id>. doi: <pub-id pub-id-type="doi">10.1523/ENEURO.0050-18.2018</pub-id>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Todorov</surname> <given-names>E</given-names></string-name>, <string-name><surname>Jordan</surname> <given-names>MI</given-names></string-name></person-group>. <article-title>Optimal feedback control as a theory of motor coordination</article-title>. <source>Nat Neurosci</source>. <year>2002</year>; <volume>5</volume>(<issue>11</issue>):<fpage>1226</fpage>–<lpage>1235</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nn963</pub-id>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Trommershauser</surname> <given-names>J</given-names></string-name>, <string-name><surname>Maloney</surname> <given-names>LT</given-names></string-name>, <string-name><surname>Landy</surname> <given-names>MS</given-names></string-name></person-group>. <article-title>Statistical decision theory and the selection of rapid, goal-directed movements</article-title>. <source>J Opt Soc Am A Opt Image Sci Vis</source>. <year>2003</year>; <volume>20</volume>(<issue>7</issue>):<fpage>1419</fpage>–<lpage>1433</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsujii</surname> <given-names>T</given-names></string-name>, <string-name><surname>Yamamoto</surname> <given-names>E</given-names></string-name>, <string-name><surname>Masuda</surname> <given-names>S</given-names></string-name>, <string-name><surname>Watanabe</surname> <given-names>S.</given-names></string-name></person-group> <article-title>Longitudinal study of spatial working memory development in young children</article-title>. <source>NeuroReport</source>. <year>2009</year>; <volume>20</volume>(<issue>8</issue>):<fpage>759</fpage>–<lpage>763</lpage>. doi: <pub-id pub-id-type="doi">10.1097/WNR.0b013e32832aa975</pub-id>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Mastrigt</surname> <given-names>NM</given-names></string-name>, <string-name><surname>Smeets</surname> <given-names>JBJ</given-names></string-name>, <string-name><surname>Van Der Kooij</surname> <given-names>K.</given-names></string-name></person-group> <article-title>Quantifying exploration in reward-based motor learning</article-title>. <source>PLOS One</source>. <year>2020</year>; <volume>15</volume>(<issue>4</issue>):<fpage>e0226789</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0226789</pub-id>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xia</surname> <given-names>L</given-names></string-name>, <string-name><surname>Master</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Eckstein</surname> <given-names>MK</given-names></string-name>, <string-name><surname>Baribault</surname> <given-names>B</given-names></string-name>, <string-name><surname>Dahl</surname> <given-names>RE</given-names></string-name>, <string-name><surname>Wilbrecht</surname> <given-names>L</given-names></string-name>, <string-name><surname>Collins</surname> <given-names>AGE</given-names></string-name></person-group>. <article-title>Modeling changes in probabilistic reinforcement learning during adolescence</article-title>. <source>PLOS Computational Biology</source>. <year>2021</year>; <volume>17</volume>(<issue>7</issue>):<fpage>e1008524</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1008524</pub-id>.</mixed-citation></ref>
</ref-list>
<app-group>
<app id="s6a">
<title>Appendix 1: Optimal exploration variability as function of motor noise</title>
<p>Simulations (<xref rid="figS7a" ref-type="fig">Figure 7—figure Supplement 2</xref>) showed that the optimal exploration variability increases as a function of decreasing motor noise (as also observed with age in the data). Here we provide an intuition and proof for this inverse relation in a simplified setting. The setting is one in which there is a discrete point target offset at +<italic>d</italic> from the desired reach (which without loss of generality we take to be at zero, <italic>x</italic><sub><italic>t</italic></sub> = 0; <xref rid="fig8" ref-type="fig">Fig. 8</xref> left panel, red line).</p>
<p>As in the preferred model, we have two independent zero-mean Gaussian noise sources for exploration variability and motor noise:
<disp-formula id="ueqn1">
<graphic xlink:href="602665v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We will examine how the expected reward depends on motor and exploration variability for the possible scenarios underlying a reach, that is (i) a reach after an unsuccessful trial and (ii) a reach after a successful trial that had exploration (i.e. the desired reach is updated). There is a third possibility which is a reach after a successful trial that had no exploration (i.e. the previous trial was successful). However, the reaches in such a scenario do not involve exploration variability (only motor noise) so is not germane to the interplay of motor and exploration variability.</p>
<sec id="s6a1">
<title>Reach after an unsuccessful trial</title>
<p>After an unsuccessful reach there is no updating of desired reach and the next reach includes both exploration variability and motor noise, (<italic>s</italic><sub><italic>t</italic></sub> = <italic>e</italic><sub><italic>t</italic></sub> + <italic>m</italic><sub><italic>t</italic></sub>). Therefore, the distribution of reach locations is given by <inline-formula><inline-graphic xlink:href="602665v2_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (<xref rid="fig8" ref-type="fig">Fig. 8</xref> left panel, blue line). The probability of hitting the target (success, <italic>r</italic><sub><italic>t</italic></sub> = 1) is given by
<disp-formula id="ueqn2">
<graphic xlink:href="602665v2_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In general, to maximize the density at <italic>d</italic> away from the mean of a Gaussian distribution (<xref rid="fig8" ref-type="fig">Fig. 8</xref> left panel, green line shows target location), the standard deviation should be <italic>d</italic>. Therefore, to maximize reward <inline-formula><inline-graphic xlink:href="602665v2_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.Rearranging gives <inline-formula><inline-graphic xlink:href="602665v2_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and hence an inverse relation between optimal exploration variability and motor noise, provided |<italic>d</italic>| &gt; σ<sub><italic>m</italic></sub> (if |<italic>d</italic>| ≤ σ<sub><italic>m</italic></sub> it is better to have no exploration). This is the fundamental reason that exploration variability should decrease as motor noise increases, so as to maximize reward.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><title>Distribution of desired reach and actual reach for a reach after an unsuccessful trial (left) and after a successful trial (right).</title>
<p>Plots show probability density across endpoint positions. In both cases the desired reach on trial <italic>t</italic> is at <italic>x</italic><sub><italic>t</italic></sub> = 0 (thin red line shows distribution as a delta function). After an unsuccessful reach (left panel) the actual reach (blue distribution) includes exploration variability and motor noise. The probability of reward depends on the height of this distribution at the target (<italic>d</italic> = 10, shown by green line). For a successful trial with exploration (right panel) the distribution (<italic>x</italic><sub><italic>t</italic></sub> and <italic>s</italic><sub><italic>t</italic></sub>, thin lines) are the same as for the left panel. However, the desired reach is updated by exploration that led to success which gives the distribution of the next desired reach (<italic>x</italic><sub><italic>t</italic>+1</sub>, thick red line) and the next actual reach is this distribution with the addition of motor noise (thick blue line). For this illustration, both motor and exploration standard deviations were set to 8 and the target was set +10 (green lines).</p></caption>
<graphic xlink:href="602665v2_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s6a2">
<title>Reach after a successful trial</title>
<p>The situation is more complicated if we consider what happens on the reach after a successful trial which had exploration. We need to consider a) the update from the successful trial and b) the probability of reward for the next trial.</p>
<p>a) If the reach <italic>s</italic><sub><italic>t</italic></sub> = <italic>e</italic><sub><italic>t</italic></sub> + <italic>m</italic><sub><italic>t</italic></sub> (<xref rid="fig8" ref-type="fig">Fig. 8</xref> right panel, blue thin line) led to success (<italic>e</italic><sub><italic>t</italic></sub> + <italic>m</italic><sub><italic>t</italic></sub> = <italic>d</italic>) then the desired reach is updated by this exploration variability (<italic>x</italic><sub><italic>t</italic>+1</sub> = <italic>e</italic><sub><italic>t</italic></sub>). We can consider the conditional distribution of <italic>e</italic><sub><italic>t</italic></sub> given it led to success, which is known (from the simple properties of Gaussians) to be:
<disp-formula id="eqn15">
<graphic xlink:href="602665v2_eqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
which is shown in <xref rid="fig8" ref-type="fig">Fig. 8</xref> (right panel, red thick line). <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref> shows that as σ<sub><italic>e</italic></sub> increases, so does the average update.</p>
<p>b) the subsequent reach is only corrupted by motor noise (as the previous reach was successful) giving <italic>s</italic><sub><italic>t</italic>+1</sub> = <italic>e</italic><sub><italic>t</italic></sub> +<italic>m</italic><sub><italic>t</italic>+1</sub>. The reach distribution (<xref rid="fig8" ref-type="fig">Fig. 8</xref> right panel, blue thick line) is therefore the same as <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref> with the addition of motor noise:
<disp-formula id="eqn16">
<graphic xlink:href="602665v2_eqn16.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We can find the exploration variability that maximizes the reward, that is maximizes the density at <italic>d</italic>. To do this we can differentiate the Gaussian distribution with mean and variance given in <xref ref-type="disp-formula" rid="eqn16">Eq. (16)</xref> with respect to σ<sub><italic>e</italic></sub> to find the maximum. With a little algebra this leads to the optimal exploration variability given by:
<disp-formula id="eqn17">
<graphic xlink:href="602665v2_eqn17.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
For an optimal σ<sub><italic>e</italic></sub> to exist this expression must be positive (or the optimal σ<sub><italic>e</italic></sub> is 0) and this can be shown to be true provided <inline-formula><inline-graphic xlink:href="602665v2_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.How the optimal σ<sub><italic>e</italic></sub> changes with σ<sub><italic>m</italic></sub> depends on the sign of the derivative of <xref ref-type="disp-formula" rid="eqn17">Eq. (17)</xref> with respect to motor noise. We need only consider the components of <xref ref-type="disp-formula" rid="eqn17">Eq. (17)</xref> that depend on σ<sub><italic>m</italic></sub>, that is:
<disp-formula id="ueqn3">
<graphic xlink:href="602665v2_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The derivative needs to be less than zero for the inverse relation to be held (i.e. increasing σ<sub><italic>m</italic></sub> reduces the optimal σ<sub><italic>e</italic></sub>). That is:
<disp-formula id="ueqn4">
<graphic xlink:href="602665v2_ueqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
which is true. Therefore the inverse relation persists even if we only consider reaches after a successful trial.</p>
<p>When the distance to the target is small compared to motor noise then exploration actually reduces reward. However, in general for a real experiment with multiple reaches and exploration variability that does not change during learning, the above carries over to the full simulations (<xref rid="figS7a" ref-type="fig">Figure 7—figure Supplement 2</xref>).</p>
</sec>
</app>
<app>
<title>Supplementary information</title>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Supp. Tab. 1.</label>
<caption><title>Statistical analysis of sex, handedness, device, and browser on behavior.</title>
<p>Results from one-way ANOVAs of participant specific factors on precision from experimental block one (baseline) and distance from target from experimental block two (learning) for each of the four tasks. For all tasks, participant specific factors did not significantly affect behavior.</p></caption>
<graphic xlink:href="602665v2_tblS1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure 1—figure supplement 1.</label>
<caption><title>Map of participant locations.</title>
<p>Thirty-eight states of the United States of America are represented in this dataset. The map was generated in Excel on Microsoft 365.</p></caption>
<graphic xlink:href="602665v2_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure 2—figure supplement 1.</label>
<caption><title>Example baseline paths for participants ages 3 to 11 years old.</title>
<p>Each trajectory begins at (0,0) and ends at Y = 24 when the penguin crosses the back edge of the ice. The final X position of each trajectory corresponds to the interpolated final position of the movement (see Methods for additional details). As available, a sample for each age bin from each input device type is provided. Note that trajectories tend to be straighter for touchscreen input compared to other devices. The twenty squares represent the target centers. Note that the full reward zone is not shown due to overlap between targets. Unrewarded and reward paths are shown as dashed and solid lines, respectively.</p></caption>
<graphic xlink:href="602665v2_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2a" position="float" fig-type="figure">
<label>Figure 2—figure supplement 2.</label>
<caption><title>Example baseline paths for participants ages 12 to 17 years old and adults.</title>
<p>Same format as <xref rid="figS2" ref-type="fig">Figure 2—figure Supplement 1</xref></p></caption>
<graphic xlink:href="602665v2_figS2a.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2b" position="float" fig-type="figure">
<label>Figure 2—figure supplement 3.</label>
<caption><title>Path length ratios.</title>
<p>The path length ratio is a measure of path curvature (path length divided by distance from first to last point of movement) for the four tasks. Significant pairwise comparisons between age bins indicated above plots as follow: * = p &lt; 0.05, + = p &lt; 0.01, and Δ = p &lt; 0.001. Bars show mean and standard error of the mean.</p></caption>
<graphic xlink:href="602665v2_figS2b.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2c" position="float" fig-type="figure">
<label>Figure 2—figure supplement 4.</label>
<caption><title>Timing information.</title>
<p>Reaction time (time from when penguin appeared until the participant clicked on the penguin to start the trial), stationary time (time from click to start of movement), movement time (time from start to end of movement) and game time (time to complete the whole task in minutes) for the 4 tasks split by age bins. Significant pairwise comparisons between age bins indicated above plots as follow: * p &lt; 0.05, + p &lt; 0.01, Δ p &lt; 0.001. Bars show mean and standard error of the mean.</p></caption>
<graphic xlink:href="602665v2_figS2c.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Figure 5—figure supplement 1.</label>
<caption><title>Model comparison for the Continuous and Discrete Probabilistic tasks.</title>
<p>Difference in Bayesian Information Criterion (BIC) between the preferred model (Model 11) and the other variants for the Continuous Probabilistic task. The variants depend on the presence (+) or absence (0) of each source of variability (σ<sub><italic>e</italic></sub>, σ<sub><italic>p</italic></sub>σ<sub><italic>m</italic></sub>); whether the learning rates (<italic>η</italic><sub><italic>e</italic></sub>, <italic>η</italic><sub><italic>p</italic></sub>) are absent (.), fit (+), set to unity (1) or <italic>η</italic><sub><italic>p</italic></sub> = <italic>η</italic><sub><italic>e</italic></sub>; and whether <italic>r</italic><sub><italic>p</italic></sub> is absent (.), set to <italic>r</italic><sub><italic>t</italic></sub> or unity (1). Degrees of freedom of each model is shown by d.o.f. The number of participant who are best fit by each model are shown for the continuous (<italic>N</italic><sub><italic>c</italic></sub>), discrete (<italic>N</italic><sub><italic>d</italic></sub>) and combined (<italic>N</italic><sub><italic>c</italic></sub> + <italic>N</italic><sub><italic>d</italic></sub>) tasks. When we restricted model selection to only the Continuous Probabilistic task Model 11 was again preferred with ΔBIC of 155 and 90 for the children alone or all participants. When we restricted model selection to only the Discrete Probabilistic task Model 11 was again preferred with ΔBIC of 6 and 53 for the children alone or all participants.</p></caption>
<graphic xlink:href="602665v2_figS5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5a" position="float" fig-type="figure">
<label>Figure 5—figure supplement 2.</label>
<caption><title>Model parameter recovery.</title>
<p>The recovered vs. true parameters for synthetic data generated by the model and then fit. Correlations are shown above the plots.</p></caption>
<graphic xlink:href="602665v2_figS5a.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5b" position="float" fig-type="figure">
<label>Figure 5—figure supplement 3.</label>
<caption><title>Example fits of the model to the Continuous Probabilistic task.</title>
<p>Fits to the same participants shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref> in the same format at <xref rid="fig5" ref-type="fig">Fig. 5b</xref>.</p></caption>
<graphic xlink:href="602665v2_figS5b.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5c" position="float" fig-type="figure">
<label>Figure 5—figure supplement 4.</label>
<caption><title>Fits to the success and failure clamp phases for the Continuous Probabilistic task.</title>
<p><bold>a</bold>. Success clamp standard deviation as a function of age for the data (blue) and model (red) with regression lines with 95% confidence interval shading. <bold>b</bold>. Same as <bold>a</bold>. for the fail clamp. <bold>c</bold>. model v.s. empirical success clamp s.d. with variance explained and correlation with p-value. <bold>d</bold>. Same as <bold>c</bold>. for the fail clamp.</p></caption>
<graphic xlink:href="602665v2_figS5c.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Figure 6—figure supplement 1.</label>
<caption><title>Example behavior, and discrete target performance in the discrete probabilistic task.</title>
<p>Same format as <xref rid="fig2" ref-type="fig">Fig. 2</xref>.</p></caption>
<graphic xlink:href="602665v2_figS6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS6a" position="float" fig-type="figure">
<label>Figure 6—figure supplement 2.</label>
<caption><title>Variability and learning in the discrete probabilistic task.</title>
<p>Same format as <xref rid="fig4" ref-type="fig">Fig. 4</xref>.</p></caption>
<graphic xlink:href="602665v2_figS6a.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS6b" position="float" fig-type="figure">
<label>Figure 6—figure supplement 3.</label>
<caption><title>Example fits of the model to the Discrete Probabilistic task.</title>
<p>Fits to the same participants shown in <xref rid="figS6" ref-type="fig">Figure 6—figure Supplement 1</xref> in the same format at <xref rid="fig5" ref-type="fig">Fig. 5b</xref>.</p></caption>
<graphic xlink:href="602665v2_figS6b.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS6c" position="float" fig-type="figure">
<label>Figure 6—figure supplement 4.</label>
<caption><title>Fits to the success and fail clamp phases for the Discrete Probabilistic task.</title>
<p>Same format at <xref rid="figS5c" ref-type="fig">Figure 5—figure Supplement 4</xref></p></caption>
<graphic xlink:href="602665v2_figS6c.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS7" position="float" fig-type="figure">
<label>Figure 7—figure supplement 1.</label>
<caption><title>Model fit parameters by age for the deterministic tasks</title>
<p>Panels in same format as <xref rid="fig5" ref-type="fig">Fig. 5c</xref>.</p></caption>
<graphic xlink:href="602665v2_figS7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS7a" position="float" fig-type="figure">
<label>Figure 7—figure supplement 2.</label>
<caption><title>Expected distance to target as a function of model parameters.</title>
<p>Expected distance to target as a function of motor noise and exploration variability for the 4 tasks. The green filled circle shows the optimal parameters to maximize reward. The white line shows the optimal exploration variability for different levels of motor noise. The gray shaded line shows the exploration variability vs. age regression line plotted against the motor noise vs. age regression with shading showing the 95% confidence interval. The gray shading shows the participant age in the regression. As the task goes from continuous to discrete, note how the youngest children (darkest end of bar) increase their exploration variability. As the task goes from probabilistic to deterministic, the youngest children decrease their motor noise and increase their exploration variability.</p></caption>
<graphic xlink:href="602665v2_figS7a.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101036.2.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Dekker</surname>
<given-names>Tessa</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study tests the development of motor reinforcement learning from toddlerhood to adulthood, using a large online sample. They show that learning improves with age in a task that, like real-life movement, involves a continuous range of response options and probabilistic rewards, and link this shift to reduced movement variability and more efficient feedback-based learning through behavioural modeling. Simplifying the task with discrete actions and deterministic outcomes boosted younger children's performance, suggesting early learning is limited by spatial and probabilistic processing. The evidence is <bold>convincing</bold>, although future work may investigate more naturalistic movement.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101036.2.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Here the authors address how reinforcement-based sensorimotor adaptation changes throughout development. To address this question, they collected many participants in ages that ranged from small children (3 years old) to adulthood (18+ years old). The authors used four experiments to manipulate whether binary and positive reinforcement was provided probabilistically (e.g., 30 or 50%) versus deterministically (e.g.,100%), and continuous (infinite possible locations) versus discrete (binned possible locations) when the probability of reinforcement varied along the span of a large redundant target. The authors found that both movement variability and the extent of adaptation changed with age.</p>
<p>Strengths:</p>
<p>The major strength of the paper is the number of participants collected (n = 385). The authors also answer their primary question, that reinforcement-based sensorimotor adaptation changes throughout development, which was shown by utilizing established experimental designs and computational modelling. They have compared an extensive number of potential models, finding the one that best fits the data while penalizing the number of free parameters.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101036.2.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this study, Hill and colleagues use a novel reinforcement-based motor learning task (&quot;RML&quot;), asking how aspects of RML change over the course of development from toddler years through adolescence. Multiple versions of the RML task were used in different samples, which varied on two dimensions: whether the reward probability of a given hand movement direction was deterministic or probabilistic, and whether the solution space had continuous reach targets or discrete reach targets. Using analyses of both raw behavioral data and model fits, the authors report four main results: First, developmental improvements reflected 3 clear changes, including increases in exploration, an increase in the RL learning rate, and a reduction of intrinsic motor noise. Second, changes to the task that made it discrete and/or deterministic both rescued performance in the youngest age groups, suggesting that observed deficits could be linked to continuous/probabilistic learning settings. Overall, the results shed light on how RML changes throughout human development, and the modeling characterizes the specific learning deficits seen in the youngest ages.</p>
<p>Strengths:</p>
<p>(1) This impressive work addresses an understudied subfield of motor control/psychology - the developmental trajectory of motor learning. It is thus timely and will interest many researchers.</p>
<p>(2) The task, analysis, and modeling methods are very strong. The empirical findings are rather clear and compelling, and the analysis approaches are convincing. Thus, at the empirical level, this study has very few weaknesses.</p>
<p>(3) The large sample sizes and in-lab replications further reflect the laudable rigor of the study.</p>
<p>(4) The main and supplemental figures are clear and concise.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101036.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The study investigates the development of reinforcement learning across the lifespan with a large sample of participants recruited for an online game. It finds that children gradually develop their abilities to learn reward probability, possibly hindered by their immature spatial processing and probabilistic reasoning abilities. Motor noise and exploration after a failure all contribute to children's subpar performance.</p>
<p>Strengths:</p>
<p>Experimental manipulations of both the continuity of movement options and the probabilistic nature of the reward function enable the inference of what cognitive factors differ between age groups.</p>
<p>
A large sample of participants is studied.</p>
<p>
The model-based analysis provides further insights into the development of reinforcement learning ability.</p>
<p>Weaknesses:</p>
<p>The conclusion that immature spatial processing and probabilistic reasoning abilities limit reinforcement learning here still needs more direct evidence.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101036.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Hill</surname>
<given-names>Nayo M</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9710-0291</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Tripp</surname>
<given-names>Haley M</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9697-1546</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Wolpert</surname>
<given-names>Daniel M</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2011-2790</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Malone</surname>
<given-names>Laura A</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9836-822X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Bastian</surname>
<given-names>Amy J</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6079-0997</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews</p>
<p>Overview of changes in the revision</p>
<p>We thank the reviewers for the very helpful comments and have extensively revised the paper. We provide point-by-point responses below and here briefly highlight the major changes:</p>
<p>(1) We expanded the discussion of the relevant literature in children and adults.</p>
<p>(2) We improved the contextualization of our experimental design within previous reinforcement studies in both cognitive and motor domains highlighting the interplay between the two.</p>
<p>(3) We reorganized the primary and supplementary results to better communicate the findings of the studies.</p>
<p>(4) The modeling has been significantly revised and extended. We now formally compare 31 noise-based models and one value-based model and this led to a different model from the original being the preferred model. This has to a large extent cleaned up the modeling results. The preferred model is a special case (with no exploration after success) of the model proposed in Therrien et al. (2018). We also provide examples of individual fits of the model, fit all four tasks and show group fits for all, examine fits vs. data for the clamp phases by age, provide measures of relative and absolute goodness of fit, and examine how the optimal level of exploration varies with motor noise.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Summary:</p>
<p>Here the authors address how reinforcement-based sensorimotor adaptation changes throughout development. To address this question, they collected many participants in ages that ranged from small children (3 years old) to adulthood (1 8+ years old). The authors used four experiments to manipulate whether binary and positive reinforcement was provided probabilistically (e.g., 30 or 50%) versus deterministically (e.g., 100%), and continuous (infinite possible locations) versus discrete (binned possible locations) when the probability of reinforcement varied along the span of a large redundant target. The authors found that both movement variability and the extent of adaptation changed with age.</p>
</disp-quote>
<p>Thank you for reviewing our work. One note of clarification. This work focuses on reinforcementbased learning throughout development but does not evaluate sensorimotor adaptation. The four tasks presented in this work are completed with veridical trajectory feedback (no perturbation).</p>
<p>The goal is to understand how children at different ages adjust their movements in response to reward feedback but does not evaluate sensorimotor adaptation. We now explain this distinction on line 35.</p>
<disp-quote content-type="editor-comment">
<p>Strengths:</p>
<p>The major strength of the paper is the number of participants collected (n = 385). The authors also answer their primary question, that reinforcement-based sensorimotor adaptation changes throughout development, which was shown by utilizing established experimental designs and computational modelling.</p>
</disp-quote>
<p>Thank you.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>Potential concerns involve inconsistent findings with secondary analyses, current assumptions that impact both interpr tation and computational modelling, and a lack of clearly stated hypotheses.</p>
<p>(1) Multiple regression and Mediation Analyses.</p>
<p>The challenge with these secondary analyses is that:</p>
<p>(a) The results are inconsistent between Experiments 1 and 2, and the analysis was not performed for Experiments 3 and 4,</p>
<p>(b) The authors used a two-stage procedure of using multiple regression to determine what variables to use for the mediation analysis, and</p>
<p>(c)The authors already have a trial-by-trial model that is arguably more insightful.</p>
<p>Given this, some suggested changes are to:</p>
<p>(a) Perform the mediation analysis with all the possible variables (i.e., not informed by multiple regression) to see if the results are consistent.</p>
<p>(b) Move the regression/mediation analysis to Supplementary, since it is slightly distracting given current inconsistencies and that the trial-by-trial model is arguably more insightful.</p>
</disp-quote>
<p>Based on these comments, we have chosen to remove the multiple regression and mediation analyses. We agree that they were distracting and that the trial-by-trial model allows for differentiation of motor noise from exploration variability in the learning block.</p>
<disp-quote content-type="editor-comment">
<p>(2) Variability for different phases and model assumptions:</p>
<p>A nice feature of the experimental design is the use of success and failure clamps. These clamped phases, along with baseline, are useful because they can provide insights into the partitioning of motor and exploratory noise. Based on the assumptions of the model, the success clamp would only reflect variability due to motor noise (excludes variability due to exploratory noise and any variability due to updates in reach aim). Thus, it is reasonable to expect that the success clamps would have lower variability than the failure clamps (which it obviously does in Figure 6), and presumably baseline (which provides success and failure feedback, thus would contain motor noise and likely some exploratory noise).</p>
<p>However, in Figure 6, one visually observes greater variability during the success clamp (where it is assumed variability only comes from motor noise) compared to baseline (where variability would come from: (a) Motor noise.</p>
<p>(b) Likely some exploratory noise since there were some failures.</p>
<p>(c) Updates in reach aim.</p>
</disp-quote>
<p>Thanks for this comment. It made us realize that some of our terminology was unintentionally misleading. Reaching to discrete targets in the Baseline block was done to a) determine if participants could move successfully to targets that are the same width as the 100% reward zone in the continuous targets and b) determine if there are age dependent changes in movement precision. We now realize that the term Baseline Variability was misleading and should really be called Baseline Precision.</p>
<p>This is an important distinction that bears on this reviewer's comment. In clamp trials, participants move to continuous targets. In baseline, participants move to discrete targets presented at different locations. Clamp Variability cannot be directly compared to Baseline Precision because they are qualitatively different. Since the target changes on each baseline trial, we would not expect updating of desired reach (the target is the desired reach) and there is therefore no updating of reach based on success or failure. The SD we calculate over baseline trials is the endpoint variability of the reach locations relative to the target centers. In success clamp, there are no targets so the task is qualitatively different.</p>
<p>We have updated the text to clarify terminology, expand upon our operational definitions, and motivate the distinct role of the baseline block in our task paradigm (line 674).</p>
<disp-quote content-type="editor-comment">
<p>Given the comment above, can the authors please:</p>
<p>(a) Statistically compare movement variability between the baseline, success clamp, and failure clamp phases.</p>
</disp-quote>
<p>Given our explanation in the previous point we don't think that comparing baseline to the clamp makes sense as the trials are qualitatively different.</p>
<disp-quote content-type="editor-comment">
<p>(b) The authors have examined how their model predicts variability during success clamps and failure clamps, but can they also please show predictions for baseline (similar to that of Cashaback et al., 2019; Supplementary B, which alternatively used a no feedback baseline)?</p>
</disp-quote>
<p>Again, we do not think it makes sense to predict the baseline which as we mention above has discrete targets compared to the continuous targets in the learning phase.</p>
<disp-quote content-type="editor-comment">
<p>(c) Can the authors show whether participants updated their aim towards their last successful reach during the success clamp? This would be a particularly insightful analysis of model assumptions.</p>
</disp-quote>
<p>We have now compared 31 models (see full details in next response) which include the 7 models in Roth et al. (2023). Several of these model variants have updating even after success with so called planning noise). We also now fit the model to the data that includes the clamp phases (we can't easily fit to success clamp alone as there are only 10 trials). We find that the preferred model is one that does not include updating after success.</p>
<disp-quote content-type="editor-comment">
<p>(d) Different sources of movement variability have been proposed in the literature, as have different related models. One possibility is that the nervous system has knowledge of 'planned (noise)' movement variability that is always present, irrespective of success (van Beers, R.J. (2009). Motor learning is optimally tuned to the properties of motor noise. Neuron, 63(3), 406-417). The authors have used slightly different variations of their model in the past. Roth et al (2023) directly Rill compared several different plausible models with various combinations of motor, planned, and exploratory noise (Roth A, 2023, &quot;Reinforcement-based processes actively regulate motor exploration along redundant solution manifolds.&quot; Proceedings of the Royal Society B 290: 20231475: see Supplemental). Their best-fit model seems similar to the one the authors propose here, but the current paper has the added benefit of the success and failure clamps to tease the different potential models apart. In light of the results of a), b), and c), the authors are encouraged to provide a paragraph on how their model relates to the various sources of movement variability and ther models proposed in the literature.</p>
</disp-quote>
<p>Thank you for this. We realized that the models presented in Roth et al. (2023) as well as in other papers, are all special cases of a more general model. Moreover, in total there are 30 possible variants of the full model so we have now fit all 31 models to our larger datasets and performed model selection (Results and Methods). All the models can be efficiently fit by Kalman smoother to the actual data (rather than to summary statistics which has sometimes been done). For model selection, we fit only the 100 learning trials and chose the preferred model based on BIC on the children's data (Figure 5—figure Supplement 1). After selecting the preferred model we then refit this model to all trials including the clamps so as to obtain the best parameter estimates.</p>
<p>The preferred model was the same whether we combined the continuous and discrete probabilistic data or just examin d each task separately either for only the children or for the children and adults combined. The preferred model is a pecial case (no exploration after success) of the one proposed in Therrien et al. (2018) and has exploration variability (after failure) and motor noise with full updating with exploration variability (if any) after success. This model differs from the model in the original submission which included a partial update of the desired reach after exploration this was considered the learning rate. The current model suggests a unity learning rate.</p>
<p>In addition, as suggested by another reviewer, we also fit a value-based model which we adapted from the model described in Giron et al. (2023). This model was not preferred.</p>
<p>We have added a paragraph to the Discussion highlighting different sources of variability and links to our model comparison.</p>
<disp-quote content-type="editor-comment">
<p>(e) line 155. Why would the success clamp be composed of both motor and exploratory noise? Please clarify in the text</p>
</disp-quote>
<p>This sentence was written to refer to clamps in general and not just success clamps. However, in the revision this sentence seemed unnecessary so we have removed it.</p>
<disp-quote content-type="editor-comment">
<p>(3) Hypotheses:</p>
<p>The introduction did not have any hypotheses of development and reinforcement, despite the discussion above setting up potential hypotheses. Did the authors have any hypotheses related to why they might expect age to change motor noise, exploratory noise, and learning rates? If so, what would the experimental behaviour look like to confirm these hypotheses? Currently, the manuscript reads more as an exploratory study, which is certainly fine if true, it should just be explicitly stated in the introduction. Note: on line 144, this is a prediction, not a hypothesis. Line 225: this idea could be sharpened. I believe the authors are speaking to the idea of having more explicit knowledge of action-target pairings changing behaviour.</p>
</disp-quote>
<p>We have included our hypotheses and predictions at two points in the paper In the introduction we modified the text to:</p>
<p>&quot;We hypothesized that children's reinforcement learning abilities would improve with age, and depend on the developmental trajectory of exploration variability, learning rate (how much people adjust their reach after success), and motor noise (here defined as all sources of noise associated with movement, including sensory noise, memory noise, and motor noise). We think that these factors depend on the developmental progression of neural circuits that contribute to reinforcement learning abilities (Raznahan et al., 2014; Nelson et al., 2000; Schultz, 1998).&quot;</p>
<p>In results we modified the sentence to:</p>
<p>&quot;We predicted that discrete targets could increase exploration by encouraging children to move to a different target after failure.”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Summary:</p>
<p>In this study, Hill and colleagues use a novel reinforcement-based motor learning task (&quot;RML&quot;), asking how aspects of RML change over the course of development from toddler years through adolescence. Multiple versions of the RML task were used in different samples, which varied on two dimensions: whether the reward probability of a given hand movement direction was deterministic or probabilistic, and whether the solution space had continuous reach targets or discrete reach targets. Using analyses of both raw behavioral data and model fits, the authors report four main results: First, developmental improvements reflected 3 clear changes, including increases in exploration, an increase in the RL learning rate, and a reduction of intrinsic motor noise. Second, changes to the task that made it discrete and/or deterministic both rescued performance in the youngest age groups, suggesting that observed deficits could be linked to continuous/probabilistic learning settings. Overall, the results shed light on how RML changes throughout human development, and the modeling characterizes the specific learning deficits seen in the youngest ages.</p>
<p>Strengths:</p>
<p>(1) This impressive work addresses an understudied subfield of motor control/psychology - the developmental trajectory of motor learning. It is thus timely and will interest many researchers.</p>
<p>(2) The task, analysis, and modeling methods are very strong. The empirical findings are rather clear and compelling, and the analysis approaches are convincing. Thus, at the empirical level, this study has very few weaknesses.</p>
<p>(3) The large sample sizes and in-lab replications further reflect the laudable rigor of the study.</p>
<p>(4) The main and supplemental figures are clear and concise.</p>
</disp-quote>
<p>Thank you.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>(1) Framing.</p>
<p>One weakness of the current paper is the framing, namely w/r/t what can be considered &quot;cognitive&quot; versus &quot;non-cognitive&quot; (&quot;procedural?&quot;) here. In the Intro, for example, it is stated that there are specific features of RML tasks that deviate from cognitive tasks. This is of course true in terms of having a continuous choice space and motor noise, but spatially correlated reward functions are not a unique feature of motor learning (see e.g. Giron et al., 2023, NHB). Given the result here that simplifying the spatial memory demands of the task greatly improved learning for the youngest cohort, it is hard to say whether the task is truly getting at a motor learning process or more generic cognitive capacities for spatial learning, working memory, and hypothesis testing. This is not a logical problem with the design, as spatial reasoning and working memory are intrinsically tied to motor learning. However, I think the framing of the study could be revised to focus in on what the authors truly think is motor about the task versus more general psychological mechanisms. Indeed, it may be the case that deficits in motor learning in young children are mostly about cognitive factors, which is still an interesting result!</p>
</disp-quote>
<p>Thank you for these comments on the framing of our study. We now clearly acknowledge that all motor tasks have cognitive components (new paragraph at line 65). We also explain why we think our tasks has features not present in typical cognitive tasks.</p>
<disp-quote content-type="editor-comment">
<p>(2) Links to other scholarship.</p>
<p>If I'm not mistaken a common observation in tudies of the development of reinforcement learning is a decrease in exploration over-development (e.g., Nussenbaum and Hartley, 2019; Giron et al., 2023; Schulz et al., 2019); this contrasts with the current results which instead show an increase. It would be nice to see a more direct discussion of previous findings showing decreases in exploration over development, and why the current study deviates from that. It could also be useful for the authors to bring in concepts of different types of exploration (e.g. &quot;directed&quot; vs &quot;random&quot;), in their interpretations and potentially in their modeling.</p>
</disp-quote>
<p>We recognize that our results differ from prior work. The optimal exploration pattern differs from task to task. We now discuss that exploration is not one size fits all, it's benefits vary depending upon the task. We have added the following paragraphs to the Discussion section:</p>
<p>&quot;One major finding from this study is that exploration variability increases with age. Some other studies of development have shown that exploration can decrease with age indicating that adults explore less compared to children (Schulz et al., 2019; Meder et al., 2021; Giron et al., 2023). We believe the divergence between our work and these previous findings is largely due to the experimental design of our study and the role of motor noise. In the paradigm used initially by Schulz et al. (2019) and replicated in different age groups by Meder et al. (2021) and Giron et al. (2023), participants push buttons on a two-dimensional grid to reveal continuous-valued rewards that are spatially correlated. Participants are unaware that there is a maximum reward available and therefore children may continue to explore to reduce uncertainty if they have difficulty evaluating whether they have reached a maxima. In our task by contrast, participants are given binary reward and told that there is a region in which reaches will always be rewarded. Motor noise is an additional factor which plays a key role in our reaching task but minimal if any role in the discretized grid task. As we show in simulations of our task, as motor noise goes down (as it is known to do through development) the optimal amount of exploration goes up (see Figure 7—figure Supplement 2 and Appendix 1). Therefore, the behavior of our participants is rational in terms of R230 increasing exploration as motor noise decreases.</p>
<p>A key result in our study is that exploration in our task reflects sensitivity to failure. Older children make larger adjustments after failure compared to younger children to find the highly rewarded zone more quickly. Dhawale et al. (2017) discuss the different contexts in which a participant may explore versus exploit (i.e., stick at the same position). Exploration is beneficial when reward is low as this indicates that the current solution is no longer ideal, and the participant should search for a better solution. Konrad et al. (2025) have recently shown this behavior in a real-world throwing task where 6 to 12 year old children increased throwing variability after missed trials and minimized variability after successful trials. This has also been shown in a postural motor control task where participants were more variable after non-rewarded trials compared to rewarded trials (Van Mastrigt et al., 2020). In general, these studies suggest that the optimal amount of exploration is dependent on the specifics of the task.&quot;</p>
<disp-quote content-type="editor-comment">
<p>(3) Modeling.</p>
<p>First, I may have missed something, but it is unclear to me if the model is actually accounting for the gradient of rewards (e.g., if I get a probabilistic reward moving at 45°, but then don't get one at 40°, I should be more likely to try 50° next then 35°). I couldn't tell from the current equations if this was the case, or if exploration was essentially &quot;unsigned,&quot; nor if the multiple-trials-back regression analysis would truly capture signed behavior. If the model is sensitive to the gradient, it would be nice if this was more clear in the Methods. If not, it would be interesting to have a model that does &quot;function approximation&quot; of the task space, and see if that improves the fit or explains developmental changes.</p>
</disp-quote>
<p>The model we use (similar to Roth et al. (2023) and Therrien et al. (2016, 2018)) does not model the gradient. Exploration is always zero-mean Gaussian. As suggested by the reviewer, we now also fit a value-based model (described starting at line 810) which we adapted from the model presented in Giron et al. (2023). We show that the exploration and noise-based model is preferred over the value-based model.</p>
<p>The multiple-trials-back regression was unsigned as the intent was to look at the magnitude and not the direction of the change in movement. We have decided to remove this analysis from the manuscript as it was a source of confusion and secondary analysis that did not add substantially to the findings of these studies.</p>
<disp-quote content-type="editor-comment">
<p>Second, I am curious if the current modeling approach could incorporate a kind of &quot;action hysteresis&quot; (aka perseveration), such that regardless of previous outcomes, the same action is biased to be repeated (or, based on parameter settings, avoided).</p>
</disp-quote>
<p>In some sense, the learning rate in the model in the original submission is highly related to perseveration. For example if the learning rate is 0, then there is complete perseveration as you simply repeat the same desired movement. If the rate is 1, there is no perseveration and values in between reflect different amounts of perseveration. Therefore, it is not easy to separate learning rate from perseveration. Adding perseveration as another parameter would likely make it and the learning unidentifiable. However, we now compare 31 models and those that have a non-unity learning rate are not preferred suggesting there is little perseveration.</p>
<disp-quote content-type="editor-comment">
<p>(4) Psychological mechanisms. There is a line of work that shows that when children and adults perform RL tasks they use a combination of working memory and trial-by-trial incremental learning processes (e.g., Master et al., 2020; Collins and Frank 2012). Thus, the observed increase in the learning rate over development could in theory reflect improvements in instrumental learning, working memory, or both. Could it be that older participants are better at remembering their recent movements in short-term memory (Hadjiosif et al., 2023; Hillman et al., 2024)?</p>
</disp-quote>
<p>We agree that cognitive processes, such as working memory or visuospatial processing, play a role in our task and describe cognitive elements of our task in the introduction (new paragraph at line 65). However, the sensorimotor model we fit to the data does a good job of explaining the variation across age, which suggests that that age-dependent cognitive processes probably play a smaller role.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public review):</bold></p>
<p>Summary:</p>
<p>The study investigates reinforcement learning across the lifespan with a large sample of participants recruited for an online game. It finds that children gradually develop their abilities to learn reward probability, possibly hindered by their immature spatial processing and probabilistic reasoning abilities. Motor noise, reinforcement learning rate, and exploration after a failure all contribute to children's subpar performance.</p>
<p>Strengths:</p>
<p>(1) The paradigm is novel because it requires continuous movement to indicate people's choices, as opposed to discrete actions in previous studies.</p>
<p>(2) A large sample of participants were recruited.</p>
<p>(3) The model-based analysis provides further insights into the development of reinforcement learning ability.</p>
</disp-quote>
<p>Thank you.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>(1 ) The adequacy of model-based analysis is questionable, given the current presentation and some inconsistency in the results.</p>
</disp-quote>
<p>Thank you for raising this concern. We have substantially revised the model from our first submission. We now compare 31 noise-based models and 1 value-based model and fit all of the tasks with the preferred model. We perform model selection using the two tasks with the largest datasets to identify the preferred model. From the preferred model, we found the parameter fits for each individual dataset and simulated the trial by trial behavior allowing comparison between all four tasks. We now show examples of individual fits as well as provide a measure of goodness of fit. The expansion of our modeling approach has resolved inconsistencies and sharpened the conclusions drawn from our model.</p>
<disp-quote content-type="editor-comment">
<p>(2) The task should not be labeled as reinforcement motor learning, as it is not about learning a motor skill or adapting to sensorimotor perturbations. It is a classical reinforcement learning paradigm.</p>
</disp-quote>
<p>We now make it clear that our reinforcement learning task has both motor and cognitive demands, but does not fall entirely within one of these domains. We use the term motor learning because it captures the fact that participants maximize reward by making different movements, corrupted by motor noise, to unmarked locations on a continuous target zone. When we look at previous ublications, it is clear that our task is similar to those that also refer to this as reinforcement motor learning Cashaback et al. (2019) (reaching task using a robotic arm in adults), Van Mastrigt et al. (2020) (weight shifting task in adults), and Konrad et al. (2025) (real-world throwing task in children). All of these tasks involve trial-by-trial learning through reinforcement to make the movement that is most effective for a given situation. We feel it is important to link our work to these previous studies and prefer to preserve the terminology of reinforcement motor learning.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewing Editor Comments:</bold></p>
</disp-quote>
<p>Thank you for this summary. Rather than repeat the extended text from the responses to the reviewers here, we point the Editor to the appropriate reviewer responses for each issue raised.</p>
<disp-quote content-type="editor-comment">
<p>The reviewers and editors have rated the significance of the findings in your manuscript as &quot;Valuable&quot; and the strength of evidence as &quot;Solid&quot; (see eLife evalutation). A consultancy discussion session to integrate the public reviews and recommendations per reviewer (listed below), has resulted in key recommendations for increasing the significance and strength of evidence:</p>
<p>To increase the Significance of the findings, please consider the following:</p>
<p>(1) Address and reframe the paper around whether the task is truly getting at a motor learning process or more generic cognitive decision-making capacities such as spatial memory, reward processing, and hypothesis testing.</p>
</disp-quote>
<p>We have revised the paper to address the comments on the framing of our work. Please see responses to the public review comments of Reviewers #2 and #3.</p>
<disp-quote content-type="editor-comment">
<p>(2) It would be beneficial to specify the differences between traditional reinforcement algorithms (i.e., using softmax functions to explore, and build representations of state-action-reward) and the reinforcement learning models used here (i.e., explore with movement variability, update reach aim towards the last successful action), and compare present findings to previous cognitive reinforcement learning studies in children.</p>
</disp-quote>
<p>Please see response to the public review comments of Reviewer #1 in which we explain the expansion of our modeling approach to fit a value-based model as well as 31 other noise-based models. In our response to the public review comments of Reviewer #2, we comment on our expanded discussion of how our findings compare with previous cognitive reinforcement learning studies.</p>
<disp-quote content-type="editor-comment">
<p>To move the &quot;Strength of Evidence&quot; to &quot;Convincing&quot;, please consider doing the following:</p>
<p>(1 ) Address some apparently inconsistent and unrealistic values of motor noise, exploration noise, and learning rate shown for individual participants (e.g., Figure 5b; see comments reviewers 1 and take the following additional steps: plotting r squares for individual participants, discussing whether individual values of the fitted parameters are plausible and whether model parameters in each age group can extrapolate to the two clamp conditions and baselines.</p>
</disp-quote>
<p>We have substantially updated our modeling approach. Now that we compare 31 noise-based models, the preferred model does not show any inconsistent or unrealistic values (see response to Reviewer #3). Additionally, we now show example individual fits and provide both relative and absolute goodness of fit (see response to Reviewer #3).</p>
<disp-quote content-type="editor-comment">
<p>(2) Relatedly, to further justify if model assumptions are met, it would be valuable to show that the current learning model fits the data better than alternative models presented in the literature by the authors themselves and by others (reviewer 1). This could include alternative development models that formalise the proposed explanations for age-related change: poor spatial memory, reward/outcome processing, and exploration strategies (reviewer 2).</p>
</disp-quote>
<p>Please see response to public review comments of Reviewer #1 in which we explain that we have now fit a value-based model as well as 31 other noise-based models providing a comparison of previous models as well as novel models. This led to a slightly different model being preferred over the model in the original submission (updated model has a learning rate of unity). These models span many of the processes previously proposed for such tasks. We feel that 32 models span a reasonable amount of space and do not believe we have the power to include memory issues or heuristic exploration strategies in the model.</p>
<disp-quote content-type="editor-comment">
<p>(3) Perform the mediation analysis with all the possible variables (i.e., not informed by multiple regression) to see if the results are more consistent across studies and with the current approach (see comments reviewer 1).</p>
</disp-quote>
<p>Please see response to public review comments of Reviewer #1. We chose to focus only on the model based analysis because it allowed us to distinguish between exploration variability and motor noise.</p>
<p>Please see below for further specific recommendations from each reviewer.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations for the author):</bold></p>
<p>(1) In general, there should be more discussion and contextualization of other binary reinforcement tasks used in the motor literature. For example, work from Jeroen Smeets, Katinka van der Kooij, and Joseph Galea.</p>
</disp-quote>
<p>Thank you for this comment. We have edited the Introduction to better contextualize our work within the reinforcement motor learning literature (see line 67 and line 83).</p>
<disp-quote content-type="editor-comment">
<p>(2) Line 32. Very minor. This sentence is fine, but perhaps could be slightly improved. “select a location along a continuous and infinite set of possible options (anywhere along the span of the bridge)&quot;</p>
</disp-quote>
<p>Thank you for this comment. We have edited the sentence to reflect this suggestion.</p>
<disp-quote content-type="editor-comment">
<p>(3) Line 57. To avoid some confusion in successive sentences: Perhaps, &quot;Both children over 12 and adolescents...&quot;.</p>
</disp-quote>
<p>Thank you for this comment. We have edited the sentence to reflect this suggestion.</p>
<disp-quote content-type="editor-comment">
<p>(4) Line 80. This is arguably not a mechanistic model, since it is likely not capturing the reward/reinforcement machinery used by the nervous system, such as updating the expected value using reward predic tion errors/dopamine. That said, this phenomenological model, and other similar models in the field, do very well to capture behaviour with a very simple set of explore and update rules.</p>
</disp-quote>
<p>We use mechanistic in the standard use in modeling, as in Levenstein et al. (2023), for example. The contrast is not with neural modeling, but with normative modeling, in which one develops a model to optimize a function (or descriptive models as to what a system is trying to achieve). In mechanistic modeling one proposes a mechanism and this can be at a state-space level (as in our case) or a neural level (as suggested my the reviewer) but both are considered mechanistic, just at different levels. Quoting Levenstein &quot;... mechanistic models, in which complex processes are summarized in schematic or conceptual structures that represent general properties of components and their interactions, are also commonly used.&quot; We now reference the Levenstein paper to clarify what we mean by mechanistic.</p>
<disp-quote content-type="editor-comment">
<p>(5) Figure 1. It would be useful to state that the x-axis in Figure 1 is in normalized units, depending on the device.</p>
</disp-quote>
<p>Thank you for this comment. We have added a description of the x-axis units to the Fig. 1 caption.</p>
<disp-quote content-type="editor-comment">
<p>(6) Were there differences in behaviour for these different devices? e.g., how different was motor noise for the mouse, trackpad, and touchscreen?</p>
</disp-quote>
<p>Thank you for this question. We did not find a significant effect of device on learning or precision in the baseline block. We have added these one way ANOVA results for each task in Supplementary Table 1.</p>
<disp-quote content-type="editor-comment">
<p>(7) Line 98. Please state that participants received reinforcement feedback during baseline.</p>
</disp-quote>
<p>Thank you for this comment. We have updated the text to specify that participants receive reward feedback during the baseline block.</p>
<disp-quote content-type="editor-comment">
<p>(8) Line 99. Did the distance from the last baseline trial influence whether the participant learned or did not learn? For example, would it place them too far from the peak success location such that it impacted learning?</p>
</disp-quote>
<p>Thank you for this question. We looked at whether the position of movement on the last baseline block trial was correlated with the first movement position in the learning block. We did not find any correlations between these positions for any of the tasks. Interestingly, we found that the majority of participants move to the center of the workspace on the first trial of the learning block for all tasks (either in the presence of the novel continuous target scene or the presentation of 7 targets all at once). We do not think that the last movement in the baseline block &quot;primed&quot; the participant for the location of the success zone in the learning block. We have added the following sentence to the Results section:</p>
<p>&quot;Note that the reach location for the first learning trial was not affected by (correlated with) the target position on the last baseline trial (p &gt; 0.3 for both children and adults, separately).&quot;</p>
<disp-quote content-type="editor-comment">
<p>(9) The term learning distance could be improved. Perhaps use distance from target.</p>
</disp-quote>
<p>Thank you for this comment. We appreciate that learning distance defined with 0 as the best value is counter intuitive. We have changed the language to be &quot;distance from target&quot; as the learning metric.</p>
<disp-quote content-type="editor-comment">
<p>(10) Line 188. This equation is correct, but to estimate what the standard deviation by the distribution of changes in reach position is more involved. Not sure if the authors carried out this full procedure, which is described in Cashaback et al., 2019; Supplemental 2.</p>
</disp-quote>
<p>There appear to be no Supplemental 2 in the referenced paper so we assume the reviewer is referring to Supplemental B which deals with a shuffling procedure to examine lag-1 correlations.</p>
<p>In our tasks, we are limited to only 9 trials to analyze in each clamp phase so do not feel a shuffling analysis is warranted. In these blocks, we are not trying to 'estimate what the standard deviation by the distribution of changes in reach position' but instead are calculating the standard deviation of the reach locations and comparing the model fit (for which the reviewer says the formula is correct) with the data. We are unclear what additional steps the reviewer is suggesting. In our updated model analysis, we fit the data including the clamp phases for better parameter estimation. We use simulations to estimate s.d. in the clamp phase (as we ensure in simulations the data does not fall outside the workspace) making the previous analytic formulas an approximation that are no longer used.</p>
<disp-quote content-type="editor-comment">
<p>(11) Line 197-199. Having done the demo task, it is somewhat surprising that a 3-year-old could understand these instructions (whose comprehension can be very different from even a 5-year old).</p>
</disp-quote>
<p>Thank you for raising this concern. We recognize that the younger participants likely have different comprehension levels compared to older participants. However, we believe that the majority of even the youngest participants were able to sufficiently understand the goal of the task to move in a way to get the video clip to play. We intentionally designed the tasks to be simple such that the only instructions the child needed to understand were that the goal was to get the video clip to play as much as possible and the video clip played based on their movement. Though the majority of younger children struggled to learn well on the probabilistic tasks, they were able to learn well on the deterministic tasks where the task instructions were virtually identical with the exception of how many places in the workspace could gain reward. On the continuous probabilistic task, we did have a small number (n = 3) of 3 to 5 year olds who exhibited more mature learning ability which gives us confidence that the younger children were able to understand the task goal.</p>
<disp-quote content-type="editor-comment">
<p>(12) Line 497: Can the authors please report the F-score and p-value separately for each of these one-way ANOVA (the device is of particular interest here).</p>
</disp-quote>
<p>Thank you for this request. We have added ina upplementarytable (Supplementary Table 1) with the results of these ANOVAs.</p>
<disp-quote content-type="editor-comment">
<p>(13) Past work has discussed how motivation influences learning, which is a function of success rate (van der Kooij, K., in 't Veld, L., &amp; Hennink, T. (2021). Motivation as a function of success frequency. Motivation and Emotion, 45, 759-768.). Can the authors please discuss how that may change throughout development?</p>
</disp-quote>
<p>Thank you for this comment. While motivation most probably plays a role in learning, in particular in a game environment, this was out of the scope of the direct focus of this work and not something that our studies were designed to test. We have added the following sentence to the discussion section to address this comment:</p>
<p>&quot;We also recognize that other processes, such as memory and motivation, could affect performance on these tasks however our study was not designed to test these processes directly and future work would benefit from exploring these other components more explicitly.&quot;</p>
<disp-quote content-type="editor-comment">
<p>(14) Supplement 6. This analysis is somewhat incomplete because it does not consider success.</p>
<p>Pekny and collegues (2015) looked at 3 trials back but considered both success and reward. However, their analysis has issues since successive time points are not i.i.d., and spurious relationships can arise. This issue is brought up by Dwahale (Dhawale, A. K., Miyamoto, Y. R., Smith, M. A., &amp; R475 Ölveczky, B. P. (2019). Adaptive regulation of motor variability. Current Biology, 29(21), 3551-3562.). Perhaps it is best to remove this analysis from the paper.</p>
</disp-quote>
<p>Thank you for this comment. We have decided to remove this secondary analysis from the paper as it was a source of confusion and did not add to the understanding and interpretation of our behavioral results.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations for the author):</bold></p>
<p>(1 ) the path length ratio analyses in the supplemental are interesting but are not mentioned in the main paper. I think it would be helpful to mention these as they are somewhat dramatic effects</p>
</disp-quote>
<p>Thank you for this comment. Path length ratios are defined in the Methods and results are briefly summarized in the Results section with a point to the supplementary figures. We have updated the text to more explicitly report the age related differences in path length ratios.</p>
<disp-quote content-type="editor-comment">
<p>(2) The second to last paragraph of the intro could use a sentence motivating the use ofthe different task features (deterministic/probabilistic and discrete/continuous).</p>
</disp-quote>
<p>Thank you for this comment. We have added an additional motivating sentence to the introduction.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations for the author):</bold></p>
<p>The paper labeled the task as one for reinforcement motor learning, which is not quite appropriate in my opinion. Motor learning typically refers to either skill learning or motor adaptation, the former for improving speed-accuracy tradeoffs in a certain (often new) motor skill task and the latter for accommodating some sensorimotor perturbations for an existing motor skill task. The gaming task here is for neither. It is more like a</p>
</disp-quote>
<p>decision-making task with a slight contribution to motor execution, i.e., motor noise. I would recommend the authors label the learning as reinforcement learning instead of reinforcement motor learning.</p>
<p>Thank you for this comment. As noted in the response to the public review comments, we agree that this task has components of classical reinforcement learning (i.e. responding to a binary reward) but we specifically designed it to require the learning of a movement within a novel game environment. We have added a new paragraph to the introduction where we acknowledge the interplay between cognitive and motor mechanisms while also underscoring the features in our task that we think are not present in typical cognitive tasks.</p>
<disp-quote content-type="editor-comment">
<p>My major concern is whether the model adequately captures subjects' behavior and whether we can conclude with confidence from model fitting. Motor noise, exploration noise, and learning rate, which fit individual learning patterns (Figure 5b), show some quite unrealistic values. For example, some subjects have nearly zero motor noise and a 100% learning rate.</p>
</disp-quote>
<p>We have now compared 31 models and the preferred model is different from the one in the first submission. The parameter fits of the new model do not saturate in any way and appear reasonable to us. The updates to the model analysis have addressed the concern of previously seen unrealistic values in the prior draft.</p>
<disp-quote content-type="editor-comment">
<p>Currently, the paper does not report the fitting quality for individual subjects. It is good to have an exemplary subject's fit shown, too. My guess is that the r-squared would be quite low for this type of data. Still, given that the children's data is noisier, it might be good to use the adult data to show how good the fitting can be (individual fits, r squares, whether the fitted parameters make sense, whether it can extrapolate to the two clamp phases). Indeed, the reliability of model fitting affects how we should view the age effect of these model parameters.</p>
</disp-quote>
<p>We now show fits to individual subjects. But since this is a Kalman smoother it fits the data perfectly by generating its best estimate of motor noise and exploration variability on each trial to fully account for the data — so in that sense <italic>R</italic><sup>2</sup> is always 1 so that is not helpful.</p>
<disp-quote content-type="editor-comment">
<p>While the BIC analysis with the other model variants provides a relative goodness of fit, it is not straightforward to provide an absolute goodness of fit such as standard <italic>R</italic><sup>2</sup> for a feedforward simulation of the model given the parameters (rather than the output of the Kalman smoother). There are two problems. First, there is no single model output. Each time the model is simulated with the fit parameters it produces a different output (due to motor noise, exploration variability and reward stochasticity). Second, the model is not meant to reproduce the actual motor noise, exploration variability and reward stochasticity of a trial. For example, the model could fit pure Gaussian motor noise across trials (for a poor learner) by accurately fitting the standard deviation of motor noise but would not be expected to actually match each data point so would have a traditional <italic>R</italic><sup>2</sup> of O.</p>
</disp-quote>
<p>To provide an overall goodness of fit we have to reduce the noise component and to do so we exam ined the traditional <italic>R</italic><sup>2</sup> between the average of all the children's data and the average simulation of the model (from the median of 1000 simulations per participant) so as to reduce the stochastic variation. The results for the continuous probabilistic and discrete probabilistic task are <italic>R</italic><sup>2</sup> of 0.41 and 0.72, respectively.</p>
<disp-quote content-type="editor-comment">
<p>Not that variability in the &quot;success clamp&quot; doe not change across ages (Figure 4C) and does not contribute to the learning effect (Figure 4F). However, it is regarded as reflecting motor noise (Figure SC), which then decreases over age from the model fitting (Figure 5B). How do we reconcile these contradictions? Again, this calls the model fitting into question.</p>
</disp-quote>
<p>For the success clamp, we only have 9 trials to calculate variability which limits our power to detect significance with age. In contrast, the model uses all 120 trials to estimate motor noise. There is a downward trend with age in the behavioral data which we now show overlaid on the fits of the model for both probabilistic conditions (Figure 5—figure Supplement 4) and Figure 6—figure Supplement 4). These show a reasonable match and although the variance explained is 1 6 and 56% (we limit to 9 trials so as to match the fail clamp), the correlations are 0.52 and 0.78 suggesting we have reasonable relation although there may be other small sources of variability not captured in the model.</p>
<disp-quote content-type="editor-comment">
<p>Figure 5C: it appears one bivariate outlier contributes a lot to the overall significant correlation here for the &quot;success clamp&quot;.</p>
</disp-quote>
<p>Recalculating after removing that point in original Fig 5C was still significant and we feel the plots mentioned in the previous point add useful information to this issue. With the new model this figure has changed.</p>
<disp-quote content-type="editor-comment">
<p>It is still a concern that the young children did not understand the instructions. Nine 3-to-8 children (out of 48) were better explained by the noisy only model than the full model. In contrast, ten of the rest of the participants (out of 98) were better explained by the noisy-only model. It appears that there is a higher percentage of the &quot;young&quot; children who didn't get the instruction than the older ones.</p>
</disp-quote>
<p>Thank you for this comment. We did take participant comprehension of the task into consideration during the task design. We specifically designed it so that the instructions were simple and straight forward. The child simply needs to understand the underlying goal to make the video clip play as often as possible and that they must move the penguin to certain positions to get it to play. By having a very simple task goal, we are able to test a naturalistic response to reinforcement in the absence of an explicit strategy in a task suited even for young children.</p>
<p>We used the updated reinforcement learning model to assess whether an individual's performance is consistent with understanding the task. In the case of a child who does not understand the task, we expect that they simply have motor noise on their reach, and crucially, that they would not explore more after failure, nor update their reach after success. Therefore, we used a likelihood ratio test to examine whether the preferred model was significantly better at explaining each participant's data compared to the model variant which had only motor noise (Model 1). Focusing on only the youngest children (age 3-5), this analysis showed that that 43, 59, 65 and 86% of children (out of N = 21, 22, 20 and 21 ) for the continuous probabilistic, discrete probabilistic, continuous deterministic, and discrete deterministic conditions, respectively, were better fit with the preferred model, indicating non-zero exploration after failure. In the 3-5 year old group for the discrete deterministic condition, 18 out of 21 had performance better fit by the preferred model, suggesting this age group understands the basic task of moving in different directions to find a rewarding location.</p>
<p>The reduced numbers fit by the preferred model for the other conditions likely reflects differences in the task conditions (continuous and/or probabilistic) rather than a lack of understanding of the goal of the task. We include this analysis as a new subsection at the end of the Results.</p>
<disp-quote content-type="editor-comment">
<p>Supplementary Figure 2: the first panel should belong to a 3-year-old not a 5-year-old? How are these panels organized? This is kind of confusing.</p>
</disp-quote>
<p>Thank you for this comment. Figure 2—figure Supplement 1 and Figure 2—figure Supplement 2 are arranged with devices in the columns and a sample from each age bin in the rows. For example in Figure 2—figure Supplement 1, column 1, row 1 is a mouse using participant age 3 to 5 years old while column 3, row 2 is a touch screen using participant age 6 to 8 years old. We have edited the labeling on both figures to make the arrangement of the data more clear.</p>
<disp-quote content-type="editor-comment">
<p>Line 222: make this a complete sentence.</p>
</disp-quote>
<p>This sentence has been edited to a complete sentence.</p>
<disp-quote content-type="editor-comment">
<p>Line 331: grammar.</p>
</disp-quote>
<p>This sentence has been edited for grammar.</p>
</body>
</sub-article>
</article>