<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">93487</article-id>
<article-id pub-id-type="doi">10.7554/eLife.93487</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.93487.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A visual efference copy-based navigation algorithm in <italic>Drosophila</italic> for complex visual environments</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0006-7642-4055</contrib-id>
<name>
<surname>Canelo</surname>
<given-names>Angel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kim</surname>
<given-names>Yeon</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Park</surname>
<given-names>Jeongmin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6681-1437</contrib-id>
<name>
<surname>Kim</surname>
<given-names>Anmo J</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Electronic Engineering, Hanyang University</institution>, Seoul, <country>South Korea</country></aff>
<aff id="a2"><label>2</label><institution>Department of Artificial Intelligence, Hanyang University</institution>, Seoul, <country>South Korea</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Tuthill</surname>
<given-names>John C</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Washington</institution>
</institution-wrap>
<city>Seattle</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Cardona</surname>
<given-names>Albert</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Cambridge</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Correspondence: <email>anmokim@hanyang.ac.kr</email> (A.J.K.)</corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-01-11">
<day>11</day>
<month>01</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP93487</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-10-28">
<day>28</day>
<month>10</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-10-14">
<day>14</day>
<month>10</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.10.11.561122"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Canelo et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Canelo et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-93487-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p><italic>Drosophila</italic> visuomotor processing has been intensively studied in recent years, leading to a qualitative understanding of individual neural circuits. However, the collective operation of these circuits during naturalistic behaviors, in which flies encounter a mixture of complex visual stimuli—including those caused by their own actions—remains unexplored. In this study, we developed an integrative model of <italic>Drosophila</italic> visuomotor processing, wherein multiple visuomotor circuits interconnect through an efference copy (EC) mechanism. To derive the model experimentally, we analyzed the wingbeat responses of flying <italic>Drosophila</italic> to individual, rotating visual patterns. We then combined these models to build an integrative model for superposed visual patterns, using three different strategies: the addition-only, the graded EC, and the all-or-none EC models. We compared orientation behaviors of these models with those of flying <italic>Drosophila</italic> that rotates their body freely in response to complex visual patterns. Results of these experiments support the all-or-none EC model, in which the amplitude of the flight turn is unimpeded by the background scene, irrespective of the visual environment. Together, our “virtual fly” model provides a formal description of vision-based navigation strategies of <italic>Drosophila</italic> in complex visual environments and offers a novel framework for assessing the role of constituent visuomotor neural circuits in real-world contexts.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd><italic>Drosophila</italic></kwd>
<kwd>visuomotor behavior</kwd>
<kwd>flight navigation</kwd>
<kwd>efference copy</kwd>
<kwd>complex visual environment</kwd>
</kwd-group>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/nisl-hyu/flightsim">https://github.com/nisl-hyu/flightsim</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Fruit flies demonstrate impressive agility in flight (<xref ref-type="bibr" rid="c6">Card and Dickinson, 2008</xref>; <xref ref-type="bibr" rid="c20">Fry et al., 2005</xref>; <xref ref-type="bibr" rid="c36">Liu et al., 2019</xref>; <xref ref-type="bibr" rid="c43">Muijres et al., 2014</xref>), and vision plays a pivotal role in the flight control, often translating directly into precise flight maneuvers. Neural circuit mechanisms underlying many of these visuomotor behaviors have been studied intensively in recent years, both structurally and functionally (<xref ref-type="bibr" rid="c17">Fenk et al., 2022</xref>; <xref ref-type="bibr" rid="c28">Joesch, 2009</xref>; <xref ref-type="bibr" rid="c29">Joesch et al., 2010</xref>; <xref ref-type="bibr" rid="c55">Takemura et al., 2013</xref>; <xref ref-type="bibr" rid="c61">Wu et al., 2016</xref>) (see (<xref ref-type="bibr" rid="c12">Currier et al., 2023</xref>; <xref ref-type="bibr" rid="c52">Ryu et al., 2022</xref>) for reviews). While we have gained a qualitative grasp of the neural substrates underlying various visual behaviors, a quantitative understanding remains less explored (<xref ref-type="bibr" rid="c1">Ache et al., 2019</xref>; <xref ref-type="bibr" rid="c39">Maisak et al., 2013</xref>; <xref ref-type="bibr" rid="c41">Mauss et al., 2015</xref>; <xref ref-type="bibr" rid="c53">Städele et al., 2020</xref>).</p>
<p>In the pioneering era of insect vision in the 1950s to the 1970s, research heavily leaned towards computational modeling. For instance, the Reichardt-Hassenstein elementary motion detector was formulated to explain visual locomotion behaviors observed in beetles (<xref ref-type="bibr" rid="c25">Hassenstein and Reichardt, 1956</xref>). Another example is the model of efference copy (EC) that was put forward to describe the visual behavior of hoverflies (<xref ref-type="bibr" rid="c11">Collett, 1980</xref>; <xref ref-type="bibr" rid="c59">von Holst and Mittelstaedt, 1950</xref>). While some recent studies have offered quantitative models for <italic>Drosophila</italic> visual circuits (<xref ref-type="bibr" rid="c5">Borst and Weber, 2011</xref>; <xref ref-type="bibr" rid="c10">Clark et al., 2011</xref>; <xref ref-type="bibr" rid="c24">Gruntman et al., 2018</xref>; <xref ref-type="bibr" rid="c57">Tanaka and Clark, 2020</xref>), most focused on circuit-level function without considering its behavioral context. To contextualize the function of these circuits within behavior, a phenomenological model integrating these visual functions within a moving animal would be invaluable.</p>
<p>One important consideration that needs to be made in such an integrative model is the interaction among different visuomotor circuits tuned to different visual features. Since these circuits in complex visual environments may activate multiple motor programs simultaneously, circuit mechanisms by which distinct sensorimotor pathways are integrated adaptively are necessary. A foundational theory regarding the interaction of multiple sensorimotor pathways is the theory of EC, which underscores the necessity for a feedback signal that filters out undesired sensory input (<xref ref-type="bibr" rid="c59">von Holst and Mittelstaedt, 1950</xref>). Previous behavioral studies argued that such a mechanism may indeed exist in some dipteran species (<xref ref-type="bibr" rid="c11">Collett, 1980</xref>; <xref ref-type="bibr" rid="c27">Heisenberg and Wolf, 1988</xref>). Recent studies in <italic>Drosophila</italic> have uncovered the neural correlates of the EC during both spontaneous and visually evoked rapid flight turns (<xref ref-type="bibr" rid="c18">Fenk et al., 2021</xref>; <xref ref-type="bibr" rid="c32">Kim et al., 2017</xref>, <xref ref-type="bibr" rid="c33">2015</xref>).</p>
<p>In this study, we present an integrative model of <italic>Drosophila</italic> visuomotor behaviors that is derived from and validated by behavioral experiments. Results of the experiments provided clear behavioral evidence of an EC in flying <italic>Drosophila</italic>, which was in line with one type of integration strategy that we used in the model. The model was able to predict the steering behavior of flying <italic>Drosophila</italic> in response to various visual patterns when they are presented individually as well as in superposition. Our model provides a framework in which detailed neural circuit models for <italic>Drosophila</italic> visuomotor processing can be implemented and tested in a real-world context.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Flight response of <italic>Drosophila</italic> for singly presented visual patterns</title>
<p>To develop a quantitative model of the visuomotor control for visual patterns presented individually, we measured wing responses of flying <italic>Drosophila</italic>. We placed a tethered, flying fly at the center of a cylindrical LED display and presented a rotating visual pattern on the display (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>). To measure the angular torque generated by the fly in response to these patterns, we subtracted the right wingbeat amplitude (R WBA) from the left wingbeat amplitude (L WBA), resulting in a time signal termed L-R WBA (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>). Previous studies showed that L-R WBA is correlated with the angular torque that a fly exerts on its body (3, 34).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1.</label>
<caption><title>Construction of flight control models for singly presented visual patterns.</title>
<p>(A) Schematic of the experimental setup (left), a frame captured by the infrared camera (middle), and a simplified schematic of the experimental setup (right). The annulus surrounding the fly schematic represents the visual display viewed from above. (B) Wing responses of a sample fly to three rotating visual patterns: bar, spot, and grating. L-R WBA at the bottom row represents the angular torque of the fly, calculated by subtracting the right wingbeat amplitude (RWBA) from the left wingbeat amplitude (LWBA). (C) L-R WBA traces of a sample fly in response to the three visual patterns. The thick black lines indicate the average of all trials (top) or the average of all flies (bottom). Thin gray lines indicate individual trials (top) or fly averages (bottom). (D) Schematic of the position-velocity-based flight control model. (E) Average wing responses of a population of flies in response to the three rotating visual patterns, either in a clockwise (red) or counterclockwise (blue) direction. Top traces show the position of each pattern. The red and blue shadings at the bottom indicate the 95% confidence interval. (F) Position and velocity functions estimated from wing responses in E. Light purple shading indicates the 95% confidence interval.</p></caption>
<graphic xlink:href="561122v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We presented three visual patterns rotating a full circle at 36°/s: a dark vertical bar, a dark spot, and a vertical grating (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>). These simple patterns, when tested in different velocities and sizes, were previously shown to elicit robust visuomotor reflexes in flying <italic>Drosophila</italic> (<xref ref-type="bibr" rid="c22">Götz, 1968</xref>; <xref ref-type="bibr" rid="c34">H. Kim et al., 2023</xref>; <xref ref-type="bibr" rid="c38">Maimon et al., 2008</xref>; <xref ref-type="bibr" rid="c56">Tammero et al., 2004</xref>). In response to a clockwise-rotating bar starting from the back, flies generated wing responses corresponding to a fictive rightward turn throughout the duration of the stimulus (<xref rid="fig1" ref-type="fig">Fig. 1 B-C</xref>). The response increased gradually and peaked after crossing the midline, then declined. In response to the moving spot, flies exhibited L-R WBA indicating turning away from the position of the spot (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>). In response to the moving grating, flies showed strong wing responses that would exert angular torque in the direction of the grating movement, also consistent with previous studies (<xref ref-type="bibr" rid="c9">Cellini et al., 2022</xref>; <xref ref-type="bibr" rid="c8">Cellini and Mongeau, 2020</xref>; <xref ref-type="bibr" rid="c56">Tammero et al., 2004</xref>).</p>
</sec>
<sec id="s2b">
<title>Flight control models of <italic>Drosophila</italic> for singly presented visual patterns</title>
<p>We built a simple model for these visual behaviors by adopting a classical approach that was originally applied to flying <italic>Musca Domestica</italic> (<xref ref-type="bibr" rid="c46">Poggio and Reichardt, 1973</xref>; <xref ref-type="bibr" rid="c48">Reichardt and Poggio, 1976</xref>). In this approach, the torque response of an animal, <italic>W(·)</italic>, is modeled as a sum of two components: one defined purely by a position response <italic>P(</italic> ψ<italic>)</italic> and the other by a position-dependent velocity response <inline-formula><inline-graphic xlink:href="561122v1_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (<xref rid="fig1" ref-type="fig">Fig. 1D</xref>).
<disp-formula id="eqn1">
<graphic xlink:href="561122v1_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In this model, the position response can be experimentally obtained if we add torque responses to a visual pattern rotating in the clockwise and counter-clockwise directions, but with respect to the same object position.
<disp-formula id="eqn2">
<graphic xlink:href="561122v1_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn3">
<graphic xlink:href="561122v1_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>W</italic><sub><italic>CW</italic></sub><italic>(</italic>ψ<italic>)</italic> denotes the wing response for the clockwise-rotating pattern, and <italic>W</italic><sub><italic>CCW</italic></sub><italic>(</italic>ψ<italic>)</italic> for the counterclockwise-rotating pattern. The velocity response, <inline-formula><inline-graphic xlink:href="561122v1_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, can be obtained similarly, but by a subtraction. To apply the above approach, we tested flies with both clockwise- and counterclockwise-rotating visual patterns (<xref rid="fig1" ref-type="fig">Fig. 1E</xref>). Wingbeat signals to the patterns were largely symmetric for the two directions, but with subtle asymmetries, which are likely due to experimental variabilities such as misalignment of the fly’s body orientation with respect to the display arena. To minimize the influence of such experimental anomalies, we combined the two L-R WBA signals after flipping one for its sign as well as the time, and used them for the further analyses.</p>
<p>When we computed the position functions, <italic>P(</italic>ψ<italic>)</italic>, for the spot and bar patterns with the above method, we found that they were symmetric with respect to the origin (<xref rid="fig1" ref-type="fig">Fig. 1F</xref>). That is, for the rotating bar, the position response was positive when the bar was in the left hemifield, but negative in the right hemifield. This result is consistent with fly’s fixation behavior to a vertical bar as this position response would have made flies turn toward the bar in both sides. For the spot pattern, the overall shape of the position function was opposite to that for the bar, which would have made flies turn away from the spot.</p>
<p>The velocity function, <italic>V(</italic>ψ<italic>)</italic>, was nearly zero for the spot, except around the frontal visual field (<xref rid="fig1" ref-type="fig">Fig. 1F</xref>), and this is consistent with weak direction dependency in the wing response (<xref rid="fig1" ref-type="fig">Fig. 1E</xref>). By contrast, the velocity function for the bar is consistently positive (<xref rid="fig1" ref-type="fig">Fig. 1F</xref>), poised to contribute to the flight turn in the direction of the bar movement (<xref ref-type="bibr" rid="c48">Reichardt and Poggio, 1976</xref>). The amplitude of the velocity response, however, varied depending on the object position, unlike the nearly constant velocity function estimated in blow flies in response to a moving bar (<xref ref-type="bibr" rid="c48">Reichardt and Poggio, 1976</xref>).</p>
<p>In response to the moving grating, the velocity response was consistently positive, but its amplitude varied depending on the position (<xref rid="fig1" ref-type="fig">Fig. 1E</xref>). The grating pattern consists of 12 periods of bright and dark vertical stripes, with each stripe spanning 15 degrees, which is larger than the fruit fly’s inter-ommatidial angle (∼5°). However, because we do not see any periodicity in the wing response to the grating (<xref rid="fig1" ref-type="fig">Fig. 1E</xref>), we reasoned that the change in position function is not because of the positional variation of the pattern. Instead, the time-dependency in the position and velocity response was likely due to non-visual factors such as adaptation or fatigue in the motor control caused by exceptionally large L-R WBA. Furthermore, the relatively slow L-R WBA change at the beginning and the end of the grating stimulus suggests a dynamical system transformation between the visual stimulus and the wing response. Thus, we expanded our models with a set of dynamical system equations involving a fatigue variable (<xref rid="figS1" ref-type="fig">Fig. S1</xref>), which successfully reproduced L-R WBAs to the three visual patterns as well as the position and velocity responses associated with each pattern. We also performed this experiment and analysis in a different strain of wild type flies (Canton S) and found that the profile of the position and velocity functions remain largely unchanged (<xref rid="figS2" ref-type="fig">Fig. S2</xref>).</p>
</sec>
<sec id="s2c">
<title>Simulating the orientation behavior of the virtual fly model to individual visual patterns</title>
<p>We further tested our model, termed “virtual fly model” hereafter, with a biophysical block by which the torque response of the fly is transformed to the actual heading change according to kinematic parameters estimated previously (Michael H Dickinson, 2005; <xref ref-type="bibr" rid="c51">Ristroph et al., 2010</xref>) (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>, see <xref ref-type="disp-formula" rid="eqn4">Equation 4</xref> in Methods and Movie S1). The virtual fly model, featuring position and velocity blocks that are conditioned on the type of the visual pattern, can now simulate the visual orientation behavior of flies in the free flight condition. This simulation experiment is reminiscent of the magnetically tethered flight assay, where a flying fly remains fixed at a position but is free to rotate around its yaw axis (<xref ref-type="bibr" rid="c4">Bender and Dickinson, 2006</xref>; <xref ref-type="bibr" rid="c9">Cellini et al., 2022</xref>). Additionally, we simplified the position and velocity functions for the bar and spot pattern with mathematically simple representations (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>, see <xref ref-type="disp-formula" rid="eqn5">Equation 5</xref>-<xref ref-type="disp-formula" rid="eqn7">7</xref> in Methods). For the grating pattern, we approximated the position response as zero and the velocity response as a constant value (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>), where no fatigue effect was considered for the sake of simplicity (<xref rid="figS1" ref-type="fig">Fig. S1</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig. 2.</label>
<caption><title>The flight control models with a bio-mechanics block predicted the dynamics of the orientation behavior to individual visual patterns.</title>
<p>(A) Schematics of three visuomotor response models with a biomechanics block. (B) Simplified version of the position and velocity responses for each pattern. (C) Simulation results for the three patterns (bar, spot, and grating) moving in a sigmoidal dynamics. The spot response was plotted with an 180° offset to facilitate comparison. Bar plots on the right show the latency of body angle with respect to the stimulus onset, measured at the 50% point of the pattern movement. (D) Simulation results for the three patterns moving in a sinusoidal dynamics. In the bar plots on the right, the amplitude was measured as the peak-to-peak amplitude, and the phase shift was calculated by measuring the peak time of the cross-correlation between the pattern and the fly heading. (E) Same as in C, but for visual patterns remaining static at 0 degree position. The simulation was performed 10 times with a gaussian noise component (gray lines). The mean response was plotted in thick colored lines. The probability density function of the body angle is shown on the right for each pattern.</p></caption>
<graphic xlink:href="561122v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig. 3.</label>
<caption><title>Magnetically tethered flight experiments confirmed orientation changes predicted by the virtual fly model.</title>
<p>(A) Schematic of the magnetically tethered flight assay with an LED display (left). The image acquired from below was analyzed to estimate the body angle (right). The stimulus protocol (bottom) consisted of four phases: alignment, ready, go, and freeze. (B) Body orientation responses of a single fly for the bar, spot and grating patterns moving horizontally. (C) Same as in B for a population of flies. The population averages were replotted at the bottom to facilitate the comparison of their dynamics. (D) Amplitude and latency of the body orientation responses. The box represents the interquartile range (IQR), with the median indicated by the horizontal black line. The whiskers extend to the minimum and maximum values within 1.5 times the IQR. Outliers are denoted by “+” marks beyond the whiskers.</p></caption>
<graphic xlink:href="561122v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We tested the model using visual patterns that moved horizontally with three distinct dynamics: a rapid sigmoidal jump, sinusoidal oscillations, and Gaussian noise. The virtual fly model’s response to the sigmoidal visual patterns varied depending on the pattern (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>). For the bar, the fly responded quickly toward the bar and refixated on it (see <xref ref-type="disp-formula" rid="eqn8">Equation 8</xref> in Methods). The spot elicited a heading change in the opposite direction, albeit at a slower pace and with the pattern positioned behind the fly at the end (see <xref ref-type="disp-formula" rid="eqn9">Equation 9</xref> in Methods). In the case of the grating, the response was the most rapid, but the heading change ceased early once the visual pattern stopped (see <xref ref-type="disp-formula" rid="eqn10">Equation 10</xref> in Methods). This is due to the absence of positional cue in the grating and consistent with experimental results in a previous study (<xref ref-type="bibr" rid="c11">Collett, 1980</xref>). We measured the visual response latencies at 50% of the bar movement for each pattern: 150 ms for the bar and grating, and 360 ms for the spot. These latencies are considerably longer than the wingbeat response latency reported previously (H. <xref ref-type="bibr" rid="c34">Kim et al., 2023</xref>). This is likely in part due to the additional delay caused by the biophysical block in our model. We also expanded our model to take into account the known delay in the visual system (<xref rid="figS3" ref-type="fig">Fig. S3A-B</xref>), but for the sake of simplicity, we used the simplified models in the following.</p>
<p>For sinusoidally oscillating patterns, we measured the peak-to-peak amplitude and the phase shift of the wing responses (<xref rid="fig2" ref-type="fig">Fig. 2D</xref> and <xref ref-type="fig" rid="figS3">S3C</xref>). The amplitude of heading change was comparable to the amplitude of the bar and grating stimuli, but much smaller for the spot pattern (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>). When we calculated the phase shift, we found that it was the longest for the spot, the shortest for the grating, and intermediate for the bar, consistent with the delay observed in response to the rapidly shifting patterns (<xref ref-type="bibr" rid="c34">H. Kim et al., 2023</xref>) (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>). We also simulated our models under the static visual patterns, but with Gaussian noise added to its body torque (<xref rid="fig2" ref-type="fig">Fig. 2E</xref>, see <xref ref-type="disp-formula" rid="eqn11">Equation 11</xref> in Methods). We observed fixation to the bar, anti-fixation to the spot, and stabilization to the grating, whereas the virtual fly model drifted randomly when no visual cue was present (<xref rid="fig2" ref-type="fig">Fig. 2E</xref>, bottom). Together, behavior of our models appeared to recapitulate the results of previous behavioral experiments (<xref ref-type="bibr" rid="c11">Collett, 1980</xref>; <xref ref-type="bibr" rid="c21">Götz, 1987</xref>; H. <xref ref-type="bibr" rid="c34">Kim et al., 2023</xref>; <xref ref-type="bibr" rid="c38">Maimon et al., 2008</xref>).</p>
</sec>
<sec id="s2d">
<title>Orientation behaviors of magnetically tethered flying <italic>Drosophila</italic> to rapidly moving visual patterns</title>
<p>We validated the prediction of our model in flies flying in an environment similar to the one used in the simulation (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>). A fly was tethered to a short steel pin fixed at a position by a vertically oriented magnetic field that also allowed it to rotate around its yaw axis with little friction (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>). We captured the image of the fly from below in response to visual stimuli and calculated the body angle <italic>post hoc</italic> (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>, Methods). To present the visual patterns at a defined position relative to the fly’s heading, we coaxed the fly to be aligned to a reference angle by oscillating a stripe pattern widely across the visual display (“alignment” phase, <xref rid="fig3" ref-type="fig">Fig. 3A</xref> bottom). We then presented for 0.5 s the initial frame of the visual pattern (“ready” phase), which is then rotated rapidly for 0.2 seconds in a sigmoidal dynamics (“go” phase) (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>). After the movement, the pattern remained static for 2 seconds (“freeze” phase).</p>
<p>For all visual patterns, the fly turned in the direction that was predicted by the simulation (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). Namely, flies turned toward the moving bar, away from the moving spot, and in the direction of the grating (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>). The mean amplitude was comparable for the three patterns at around 20 degrees, which was almost half that of the angle of the pattern displacement (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>). While the small response angle was expected for the grating pattern (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>), the bar and spot response amplitudes were too small compared to the model prediction. This is in contrast to the match between the model prediction and behavioral results for a slow moving object (<xref rid="figS1" ref-type="fig">Fig. S1C</xref>). Recent studies showed that the visual responses of wings and heads are strongly suppressed by proprioceptive feedback (<xref ref-type="bibr" rid="c7">Cellini and Mongeau, 2022</xref>; <xref ref-type="bibr" rid="c50">Rimniceanu et al., 2023</xref>). Namely, the prediction of our model is larger than the actual heading change amplitude, possibly because we derived our model from the behavioral assay with no proprioceptive feedback and tested it with the one with proprioceptive feedback. The heading response latency was the longest for the spot, and similar for the bar and grating (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>), in line with the virtual fly model (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>). The discrepancy in the latency between the model and the fly might be again attributable to various factors such as the approximation in the model and different neuromechanical parameters between rigidly and magnetically tethered flight conditions.</p>
<p>With these experimental results in a magnetically tethered assay, we confirmed that our virtual fly models captured essential dynamics of the vision-based flight control in <italic>Drosophila</italic>, when the patterns are presented individually. In natural conditions, however, multiple patterns may be presented simultaneously in a visual scene. This led us to further develop the model to add a capacity of controlling their orientation under more complex visual environments.</p>
</sec>
<sec id="s2e">
<title>Integrative visuomotor control models in a complex visual environment</title>
<p>In natural environments, various visual patterns might simultaneously activate distinct visuomotor circuits. These circuits, if activated, may normally trigger either synergistic or opposing actions. For instance, when a fly turns to track a foreground object moving against a static background (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>), it experiences rotational optic flow from the background in a direction opposite to the object-evoked flight turn. This would in turn activate the visual stability reflex that opposes the object-evoked turn. Therefore, this raises the question as to how outputs from different visuomotor circuits integrate to control a shared actuator, such as the wing motor system. We considered three integrative models with different strategies. The first model is termed “addition-only model”, in which output of the bar and the grating response circuits are added to control the flight orientation (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>, see <xref ref-type="disp-formula" rid="eqn12">Equation 12</xref> in Methods).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig. 4.</label>
<caption><title>Three integrative models of the visuomotor control and their predictions in a complex visual environment.</title>
<p>(A) Schematic of the visual environments used in the simulation. A moving bar is presented as a foreground object over the static grating background. (B) A diagram of the addition-only model. Bar and grating response circuits are joined at their output by addition. (C) A diagram of the graded EC model. An EC block translates the bar-evoked motor command to the negative image of the predicted grating input to counteract visual feedback. (D) A diagram of the all-or-none EC model. An EC switches off the grating response circuit during the bar-evoked turn. (E,F,G) Simulation results of the three models. The bar position and the heading of the virtual fly model (top). The associated torques as well as the EC signal (bottom).</p></caption>
<graphic xlink:href="561122v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In the second and third models, an EC is used to set priorities between different visuomotor circuits (<xref rid="fig4" ref-type="fig">Fig. 4C,D</xref>). In particular, the EC is derived from the bar-induced motor command and is sent to the grating response system to nullify visual feedback caused by the bar-evoked turn (22, 41). Although recent studies showed the existence of cell-type-specific EC-like inputs in <italic>Drosophila</italic> visual neurons (<xref ref-type="bibr" rid="c18">Fenk et al., 2021</xref>; <xref ref-type="bibr" rid="c32">Kim et al., 2017</xref>, <xref ref-type="bibr" rid="c33">2015</xref>), it has not been addressed whether the amplitude of these inputs are fixed or modifiable depending on the amplitude of the turn. Thus, we proposed two EC-based models. In the “graded” EC model (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>, see <xref ref-type="disp-formula" rid="eqn13">Equation 13</xref> in Methods), the amplitude of the EC, ε, is adjusted according to the predicted optic flow feedback (eq.12). An example of such an EC has been characterized in detail in the cerebellum-like structure in the electric fish (<xref ref-type="bibr" rid="c3">Bell, 1981</xref>). On the other hand, in the all-or-none EC model (<xref rid="fig4" ref-type="fig">Fig. 4D</xref>, see <xref ref-type="disp-formula" rid="eqn14">Equation 14</xref> in Methods), the grating response is blocked entirely by EC, η, during the bar-evoked turn regardless of the amplitude of the flight turn. An example for such a type of EC has been reported in the auditory neurons of crickets during stridulation (<xref ref-type="bibr" rid="c47">Poulet and Hedwig, 2006</xref>).</p>
<p>When we simulated the addition-only model, we observed that the virtual fly model reduced the error angle rather slowly, as expected (<xref rid="fig4" ref-type="fig">Fig. 4E</xref>, magenta trace). The slowdown is due to the torque generated by the grating, <italic>F</italic><sub><italic>grtng</italic></sub>, which has the opposite sign to that of the bar-evoked torque, <italic>F</italic><sub><italic>bar</italic></sub> (<xref rid="fig4" ref-type="fig">Fig. 4E</xref>, bottom). In the graded EC model, we observed that the bar response was not impeded (<xref rid="fig4" ref-type="fig">Fig. 4F</xref>, orange trace in the top), and the heading dynamics matched that of the simulation with the empty background (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>). The torque due to the grating, <italic>F</italic><sub><italic>grtng</italic></sub>, remained at zero because the graded EC (<xref rid="fig4" ref-type="fig">Fig. 4F</xref>, dotted black trace in the bottom) counteracted the input to the grating response (<xref rid="fig4" ref-type="fig">Fig. 4F</xref>, solid black trace in the bottom). In the all-or-none EC model, the response to the bar was not dampened either (<xref rid="fig4" ref-type="fig">Fig. 4G</xref>, brown trace in the top) because the grating response was fully suppressed by the EC signal, η, during the turn (<xref rid="fig4" ref-type="fig">Fig. 4G</xref>, dotted brown trace in the bottom).</p>
<p>Our results suggested that the EC can be used to combine multiple visual reflexes and allows for a high-priority sensorimotor circuit (e.g., object tracking/avoidance) to silence low-priority pathways (e.g., visual stability response). We tested our models also in other visual settings and found that the EC-based models exhibit shorter latencies than the addition-only model (<xref rid="figS4" ref-type="fig">Fig. S4</xref>). Our EC-based models solve an important problem for visuomotor integration.</p>
</sec>
<sec id="s2f">
<title>For changing visual environments, the graded EC model requires auto-tuning, but not the all-or-none EC model</title>
<p>Both the EC-based models were shown to disengage the visual stability reflex during object-evoked flight turns. The two models, however, differ in that the graded EC model is capable of responding to subtle mismatches between the predicted and actual background optic flow (<xref rid="figS4" ref-type="fig">Fig. S4C</xref>), whereas the all-or-none EC model should always exhibit the same dynamic regardless of the background. For example, when the visual environment changes suddenly, such as when moving from a forest to a desert (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>), the dynamics of object-evoked flight turns would be unaffected in the all-or-none EC model, whereas significant heading response change would occur in the case of graded EC model.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig. 5.</label>
<caption><title>Behavior of EC-based models in changing visual backgrounds.</title>
<p>(A) Two different visual environments, one with few objects in the background representing a weak visual feedback and the other with many objects such as trees representing a strong visual feedback. (B) A diagram of the auto-tuned, graded EC model, where a multi-layered perceptron (MLP) is used to adapt the EC amplitude to the changing visual feedback. The loss function is computed based on the mean squared error (MSE) between the angular position of the fly when the efference copy amplitude matches that of the visual feedback. (C) Simulation results for a moving bar with a static background over iterations when the visual feedback increases at the 400th iteration. The top plot depicts the change of the feedback amplitude and the EC amplitude, the second panel the MSE, and the third panel the 50% latency of the heading response. The bottom plot show the sample traces before and after the feedback change. (D) Same as in C, but when the feedback decreases at the 400th iteration. (E) Same as in C, but for the all-or-none EC model.</p></caption>
<graphic xlink:href="561122v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>This also suggests that the graded EC model is required to update its EC according to changing visual environments, adjusting the prediction of reafferent inputs adaptively. Such modifiable ECs were reported in the electric fish’s cerebellum-like structure (<xref ref-type="bibr" rid="c30">Kennedy et al., 2014</xref>). We thus implemented an algorithm by which the amplitude of EC is auto-tuned by a multi-layer perceptron (MLP). The MLP generate the EC amplitude, <italic>a</italic>, as a prediction for the current feedback amplitude, <italic>V</italic><sub><italic>grating</italic></sub>. In this model, the tracking error during each iteration of the visuomotor response was calculated as a loss function, and the weight parameters in the MLP were updated in a direction minimizing this error (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>, see <xref ref-type="disp-formula" rid="eqn15">Equation 15</xref> in Methods).</p>
<p>Using this model, we simulated a moving bar over a static grating for 2000 iterations, where the background changes from a sparse optic flow to a dense one at the 400th iteration (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>). We observed that the response of the virtual fly model to the bar pattern slowed down significantly when the background changed, and as a result the error and the latency increased significantly (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>). Over iterations, the EC amplitude increased toward the new feedback amplitude, and the latency decreased toward the initial value (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>, bottom two rows). When the background changed from the dense to sparse optic flow, we observed the opposite behavior (<xref rid="fig5" ref-type="fig">Fig. 5D</xref>). For the all-or-none EC model, however, the background change did not result in any change in the flight turn dynamics, as expected (<xref rid="fig5" ref-type="fig">Fig. 5E</xref>).</p>
</sec>
<sec id="s2g">
<title>Behavioral evidence of a visual efference copy in flying <italic>Drosophila</italic></title>
<p>To determine whether flying <italic>Drosophila</italic> use an EC-based mechanism and also to dissociate between the two EC-based models, we replicated our simulation in the behavioral experiment using the magnetically tethered flight assay. In particular, we measured the amplitude of the orientation response to the moving vertical bar against two different random dot backgrounds that differ in their densities (<xref rid="fig6" ref-type="fig">Fig. 6A</xref> and Movie S2). We first tested how different the visual stability responses are between the two random dot patterns. As expected, the body orientation change was considerably larger with the dense background compared to the sparse background (<xref rid="fig6" ref-type="fig">Fig. 6B</xref>). Note also that the response amplitude to the dense-dot background was comparable to that of the grating motion that we observed previously (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>). This indicates that, unless the stability reflex is suppressed during the flies’ object-evoked turns, the turn should slow down more strongly with the dense background than the sparse one.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Fig. 6.</label>
<caption><title>Dynamics of object-evoked flight turns were not affected by the background pattern.</title>
<p>(A) Temporal profile of the visual stimulus used to test the bar response when the background changes from the uniform to the dense-dot background. In each frame, the horizontal midline of the display is sampled and plotted over time (top). Sample pattern images at the moment of the bar movement onset (bottom middle). (B) Body angle changes to the rotation of the random dot backgrounds in two different densities (left). The amplitude is significantly larger for the dense background than the sparse one (right, Wilcoxon rank sum test). (C) Average body orientation traces in response to horizontally moving bars when the initial uniform, bright background is kept the same or changed to one of the two random-dot backgrounds. The thick colored lines in the top plots represent the population average, whereas the gray lines represent an average for individual flies. Box-and-whisker plots depict the amplitude (middle) and latency (bottom) of the body angle change. Error bars (bottom) indicate 95% confidence interval. (D) Same as in (C), but for different combinations of background patterns. (E) Schematic of the stimulus protocol for testing adaptation in the bar response with a maintained background. The background was maintained in either sparse or dense dot patterns for 7 minutes (“priming phase”) and then changed to a different density. After this change, the same background pattern was maintained for 14 minutes (“test phase” #1 and #2). (F) Population-averaged body angle traces in response to a moving bar in three different phases.</p></caption>
<graphic xlink:href="561122v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We then assessed the fly’s bar-evoked flight turns with changing visual background (<xref rid="fig6" ref-type="fig">Fig. 6A,C</xref>), as in the simulation (<xref rid="fig5" ref-type="fig">Fig. 5C-E</xref>). That is, we ran the “alignment” and “ready” phase with a uniform, bright background and then either kept it the same or changed to one of the two random dot backgrounds at the moment of the bar movement onset (<xref rid="fig6" ref-type="fig">Fig. 6A</xref>). To match the saliency level of the bar pattern the same in two different backgrounds, we kept the foreground visual field (±55° from the center) the same as the initial, uniform background. Surprisingly, we found that the bar response was virtually the same regardless of the background dot density (<xref rid="fig6" ref-type="fig">Fig. 6C</xref>).</p>
<p>To further corroborate this result, we carried out the same experiment, but with the dots filling the entire panoramic visual field and the background changed 0.25 s prior to the bar movement onset (see <xref rid="figS5" ref-type="fig">Fig. S5A</xref> for the depiction of the protocol). We tested flies with four different combinations of background patterns: sparse-to-sparse, sparse-to-dense, dense-to-dense, and dense-to-sparse. We discovered that the amplitude of the flight turn, in terms of both the body orientation and the head yaw angle, was not significantly influenced by the background pattern (<xref rid="fig6" ref-type="fig">Fig. 6D</xref>, <xref rid="figS5" ref-type="fig">Fig. S5B</xref>). Together, these results challenged the addition-only model, which suggested the strong dependency of heading responses on the visual feedback from the background pattern. Furthermore, our results also counter the graded EC model because in this model, the background change at the time of or immediately prior to the bar movement would have caused a mismatch between predicted and actual optic flow feedback, leading to response change, as shown in the simulation (<xref rid="fig5" ref-type="fig">Fig. 5C,D</xref>). We did not observe such changes in the behaving animal. Furthermore, no significant response change was observed even when we repeated the above experiment in response to a looming disc pattern in place of the moving bar (<xref rid="figS5" ref-type="fig">Fig. S5C-E</xref>). That is, the loom-evoked flight turn was not affected either by the change in the background optic flow density.</p>
<p>To further test the possibility of the auto-tuned, graded efference copy, we tested the fly’s bar response against a maintained background for an extended period of flight (∼14 minutes). Prior to this extended exposure to a single background pattern, we primed the fly with a random-dot background in a different density for roughly 7 minutes (<xref rid="fig6" ref-type="fig">Fig. 6E</xref>). The resulting data revealed no significant changes in the flight turn amplitude across the three phases: a priming phase with one background (∼7 min), an early test phase with the other background (∼7 min), and a late test phase with the maintained background (∼7 min) (<xref rid="fig6" ref-type="fig">Fig. 6F</xref>). Altogether, we observed that flies’ object-evoked flight turns were not significantly affected by the density of random dots in the background, at least in the time scale that we could have tested. Altogether, our results suggested that flying <italic>Drosophila</italic> uses the all-or-none EC strategy in complex visual environments (<xref rid="fig4" ref-type="fig">Fig. 4D</xref>), wherein the visual stability response is entirely suppressed during object-evoked turns (<xref rid="fig5" ref-type="fig">Fig. 5E</xref>).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p><italic>Drosophila</italic> visuomotor circuits have been successfully dissected in recent years, but in a fragmented manner for limited sensory and behavioral contexts. To build a framework for quantitatively testing these circuits and their interactions in complex visual environments, we developed an integrative model of visuomotor flight control. In this model, multiple visual features collectively determine flight heading through an EC mechanism. The model accurately predicted the flight orientation changes in response to singly presented patterns as well as to superpositions of those visual patterns. Our results provide definitive behavioral evidence for a visual EC in <italic>Drosophila</italic> and offer a framework for the potential implementation and testing of detailed models of constituent neural circuits.</p>
<sec id="s3a">
<title>Models of <italic>Drosophila</italic> visuomotor processing</title>
<p>Visual systems extract features from the environment by calculating spatiotemporal relationships of neural activities within an array of photoreceptors. In <italic>Drosophila</italic>, these calculations occur initially on a local scale in the peripheral layers of the optic lobe (<xref ref-type="bibr" rid="c24">Gruntman et al., 2018</xref>; <xref ref-type="bibr" rid="c31">Ketkar et al., 2020</xref>). When these features enter the central brain, they are integrated across a large visual field by converging either to a small brain region or to a single cell (<xref ref-type="bibr" rid="c41">Mauss et al., 2015</xref>; <xref ref-type="bibr" rid="c61">Wu et al., 2016</xref>). These features are subsequently relayed to other central brain regions or descending neural circuits, which ultimately control action (<xref ref-type="bibr" rid="c2">Aymanns et al., 2022</xref>; <xref ref-type="bibr" rid="c45">Namiki et al., 2018</xref>). With recent advances in understanding the structural and functional principles of its visuomotor circuits, <italic>Drosophila</italic> presents a unique opportunity to reverse-engineer the entire visuomotor processes. The mechanisms for key visual computations such as direction-dependent motion computation, collision avoidance, stability reflex, object tracking, and navigation are under active investigation, and <italic>in silico</italic> models for some of these sensory computations are already available (<xref ref-type="bibr" rid="c5">Borst and Weber, 2011</xref>; <xref ref-type="bibr" rid="c24">Gruntman et al., 2018</xref>). In the motor end, the neuromechanical interaction between the animal’s actuator and external world was modeled in a recent study for walking (<xref ref-type="bibr" rid="c37">Lobato-Rios et al., 2022</xref>). What remains to be explored is the mapping of the features in the visual space onto a coordinated activation pattern of motor neurons by higher-order brain circuits, in a way to optimize the organism’s objective functions (i.e., survival and reproduction). Our study provides an integrative model of the visuomotor mapping in <italic>Drosophila</italic>. When expanded with neuromechanical models as well as models of visual circuits, the <italic>in silico</italic> model can not only be used for the quantitative understanding of the visuomotor processing but also for building insect-inspired robots (<xref ref-type="bibr" rid="c19">Franceschini et al., 1992</xref>; <xref ref-type="bibr" rid="c60">Webb, 2002</xref>; <xref ref-type="bibr" rid="c62">Yue and Rind, 2005</xref>).</p>
<p>In upcoming studies, our model will be expanded to incorporate additional details of visual behaviors as well as the underlying neural circuits. For instance, recent studies have shown that flies fixate on a rotating bar by interspersing rapid flight turns (or saccades) with stable heading periods, an algorithm referred to as the “integrate-and-saccade” strategy (G. <xref ref-type="bibr" rid="c34">Kim et al., 2023</xref>; <xref ref-type="bibr" rid="c42">Mongeau and Frye, 2017</xref>). The distinction between the integrate-and-saccade strategy and smooth tracking is crucial because it strongly suggests the existence of distinct neural circuits for the two visually guided orientation behaviors. However, when we averaged wingbeat signals across trials to determine the position and velocity functions (<xref rid="fig1" ref-type="fig">Fig. 1</xref>), we effectively approximated the integrate-and-saccade strategy as a smooth pursuit movement. Future experimental and modeling research will refine our current model with behavioral and biophysical details.</p>
</sec>
<sec id="s3b">
<title>Visual efference copy in <italic>Drosophila</italic></title>
<p>Under natural conditions, various visual features in the environment are poised to concurrently activate cognate motor programs, which may interfere with each other through reafferent inputs. It is thus crucial for the central brain to arbitrate the motor signals originating from different sensorimotor circuits. The EC, as a copy of a motor command, participates in this process by modifying or inhibiting specific sensory signaling based on the motor context (<xref ref-type="bibr" rid="c32">Kim et al., 2017</xref>, <xref ref-type="bibr" rid="c33">2015</xref>; <xref ref-type="bibr" rid="c59">von Holst and Mittelstaedt, 1950</xref>). Recent studies reported such EC-like signals in <italic>Drosophila</italic> visual neurons (<xref ref-type="bibr" rid="c32">Kim et al., 2017</xref>, <xref ref-type="bibr" rid="c33">2015</xref>; <xref ref-type="bibr" rid="c59">von Holst and Mittelstaedt, 1950</xref>). One type of EC-like signals were identified in a set of wide-field visual motion-sensing neurons that was shown to control the neck movement for the gaze stability (<xref ref-type="bibr" rid="c32">Kim et al., 2017</xref>). The EC-like signals in these cells were bidirectional depending on the direction of flight turning, and their amplitudes were quantitatively tuned to that of the expected visual input in each cell type. Another type of EC-like signals observed in the <italic>Drosophila</italic> visual system possesses contrasting features. It entirely suppresses visual signaling, irrespective of the direction of the self-generated turn (<xref ref-type="bibr" rid="c33">Kim et al., 2015</xref>). Our behavioral experiments confirmed for the first time that the EC in <italic>Drosophila</italic> vision acts to fully suppress optic flow-induced stability reflexes during visually evoked flight turns. Why would <italic>Drosophila</italic> use the all-or-none EC instead of the graded one? Recent studies on electric fish suggested that a large array of neurons in a multi-layer network are crucial for generating the modifiable efference copy signal matched to the current environment (<xref ref-type="bibr" rid="c44">Muller et al., 2019</xref>). Given their small-sized brain, flies might opt for a more economical design for suppressing unwanted visual inputs regardless of the visual environments. Circuits mediating such a type of EC were identified in the cricket auditory system during stridulation (<xref ref-type="bibr" rid="c47">Poulet and Hedwig, 2006</xref>), for example. Our study strongly suggests the existence of a similar circuit mechanism in the <italic>Drosophila</italic> visual system.</p>
</sec>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Fly stocks and rearing</title>
<p>We used female Oregon-R <italic>Drosophila melanogaster</italic> 2-7 days post-eclosion for the experiments, unless mentioned otherwise. Flies were reared on standard cornmeal agar in 25°C incubators with a 12h-12h light/dark cycle and in 70% humidity. For all experiments, flies were cold-anesthetized at 4°C and tethered either to a tungsten pin for the rigidly tethered experiments (<xref rid="fig1" ref-type="fig">Fig. 1</xref>) or to a steel pin for the magnetically tethered experiments (<xref rid="fig3" ref-type="fig">Figs. 3</xref>,<xref rid="fig6" ref-type="fig">6</xref>).</p>
</sec>
<sec id="s4b">
<title>Visual display</title>
<p>We used a modular LED display system composed of 8x8 dot matrix LED arrays (<xref ref-type="bibr" rid="c49">Reiser and Dickinson, 2008</xref>). The visual stimuli were displayed on the interior of a cylindrical display arena featuring green LED pixels with a 570-nm peak wavelength, facing inward. The display covered 360 degrees (96 pixels) in the azimuth and 94 degrees (32 pixels) in elevation. From the animal’s perspective, each LED pixel subtended less than or equal to 3.75 degrees, which is narrower than the inter-ommatidial angle of the fly (∼5 degrees) (<xref ref-type="bibr" rid="c23">Götz, 1965</xref>).</p>
</sec>
<sec id="s4c">
<title>Magnetic tethering setup</title>
<p>We constructed a magnetic tethering setup following the designs used in previous studies (<xref ref-type="bibr" rid="c16">Duistermars and Frye, 2008</xref>; <xref ref-type="bibr" rid="c32">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="c42">Mongeau and Frye, 2017</xref>). The support frame for the two neodymium magnets was designed in Fusion 360 (Autodesk, Inc.) and manufactured using a 3D printer (Ender 3 S1, Creality) (see <xref rid="fig3" ref-type="fig">Fig. 3(A)</xref>). The bottom magnet had a ring shape with an outer diameter of 32 mm, an inner diameter of 13 mm, and a height of 10 mm. The top magnet was cylinder-shaped with a diameter of 12 mm and a height of 10 mm. The distance between the top of the bottom magnet and the bottom of the top magnet was 25 mm. A V-shaped jewel bearing (Swiss Jewel Company) was attached to the bottom surface of the top magnet using epoxy and was used to anchor a steel pin-tethered fly. We positioned a Prosilica GE-680C camera (Allied Vision Inc.) with a macro lens (Infinistix, 1.5x zoom, 94-mm working distance) on the ground to capture the fly’s behavior from below. To illuminate the fly without interfering with its vision, we affixed four infrared LEDs (850-nm peak wavelength) concentrically to the top surface of the bottom magnet support. Furthermore, we applied matte black acrylic paint (Chroma Inc.) to the bottom surface of the top magnet to minimize background glare in the acquired images and to the top surface of the bottom magnet to minimize visual interference to flies.</p>
</sec>
<sec id="s4d">
<title>Visual stimuli</title>
<p>To estimate the position and velocity functions, we measured flies’ wing responses in response to three visual patterns moving at the speed of 36°/s for 10 seconds (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). The bar pattern was a dark vertical stripe with a width of 15° (4 pixels) on a bright background that moved laterally from the back to back for a full rotation in either clockwise or counterclockwise directions. The spot pattern was a 2x2-pixel dark square on a bright background and moved in the same dynamics as the bar. The grating pattern consisted of 12 cycles of dark and bright vertical stripes, each with a width of 15° (4 pixels). For the magnetically tethered flight experiments (<xref rid="fig3" ref-type="fig">Figs. 3</xref>,<xref rid="fig6" ref-type="fig">6</xref>), the patterns moved rapidly 45 degrees in a sigmoid dynamics. Each pattern consisted of 4 distinct phases: alignment, ready, go, and freeze. The vertical pattern used for the alignment consisted of 3 dark, 2 bright, 1 dark, 2 bright, and 3 dark pixel vertical stripes, spanning a total of 41.25° (11 pixels) in width. To align the orientation of the fly to the reference angle, this pattern moved horizontally in a sinusoidal dynamics for 4-5 seconds at the frequency 1 period/s, with its envelope amplitude decreasing linearly (Movie S2). The bar pattern turned to a dark uniform bar over a bright background with a width of 11.25° (3 pixels) and was used for the rest of the stimulus period. The random dot pattern was created by adding dark pixels at a random position in the display arena. The sparse dot pattern had these dots in 7% of the total pixels, while the dense dot pattern had 40%. To compensate for any asymmetry that may exist in the rigidly tethered flight experiment, each pattern was presented multiple times in two opposing directions. Similarly, in the magnetically tethered flight experiment, we presented the same pattern in two opposing positions, and in the two opposing directions. We inverted all the body orientation signals in the counterclockwise trials and merged them with the clockwise trials.</p>
</sec>
<sec id="s4e">
<title>Behavioral experiment</title>
<p>For the rigidly tethered flight experiments (<xref rid="fig1" ref-type="fig">Fig. 1</xref>), we glued the anterior-dorsal part of the fly’s thorax to a tungsten pin using a UV-cured glue (Bondic). The other end of the pin was inserted into a pin receptacle located at the center of the visual display. The fly body orientation was tilted down from the horizontal by 30° to match the normal flight attitude (<xref ref-type="bibr" rid="c13">David, 1978</xref>). The fly was illuminated by four 850-nm LEDs on the ring-shaped platform positioned on the top of the camera (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>). The fly was imaged from the bottom by a GE-680 camera at 60 frame/s with a macro zoom lens (MLM3X-MP, Computar, NC, USA) and an infrared long-pass filter. Wingbeat signals were analyzed in real-time by FView and motmot package (<xref ref-type="bibr" rid="c54">Straw and Dickinson, 2009</xref>). For the magnetically tethered flight experiments (<xref rid="fig3" ref-type="fig">Figs. 3</xref>,<xref rid="fig6" ref-type="fig">6</xref>), we instead used a short steel pin to levitate the fly by a magnetic field.</p>
</sec>
<sec id="s4f">
<title>Data analysis</title>
<p>For quantifying wing responses from the rigidly tethered flight experiment, we first subtracted the left wingbeat amplitude from the right wingbeat amplitude to calculate the L-R WBA for each recording (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>). For each fly, we determined the stimulus-triggered L-R WBA for every pattern, provided that the number of trials in which the animal continuously flew was at least 6. We then computed the mean L-R WBA across the trials for each fly, and from these individual means, we determined the population-averaged L-R WBA (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). In the magnetically tethered flight experiment, we captured images of the flies at 60 frames per second (<xref rid="fig3" ref-type="fig">Figs. 3</xref>,<xref rid="fig6" ref-type="fig">6</xref>). The camera was externally triggered by an Ubuntu computer via a USB-connected microcontroller board (Teensy, PJRC), and the trigger signal was stored on the Windows computer using WinEDR (University of Strathclyde, Glasgow), along with the stimulus type and position signals.The acquired images, along with their timestamps, were saved on the Ubuntu computer using FView (<xref ref-type="bibr" rid="c54">Straw and Dickinson, 2009</xref>). To analyze body kinematics (<xref rid="fig3" ref-type="fig">Figs. 3</xref>,<xref rid="fig6" ref-type="fig">6</xref>), we wrote a custom machine vision code in Matlab. Namely, we calculated the body angle by binarizing each frame, performing an eroding operation to remove small non-fly objects, and then obtaining the orientation of the largest region using <italic>regionprops()</italic> function in Matlab (Movie S3). To measure the amplitude of the visually evoked responses (<xref rid="fig3" ref-type="fig">Figs 3D</xref>, <xref ref-type="fig" rid="fig6">6B-D</xref>), we subtracted the mean body angle during the 300-ms interval immediately prior to the stimulus onset from the mean in the 300-ms interval starting 200 ms after the onset. The 50% latency was measured as the time at which the body angle crosses the 50% of the total pattern displacement from the baseline, with respect to the pattern movement onset (<xref rid="fig3" ref-type="fig">Figs. 3E</xref>). Additionally, we measured the head angle (<xref rid="figS5" ref-type="fig">Fig. S5B</xref>) by tracking the position of both antennae with respect to the neck (Movie S3) through the deep learning-based software DeepLabCut (<xref ref-type="bibr" rid="c40">Mathis et al., 2018</xref>). The body kinematic variables were stored separately from the stimulus parameters and combined <italic>post hoc</italic> for further analyses. To synchronize these signals, we inserted a 2-s pause in the camera trigger at the beginning and end of each experiment. These no-trigger intervals appeared in both sets of data files and were used to align kinematic variables acquired from the image data to the stimulus parameters.</p>
</sec>
<sec id="s4g">
<title>Statistical analysis</title>
<p>All statistical analysis was performed in Matlab. Heading responses to random dot patterns in the two different densities were analyzed by Wilcoxon rank-sum test (<xref rid="fig6" ref-type="fig">Fig. 6B</xref>). Responses to moving bars (<xref rid="fig6" ref-type="fig">Fig. 6C-D</xref>) and to looming discs (<xref rid="figS5" ref-type="fig">Fig. S5D-E</xref>) for different background profiles were analyzed by one-way ANOVA test.</p>
<sec id="s4g1">
<title>Biomechanics models of flying <italic>Drosophila</italic> (<xref rid="fig2" ref-type="fig">Fig. 2</xref>)</title>
<p>The orientation of the fly is obtained through a second order dynamical system in which the position and velocity components are added up as the input.
<disp-formula id="eqn4">
<graphic xlink:href="561122v1_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where ψ<sub><italic>fly</italic></sub>is the angular position of the fly, <inline-formula><inline-graphic xlink:href="561122v1_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> the angular velocity, <inline-formula><inline-graphic xlink:href="561122v1_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> the angular acceleration, <italic>I</italic> = 6·10<sup>-14</sup> kg m<sup>2</sup> the moment of inertia of the fly’s body, β = 1·10<sup>-11</sup> kg m<sup>2</sup> s<sup>-1</sup> the drag coefficient of its wings (Michael H. Dickinson, 2005; <xref ref-type="bibr" rid="c51">Ristroph et al., 2010</xref>), ψ<sub><italic>e</italic></sub> the error angle between the fly and the pattern, and <inline-formula><inline-graphic xlink:href="561122v1_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> the error velocity.</p>
<p>We approximated the experimentally obtained position and velocity functions with the following simple mathematical representations for each pattern (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>).
<disp-formula id="eqn5">
<graphic xlink:href="561122v1_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn6">
<graphic xlink:href="561122v1_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn7">
<graphic xlink:href="561122v1_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We have defined the dynamical system (eq.4) as a system of ordinary differential equations (ODE), and solved it via Runge-Kutta method 45 through the MATLAB function ode45. We have defined a system of ODE for each visual pattern (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>) as follows:
<disp-formula id="eqn8">
<graphic xlink:href="561122v1_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn9">
<graphic xlink:href="561122v1_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn10">
<graphic xlink:href="561122v1_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn11">
<graphic xlink:href="561122v1_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>n</italic> is the gaussian noise component.</p>
</sec>
<sec id="s4g2">
<title>Addition-only model (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>)</title>
<p>By following the same ODE system approach, we defined the integration of the torque response from the bar system, <italic>F</italic><sub><italic>bar</italic></sub>, and from the grating system, <italic>F</italic><sub><italic>grtng</italic></sub>, additively in the addition-only model as follows.
<disp-formula id="eqn12">
<graphic xlink:href="561122v1_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s4g3">
<title>Graded EC-based model (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>)</title>
<p>In the case of the graded EC-based model, we defined the integration of the torque responses through an additional ODE that produced the same angular velocity value due to the bar, <inline-formula><inline-graphic xlink:href="561122v1_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and it was subtracted from the grating torque.
<disp-formula id="eqn13">
<graphic xlink:href="561122v1_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>ε</italic> is the predicted value of <inline-formula><inline-graphic xlink:href="561122v1_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula>by the EC, and <inline-formula><inline-graphic xlink:href="561122v1_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> its change over time.</p>
</sec>
<sec id="s4g4">
<title>All-or-none EC-based model (<xref rid="fig4" ref-type="fig">Fig. 4D</xref>)</title>
<p>In the all-or-none EC-based strategy, the grating torque, <italic>F</italic><sub><italic>grating</italic></sub>, is switched off to 0 when the bar-evoked torque is non-zero.
<disp-formula id="eqn14">
<graphic xlink:href="561122v1_eqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where η is an indicator function that takes the value of 1 when |<italic>F</italic><sub><italic>bar</italic></sub>| &gt; 1. 5·10<sup>−11</sup> and 0 elsewhere.</p>
</sec>
<sec id="s4g5">
<title>Auto-tuning mechanism for the graded EC-based model (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>)</title>
<p>We added a multi-layer-perceptron (MLP) to the graded EC-based model (eq.13). The output of the MLP is injected into the EC (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>). The equations and pseudocode for these compuations are as follows:</p>
<p><italic>Initialize</italic> MLP wi<italic>th random parameters w</italic><sub><italic>out</italic></sub>, <italic>w</italic><sub><italic>h</italic></sub></p>
<p><italic>for iteration</italic> = 1, <italic>M do</italic>
<disp-formula id="eqn15">
<graphic xlink:href="561122v1_eqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn15a">
<graphic xlink:href="561122v1_eqn15a.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where M = 2000 is the number of iterations, T = 2.5 s is the time range, Δ<italic>V</italic><sub><italic>grating</italic></sub> is the change in the visual feedback, α is the output of the MLP, r<sub>h</sub> is the output of the hidden layer, r<sub>in</sub> is the input, ψ<sub>fly0</sub> is the heading of the fly in non changeable visual feedback conditions, and γ=1·10<sup>-4</sup> is the learning rate.</p>
</sec>
</sec>
<sec id="s4h">
<title>Fly simulator</title>
<p>We have built a graphic user interface (GUI), where the user can simulate the magno-tethered flying fly for the three simple visual patterns, as well as their superpositions (e.g., bar and grating) for the addition-only and EC-based models. The GUI shows the heading and torque traces in real-time, as well as an animated fruit fly in the center of a LED cylinder, which rotates in response to visual patterns (Movie S1). The GUI allows one to choose the pattern to be displayed, and the maximum angular position amplitude of the stimulus. It also permits a recording of the simulation to generate its movie.</p>
</sec>
</sec>
<sec id="s5">
<title>Code availability</title>
<p>The essential code used to generate the primary results and conduct the simulations for this study is available in our GitHub repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/nisl-hyu/flightsim">https://github.com/nisl-hyu/flightsim</ext-link>).</p>
</sec>
<sec id="d1e1562" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1650">
<label>Supplemental Movie1</label>
<media xlink:href="supplements/561122_file03.mp4"/>
</supplementary-material>
<supplementary-material id="d1e1657">
<label>Supplemental Movie2</label>
<media xlink:href="supplements/561122_file04.mp4"/>
</supplementary-material>
<supplementary-material id="d1e1664">
<label>Supplemental Movie3</label>
<media xlink:href="supplements/561122_file05.mp4"/>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We would like to thank all the lab members for the discussion and comments on the manuscript.</p>
<p>This research was supported by the Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) under Grant (No.2020-0-01373, Artificial Intelligence Graduate School Program (Hanyang University)); by Basic Science Research Program, the Bio &amp; Medical Technology Development Program, and Police-Lab 2.0 Program through the National Research Foundation of Korea (NRF) funded by the Korea Government (MSIT) under Grant NRF2020R1A4A1016840, NRF2021M3E5D2A01023888, NRF2022R1A2C2007599, NRF2022M3E5E8081195, and RS-2023-00243032.</p>
</ack>
<sec id="s6">
<title>Author contributions</title>
<p>A.C. and A.J.K. conceived the study and wrote the manuscript. A.C., with input from A.J.K., performed all the simulations. Behavioral experiments were designed by A.C. and A.J.K., carried out by Y.K. and J.P, and analyzed by A.C. and A.J.K.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Ache</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Polsky</surname> <given-names>J</given-names></string-name>, <string-name><surname>Alghailani</surname> <given-names>S</given-names></string-name>, <string-name><surname>Parekh</surname> <given-names>R</given-names></string-name>, <string-name><surname>Breads</surname> <given-names>P</given-names></string-name>, <string-name><surname>Peek</surname> <given-names>MY</given-names></string-name>, <string-name><surname>Bock</surname> <given-names>DD</given-names></string-name>, <string-name><surname>von Reyn</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Card</surname> <given-names>GM</given-names></string-name>. <year>2019</year>. <article-title>Neural basis for looming size and velocity encoding in the Drosophila giant fiber escape pathway</article-title>. <source>Curr Biol</source> <volume>29</volume>:<fpage>1073</fpage>–<lpage>1081</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Aymanns</surname> <given-names>F</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>C-L</given-names></string-name>, <string-name><surname>Ramdya</surname> <given-names>P.</given-names></string-name> <year>2022</year>. <article-title>Descending neuron population dynamics during odor-evoked and spontaneous limb-dependent behaviors</article-title>. <source>eLife</source> <volume>11</volume>:<fpage>e81527</fpage>. doi:<pub-id pub-id-type="doi">10.7554/eLife.81527</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Bell</surname> <given-names>CC</given-names></string-name>. <year>1981</year>. <article-title>An efference copy which is modified by reafferent input</article-title>. <source>Sci N Y NY</source> <volume>214</volume>:<fpage>450</fpage>–<lpage>453</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Bender</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name>. <year>2006</year>. <article-title>Visual stimulation of saccades in magnetically tethered Drosophila</article-title>. <source>J Exp Biol</source> <volume>209</volume>:<fpage>3170</fpage>–<lpage>3182</lpage>. doi:<pub-id pub-id-type="doi">10.1242/jeb.02369</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Borst</surname> <given-names>A</given-names></string-name>, <string-name><surname>Weber</surname> <given-names>F.</given-names></string-name> <year>2011</year>. <article-title>Neural Action Fields for Optic Flow Based Navigation: A Simulation Study of the Fly Lobula Plate Network</article-title>. <source>PLOS ONE</source> <volume>6</volume>:<fpage>e16303</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0016303.t002</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Card</surname> <given-names>G</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name>. <year>2008</year>. <article-title>Visually mediated motor planning in the escape response of Drosophila</article-title>. <source>Curr Biol CB</source> <volume>18</volume>:<fpage>1300</fpage>–<lpage>1307</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2008.07.094</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Cellini</surname> <given-names>B</given-names></string-name>, <string-name><surname>Mongeau</surname> <given-names>J-M.</given-names></string-name> <year>2022</year>. <article-title>Nested mechanosensory feedback actively damps visually guided head movements in Drosophila</article-title>. <source>eLife</source> <volume>11</volume>:<fpage>e80880</fpage>. doi:<pub-id pub-id-type="doi">10.7554/eLife.80880</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Cellini</surname> <given-names>B</given-names></string-name>, <string-name><surname>Mongeau</surname> <given-names>J-M.</given-names></string-name> <year>2020</year>. <article-title>Active vision shapes and coordinates flight motor responses in flies</article-title>. <source>Proc Natl Acad Sci</source> <volume>117</volume>:<fpage>23085</fpage>–<lpage>23095</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1920846117</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Cellini</surname> <given-names>B</given-names></string-name>, <string-name><surname>Salem</surname> <given-names>W</given-names></string-name>, <string-name><surname>Mongeau</surname> <given-names>J-M.</given-names></string-name> <year>2022</year>. <article-title>Complementary feedback control enables effective gaze stabilization in animals</article-title>. <source>Proc Natl Acad Sci</source> <volume>119</volume>:<fpage>e2121660119</fpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.2121660119</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Clark</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Bursztyn</surname> <given-names>L</given-names></string-name>, <string-name><surname>Horowitz</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Schnitzer</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Clandinin</surname> <given-names>TR</given-names></string-name>. <year>2011</year>. <article-title>Defining the computational structure of the motion detector in Drosophila</article-title> <source>Neuron</source> <volume>70</volume>:<fpage>1165</fpage>–<lpage>1177</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2011.05.023</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Collett</surname> <given-names>T.</given-names></string-name> <year>1980</year>. <article-title>Angular tracking and the optomotor response an analysis of visual reflex interaction in a hoverfly</article-title>. <source>J Comp Physiol</source> <volume>140</volume>:<fpage>145</fpage>–<lpage>158</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Currier</surname> <given-names>TA</given-names></string-name>, <string-name><surname>Pang</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Clandinin</surname> <given-names>TR</given-names></string-name>. <year>2023</year>. <article-title>Visual processing in the fly, from photoreceptors to behavior</article-title>. <source>Genetics</source> <volume>224</volume>:<fpage>iyad064</fpage>. doi:<pub-id pub-id-type="doi">10.1093/genetics/iyad064</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>David</surname> <given-names>CT</given-names></string-name>. <year>1978</year>. <article-title>The relationship between body angle and flight speed in free-flying Drosophila</article-title>. <source>Physiol Entomol</source> <volume>3</volume>:<fpage>191</fpage>–<lpage>195</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1365-3032.1978.tb00148.x</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Dickinson Michael</surname> <given-names>H.</given-names></string-name> <year>2005</year>. <article-title>The initiation and control of rapid flight maneuvers in fruit flies</article-title>. <source>Integr Comp Biol</source> <volume>45</volume>:<fpage>274</fpage>–<lpage>281</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Dickinson Michael</surname> <given-names>H.</given-names></string-name> <year>2005</year>. <article-title>The Initiation and Control of Rapid Flight Maneuvers in Fruit Flies1</article-title>. <source>Integr Comp Biol</source> <volume>45</volume>:<fpage>274</fpage>–<lpage>281</lpage>. doi:<pub-id pub-id-type="doi">10.1093/icb/45.2.274</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Duistermars</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Frye</surname> <given-names>M.</given-names></string-name> <year>2008</year>. <article-title>A Magnetic Tether System to Investigate Visual and Olfactory Mediated Flight Control in Drosophila</article-title>. <source>J Vis Exp</source> <volume>1063</volume>. doi:<pub-id pub-id-type="doi">10.3791/1063</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="other"><string-name><surname>Fenk</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Avritzer</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Weisman</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Nair</surname> <given-names>A</given-names></string-name>, <string-name><surname>Randt</surname> <given-names>LD</given-names></string-name>, <string-name><surname>Mohren</surname> <given-names>TL</given-names></string-name>, <string-name><surname>Siwanowicz</surname> <given-names>I</given-names></string-name>, <string-name><surname>Maimon</surname> <given-names>G.</given-names></string-name> <year>2022</year>. <article-title>Muscles that move the retina augment compound eye vision in Drosophila</article-title>. <source>Nature</source>. doi:<pub-id pub-id-type="doi">10.1038/s41586-022-05317-5</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="other"><string-name><surname>Fenk</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Maimon</surname> <given-names>G.</given-names></string-name> <year>2021</year>. <article-title>Suppression of motion vision during course-changing, but not course-stabilizing, navigational turns</article-title>. <source>Curr Biol</source>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Franceschini</surname> <given-names>N</given-names></string-name>, <string-name><surname>Pichon</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Blanes</surname> <given-names>C</given-names></string-name>, <string-name><surname>Brady</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Barlow</surname> <given-names>HB</given-names></string-name>, <string-name><surname>Frisby</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Horridge</surname> <given-names>GA</given-names></string-name>, <string-name><surname>Jeeves</surname> <given-names>MA</given-names></string-name>. <year>1992</year>. <article-title>From insect vision to robot vision</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source> <volume>337</volume>:<fpage>283</fpage>–<lpage>294</lpage>. doi:<pub-id pub-id-type="doi">10.1098/rstb.1992.0106</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Fry</surname> <given-names>SN</given-names></string-name>, <string-name><surname>Sayaman</surname> <given-names>R</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name>. <year>2005</year>. <article-title>The aerodynamics of hovering flight in Drosophila</article-title>. <source>J Exp Biol</source> <volume>208</volume>:<fpage>2303</fpage>–<lpage>2318</lpage>. doi:<pub-id pub-id-type="doi">10.1242/jeb.01612</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="other"><string-name><surname>Götz</surname> <given-names>KG</given-names></string-name>. <year>1987</year>. <article-title>Course-control, metabolism and wing interference during ultralong tethered flight in Drosophila melanogaster</article-title>. <source>J Exp Biol</source>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Götz</surname> <given-names>KG</given-names></string-name>. <year>1968</year>. <article-title>Flight control in Drosophila by visual perception of motion</article-title>. <source>Kybernetik</source> <volume>4</volume>:<fpage>199</fpage>–<lpage>208</lpage>. doi:<pub-id pub-id-type="doi">10.1007/BF00272517</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Götz</surname> <given-names>KG</given-names></string-name>. <year>1965</year>. <article-title>Die optischen Übertragungseigenschaften der Komplexaugen von Drosophila</article-title>. <source>Kybernetik</source> <volume>2</volume>:<fpage>215</fpage>–<lpage>221</lpage>. doi:<pub-id pub-id-type="doi">10.1007/BF00306417</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Gruntman</surname> <given-names>E</given-names></string-name>, <string-name><surname>Romani</surname> <given-names>S</given-names></string-name>, <string-name><surname>Reiser</surname> <given-names>MB</given-names></string-name>. <year>2018</year>. <article-title>Simple integration of fast excitation and offset, delayed inhibition computes directional selectivity in Drosophila</article-title>. <source>Nat Neurosci</source> <volume>21</volume>:<fpage>250</fpage>–<lpage>257</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41593-017-0046-4</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Hassenstein</surname> <given-names>B</given-names></string-name>, <string-name><surname>Reichardt</surname> <given-names>W.</given-names></string-name> <year>1956</year>. <article-title>Systemtheoretische analyse der zeit-, reihenfolgen-und vorzeichenauswertung bei der bewegungsperzeption des rüsselkäfers chlorophanus</article-title>. <source>Z Für Naturforschung</source> <volume>11</volume>:<fpage>513</fpage>–<lpage>524</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Heisenberg</surname> <given-names>M</given-names></string-name>, <string-name><surname>Wolf</surname> <given-names>R.</given-names></string-name> <year>1993</year>. <article-title>The sensory-motor link in motion-dependent flight control of flies</article-title>. <source>Rev Oculomot Res</source> <volume>5</volume>:<fpage>265</fpage>–<lpage>283</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Heisenberg</surname> <given-names>M</given-names></string-name>, <string-name><surname>Wolf</surname> <given-names>R.</given-names></string-name> <year>1988</year>. <article-title>Reafferent control of optomotor yaw torque inDrosophila melanogaster</article-title>. <source>J Comp Physiol A</source> <volume>163</volume>:<fpage>373</fpage>–<lpage>388</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="other"><string-name><surname>Joesch</surname> <given-names>M.</given-names></string-name> <year>2009</year>. <source>Lobula plate tangential cells in Drosophila melanogaster; Response properties, synaptic organization &amp; input channels</source>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Joesch</surname> <given-names>M</given-names></string-name>, <string-name><surname>Schnell</surname> <given-names>B</given-names></string-name>, <string-name><surname>Raghu</surname> <given-names>SV</given-names></string-name>, <string-name><surname>Reiff</surname> <given-names>DF</given-names></string-name>, <string-name><surname>Borst</surname> <given-names>A.</given-names></string-name> <year>2010</year>. <article-title>ON and OFF pathways in Drosophila motion vision</article-title>. <source>Nature</source> <volume>468</volume>:<fpage>300</fpage>–<lpage>304</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature09545</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Kennedy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wayne</surname> <given-names>G</given-names></string-name>, <string-name><surname>Kaifosh</surname> <given-names>P</given-names></string-name>, <string-name><surname>Alviña</surname> <given-names>K</given-names></string-name>, <string-name><surname>Abbott</surname> <given-names>L</given-names></string-name>, <string-name><surname>Sawtell</surname> <given-names>NB</given-names></string-name>. <year>2014</year>. <article-title>A temporal basis for predicting the sensory consequences of motor commands in an electric fish</article-title>. <source>Nat Neurosci</source> <volume>17</volume>:<fpage>416</fpage>–<lpage>422</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Ketkar</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Sporar</surname> <given-names>K</given-names></string-name>, <string-name><surname>Gür</surname> <given-names>B</given-names></string-name>, <string-name><surname>Ramos-Traslosheros</surname> <given-names>G</given-names></string-name>, <string-name><surname>Seifert</surname> <given-names>M</given-names></string-name>, <string-name><surname>Silies</surname> <given-names>M.</given-names></string-name> <year>2020</year>. <article-title>Luminance Information Is Required for the Accurate Estimation of Contrast in Rapidly Changing Visual Contexts</article-title>. <source>Curr Biol</source> <volume>30</volume>:<fpage>657</fpage>–<lpage>669</lpage>.e4. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2019.12.038</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Kim</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Fenk</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Lyu</surname> <given-names>C</given-names></string-name>, <string-name><surname>Maimon</surname> <given-names>G.</given-names></string-name> <year>2017</year>. <article-title>Quantitative Predictions Orchestrate Visual Signaling in Drosophila</article-title>. <source>Cell</source> <volume>168</volume>:<fpage>280</fpage>–<lpage>294</lpage>.e12. doi:<pub-id pub-id-type="doi">10.1016/j.cell.2016.12.005</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Kim</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Fitzgerald</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Maimon</surname> <given-names>G.</given-names></string-name> <year>2015</year>. <article-title>Cellular evidence for efference copy in Drosophila visuomotor processing</article-title>. <source>Nat Neurosci</source> <volume>18</volume>:<fpage>1247</fpage>–<lpage>1255</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Kim</surname> <given-names>G</given-names></string-name>, <string-name><surname>An</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ha</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>AJ</given-names></string-name>. <year>2023</year>. <article-title>A deep learning analysis of Drosophila body kinematics during magnetically tethered flight</article-title>. <source>J Neurogenet</source> <volume>37</volume>:<fpage>47</fpage>–<lpage>56</lpage>. doi:<pub-id pub-id-type="doi">10.1080/01677063.2023.2210682</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Kim</surname> <given-names>H</given-names></string-name>, <string-name><surname>Park</surname> <given-names>H</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>AJ</given-names></string-name>. <year>2023</year>. <article-title>A visuomotor circuit for evasive flight turns in Drosophila</article-title>. <source>Curr Biol</source> <volume>33</volume>:<fpage>321</fpage>–<lpage>335</lpage>.e6. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2022.12.014</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Liu</surname> <given-names>P</given-names></string-name>, <string-name><surname>Sane</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Mongeau</surname> <given-names>J-M</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>J</given-names></string-name>, <string-name><surname>Cheng</surname> <given-names>B.</given-names></string-name> <year>2019</year>. <article-title>Flies land upside down on a ceiling using rapid visually mediated rotational maneuvers</article-title>. <source>Sci Adv</source> <volume>5</volume>:<fpage>eaax1877</fpage>. doi:<pub-id pub-id-type="doi">10.1126/sciadv.aax1877</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Lobato-Rios</surname> <given-names>V</given-names></string-name>, <string-name><surname>Ramalingasetty</surname> <given-names>ST</given-names></string-name>, <string-name><surname>Özdil</surname> <given-names>PG</given-names></string-name>, <string-name><surname>Arreguit</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ijspeert</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Ramdya</surname> <given-names>P.</given-names></string-name> <year>2022</year>. <article-title>NeuroMechFly, a neuromechanical model of adult Drosophila melanogaster</article-title>. <source>Nat Methods</source> <volume>19</volume>:<fpage>620</fpage>–<lpage>627</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41592-022-01466-7</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Maimon</surname> <given-names>G</given-names></string-name>, <string-name><surname>Straw</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name>. <year>2008</year>. <article-title>A simple vision-based algorithm for decision making in flying Drosophila</article-title>. <source>Curr Biol</source> <volume>18</volume>:<fpage>464</fpage>–<lpage>470</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Maisak</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Haag</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ammer</surname> <given-names>G</given-names></string-name>, <string-name><surname>Serbe</surname> <given-names>E</given-names></string-name>, <string-name><surname>Meier</surname> <given-names>M</given-names></string-name>, <string-name><surname>Leonhardt</surname> <given-names>A</given-names></string-name>, <string-name><surname>Schilling</surname> <given-names>T</given-names></string-name>, <string-name><surname>Bahl</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rubin</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Nern</surname> <given-names>A</given-names></string-name>, <string-name><surname>Dickson</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Reiff</surname> <given-names>DF</given-names></string-name>, <string-name><surname>Hopp</surname> <given-names>E</given-names></string-name>, <string-name><surname>Borst</surname> <given-names>A.</given-names></string-name> <year>2013</year>. <article-title>A directional tuning map of Drosophila elementary motion detectors</article-title>. <source>Nature</source> <volume>500</volume>:<fpage>212</fpage>–<lpage>216</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature12320</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Mathis</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mamidanna</surname> <given-names>P</given-names></string-name>, <string-name><surname>Cury</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Abe</surname> <given-names>T</given-names></string-name>, <string-name><surname>Murthy</surname> <given-names>VN</given-names></string-name>, <string-name><surname>Mathis</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Bethge</surname> <given-names>M.</given-names></string-name> <year>2018</year>. <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nat Neurosci</source> <volume>21</volume>:<fpage>1281</fpage>–<lpage>1289</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Mauss</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Pankova</surname> <given-names>K</given-names></string-name>, <string-name><surname>Arenz</surname> <given-names>A</given-names></string-name>, <string-name><surname>Nern</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rubin</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Borst</surname> <given-names>A.</given-names></string-name> <year>2015</year>. <article-title>Neural Circuit to Integrate Opposing Motions in the Visual Field</article-title>. <source>Cell</source> <volume>162</volume>:<fpage>351</fpage>–<lpage>362</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cell.2015.06.035</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Mongeau</surname> <given-names>J-M</given-names></string-name>, <string-name><surname>Frye</surname> <given-names>MA</given-names></string-name>. <year>2017</year>. <article-title>Drosophila Spatiotemporally Integrates Visual Signals to Control Saccades</article-title>. <source>Curr Biol</source> <volume>27</volume>:<fpage>2901</fpage>–<lpage>2914</lpage>.e2. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2017.08.035</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Muijres</surname> <given-names>FT</given-names></string-name>, <string-name><surname>Elzinga</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Melis</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name>. <year>2014</year>. <article-title>Flies evade looming targets by executing rapid visually directed banked turns</article-title>. <source>Science</source> <volume>344</volume>:<fpage>172</fpage>–<lpage>177</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1248955</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Muller</surname> <given-names>SZ</given-names></string-name>, <string-name><surname>Zadina</surname> <given-names>AN</given-names></string-name>, <string-name><surname>Abbott</surname> <given-names>LF</given-names></string-name>, <string-name><surname>Sawtell</surname> <given-names>NB</given-names></string-name>. <year>2019</year>. <article-title>Continual Learning in a Multi-Layer Network of an Electric Fish</article-title>. <source>Cell</source> <volume>179</volume>:<fpage>1382</fpage>–<lpage>1392</lpage>.e10. doi:<pub-id pub-id-type="doi">10.1016/j.cell.2019.10.020</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Namiki</surname> <given-names>S</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name>, <string-name><surname>Wong</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Korff</surname> <given-names>W</given-names></string-name>, <string-name><surname>Card</surname> <given-names>GM</given-names></string-name>. <year>2018</year>. <article-title>The functional organization of descending sensory-motor pathways in Drosophila</article-title>. <source>eLife</source> <volume>7</volume>:<fpage>e10806</fpage>. doi:<pub-id pub-id-type="doi">10.7554/elife.34272</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Poggio</surname> <given-names>T</given-names></string-name>, <string-name><surname>Reichardt</surname> <given-names>W.</given-names></string-name> <year>1973</year>. <article-title>A theory of the pattern induced flight orientation of the fly Musca domestica</article-title>. <source>Kybernetik</source> <volume>12</volume>:<fpage>185</fpage>–<lpage>203</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Poulet</surname> <given-names>JFA</given-names></string-name>, <string-name><surname>Hedwig</surname> <given-names>B.</given-names></string-name> <year>2006</year>. <article-title>The cellular basis of a corollary discharge</article-title>. <source>Sci N Y NY</source> <volume>311</volume>:<fpage>518</fpage>–<lpage>522</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1120847</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Reichardt</surname> <given-names>W</given-names></string-name>, <string-name><surname>Poggio</surname> <given-names>T.</given-names></string-name> <year>1976</year>. <article-title>Visual control of orientation behaviour in the fly: Part I. A quantitative analysis</article-title>. <source>Q Rev Biophys</source> <volume>9</volume>:<fpage>311</fpage>–<lpage>375</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Reiser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name>. <year>2008</year>. <article-title>A modular display system for insect behavioral neuroscience</article-title>. <source>J Neurosci Methods</source> <volume>167</volume>:<fpage>127</fpage>–<lpage>139</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.07.019</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Rimniceanu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Currea</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Frye</surname> <given-names>MA</given-names></string-name>. <year>2023</year>. <article-title>Proprioception gates visual object fixation in flying flies</article-title>. <source>Curr Biol</source> <volume>33</volume>:<fpage>1459</fpage>–<lpage>1471</lpage>.e3. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2023.03.018</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Ristroph</surname> <given-names>L</given-names></string-name>, <string-name><surname>Bergou</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Ristroph</surname> <given-names>G</given-names></string-name>, <string-name><surname>Coumes</surname> <given-names>K</given-names></string-name>, <string-name><surname>Berman</surname> <given-names>GJ</given-names></string-name>, <string-name><surname>Guckenheimer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>ZJ</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>I.</given-names></string-name> <year>2010</year>. <article-title>Discovering the flight autostabilizer of fruit flies by inducing aerial stumbles</article-title>. <source>Proc Natl Acad Sci</source> <volume>107</volume>:<fpage>4820</fpage>–<lpage>4824</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Ryu</surname> <given-names>L</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>SY</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>AJ</given-names></string-name>. <year>2022</year>. <article-title>From Photons to Behaviors: Neural Implementations of Visual Behaviors in Drosophila</article-title>. <source>Front Neurosci</source> <volume>16</volume>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Städele</surname> <given-names>C</given-names></string-name>, <string-name><surname>Keles</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Mongeau</surname> <given-names>J-M</given-names></string-name>, <string-name><surname>Frye</surname> <given-names>MA</given-names></string-name>. <year>2020</year>. <article-title>Non-canonical Receptive Field Properties and Neuromodulation of Feature-Detecting Neurons in Flies</article-title>. <source>Curr Biol</source> <volume>30</volume>:<fpage>2508</fpage>–<lpage>2519</lpage>.e6. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2020.04.069</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Straw</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name>. <year>2009</year>. <article-title>Motmot, an open-source toolkit for realtime video acquisition and analysis</article-title>. <source>Source Code Biol Med</source> <volume>4</volume>:<fpage>5</fpage>. doi:<pub-id pub-id-type="doi">10.1186/1751-0473-4-5</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Takemura</surname> <given-names>S-Y</given-names></string-name>, <string-name><surname>Bharioke</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Nern</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vitaladevuni</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rivlin</surname> <given-names>PK</given-names></string-name>, <string-name><surname>Katz</surname> <given-names>WT</given-names></string-name>, <string-name><surname>Olbris</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Plaza</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Winston</surname> <given-names>P</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>T</given-names></string-name>, <string-name><surname>Horne</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Fetter</surname> <given-names>RD</given-names></string-name>, <string-name><surname>Takemura</surname> <given-names>S</given-names></string-name>, <string-name><surname>Blazek</surname> <given-names>K</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>L-A</given-names></string-name>, <string-name><surname>Ogundeyi</surname> <given-names>O</given-names></string-name>, <string-name><surname>Saunders</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Shapiro</surname> <given-names>V</given-names></string-name>, <string-name><surname>Sigmund</surname> <given-names>C</given-names></string-name>, <string-name><surname>Rubin</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Scheffer</surname> <given-names>LK</given-names></string-name>, <string-name><surname>Meinertzhagen</surname> <given-names>IA</given-names></string-name>, <string-name><surname>Chklovskii</surname> <given-names>DB</given-names></string-name>. <year>2013</year>. <article-title>A visual motion detection circuit suggested by Drosophila connectomics</article-title> <source>Nature</source> <volume>500</volume>:<fpage>175</fpage>–<lpage>181</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature12450</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Tammero</surname> <given-names>LF</given-names></string-name>, <string-name><surname>Frye</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name>. <year>2004</year>. <article-title>Spatial organization of visuomotor reflexes in Drosophila</article-title>. <source>J Exp Biol</source> <volume>207</volume>:<fpage>113</fpage>–<lpage>122</lpage>. doi:<pub-id pub-id-type="doi">10.1242/jeb.00724</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="other"><string-name><surname>Tanaka</surname> <given-names>R</given-names></string-name>, <string-name><surname>Clark</surname> <given-names>DA</given-names></string-name>. <year>2020</year>. <article-title>Object-Displacement-Sensitive Visual Neurons Drive Freezing in Drosophila</article-title>. <source>Curr Biol CB</source> <fpage>1</fpage>–<lpage>28</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2020.04.068</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="other"><string-name><surname>von Holst</surname> <given-names>E</given-names></string-name>, <string-name><surname>Mittelstaedt</surname> <given-names>H.</given-names></string-name> <year>1971</year>. <article-title>The principle of reafference: Interactions between the central nervous system and the peripheral organs</article-title>. <source>Percept Process Stimul Equiv Pattern Recognit</source> <fpage>41</fpage>–<lpage>72</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>von Holst</surname> <given-names>E</given-names></string-name>, <string-name><surname>Mittelstaedt</surname> <given-names>H.</given-names></string-name> <year>1950</year>. <article-title>The principle of reafference: Interactions between the central nervous system and the peripheral organs</article-title>. <source>Naturwissenschaften</source> <volume>37</volume>:<fpage>464</fpage>–<lpage>476</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><surname>Webb</surname> <given-names>B.</given-names></string-name> <year>2002</year>. <article-title>Robots in invertebrate neuroscience</article-title>. <source>Nature</source> <volume>417</volume>:<fpage>359</fpage>–<lpage>363</lpage>. doi:<pub-id pub-id-type="doi">10.1038/417359a</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><string-name><surname>Wu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Nern</surname> <given-names>A</given-names></string-name>, <string-name><surname>Williamson</surname> <given-names>WR</given-names></string-name>, <string-name><surname>Morimoto</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Reiser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Card</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Rubin</surname> <given-names>GM</given-names></string-name>. <year>2016</year>. <article-title>Visual projection neurons in the Drosophila lobula link feature detection to distinct behavioral programs</article-title>. <source>eLife</source> <volume>5</volume>:<fpage>e21022</fpage>. doi:<pub-id pub-id-type="doi">10.7554/eLife.21022</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="other"><string-name><surname>Yue</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rind</surname> <given-names>FC</given-names></string-name>. <year>2005</year>. <article-title>A Collision Detection System for a Mobile Robot Inspired by the Locust Visual SystemProceedings of the 2005 IEEE International Conference on Robotics and Automation</article-title>. <source>Presented at the Proceedings of the 2005 IEEE International Conference on Robotics and Automation</source>. pp. <fpage>3832</fpage>–<lpage>3837</lpage>. doi:<pub-id pub-id-type="doi">10.1109/ROBOT.2005.1570705</pub-id></mixed-citation></ref>
</ref-list>
<sec>
<fig id="figS1" position="float" fig-type="figure">
<label>Fig. S1.</label>
<caption><title>(Related to <xref rid="fig1" ref-type="fig">Fig. 1</xref>) An expanded model predicts the history-dependent dynamics in the wing responses.</title>
<p>(A) Schematic of an expanded flight control model with a motor-wing dynamics and a fatigue block. (B) A heat map indicating the errors in the grating responses for different combinations of the fatigue block parameters. (C) Visually evoked wing responses to the three rotating patterns and the predictions of the two models. (D) Position and velocity responses of the simple and expanded models.</p></caption>
<graphic xlink:href="561122v1_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Fig. S2.</label>
<caption><title>(Related to <xref rid="fig1" ref-type="fig">Fig. 1</xref>) Construction of flight control models for Canton S flies.</title>
<p>(A) Schematic of the experimental setup (left), a frame captured by the infrared camera (middle), and a simplified schematic of the experimental setup (right). The annulus surrounding the fly schematic represents the visual display viewed from above. (B) L-R WBA traces of a sample fly in response to the bar and spot visual patterns. The thick black lines indicate the average of all trials (top) or the average of all flies (bottom). Thin gray lines indicate individual trials (top) or fly averages (bottom). (C) Schematic of the position-velocity-based flight control model. (D) Position and velocity functions estimated from wing responses in C. Light purple shading indicates 95% confidence interval.</p></caption>
<graphic xlink:href="561122v1_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Fig. S3.</label>
<caption><title>(Related to <xref rid="fig2" ref-type="fig">Fig. 2</xref>) The response latency of the flight control model.</title>
<p>(A) Same as in <xref rid="fig2" ref-type="fig">Fig. 2A</xref> with an additional delay block. (B) Same as in <xref rid="fig2" ref-type="fig">Fig. 2C</xref> comparing delayed and non-delayed heading of the virtual fly model for each visual pattern. (C) Same as in <xref rid="fig2" ref-type="fig">Fig. 2C</xref> for different peak amplitudes of the sigmoid-like pattern trajectory and different frequencies of the sine-like trajectory. Plots on the right show the change of delay with respect to the stimulus amplitude (top) and the frequency of the sine stimulus (bottom two).</p></caption>
<graphic xlink:href="561122v1_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Fig. S4.</label>
<caption><title>(Related to <xref rid="fig4" ref-type="fig">Fig. 4</xref>) Orientation behaviors of the two efference copy-based models for different stimulus conditions.</title>
<p>(A) Diagrams of the graded and all-or-none EC models. (B) The change in the EC signals with respect to the bar evoked flight torque. (C) Heading and torque responses to four different stimulus conditions: a moving bar in the first step, a moving bar and grating leftwards in the second step, a moving bar rightwards and grating leftwards in the third step, and a moving grating in the fourth step. (D) Bar plots indicating the amplitude of the response and the latency with respect to the stimulus onset, measured at the 50% point of the pattern movement for the different stimulus combinations. Dotted lines were added to facilitate the comparison across conditions. The all-or-none EC model exhibited less variability than the graded EC model.</p></caption>
<graphic xlink:href="561122v1_figS4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Fig. S5.</label>
<caption><title>(Related to <xref rid="fig6" ref-type="fig">Fig. 6</xref>) Unexpected changes of the background pattern did not affect loom-evoked behavioral responses.</title>
<p>(A) Temporal change of the visual stimulus profile along the horizontal midline of the display. Body angle responses are shown in <xref rid="fig6" ref-type="fig">Fig. 6D</xref>. (B) Head angle responses to the stimulus profile in A with four different combinations of background patterns. The head angle was calculated from DeepLabCut. (C) On the left, schematic of the experimental display showing the initial position of the loom stimulus. On the right, stimulus profile and background combinations. (D) Body angle responses to the stimulus profile in C, for the different background combinations. (E) Same as in D for different background combinations.</p></caption>
<graphic xlink:href="561122v1_figS5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><bold>Movie S1 (Related to <xref rid="fig2" ref-type="fig">Fig. 2</xref>)</bold>. A movie showing a simulation using the GUI fly simulator. The simulation corresponds to a moving bar stimulus. On the left the virtual fly model animation, and the simulated LED display, on the right the plots showing the bar position, body angle, and the torque.</p>
<p><bold>Movie S2 (Related to <xref rid="fig6" ref-type="fig">Fig. 6</xref>)</bold>. A movie showing visual patterns used for testing object-evoked flight turns in changing backgrounds.</p>
<p><bold>Movie S3 (Related to <xref rid="fig3" ref-type="fig">Figs. 3</xref></bold>,<bold><xref ref-type="fig" rid="fig6">6</xref>)</bold>. A fly movie corresponding to an experimental trial in the magno-tethering setup showing the original frame, the processed frame to obtain the body angle, and the frame where the body parts are tracked via DeepLabCut. In the plots below the frames, the stimulus position, body orientation, and head angle are shown respectively. The motion of the body, and head are synchronized with the video. The video was captured at 60 frames per second.</p>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93487.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Tuthill</surname>
<given-names>John C</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Washington</institution>
</institution-wrap>
<city>Seattle</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study investigates the implementation of an efference copy mechanism in the visual flight control system of Drosophila, a topic of broad interest to sensorimotor neuroscientists. Although the behavioral data and computational analysis are <bold>solid</bold>, the lack of physiological data, as well as the absence of flight saccades in the model, provide <bold>incomplete</bold> support for the paper's conclusions.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93487.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
The manuscript &quot;Drosophila Visuomotor Integration: An Integrative Model and Behavioral Evidence of Visual Efference Copy&quot; provides an integrative model of the visuomotor control in Drosophila melanogaster. This model presents an experimentally derived model based on visually evoked wingbeat pattern recordings of three strategically selected visual stimulus types with well-established behavioral response characteristics. By testing variations of these models, the authors demonstrate that the virtual model behavior can recapitulate the recorded wing beat behavioral results and those recorded by others for these specific stimuli when presented individually. Yet, the novelty of this study and their model is that it allows predictions for natural visual scenes in which multiple visual stimuli occur simultaneously and may have opposite or enhancing effects on behavior. Testing three models that would allow interactions of these visual modalities, the authors show that using a visual efference copy signal allows visual streams to interact, replicating behavior recorded when multiple stimuli are presented simultaneously. Importantly, they validated the prediction of this model in real flies using magnetically tethered flies, e.g., presenting moving bars with varying backgrounds. In conclusion, the presented manuscript presents a commendable effort in developing and demonstrating the validity of a mixture model that allows predictions of the behavior of Drosophila in natural visual environments.</p>
<p>Strengths:</p>
<p>
Overall, the manuscript is well-structured and clear in its presentation, and the modeling and experimental research are methodically conducted and illustrated in visually appealing and easy-to-understand figures and their captions.</p>
<p>The manuscript employs a thorough, logical approach, combining computational modeling with experimental behavioral validation using magnetically tethered flies. This iterative integration of simulation and empirical behavioral evidence enhances the credibility of the findings.</p>
<p>The associated code base is well documented and readily produces all figures in the document.</p>
<p>Suggestions:</p>
<p>
However, while the experiments provide evidence for the use of a visual efference copy, the manuscript would be even more impressive if it presented specific predictions for the neural implementation or even neurophysiological data to support this model. Or, at the very least, a thorough discussion. Nonetheless, these models and validating behavioral experiments make this a valuable contribution to the field; it is well executed and addresses a significant gap in the modeling of fly behavior and holistic understanding of visuomotor behaviors.</p>
<p>Here are a few points that should be addressed:</p>
<p>
1. The biomechanics block (Figure 2) should be elaborated on, to explain its relevance to behavior and relation to the underlying neural mechanisms.</p>
<p>
2. It is unclear how the three integrative models with different strategies were chosen or what relevance they have to neural implementation. This should be explained and/or addressed.</p>
<p>
3. There should be a discussion of how the visual efference could be represented in the biological model and an evaluation of the plausibility and alternatives.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93487.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>It has been widely proposed that the neural circuit uses a copy of motor command, an efference copy, to cancel out self-generated sensory stimuli so that intended movement is not disturbed by the reafferent sensory inputs. However, how quantitatively such an efference copy suppresses sensory inputs is unknown. Here, Canelo et al. tried to demonstrate that an efference copy operates in an all-or-none manner and that its amplitude is independent of the amplitude of the sensory signal to be suppressed. Understanding the nature of such an efference copy is important because animals generally move during sensory processing, and the movement would devastatingly distort that without a proper correction. The manuscript is concise and written very clearly. However, experiments do not directly demonstrate if the animal indeed uses an efference copy in the presented visual paradigms and if such a signal is indeed non-scaled. As it is, it is not clear if the suppression of behavioral response to the visual background is due to the act of an efference copy (a copy of motor command) or due to an alternative, more global inhibitory mechanism, such as feedforward inhibition at the sensory level or attentional modulation. To directly uncover the nature of an efference copy, physiological experiments are necessary. If that is technically challenging, it requires finding a behavioral signature that unambiguously reports a (copy of) motor command and quantifying the nature of that behavior.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93487.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
Canelo et al. used a combination of mathematical modeling and behavioral experiments to ask whether flies use an all-or-none EC model or a graded EC model (in which the turn amplitude is modulated by wide-field optic flow). Particularly, the authors focus on the bar-ground discrimination problem, which has received significant attention in flies over the last 50-60 years. First, they use a model by Poggio and Reichardt to model flight response to moving small-field bars and spots and wide-field gratings. They then simulate this model and compare simulation results to flight responses in a yaw-free tether and find generally good agreement. They then ask how flies may do bar-background discrimination (i.e. complex visual environment) and invoke different EC models and an additive model (balancing torque production due to background and bar movement). Using behavioral experiments and simulation supports the notion that flies use an all-or-none EC since flight turns are not influenced by the background optic flow. While the study is interesting, there are major issues with the conceptual framework.</p>
<p>Strengths:</p>
<p>
- They ask a significant question related to efference copies during volitional movement.</p>
<p>
- The methods are well detailed and the data (and statistics) are presented clearly.</p>
<p>
- The integration of behavioral experiments and mathematical modeling of flight behavior.</p>
<p>
- The figures are overall very clear and salient.</p>
<p>Weaknesses:</p>
<p>
- Omission of saccades: While the authors ask a significant question related to the mechanism of bar-ground discrimination, they fail to integrate an essential component of the Drosophila visuomotor responses: saccades. Indeed, the Poggio and Reichardt model, which was developed almost 50 years ago, while appropriate to study body-fixed flight, has a severe limitation: it does not consider saccades. The authors identify this major issue in the Discussion by citing a recent switched, integrate-and-fire model (Mongeau &amp; Frye, 2017). The authors admit that they &quot;approximated&quot; this model as a smooth pursuit movement. However, I disagree that it is an approximation; rather it is an omission of a motor program that is critical for volitional visuomotor behavior. Indeed, saccades are the main strategy by which Drosophila turn in free flight and prior to landing on an object (i.e. akin to a bar), as reported by the Dickinson group (Censi et al., van Breugel &amp; Dickinson [not cited]). Flies appear to solve the bar-ground discrimination problem by switching between smooth movement and saccades (Mongeau &amp; Frye, 2017; Mongeau et al., 2019 [not cited]). Thus, ignoring saccades is a major issue with the current study as it makes their model disconnected from flight behavior, which has been studied in a more natural context since the work of Poggio.</p>
<p>Critically, recent work showed that a group of columnar neurons (T3) appear specialized for saccadic bar tracking through integrate-and-fire computations, supporting the notion of parallel visual circuits for saccades and smooth movement (Frighetto &amp; Frye, 2023 [not cited]).</p>
<p>A major theme of this work is bar fixation, yet recent work showed that in the presence of proprioceptive feedback, flies do not actually center a bar (Rimniceanu &amp; Frye, 2023). Furthermore, the same study found that yaw-free flies do not smoothly track bars but instead generate saccades. Thus prior work is in direct conflict with the work here. This is a major issue that requires more engagement by the authors.</p>
<p>- Relevance of the EC model: EC-related studies by the authors linked cancellation signals to saccades (Kim et al, 2014 &amp; 2017). Puzzlingly, the authors applied an EC model to smooth movement, when the authors' own work showed that smooth course stabilizing flight turns do not receive cancellation signals (Fenk et al., 2021). Thus, in Fig. 4C, based on the state of the field, the efference copy signal should originate from the torque commands to initiate saccades, and not from torque to generate smooth movement. As this group previously showed, cancellation signals are quantitatively tuned to that of the expected visual input during saccades. Importantly, this tuning would be to the anticipated saccadic turn optic flow. Thus the authors' results supporting an all-or-none model appear in direct conflict with the author's previous work. Further, the addition-only model is not particularly helpful as it has been already refuted by behavioral experiments (Rimneceanu &amp; Frye, Mongeau &amp; Frye).</p>
<p>- Behavioral evidence for all-or-none EC model: The authors state &quot;unless the stability reflex is suppressed during the flies' object evoked turns, the turns should slow down more strongly with the dense background than the sparse one&quot;. This hypothesis is based on the fact that the optomotor response magnitude is larger with a denser background, as would be predicted by an EMD model (because there are more pixels projected onto the eye). However, based on the authors' previous work, the EC should be tuned to optic flow and thus the turning velocity (or amplitude). Thus the EC need not be directly tied to the background statistics, as they claim. For instance, I think it would be important to distinguish whether a mismatch in reafferent velocity (optic flow) links to distinct turn velocities (and thus position). This would require moving the background at different velocities (co- and anti-directionally) at the onset of bar motion. Overall, there are alternative hypotheses here that need to be discussed and more fully explored (as presented by Bender &amp; Dickinson and in work by the Maimon group).</p>
</body>
</sub-article>
</article>