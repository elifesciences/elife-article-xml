<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">89836</article-id>
<article-id pub-id-type="doi">10.7554/eLife.89836</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89836.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Attention Modulates Human Visual Responses to Objects by Tuning Sharpening</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5775-6595</contrib-id>
<name>
<surname>Doostani</surname>
<given-names>Narges</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2068-7316</contrib-id>
<name>
<surname>Hossein-Zadeh</surname>
<given-names>Gholam-Ali</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4190-6071</contrib-id>
<name>
<surname>Cichy</surname>
<given-names>Radoslaw Martin</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1830-2501</contrib-id>
<name>
<surname>Vaziri-Pashkam</surname>
<given-names>Maryam</given-names>
</name>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>School of Cognitive Sciences, Institute for Research in Fundamental Sciences</institution>, Tehran, <country>Iran</country></aff>
<aff id="a2"><label>2</label><institution>School of Electrical Engineering, University of Tehran</institution>, Tehran, <country>Iran</country></aff>
<aff id="a3"><label>3</label><institution>Department of Education and Psychology, Freie Universität Berlin</institution>, Berlin, <country>Germany</country></aff>
<aff id="a4"><label>4</label><institution>Berlin School of Mind and Brain, Faculty of Philosophy, Humboldt-Universität zu Berlin</institution>, Berlin, <country>Germany</country></aff>
<aff id="a5"><label>5</label><institution>Bernstein Center for Computational Neuroscience Berlin</institution>, Berlin, <country>Germany</country></aff>
<aff id="a6"><label>6</label><institution>Laboratory of Brain and Cognition, National Institute of Mental Health</institution>, Bethesda, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Peelen</surname>
<given-names>Marius V</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Radboud University Nijmegen</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Moore</surname>
<given-names>Tirin</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University, Howard Hughes Medical Institute</institution>
</institution-wrap>
<city>Stanford</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Corresponding Authors: Narges Doostani, Maryam Vaziri-Pashkam Email: <email>narges.doostani.d@gmail.com</email>, <email>maryam.vaziri-pashkam@nih.gov</email></corresp>
<fn fn-type="others"><p>The authors declare no competing interests.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-09-12">
<day>12</day>
<month>09</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP89836</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-06-22">
<day>22</day>
<month>06</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-06-05">
<day>05</day>
<month>06</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.06.01.543205"/>
</event>
</pub-history>
<permissions>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">
<ali:license_ref>https://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref>
<license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-89836-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Visual stimuli compete with each other for cortical processing and attention biases this competition in favor of the attended stimulus. How does the relationship between the stimuli affect the strength of this attentional bias? Here, we used functional MRI to explore the effect of target-distractor similarity in neural representation on attentional modulation in the human visual cortex using univariate and multivariate pattern analyses. Using stimuli from four object categories (human bodies, cats, cars and houses), we investigated attentional effects in the primary visual area V1, the object-selective regions LO and pFs, the body-selective region EBA, and the scene-selective region PPA. We demonstrated that the strength of the attentional bias towards the target is not fixed but decreases with increasing distractor-target similarity. Simulations provided evidence that this result pattern is explained by tuning sharpening rather than an increase in gain. Our findings provide a mechanistic explanation for behavioral effects of target-distractor similarity on attentional biases and suggest tuning sharpening as the underlying mechanism in object-based attention.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>object-based attention</kwd>
<kwd>biased competition</kwd>
<kwd>target-distractor similarity</kwd>
<kwd>tuning sharpening</kwd>
<kwd>response gain</kwd>
<kwd>human visual cortex</kwd>
<kwd>fMRI</kwd>
</kwd-group>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Everyday visual scenes typically contain a large number of stimuli. Since processing all the incoming information is impossible due to the brain’s limited neural resources, different stimuli compete for cortical representation and processing <sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c6">6</xref></sup>. This competition can be biased by the top-down signal of attention to enhance the parts of input that are most relevant to the task at hand <sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c7">7</xref></sup>. Evidence from electrophysiology and fMRI studies have demonstrated the role of attention in biasing the competition by enhancing the response related to the attended stimulus <sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c6">6</xref></sup> by approximately 30% compared to its response when unattended, in both electrophysiology studies of the monkey brain <sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c11">11</xref></sup> and fMRI studies of the human brain <sup><xref ref-type="bibr" rid="c6">6</xref></sup>.</p>
<p>Competition and attentional bias likely depend on the nature of the visual scenes rather than being universally uniform. Behavioral studies indicate that the competition between stimuli is contentdependent <sup><xref ref-type="bibr" rid="c12">12</xref></sup>, with higher competition between stimuli that are located closer to each other <sup><xref ref-type="bibr" rid="c13">13</xref></sup>, or between stimuli with more similar cortical representation patterns <sup><xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c14">14</xref></sup>. This suggests that the attentional bias might also be affected by the relationship between the competing stimuli, such as the similarity of their cortical representation. Further, behavioral studies on the effect of target-distractor similarity on performance have proposed that lower performance for more similar target-distractor pairs is due to the fact that the neural resources needed for detailed processing are shared to a greater extent <sup><xref ref-type="bibr" rid="c12">12</xref></sup>. However, a direct neuroscientific investigation of how target-distractor similarity affects visual representations, and a mechanistic explanation of how shared resources affect attentional biases is missing.</p>
<p>Here, we investigated the impact of similarity in cortical representation on attentional bias and the underlying mechanism with empirical and theoretical tools. First, using functional MRI and unias well as multivariate analysis, we investigated how the top-down effect of attention varies as target-distractor similarity changes for multiple presented objects. Specifically, we found that the strength of the attentional bias towards the target decreases with increasing target-distractor similarity in cortical representation.</p>
<p>Second, using simulations of neuronal populations we determined how this effect arises from attentional enhancement of neural responses. We considered two known mechanisms through which attention affects neural firing rate: response gain and tuning sharpening. The response gain model predicts a multiplicative scaling of responses through which neural responses are increased by a gain factor <sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup>. The tuning sharpening model instead proposes that attentional enhancement depends on the neuronal tuning for the attended stimulus, leading to an increase in response for optimal stimuli, and little change in response or at times even response suppression for non-optimal stimuli <sup><xref ref-type="bibr" rid="c17">17</xref></sup>. We find that the empirically-observed relationship between attentional enhancement and target-distractor similarity are predicted by the tuning sharpening model, but not the response gain model.</p>
<p>Together, our results show that attentional enhancement is dependent on the similarity between the target and the distractor in neural representation, and a more similar distractor causes the target to receive less attentional bias in the competition. Moreover, these results suggest tuning sharpening as the underlying mechanism of attentional enhancement during object-based attention.</p>
</sec>
<sec id="s2">
<title>Materials and Methods</title>
<sec id="s2a">
<title>Main experiment</title>
<sec id="s2a1">
<title>Participants</title>
<p>17 healthy human participants (9 females, age: mean ±s.d. = 29.29±4.5 years) with normal or corrected-to-normal vision took part in the study. Participants gave written consent and received payment for their participation in the experiment. Data collection was approved by the Ethics Committee of the Institute for Research in Fundamental Sciences, Tehran.</p>
<p>The behavioral data for two participants was not correctly saved during the scanning due to technical problems. While we used the fMRI data of these two participants, all behavioral reports include the performance of the 15 participants for whom the behavioral data was properly saved.</p>
</sec>
<sec id="s2a2">
<title>Stimulus set and experimental design</title>
<p>To determine the effect of target-distractor similarity on attentional modulation, we used object stimuli from four categories (human bodies, cars, houses, and cats). We presented stimuli from each category in semi-transparent form, either in isolation (isolated conditions), or paired with stimuli from another category (paired conditions). Thus, the experiment consisted of 16 conditions: 4 isolated conditions in which isolated stimuli from one of the four categories were presented, and 12 paired conditions (6 category pairs ×2 attentional targets for each pair) in which a target stimulus from the cued category was superimposed with a distractor stimulus from another category for all category combinations. <xref rid="fig1" ref-type="fig">Figure 1B</xref> depicts all stimulus conditions. We used isolated conditions to assess the similarity between different categories, and paired conditions to determine the effect of similarity in a category pair on attentional modulation.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Stimuli, paradigm and regions of interest.</title>
<p>(A) Top images represent the four categories used in the main experiment: body, car, house and cat. The stimulus set consisted of 10 exemplars from each category (here: cats), with exemplars differing in pose and 3D-orientation. (B) The experimental design comprised 16 task conditions (12 paired, 4 isolated). The 4×4 matrix on the left illustrates the 12 paired conditions, with the to-be-attended category (outlined in orange for illustration purposes, not present in the experiment) on the y-axis and the to-be-ignored category on the x-axis. The right column illustrates the four isolated conditions. (C) Experimental paradigm. A paired block is depicted with superimposed body and house stimuli. In this example block, house stimuli were cued as target, and the participant responded on the repetition of the exact same house in two consecutive trials, as marked here by the arrow. (D) Regions of interest for an example participant; the primary visual cortex V1, the object-selective regions LO and pFs, the body-selective region EBA, and the scene-selective region PPA.</p></caption>
<graphic xlink:href="543205v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The stimulus set consisted of gray-scaled images from the four object categories of human bodies, cats, cars and houses, similar to stimuli used in previous studies <sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c20">20</xref></sup>. Each category consisted of 10 exemplars all varying in identity, 3D-orientation (for houses and cars), and pose (for bodies and cats, see <xref rid="fig1" ref-type="fig">Figure 1A</xref>).</p>
<p>Images were presented in a gray background square presented at the center of the screen, subtending 10.2° of visual angle. A red fixation point subtending 0.45° of visual angle was presented at the center of the screen throughout the run (<xref rid="fig1" ref-type="fig">Figure 1C</xref>).</p>
</sec>
<sec id="s2a3">
<title>Procedure</title>
<p>We used a blocked design for the main experiment. At the beginning of each block, participants were cued by a word to attend to either bodies, cars, houses, or cats. During the block, participants maintained attention on the images from the cued category, and performed a one-back repetition detection task on them by pressing the response button when the same stimulus from the attended category appeared in two consecutive trials. Repetition occurred 2-3 times at random times in each block. The experiment consisted of 16 block types, corresponding to the 16 task conditions (<xref rid="fig1" ref-type="fig">Figure 1B</xref>).</p>
<p>Each block lasted for 10 s, starting with the cue word presented for 1 s, followed by 1 s of fixation. Then, ten images from the cued category were presented in isolation or paired with ten images from another category. Each image was presented for 400 ms, followed by 400 ms of fixation (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). There were 8 s of fixation in between the blocks, and a final 8-s fixation after the last block.</p>
<p>We organized blocks in runs, each lasting 4 min 56 s. Each run started with 8 s of fixation followed by block presentations. The presentation order of the 16 task conditions was counterbalanced across each experimental run. 10 participants completed 16 runs and 7 participants completed 12 runs of the main experiment.</p>
</sec>
</sec>
<sec id="s2b">
<title>Localizer experiments</title>
<p>We investigated five different regions of interest: the primary visual cortex (V1), the object-selective areas lateral occipital cortex (LO) and posterior fusiform (pFs), the body-selective extrastriate body area (EBA), and the scene-selective parahippocampal place area (PPA). To define these regions of interest (ROIs), each participant completed four localizer runs described in detail below.</p>
<sec id="s2b1">
<title>Early visual area localizer</title>
<p>We used meridian mapping to localize the primary visual cortex V1. Participants viewed a black-and-white checkerboard pattern through a 60 degree polar angle wedge aperture. The wedge was presented either horizontally or vertically. Participants were asked to detect luminance changes in the wedge in a blocked-design paradigm. Each run consisted of four horizontal and four vertical blocks, each lasting 16 s, with 16 s of fixation in between. A final 16 s fixation followed the last block. Each run lasted 272 s. The order of the blocks was counterbalanced within each run. Participants completed two runs of this localizer.</p>
</sec>
<sec id="s2b2">
<title>Category localizer</title>
<p>We used a category localizer to localize the cortical regions selective to scenes (PPA), bodies (EBA), and objects (LO, pFs). In a blocked-design paradigm, participants viewed stimuli from the five categories of faces, scenes, objects, bodies, and scrambled images. Each localizer run contained two 16-s blocks of each category, with the presentation order counterbalanced within each run. An 8-s fixation period was presented at the beginning, in the middle, and at the end of the run. In each block, 20 stimuli from the same category were presented. Stimuli were presented for 750 ms followed by 50 ms of fixation on a gray background screen. Participants were asked to maintain their fixation on a red circle at the center of the screen throughout and press a key when they detected a slight jitter in the stimuli that happened 2-3 times per block. Each run lasted 344 s. Participants completed two runs of this localizer.</p>
</sec>
</sec>
<sec id="s2c">
<title>Stimulus presentation inside the scanner</title>
<p>We back-projected the stimuli onto a screen positioned at the rear of the magnet using an LCD projector with a refresh rate of 60 Hz and a spatial resolution of 768 ×1024. Participants observed the screen through a mirror attached to the head coil.</p>
</sec>
<sec id="s2d">
<title>MRI data acquisition</title>
<p>We recorded the data of 10 participants using the Siemens 3T Tim Trio MRI system with a 32-channel head coil at the Institute for Research in Fundamental Sciences (IPM). We collected the data of 7 additional participants on a Siemens Prisma MRI system using a 64-channel head coil at the National Brain-mapping Laboratory (NBML). For each participant, we performed a whole-brain anatomical scan using a T1-weighted MPRAGE sequence. For the functional scans, including the main experiment and the localizer experiments, we acquired 33 slices parallel to the AC-PC line using T2<sup>*</sup>-weighted gradient-echo echo-planar imaging (EPI) sequences covering the whole brain (TR=2 s, TE=30 ms, flip angle = 90°, voxel size=3 × 3 × 3 mm<sup>3</sup>, matrix size = 64 × 64).</p>
</sec>
<sec id="s2e">
<title>fMRI data preprocessing</title>
<p>We performed fMRI data analysis using FreeSurfer (<ext-link ext-link-type="uri" xlink:href="https://surfer.nmr.mgh.harvard.edu">https://surfer.nmr.mgh.harvard.edu</ext-link>), Freesurfer Functional Analysis Stream (FsFast <sup><xref ref-type="bibr" rid="c21">21</xref></sup>) and in-house MATLAB codes. fMRI data preprocessing steps included 3D motion correction, slice timing correction, and linear and quadratic trend removal. We performed no spatial smoothing on the data. We used a double gamma function to model the hemo-dynamic response function. We eliminated the first four volumes (8 s) of each run to avoid the initial magnetization transient.</p>
</sec>
<sec id="s2f">
<title>fMRI data analysis</title>
<p>For the main experiment, we performed a general linear model (GLM) analysis for each participant to estimate voxel-wise regression coefficients in each of the 16 task conditions. The onset and duration of each block were convolved with a hemodynamic response function and were then entered to the GLM as regressors. We also included movement parameters and linear and quadratic nuisance regressors in the GLM. We used these voxel-wise coefficients from the regions of interest (ROIs) as the basis for all further analyses.</p>
<p>For the early visual area localizer experiment, we estimated voxel regression coefficients in each of the two conditions (i.e., vertical and horizontal wedge) using a separate GLM. After convolving with a hemodynamic response function, the onset and duration of each block were entered to the GLM as regressors of interest. We also included movement parameters and linear and quadratic nuisance regressors in the GLM. We used the obtained coefficients to define the V1 ROI.</p>
<p>For the category localizer, we used another GLM to estimate voxel-wise regression coefficients in the five task conditions (i.e. faces, scenes, objects, bodies, and scrambled images). The GLM procedure was similar to the other two experiments. We then used these estimated coefficients to define the LO, pFs, EBA, and PPA ROIs.</p>
<sec id="s2f1">
<title>Definition of ROIs</title>
<p>We determined the V1 ROI using a contrast of horizontal versus vertical polar angle wedges that reveals the topographic maps in the occipital cortex <sup><xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref></sup>. To define the object-selective areas LO in the lateral occipital cortex and pFs in the posterior fusiform gyrus <sup><xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c25">25</xref></sup>, we used a contrast of objects versus scrambled images. Active voxels in the lateral occipital and ventral occipitotemporal cortex were selected as LO and pFS, respectively, following the procedure described by Kourtzi and Kanwisher <sup><xref ref-type="bibr" rid="c26">26</xref></sup>. We used a contrast of scenes versus objects for defining the scene-selective area PPA in the parahippocampal gyrus <sup><xref ref-type="bibr" rid="c27">27</xref></sup>, and a contrast of bodies versus objects for defining the body-selective area EBA in the lateral occipitotemporal cortex <sup><xref ref-type="bibr" rid="c28">28</xref></sup>. We thresholded the activation maps for both the early visual localizer and the category localizer at <italic>p &lt;</italic> 0.001, uncorrected.</p>
</sec>
<sec id="s2f2">
<title>Univariate fMRI analysis</title>
<p>We first used a univariate analysis to determine the effect of attention for different category pairs. Using the voxel-wise coefficients of the isolated conditions associated with each category, we examined the relative response of each voxel to the two categories for each category pair. This relative response determined which of the two categories was more preferred by the voxel. Therefore, for each category pair and each voxel, the category that elicited a higher response in the isolated condition was assigned the relatively more preferred category (<italic>M</italic>) label and the other the relatively less preferred category (<italic>L</italic>) label.</p>
</sec>
<sec id="s2f3">
<title>Univariate distance based on the isolated conditions</title>
<p>We had 6 pairs of categories: Body-Car, Body-House, Body-Cat, Car-House, Car-Cat and House-Cat. As a measure of the difference between the response evoked by each of the two categories in a pair, we defined a univariate distance. We calculated the univariate distance for each pair of categories simply as the difference in voxel responses of the two isolated conditions (<xref ref-type="disp-formula" rid="eqn1">Equation 1</xref>):
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="543205v1_eqn1.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here, <italic>R</italic> denotes the average voxel response across runs, and the subscripts <italic>M</italic> and <italic>L</italic> denote the presence of the more preferred and the less preferred stimuli, respectively. The superscript <italic>at</italic> denotes the attended stimulus. Note that in the isolated conditions, the presented stimulus was always attended. Thus, <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline1.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the average response related to the isolated preferred stimulus, and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline2.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the average response to the isolated less preferred stimulus. For example, the Body-Car univariate distance was assessed by <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline3.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for voxels more responsive to bodies than cars, and by <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline4.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for voxels more responsive to cars than bodies. Thus, according to this measure, two categories that elicited closer responses had less univariate distance, indicating more similarity in univariate response between the two categories.</p>
</sec>
<sec id="s2f4">
<title>Univariate attentional modulation based on the paired conditions</title>
<p>For each of the 6 category pairs, we had two paired conditions, in which stimuli from both categories were presented, but with attention directed to either one or the other category (for example, <italic>Body</italic><sup><italic>at</italic></sup><italic>Car</italic> and <italic>BodyCar</italic><sup><italic>at</italic></sup> conditions for the Body-Car pair, with the superscript <italic>at</italic> denoting the to-be-attended stimulus). Since these paired conditions differed only in the attentional target and not in the stimuli, any difference observed in cortical response can be uniquely ascribed to the shift in attention <sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup>. We thus defined attentional modulation for each pair of categories as the change in response when attention shifted from the more preferred stimulus to the other:
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="543205v1_eqn2.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here, <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline5.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> denotes the response related to the paired condition with attention directed to the more preferred category, while <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline6.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>is the elicited response when attending the less preferred category in the pair. For example, considering the Body-Car pair, attentional modulation was assessed by <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline7.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>for a voxel preferring bodies to cars, and by <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline8.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>for a voxel preferring cars to bodies.</p>
</sec>
<sec id="s2f5">
<title>Multivariate pattern analysis</title>
<p>To determine the effect of attention at the multivariate level, and to examine the attentional bias that the representation of each stimulus receives, we used a multivariate pattern analysis. Here, rather than comparing the mean values of voxel-wise coefficients in each ROI, we instead considered the ROI response pattern in each condition as a response vector, with the voxel-wise coefficients as its elements. Therefore, we had 16 response vectors, one for each task condition, in each ROI. Similar to the univariate analysis, we used the responses in the isolated conditions to assess category distance, and the responses in the paired conditions to evaluate the effect of attention.</p>
<p><xref rid="fig2" ref-type="fig">Figure 2</xref> illustrates the response vectors for two stimulus categories (here termed <italic>x</italic> and <italic>y</italic>) in both the isolated and the paired conditions. The four vectors <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline9.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline10.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> represent the response patterns of the four conditions related to the x-y category pair, with <italic>V</italic> representing the response vector in an ROI, subscripts <italic>x</italic> and <italic>y</italic> denoting the presence of the <italic>x</italic> and <italic>y</italic> stimuli, respectively, and the superscript <italic>att</italic> denoting the attended stimulus. Therefore, <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline11.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> represents the response vector related to the isolated <italic>x</italic> condition (in which <italic>x</italic> was automatically attended), and<inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline12.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>represents the response vector related to the paired <italic>xy</italic> condition with attention directed to the <italic>y</italic> stimulus.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Response vectors related to <italic>x</italic> and <italic>y</italic> stimuli in isolated and paired conditions.</title>
<p><inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline39.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline40.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> denote the response vectors related to isolated <italic>x</italic> and isolated <italic>y</italic> conditions. <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline41.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline42.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> illustrate the response in paired conditions, when attention is directed to stimulus <italic>x</italic> and <italic>y</italic>, respectively. Each paired response was projected on the two isolated responses <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline43.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline44.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. <italic>a</italic><sub>1</sub> and <italic>b</italic><sub>1</sub> represent the weight of isolated <italic>x</italic> response in the pair response, respectively for the <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline45.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline46.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>responses. <italic>a</italic><sub>2</sub> and <italic>b</italic><sub>2</sub> represent the weight of the isolated <italic>y</italic> response in the <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline47.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline48.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> paired response, respectively.</p></caption>
<graphic xlink:href="543205v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2f6">
<title>Multivariate distance based on the isolated conditions</title>
<p>As illustrated in <xref rid="fig2" ref-type="fig">Figure 2</xref>, the two isolated response vectors <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline13.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline14.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> have a certain distance because the response across the voxels varies for the two stimuli. For two stimuli that elicit more similar response patterns in an ROI, the isolated response vectors are closer to each other. Thus, we defined the multivariate distance between the two isolated response vectors <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline15.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline16.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> in each ROI using Pearson’s correlation, as shown in <xref ref-type="disp-formula" rid="eqn3">Equation 3</xref>:
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="543205v1_eqn3.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline17.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline18.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> represent the response vectors related to isolated <italic>x</italic> and <italic>y</italic> conditions, and <italic>ρ</italic> denotes Pearson’s correlation between the two response vectors. For stimuli with more similar response patterns, the correlation between their response vectors will be higher, leading to lower multivariate distance.</p>
</sec>
<sec id="s2f7">
<title>Multivariate effect of attention based on the paired conditions (attentional weight shift)</title>
<p>Similar to the isolated conditions, we considered the response pattern in the paired conditions as vectors, <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline19.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline20.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (<xref rid="fig2" ref-type="fig">Figure 2</xref>). The response vectors in the paired conditions can be written as a linear combination of the response vectors in the isolated conditions, with an error term denoting the deviation of the paired-condition responses from the plane defined by the isolated-condition responses <sup><xref ref-type="bibr" rid="c6">6</xref></sup>, as shown in <xref ref-type="disp-formula" rid="eqn4">Equation 4</xref>:
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="543205v1_eqn4a.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn5">
<alternatives><graphic xlink:href="543205v1_eqn4b.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here, parameters <italic>a</italic><sub>1</sub> and <italic>a</italic><sub>2</sub> are the weights of the isolated <italic>x</italic> and <italic>y</italic> responses, respectively, when <italic>x</italic> is attended, and parameters <italic>b</italic><sub>1</sub> and <italic>b</italic><sub>2</sub> are the respective weights of isolated <italic>x</italic> and <italic>y</italic> responses when <italic>y</italic> is attended. The weights are determined by projecting the paired vectors on each of the isolated vectors (<xref rid="fig2" ref-type="fig">Figure 2</xref>). <italic>c</italic><sub>1</sub> and <italic>c</italic><sub>2</sub> denote the error terms related to the deviation of the <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline21.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline22.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> from the <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline23.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> plane, respectively. A higher <italic>a</italic><sub>1</sub> compared to <italic>a</italic><sub>2</sub> indicates that the paired response pattern is more similar to <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline24.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> compared to <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline25.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and vice versa.</p>
<p>In the presence of two stimuli, if attention could completely remove the effect of the unattended stimulus, the paired response would be the same as the response to the isolated attended stimulus. However, the information related to the unattended stimulus is not fully removed and attention has been shown to only increase the weight of the response related to the attended stimulus in the paired response <sup><xref ref-type="bibr" rid="c6">6</xref></sup>. To examine whether this increase in the weight of the attended stimulus is constant or if it depends on the similarity of the two stimuli in cortical representation, we defined the weight shift as the multivariate effect of attention:
<disp-formula id="eqn6">
<alternatives><graphic xlink:href="543205v1_eqn5.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here, <italic>a</italic><sub>1</sub>, <italic>a</italic><sub>2</sub>, <italic>b</italic><sub>1</sub>,and <italic>b</italic><sub>2</sub> are the weights of the isolated responses, estimated using <xref ref-type="disp-formula" rid="eqn4">Equation 4</xref>. The weight shift, Δ<italic>w</italic>, is the change in the relative weight of the isolated <italic>x</italic> response in the paired response when attention shifts from <italic>x</italic> to <italic>y</italic>. A higher Δ<italic>w</italic> for a category pair indicates that attention is more efficient in removing the effect of the unattended stimulus in the pair.</p>
</sec>
</sec>
<sec id="s2g">
<title>Simulations</title>
<p>We investigated the mechanisms underlying the observed effect of stimulus similarity on attentional modulation using simulations. We considered two models for attentional enhancement: a response gain model <sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup> and a tuning sharpening model <sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c32">32</xref></sup>.</p>
<p>According to the response gain model, attention to an object multiplicatively increases neural responses to that object (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). For instance, for a body-selective neuron, this mechanism can be implemented using <xref ref-type="disp-formula" rid="eqn6">Equation 6</xref>:</p>
<p>
<disp-formula id="eqn7">
<alternatives><graphic xlink:href="543205v1_eqn6a.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="543205v1_eqn6b.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here, <italic>R</italic><sub><italic>Body</italic></sub> is the neuron’s response to an ignored body stimulus, and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline26.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>is the response of the neuron to the attended body stimulus, which is enhanced by the attention factor, <italic>β. R</italic><sub><italic>Car</italic></sub> and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline27.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>in <xref ref-type="disp-formula" rid="eqn6">Equation 6b</xref> denote the response of the same body-selective neuron to an ignored and an attended car stimulus, respectively. The response gain model posits that attention to either stimulus enhances the response of the neuron by the same attention factor. This multiplicative scaling preserves the shapes of the neurons’ tuning curves <sup><xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c15">15</xref></sup>.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Attentional modulation by the response gain model and tuning sharpening model.</title>
<p>We illustrate the models here for the example of a neuron with high selectivity for cat stimuli. Solid curves denote the response to unattended stimuli and dashed curves denote the response to attended stimuli. (A) According to the response gain model, the response of the neuron to attended stimuli is scaled by a constant attention factor. Therefore, the response of the cat-selective neuron to an attended stimulus is enhanced to the same degree for all stimuli. (B) According to the tuning sharpening model, the response modulation by attention depends on the neuron’s tuning for the attended stimulus. Therefore, for optimal and near-optimal stimuli such as cat and body stimuli the response is highly increased, while for non-optimal stimuli such as houses, the response is suppressed.</p></caption>
<graphic xlink:href="543205v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In contrast, according to the tuning sharpening model, attention to an object increases neural responses relative to their responsiveness to that object (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). Therefore, while the response of a neuron is substantially enhanced when an optimal stimulus is attended, its response to an attended non-optimal stimulus is increased to a lesser degree, or even decreased. The tuning sharpening model thus predicts a sharpening of the neurons’ tuning curve with attention <sup><xref ref-type="bibr" rid="c32">32</xref></sup>.</p>
<p>We implemented this mechanism using <xref ref-type="disp-formula" rid="eqn7">Equation 7</xref>:
<disp-formula id="eqn9">
<alternatives><graphic xlink:href="543205v1_eqn7a.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn10">
<alternatives><graphic xlink:href="543205v1_eqn7b.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn11">
<alternatives><graphic xlink:href="543205v1_eqn7c.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn12">
<alternatives><graphic xlink:href="543205v1_eqn7d.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
In the above equations, <italic>R</italic><sub><italic>Body</italic></sub>, <italic>R</italic><sub><italic>Car</italic></sub>, <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline28.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline29.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>denote the neuron’s response to ignored body, ignored car, attended body, and attended car stimuli, respectively. Parameters <italic>s</italic><sub>1</sub> and <italic>s</italic><sub>2</sub> likewise denote the degree of the neuron’s selectivity to body and car stimuli. Parameter <italic>β</italic> is the attention factor. <italic>R</italic><sub><italic>max</italic></sub> is the response of the neuron to its optimal stimulus.</p>
<p>We simulated the action of the response gain model and the tuning sharpening model using numerical simulations. We composed a neural population of 10<sup>6</sup> neurons in equal proportions body-, car-, cat- or house-selective. Each neuron also responded to object categories other than its preferred category, but to a lesser degree and with variation. The neural responses and the attention factor were randomly chosen from a range comparable with neural studies of attention and object recognition in the ventral visual cortex <sup><xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c34">34</xref></sup>.</p>
<p>Attention was implemented according to the above equations. Using <xref ref-type="disp-formula" rid="eqn6">Equations 6</xref> and <xref ref-type="disp-formula" rid="eqn7">7</xref>, we calculated the response of each neuron to the same 16 conditions as our main fMRI experiment. Then, we randomly chose 1000 neurons with similar selectivity from the population, and averaged their responses to make up a voxel.</p>
<p>We modeled two neural populations: an object-selective population with mixed preference across voxels, and a second population with similar preference for all voxels. The former population models the object-selective cortex that does not have a clear preference for any one category. The latter population models category-specific regions that respond more strongly to one category of objects than to others. Finally, we performed the same univariate and multivariate analyses as those used for the fMRI data to compare the predictions of each model with the observed data.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Behavioral results</title>
<p>Participants performed a one-back task to maintain attention towards the cued stimuli. Accuracy in each experimental run was checked during the scan to ensure that participants followed the instructions. Participants had an average performance of 90.49% across all runs, confirming effective attention towards the cued stimuli (chance level without attention 50%). As expected, average performance in the isolated conditions (94.82%±0.046) was significantly higher than in the paired conditions (89% ± 0.07, with <italic>t</italic>(14) = 7.2 and <italic>p &lt;</italic> 0.0001), since detecting a repetition in the superimposed case was more difficult.</p>
<sec id="s3a1">
<title>Attentional modulation varies dependent on target-distractor difference in response</title>
<p>We considered the effect of attention in five ROIs: the primary visual cortex V1, the object-selective regions LO and pFs, the body-selective region EBA, and the scene-selective region PPA. We obtained the voxel-wise responses through a general linear model in those ROIs for all task conditions, consisting of four isolated conditions (blocks with isolated stimuli from one category) and 12 paired conditions (blocks with superimposed stimuli from two categories, see <xref rid="fig1" ref-type="fig">Figure 1B</xref>). There were 6 combinations of category pairs: Body-Car, Body-House, Body-Cat, Car-House, Car-Cat and House-Cat. For each voxel we determined its relative preference for the two categories of each category pair, based on its response to the two categories in isolation. Thus, for each pair, one category was labeled as the more preferred category (<italic>M</italic>), and the other as the less preferred category (<italic>L</italic>). Considering the isolated and paired conditions related to each category pair, we hereafter refer to the conditions related to each category pair as <italic>M</italic><sup><italic>at</italic></sup>, <italic>M</italic><sup><italic>at</italic></sup><italic>L, ML</italic><sup><italic>at</italic></sup>, and <italic>L</italic><sup><italic>at</italic></sup>, with <italic>M</italic> and <italic>L</italic> denoting the more preferred and the less preferred categories for each voxel, and the superscript <italic>at</italic> denoting the attended stimulus.</p>
<p>For instance, for the Body-Car pair, for a voxel that showed a higher response to body stimuli than to car stimuli, the four associated conditions related to the pair were referred to as <italic>M</italic><sup><italic>at</italic></sup> (attended body stimuli), <italic>M</italic><sup><italic>at</italic></sup><italic>L</italic> (attended body stimuli paired with ignored car stimuli), <italic>ML</italic><sup><italic>at</italic></sup> (attended car stimuli paired with ignored body stimuli), and <italic>L</italic><sup><italic>at</italic></sup> (attended car stimuli). If the same voxel was more responsive to cats than bodies, then the four conditions related to the Body-Cat pair would be referred to as: <italic>M</italic><sup><italic>at</italic></sup> (attended cat stimuli), <italic>M</italic><sup><italic>at</italic></sup><italic>L</italic> (attended cat stimuli paired with ignored body stimuli), <italic>ML</italic><sup><italic>at</italic></sup> (attended body stimuli paired with ignored cat stimuli), and <italic>L</italic><sup><italic>at</italic></sup> (attended body stimuli).</p>
<p>We next determined the amount of attentional modulation for each category pair using the voxel-wise coefficients related to the two paired conditions, <italic>M</italic><sup><italic>at</italic></sup><italic>L</italic> and <italic>ML</italic><sup><italic>at</italic></sup>. We defined attentional modulation for each category pair as the change in response when attention shifted from the <italic>M</italic> category to the <italic>L</italic> category in the presence of both stimuli <sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup> (see <xref rid="fig4" ref-type="fig">Figure 4</xref>, illustrated for all pairs in EBA).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Average voxel response in EBA for each pair of stimulus categories.</title>
<p>The x-axis labels represent the 4 conditions related to each category pair, <italic>M</italic><sup><italic>at</italic></sup>, <italic>M</italic><sup><italic>at</italic></sup><italic>L, ML</italic><sup><italic>at</italic></sup>, <italic>L</italic><sup><italic>at</italic></sup>, with <italic>M</italic> and <italic>L</italic> denoting the presence of the more preferred and the less preferred category and the superscript <italic>at</italic> denoting the attended category. For instance, <italic>M</italic><sup><italic>at</italic></sup> refers to the condition in which the more preferred stimulus was presented in isolation (and automatically attended), and <italic>ML</italic><sup><italic>at</italic></sup> refers to the paired condition in which the less preferred stimulus was attended to. Red arrows in each panel illustrate the observed attentional modulation (AM) caused by the shift of attention from the more preferred to the less preferred stimulus. Green arrows in panels B and C illustrate the difference in the response to isolated stimuli. Error bars represent standard errors of the mean. N = 17 human participants.</p></caption>
<graphic xlink:href="543205v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We observed a significant reduction in response when attention shifted from the <italic>M</italic> stimulus to the <italic>L</italic> stimulus for all pairs in the higher-level ROIs (<italic>ts &gt;</italic> 3, <italic>ps &lt;</italic> 0.04, <italic>corrected</italic>) except for the Body-Car, Body-Car, and Car-Cat pairs in PPA (<italic>ts &lt;</italic> 2, <italic>ps &gt;</italic> 0.3, <italic>corrected</italic>) and the Car-House pair in EBA (<italic>t</italic>(16) = 1, <italic>p</italic> = 0.9, <italic>corrected</italic>). In V1, we observed no significant attentional modulation for any pairs (<italic>ts &lt;</italic> 2.5, <italic>ps &gt;</italic> 0.1, <italic>corrected</italic>) except for the Body-Car pair (<italic>t</italic>(16) = 3.8, <italic>p &lt;</italic> 0.01, <italic>corrected</italic>). Thus, the observed effect was limited to higher-level visual areas. Since presented stimuli were the same in both conditions, this effect is unequivocally due to attentional modulation caused by the shift in attention.</p>
<p>Closer comparison of the results suggests that for pairs with significant attentional modulation, the modulation is not uniform. Instead, attentional modulation was greater for pairs in which the <italic>M</italic> and <italic>L</italic> stimuli elicited more different responses compared to pairs with <italic>M</italic> and <italic>L</italic> stimuli eliciting closer responses. For example, we observed a larger attentional modulation for the Body-House pair (<xref rid="fig4" ref-type="fig">Figure 4B</xref>) compared to the Body-Cat pair (<xref rid="fig4" ref-type="fig">Figure 4C</xref>) in all ROIs (<italic>ts &gt;</italic> 4, <italic>ps &lt;</italic> 0.001, <xref rid="fig4" ref-type="fig">Figures 4B-C</xref>, compare the size of the red arrows) except for V1 (<italic>t</italic>(16) = 0.65, <italic>p</italic> = 0.5). Comparing the isolated responses for these two pairs, we observed that the difference between the response of the isolated Body and isolated House conditions was generally higher than the difference between the isolated Body and isolated Cat conditions in all ROIs (<italic>ts &gt;</italic> 4, <italic>ps &lt;</italic> 0.001, <xref rid="fig4" ref-type="fig">Figure 4B-C</xref>, compare the size of the green arrows).</p>
<p>To examine this relationship quantitatively for all category pairs, we used two approaches. First, in a univariate analysis using average voxel responses, we determined the relationship between the observed attentional modulation and the difference in isolated responses. Next, in a multivariate pattern analysis, we considered the response patterns in each ROI and looked for the underlying basis of this variation in attentional modulation at the multivariate level. This analysis enabled us to determine whether the bias of attention on the representation of the attended stimulus differed for different category pairs.</p>
</sec>
<sec id="s3a2">
<title>Attentional modulation decreases for target-distractor pairs that elicit closer responses</title>
<p>We first used a univariate analysis to determine the relationship between attentional modulation and category distance across pairings and in different ROIs. After determining the voxel-wise <italic>M</italic> and <italic>L</italic> categories for each category pair, we calculated the difference in the isolated response elicited by the two categories (univariate category distance) using the two isolated conditions <italic>M</italic><sup><italic>at</italic></sup> and <italic>L</italic><sup><italic>at</italic></sup> (<xref ref-type="disp-formula" rid="eqn1">Equation 1</xref>).</p>
<p>We then assessed the attentional modulation related to the pair as the amount of the reduction in response when attention shifted from the <italic>M</italic> stimulus to the <italic>L</italic> stimulus in the paired presentation of both stimuli (<xref ref-type="disp-formula" rid="eqn2">Equation 2</xref>). For instance, for the Body-Car pair and a voxel more responsive to bodies than cars, univariate category distance was calculated by <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline30.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and univariate attentional modulation was calculated by <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline31.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>We observed a significantly positive correlation between univariate attentional modulation and category distance in all ROIs (<italic>ts &gt;</italic> 4, <italic>ps &lt;</italic> 0.001) except V1 (<italic>t</italic>(16) = 0.67, <italic>p</italic> = 0.51, see <xref rid="fig5" ref-type="fig">Figure 5</xref>). These results demonstrate that for stimuli that elicit more different responses, attention causes a greater response modulation, while the shift of attention between stimuli with more similar responses causes little response change.This indicates that the amount of attentional modulation is related to the response difference between the two presented stimuli.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Attentional modulation versus category distance in each ROI.</title>
<p>(A-E) The value of attentional modulation versus category distance. <italic>M</italic><sup><italic>at</italic></sup><italic>L</italic> and <italic>ML</italic><sup><italic>at</italic></sup> denote the two paired conditions with attention directed to the more preferred (M) or less preferred (L) stimulus, respectively. <italic>M</italic><sup><italic>at</italic></sup> and <italic>L</italic><sup><italic>at</italic></sup> represent the isolated conditions, respectively, with the more preferred or the less preferred stimulus presented in isolation. Each circle represents the values related to one category pair. Note that the data illustrated here are averaged across subjects only for illustration purposes. <italic>R</italic><sup><xref ref-type="bibr" rid="c2">2</xref></sup> was calculated for individual participants and statistical significance using t-tests across participants as illustrated in panel F. (F) <italic>R</italic><sup><xref ref-type="bibr" rid="c2">2</xref></sup> for the correlation between attentional modulation and category distance in each ROI. Asterisks indicate that the correlation coefficients are significantly positive. Error bars represent standard errors of the mean. N = 17 human participants.</p></caption>
<graphic xlink:href="543205v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3a3">
<title>The multivariate effect of attention decreases for more similar target-distractor pairs</title>
<p>The univariate analysis above considers average response only and thus cannot capture other aspects of response variance. For example, in an object-selective region with diverse selectivity for different objects, the average response to body and house stimuli is close, but the response pattern may be very different since voxels highly responsive to bodies do not show high responses to houses, and vice versa. Thus, we had to consider voxel preferences in the univariate analysis to observe the difference in response between the two categories.</p>
<p>We complement the univariate approach with a multivariate pattern analysis to assess the relationship between the effect of attention and category distance at the multivariate level. By considering the whole response pattern in an ROI to each stimulus, we can compare the responses to each stimulus without considering voxel preferences. Moreover, using this method we can determine the weight of the response to each isolated stimulus in the total response, and determine the attentional bias related to each category pair.</p>
<p>The multivariate representation of two simultaneously-presented stimuli can be written as the weighted average of the representations of the two stimuli presented in isolation <sup><xref ref-type="bibr" rid="c6">6</xref></sup>: When one stimulus is attended, the weight of the response to that stimulus increases in the multivariate representation.</p>
<p>Taking this approach, for each category pair (e.g. Body-Car), we considered the multivariate representation of the two paired conditions (<inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline32.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline33.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with <italic>V</italic> denoting the multivariate response pattern of each condition), and determined the weight of each of the isolated-stimulus responses (<inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline34.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline35.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>) in the paired response (<xref rid="fig2" ref-type="fig">Figure 2</xref>). We then calculated the difference between the weight of each stimulus when it was the target and when it was the distractor (e.g. for the Body-Car pair, the difference between the weight of <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline36.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> in <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline37.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="543205v1_inline38.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>).</p>
<p>If attention could perfectly remove the effect of the distractor, the weight of the attended stimulus would equal one and the representation of the pair would be identical to the representation of the isolated target. In this case, the difference between the weight of the stimulus representation when attended and ignored would be a maximal value of one. However, if the distractor is not completely removed, this leads to a weight shift value smaller than one. Thus, the magnitude of the weight shift is an indicator of the efficiency of attention, with greater values indicating a higher efficiency of attention in removing the distractor.</p>
<p>To compare the efficiency of attention across category pairs, we calculated the weight shift for each category pair (<xref ref-type="disp-formula" rid="eqn5">Equation 5</xref>). Then, to determine whether this multivariate effect of attention was dependent on the similarity between the target and the distractor in their cortical representation, we calculated the multivariate category distance for each category pair (<xref ref-type="disp-formula" rid="eqn3">Equation 3</xref>).</p>
<p>We observed that the attentional weight shift was not constant for different category pairs, and that weight shift and category distance correlated positively in LO, pFs and EBA (<italic>ts &gt;</italic> 4.4, <italic>ps &lt;</italic> 5 ×10<sup><italic>–</italic>3</sup>, <xref rid="fig6" ref-type="fig">Figure 6</xref>), marginally significantly in PPA (<italic>t</italic>(16) = 1.8, <italic>p</italic> = 0.09), and not in V1 (<italic>t</italic>(16) = 0.42, <italic>p</italic> = 0.68). These results indicate that the attentional bias towards a stimulus in a pair decreases as the similarity between the two stimuli in neural representation increases.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Weight shift versus category distance in each ROI.</title>
<p>(A-E) Attentional weight shift versus category distance. Each circle represents the values related to one category pair. Note that the data illustrated here are averaged across subjects only for illustration purposes. <italic>R</italic><sup><xref ref-type="bibr" rid="c2">2</xref></sup> was calculated for individual participants and statistical significance using t-tests across participants as illustrated in panel F. (F) <italic>R</italic><sup><xref ref-type="bibr" rid="c2">2</xref></sup> for the correlation between attentional weight shift and category distance in each ROI. Asterisks indicate that the correlations are significantly positive. Error bars represent standard errors of the mean. N = 17 human participants.</p></caption>
<graphic xlink:href="543205v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3b">
<title>Tuning sharpening predicts the dependence of attentional modulation on target-distractor similarity</title>
<p>We observed empirically that attentional enhancement is not constant and content-independent, but rather depends on the response similarity between the target and the distractor. We next asked whether gain increase or tuning changes predict the observed effect of target-distractor similarity on attentional modulation.</p>
<p>Based on the response gain model, attention increases neural responses by scaling the responses by a constant attention factor <sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup>. Therefore, the response gain model predicts that attention scales the neurons’ tuning function without affecting its shape (<xref rid="fig3" ref-type="fig">Figure 3A</xref>).</p>
<p>In contrast, the tuning sharpening model proposes that attention enhances the response of each neuron based on its preference to the attended stimulus <sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c32">32</xref></sup>. Therefore, this model predicts that attention causes a sharpening of the neurons’ tuning function, with a sharp increase in the response to optimal stimuli, and no increase in the response to the non-optimal stimuli (<xref rid="fig3" ref-type="fig">Figure 3B</xref>).</p>
<p>To examine which of these mechanisms could account for the observed results, we simulated the responses of a neural population to isolated or paired stimuli from the four categories of bodies, cars, houses and cats. Equivalent to the fMRI experiment, we determined neuronal responses to stimuli presented either in isolation or paired with stimuli from another category (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). We implemented attentional modulation of the neural responses either using the response gain model (<xref ref-type="disp-formula" rid="eqn6">Equation 6</xref>), or the tuning sharpening model (<xref ref-type="disp-formula" rid="eqn7">Equation 7</xref>). We then used the univariate and multivariate analyses equivalent to those used for the fMRI data to determine which model predicts the empirical data.</p>
<p>We created two neural populations: i) a population with similar selectivity across all neurons to represent a region with strong preference for a specific object category, in which neurons generally show high response to stimuli from that category, and ii) a population with varying selectivity across neurons, representing object-selective regions, in which neurons show different selectivities. Then we assessed attentional modulation using the reduction in response when attention shifted from the stronger to the weaker stimulus in a pair (<xref ref-type="disp-formula" rid="eqn2">Equation 2</xref>), and examined its relationship with univariate category distance (<xref ref-type="disp-formula" rid="eqn1">Equation 1</xref>).</p>
<p>We found that the response gain model predicted no relationship between attentional modulation and category distance in either population (<xref rid="fig7" ref-type="fig">Figure 7A-B</xref>). In contrast, the tuning sharpening model predicted a positive correlation between attentional modulation and category distance in both neural populations (<xref rid="fig7" ref-type="fig">Figure 7C-D</xref>). Thus, the tuning sharpening model provides a better prediction of the empirical data compared to the response gain model.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7:</label>
<caption><title>Attentional modulation as a function of category distance, as predicted by the two attentional mechanisms.</title>
<p><italic>M</italic><sup><italic>at</italic></sup><italic>L</italic> and <italic>ML</italic><sup><italic>at</italic></sup> denote the two paired conditions with attention directed to the more preferred (<italic>M</italic>) or the less preferred (<italic>L</italic>) stimulus, respectively. <italic>M</italic><sup><italic>at</italic></sup> and <italic>L</italic><sup><italic>at</italic></sup> represent the isolated conditions, respectively with the <italic>M</italic> or the <italic>L</italic> stimulus presented in isolation. Top panels represent predictions in a region with strong preference for a specific category, and bottom panels illustrate predictions in an object-selective region. Each circle represents a pair of categories. (A) Predicted attentional modulation based on the gain model in a region with strong preference for a specific category. Predicted attentional modulation based on the gain model in an object-selective region. (C) Predicted attentional modulation based on the tuning model in a region with strong preference for a specific category. (D) Predicted attentional modulation based on the tuning model in an object-selective region.</p></caption>
<graphic xlink:href="543205v1_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Next, for the multivariate analysis, we assessed the attentional weight shift for each category pair as attention shifted from one stimulus to the other (<xref ref-type="disp-formula" rid="eqn5">Equation 5</xref>), and examined its relationship with the multivariate category distance (<xref ref-type="disp-formula" rid="eqn3">Equation 3</xref>). Here, too, we find that the response gain model predicted no relationship between attentional weight shift and category distance (<xref rid="fig8" ref-type="fig">Figure 8A-B</xref>). In contrast, the tuning sharpening model predicted a positive relationship between weight shift and category distance, providing further evidence for tuning sharpening as the underlying mechanism for attentional enhancement.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8:</label>
<caption><title>Predicted weight shift as a function of category distance.</title>
<p>Weight shift for each pair is calculated using <xref ref-type="disp-formula" rid="eqn5">Equation 5</xref>. Category distance represents the difference in multi-voxel representation between responses to the two isolated stimuli, calculated by <xref ref-type="disp-formula" rid="eqn3">Equation 3</xref>. Top panels are related to predictions in a region with strong preference for a specific category and bottom panels illustrate predictions in an object-selective region. (A) Weight shift predicted by the gain model in a region with strong preference for a specific category. (B) Weight shift predicted by the gain model in an object-selective region. (C) Weight shift predicted by the tuning model in a region with strong preference for a specific category. (D) Weight shift predicted by the tuning model in an object-selective region.</p></caption>
<graphic xlink:href="543205v1_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In sum, the tuning model predicts the empirically-observed effect of target-distractor similarity on attentional modulation both at the univariate and at the multivariate level, while the response gain model does not.</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>Visual stimuli compete for resources in the brain. The biased competition model posits that attention to a stimulus biases this competition in favor of the attended stimulus <sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c7">7</xref></sup>. Here, we examined the change in this attentional bias by systematically varying the target and distractors. Using fMRI, we showed that rather than being a constant top-down bias, attentional modulation depends on the similarity between the target and the distractor in their cortical representation, both at the univariate level and at the multivariate level. Using simulations, we arbitrated between the response gain model and the tuning sharpening model as mechanisms of attention for the observed effect, and showed that the empirical results were explained by the latter and not the former.</p>
<sec id="s4a">
<title>Effect of target-distractor similarity on attentional modulation</title>
<p>Using stimuli from four object categories, our study reveals the neural basis of the attentional effect graded by target-distractor similarity in the human brain both at the univariate level and at the multivariate level. This finding has two important implications:</p>
<p>First, our results show that in the competition between multiple stimuli, the attentional bias is not constant. Previous studies have shown attentional modulation in the human brain as an average value without considering its variance for different pairings of targets and distractors <sup><xref ref-type="bibr" rid="c6">6</xref></sup>. These previous accounts of attention cannot explain the variance in performance for the same number of stimuli from different categories. Assessing the role of stimulus content in the bias caused by attention, we confirm that attention enhances the response related to the target. We refine our understanding by showing that however the attentional bias offers less advantage for a more similar target-distractor pair.</p>
<p>Second, this finding provides direct neural evidence for the adverse effects of target-distractor similarity on performance, as previously reported in behavioral studies <sup><xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c14">14</xref></sup>. While behavioral data have suggested that this effect is due to limitation in processing, no investigation has been made to determine the underlying reason or find a mechanistic explanation. Our results demonstrate that this reduction in performance is because the representation of the target (relative to the distractor) is less effectively enhanced by attention when the target becomes more similar to the distractor.</p>
<p>We observed a significant attentional modulation in higher-level regions of the occipito-temporal cortex, but not in V1. Evidence on the effect of attention on V1 responses are divergent, with some previous neuroimaging studies showing a significant effect of attention on neural responses <sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup>, while others reporting no significant effect of attention <sup><xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c37">37</xref></sup>. We believe that this apparent discrepancy results from the form of attention under study. Here, we study object-based attention with a superimposed design that excludes response modulation by space-based attention. Previous reports of significant attentional modulation in V1 include studies of space-based attention with stimuli presented at different locations <sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup>. Considering the high reliance of V1 responses to location, the effect of attention is less pronounced when the two stimuli are presented at the same location, as is the case in the present study.</p>
</sec>
<sec id="s4b">
<title>A model for object-based attentional enhancement</title>
<p>Using a simulation approach, we provide a mechanistic explanation for the observed graded attentional effect. Our modeling results have two implications:</p>
<p>First, we demonstrate that tuning sharpening, but not response gain, predicts the observed reduction in the effect of attention for more similar target-distractor pairs both at the univariate and at the multivariate level. Previous research has shown that a change in the tuning function improves attentional selection at high external noise levels <sup><xref ref-type="bibr" rid="c32">32</xref></sup>. Our results indicate that a change in tuning function could also lead to behavioral disadvantage in an environment where the target is not very different from the surrounding items. When attention is directed towards the target, the response to non-target objects that are more similar to the target is also enhanced, albeit to a lesser amount, leading to an overall weaker effect of attention for a more similar target-distractor pair.</p>
<p>Second, providing evidence from the human brain in favor of tuning sharpening, we suggest tuning sharpening as the underlying mechanism in the domain of object-based attention. By comparing the response gain model and the tuning sharpening model directly in a single study, we provide strong evidence that arbitrates between the theories. The effects of attention have generally been explained by attention acting through increasing the contrast or response gain, especially for space-based attention <sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c38">38</xref></sup>. However, a simple increase in gain cannot explain all reported effects of attention, and a change in the shape of the tuning curves has been observed during visual search <sup><xref ref-type="bibr" rid="c39">39</xref></sup>, and feature-based attention <sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c40">40</xref>,<xref ref-type="bibr" rid="c32">32</xref></sup>.</p>
<p>It is important to note that our speculation on the role of tuning sharpening in object-based attention is based on simulations and not neural data. To ascertain tuning sharpening as the underlying mechanism for object-based attention, intracranial recordings from the human brain are needed.</p>
</sec>
</sec>
<sec id="s5">
<title>Conclusion</title>
<p>In sum, our results unravel the cortical basis by which target-distractor similarity affects attentional modulation, and indicate tuning sharpening as the underlying mechanism for response enhancement during object-based attention.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgement</title>
<p>We thank Sajad Aghapour for helpful discussions. We thank Kiarash Farahmandrad for help with the graphical illustration of the vector plot. Maryam Vaziri-Pashkam was supported by NIH Intramural Research Program ZIA-MH002035.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><given-names>Robert</given-names> <surname>Desimone</surname></string-name> and <string-name><given-names>John</given-names> <surname>Duncan</surname></string-name>. <article-title>Neural mechanisms of selective visual attention</article-title>. <source>Annual review of neuroscience</source>, <volume>18</volume>(<issue>1</issue>):<fpage>193</fpage>–<lpage>222</lpage>, <year>1995</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><given-names>Sabine</given-names> <surname>Kastner</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>De Weerd</surname></string-name>, <string-name><given-names>Robert</given-names> <surname>Desimone</surname></string-name>, and <string-name><given-names>Leslie G</given-names> <surname>Ungerleider</surname></string-name>. <article-title>Mechanisms of directed attention in the human extrastriate cortex as revealed by functional mri</article-title>. <source>science</source>, <volume>282</volume>(<issue>5386</issue>):<fpage>108</fpage>–<lpage>111</lpage>, <year>1998</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><given-names>John H</given-names> <surname>Reynolds</surname></string-name>, <string-name><given-names>Leonardo</given-names> <surname>Chelazzi</surname></string-name>, and <string-name><given-names>Robert</given-names> <surname>Desimone</surname></string-name>. <article-title>Competitive mechanisms subserve attention in macaque areas v2 and v4</article-title>. <source>Journal of Neuroscience</source>, <volume>19</volume>(<issue>5</issue>):<fpage>1736</fpage>–<lpage>1753</lpage>, <year>1999</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><given-names>Diane M</given-names> <surname>Beck</surname></string-name> and <string-name><given-names>Sabine</given-names> <surname>Kastner</surname></string-name>. <article-title>Stimulus context modulates competition in human extrastriate cortex</article-title>. <source>Nature neuroscience</source>, <volume>8</volume>(<issue>8</issue>):<fpage>1110</fpage>–<lpage>1116</lpage>, <year>2005</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><given-names>Stephanie</given-names> <surname>McMains</surname></string-name> and <string-name><given-names>Sabine</given-names> <surname>Kastner</surname></string-name>. <article-title>Interactions of top-down and bottom-up mechanisms in human visual cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>31</volume>(<issue>2</issue>):<fpage>587</fpage>–<lpage>597</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><given-names>Leila</given-names> <surname>Reddy</surname></string-name>, <string-name><given-names>Nancy G</given-names> <surname>Kanwisher</surname></string-name>, and <string-name><given-names>Rufin</given-names> <surname>VanRullen</surname></string-name>. <article-title>Attention and biased competition in multi-voxel object representations</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>106</volume>(<issue>50</issue>):<fpage>21447</fpage>–<lpage>21452</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><string-name><given-names>Jeffrey</given-names> <surname>Moran</surname></string-name> and <string-name><given-names>Robert</given-names> <surname>Desimone</surname></string-name>. <article-title>Selective attention gates visual processing in the extrastriate cortex</article-title>. <source>Science</source>, <volume>229</volume>(<issue>4715</issue>):<fpage>782</fpage>–<lpage>784</lpage>, <year>1985</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><string-name><given-names>Stefan</given-names> <surname>Treue</surname></string-name> and <string-name><given-names>John HR</given-names> <surname>Maunsell</surname></string-name>. <article-title>Attentional modulation of visual motion processing in cortical areas mt and mst</article-title>. <source>Nature</source>, <volume>382</volume>(<issue>6591</issue>):<fpage>539</fpage>–<lpage>541</lpage>, <year>1996</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><given-names>John H</given-names> <surname>Reynolds</surname></string-name> and <string-name><given-names>Robert</given-names> <surname>Desimone</surname></string-name>. <article-title>Interacting roles of attention and visual salience in v4</article-title>. <source>Neuron</source>, <volume>37</volume>(<issue>5</issue>):<fpage>853</fpage>–<lpage>863</lpage>, <year>2003</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><given-names>Stefan</given-names> <surname>Treue</surname></string-name> and <string-name><given-names>John HR</given-names> <surname>Maunsell</surname></string-name>. <article-title>Effects of attention on the processing of motion in macaque middle temporal and medial superior temporal visual cortical areas</article-title>. <source>Journal of Neuroscience</source>, <volume>19</volume>(<issue>17</issue>):<fpage>7591</fpage>–<lpage>7602</lpage>, <year>1999</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><given-names>Mazyar</given-names> <surname>Fallah</surname></string-name>, <string-name><given-names>Gene R</given-names> <surname>Stoner</surname></string-name>, and <string-name><given-names>John H</given-names> <surname>Reynolds</surname></string-name>. <article-title>Stimulus-specific competitive selection in macaque extrastriate visual area v4</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>104</volume>(<issue>10</issue>):<fpage>4165</fpage>–<lpage>4169</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><string-name><given-names>Michael A</given-names> <surname>Cohen</surname></string-name>, <string-name><given-names>Talia</given-names> <surname>Konkle</surname></string-name>, <string-name><given-names>Juliana Y</given-names> <surname>Rhee</surname></string-name>, <string-name><given-names>Ken</given-names> <surname>Nakayama</surname></string-name>, and <string-name><given-names>George A</given-names> <surname>Alvarez</surname></string-name>. <article-title>Processing multiple visual objects is limited by overlap in neural channels</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>111</volume>(<issue>24</issue>):<fpage>8955</fpage>–<lpage>8960</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><given-names>Steven L</given-names> <surname>Franconeri</surname></string-name>, <string-name><given-names>George A</given-names> <surname>Alvarez</surname></string-name>, and <string-name><given-names>Patrick</given-names> <surname>Cavanagh</surname></string-name>. <article-title>Flexible cognitive resources: competitive content maps for attention and memory</article-title>. <source>Trends in cognitive sciences</source>, <volume>17</volume>(<issue>3</issue>):<fpage>134</fpage>–<lpage>141</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><given-names>Michael A</given-names> <surname>Cohen</surname></string-name>, <string-name><given-names>George A</given-names> <surname>Alvarez</surname></string-name>, <string-name><given-names>Ken</given-names> <surname>Nakayama</surname></string-name>, and <string-name><given-names>Talia</given-names> <surname>Konkle</surname></string-name>. <article-title>Visual search for object categories is predicted by the representational architecture of high-level visual cortex</article-title>. <source>Journal of neurophysiology</source>, <volume>117</volume>(<issue>1</issue>):<fpage>388</fpage>–<lpage>402</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><string-name><given-names>Carrie J</given-names> <surname>McAdams</surname></string-name> and <string-name><given-names>John HR</given-names> <surname>Maunsell</surname></string-name>. <article-title>Effects of attention on orientation-tuning functions of single neurons in macaque cortical area v4</article-title>. <source>Journal of Neuroscience</source>, <volume>19</volume>(<issue>1</issue>):<fpage>431</fpage>–<lpage>441</lpage>, <year>1999</year>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><string-name><given-names>John H</given-names> <surname>Reynolds</surname></string-name> and <string-name><given-names>Leonardo</given-names> <surname>Chelazzi</surname></string-name>. <article-title>Attentional modulation of visual processing</article-title>. <source>Annu. Rev. Neurosci</source>., <volume>27</volume>:<fpage>611</fpage>–<lpage>647</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><string-name><given-names>Julio C</given-names> <surname>Martinez-Trujillo</surname></string-name> and <string-name><given-names>Stefan</given-names> <surname>Treue</surname></string-name>. <article-title>Feature-based attention increases the selectivity of population responses in primate visual cortex</article-title>. <source>Current biology</source>, <volume>14</volume>(<issue>9</issue>):<fpage>744</fpage>–<lpage>751</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><string-name><given-names>Maryam</given-names> <surname>Vaziri-Pashkam</surname></string-name> and <string-name><given-names>Yaoda</given-names> <surname>Xu</surname></string-name>. <article-title>Goal-directed visual processing differentially impacts human ventral and dorsal visual representations</article-title>. <source>Journal of Neuroscience</source>, <volume>37</volume>(<issue>36</issue>):<fpage>8767</fpage>–<lpage>8782</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><string-name><given-names>Maryam</given-names> <surname>Vaziri-Pashkam</surname></string-name> and <string-name><given-names>Yaoda</given-names> <surname>Xu</surname></string-name>. <article-title>An information-driven 2-pathway characterization of occipitotemporal and posterior parietal visual object representations</article-title>. <source>Cerebral Cortex</source>, <volume>29</volume>(<issue>5</issue>):<fpage>2034</fpage>–<lpage>2050</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><string-name><given-names>Yaoda</given-names> <surname>Xu</surname></string-name> and <string-name><given-names>Maryam</given-names> <surname>Vaziri-Pashkam</surname></string-name>. <article-title>Task modulation of the 2-pathway characterization of occipitotemporal and posterior parietal visual object representations</article-title>. <source>Neuropsychologia</source>, <volume>132</volume>:<fpage>107140</fpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><given-names>Anders M</given-names> <surname>Dale</surname></string-name>, <string-name><given-names>Bruce</given-names> <surname>Fischl</surname></string-name>, and <string-name><given-names>Martin I</given-names> <surname>Sereno</surname></string-name>. <article-title>Cortical surface-based analysis: I. segmentation and surface reconstruction</article-title>. <source>Neuroimage</source>, <volume>9</volume>(<issue>2</issue>):<fpage>179</fpage>–<lpage>194</lpage>, <year>1999</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><given-names>Martin I</given-names> <surname>Sereno</surname></string-name>, <string-name><given-names>AM</given-names> <surname>Dale</surname></string-name>, <string-name><given-names>JB</given-names> <surname>Reppas</surname></string-name>, <string-name><given-names>KK</given-names> <surname>Kwong</surname></string-name>, <string-name><given-names>JW</given-names> <surname>Belliveau</surname></string-name>, <string-name><given-names>TJ</given-names> <surname>Brady</surname></string-name>, <string-name><given-names>BR</given-names> <surname>Rosen</surname></string-name>, and <string-name><given-names>RB</given-names> <surname>Tootell</surname></string-name>. <article-title>Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title>. <source>Science</source>, <volume>268</volume>(<issue>5212</issue>):<fpage>889</fpage>–<lpage>893</lpage>, <year>1995</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><given-names>Roger BH</given-names> <surname>Tootell</surname></string-name>, <string-name><given-names>Nouchine K</given-names> <surname>Hadjikhani</surname></string-name>, <string-name><given-names>Wim</given-names> <surname>Vanduffel</surname></string-name>, <string-name><given-names>Arthur K</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Janine D</given-names> <surname>Mendola</surname></string-name>, <string-name><given-names>Martin I</given-names> <surname>Sereno</surname></string-name>, and <string-name><given-names>Anders M</given-names> <surname>Dale</surname></string-name>. <article-title>Functional analysis of primary visual cortex (v1) in humans</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>95</volume>(<issue>3</issue>):<fpage>811</fpage>–<lpage>817</lpage>, <year>1998</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><given-names>Rafael</given-names> <surname>Malach</surname></string-name>, <string-name><given-names>JB</given-names> <surname>Reppas</surname></string-name>, <string-name><given-names>RR</given-names> <surname>Benson</surname></string-name>, <string-name><given-names>KK</given-names> <surname>Kwong</surname></string-name>, <string-name><given-names>H</given-names> <surname>Jiang</surname></string-name>, <string-name><given-names>WA</given-names> <surname>Kennedy</surname></string-name>, <string-name><given-names>PJ</given-names> <surname>Ledden</surname></string-name>, <string-name><given-names>TJ</given-names> <surname>Brady</surname></string-name>, <string-name><given-names>BR</given-names> <surname>Rosen</surname></string-name>, and <string-name><given-names>RB</given-names> <surname>Tootell</surname></string-name>. <article-title>Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>92</volume>(<issue>18</issue>):<fpage>8135</fpage>–<lpage>8139</lpage>, <year>1995</year>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><string-name><given-names>Kalanit</given-names> <surname>Grill-Spector</surname></string-name>, <string-name><given-names>Tammar</given-names> <surname>Kushnir</surname></string-name>, <string-name><given-names>Talma</given-names> <surname>Hendler</surname></string-name>, <string-name><given-names>Shimon</given-names> <surname>Edelman</surname></string-name>, <string-name><given-names>Yacov</given-names> <surname>Itzchak</surname></string-name>, and <string-name><given-names>Rafael</given-names> <surname>Malach</surname></string-name>. <article-title>A sequence of object-processing stages revealed by fmri in the human occipital lobe</article-title>. <source>Human brain mapping</source>, <volume>6</volume>(<issue>4</issue>):<fpage>316</fpage>–<lpage>328</lpage>, <year>1998</year>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><string-name><given-names>Zoe</given-names> <surname>Kourtzi</surname></string-name> and <string-name><given-names>Nancy</given-names> <surname>Kanwisher</surname></string-name>. <article-title>Cortical regions involved in perceiving object shape</article-title>. <source>Journal of Neuroscience</source>, <volume>20</volume>(<issue>9</issue>):<fpage>3310</fpage>–<lpage>3318</lpage>, <year>2000</year>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><string-name><given-names>Russell</given-names> <surname>Epstein</surname></string-name>, <string-name><given-names>Alison</given-names> <surname>Harris</surname></string-name>, <string-name><given-names>Damian</given-names> <surname>Stanley</surname></string-name>, and <string-name><given-names>Nancy</given-names> <surname>Kanwisher</surname></string-name>. <article-title>The parahippocampal place area: recognition, navigation, or encoding?</article-title> <source>Neuron</source>, <volume>23</volume>(<issue>1</issue>):<fpage>115</fpage>–<lpage>125</lpage>, <year>1999</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><string-name><given-names>Paul E</given-names> <surname>Downing</surname></string-name>, <string-name><given-names>Yuhong</given-names> <surname>Jiang</surname></string-name>, <string-name><given-names>Miles</given-names> <surname>Shuman</surname></string-name>, and <string-name><given-names>Nancy</given-names> <surname>Kanwisher</surname></string-name>. <article-title>A cortical area selective for visual processing of the human body</article-title>. <source>Science</source>, <volume>293</volume>(<issue>5539</issue>):<fpage>2470</fpage>–<lpage>2473</lpage>, <year>2001</year>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><string-name><given-names>Narges</given-names> <surname>Doostani</surname></string-name>, <string-name><given-names>Gholam-Ali</given-names> <surname>Hossein-Zadeh</surname></string-name>, and <string-name><given-names>Maryam</given-names> <surname>Vaziri-Pashkam</surname></string-name>. <article-title>The normalization model predicts responses during object-based attention in the human visual cortex</article-title>. <source>eLife</source>, <volume>12</volume>:<fpage>e75726</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><string-name><surname>Amy</surname> <given-names>M.</given-names></string-name> <article-title>Ni, Supratim Ray, and John H.R. Maunsell. Tuned normalization explains the size of attention modulations</article-title>. <source>Neuron</source>, <volume>73</volume>(<issue>4</issue>):<fpage>803</fpage> –<lpage>813</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><string-name><given-names>Kathleen M</given-names> <surname>O’Craven</surname></string-name>, <string-name><given-names>Paul E</given-names> <surname>Downing</surname></string-name>, and <string-name><given-names>Nancy</given-names> <surname>Kanwisher</surname></string-name>. <article-title>fmri evidence for objects as the units of attentional selection</article-title>. <source>Nature</source>, <volume>401</volume>(<issue>6753</issue>):<fpage>584</fpage>–<lpage>587</lpage>, <year>1999</year>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><string-name><given-names>Sam</given-names> <surname>Ling</surname></string-name>, <string-name><given-names>Taosheng</given-names> <surname>Liu</surname></string-name>, and <string-name><given-names>Marisa</given-names> <surname>Carrasco</surname></string-name>. <article-title>How spatial and feature-based attention affect the gain and tuning of population responses</article-title>. <source>Vision research</source>, <volume>49</volume>(<issue>10</issue>):<fpage>1194</fpage>–<lpage>1204</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><given-names>Stefan</given-names> <surname>Treue</surname></string-name> and <string-name><given-names>Julio C Martinez</given-names> <surname>Trujillo</surname></string-name>. <article-title>Feature-based attention influences motion processing gain in macaque visual cortex</article-title>. <source>Nature</source>, <volume>399</volume>(<issue>6736</issue>):<fpage>575</fpage>–<lpage>579</lpage>, <year>1999</year>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><string-name><given-names>Pinglei</given-names> <surname>Bao</surname></string-name> and <string-name><given-names>Doris Y</given-names> <surname>Tsao</surname></string-name>. <article-title>Representation of multiple objects in macaque category-selective areas</article-title>. <source>Nature communications</source>, <volume>9</volume>(<issue>1</issue>):<fpage>1774</fpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><string-name><given-names>David C</given-names> <surname>Somers</surname></string-name>, <string-name><given-names>Anders M</given-names> <surname>Dale</surname></string-name>, <string-name><given-names>Adriane E</given-names> <surname>Seiffert</surname></string-name>, and <string-name><given-names>Roger BH</given-names> <surname>Tootell</surname></string-name>. <article-title>Functional mri reveals spatially specific attentional modulation in human primary visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>96</volume>(<issue>4</issue>):<fpage>1663</fpage>–<lpage>1668</lpage>, <year>1999</year>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><string-name><given-names>Sunil P</given-names> <surname>Gandhi</surname></string-name>, <string-name><given-names>David J</given-names> <surname>Heeger</surname></string-name>, and <string-name><given-names>Geoffrey M</given-names> <surname>Boynton</surname></string-name>. <article-title>Spatial attention affects brain activity in human primary visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>96</volume>(<issue>6</issue>):<fpage>3314</fpage>–<lpage>3319</lpage>, <year>1999</year>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><string-name><given-names>Maurizio</given-names> <surname>Corbetta</surname></string-name>, <string-name><given-names>Francis M</given-names> <surname>Miezin</surname></string-name>, <string-name><given-names>Susan</given-names> <surname>Dobmeyer</surname></string-name>, <string-name><given-names>Gordon L</given-names> <surname>Shulman</surname></string-name>, and <string-name><given-names>Steven E</given-names> <surname>Petersen</surname></string-name>. <article-title>Attentional modulation of neural processing of shape, color, and velocity in humans</article-title>. <source>Science</source>, <volume>248</volume>(<issue>4962</issue>):<fpage>1556</fpage>–<lpage>1559</lpage>, <year>1990</year>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><string-name><given-names>Kai J</given-names> <surname>Fox</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Birman</surname></string-name>, and <string-name><given-names>Justin L</given-names> <surname>Gardner</surname></string-name>. <article-title>Gain, not concomitant changes in spatial receptive field properties, improves task performance in a neural network attention model</article-title>. <source>Elife</source>, <volume>12</volume>:<fpage>e78392</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><string-name><surname>Tolga</surname> <given-names>Cukur</given-names></string-name>, <string-name><given-names>Shinji</given-names> <surname>Nishimoto</surname></string-name>, <string-name><given-names>Alexander G</given-names> <surname>Huth</surname></string-name>, and <string-name><given-names>Jack L</given-names> <surname>Gallant</surname></string-name>. <article-title>Attention during natural vision warps semantic representation across the human brain</article-title>. <source>Nature neuroscience</source>, <volume>16</volume>(<issue>6</issue>):<fpage>763</fpage>–<lpage>770</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><string-name><given-names>Stephen V</given-names> <surname>David</surname></string-name>, <string-name><given-names>Benjamin Y</given-names> <surname>Hayden</surname></string-name>, <string-name><given-names>James A</given-names> <surname>Mazer</surname></string-name>, and <string-name><given-names>Jack L</given-names> <surname>Gallant</surname></string-name>. <article-title>Attention to stimulus features shifts spectral tuning of v4 neurons during natural vision</article-title>. <source>Neuron</source>, <volume>59</volume>(<issue>3</issue>):<fpage>509</fpage>–<lpage>521</lpage>, <year>2008</year>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89836.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Peelen</surname>
<given-names>Marius V</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Radboud University Nijmegen</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study has the potential to shed mechanistic light on how attention mechanisms that influence competition between multiple visual stimuli are modulated by the relative neural similarity of these stimuli. The study implements an interesting experimental design that provides relevant data, especially for future modeling efforts. However, the presented evidence is considered <bold>incomplete</bold> due to some features of the design and model, as well as certain analysis choices.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89836.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
The authors report an fMRI investigation of the neural mechanisms by which selective attention allows capacity-limited perceptual systems to preferentially represent task-relevant visual stimuli. Specifically, they examine competitive interactions between two simultaneously-presented items from different categories, to reveal how task-directed attention to one of them modulates the activity of brain regions that respond to both. The specific hypothesis is that attention will bias responses to be more like those elicited by the relevant object presented on its own, and further that this modulation will be stronger for more dissimilar stimulus pairs. This pattern was confirmed in univariate analyses that measured the mass response of a priori regions of interest, as well as multivariate analyses that considered the patterns of evoked activity within the same regions. The authors follow these neuroimaging results with a simulation study that favours a &quot;tuning&quot; mechanism of attention (enhanced responses to highly effective stimuli, and suppression for ineffective stimuli) to explain this pattern.</p>
<p>Strengths:</p>
<p>
The manuscript clearly articulates a core issue in the cognitive neuroscience of attention, namely the need to understand how limited perceptual systems cope with complex environments in the service of the observer's goals. The use of a priori regions of interest, and the inclusion of both univariate and multivariate analyses as well as a simple model, are further strengths. The authors carefully derive clear indices of attentional effects (for both univariate and multivariate analyses) which makes explication of their findings easy to follow.</p>
<p>Weaknesses:</p>
<p>
There are some relatively minor weaknesses in presentation, where the motivation behind some of the procedural decisions could be clearer. There are some apparently paradoxical findings reported -- namely, cases in which the univariate response to pairs of stimuli is greater than to the preferred stimulus alone -- that are not addressed. It is possible that some of the main findings may be attributable to range effects: notwithstanding the paradox just noted, it seems that a floor effect should minimise the range of possible attentional modulation of the responses to two highly similar stimuli. One possible limitation of the modelled results is that they do not reveal any attentional modulation at all under the assumptions of the gain model, for any pair of conditions, implying that as implemented the model may not be correctly capturing the assumptions of that hypothesis.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89836.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
In an fMRI study requiring participants to attend to one or another object category, either when the object was presented in isolation or with another object superimposed, the authors compared measured univariate and multivariate activation from object-selective and early visual cortex to predictions derived from response gain and tuning sharpening models. They observed a consistent result across higher-level visual cortex that more-divergent responses to isolated stimuli from category pairs predicted a greater modulation by attention when attending to a single stimulus from the category pair presented simultaneously, and argue via simulations that this must be explained by tuning sharpening for object categories.</p>
<p>Strengths:</p>
<p>
- Interesting experiment design &amp; approach - testing how category similarity impacts neural modulations induced by attention is an important question, and the experimental approach is principled and clever.</p>
<p>- Examination of both univariate and multivariate signals is an important analysis strategy.</p>
<p>- The acquired dataset will be useful for future modeling studies.</p>
<p>Weaknesses:</p>
<p>
- The experimental design does not allow for a neutral 'baseline' estimate of neural responses to stimulus categories absent attention (e.g., attend fixation), nor of the combination of the stimulus categories. This seems critical for interpreting results (e.g., how should readers understand univariate results like that plotted in Fig. 4C-D, where the univariate response is greater for 2 stimuli than one, but the analyses are based on a shift between each extreme activation level?).</p>
<p>- Related, simulations assume there exists some non-attended baseline state of each individual object representation, yet this isn't measured, and the way it's inferred to drive the simulations isn't clearly described.</p>
<p>- Some of the simulation results seem to be algebraic (univariate; Fig. 7; multivariate, gain model; Fig. 8).</p>
<p>- Cross-validation does not seem to be employed - strong/weak categories seem to be assigned based on the same data used for computing DVs of interest - to minimize the potential for circularity in analyses, it would be better to define preferred categories using separate data from that used to quantify - perhaps using a cross-validation scheme? This appears to be implemented in Reddy et al. (2009), a paper implementing a similar multivariate method and cited by the authors (their ref 6).</p>
<p>- Multivariate distance metric - why is correlation/cosine similarity used instead of something like Euclidean or Mahalanobis distance? Correlation/cosine similarity is scale-invariant, so changes in the magnitude of the vector would not change distance, despite this likely being an important data attribute to consider.</p>
<p>- Details about simulations implemented (and their algebraic results in some cases) make it challenging to interpret or understand these results. E.g., the noise properties of the simulated data aren't disclosed, nor are precise (or approximate) values used for simulating attentional modulations.</p>
<p>- Eye movements do not seem to be controlled nor measured. Could it be possible that some stimulus pairs result in more discriminable patterns of eye movements? Could this be ruled out by some aspect of the results?</p>
<p>- A central, and untested/verified, assumption is that the multivariate activation pattern associated with 2 overlapping stimuli (with one attended) can be modeled as a weighted combination of the activation pattern associated with the individual stimuli. There are hints in the univariate data (e.g., Fig. 4C; 4D) that this might not be justified, which somewhat calls into question the interpretability of the multivariate results.</p>
<p>- Throughout the manuscript, the authors consistently refer to &quot;tuning sharpening&quot;, an idea that's almost always used to reference changes in the width of tuning curves for specific feature dimensions (e.g., motion direction; hue; orientation; spatial position). Here, the authors are assaying tuning to the category (across exemplars of the category). The link between these concepts could be strengthened to improve the clarity of the manuscript.</p>
</body>
</sub-article>
</article>