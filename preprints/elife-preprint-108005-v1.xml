<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108005</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108005</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108005.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Evolutionary Biology</subject>
</subj-group>
</article-categories><title-group>
<article-title>Importance of higher-order epistasis in protein sequence-function relationships</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Sethi</surname>
<given-names>Palash</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1373-4746</contrib-id>
<name>
<surname>Zhou</surname>
<given-names>Juannan</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
<email>juannanzhou@ufl.edu</email>
</contrib>
<aff id="a1"><label>a</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02y3ad647</institution-id><institution>Department of Biology, University of Florida</institution></institution-wrap>, <city>Gainesville</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Bitbol</surname>
<given-names>Anne-Florence</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Ecole Polytechnique Federale de Lausanne (EPFL)</institution>
</institution-wrap>
<city>Lausanne</city>
<country country="CH">Switzerland</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Walczak</surname>
<given-names>Aleksandra M</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>CNRS</institution>
</institution-wrap>
<city>Paris</city>
<country country="FR">France</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-09-24">
<day>24</day>
<month>09</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP108005</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-06-27">
<day>27</day>
<month>06</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-06-09">
<day>09</day>
<month>06</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.22.614318"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Sethi &amp; Zhou</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Sethi &amp; Zhou</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108005-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Protein sequence–function relationships are inherently complex, as amino acids at different positions can interact in highly unpredictable ways. A key question for protein evolution and engineering is how often epistasis extends beyond pairwise interactions to involve three or more positions. Although experimental data has accumulated rapidly in recent years, addressing this question remains challenging, as the number of possible interactions is typically enormous even for proteins of moderate size. Here, we introduce an interpretable machine learning framework for studying higher-order epistasis scalable to full-length proteins. Our model builds on the transformer architecture, with key modifications allowing us to assess the importance of higher-order interactions by fitting a series of models with increasing complexity. Applying our method to 10 large protein sequence-function datasets, we found that while additive effects explain the majority of the variance, within the epistatic component, the contribution of higher-order epistasis ranges from negligible to up to 60%. We also found higher-order epistasis is particularly important for generalizing locally sampled fitness data to distant regions of sequence space and for modeling an additional multi-peak fitness landscape. Our findings suggest that higher-order epistasis can play important roles in protein sequence-function relationships, and thus should be properly considered in protein engineering and evolutionary data analysis.</p>
</abstract>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id>
<institution>National Institutes of Health</institution>
</institution-wrap>
</funding-source>
<award-id>R35GM154908</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Abstract, introduction, and discussion have been revised for better language and clarity.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Understanding how amino acid sequence determines protein function is critical for biomedical engineering, synthetic biology, and evolutionary biology [<xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c4">4</xref>]. High-throughput techniques such as deep mutational scanning (DMS) have enabled the functional characterization of thousands to millions of protein variants in a massively parallel manner [<xref ref-type="bibr" rid="c5">5</xref>–<xref ref-type="bibr" rid="c24">24</xref>]. A general observation from these empirical studies is that while sequence–function relationships can often be reasonably approximated by the independent contributions of individual mutations, epistatic interactions between amino acids also play important roles and must be properly accounted for to accurately model protein function [<xref ref-type="bibr" rid="c25">25</xref>–<xref ref-type="bibr" rid="c29">29</xref>].</p>
<p>Although pairwise interactions between amino acids are both predicted by biophysical models and widely supported by empirical data [<xref ref-type="bibr" rid="c30">30</xref>–<xref ref-type="bibr" rid="c33">33</xref>], our understanding of how frequently epistasis extends beyond pairwise interactions to involve three or more positions remains incomplete. These higher-order interactions pose significant conceptual and practical challenges for deciphering protein sequence–function relationships. A thorough assessment of their prevalence is therefore essential for informing future experimental designs and the development of analytical methods [<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c34">34</xref>]. Current evidence on this question is mixed: some studies suggest that higher-order epistasis is widespread and plays a significant role in protein function [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c35">35</xref>– <xref ref-type="bibr" rid="c37">37</xref>], while others argue that protein sequence–function landscapes are simpler and epistasis beyond pairwise interactions is rare [<xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c33">33</xref>].</p>
<p>For example, a recent paper [<xref ref-type="bibr" rid="c31">31</xref>] investigated the genetic architecture for 20 protein datasets with combinatorial mutagenesis by comparing the performance of pairwise and higher-order epistatic model, while simultaneously fitting a sigmoidal function to account for non-specific epistasis. In contrast to conclusions from some of the earlier studies, the authors observed small to negligible contribution of higher-order epistasis and argued that most protein fitness landscapes can be well characterized by only additive effects and pairwise interactions. However, a key limitation of this study is that it is based on relatively small-scale protein sequence-function relationships, involving either exhaustive amino acid substitutions at no more than four sites or simultaneous single amino acid substitutions at up to 20 sites.</p>
<p>To draw general conclusions about the role of higher-order epistasis, we must extend these tests to larger protein sequence-function relationships as interactions beyond pairwise epistasis become more probable due to simple combinatorics and biophysical principles. However, addressing higher-order epistasis in full-length proteins is challenging because naive approaches to modeling higher-order interactions using parametric regression methods lead to extremely high computational demands, as well as issues in overfitting and model interpretability due to the enormous number of model parameters. Although nonparametric methods represent a desirable, and highly scalable approach by modeling higher-order interactions without explicitly fitting regression coefficients [<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c38">38</xref>], generalizing it to account for the confounding effects of non-specific epistasis using a global nonlinearity [<xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c40">40</xref>] is not straightforward due to the resulting non-Gaussian likelihood [<xref ref-type="bibr" rid="c41">41</xref>]. Artificial neural networks (ANNs) have recently been widely applied to model sequence-function relationships [<xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c42">42</xref>, <xref ref-type="bibr" rid="c43">43</xref>]. Their flexibility makes them well-suited to capturing complex, higher-order epistatic interactions. However, näively applying ANNs provides limited insight into the prevalence or structure of higher-order epistasis. This is because there is no direct correspondence between the architecture of a neural network and the specific orders of epistasis it models. As a result, the standard approach of detecting higher-order interactions by comparing models with and without higher-order terms is not readily applicable to ANNs.</p>
<p>To address this challenge, we develop a modified transformer architecture. The key feature of our method is that it enables explicit control over the maximum order of epistasis the network fits by simply adjusting the number of attention layers. This design allows us to systematically assess the contribution of higher-order interactions by fitting a series of models with increasing epistatic complexity and evaluating their predictive performance. Crucially, unlike traditional regression-based approaches, our method captures higher-order interactions implicitly through learned neural network weights. As a result, model complexity does not grow exponentially with sequence length or interaction order. This enables us to analyze higher-order epistasis involving up to eight sites, while accurately accounting for nonspecific epistasis in sequence-function datasets for full-length protein, whereas previous methods can only achieve this for a limited number of mutagenized positions.</p>
<p>We begin by introducing the main ideas of our method and demonstrating its application to a simulated dataset. Next, we present results on the role of higher-order epistasis across 10 combinatorial protein DMS datasets. We then provide a detailed analysis of how higher-order epistasis contributes to out-of-distribution predictions when the training data consist of locally sampled genotypes. Finally, we investigate how higher-order epistasis shapes a multi-peak fitness landscapes by jointly analyzing the combinatorial DMS data from four orthologous green fluorescent proteins.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Epistatic transformer for fitting higher-order epistasis</title>
<p>We aim to quantify the contribution of higher-order specific epistatic interactions to the total phenotypic variance measured for empirical protein sequence-function relationships. Our overall strategy is to fit models of increasing complexity to these datasets and examine how inclusion of higher-order interactions improves model generalization to held-out test genotypes.</p>
<p>To account for epistasis resulting from interactions among specific mutations, as well as from nonlinear scaling on the observation scale—i.e., nonspecific or global epistasis[<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c40">40</xref>]—we study models of the general form
<disp-formula id="eqn1">
<graphic xlink:href="614318v2_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here <italic>x</italic> = <italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>,, … <italic>x</italic><sub><italic>L</italic></sub> is the amino acid sequence of a protein of length <italic>L. ϕ</italic>(<italic>x</italic>) is a function that models the independent effects of individual amino acids as well as specific epistatic interactions, such that
<disp-formula id="eqn2">
<graphic xlink:href="614318v2_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Specifically, <italic>e</italic>(<italic>x</italic><sub><italic>i</italic></sub>) captures the independent effect of residue <italic>x</italic><sub><italic>i</italic></sub> at site <italic>i, e</italic>(<italic>x</italic><sub><italic>i</italic></sub>, <italic>x</italic><sub><italic>j</italic></sub>) captures the interaction effect between residues <italic>x</italic><sub><italic>i</italic></sub> and <italic>x</italic><sub><italic>j</italic></sub>. <italic>e</italic>(<italic>x</italic><sub><italic>i</italic></sub>, <italic>x</italic><sub><italic>j</italic></sub>, <italic>x</italic><sub><italic>k</italic></sub>) accounts for three-way interaction, etc. In our framework, <italic>ϕ</italic> may include epistatic interactions among up to <italic>K</italic> ≤ <italic>L</italic> residues. <italic>g</italic> is a nonlinear monotonic function that transforms the latent phenotype <italic>ϕ</italic>(<italic>x</italic>) to the measurement scale for modeling global epistasis [<xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c43">43</xref>–<xref ref-type="bibr" rid="c46">46</xref>]. Importantly, <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref> allows us to decompose the sequence-function relationship as a nonspecific component <italic>ϕ</italic>, and a specific component <italic>f</italic>, such that we may study the contribution of higher-order interactions by changing the complexity of the function <italic>ϕ</italic> with increasing values of <italic>K</italic> and examine the generalizability of the trained model to novel genotypes.</p>
<p>Fitting the function <italic>ϕ</italic> is challenging when higher-order interactions are considered, as the number of regression coefficients in <xref ref-type="disp-formula" rid="eqn2">Eq. 2</xref> grows roughly exponentially with interaction order, such that for a protein of moderate length, e.g. 100 amino acids, there are <inline-formula><inline-graphic xlink:href="614318v2_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> billion three-way interactions and 511 billion four-way interactions. Thus for large protein sequence spaces, current methods that directly fit the regression coefficients in <xref ref-type="disp-formula" rid="eqn2">Eq. 2</xref> cannot model interactions beyond pairs of sites [<xref ref-type="bibr" rid="c46">46</xref>].</p>
<p>Neural networks provide an attractive alternative for fitting epistasis interactions due to their ability at fitting complex nonlinear relationships. Here instead of directly fitting the the regression coefficients in <xref ref-type="disp-formula" rid="eqn2">Eq. 2</xref>, our strategy is to implicitly infer the function <italic>ϕ</italic> by fitting the weights of the neural network such that the network predictions <italic>ϕ</italic>(<italic>x</italic>), when decomposed as <xref ref-type="disp-formula" rid="eqn2">Eq. 2</xref> contains epistatic interactions exactly up to a given order <italic>K</italic>. This is an attractive option as the complexity of the neural network need not scale exponentially with interaction order. Furthermore, many existing machine learning packages with GPU support may be employed to ensure fast model training.</p>
<p>The challenge in implementing this strategy is that existing neural network architectures do not allow us control the highest order of specific epistasis fit by the model, thus making it impossible to test if inclusion of pairwise or higher order epistasis improves model prediction. Instead, these models usually can only fit all orders of interactions simultaneously. Consider, for example, one of the simplest architecture, the multilayer perceptron (MLP). In an MLP, the value for a hidden unit is first calculated by taking a weighted sum of inputs across different positions. To model nonlinear relationships between the inputs, the sum is then passed through a nonlinear activation function. Translating this to the language of genotype-phenotype models, we see that the model performs global epistasis-like operations on the input sequence where the underlying phenotype is additive with respect to the positions [<xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c43">43</xref>]. We can add more layers to perform similar operations, which allows one to model complex specific epistasis among the input positions. However, one can easily see that the function by an MLP cannot be decomposed into a <italic>ϕ</italic> and a <italic>g</italic> component as in <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>, due to of the lack of compartmentalization of global epsitasis and specific epistasis operations.</p>
<p>Here, we developed a novel neural network architecture, the ‘epistatic transformer’, for modeling specific epistatic interactions of fixed orders. The model allows one to fit <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>, with <italic>ϕ</italic> containing specific epistasis among up to <italic>K</italic> = 2<sup><italic>M</italic></sup> sites with <italic>M</italic> attention layers, such that by setting <italic>M</italic> = 1, 2, 3, …, we can fit pairwise, four-way, eight-way, or higher-order interaction models. Our model is based on a transformer architecture [<xref ref-type="bibr" rid="c47">47</xref>] typically employed for regression and classification tasks. The high-level model architecture is shown in <xref rid="fig1" ref-type="fig">Figure 1a</xref>. The input protein sequence <italic>x</italic><sub>1</sub>, …, <italic>x<sub>L</sub></italic> is first used to generate amino acid embeddings <inline-formula><inline-graphic xlink:href="614318v2_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> of dimension <italic>d</italic>. The embeddings are then passed through <italic>M</italic> modified multi-head attention layers, such that the output embeddings <inline-formula><inline-graphic xlink:href="614318v2_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> contain specific interactions exactly up to order 2<sup><italic>M</italic></sup>. We then calculate a single weighted sum of the flattened embeddings, which is passed through a nonlinear function <italic>g</italic> for modeling global epistasis.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Epistatic transformer for jointly modeling fixed-order specific epistasis and nonspecific epistasis.</title>
<p>a. Overall model architecture. The input amino acid token <italic>x</italic><sub><italic>l</italic></sub> is first used to generate position-specific amino acid embeddings <italic>z</italic><sup>0</sup>. The embeddings are then passed through <italic>M</italic> layers of modified multi-head attention (MHA) such that the output embeddings contain specific epistasis up to order 2<sup><italic>M</italic></sup>. Left: Details for a single MHA layer. Hidden states <italic>z</italic><sup><italic>m</italic></sup> from the previous layer is passed through linear layers to generate the query (<italic>Q</italic>) and key (<italic>K</italic>) tensors, while the raw embeddings <italic>Z</italic><sup>0</sup> are used directly as the value (<italic>V</italic>) tensor. Attention weights are calculated by taking scaled dot products between <italic>Q</italic> and <italic>K</italic>, which are used to generate the final output of this layer. This bypassing of the raw embedding <italic>Z</italic><sup>0</sup>, together with the removal of LayerNorm, softmax operation on the attention weights, and the feedforward layer, allow us to model only interactions among up to 2<sup><italic>M</italic></sup> sites when using <italic>M</italic> MHA layers. b. Automatic hyperparameter search scheme using Optuna [<xref ref-type="bibr" rid="c48">48</xref>]</p></caption>
<graphic xlink:href="614318v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We model interaction among input positions through the multi-head attention (MHA) mechanism [<xref ref-type="bibr" rid="c47">47</xref>]. In a standard attention layer, three different linear transformations are applied to an input embedding vector to generate the query <italic>Q</italic>, key <italic>K</italic>, and value <italic>V</italic> vectors. Attention scores are computed by comparing the query of a focal position to the keys at all positions in the input sequence. A weighted sum of values across all positions with weights provided by the attention scores are then added to the input embedding to produce the position specific outputs. Importantly, pairwise interaction occurs during the computation of attention scores, where each query token interacts with all key tokens through a dot product operation followed by a softmax function. We can see that as one increases the number of attention layers <italic>M</italic>, the order of interactions occurring at a position increases roughly exponentially. This property makes the transformer architecture a potential framework for fitting a series of epistatic models with increasing complexity.</p>
<p>However, the original transformer architecture is not sufficient for fitting specific epistasis, as standard transformer models have the same problem of mixing global epistasis and specific epistasis operations. Here we made three key modifications to the MHA layer (<xref rid="fig1" ref-type="fig">Figure 1a</xref>, left panel; Methods) to allow us to precisely control the orders of specific epistasis in the intermediate model output and to better fit the complex epistatic interactions in the data. First, we removed all non-linear operations in the hidden layers of the model, such that the <italic>ϕ</italic> function we infer only contains specific interactions. In particular, we remove the softmax operation on the attention weights of the MHA layer, the LayerNorm [<xref ref-type="bibr" rid="c49">49</xref>] operation on the embeddings across all positions, as well as the feedforward network applied to each position. Second, to ensure that the order of specific epistasis in our model scales exactly as 2<sup><italic>M</italic></sup> with the number of layers <italic>M</italic>, we replace the value (<italic>V</italic>) input to each MHA layer with the raw amino acid embedding <inline-formula><inline-graphic xlink:href="614318v2_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Thus, by setting <italic>M</italic> = 1, 2, 3, we are able to fit <xref ref-type="disp-formula" rid="eqn2">Eq. 2</xref> with up to pairwise, four-way, and eight-way interactions, respectively (see Supplemental Methods for proof). Third, we used a special positional encoding for the input amino acids. Without positional information, the same residues across different positions will interact in the same manner. To prevent this, typical transformer models combine continuous positional encoding with the input embedding. However, in our experiments, we found that this is insufficient to capture the range of interactions between residues in real protein datasets. Thus, in our model, we introduce a discrete positional encoding scheme, where the original amino acid token <italic>x</italic><sub><italic>l</italic></sub> is transformed to <inline-formula><inline-graphic xlink:href="614318v2_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>a</italic> is the alphabet size (usually <italic>a</italic> = 20), thereby effectively allowing us to treat amino acids at different positions in completely different ways.</p>
<p>While our modified transformer architecture allows us to control the maximal order of specific epistasis incorporated into the model, we also experimented with different choices of nonlinear function <italic>g</italic> for mapping the sum of the flattened embeddings of the last transformer layer to a scalar value in order to model global epistasis. Specifically, we tested the performance of sums of various numbers of independent sigmoid functions[<xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c46">46</xref>]. We found that a single scaled sigmoidal function with four parameters <inline-formula><inline-graphic xlink:href="614318v2_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> performs equally well as more complex nonlinear functions for all datasets examined. For all subsequent analyses we therefore used this simple logistic link function to model nonspecific epistasis.</p>
<p>Our preliminary experiment showed that the performance of the trained models can be sensitive to choices of certain hyperparameters. Thus, to ensure that we fit the optimal model within a family of models with <italic>M</italic> attention layers, we used a hyperparameter optimization strategy. Specifically, we train our model iteratively on a training dataset while varying the values for hyperparameters including batch size, hidden dimensions, dropout rate, and number of epochs, while keeping the number of attention heads and learning rate constant to ensure a low dimensional search space. The trained model is evaluated on a validation dataset to inform the choice of hyperparameters of the next iteration. We used Optuna [<xref ref-type="bibr" rid="c48">48</xref>] to automate the hyperparameter search, with 200 updates and validation <italic>R</italic><sup>2</sup> as the criterion (<xref rid="fig1" ref-type="fig">Figure 1b</xref>). The model trained under the optimal hyperparameters was then evaluated on the independent test dataset to produce the test <italic>R</italic><sup>2</sup> value.</p>
</sec>
<sec id="s2b">
<title>Epistatic transformer recapitulates the genetic architecture of a simulated fitness landscape</title>
<p>We first apply our model to a simulated dataset to characterize its behavior when the ground truth is known. We simulated a sequence-function map for a binary sequence space with 13 sites, with specific epistatic interactions up to order 8 (see Supplemental Methods). The choice of a small genotype space allows us to easily generate data for all possible genotypes and analytically characterize the formal structure of epistasis in the simulated data and the predictions in terms of variance components (proportions of total phenotypic variance explained by additive effects, pairwise, and higher-order interactions), which is not possible for larger sequence spaces.</p>
<p><xref rid="fig2" ref-type="fig">Figure 2a</xref> summarizes the structure of specific epistasis in our simulated data in terms of the fraction of total phenotypic variance due to contributions of additive effects and epistatic interactions up a specified order (i.e., cumulative variance explained). We can see that additive effects only accounts for 10% of the total variance, whereas additive effects and pairwise interactions together explain 30% of variance. Further, 70% of the total phenotypic variance is due to interactions among up to four sites. Finally, considering interactions among up to 8 sites allow us to explain all variance in the data.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Epistatic transformer is able to capture the genetic architecture of simulated sequence-function relationships.</title>
<p>Data was simulated for a binary fitness landscape with 13 sites, with specific epistasis among up to 8 sites. a. Test <italic>R</italic><sup>2</sup> can be used to estimate cumulative variance components. Bar plot shows the cumulative variance explained by each order of interaction (proportion of variance due to epistasis up to a given order). Error plot shows the test <italic>R</italic><sup>2</sup> for additive, pairwise, four-way, and eight-way interaction models using epistatic transformer to 90% of simulated data. Error bars correspond to one standard deviation with 5 random train-test splits. b. Epistatic transformer recapitulates true variance components. Bar plots correspond to variance components(proportion of variance explained by interactions of a given order) for the simulated landscape, the pairwise, 4-way, and 8-way models. Note that pairwise model only contains interaction up to second order, and 4th-order model only contains interaction among up to 4 positions, while 8th-order model model contains interaction among up to 8 positions with variance components aligning with the ground truth. c. Epistatic transformer model becomes increasingly complex with longer training. The three curves correspond to the variance component decomposition of the full landscape inferred by the 8th-order model at each training epoch. Blue: variance components for additive and pairwise interactions. Orange: variance components for three-way and four-way interactions. Green: variance components for interaction order higher than four.</p></caption>
<graphic xlink:href="614318v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Next, we examine if we can infer the structure of specific epistasis in the data by fitting a series of epistatic transformer model with increasing complexity. Specifically, we fit the epistatic transformer model to 90% of randomly sampled data and examine its out-of-sample performance when the number of MHA layer is set to be 1, 2, or 3, corresponding to pairwise, 4th-order, and 8th-order interaction models (<xref rid="fig2" ref-type="fig">Figure 2a</xref>). To ensure that our epistatic transformer model can capture most epistatic interactions even when underparameterized, we constrain the hidden dimension to fewer than 8 and limit the number of attention heads to fewer than 2. Under these constraints, the best-performing epistatic transformer models with 1, 2, and 3 layers contain up to 1,926, 3,014, and 4,102 parameters, respectively—all smaller than the size of the training dataset. We see that the test <italic>R</italic><sup>2</sup> increases with the interaction order of the model and remains slightly below—but close to—the ground truth cumulative variance explained, despite all models having fewer parameters than training data points. This results confirm that our model can be used to provide lower bounds for the contribution of pairwise and higher-order epistasis when fit to randomly sample training data.</p>
<p>We provided a mathematical proof (Supplemental Methods) that highest order of specific epistasis fit by our model with <italic>M</italic> MHA layers is exactly 2<sup><italic>M</italic></sup>. To confirm this result and demonstrate that an all-order epistatic transformer model can capture arbitrary patterns of epistasis in the data, we directly compare the variance components (i.e., the fraction of variance explained by additive effects or epistasis of specific orders) between the data and the sequence-function relationships reconstructed by the low-order and all-order epistatic transformer models (<xref rid="fig2" ref-type="fig">Figure 2b</xref>). We see that for the low-order interaction models, the variance components are truncated at the highest order the model can capture, e.g., a 4th-order interaction model with 2 epistatic MHA layers only contains interactions among up to 4 sites. This results confirm that we can precisely control the order of specific epistasis in our model by varying the number of MHA layers. In contrast, the 3-layer model contains variance components up to order 8. Furthermore, comparing the top and bottom panel of <xref rid="fig2" ref-type="fig">Figure 2b</xref>, we can see that the variance components of the inferred landscape can faithfully recapitulate the ground-true variance components.</p>
<p>Lastly, we examine how our model fits higher-order interaction during training. In <xref rid="fig2" ref-type="fig">Figure 2c</xref>, we separately plotted the variance components of the 8th-order interaction model due to additive + pairwise interaction, three-way + four-way interaction, and interactions of order higher than 4 at different points along the training process. We see that when training begins, the model is predominantly fitting the additive and pairwise components. As training continues, the three-way and four-way interactions become increasingly more important, while the low-order contribution drops. Finally, we see an increase in the contribution of higher-order (<italic>&gt;</italic> 4) interactions late in the training process (after epoch 500), leading to the convergence of of the variance components of the model landscape to the ground truth. This result shows an interesting behavior of our model which seems to first learn the simple features of the sequence-function relationship (additive and pairwise), before preceding to model the more complex interactions.</p>
</sec>
<sec id="s2c">
<title>Application to experimental protein sequence-function data</title>
<p>To use the epistatic transformer model to characterize the importance higher-order interactions in empirical protein fitness landscapes, we first curated a list of 10 deep mutational scanning datasets. Our criteria for choosing suitable datasets were (1) the sequence space should be large to allow higher-order epistasis to play significant roles. Specifically, the number of variable sites should be ideally larger than 15 for binary DMS datasets, and at least 5 when all amino acid mutations were constructed at every site. (2) the dataset should contain a large proportion of higher-order mutants, i.e., genotypes that are more than 3 mutations away from a reference wild type, to provide enough data for our model to detect higher-order interactions. We also applied a quality control step by fitting a simple linear regression to the data and only retain datasets where the test <italic>R</italic><sup>2</sup> of the additive model is over 0.3 (Methods).</p>
<p>The resulting 10 datasets (<xref rid="tbl1" ref-type="table">Table 1</xref>) encompass a diverse set of proteins and phenotypes including protein abundance, binding, fluorescence, and cellular fitness. Importantly, the selected experiments were conducted in sequence spaces significantly larger than datasets used in previous studies on higher-order epistasis in proteins, with mutations spanning tens of amino acid sites, extending up to the entire length of the protein (e.g. the GFP datasets). The chosen datasets also exhibit a wide range of data distribution. In Supplemental Figure 1, we can see that while the measured genotypes are centered around the wild type sequence in some datasets (for example the GFP and His3 data), in other datasets the genotypes are concentrated at an intermediate distance to the WT (for example, GRB-1). Furthermore, the sparsity of the data also varies greatly. For instance, in the GRB-3-abundance and binding data, virtually all genotypes have been measured, whereas for large sequence spaces the proportion of measured genotype usually decays exponentially with Hamming distance, resulting in extremely low sampling of distant regions of the sequence space (e.g. data for various GFPs).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>Combinatorial protein mutagenesis datasets used in this paper.</title></caption>
<graphic xlink:href="614318v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>We applied the epistatic transformer model and a linear model with independent contributions from sites to these 10 datasets to examine how higher-order epistasis improves the ability to predict the phenotypes of novel sequences. For each dataset, we randomly assign random samples of all measured genotypes as training data. We used epistatic transformer models to fit pairwise, 4th-order, and 8th-order models in the form of <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>, corresponding to models with 1, 2, and 3 layers of modified MHA attention. In addition, we also fit an additive model of the form in <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>, which again includes composition with a sigmoid function in order to address any non-specific epistasis. To ensure that we fit the best model to the training data, we used the procedure in <xref rid="fig1" ref-type="fig">Figure 1b</xref> to identify the optimal hyperparameter combination using cross validation on a validation dataset, with 200 tuning steps (Methods).</p>
<p>We first observe that the additive model often provides a reasonably good fit to the data and explains the predominant proportion of total variance (test <italic>R</italic><sup>2</sup> <italic>&gt;</italic> 0.55 for all datasets, when trained on 80% of randomly sampled genotypes; Supplemental Figure 2. For some datasets, the additive model can explain as much as 90% of the variance in the test data (e.g. GRB-3-abundance, His-S5, and His-S12). We also found that incorporating global non-specific epistasis to the model with only additive effects leads to uniform improvement in performance (Supplemental Figure 2). The improvement varies across datasets and range from less than 10% improvement in <italic>R</italic><sup>2</sup> in the three GRB datasets, to up to 30% in the cgreGFP dataset.</p>
<p>We next summarize the performance of our epistatic transformer models trained on 80% of randomly sampled genotypes (<xref rid="fig3" ref-type="fig">Figure 3</xref>). Here we use two metrics to gauge how incorporating epistasis and in particular, higher-order epistasis improves model prediction. The first metric, <inline-formula><inline-graphic xlink:href="614318v2_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula> measures the proportion of total variance in the test data explained by all orders of specific epistasis, equal to the difference in test <italic>R</italic><sup>2</sup> between the 8-th order epistatic transformer model with <italic>M</italic> = 3 (i.e., with 3 epistatic MHA layers, which fits up to 8-th order interactions) and the additive model. The second metric ‘percent epistatic variance’ measures the relative contribution of pairwise or higher-order interactions. This metric is calculated by first taking the difference in <italic>R</italic><sup>2</sup> between an epistatic transformer model with given number of MHA layers and the preceding, less complex model. The percentage of epistatic variance is then acquired by dividing the resulting quantity by <inline-formula><inline-graphic xlink:href="614318v2_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (for example, to quantify the importance of 3- and 4-th order epistasis, we calculate <inline-formula><inline-graphic xlink:href="614318v2_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Thus, the product of <inline-formula><inline-graphic xlink:href="614318v2_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and percent epistatic variance returns the percentage of total variance in phenotype explained by certain orders of epistasis.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Importance of pairwise and higher-order specific epistasis in 10 experimental protein-sequence function datasets.</title>
<p>For each dataset, pairwise, 4th-order and 8th-order models were fit using epistatic transformer with 1, 2, and 3 layers of epistatic multihead attention (MHA), along with an additive model. All models contain a final nonlinear activation function mapping a scalar value to the measurement scale for modeling non-specific epistasis. Models were fit to 80% of training data generated by randomly sampling all available data and evaluated on random test genotypes. In each panel, the number <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="614318v2_inline19.gif"/></inline-formula> on the upper right corner is equal to the proportion of total variance in the test data explained by all orders of specific epistasis, equal to the difference between the <italic>R</italic><sup>2</sup> of the 8-th order epistatic transformer model and the additive model. Importance of epistatic interactions of different orders is measured by percent epistatic variance, equal to the gain in <italic>R</italic><sup>2</sup> by fitting an additional layer of MHA, normalized by <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="614318v2_inline20.gif"/></inline-formula>. For example, the percent epistatic variance due to pairwise interactions is equal to the difference in <italic>R</italic><sup>2</sup> between the pairwise and additive model divided by <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="614318v2_inline21.gif"/></inline-formula>, and the percent epistatic variance due to 3-way and 4-way interactions is equal to the difference in <italic>R</italic><sup>2</sup> by the 4-th order model and the pairwise divided by <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="614318v2_inline22.gif"/></inline-formula>. Error bars represent 1 standard deviation calculated from 3 replicates.</p></caption>
<graphic xlink:href="614318v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We first observe that the contribution of specific epistasis can vary significantly across datasets, with <inline-formula><inline-graphic xlink:href="614318v2_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (top right corner of each panel in <xref rid="fig3" ref-type="fig">Figure 3</xref>) ranging from 0.01 (His3-S5 and His3-S12) to 0.23 (GRB-1). Second, we found that while pairwise interactions are typically the predominant form of epistasis, consistent with previous findings, higher-order epistasis can also play moderate to substantial roles. We found the strongest contribution of higher-order epistasis in the GRB-1 dataset, wherein three-way and four-way interactions together account for 62% of the epistatic variance, compared to 36% attributed to pairwise interactions (<xref rid="fig3" ref-type="fig">Figure 3</xref>). In contrast, we observed that interactions of order <italic>&gt;</italic> 4 account for very small amount of epistatic variance, suggesting that three-way and four-way interactions are the predominant forms of epistasis in this dataset.</p>
<p>We also observed moderate contributions by higher-order epistasis in other datasets (e.g. AAV2-Capsid, ppluGFP, and CreiLOV), where epistasis of order 3-8 collectively accounts for roughly 1<italic>/</italic>5 to 1<italic>/</italic>3 of the total epistatic variance. Additionally, we found that the higher-order epistasis can make more substantial contributions when training data is more sparse, sometimes accounting for <italic>&gt;</italic> 80% of the total epistatic variance (e.g. the AA2-Capisd and cgreGFP dataset, Supplemental Figure 3). Similar phenomenon has been observed previously, for instance, in non-parametric kernel methods [<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c38">38</xref>]. This suggests that higher-order models may, counterintuitively, provide a more parsimonious fit to sparse training data than the pairwise interaction model.</p>
</sec>
<sec id="s2d">
<title>Improvement in prediction accuracy in higher-order models is due to specific epistasis</title>
<p>In order to understand why higher-order epistasis improves model prediction, we next examined the detailed model fit for the pairwise and higher-order models for the two datasets with the largest contribution of higher-order interactions, specifically the AAV2-Capsid and the GRB-1 datasets. In <xref rid="fig4" ref-type="fig">Figure 4</xref>, we show scatter plots comparing the actual experimental measurements for the test genotypes vs. the predicted latent phenotype (<italic>ϕ</italic>) before activation by the global epistasis function <italic>g</italic> (left panel) and the final predicted phenotype (<italic>y</italic>, containing both non-specific and specific epistasis, right panel). We first see that both datasets contain a prominent nonlinear relationship (red curve on the left panels), where genotypes with low predicted <italic>ϕ</italic> values, and genotypes with high predicted <italic>ϕ</italic> (to a lesser extent) are bounded by the sigmoid function. Next, we see that the final predictions for both models and datasets appear to have a linear correlation with the measurements. We also notice that the nonlinear functions for the pairwise and 8-th order models are highly similar, although the nonlinear function in these two models are fit separately.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Improvement in prediction accuracy in higher-order models is due to specific epistasis.</title>
<p>Scatter plots show the observed phenotypes (<italic>y</italic>) vs. latent model predictions (<italic>ϕ</italic>), or the final model predictions (<italic>ŷ</italic>)) for the test genotypes, for the pairwise and 8th-order epistatic transformer models fit to the GRB-1 and AAV2-Capsid datasets. Models were fit to 80% of randomly sampled training data.</p></caption>
<graphic xlink:href="614318v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Together, these results confirm that both the pairwise and higher-order epistatic transformer models have adequately captured the non-specific epistasis in the data, suggesting that the improvement by the higher-order order epistatic model over the pairwise model is due to the its ability to better fit the residual variance due to specific higher-order epistasis.</p>
<p>It is noteworthy that, for both the GRB-1 and AAV2 Capsid datasets, the 8th-order model offers better predictions across the entire range of measured phenotypes. This is evident from the reduced spread of test data points for different observed <italic>y</italic> values, both on the latent and predicted phenotype scales. Moreover, the improvement in prediction accuracy is most significant for genotypes with high or low observed <italic>y</italic> values (Supplemental Figure 4), indicating the higher-order model’s ability to provide more parsimonious fits for genotypes at the extremes of the measurement scale.</p>
</sec>
<sec id="s2e">
<title>Higher-order epistasis is important for predicting distant genotypes</title>
<p>Many of the datasets analyzed here consist of genotypes generated by introducing mutations to many positions of a wild type or reference genotype. This naturally leads to highly localized samples within the complete genotype space. A critical question then arises regarding whether models trained on these local data can be generalized to genotypes distant from the training samples. In this section, we provide a comprehensive characterization of our trained epistatic models in terms of their performance at predicting the phenotypes for genotypes at increasing distances to the training data. Specifically, we focus on the AAV2-Capsid and the cgreGFP dataset, where the data consist of measurements of genotypes in a dense local cloud concentrated near the wild-type and sparse higher-order mutants at greater mutational distances (Supplemental Figure 1). Furthermore, in both datasets we observed moderate improvement in predictive performance on randomly sampled test genotypes by incorporating higher-order epistasis. Thus, we are interested in whether modeling higher-order epistasis leads to more pronounced performance gains in certain classes of genotypes.</p>
<p>Specifically, we used the mean Hamming distance as a measurement of the proximity of a genotype to the training data, such that for a test genotype <italic>x</italic>, its mean Hamming distance <inline-formula><inline-graphic xlink:href="614318v2_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>D</italic>(<italic>x, x</italic><sup><italic>′</italic></sup>) is the Hamming distance between <italic>x</italic> and a training genotype <italic>x</italic><sup><italic>′</italic></sup>. We then grouped test genotypes into discrete bins by their mean Hamming distance and calculated the percent epistatic variance values for each bin using epistatic transformer models trained with a random training set consisting of 20% of the total data. Here we focused on models trained on 20% of randomly sampled data, instead of 80% as in <xref rid="fig3" ref-type="fig">Figure 3</xref>, to increase the number of test genotypes at higher distance classes from the training data, thereby enabling a better assessment of the model’s generalizability.</p>
<p>For the AAV2-Capsid dataset, we first observed that the density of test genotypes decreases with mean distances, as a result of the localized data distribution (<xref rid="fig5" ref-type="fig">Figure 5a</xref>, top panel). This is accompanied by an overall decrease in the observed phenotypic scores (<italic>y</italic>) for genotypes in each class (<xref rid="fig5" ref-type="fig">Figure 5a</xref>, second panel). Furthermore, we also found that the model performance decreases monotonically with mean distance, such that the <italic>R</italic><sup>2</sup> of the additive model starts out at around 0.8 for nearby genotypes and drops to 0.4 for distant genotypes (<xref rid="fig5" ref-type="fig">Figure 5a</xref>, third panel). The <italic>R</italic><sup>2</sup> for the full epistatic transformer model with 3 MHA layers showed similar trend of decline with increasing distance, but maintained a relatively constant advantage over the additive model, with <inline-formula><inline-graphic xlink:href="614318v2_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula> ranging from 0.05 to 0.14. Importantly, we found that the contribution of higher-order interactions measured in terms of percent epistatic variance increases substantially with distance to the data (<xref rid="fig5" ref-type="fig">Figure 5a</xref>, bottom panel). Specifically, for nearby genotypes, the pairwise and the 4-th order model explain similar proportions of variance. However, as we move to more distant genotypes, the proportion of variance explained by the pairwise model decreases, which is accompanied by a dramatic increase in the contribution from higher-order interactions, such that at distance <italic>&gt;</italic> 10, interactions of order 3-8 virtually account for all variance due to epistasis, translating to 10 − 12% of additional total variance explained in the test data.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Importance of higher-order epistasis in predicting phenotypes for distant genotypes in the AAV2-Capsid (a) and the cgreGFP (b) datasets.</title>
<p>For each dataset, genotypes are binned to discrete distance classes by their mean Hamming distances to the training data, which consist of 20% of randomly sampled genotypes. For both datasets, we retain only distance classes where the additive model has a test <italic>R</italic><sup>2</sup> <italic>&gt;</italic> 0.3. Top panel: Distribution of mean Hamming distance in the randomly sampled test data. Second panel: Distribution of the observed phenotypic values (<italic>y</italic>) for each distance class. Third panel: Test <italic>R</italic><sup>2</sup> under the additive model and the 8-th order epistatic transformer for genotypes at different distance classes. The gap between the two curves is equal to <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="614318v2_inline23.gif"/></inline-formula> for each distance class. Bottom panel: importance of specific pairwise and higher-order epistasis at different distance classes, measured by percent epistatic variance, equal to the gain in <italic>R</italic><sup>2</sup> by fitting an additional layer of MHA, normalized by <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="614318v2_inline24.gif"/></inline-formula>. All metrics were calculated for models fit to one training sample consisting of 20% of randomly sampled genotypes. Error bars represent 1 standard deviation calculated by bootstrapping the test data with 10 replicates.</p></caption>
<graphic xlink:href="614318v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>One drawback of using the <italic>R</italic><sup>2</sup> values within a distance class as a measure of model performance is that it may be sensitive to the overall phenotypic variance within a distance class. For example, if all genotypes within the class have the same score (e.g. are all nonfunctional), the models can predict the correct constant, but this accuracy can not be correctly characterized by the <italic>R</italic><sup>2</sup> value. Thus, we repeat the same analysis above by comparing the mean squared errors (MSE) between the model predictions and the true phenotypes within each distance classes (Supplemental Figure 5). The new analysis with MSE confirms the results in <xref rid="fig5" ref-type="fig">Figure 5</xref>, in that the pairwise model reduces the prediction error of the additive model primarily for genotypes close to the data, and that the performance gain by higher-order models increases with more distant genotypes.</p>
<p>We found similar data and phenotype distribution in the cgreGFP dataset (<xref rid="fig5" ref-type="fig">Figure 5b</xref>, first three panels). The predictive accuracy of our models drop more rapidly than in the AAV2-Capsid dataset, such that the test <italic>R</italic><sup>2</sup> of the additive model drops below 0.34 for genotypes at mean distance <italic>&gt;</italic> 9. The performance of the 3-layer epistatic transformer model also decreases with distance, but at a slower rate than the additive model, with a <italic>R</italic><sup>2</sup> = 0.48 at distance 9, equivalent to <inline-formula><inline-graphic xlink:href="614318v2_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, compared with <inline-formula><inline-graphic xlink:href="614318v2_inline15.gif" mimetype="image" mime-subtype="gif"/></inline-formula> distance 5. The pairwise interactions have a strong contribution (accounting for 61% of epistatic variance) at distance 5. However, for further distances, higher-order interactions become increasingly more important, such that at distance ≥ 8, almost all the epistatic variance is due to interactions among more than 4 sites, which explains an additional 10% of the total variance within the distance classes. Similar results were found when the analysis were repeated using MSE (Supplemental Figure 5).</p>
<p>In addition to the AAV2-Capsid and the cgreGFP dataset, we also apply the same analyses to four other datasets (His-S2, -S5, -S12, and ppluGFP) where data contains sufficient numbers of distance classes (Supplemental Figure 6). We observed that the higher-order epistasis plays moderate to important roles at further distances in three of the four datasets, with the exception being the His-S12, where little effects of epistasis was observed at all distances. We observed that higher-order epistasis plays a moderate to significant role at greater distances in three of the four datasets, with the exception of the His-S12 dataset, where epistatic effects were minimal or absent at all distances.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Higher-order epistasis in a multi-peak fitness landscape, consisted of four green fluorescent protein (GFP) orthologs (avGFP, amacGFP, ppluGFP2, cgreGFP).</title>
<p>a. PCA coordinates for the one-hot embeddings of the all protein genotypes. Genotypes are extremely centered around the four wild types (WT), which exhibit varying degrees of sequence divergence. b. Scatter plots of shared mutational effects among the four GFPs, fit using separate additive model with a nonlinear sigmoid activation function. c. Higher-order epistasis allows better generalization to distant regions in sequence space. Models were fit use single and double mutant data for all GFPs. Models were tested in different distance classes, each containing all genotypes at given Hamming distance to their corresponding WT sequence. Error bars represent 1 standard deviation calculated by resampling random 90% of the test genotypes with 10 replicates. d. Higher-order epistasis allows better generalization across local peaks. Rows: GFP orthologs used to train the models. Columns: GFP orthologs used to test performance of the trained model. <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="614318v2_inline25.gif"/></inline-formula> at top right corner of each panel measures the proportion of total variance explained by specific epistasis among up to 8 positions. Error bars correspond to 1 standard deviation across three model replicates with different training and test genotypes.</p></caption>
<graphic xlink:href="614318v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Together, our analyses using the AAV2-Capsid and the cgreGFP dataset showed that higher-order interactions can be more important for generalizing models trained on locally sampled data to distant regions of the sequence-function relationship. Furthermore, assessing the importance of higher-order epistasis using test data drawn from the same distribution as the training samples (e.g. in <xref rid="fig3" ref-type="fig">Figure 3</xref>) can be misleading as the signal of higher-order epistasis may be much more prominent for distant genotypes, but this can be easily obscured as distant genotypes typically consist of a small proportions of the all available data.</p>
</sec>
<sec id="s2f">
<title>Higher-order epistasis for a multi-peak fitness landscape</title>
<p>Most of the datasets we have considered so far (with the exception of the GRB datasets and the CreiLOV dataset) consist of locally sampled genotypes generated by mutagenizing a WT sequence. These local fitness landscapes may not be ideal for detecting higher-order epistasis, which exerts its effects by modifying mutational effects or pairwise interactions across different genetic backgrounds ([<xref ref-type="bibr" rid="c38">38</xref>]). For locally sampled data, these effects may exhibit strong similarity due to lack of divergence between the genetic backgrounds, thereby making higher-order epistasis challenging to detect. To circumvent this limitation of locally sampled fitness landscape data, in this section, we examine the importance of higher-order epistasis by modeling a multi-peak landscape consisting of locally sampled data for four protein orthologs. In particular, the dataset we use is derived from the deep mutational scanning experiments for four full-length green fluorescent proteins, avGFP, amacGFP, cgreGFP, and ppluGFP [<xref ref-type="bibr" rid="c20">20</xref>] (note that cgreGFP and ppluGFP2 were studied individually in the previous section). A single dataset was generated by merging the four GFP datasets based on the multiple sequence alignment of the four WT sequences [<xref ref-type="bibr" rid="c20">20</xref>]. The four proteins exhibit moderate to very high sequence divergence ranging from 18% (between avGFP and amacGFP) to 83% (between ppluGFP2 and amacGFP) (<xref rid="fig6" ref-type="fig">Figure 6a</xref>).</p>
<p>This divergence between the genetic backgrounds among the four local GFP peaks can lead to higher variability in the local mutational and epistatic effects than when the local peaks are individually considered, which may allow us to better detect higher-order epistasis. A previous analysis of this combined dataset [<xref ref-type="bibr" rid="c20">20</xref>] showed that the four fitness peaks shared some gross similarities. For example, all GFPs show a threshold effect where the fluorescence level typically exhibits a sudden drop for variants accumulating mutations past a certain number. The authors also found that mutations in the chromophore are uniformly deleterious, while mutations on buried residues had stronger effect than surface sites. But the authors also noted substantial differences in the structure of these local landscapes, including the sharpness of the peaks and the importance of local epistasis [<xref ref-type="bibr" rid="c20">20</xref>]. To provide additional coarse characterization of the dissimilarity among the four local peaks, we first directly examine how the effects of mutations vary between GFP orthologs. Specifically, for each dataset we fit a model with additive mutational effects and a sigmoidal activation function. The parameters of the sigmoid function was shared among all four datasets to ensure the inferred mutational effects are on the same scale. In <xref rid="fig6" ref-type="fig">Figure 6b</xref>, we showed the scatter plots for the effects of shared mutations between pairs of GFPs. We found that mutational effects overall have low to moderate correlation between GFP orthologs, with Pearson <italic>r</italic> ranging between 0.15 to 0.46. Interestingly, we did not observe correlation between sequence divergence and mutational effect similarity. For example, the strongest correlation (<italic>r</italic> = 0.46) is between amacGFP and ppluGFP2, which have 83% sequence divergence.</p>
<p>With this basic understanding of the divergence between the local GFP fitness landscapes, we proceed to quantify the importance of higher-order epistasis in this multi-peak landscape. We first fit the epistatic transformer model to randomly sampled variants from all four GFP orthologs. In Supplemental Figure 7, we show the percent epistatic variance values for the pairwise, 4-th, and 8-th order epistatic transformer models trained on 80% of all data on randomly sampled test genotypes from all four local peaks. We first see that fitting epistasis of all orders leads to 12% increase in test <italic>R</italic><sup>2</sup>. In particular, we found that pairwise interactions is the predominant form of epistasis, while higher-order epistasis accounts for 15% of the total test set epistatic variance.</p>
<p>This results suggest that pairwise interactions are capable of capturing most of the phenotypic variances among distant local fitness peaks, potentially by accounting for the divergence in local mutational effects (e.g. <xref rid="fig6" ref-type="fig">Figure 6b</xref>). Further, higher-order interactions do seem to play a role in determining the overall structure of this multi-peak landscape, although the role assessed by prediction for randomly sample genotypes seems moderate.</p>
<p>Next, we trained our models on two tasks where higher-order epistasis may play more critical roles. For the first task, we fit our models simultaneously to all four GFPs, but only used training data consisting of single and double mutants. We then assessed the performance of pairwise and higher-order epistatic transformer models on predicting phenotypes for distant, higher-order mutants. Our hypothesis is that although these local data only contain information for the effects of mutations and pairwise interactions, our models may nonetheless learn a coarse approximation of the actual logic of higher-order interactions based on how these local effects vary across the four divergent genetic background. This may lead to higher generalizability to distant genotypes, which likely are more affected by higher-order epistasis (e.g. <xref rid="fig5" ref-type="fig">Figure 5</xref>).</p>
<p>Our results (<xref rid="fig6" ref-type="fig">Figure 6c</xref>) show that the prediction accuracy of both the additive model and the full epistatic model with 3 epistatic MHA layers decreases for test genotypes farther from the local fitness peaks. Interestingly, although the total variance due to epistasis <inline-formula><inline-graphic xlink:href="614318v2_inline16.gif" mimetype="image" mime-subtype="gif"/></inline-formula> remains relatively constant, as evidenced by the gap between the two <italic>R</italic><sup>2</sup> curves, contributions from higher-order interactions become increasingly important for genotypes more distant to their respective wild types, such that at Hamming distance of 7-9, roughly half of the epistatic variance is due to higher-order interactions.</p>
<p>This implies that higher-order interactions can be learned from assaying only the single and double mutants surrounding different fitness peaks, potentially by modeling how the these local effects vary across divergent genetic backgrounds, and that this can in turn help generalizing models trained on these highly localized samples to distant, higher-order mutants.</p>
<p>For the second task, we fit models with increasing complexity to random training genotypes for each GFP and examine how models fit to individual local peaks can extrapolate to other orthologous GFP landscapes. Our hypothesis is that the higher-order epistasis learned by our model when fit to one local fitness dataset consisting of single, double, and higher-order mutants will help predict how locally observed mutational effects and pairwise epistasis generalize to highly divergent, novel backgrounds, thus enabling better prediction. Specifically, for each GFP, we fit the models using 50% of randomly chosen genotypes and validation data consisting of the most distant 30% of the remaining genotypes. We then assess model generalizability by predicting fitness for random test genotypes for each of the three GFP orthologs. We first note that the the higher-order models explain low to moderate proportions of variance on test genotypes within the same local peak (<xref rid="fig6" ref-type="fig">Figure 6d</xref>, diagonal panels). In contrast, when focusing on predicting genotypes from orthologous fitness peaks (<xref rid="fig6" ref-type="fig">Figure 6d</xref>, off-diagonal panels), we notice that higher-order interactions frequently show more substantial contributions to total epistatic variance, sometimes on par with or exceeding the proportion of variance explained by the pairwise model (e.g. model trained on ppluGFP, evaluated on avGFP and cgreGFP). This result shows that higher-order epsitasis is important in determining the structure of this multipeak landscape. Furthermore, by learning higher-order epistasis from data sampled from locally sampled data, our epistatic transformer model is able to achieve better generalization to distant fitness peaks.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this paper, we developed a novel neural network-based method capable of detecting higher-order epistasis in full-length protein sequence-function relationships. The study of epistasis in high-throughput protein datasets has been historically hindered by the combinatorial complexity of fitting higher-order interaction terms, which makes existing methods applicable to protein sequence-function datasets with at most 3-4 positions mutagenized to every other amino acid in a combinatorially complete manner. By contrast, our epistatic transformer model fits epistatic interactions implicitly through a neural network, and thus can be easily applied to study epistasis among hundreds of amino acid positions.</p>
<p>We applied our method to 10 protein sequence-function datasets. Our results showed that higher-order epistasis can have subtle to substantial effects in determining the overall structure of protein-sequence-function relationships. The strongest effect of higher-order epistasis was observed in the abundance data for the SH3 domain of the protein GRB2, where higher-order epistasis explains 15% of the total variance in test genotypes, representing nearly two thirds of the total variance due to epistasis. This finding contrasts with a previous study [<xref ref-type="bibr" rid="c31">31</xref>] where Park et al. discovered no significant contribution by higher-order epistasis in 20 protein sequence-function relationship datasets. This discrepancy can be explained by the different scales of the study datasets. While Park et al. exclusively focused on combinatorially complete sequence-function data for small protein sequence space, we have predominantly used locally sampled sequence-function data embedded in much larger genotypic spaces, where higher-order epistasis likely plays more important roles due to combinatorial and biophysical reasons [<xref ref-type="bibr" rid="c51">51</xref>]. Our finding suggests that while some protein sequence-function relationship may be simple and explainable by additive effects, pairwise epistasis, and non-specific epistasis, higher-order specific epistasis can play a critical role in other cases and thus must be properly considered to fully understand the structure of protein sequence-function relationships.</p>
<p>It is also noteworthy that, in most of the other datasets we analyzed, the contribution of higher-order interactions is relatively modest when measured in terms of the total variance they explained. This observation aligns with the conclusions of Park et al. [<xref ref-type="bibr" rid="c31">31</xref>], who found that most protein sequence–function relationships can be well modeled using nonspecific and pairwise specific epistasis. One possible explanation for this limited apparent contribution of higher-order interactions is that these protein-sequence function relationships are truly devoid of higher-order interactions. Alternatively, higher-order epistasis may be present but obscured by experimental limitations, potentially due to the choice of mutagenized sites that do not interact strongly, or the limited scope of local sampling which does not ensure adequate statistical power to detect high-order interactions.</p>
<p>Importantly, even when datasets show little evidence of higher-order epistasis in summary statistics like <italic>R</italic><sup>2</sup>, such interactions can still play a crucial role in determining the phenotypes of a minority of genotypes with idiosyncratic behavior [<xref ref-type="bibr" rid="c52">52</xref>]. We demonstrated this by evaluating the importance of higher-order epistasis for making predictions for out-of-distribution genotypes. In particular, we showed that higher-order effects can be essential for predicting phenotypes in sparsely sampled regions of the sequence space, and for generalizing from data centered on one local GFP fitness peak to other, more distant orthologs. These findings suggest that coarse-grained metrics such as <italic>R</italic><sup>2</sup> may indeed fail to fully capture the functional significance of higher-order epistasis. Relying on such summary statistics to guide protein engineering or interpret evolutionary patterns can therefore be misleading, particularly when extrapolating beyond the range of observed data into unexplored regions of the sequence space.</p>
<p>Our results may present a challenge to researchers in protein engineering and evolution as they suggest that higher-order epistasis cannot be altogether ignored when studying protein-sequence function relationships. We want to point out that the extent to which researchers should ‘worry’ [<xref ref-type="bibr" rid="c28">28</xref>] about higher-order epistasis depends on the particular application. For example, based on the evidence of the Park et al. study, higher-order epistasis is likely to be less prevalent when only a small number of sites are mutagenized. Similarly, higher-order epistasis may also play a less important role in determining the structure of local sequence-function relationships. However, if the researcher is interested in how large numbers of mutations across the whole protein combine to determine function, then higher-order epistasis should be properly incorporated to enable an faithful construction of the sequence-function relationship. Reassuringly, we found that higher-order epistasis can be effectively modeled based on moderate size data to allow high overall prediction accuracy.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Epistatic transformer architecture</title>
<p>In this section, we provide a detailed description of the architecture of the epistatic transformer model in <xref rid="fig1" ref-type="fig">Figure 1</xref>. Our input <italic>x</italic> consists of tokenized protein sequences, i.e. <italic>x</italic> = <italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, <italic>· · ·, x</italic><sub><italic>L</italic></sub>, where <italic>x</italic><sub><italic>l</italic></sub> is an integer between 0 and <italic>a</italic>−1 (<italic>a</italic> is the alphabet size, which depends on the study dataset). We used a special positional encoding to transform the raw input sequence such that for the new sequence <inline-formula><inline-graphic xlink:href="614318v2_inline17.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. That is, we essentially treat amino acids from different positions completely differently. This is different from typical protein language modeling where a continuous positional encoding is added to the amino acid embeddings, such that the identities of the amino acids are still largely preserved to allow the model to learn general rules of interaction among residues while informed by residue positions when trained on large number of natural protein sequences. In contrast, here we aim to have the maximum flexibility to allow the same amino acid combinations to have different interactions at different positions, in order to learn a model that well approximates the parameterization in <xref ref-type="disp-formula" rid="eqn2">Eq. 2</xref>.</p>
<p>The new tokenized sequence <italic>x</italic><sup><italic>′</italic></sup> is passed through an embedding layer to generate position-wise continuous representations <inline-formula><inline-graphic xlink:href="614318v2_inline18.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where each position consists of <italic>d</italic> hidden dimensions. The embeddings are then passed through <italic>M</italic> layers of modified multi-head attention (MHA) to model epistatic interactions among sites. Our epistatic MHA layer is built on the standard MHA layer, but with some key differences. Specifically, the input <italic>Z</italic><sub><italic>m</italic></sub> ∈ ℝ<sup><italic>d×L</italic></sup> from MHA layer <italic>m</italic> is first used to generate the query and key tensors for <italic>H</italic> attention heads in layer <italic>m</italic> + 1, such that for head <italic>h</italic>, the query and key tensors are
<disp-formula id="eqn3">
<graphic xlink:href="614318v2_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn4">
<graphic xlink:href="614318v2_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Importantly, the value tensor <italic>V</italic> is directly generated from the raw embedding <italic>Z</italic><sub>0</sub>, instead of <italic>Z</italic><sub><italic>m</italic></sub>
<disp-formula id="eqn5">
<graphic xlink:href="614318v2_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In the Supplement, we show how this modification is essential to ensure that <italic>M</italic> layers of MHA leads to a models that strictly contain specific epistasis of order up to 2<sup><italic>M</italic></sup>. Next, for each head, we perform a modified scaled dot attention (<xref rid="fig1" ref-type="fig">Figure 1</xref>)
<disp-formula id="eqn6">
<graphic xlink:href="614318v2_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The key difference between this equation and the standard scaled dot attention is the lack of a softmax normalization, which is used to convert the raw attention scores obtained from the dot product of the query and key vectors into a probability distribution over the input positions. Since in the standard scaled dot attention the softmax operation would be applied across positions for each embedding dimension, it acts similarly to a global epistasis nonlinearity. This would cause the hidden nodes in the model to contain interactions of all orders, which is unsuitable for our purpose of only fitting specific epistasis up to a particular order. Using our modified scaled dot attention, the output from each head is then concatenated and passed through a linear layer to generate the output <italic>Z</italic><sub><italic>m</italic>+1</sub>
<disp-formula id="eqn7">
<graphic xlink:href="614318v2_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The output of the final <italic>M</italic> th MHA layer is then flattened and converted to a scalar <italic>ϕ</italic>
<disp-formula id="eqn8">
<graphic xlink:href="614318v2_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The scalar <italic>ϕ</italic> can be interpreted as a hidden phenotype that only contains additive effects and effects due to specific interactions. Note that our construction of the model architecture makes sure that <italic>ϕ</italic> only contains specific interactions of order up to 2<sup><italic>M</italic></sup>. The scalar <italic>ϕ</italic> is then passed through a sigmoid function mapping the hidden phenotype to the measurement scale to account for any non-specific epistasis
<disp-formula id="eqn9">
<graphic xlink:href="614318v2_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>a, μ, s</italic>, and <italic>b</italic> are trainable model parameters, which allow us to shift and scale the standard sigmoid function. We also experimented with more complex nonlinear activation functions parameterized by a sum of independent sigmoid functions. We did not observe improvement with this method over <xref ref-type="disp-formula" rid="eqn9">Eq. 9</xref>. Thus we trained all models using <xref ref-type="disp-formula" rid="eqn9">Eq. 9</xref> as the global epistasis function. Another key difference between our model and standard transformer models is the lack of LayerNorm operation which is typically used in transformer models to normalize the activations of neurons within a layer. Similar to the softmax function, LayerNorm is a nonlinear function over all positions, which will introduce undesirable global epistasis effect in the hidden layers.</p>
</sec>
<sec id="s4b">
<title>Model training</title>
<p>For all datasets studied in the main text, we train the epistatic transformer model with 1, 2, and 3 layers of attention, corresponding to models with specific epistasis among up to 2, 4 and 8 sites. In addition to the epistatic models, we also trained an additive model to benchmark the importance of specific epistasis. The additive model uses the same sigmoid activation function in <xref ref-type="disp-formula" rid="eqn9">Eq. 9</xref> to model nonspecific epistasis. However, its hidden phenotype <italic>ϕ</italic> was calculated as a simple weighted sum of the flattened one-hot embeddings of the input sequence, which was used to model the sum of additive effects of all mutations.</p>
<p>As mentioned in the main text, we use Optuna [<xref ref-type="bibr" rid="c48">48</xref>] to optimize hyperparameters including hidden dimension (<italic>d</italic>), number of train epochs, batch size, and dropout rate. Optuna uses a Bayesian optimization algorithm called Tree-structured Parzen Estimator to model the history record of trials to determine which hyperparameter values to use for the next iteration. We use the <italic>R</italic><sup>2</sup> value on a validation dataset to guide 200 Optuna optimization steps for all models. In our preliminary study, we found that learning rate and the number of attention heads have little effect on the final model performance, we therefore used 8 attention heads and the Adam optimizer with fixed learning rate (0.001) to train all models. All code was written in PyTorch v2.2.0. Individual models were trained on one NVIDIA Ampere A100 GPU with 80GB memory.</p>
</sec>
<sec id="s4c">
<title>Selection of experimental datasets</title>
<p>We used the following criteria for selecting datasets for detecting higher-order epistasis by fitting our epistatic transformer model. First, the dataset must contain high-throughput measurements for proteins, and ideally embedded in a large sequence space to allow potentially non-significant contribution by higher-order epistasis. Second, the dataset must contain substantial numbers of higher-order mutants, i.e. genotypes with at least three mutations relative to the reference (WT) sequence, to allow higher-order epistasis to play a role. We found that combinatorial mutagenesis datasets satisfying these criteria are rare compared with the large body of studies that measure single mutant and/or double mutant effects. Our research result in 7 publications. However, initial fitting of our models to datasets from two publications [<xref ref-type="bibr" rid="c53">53</xref>, <xref ref-type="bibr" rid="c54">54</xref>] resulted in very low model performance (<italic>R</italic><sup>2</sup> <italic>&lt;</italic> .1 for all models), which were subsequently discarded. The final datasets are listed in <xref rid="tbl1" ref-type="table">Table 1</xref>, containing data from five publications.</p>
<p>Three publications contain measurements for multiple proteins or functions. We used all three datasets from the ‘GRB2-SH3 domain’ study by Faure et al. [<xref ref-type="bibr" rid="c23">23</xref>]. The His3 publication [<xref ref-type="bibr" rid="c22">22</xref>] contains fitness measurements for combinatorial variants in 12 non-overlapping segments tiling the His3 protein. We chose segment 2, 5, and 12 for our study as these three segments showed the lowest <italic>R</italic><sup>2</sup> when we fit an additive + sigmoid model, thus allowing for more room of improvement by the epistatic models. The GFP publication [<xref ref-type="bibr" rid="c20">20</xref>] contains fluorescence data for three GFPs (amacGFP, ppluGFP, and cgreGFP), complemented by a fourth GFP (avGFP) [<xref ref-type="bibr" rid="c43">43</xref>] from an earlier study. For examining our model performance on random training data, we only used the cgreGFP and the ppluGFP datasets. The aligned dataset containing all four GFPs were used to detect the importance of higher-order epistasis in the multi-peak sequence-function relationship formed by these orthologs.</p>
</sec>
</sec>
</body>
<back>
<sec id="s5" sec-type="data-availability">
<title>Data Availability</title>
<p>Computer code to replicate all analyses can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/juannanzhou/EpistaticTransformer">https://github.com/juannanzhou/EpistaticTransformer</ext-link>.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank David McCandlish for thoughtful comments and suggestions. Research reported in this publication was supported by the National Institute of General Medical Sciences of the National Institutes of Health under award number R35GM154908, and University of Florida College of Liberal Arts &amp; Sciences.</p>
</ack>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
<sec id="s6">
<title>Author contributions</title>
<p>J.Z planned research; P.S and J.Z performed research; and J.Z wrote the paper.</p>
</sec>
</sec>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Supplement</label>
<media xlink:href="supplements/614318_file03.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Philip A</given-names> <surname>Romero</surname></string-name> and <string-name><given-names>Frances H</given-names> <surname>Arnold</surname></string-name></person-group>. “<article-title>Exploring protein fitness landscapes by directed evolution</article-title>”. In: <source>Nature reviews Molecular cell biology</source> <volume>10</volume>.<issue>12</issue> (<year>2009</year>), pp. <fpage>866</fpage>–<lpage>876</lpage>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Charlotte M</given-names> <surname>Miton</surname></string-name> and <string-name><given-names>Nobuhiko</given-names> <surname>Tokuriki</surname></string-name></person-group>. “<article-title>How mutational epistasis impairs predictability in protein evolution and design</article-title>”. In: <source>Protein Science</source> <volume>25</volume>.<issue>7</issue> (<year>2016</year>), pp. <fpage>1260</fpage>–<lpage>1272</lpage>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Tyler N</given-names> <surname>Starr</surname></string-name> and <string-name><given-names>Joseph W</given-names> <surname>Thornton</surname></string-name></person-group>. “<article-title>Epistasis in protein evolution</article-title>”. In: <source>Protein science</source> <volume>25</volume>.<issue>7</issue> (<year>2016</year>), pp. <fpage>1204</fpage>–<lpage>1218</lpage>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Rosalie</given-names> <surname>Lipsh-Sokolik</surname></string-name> and <string-name><given-names>Sarel J</given-names> <surname>Fleishman</surname></string-name></person-group>. “<article-title>Addressing epistasis in the design of protein function</article-title>”. In: <source>Proceedings of the National Academy of Sciences</source> <volume>121</volume>.<issue>34</issue> (<year>2024</year>), <fpage>e2314999121</fpage>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Douglas M</given-names> <surname>Fowler</surname></string-name> <etal>et al.</etal></person-group> “<article-title>High-resolution mapping of protein sequence-function relationships</article-title>”. In: <source>Nat. Methods</source> <volume>7</volume>.<issue>9</issue> (<year>2010</year>), pp. <fpage>741</fpage>–<lpage>746</lpage>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Lea M</given-names> <surname>Starita</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Activity-enhancing mutations in an E3 ubiquitin ligase identified by high-throughput mutagenesis</article-title>”. In: <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>110</volume>.<issue>14</issue> (<year>2013</year>), <fpage>E1263</fpage>–<lpage>E1272</lpage>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Daniel</given-names> <surname>Melamed</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Deep mutational scanning of an RRM domain of the Saccharomyces cerevisiae poly (A)-binding protein</article-title>”. In: <source>RNA</source> <volume>19</volume>.<issue>11</issue> (<year>2013</year>), pp. <fpage>1537</fpage>–<lpage>1551</lpage>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C</given-names> <surname>Anders Olson</surname></string-name>, <string-name><given-names>Nicholas C</given-names> <surname>Wu</surname></string-name>, and <string-name><given-names>Ren</given-names> <surname>Sun</surname></string-name></person-group>. “<article-title>A comprehensive biophysical description of pairwise epistasis throughout an entire protein domain</article-title>”. In: <source>Curr. Biol</source>. <volume>24</volume>.<issue>22</issue> (<year>2014</year>), pp. <fpage>2643</fpage>–<lpage>2651</lpage>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Michael B</given-names> <surname>Doud</surname></string-name>, <string-name><given-names>Orr</given-names> <surname>Ashenberg</surname></string-name>, and <string-name><given-names>Jesse D</given-names> <surname>Bloom</surname></string-name></person-group>. “<article-title>Site-specific amino acid preferences are mostly conserved in two closely related protein homologs</article-title>”. In: <source>Mol. Biol. Evol</source>. <volume>32</volume>.<issue>11</issue> (<year>2015</year>), pp. <fpage>2944</fpage>–<lpage>2960</lpage>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Anna I</given-names> <surname>Podgornaia</surname></string-name> and <string-name><given-names>Michael T</given-names> <surname>Laub</surname></string-name></person-group>. “<article-title>Pervasive degeneracy and epistasis in a protein-protein interface</article-title>”. In: <source>Science</source> <volume>347</volume>.<issue>6222</issue> (<year>2015</year>), pp. <fpage>673</fpage>–<lpage>677</lpage>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Karen S</given-names> <surname>Sarkisyan</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Local fitness landscape of the green fluorescent protein</article-title>”. In: <source>Nature</source> <volume>533</volume>.<issue>7603</issue> (<year>2016</year>), p. <fpage>397</fpage>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Barrett</given-names> <surname>Steinberg</surname></string-name> and <string-name><given-names>Marc</given-names> <surname>Ostermeier</surname></string-name></person-group>. “<article-title>Shifting fitness and epistatic landscapes reflect trade-offs along an evolutionary pathway</article-title>”. In: <source>J. Mol. Biol</source>. <volume>428</volume>.<issue>13</issue> (<year>2016</year>), pp. <fpage>2730</fpage>–<lpage>2743</lpage>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Claudia</given-names> <surname>Bank</surname></string-name> <etal>et al.</etal></person-group> “<article-title>On the (un)predictability of a large intragenic fitness landscape</article-title>”. In: <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>113</volume>.<issue>49</issue> (<year>2016</year>), pp. <fpage>14085</fpage>–<lpage>14090</lpage>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Tyler N</given-names> <surname>Starr</surname></string-name>, <string-name><given-names>Lora K</given-names> <surname>Picton</surname></string-name>, and <string-name><given-names>Joseph W</given-names> <surname>Thornton</surname></string-name></person-group>. “<article-title>Alternative evolutionary histories in the sequence space of an ancient protein</article-title>.” In: <source>Nature</source> <volume>549</volume>.<issue>7672</issue> (<month>Sept</month>. <year>2017</year>), pp. <fpage>409</fpage>–<lpage>413</lpage>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Victoria O</given-names> <surname>Pokusaeva</surname></string-name> <etal>et al.</etal></person-group> “<article-title>An experimental assay of the interactions of amino acids from orthologous sequences shaping a complex fitness landscape</article-title>”. In: <source>PLos Genet</source>. <volume>15</volume>.<issue>4</issue> (<year>2019</year>), <fpage>e1008079</fpage>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Calin</given-names> <surname>Plesa</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Multiplexed gene synthesis in emulsions for exploring protein functional landscapes</article-title>”. In: <source>Science</source> <volume>359</volume>.<issue>6373</issue> (<year>2018</year>), pp. <fpage>343</fpage>–<lpage>347</lpage>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Drew S</given-names> <surname>Tack</surname></string-name> <etal>et al.</etal></person-group> “<article-title>The genotype-phenotype landscape of an allosteric protein</article-title>”. In: <source>Molecular systems biology</source> <volume>17</volume>.<issue>3</issue> (<year>2021</year>), <fpage>e10179</fpage>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Tyler N</given-names> <surname>Starr</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Deep mutational scanning of SARS-CoV-2 receptor binding domain reveals constraints on folding and ACE2 binding</article-title>”. In: <source>Cell</source> <volume>182</volume>.<issue>5</issue> (<year>2020</year>), pp. <fpage>1295</fpage>–<lpage>1310</lpage>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Louisa Gonzalez</given-names> <surname>Somermeyer</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Heterogeneity of the GFP fitness landscape and data-driven protein design</article-title>”. In: <source>eLife</source> <volume>11</volume> (<month>May</month> <year>2022</year>). <fpage>e75842</fpage>. ISSN: <issn>2050-084X</issn>. doi: <pub-id pub-id-type="doi">10.7554/eLife.75842</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Louisa Gonzalez</given-names> <surname>Somermeyer</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Heterogeneity of the GFP fitness landscape and data-driven protein design</article-title>”. In: <source>Elife</source> <volume>11</volume> (<year>2022</year>), <fpage>e75842</fpage>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Drew H</given-names> <surname>Bryant</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Deep diversification of an AAV capsid protein by machine learning</article-title>”. In: <source>Nature Biotechnology</source> <volume>39</volume>.<issue>6</issue> (<year>2021</year>), pp. <fpage>691</fpage>–<lpage>696</lpage>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Victoria O</given-names> <surname>Pokusaeva</surname></string-name> <etal>et al.</etal></person-group> “<article-title>An experimental assay of the interactions of amino acids from orthologous sequences shaping a complex fitness landscape</article-title>”. In: <source>PLoS genetics</source> <volume>15</volume>.<issue>4</issue> (<year>2019</year>), <fpage>e1008079</fpage>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Andre J</given-names> <surname>Faure</surname></string-name> <etal>et al.</etal></person-group> “<article-title>The genetic architecture of protein stability</article-title>”. <source>bioRxiv</source> (<year>2023</year>).</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Justin B</given-names> <surname>Kinney</surname></string-name> and <string-name><given-names>David M</given-names> <surname>McCandlish</surname></string-name></person-group>. “<article-title>Massively parallel assays and quantitative sequence–function relationships</article-title>”. In: <source>Annual review of genomics and human genetics</source> <volume>20</volume> (<year>2019</year>), pp. <fpage>99</fpage>–<lpage>127</lpage>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Patrick C</given-names> <surname>Phillips</surname></string-name></person-group>. “<article-title>Epistasis—the essential role of gene interactions in the structure and evolution of genetic systems</article-title>”. In: <source>Nat. Rev. Genet</source>. <volume>9</volume>.<issue>11</issue> (<year>2008</year>), pp. <fpage>855</fpage>–<lpage>867</lpage>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Dmitry A</given-names> <surname>Kondrashov</surname></string-name> and <string-name><given-names>Fyodor A</given-names> <surname>Kondrashov</surname></string-name></person-group>. “<article-title>Topological features of rugged fitness landscapes in sequence space</article-title>”. In: <source>Trends Genet</source>. <volume>31</volume>.<issue>1</issue> (<year>2015</year>), pp. <fpage>24</fpage>–<lpage>33</lpage>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Júlia</given-names> <surname>Domingo</surname></string-name>, <string-name><given-names>Pablo</given-names> <surname>Baeza-Centurion</surname></string-name>, and <string-name><given-names>Ben</given-names> <surname>Lehner</surname></string-name></person-group>. “<article-title>The Causes and Consequences of Genetic Interactions (Epistasis)</article-title>”. In: <source>Annu. Rev. Genomics Hum. Genet</source>. <volume>20</volume> (<year>2019</year>).</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Daniel M</given-names> <surname>Weinreich</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Should evolutionary geneticists worry about higher-order epistasis?</article-title>” In: <source>Curr. Opin. Genet. Dev</source>. <volume>23</volume>.<issue>6</issue> (<year>2013</year>), pp. <fpage>700</fpage>–<lpage>707</lpage>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Juannan</given-names> <surname>Zhou</surname></string-name> and <string-name><given-names>David M</given-names> <surname>McCandlish</surname></string-name></person-group>. “<article-title>Minimum epistasis interpolation for sequence-function relationships</article-title>”. In: <source>Nature Communications</source> <volume>11</volume>.<issue>1</issue> (<year>2020</year>), pp. <fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Frank J</given-names> <surname>Poelwijk</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Socolich</surname></string-name>, and <string-name><given-names>Rama</given-names> <surname>Ranganathan</surname></string-name></person-group>. “<article-title>Learning the pattern of epistasis linking genotype and phenotype in a protein</article-title>”. In: <source>Nat. Commun</source>. <volume>10</volume>.<issue>1</issue> (<year>2019</year>), pp. <fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Yeonwoo</given-names> <surname>Park</surname></string-name>, <string-name><given-names>Brian PH</given-names> <surname>Metzger</surname></string-name>, and <string-name><given-names>Joseph W</given-names> <surname>Thornton</surname></string-name></person-group>. “<article-title>The simplicity of protein sequence-function relationships</article-title>”. <source>bioRxiv</source> (<year>2023</year>).</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C</given-names> <surname>Anders Olson</surname></string-name>, <string-name><given-names>Nicholas C</given-names> <surname>Wu</surname></string-name>, and <string-name><given-names>Ren</given-names> <surname>Sun</surname></string-name></person-group>. “<article-title>A comprehensive biophysical description of pairwise epistasis throughout an entire protein domain</article-title>”. In: <source>Current biology</source> <volume>24</volume>.<issue>22</issue> (<year>2014</year>), pp. <fpage>2643</fpage>–<lpage>2651</lpage>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Brian PH</given-names> <surname>Metzger</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Epistasis facilitates functional evolution in an ancient transcription factor</article-title>”. In: <source>Elife</source> <volume>12</volume> (<year>2024</year>), <fpage>RP88737</fpage>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Daniel M</given-names> <surname>Weinreich</surname></string-name> <etal>et al.</etal></person-group> “<article-title>The influence of higher-order epistasis on biological fitness landscape to-pography</article-title>”. In: <source>Journal of Statistical Physics</source> <volume>172</volume>.<issue>1</issue> (<year>2018</year>), pp. <fpage>208</fpage>–<lpage>225</lpage>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nicholas C.</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>C. Anders</given-names> <surname>Olson</surname></string-name>, and <string-name><given-names>Ren</given-names> <surname>Sun</surname></string-name></person-group>. “<article-title>High-throughput identification of protein mutant stability computed from a double mutant fitness landscape</article-title>”. In: <source>Protein Sci</source>. <volume>25</volume>.<issue>2</issue> (<year>2016</year>), pp. <fpage>530</fpage>–<lpage>539</lpage>. ISSN: <issn>1469-896X</issn>. doi: <pub-id pub-id-type="doi">10.1002/pro.2840</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Daniel M.</given-names> <surname>Weinreich</surname></string-name> <etal>et al.</etal></person-group> “<article-title>The Influence of Higher-Order Epistasis on Biological Fitness Landscape Topography</article-title>”. In: <source>J. Stat. Phys</source>. <volume>172</volume>.<issue>1</issue> (<year>2018</year>), pp. <fpage>208</fpage>–<lpage>225</lpage>. ISSN: <issn>0022-4715</issn>. doi: <pub-id pub-id-type="doi">10.1007/s10955-018-1975-3</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Gloria</given-names> <surname>Yang</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Higher-order epistasis shapes the fitness landscape of a xenobiotic-degrading en-zyme</article-title>”. In: <source>Nature Chemical Biology</source> <volume>15</volume>.<issue>11</issue> (<year>2019</year>), pp. <fpage>1120</fpage>–<lpage>1128</lpage>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Juannan</given-names> <surname>Zhou</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Higher-order epistasis and phenotypic prediction</article-title>”. In: <source>Proceedings of the National Academy of Sciences</source> <volume>119</volume>.<issue>39</issue> (<year>2022</year>), <fpage>e2204233119</fpage>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jakub</given-names> <surname>Otwinowski</surname></string-name>, <string-name><given-names>David Martin</given-names> <surname>McCandlish</surname></string-name>, and <string-name><given-names>Joshua B</given-names> <surname>Plotkin</surname></string-name></person-group>. “<article-title>Inferring the shape of global epistasis</article-title>”. In: <source>Proceedings of the National Academy of Sciences</source> <volume>115</volume>.<issue>32</issue> (<year>2018</year>), <fpage>E7550</fpage>–<lpage>E7558</lpage>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ammar</given-names> <surname>Tareen</surname></string-name> <etal>et al.</etal></person-group> “<article-title>MAVE-NN: learning genotype-phenotype maps from multiplex assays of variant effect</article-title>”. In: <source>Genome biology</source> <volume>23</volume>.<issue>1</issue> (<year>2022</year>), p. <fpage>98</fpage>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Carl Edward</given-names> <surname>Rasmussen</surname></string-name> and <string-name><given-names>Christopher K I</given-names> <surname>Williams</surname></string-name></person-group>. <source>Gaussian processes for machine learning</source>. <publisher-name>MIT Press</publisher-name>, <year>2006</year>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Amirali</given-names> <surname>Aghazadeh</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Epistatic Net allows the sparse spectral regularization of deep neural networks for inferring fitness functions</article-title>”. In: <source>Nature Communications</source> <volume>12</volume>.<issue>1</issue> (<year>2021</year>), pp. <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Karen S</given-names> <surname>Sarkisyan</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Local fitness landscape of the green fluorescent protein</article-title>”. In: <source>Nature</source> <volume>533</volume>.<issue>7603</issue> (<year>2016</year>), pp. <fpage>397</fpage>–<lpage>401</lpage>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Zachary R</given-names> <surname>Sailer</surname></string-name> and <string-name><given-names>Michael J</given-names> <surname>Harms</surname></string-name></person-group>. “<article-title>Detecting High-Order Epistasis in Nonlinear Genotype-Phenotype Maps</article-title>”. In: <source>Genetics</source> <volume>205</volume>.<issue>3</issue> (<year>2017</year>), pp. <fpage>1079</fpage>–<lpage>1088</lpage>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Peter D</given-names> <surname>Tonner</surname></string-name>, <string-name><given-names>Abe</given-names> <surname>Pressman</surname></string-name>, and <string-name><given-names>David</given-names> <surname>Ross</surname></string-name></person-group>. “<article-title>Interpretable modeling of genotype–phenotype landscapes with state-of-the-art predictive power</article-title>”. In: <source>Proceedings of the National Academy of Sciences</source> <volume>119</volume>.<issue>26</issue> (<year>2022</year>), <fpage>e2114021119</fpage>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ammar</given-names> <surname>Tareen</surname></string-name> <etal>et al.</etal></person-group> “<article-title>MAVE-NN: learning genotype-phenotype maps from multiplex assays of variant effect</article-title>”. In: <source>Genome Biology</source> <volume>23</volume>.<issue>1</issue> (<year>2022</year>), pp. <fpage>1</fpage>–<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ashish</given-names> <surname>Vaswani</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Attention is all you need</article-title>”. In: <source>Advances in neural information processing systems</source> <volume>30</volume> (<year>2017</year>).</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Takuya</given-names> <surname>Akiba</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Optuna: A next-generation hyperparameter optimization framework</article-title>”. In: <conf-name>Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</conf-name>. <year>2019</year>, pp. <fpage>2623</fpage>–<lpage>2631</lpage>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Jimmy Lei</given-names> <surname>Ba</surname></string-name>, <string-name><given-names>Jamie Ryan</given-names> <surname>Kiros</surname></string-name>, and <string-name><given-names>Geoffrey E</given-names> <surname>Hinton</surname></string-name></person-group>. “<article-title>Layer normalization</article-title>”. <source>arXiv</source> <pub-id pub-id-type="arxiv">1607.06450</pub-id> (<year>2016</year>).</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yongcan</given-names> <surname>Chen</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Deep Mutational Scanning of an Oxygen-Independent Fluorescent Protein CreiLOV for Comprehensive Profiling of Mutational and Epistatic Effects</article-title>”. In: <source>ACS Synthetic Biology</source> <volume>12</volume>.<issue>5</issue> (<year>2023</year>), pp. <fpage>1461</fpage>–<lpage>1473</lpage>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Eric A</given-names> <surname>Ortlund</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Crystal structure of an ancient protein: evolution by conformational epistasis</article-title>”. In: <source>Science</source> <volume>317</volume>.<issue>5844</issue> (<year>2007</year>), pp. <fpage>1544</fpage>–<lpage>1548</lpage>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Thomas</given-names> <surname>Dupic</surname></string-name>, <string-name><given-names>Angela M</given-names> <surname>Phillips</surname></string-name>, and <string-name><given-names>Michael M</given-names> <surname>Desai</surname></string-name></person-group>. “<article-title>Protein sequence landscapes are not so simple: on reference-free versus reference-based inference</article-title>”. <source>bioRxiv</source> (<year>2024</year>).</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Philipp</given-names> <surname>Koch</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Optimization of the antimicrobial peptide Bac7 by deep mutational scanning</article-title>”. In: <source>BMC biology</source> <volume>20</volume>.<issue>1</issue> (<year>2022</year>), p. <fpage>114</fpage>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Remkes A</given-names> <surname>Scheele</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Droplet-based screening of phosphate transfer catalysis reveals how epistasis shapes MAP kinase interactions with substrates</article-title>”. In: <source>Nature Communications</source> <volume>13</volume>.<issue>1</issue> (<year>2022</year>), p. <fpage>844</fpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108005.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bitbol</surname>
<given-names>Anne-Florence</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Ecole Polytechnique Federale de Lausanne (EPFL)</institution>
</institution-wrap>
<city>Lausanne</city>
<country>Switzerland</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>In this <bold>important</bold> work, the authors present a new transformer-based neural network designed to isolate and quantify higher-order epistasis in protein sequences. They provide <bold>solid</bold> evidence that higher-order epistasis can play key roles in protein function. This work will be of interest to the communities interested in modeling biological sequence data and understanding mutational effects.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108005.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors present an approach that uses the transformer architecture to model epistasis in deep mutational scanning datasets. This is an original and very interesting idea. Applying the approach to 10 datasets, they quantify the contribution of higher-order epistasis, showing that it varies quite extensively.</p>
<p>Suggestions:</p>
<p>(1) The approach taken is very interesting, but it is not particularly well placed in the context of recent related work. MAVE-NN, LANTERN, and MoCHI are all approaches that different labs have developed for inferring and fitting global epistasis functions to DMS datasets. MoCHI can also be used to infer multi-dimensional global epistasis (for example, folding and binding energies) and also pairwise (and higher order) specific interaction terms (see 10.1186/s13059-024-03444-y and 10.1371/journal.pcbi.1012132). It doesn't distract from the current work to better introduce these recent approaches in the introduction. A comparison of the different capabilities of the methods may also be helpful. It may also be interesting to compare the contributions to variance of 1st, 2nd, and higher-order interaction terms estimated by the Epistatic transformer and MoCHI.</p>
<p>(2) <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004771">https://doi.org/10.1371/journal.pcbi.1004771</ext-link> is another useful reference that relates different metrics of epistasis, including the useful distinction between biochemical/background-relative and background-averaged epistasis.</p>
<p>(3) Which higher-order interactions are more important? Are there any mechanistic/structural insights?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108005.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper presents a novel transformer-based neural network model, termed the epistatic transformer, designed to isolate and quantify higher-order epistasis in protein sequence-function relationships. By modifying the multi-head attention architecture, the authors claim they can precisely control the order of specific epistatic interactions captured by the model. The approach is applied to both simulated data and ten diverse experimental deep mutational scanning (DMS) datasets, including full-length proteins. The authors argue that higher-order epistasis, although often modest in global contribution, plays critical roles in extrapolation and capturing distant genotypic effects, especially in multi-peak fitness landscapes.</p>
<p>Strengths:</p>
<p>(1) The study tackles a long-standing question in molecular evolution and protein engineering: &quot;how significant are epistatic interactions beyond pairwise effects?&quot; The question is relevant given the growing availability of large-scale DMS datasets and increasing reliance on machine learning in protein design.</p>
<p>(2) The manuscript includes both simulation and real-data experiments, as well as extrapolation tasks (e.g., predicting distant genotypes, cross-ortholog transfer). These well-rounded evaluations demonstrate robustness and applicability.</p>
<p>(3) The code is made available for reproducibility.</p>
<p>Weaknesses:</p>
<p>(1) The paper mainly compares its transformer models to additive models and occasionally to linear pairwise interaction models. However, other strong baselines exist. For example, the authors should compare baseline methods such as &quot;DANGO: Predicting higher-order genetic interactions&quot;. There are many works related to pairwise interaction detection, such as: &quot;Detecting statistical interactions from neural network weights&quot;, &quot;shapiq: Shapley interactions for machine learning&quot;, and &quot;Error-controlled non-additive interaction discovery in machine learning models&quot;.</p>
<p>(2) While the transformer architecture is cleverly adapted, the claim that it allows for &quot;explicit control&quot; and &quot;interpretability&quot; over interaction order may be overstated. Although the 2^M scaling with MHA layers is shown empirically, the actual biological interactions captured by the attention mechanism remain opaque. A deeper analysis of learned attention maps or embedding similarities (e.g., visualizations, site-specific interaction clusters) could substantiate claims about interpretability.</p>
<p>(3) The distinction between nonspecific (global) and specific epistasis is central to the modeling framework, yet it remains conceptually underdeveloped. While a sigmoid function is used to model global effects, it's unclear to what extent this functional form suffices. The authors should justify this choice more rigorously or at least acknowledge its limitations and potential implications.</p>
<p>(4) The manuscript refers to &quot;pairwise&quot;, &quot;3-4-way&quot;, and &quot;&gt;4-way&quot; interactions without always clearly defining the boundaries of these groupings or how exactly the order is inferred from transformer layer depth. This can be confusing to readers unfamiliar with the architecture or with statistical definitions of interaction order. The authors should clarify terminology consistently. Including a visual mapping or table linking a number of layers to the maximum modeled interaction order could be helpful.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108005.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Sethi and Zou present a new neural network to study the importance of epistatic interactions in pairs and groups of amino acids to the function of proteins. Their new model is validated on a small simulated data set and then applied to 10 empirical data sets. Results show that epistatic interactions in groups of amino acids can be important to predict the function of a protein, especially for sequences that are not very similar to the training data.</p>
<p>Strengths:</p>
<p>The manuscript relies on a novel neural network architecture that makes it easy to study specifically the contribution of interactions between 2, 3, 4, or more amino acids. The study of 10 different protein families shows that there is variation among protein families.</p>
<p>Weaknesses:</p>
<p>The manuscript is good overall, but could have gone a bit deeper by comparing the new architecture to standard transformers, and by investigating whether differences between protein families explain some of the differences in the importance of interactions between amino acids. Finally, the GitHub repository needs some more information to be usable.</p>
</body>
</sub-article>
</article>