<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">106812</article-id>
<article-id pub-id-type="doi">10.7554/eLife.106812</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106812.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>When do measured representational distances reflect the neural representational geometry?</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Botero</surname>
<given-names>Veronica Bossio</given-names>
</name>
<xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7433-9005</contrib-id>
<name>
<surname>Kriegeskorte</surname>
<given-names>Nikolaus</given-names>
</name>
<xref ref-type="aff" rid="aff1"/><email>nk2765@columbia.edu</email>
</contrib>
<aff id="aff1"><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University</institution></institution-wrap>, <city>New York</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Kahnt</surname>
<given-names>Thorsten</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>National Institute on Drug Abuse Intramural Research Program</institution>
</institution-wrap>
<city>Baltimore</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Behrens</surname>
<given-names>Timothy E</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-06-27">
<day>27</day>
<month>06</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP106812</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-04-29">
<day>29</day>
<month>04</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-12-30">
<day>30</day>
<month>12</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.12.30.630743"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Botero &amp; Kriegeskorte</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Botero &amp; Kriegeskorte</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-106812-v1.pdf"/>
<abstract>
<title>Abstract</title><p>The representational geometry of a brain region can be characterized by the distances among neural activity patterns for a set of experimental conditions. Researchers routinely estimate representational distances from brain-activity measurements that either sparsely sample the underlying neural population (e.g. neural recordings) or pool across the activity of many neurons (e.g. fMRI voxels). Here we use theory and simulations to clarify under what circumstances representational distances estimated from brain-activity measurements reflect the representational geometry of the underlying neural population, and what distortions must be expected under other circumstances. We demonstrate that the estimated representational distances are undistorted if single neurons are sampled at random. For voxels that take non-negatively weighted linear combinations, the resulting geometry is linearly distorted, correctly reflecting the population-mean dimension, while downscaling all orthogonal dimensions, for which the averaging cancels a large portion of the signal. Surprisingly, removing the mean from voxel patterns recovers the underlying representational geometry exactly in expectation under idealized conditions. This explains why the correlation distance, the most popular measure of representational dissimilarity in neuroimaging studies, “works” so well, yielding geometries that can appear similar between fMRI and neural recordings. The Euclidean (or Mahalanobis) distance computed after removing the mean of each pattern (without normalizing its variance) is an attractive alternative to the correlation distance in that it corrects for the inflated relative contribution of the population-mean dimension, while avoiding the drawback of the correlation distance: it can be large for confusable low-norm patterns, failing to reflect decodability. Our results demonstrate that measured representational distances reflect the neural representational geometry when (1) single neurons are sampled at random or (2) the weights with which the measured responses sample the neurons are drawn i.i.d. and (2a) the weights are drawn from a zero-mean distribution or (2b) the population mean is the same for all conditions or (2c) the mean is removed from each estimated pattern. We discuss practical implications for analyses of neural representational geometries.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<label>1</label><title>Introduction</title>
<p>The representational geometry of a neural population code [<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c32">32</xref>, <xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c44">44</xref>] is characterized by the distances among the patterns of neural responses to a set of experimental conditions. Studying representational geometries enables researchers to abstract from the tuning of individual neurons and focus on which representational distinctions are emphasized and which are deemphasized, and to what extent, by the neural population as a whole. The representational distance matrix as a summary statistic enables direct comparisons between representations in brains and models, without requiring a correspondency mapping between the two representations [[<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c43">43</xref>]; for related approaches, see also [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c12">12</xref>]].</p>
<p>In practice, researchers estimate representational geometries based on experimental measurements of brain activity. However, the activity measurements are often severely limited samples of the underlying neural population activity in the brain region of interest. For example, single-cell recordings typically sample a small fraction of the neurons in the population. A study might sample 100 out of many millions of neurons in the cortical area studied. Functional magnetic resonance imaging (fMRI) achieves continuous coverage of the brain, but fMRI voxels (ranging from 4 mm in width to below 1 mm) reflect the average activity of tens or hundreds of thousands of neurons over several seconds[<xref ref-type="bibr" rid="c34">34</xref>].</p>
<p>Is it even possible to estimate the neural representational geometry from neural population activity measurements that subsample a tiny fraction of the dimensions? And if so, under what conditions will the representational distances we estimate from our data be good estimates of the true representational distances we would have estimated if we had recordings for the entire neural population?</p>
<sec id="s1a">
<title>Surprising sensitivity</title>
<p>Representational geometries estimated from single-cell recordings and fMRI can appear strikingly similar. For example, Kriegeskorte et al. [<xref ref-type="bibr" rid="c31">31</xref>] compared inferior temporal representations of object images between macaques measured with sequential electrode recordings [674 cells, [<xref ref-type="bibr" rid="c23">23</xref>]] and humans measured with blood-oxygen-level-dependent (BOLD) fMRI using 2-mm isotropic voxels. The representational dissimilarity matrices for 92 object images were computed using the correlation distance (1 <italic>— r</italic>, where <italic>r</italic> is the Pearson correlation coefficient) and showed the same prominent categorical clusters and also significant within-category correlations of the representational dissimilarities between the macaque recordings and the human fMRI data. Other studies have found surprising sensitivity of BOLD fMRI to information thought to be inaccessible with voxels averaging across tens or hundreds of thousands of neurons. For example, the orientation of a visual grating can be decoded from the fMRI pattern it elicits in human primary visual cortex using 3-mm isotropic voxels [<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c17">17</xref>]. There has been some controversy as to whether (a) the 3-mm isotropic fMRI voxel patterns reflect subvoxel-scale neural pattern information through biased sampling or aliasing [<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c41">41</xref>, <xref ref-type="bibr" rid="c2">2</xref>] or (b) the decodable information actually resides lower spatial-frequency bands of the neural activity patterns [<xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c40">40</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>]. This controversy about the sensititivity of BOLD fMRI with 3-mm voxels notwithstanding, there is evidence that fMRI using smaller voxels down to the submillimeter scale and ultra-high fields [<xref ref-type="bibr" rid="c8">8</xref>] can reflect columnar [<xref ref-type="bibr" rid="c47">47</xref>, <xref ref-type="bibr" rid="c48">48</xref>] and even laminar [<xref ref-type="bibr" rid="c19">19</xref>] patterns of cortical activity.</p>
</sec>
<sec id="s1b">
<title>Voxels average locally, canceling contrast signals</title>
<p>Even if fMRI is surprisingly sensitive to fine-grained neural activity patterns, its sensitivity certainly falls off at the submillimeter scale. An undistorted reflection of the neural representational geometry would require that it is <italic>equally</italic> sensitive to all dimensions of the multivariate neural response space. Simulations show that the local averaging of activity in fMRI voxels can strongly affect the apparent representational geometry when the code is spatially structured at multiple spatial scales [<xref ref-type="bibr" rid="c26">26</xref>]. For a V1-like representation, for example, with local orientation columns and global retinotopy, orientation signals (which are carried by fine-grained patterns) will cancel more than location signals (which are carried by coarse-scale patterns). The more severe canceling of orientation than of location contrast signals distorts the apparent representational geometry. Kriegeskorte and Diedrichsen [<xref ref-type="bibr" rid="c26">26</xref>] concluded that accounting for local averaging in voxels is important to ensure correct inference of the datagenerating brain-computational model. Empirically, it has been shown that face-identity information in the macaque face patch AL is prominently reflected in neural recordings but not fMRI data [<xref ref-type="bibr" rid="c13">13</xref>]. Some empirical fMRI studies have explicitly considered and modeled the effects of local averaging in voxels [<xref ref-type="bibr" rid="c42">42</xref>, <xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c43">43</xref>].</p>
<p>The aim of this paper is to clarify under what conditions, if any, the neural representational geometry is correctly reflected in representational distance estimates that are based on measurements, as provided by cell recordings and fMRI, of only a small sample of the dimensions of neural population activity. By “correctly reflected”, we mean that our distance estimates reflect all aspects of neuronal tuning (e.g. orientation and position information for V1) equally, such that, if we had a sufficiently large number of measurement channels (e.g. recorded neurons or fMRI voxels), our distance estimates would be proportional (except for the effects of measurement noise) to the neural representational distances we would have obtained from recordings of the entire neural population.</p>
<p>We will describe the ideal conditions under which the estimated distances are undistorted in expectation. Note that under such ideal conditions, the following two challenges for analysis remain: (1) There would still be random variation of the estimates arising from noise in the measurements and from the random sampling of a limited number of measurement channels. (2) Naive estimation of the representational distances with non-negative estimators would still suffer from positive estimation bias. These two issues have been addressed in previous work on unbiased distance estimation [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c45">45</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c27">27</xref>] and statistical inference on representational geometries [<xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c43">43</xref>] and are orthogonal to the question we are concerned with here.</p>
</sec>
<sec id="s1c">
<title>Cell recordings and fMRI sample projections of the geometry</title>
<p>We can think of both electrodes and voxels as sampling <italic>projections</italic> of the neural representational geometry. Imagine measuring the activity of a population of <italic>N</italic> neurons as they respond to a set of stimuli. Each response pattern of the population can be viewed as a point in and <italic>N</italic> -dimensional space. We can think of each neuron’s activity as encoding the coordinate along a particular axis of this space (where the axes are by definition orthogonal). The set of unique response patterns to <italic>P</italic> stimuli constitutes a point cloud in this <italic>N</italic> -dimensional space, whose pairwise distances define the neural representational geometry (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). Taking measurements of these neurons can be viewed as projecting the point cloud onto a new set of axes determined by the measurement channels (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). For example, an electrophysiological recording of individual neurons with an electrode array can be viewed as a projection of the point cloud onto the subspace spanned by the axes corresponding to the neurons captured by the array. In the example shown in <xref rid="fig1" ref-type="fig">Fig. 1a</xref>, the entire population consists of three neurons and the measurement of the first two neurons would result in an orthogonal projection of the activity patterns onto two axes defined by the measurement channels (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Brain activity measurements as sampling projections of the neural response patterns.</title>
    <p><bold>a.</bold> Neural activity response patterns can be viewed as points in a multidimensional metric space in which the dimensions correspond to the activity of single neurons. <bold>a.</bold> Example of a population of three neurons, each gray point represents the population response to one experimental condition. <bold>b.</bold> Projection of the neural response patterns onto the measured response space. Each point is recoded along a new set of axes given by the measurement channels. <bold>c.</bold> Measurement channels that reflect the activity of the neural population as weighted averages with only non-negative weights can be viewed as sampling axes that lie on the all-positive orthant. Panel <bold>c</bold> shows a set of response patterns with a spherical geometry (gray) and two example all-positive sampling axes (red). <bold>d.</bold> Recoding the points on the sphere along the sampling axes results in a geometry that is linearly stretched along the dimension of the neural population average. <bold>e.</bold> A set of non-negative sampling axes. Each red vector represents a measurement channel whose entries correspond to the neural sampling weights. These vectors can also be seen as points in the neural response space, represented by the red tips. The fact that all the weights are non-negative makes the sampling axes closely aligned with the all-1 vector.. <bold>f.</bold> Removing the mean activation from each measured pattern is equivalent to rigidly centering the tips of the original sampling weight vectors on the origin of the neural response space. This corrects the oversampling of the population-mean (all-1) axis, dispersing the effective axes along which the neural patterns are sampled, such that all directions are equally represented in expectation and the neural representational geometry is thus undistorted.</p></caption>
<graphic xlink:href="630743v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>A representational distance is a property of the neural population. Estimating it from a sample of neurons is analogous to estimating other population parameters from samples. Consider estimating the height of the Chilean population by measuring the height of a random sample of 500 Chilean individuals. We would expect the average height of the random sample to be a good estimate of the average height of the Chilean population. In estimating the representational distance, we would expect the average squared difference across a sufficiently large random sample of particular neurons to be a good estimate of the average squared difference across the entire neural population. The key thing to note in this analogy is that representational distances can be estimated as averages of the squared differences between neural activities. The sample average of the squared differences is a good estimate of the population average of the squared differences, and the latter is proportional to the population squared Euclidean distance. The proportionality constant depends on the number of neurons. However, researchers using representational similarity analysis [<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c43">43</xref>] usually disregard the scaling and rely only on the distance ratios to make inferences.</p>
<p>If, instead of sampling a subset of neurons, each channel measured a linear combination of the activity of the neurons in the population with random weights, the measurement process would amount to a projection of the point cloud onto a set of randomly oriented vectors [<xref ref-type="bibr" rid="c16">16</xref>]. Random projections can yield undistorted distance estimates [<xref ref-type="bibr" rid="c20">20</xref>]. In particular, if the projection weights are drawn i.i.d. from a zero-mean distribution, then the distances will be undistorted in expectation. Moreover, the Johnson-Lindenstrauss Lemma [<xref ref-type="bibr" rid="c20">20</xref>] states that the number of measurement channels required for a good approximation does not depend on the number of neurons and grows only logarithmically with the number of stimuli. The Johnson-Lindenstrauss Lemma thus suggests (1) that even measurements that combine many neurons might yield undistorted estimates of the geometry, and (2) that a realistically modest number of measurement channels might suffice.</p>
<p>Unfortunately, the Johnson-Lindenstrauss Lemma doesn’t directly apply to either fMRI voxels or cell recordings, where the sampling weights will not have expected values of zero. We can think of fMRI voxels as taking locally extended and overlapping non-negatively weighted averages [<xref ref-type="bibr" rid="c25">25</xref>] and of array recordings as taking sparse punctate samples. In either case, the linear combinations will not use negative weights, and thus the mean weight will not be zero.</p>
</sec>
<sec id="s1d">
<title>Contribution of this paper</title>
<p>Using theoretical analysis and simulations we demonstrate several facts. If each sample consists of a single neuron and neurons are sampled at random, the estimated representational distances are undistorted and even just 50 neurons can yield a good correlation with the ground-truth distances. Sampling neurons with (hypothetical or simulated) voxels that use non-negatively weighted linear combinations, where the weight of each neuron is drawn i.i.d. from a non-negative weight distribution, results in a geometry that correctly reflects the population-mean dimension, while deemphasizing dimensions orthogonal to the population-mean dimension. The resulting apparent geometry is scaled down uniformly for all dimensions orthogonal to the neural-population mean dimension, while remaining unscaled along the neural-population mean dimension. Because we always disregard the overall scaling when comparing representational distance matrices, we can equivalently (and perhaps more intuitively) imagine the apparent geometry as linearly stretched along the neural-population-mean axis.</p>
<p>Pattern information analyses and representational similarity analyses often remove the regional-mean activity from each pattern before assessing the distance or decoding accuracy [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c35">35</xref>]. This normalization step can be motivated as making pattern analyses more complementary to regional-mean activation analyses. An alternative motivation is that removing the mean from each pattern deemphasizes the neural-population mean axis, which is overemphasized in fMRI relatively to all other dimensions of the neural population response. Removal of each pattern’s mean is also implicit to the correlation distance, which is the most widely used representational dissimilarity measure.</p>
<p>Removing the mean from each fMRI pattern estimate may be expected to reduce the information content. However, the mathematical and simulation results of this study show that it can help reveal the underlying neural representational geometry, correcting the distortion caused by local averaging if voxels sample random sets of neurons. Our results explain why the correlation distance “works” so well as a measure of representational dissimilarity in the context of fMRI data. [<xref ref-type="bibr" rid="c31">31</xref>] used the correlation distance to measure representational dissimilarities and revealed a close match between the representational geometries estimated from human fMRI and monkey cell recordings. We show below how the mean removal implicit to the correlation distance revealed the matching representational geometries.</p>
<p>The correlation distance has been by far the most popular representational dissimilarity estimator in fMRI [<xref ref-type="bibr" rid="c33">33</xref>, <xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c46">46</xref>, <xref ref-type="bibr" rid="c7">7</xref>]. The correlation distance is proportional to the squared Euclidean distance measured after removing the mean and normalizing the variance of each pattern (see Supplemental material 7.4). The variance normalization causes a serious drawback: The correlation distance cannot be interpreted in terms of stimulus decodability because identical patterns of low amplitude will have low correlations (because noise dominates) and therefore large correlation distances; [<xref ref-type="bibr" rid="c45">45</xref>]. Our results here suggest that the benefits of the correlation distance result from mean removal. Removing the mean from each pattern is well motivated by the goal to investigate the underlying neural representational geometry – within the limits imposed by fMRI. We can get the benefit of mean removal without the drawback of variance normalization, by replacing the correlation distance by the squared Euclidean or Mahalanobis distance (or an unbiased estimator of these; [<xref ref-type="bibr" rid="c9">9</xref>]), computed after mean removal.</p>
<p>The structure of the paper is as follows. We begin by formulating the problem of estimating representational geometries from measurements of brain activity in mathematical terms. Our theoretical analysis uses models of neural activity sampling to reveal the distortion of the neural representational geometry that is expected. These analyses reveal, in particular, under what circumstances distance estimates are guaranteed to be undistorted in expectation despite limited measurements. We then describe simulations based on ground-truth geometries to confirm the theoretical results and illustrate the effect that removing the mean from the measurement patterns has in correcting the distortion. Finally, we discuss the implications of these results for interpreting past empirical findings and for designing future analyses of representational geometries.</p>
</sec>
</sec>
<sec id="s2">
<label>2</label><title>Results</title>
<sec id="s2a">
<label>2.1</label><title>Theory</title>
<p>We consider measuring the responses of <italic>N</italic> neurons with <italic>M</italic> measurement channels. Each measurement channel <italic>π<sub>i</sub></italic> (<italic>i</italic> = 1<italic>..M</italic>) reflects the neural activity pattern <bold>x</bold> (<italic>N ⇥</italic> 1 column vector) as a linear combination with random weights <italic>w<sub>ij</sub></italic> ∈ ℝ drawn i.i.d. from some probability distribution over the real numbers (<italic>j</italic> = 1<italic>..N</italic>). The univariate distribution from which each <italic>w<sub>ij</sub></italic> is independently drawn could be of any shape. In particular, it could be a zero-mean Gaussian, a Gaussian with a positive mean, a non-negative distribution, or a binary distribution with a spike at zero and another at one. We can think of fMRI voxels as sampling with weights drawn from a non-negative distribution: Each voxel takes a local non-negatively weighted linear combination of the neural activity. We can think of invasive neurophysiological recordings as sampling with weights from a binary distribution with a spike at zero and another spike at one: Each electrode tip takes a punctate sample of the neural activity, perhaps reflecting spiking activity of multiple local neurons.</p>
<p>We assume here that the measurements are not affected by noise. Noise creates a positive bias for distance estimates, which can be removed through cross-validation [<xref ref-type="bibr" rid="c45">45</xref>]. However, this is an orthogonal issue to the one we consider here. The bias we are concerned with is not caused by noise, but by the greater sensitivity to the neural population-mean dimension of the neural response space that arises when measurement channels average with non-negative weights across the neurons.</p>
<sec id="s2a1">
<label>2.1.1</label><title>Measured dissimilarities are unbiased for zero-mean neural sampling weights or equal-mean neural patterns</title>
<p>For this independent sampling model, the expectation of the squared Euclidean distance between the measurements <italic>π</italic>(<bold>x</bold>) and <italic>π</italic>(<bold>y</bold>) (<italic>M</italic> × 1 column vectors) of two neural activity patterns <bold>x</bold> and <bold>y</bold> decomposes into two additive terms:
<disp-formula id="eqn1">
<graphic xlink:href="630743v1_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where
<disp-formula id="eqn2">
<graphic xlink:href="630743v1_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<bold>W</bold> is an <italic>M</italic> × <italic>N</italic> matrix whose entries are <italic>w<sub>ij</sub></italic>, drawn i.i.d. from a univariate weight density. (Proof in Supplemental material 7.1. See also, [<xref ref-type="bibr" rid="c43">43</xref>])</p>
<p><xref rid="eqn1" ref-type="disp-formula">Equation 1</xref> shows that the neural population mean dimension of the neural response space is overrepresented relative to all other dimensions when the representational dissimilarities are naively computed from the measurement channels, unless the univariate weight distribution has mean zero <inline-formula><inline-graphic xlink:href="630743v1_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. All other dimensions are represented without bias as the measurements take random linear combinations of neural activity. If either the mean of the weight distribution is zero or the mean neural activity is the same for each activity pattern <inline-formula><inline-graphic xlink:href="630743v1_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> then the second term in <xref rid="eqn1" ref-type="disp-formula">Eq. 1</xref> is zero, and all representational dissimilarities are correctly reflected in the measured representational dissimilarities in expectation.</p>
</sec>
<sec id="s2a2">
<label>2.1.2</label><title>The apparent representational geometry is linearly stretched along the all-1 direction of the neural response space</title>
<p>For the linear random sampling model we consider here, the apparent representational geometry (i.e. the geometry implied by the Euclidean distances among the measured patterns) is distorted in a particularly simple way: It is stretched along the all-1 direction of the neuronal response space (<xref rid="fig1" ref-type="fig">Fig. 1</xref> c, d). The assumption that each measurement channel randomly samples the neurons entails that all directions orthogonal to the all-1 direction are equally represented. However, unless the weight distribution has a mean of 0, the projection axes corresponding to the measurement channels (<xref rid="fig1" ref-type="fig">Fig. 1</xref> c, e) are somewhat aligned with the all-1 vector, oversampling the component along the all-1 direction, which causes the stretching of the apparent representational geometry along the all-1 direction.</p>
<p>Note that we care only about the relative representation of components along the all-1 direction and orthogonal to this direction, not about the overall scale factor. This justifies thinking of the geometry as <italic>stretched</italic> along the all-1 direction. If we cared about the absolute distances, it would be more appropriate to think of the geometry as linearly shrunk along all directions orthogonal to the all-1 vector because it is signal cancellation caused by averaging that reduces our sensitivity to pattern differences orthogonal to the all-1 vector, whereas averaging neural responses does not reduce our sensitivity to changes of the mean of the neural responses.</p>
<p>The factor <italic>f<sub>shrink</sub></italic> by which orthogonal directions are shrunk relative to the all-1 direction follows directly from <xref rid="eqn1" ref-type="disp-formula">Eq. 1</xref> (see Supplemental material 7.2 for derivation):
<disp-formula id="eqn3">
<graphic xlink:href="630743v1_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
To see how this factor follows from <xref rid="eqn1" ref-type="disp-formula">Eq. 1</xref>, consider that the second term of <xref rid="eqn1" ref-type="disp-formula">Eq. 1</xref> is zero for two neural response patterns differing only orthogonal to the all-1 vector of the neural response space, so the measured squared Euclidean distance is the neural squared Euclidean distance times Var[<italic>w</italic>]. For two neural response patterns differing only along the all-1 vector in the neural response space, by contrast, both terms from <xref rid="eqn1" ref-type="disp-formula">Eq. 1</xref> matter, and the measured squared Euclidean distance is the neural squared Euclidean distance times Var[<italic>w</italic>]. Note that, consistent with the discussion above, the geometry is undistorted when the weights distribution has expected value <inline-formula><inline-graphic xlink:href="630743v1_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which renders the ratio 1.</p>
</sec>
<sec id="s2a3">
<label>2.1.3</label><title>Dissimilarities among the mean-removed measured patterns provide unbiased estimates of neural dissimilarities</title>
<p>We now show a perhaps counterintuitive fact: When we remove the mean activation across channels from each measured activity pattern, then the squared Euclidean distances among the (mean-removed) measured patterns correctly reflect the neural dissimilarities without any bias in the representation of the neural population mean dimension.</p>
<p>It is intuitive that removing the mean of each measured pattern would reduce the overrepresentation of the neural population mean dimension that results when the weights have non-zero mean. However, it is not obvious why removing the mean of each measured pattern <italic>exactly</italic> removes the bias and results in dissimilarity estimates that, in expectation, correctly reflect both the neural population mean dimension and all other dimensions, without either overor underrepresenting the neural population mean dimension.</p>
<p>If the mean of the measurements perfectly reflected the neural population mean, then removing the mean of each measured pattern would collapse all variability along the neural population mean and overcorrect: Any differences along the neural population mean dimension would not be reflected in the measured representational dissimilarities at all anymore. However, the mean of the measurements does not perfectly represent the neural population mean dimension. The measurement mean deviates in such a way that removing it entails appropriate representation of the neural population mean dimension relative to all other dimensions.</p>
<p>Removing the mean from the measured voxel pattern <bold>v</bold> = <bold>Wx</bold> to obtain the centered voxel pattern v̅ can be achieved by pre-multiplication with a centering matrix <inline-formula><inline-graphic xlink:href="630743v1_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which centers each incoming column:
<disp-formula id="eqn4">
<graphic xlink:href="630743v1_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Centering the voxel pattern <bold>v</bold> is equivalent to centering each column of the neural sampling weights matrix <bold>W</bold>. The centered voxel pattern v̅, thus, behaves as if it were an activity pattern measured with zero-mean neural sampling weights <inline-formula><inline-graphic xlink:href="630743v1_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where the bias (second term of right hand side in <xref rid="eqn1" ref-type="disp-formula">Eq. 1</xref>) vanishes (see Supplemental material7.3.1 for more details).</p>
<p>Note that centering the measured patterns (or equivalently each column of <bold>W</bold>) is not exactly equivalent to drawing weights i.i.d. from a centered version of the univariate distribution that the weights are drawn from. If the univariate weight distribution were centered, then each column of <bold>W</bold> would not exactly sum to 0. The expected value of the sum of weights would be 0, but there would be a small deviation reflecting the randomness of the sample. However, in expectation, centering the measured patterns yields the same distance estimate as measuring with a technique where each “voxel” samples with 0-mean weights.</p>
<p>To understand the effects of removing the mean from each pattern intuitively, we need to consider the <italic>rows</italic> of <bold>W</bold>, the neural sampling weight vectors, whose tips we can think of as an ensemble of points in the (high-dimensional) neural response space (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>). Centering each column of <bold>W</bold> amounts to centering of the ensemble of tips of the neural sampling weight vectors on the origin of the neural response space (<xref rid="fig1" ref-type="fig">Fig. 1f</xref>). As the tips are moved together (each along the same vector, thus rigidly maintaining their geometric relationships), each vector continues to emanate from the origin, so the direction it points in changes. The changes of direction entailed by centering the ensemble of tips changes the dimension of neural response space that each vector samples. The projection axes are then no longer aligned around the all-1 vector.</p>
<p>Note that centering the ensemble of tips does not change the geometry of the ensemble (the shape of the distribution of tips) over the neural response space. It only changes the means (making the mean weight for each neuron zero). As a consequence, projecting the weight vectors onto any given axis of neuronal response space yields the same (scalar) variance when done before or after centering. This is why the centering yields an equal distance estimate for the measurement of unit-norm pattern difference vectors of any orientation in expectation. After centering, the projection axes equally sample all directions.</p>
<p>The formal result that the squared Euclidean distance between the mean-removed measurement patterns <inline-formula><inline-graphic xlink:href="630743v1_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> provides an unbiased estimate of the distance between the original neural patterns can be stated as follows:
<disp-formula id="eqn5">
<graphic xlink:href="630743v1_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
(See Supplemental material 7.3.2 for proof.)</p>
</sec>
</sec>
<sec id="s2b">
<label>2.2</label><title>Simulations</title>
<sec id="s2b1">
<label>2.2.1</label><title>Simulations based on random ground-truth geometries</title>
<p>We simulated neural activity patterns for a set of experimental conditions by drawing points randomly from a multivariate probability distribution. This provided a ground-truth representational geometry that we compared to the geometries obtained by measuring the activity patterns with different sampling models. We used sampling models that mimic the effect of measuring a population of neurons using single-cell recordings and fMRI.</p>
<sec id="s2b1a">
<title>Simulations confirm theoretical results</title>
<p><xref rid="fig2" ref-type="fig">Fig. 2</xref> shows representational dissimilarity matrices (RDM) for an example ground-truth simulated geometry and the geometries obtained by sampling the simulated neural patterns with linear combinations of the neurons. We consider two variants of an independent-weight-linear-combination-sampling (IWLCS) model. In the random-projection sampling model, weights are drawn i.i.d. from a zero-mean Gaussian. In the voxel sampling model, weights are drawn i.i.d. from a non-negative distribution. In the voxel model, thus, each channel reflects a non-negatively weighted linear combination of the neuronal activity (see Methods, 4.1.2). The dissimilarity matrices illustrate that the distances estimated using the raw measurements from the voxel sampling model (<xref rid="fig2" ref-type="fig">Fig. 2</xref>, bottom middle) are poorly correlated with the true neural distances (<xref rid="fig2" ref-type="fig">Fig. 2</xref>, top left). The distorted distance estimates are highly correlated with the absolute differences of the population-mean response (<xref rid="fig2" ref-type="fig">Fig. 2</xref>, bottom left), illustrating that the population-mean dimension is overrepresented in the measurements. As predicted by the theoretical results, removing the mean from the measured patterns before estimating the dissimilarities visibly corrects for the distortion, yielding an RDM that captures the ground-truth distances (<xref rid="fig2" ref-type="fig">Fig. 2</xref>, bottom right). For the random projection reference model, both the raw pattern distance estimates (top middle) and the mean-removed pattern distance estimates (top right) reflect the ground-truth geometry well. The mean removal here is similar to removing a randomly chosen dimension and does not strongly alter the apparent geometry.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Representational dissimilarity matrices for an example randomly generated geometry and simulated measurements.</title>
<p>The left column shows squared Euclidean distances between the pairs of simulated neuronal patterns (top) and the absolute difference after averaging over all the neurons for each activity pattern (bottom). The middle column shows the distances between the patterns obtained after a random projection (top) and a simulation of voxel sampling (bottom). The right column shows the squared Euclidean distances between the pairs of activity patterns after subtracting the mean over the channels for each individual simulated experimental condition for random projection sampling (top) and simulated voxel sampling (bottom). For random projections (top), note that mean removal amounts to the removal of one of many dimensions of the representational space and does not appreciably alter the apparent representational geometry. For the voxel sampling model (bottom), mean removal corrects the overemphasis on the population-mean dimension (which results from averaging within voxels), revealing the otherwise hidden neural representational geometry.</p></caption>
<graphic xlink:href="630743v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We quantified these results by calculating the Pearson correlation coefficient between the simulated ground-truth RDMs and the measured RDMs, varying the number of measurement channels. For each simulation, we randomly generated a new ground-truth geometry. The results across simulations are shown in <xref rid="fig3" ref-type="fig">Fig. 3</xref>. We simulated the measurement process using two models of fMRI voxel sampling (both non-negative IWLCS models, one of them sparse), one model of single-cell array recordings (using a subsample of the neurons), and random projections for reference (for details on the measurement models, see Methods, <xref rid="s4a2" ref-type="sec">section 4.1.2</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Correlation between representational distances of ground-truth simulated activity patterns and simulated measured activity patterns.</title>
<p>Plots show the Pearson RDM correlation (vertical axes) between the simulated neural pattern RDM and the simulated measured pattern RDMs for five measurement models (lines) averaged across simulations. The shaded region around each individual line indicates the standard error of the mean across simulations. For each of a sample of randomly generated representational geometries, we simulate measuring the neuronal population varying the number of channels (horizontal axis). Panels show the RDM correlation between: <bold>a.</bold> the squared Euclidean distance RDM of the ground-truth simulated neuronal patterns and each of the squared Euclidean distance RDMs for simulated measured patterns. <bold>b.</bold> the squared Euclidean distance RDM of simulated neuronal patterns and the RDM of simulated measurement patterns after mean removal. <bold>c.</bold> the squared Euclidean distance RDM of the simulated neuronal patterns and the correlation distance RDM of the simulated measurement patterns. <bold>d.</bold> the RDMs obtained by taking the absolute differences of the average over all the simulated neurons/channels for each activity pattern. <bold>e.</bold> the RDM of simulated neuronal patterns after mean removal and the RDM of simulated measured patterns after mean removal. <bold>f.</bold> the correlation distance RDM of the simulated neuronal patterns and the correlation distance RDM of the simulated measurement patterns.</p></caption>
<graphic xlink:href="630743v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The neural distances are captured equally well by the random projection models and the subsample model (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>). In contrast, non-negative IWLCS models result in distances that correlate poorly with the neural distances regardless of the number of measurement channels (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>). Notably, the mean component of the ground-truth RDM is recovered accurately from the population-mean distances obtained with the non-negative weights models, which illustrates the overrepresentation of the population-mean dimension resulting from non-negative sampling (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>). Consistent with the theoretical results, removing the mean from the measured patterns before estimating the dissimilarities recovers the underlying geometry in expectation. The mean-removed RDM is highly correlated with the ground-truth RDM for every model for as few as 50 measurement channels (<xref rid="fig3" ref-type="fig">Fig. 3b</xref>).</p>
<p>The theoretical results, perhaps counterintuitively, suggested that removing the mean from the measured patterns does not entail undersampling of the true neural population mean. To assess whether this prediction held in the simulations, we removed the ground-truth population mean from the ground-truth neural patterns and compared the resulting simulated neural pattern dissimilarities to the mean-removed measured pattern dissimilarities. For the non-negatively weighted average models, this comparison yields lower RDM correlations than for the original ground-truth neural patterns, consistent with the prediction from the theoretical results that the mean-removed measured patterns reflect all dimensions (including the true neural population-mean dimension) equally in expectation (<xref rid="fig3" ref-type="fig">Fig. 3e</xref>).</p>
</sec>
<sec id="s2b1b">
<title>Removing the mean from measured activity patterns provides better estimates of the neural representational geometry than using the correlation distance</title>
<p>A common practice when analyzing measurements of brain activity using RSA is to use the correlation distance to characterize the geometry of the measured patterns. The correlation-distance RDM is proportional to the squared Euclidean distance RDM computed after mean removal and variance normalization of each pattern (details in Supplemental material 7.4). Given the implicit removal of the mean across channels from each pattern when using the correlation RDM, we asked how well the correlation-distance RDM captures the simulated neuronal RDM. We found that using the correlationdistance RDM yields similar RDM correlations as using the squared-Euclidean RDM after meanremoval (without normalizing the pattern variance). The correlation distances among the measured patterns are highly correlated with the squared Euclidean distances in the ground truth geometry (<xref rid="fig3" ref-type="fig">Fig. 3c</xref>). As before, when we removed the ground-truth neural population-mean from the groundtruth neural patterns, the RDM correlation with the mean-removed measured pattern RDM was reduced (<xref rid="fig3" ref-type="fig">Fig. 3e, f</xref>).</p>
<p>Our results show that pattern mean removal alone (without the pattern variance normalization implicit to the correlation distance) yields significantly higher RDM correlations with the ground-truth neural RDM for both voxel models tested (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). The lower RDM correlation with the ground-truth RDM can be explained by the additional transformation introduced by the correlation distance: the divisive normalization by the variance for each pattern. Our simulations suggest that this extra step degrades the accuracy of distance estimates.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Pattern-mean removal alone is more effective than pattern-mean removal and pattern-variance normalization at correcting the population-mean overemphasis in the measured representational geometry.</title>
<p>We can compute the squared Euclidean distance on raw (red) or normalized measured patterns. Normalization can consist in subtracting the mean across measurement channels (e.g. voxels) from each pattern (beige) or in subtracting the mean and applying divisive normalization to scale the pattern variance to 1 (blue). When the squared Euclidean distance is computed after mean and variance normalization, it equals twice the Pearson correlation distance (derivation in Supplement Section 7.4) and thus yields the same RDM correlations (blue). Bars indicate the RDM correlation between the ground-truth neural RDM (squared Euclidean distance) and the RDMs containing the squared Euclidean distances among the measured patterns. Each subplot corresponds to a a different measurement model. If the measurements sample random projections (weights drawn i.i.d. from a 0-mean Gaussian, an unrealistic scenario shown in the left panel), the squared Euclidean distances of the measurement channels reflect the ground-truth neuronal representational geometry well. Mean removal hardly aflects the RDM correlation, but mean removal and variance normalization (implicit in the correlation distance) performs markedly worse. If the measurements sample non-negatively weighted averages (a scenario comparable to fMRI voxels, center panel), mean removal greatly improves the RDM correlation with the ground-truth neuronal RDM in the estimated RDM. Additional normalization of the pattern variance (or, equivalently, the use of correlation distance) markedly degrades the estimate of the neural representational geometry. If the measurements consist in a random subsample of the neurons (an idealization of the typically somewhat biased samples in neural recording experiments, right panel), the estimate of the representational geometry is highly accurate. Mean removal then slightly degrades the RDM correlation with the ground truth. Mean removal and variance normalization markedly degrade the RDM correlation with the ground truth. The error bars represent the standard error of the mean across simulations with different ground-truth representational geometries</p></caption>
<graphic xlink:href="630743v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s2b2">
<label>2.2.2</label><title>Simulations based on neural data</title>
<p>[<xref ref-type="bibr" rid="c31">31</xref>] analyzed inferior temporal (IT) activity patterns elicited by images of objects belonging to different categories in humans and macaques. The activity patterns were measured using fMRI in humans (blood-oxygen-level-dependent fMRI at 3 Tesla using 2-mm-wide isotropic voxels) and cell recordings in macaques (data set from [<xref ref-type="bibr" rid="c23">23</xref>]). The representational dissimilarity matrices obtained in this study were highly correlated between the two species. We now use simulations based on these empirical data to provide a plausible explanation as to why the representational geometry appears to be conserved despite the radical difference in the two measurement techniques.</p>
<p>Using the empirically observed dissimilarities among stimuli in the monkey IT, we created embeddings of high-dimensional patterns corresponding to neural activations in response to the visual stimuli (see Methods <xref rid="s4b" ref-type="sec">Section 4.2</xref>). We used these patterns as the ground-truth reference geometry and, as before, sampled this simulated neural population using various measurement models.</p>
<p><xref rid="fig5" ref-type="fig">Fig. 5</xref> shows RDMs from one example measurement simulation with 50 channels. The squared Euclidean distances between patterns estimated from the voxel sampling model appear distorted relative to the ground-truth pattern distances. The RDM obtained after mean-removal, in contrast, visually closely resembles the neural RDM. For the subpopulation sampling model both RDMs apparently reflect the true geometry well.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Representational dissimilarity matrices for empirical data and simulated measurement patterns.</title>
<p>Panel <bold>a</bold> shows the RDM of the distances between the activity patterns generated from cell recordings of 674 neurons in monkey IT [<xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c31">31</xref>], which we use as the ground-truth geometry for the simulated measurements. Panel <bold>b</bold>: Left column shows the RDM of squared Euclidean distances between simulated measurements taken with the subpopulation sampling model (top) and non-negative IWLCS model (bottom). Middle column shows the squared Euclidean distance RDMs taken from the simulated measurements after removing the mean from each measured pattern. The right column shows the RDMs obtained using the correlation distance on the simulated measurements, which implicitly removes the mean, and matches the analyses used in the empirical study.</p></caption>
<graphic xlink:href="630743v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We replicated the analyses from [<xref ref-type="bibr" rid="c31">31</xref>] on our simulated measurements. To match the study’s methods we used the Pearson correlation distance (1-r) as the dissimilarity estimator (<xref rid="fig5" ref-type="fig">Fig. 5</xref>, right). The correlation distance implicitly removes the mean from each measured pattern. The distribution of representational dissimilarities between ground-truth response patterns and experimental and simulated measurement patterns are shown in <xref rid="fig6" ref-type="fig">Fig. 6</xref>. The correlations obtained with the voxel sampling model (<xref rid="fig6" ref-type="fig">Fig. 6b</xref>) closely resemble those observed with the fMRI measurements from human IT in the original study (<xref rid="fig6" ref-type="fig">Fig. 6a</xref>). The subpopulation model yielded correlations that closely matched the monkey IT correlations (<xref rid="fig6" ref-type="fig">Fig. 6c</xref>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Correlation of representational dissimilarities between monkey and human IT for empirical data and simulated measurements.</title>
<p>Panel <bold>a</bold> shows the results from the empirical data in the original study. Each dot marks the correlation between activity patterns for a pair of stimuli as measured in monkey IT (horizontal axis) and human IT (vertical axis). Simulation results are shown in panel <bold>b</bold>, where voxels were simulated as sampling a neural population with non-negatively weighted means. The simulated neural population encoded a representational geometry as observed in the monkey data, so the neural pattern dissimilarities (correlation distance: 1 <italic>— r</italic>, horizontal axes) match those of the actual neural recording data (left panel). These ground-truth dissimilarities are plotted against the dissimilarities among thesimulated voxel response patterns (correlation distance: 1 <italic>— r</italic>, vertical axis). Panel <bold>c</bold> shows results of a similar simulation, where the measurement model selects a random subset of neurons (subpopulation sampling). The concentration around the diagonal shows that the representational dissimilarities (1<italic>—r</italic> as before for both sampling) can be quite accurately estimated from a subpopulation of 200 neurons, if these neurons are sampled randomly.</p></caption>
<graphic xlink:href="630743v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>These empirically-grounded simulations demonstrate that modeling single-cell recordings and fMRI measurements as projections of the neural response space can reveal the dissimilarities between real neural patterns when the correlation distance is used as the RDM estimator. Moreover, they highlight the necessity and sufficience of removing the mean from the measured patterns —- an operation performed implicitly by the correlation distance—to uncover the close correspondence between these two measurement modalities. In contrast, estimating RDMs directly from raw patterns fails to achieve this close match.</p>
</sec>
</sec>
</sec>
<sec id="s3">
<label>3</label><title>Discussion</title>
<p>Our theoretical and simulation results promise that electrophysiological recordings and fMRI can give us good estimates of the neural-population representational geometry in certain scenarios. However, it is important to understand the assumptions these results depend on. Our encouraging results hold when measurement channels sample neurons linearly with weights drawn independently from some univariate distribution. This assumption is not in general realistic for either neural recordings or fMRI voxels. However, it provides an important idealized reference.</p>
<p>For fMRI, the non-negative independent-weight-linear-combination-sampling (IWLCS) model captures a best-case scenario in one sense and a worst-case scenario in another. It is a best-case scenario in the sense that all directions orthogonal to the all-1 direction of the neural response space are sampled equally and in that mean-removal enables the recovery of the original distances in expectation. It is a worst-case scenario in that each voxel samples a large number of independent dimensions of the neural response space, leading to extreme cancellation of tuning curves, which entails shrinkage of the geometry by a large factor in all dimensions orthogonal to the all-1 dimension. The non-negative IWLCS model is probably closer to reality for fMRI voxels if we assume that the sampled units are not single neurons but cortical columns of similarly tuned neurons. This reduces the number <italic>N</italic> of underlying representational dimensions and the number <italic>k</italic> of dimensions linearly combined by each voxel. The non-negative IWLCS model then correctly reflects that each voxel tends to average a less diverse set of tuning curves with subsets of the neurons that are correlated in their tuning, which entails less severe cancellation of tuning curves.</p>
<p>We now summarize the key insights of this study. <xref rid="fig7" ref-type="fig">Fig. 7</xref> gives an overview. The measurement models we consider can be categorized into three main types:
<list list-type="alpha-lower">
<list-item><p><bold>Random projections (</bold><xref rid="fig7" ref-type="fig">Fig. 7a</xref><bold>)</bold>: We consider the theoretical scenario of sampling the neural activity as linear combinations with the weights drawn from a zero-mean Gaussian distribution. Under these conditions, the pairwise distances of measured activity patterns remain undistorted in expectation. Moreover a relatively small number of channels (e.g. 100) is sufficient for good estimates of the representational geometry. Unfortunately, this scenario does not hold for any real measurement modality.</p></list-item>
<list-item><p><bold>Electrophysiological recordings (</bold><xref rid="fig7" ref-type="fig">Fig. 7b</xref><bold>)</bold>: We consider measurements where the individual activity of neurons is sampled invasively. Two potential scenarios arise:
<list list-type="bullet">
<list-item><p><bold>Random sampling</bold>: When recordings effectively represent a random sample of all neurons in the area, the measured representational geometry remains undistorted. Again, a small number of channels (e.g. 100) is sufficient for good estimates of the representational geometry. Pattern mean removal is not required or desirable in this scenario; it slightly reduces the accuracy of the geometry estimate as shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref>.</p></list-item>
<list-item><p><bold>Biased sampling</bold>: Situations in which the neural selectivities are organized in a spatially structured manner, and the array only samples a fraction of this structure, present challenges. For instance, if an array is placed over a region that shows a particular selectivity, the measured representational geometry might fail to capture stimulus distinctions encoded in a different subset of the neural population of interest. As a result, the apparent geometry can be distorted in complex ways. Taking such distortions into account in inference on brain-computational models would require modeling the measurement-related biases and our uncertainty about them.</p></list-item>
</list>
</p></list-item>
<list-item><p><bold>fMRI measurements (</bold><xref rid="fig7" ref-type="fig">Fig. 7c</xref><bold>)</bold>: We consider voxels that sample local averages of neural activity with non-negative weights. Depending on the spatial organization of the encoded information, outcomes can vary:
<list list-type="bullet">
<list-item><p><bold>Code is spatially unstructured or structured at a single scale</bold>: A code that has no spatial organization justifies the assumption that the channels sample with non-negative weights drawn independently from identical distributions. Structure at a single scale would hold if there were clusters of similarly tuned neurons (e.g. cortical columns), but these clusters were randomly arranged in the region. The latter situation is a sweet spot for fMRI: the clustering reduces signal cancellation due to averaging in voxels, yet the random arrangement of the clusters prevents complex distortions of the representational geometry (beyond overemphasis on the neural population mean). Whether the code is unstructured or structured at a single scale, two scenarios must be considered:
<list list-type="simple">
<list-item><label>i.</label><p>If the neural population mean is constant across conditions, then the measured representational geometry remains undistorted in expectation. Removing the mean from each the measured response pattern is not required (though it also will not hurt) for accurate reflection of the neural population representational geometry in the measured squared Euclidean distances.</p></list-item>
<list-item><label>ii.</label><p>If the neural population mean varies across conditions, the measured squared Euclidean RDM is a linear combination of the original neuronal pattern RDM and the populationmean activation RDM. In this case, removing the mean from the measured patterns provides an undistorted estimate of the underlying representational geometry.</p></list-item>
</list>
</p></list-item>
<list-item><p><bold>Code is spatially structured at multiple scales</bold>: Codes that are structured at multiple scales will yield RDMs distorted in complex ways, reflecting the degrees to which different dimensions of the neural population code are cancelled by averaging inside voxels. For example, V1 is structured locally into orientation columns and globally into a retinotopic map. Orientation signals will be reduced substantially by averaging in voxels, whereas location signals will cancel much less because they reside at a coarser scale and are therefore more strongly reflected in fMRI patterns. Accounting for such complex distortions in evaluating brain-computational models requires models that predict the multi-scale spatial structure of the code, such that the distortions arising from locally-averaging voxels can be accounted for</p></list-item>
</list>
</p></list-item>
</list>
</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7:</label>
<caption><title>When do measured representational distances reflect the neural representational geometry?</title>
<p>Three scenarios of brain-activity measurement conditions and their consequences for the resulting representational geometry. <bold>a.</bold> In the theoretical case of random projection sampling, the measured representational dissimilarities are an unbiased estimate of the neural pattern representational dissimilarities. <bold>b.</bold> For electrophysiological recordings, we consider two scenarios. In the first, the sampled cells are randomly selected from the underlying population, in which case the theoretical results and simulations in this study indicate that the apparent geometry remains undistorted. In the second scenario, where cells are spatially organized and measurements sample only a region of the population, the apparent representational geometry is expected to be distorted in complex ways, requiring separate modeling.. <bold>c.</bold> When sampling with fMRI voxels, if the underlying neural code is spatially structured at multiple scales, the geometry of the measured patterns will be distorted in complex ways that require modeling. In contrast, if the neural code is unstructured, two cases arise. First, if the mean activation across neurons is the same across all experimental conditions, the RDM derived from the measurements will provide an undistorted estimate of the neuronal RDM. Second, if the population mean varies across conditions, the apparent geometry will be linearly distorted along the population mean dimension. In this case, removing the mean activation from each measured pattern restores the underlying neural RDM in expectation.</p></caption>
<graphic xlink:href="630743v1_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s3a">
<title>Multiple distortions affecting estimates of representational geometry</title>
<p>In this paper, we analyzed a particular source of distortion of estimates of the representational geometry that arises from the way measurement channels sample neural activity. It is important to note that the extent to which the measured geometry will be able to capture the true geometry is also affected by the fact that the measurements provide noisy estimates of the true activity patterns. A well-understood additional source of distortion that occurs when measuring representational geometries from brain activity data is the positive bias in the estimated distances between measured patterns. Consider the case where the neural patterns are identical (i.e. their true distance is zero). The measurement noise will cause errors in the pattern estimates, which will yield distances larger than 0. In general, high-dimensional measured patterns will tend to be further apart than the underlying true (i.e. noise free) patterns. This positive bias increases with the level of noise. This challenge has been investigated previously [<xref ref-type="bibr" rid="c45">45</xref>], and cross-validated distance estimators [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c9">9</xref>] have been introduced to remove it.</p>
<p>Our findings here demonstrate the overrepresentation of the population-mean dimension in measurements that average neural responses—a separate phenomenon independent of the noise. While mean removal can mitigate this, it will not mitigate the distortion effects induced by noise. In order to account for both of these sources of distortion, cross-validated distance estimators can be applied to pattern estimates after mean removal.</p>
</sec>
<sec id="s3b">
<title>Removing the mean from voxel patterns in fMRI measurements offers the benefits of the correlation distance while avoiding its drawbacks</title>
<p>The correlation distance between two patterns (1 <italic>— r</italic>, where <italic>r</italic> is the Pearson correlation coefficient) is proportional to the squared Euclidean distance computed after normalizing each pattern by subtracting its mean and dividing by its standard deviation (thus normalizing the mean to 0 and the variance to 1). The divisive pattern normalization introduces a notable pitfall: the resulting correlation distance does not reliably reflect linear discriminability. Consider two patterns corresponding to different conditions neither of which drives a significant response in our region of interest. The pattern estimates are dominated by noise and will be essentially uncorrelated (<italic>r ≈</italic> 0). Consequently, the correlation distance will be close to 1, suggesting that the patterns are highly dissimilar. However, a linear decoder would find that two conditions that drive no distinct responses are not decodable. If we remove the mean from fMRI pattern measurements, but do not perform variance normalization, we get the advantage of the correlation distance (correcting for the overrepresentation of the neural population mean dimension) while avoiding its pitfall (weak relationship to decodability).</p>
</sec>
</sec>
<sec id="s4">
<label>4</label><title>Methods</title>
<sec id="s4a">
<label>4.1</label><title>Random geometry simulations</title>
<sec id="s4a1">
<label>4.1.1</label><title>Simulated activity patterns</title>
<p>In each simulation, we generated a random “ground truth” geometry of neural activity response patterns. We generated 100 data points derived from a Gaussian mixture in a five-dimensional space. These points were then embedded into a 1024-dimensional space via a random rotation. This resulted in the simulated activity of a population of 1024 neurons to 100 different experimental conditions. The proportion of the total response pattern variance attributed to the mean dimension averaged 25% across all simulations. We created a new random geometry for each new simulation.</p>
</sec>
<sec id="s4a2">
<label>4.1.2</label><title>Measurement models</title>
<p>We sampled the simulated neural activity patterns using models that mimic certain properties of different brain-activity measurement modalities.
<list list-type="bullet">
<list-item><p><bold>Zero-mean independent-weight-linear-combination-sampling (IWLCS) model.</bold> This model samples a linear combination of the activity across the entire neuron population, with weights independently drawn from a standard Gaussian distribution. The resulting simulation approximates a Gaussian random projection, providing expected undistorted measurements. This model represents ideal sampling with random projections. It serves as a reference, rather than as a model of any actual measurement modality.</p></list-item>
<list-item><p><bold>Non-negative IWLCS.</bold> In this model, each channel samples a linear combination of the neural activity across the entire simulated neuron population. Weights are independently drawn from a truncated standard normal distribution (negative half set to zero), ensuring all weights are non-negative.</p></list-item>
<list-item><p><bold>Sparse non-negative IWLCS model.</bold> As in the previous model, each channel samples a linear combination from the neuron population. However, the weights are zero for a random 90% of the neurons, and drawn from a half-normal distribution for the remaining 10%.</p></list-item>
<list-item><p><bold>Sparse zero-mean IWLCS model.</bold> A sparse variation of the Gaussian model is also included, where 90% of the weights are zero.</p></list-item>
<list-item><p><bold>Random subpopulation model.</bold> This model emulates single-cell recording of brain activity by sampling the activity of a randomly selected subset of neurons in the population, with one simulated neuron represented per channel. Note that this model is not an IWLCS model, since the sampling axes are one-hot vectors, with exactly one neuronal weight equal to 1 and all other weights equal to 0. As the weights are not sampled identically and independently, this model does not satisfy the IWLCS assumptions.</p></list-item>
</list>
</p>
</sec>
<sec id="s4a3">
<label>4.1.3</label><title>Representational geometry comparisons</title>
<p>We quantified the extent of similarity between the relationships among the neuronal patterns and those among the measured patterns by comparing their representational dissimilarity matrices (RDMs).</p>
<p>We computed the average Pearson correlation between pairs of RDMs across simulations. Six comparisons were performed in total. For each measurement model, we calculated the correlation between: a.) The RDMs of the squared Euclidean distances derived from simulated neuronal patterns (neuron RDM) and those derived from the measured patterns (measured RDM). b.) The one-dimensional RDM of absolute differences between the mean activations in the simulated neural population (neuron mean RDM) and the one-dimensional RDM of absolute differences between the means across the measurement channels (measured mean RDM). c.) The neuron RDM and the RDM of squared Euclidean distances from the simulated measured patterns, after removing the mean measurement from each pattern (mean-removed measured RDM). d.) The RDM of squared Euclidean distances from the simulated neural response patterns, after removing the population mean activation from each pattern, and the mean-removed measured RDM. e.) The neuron RDM and the RDM consisting of correlation distances between the simulated measured patterns (measured correlation RDM). f.) The RDM of correlation distances from the simulated neuronal patterns (neuron correlation RDM) and the measured correlation RDM.</p>
</sec>
</sec>
<sec id="s4b">
<label>4.2</label><title>Simulations based on neural data</title>
<sec id="s4b1">
<title>Empirical data</title>
<p>The neural recording dataset underpinning our simulations is from the study [<xref ref-type="bibr" rid="c23">23</xref>] and has also been used in [<xref ref-type="bibr" rid="c31">31</xref>]. The latter study compared the representational geometry measured with fMRI in the human ventral temporal cortex to the representational geometry estimated from neural recordings in macaque inferior temporal cortex (IT) in response to images of various object categories. We used the 92 <italic>×</italic> 92 representational dissimilarity matrix for macaque IT used in [<xref ref-type="bibr" rid="c31">31</xref>] as a basis for our simulations.</p>
</sec>
<sec id="s4b2">
<title>Ground-truth neural pattern generation</title>
    <p>Using the observed representational dissimilarities from monkey IT, we generated embeddings of neural activation patterns in a 1000-dimensional simulated neural response space. To do so, we converted the monkey IT RDM (correlation distances between patterns) into a positive definite linear kernel (similarity) matrix <italic>K</italic> by double centering and multiplying by -0.5 [<xref ref-type="bibr" rid="c10">10</xref>]. We then used the Cholesky decomposition of the kernel matrix (<italic>K</italic> = <italic>LL<sup>&gt;</sup></italic>) to sample a random stimulus-response matrix conforming exactly to the kernel matrix. To do this, we first sampled a random matrix <italic>U</italic> from a standard normal distribution and normalized it such that its rows had zero mean and unit variance, ensuring no prior structure was imposed. We then multiplied <italic>U</italic> by the Cholesky factor <italic>L</italic>, effectively mapping the random matrix into the space defined by the kernel <italic>K</italic>. Because of the equivalence between linear kernel matrices and squared Euclidean distance matrices, this procedure ensures that the squared Euclidean distances between the patterns in the stimulus-response matrix match the ground-truth RDM. This method for simulating neural response patterns is implemented in the Python RSA toolbox (<ext-link ext-link-type="uri" xlink:href="https://github.com/rsagroup/rsatoolbox">https://github.com/rsagroup/rsatoolbox</ext-link>, [<xref ref-type="bibr" rid="c43">43</xref>]).</p>
<p>The resulting patterns all have the same mean activation. In order to illustrate the distortion that sampling with simulated fMRI voxels creates in the apparent geometry, it is necessary that there be some variation among these means (otherwise, the term in <xref rid="eqn1" ref-type="disp-formula">Eq. 1</xref> would be zero for every pair, and the distance estimates would be undistorted in expectation). To introduce variation along the mean, we added a random mean to each of the 92 patterns. The squared Euclidean distances between these patterns are consistent with the Kriegeskorte et al. study (after mean-removal) and served as our simulated ground-truth neural population for subsequent analysis.</p>
</sec>
</sec>
</sec>
</body>
 <back>
     <sec id="d1e1213" sec-type="additional-information">
         <title>Additional information</title>
         <sec id="s5">
             <title>Author contributions</title>
             <p>N.K. conceived the study. V.B.B. ran simulations and analyzed data. V.B.B. and N.K. interpreted the theoretical and simulation results. V.B.B. and N.K. wrote and edited the manuscript.</p>
         </sec>
     </sec>
<sec id="suppd1e1363" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e1354">
<label>Supplemental material</label>
<media xlink:href="supplements/630743_file02.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alink</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Krugliak</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Walther</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Kriegeskorte</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2013</year>). <article-title>fMRI orientation decoding in V1 does not require global maps or globally coherent orientation stimuli</article-title>. <source>Frontiers in Psychology</source>, <volume>4</volume>:<fpage>493</fpage>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alink</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Walther</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Krugliak</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Kriegeskorte</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Local opposite orientation preferences in V1: fMRI sensitivity to fine-grained pattern information</article-title>. <source>Scientific Reports</source>, <volume>7</volume>(<issue>1</issue>):<fpage>7128</fpage>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Allefeld</surname>, <given-names>C.</given-names></string-name> and <string-name><surname>Haynes</surname>, <given-names>J.-D</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Searchlight-based multi-voxel pattern analysis of fMRI by cross-validated MANOVA</article-title>. <source>NeuroImage</source>, <volume>89</volume>:<fpage>345</fpage>–<lpage>357</lpage>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arbuckle</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Yokoi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pruszynski</surname>, <given-names>J. A.</given-names></string-name>, and <string-name><surname>Diedrichsen</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Stability of representational geometry across a wide range of fMRI activity levels</article-title>. <source>NeuroImage</source>, <volume>186</volume>:<fpage>155</fpage>–<lpage>163</lpage>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carlin</surname>, <given-names>J. D.</given-names></string-name> and <string-name><surname>Kriegeskorte</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Adjudicating between face-coding models with individual-face fMRI responses</article-title>. <source>PLOS Computational Biology</source>, <volume>13</volume>(<issue>7</issue>):<fpage>e1005604</fpage>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chung</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Abbott</surname>, <given-names>L. F</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Neural population geometry: An approach for understanding biological and artificial neural networks</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>70</volume>:<fpage>137</fpage>–<lpage>144</lpage>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Davis</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Poldrack</surname>, <given-names>R. A</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Quantifying the Internal Structure of Categories Using a Neural Typicality Measure</article-title>. <source>Cerebral Cortex</source>, <volume>24</volume>(<issue>7</issue>):<fpage>1720</fpage>–<lpage>1737</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Martino</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Yacoub</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Kemper</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Moerel</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Uludağ</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>De Weerd</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Ugurbil</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Goebel</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Formisano</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2018</year>). <article-title>The impact of ultra-high field MRI on cognitive and computational neuroimaging</article-title>. <source>NeuroImage</source>, <volume>168</volume>:<fpage>366</fpage>–<lpage>382</lpage>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Diedrichsen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Berlot</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Schütt</surname>, <given-names>H. H.</given-names></string-name>, <string-name><surname>Shahbazi</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Comparing representational geometries using whitened unbiased-distance-matrix similarity</article-title>. <source>arXiv</source></mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Diedrichsen</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Kriegeskorte</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Representational models: A common framework for understanding encoding, pattern-component, and representational-similarity analysis</article-title>. <source>PLOS Computational Biology</source>, <volume>13</volume>(<issue>4</issue>):<fpage>e1005508</fpage>. </mixed-citation></ref>
    <ref id="c11"><label>11.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Diedrichsen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Provost</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Zareamoghaddam</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2016</year>). <article-title>On the distribution of crossvalidated Mahalanobis distances</article-title>. <source>arXiv</source>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Diedrichsen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ridgway</surname>, <given-names>G. R.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, and <string-name><surname>Wiestler</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Comparing the similarity and spatial structure of neural representations: A pattern-component model</article-title>. <source>NeuroImage</source>, <volume>55</volume>(<issue>4</issue>):<fpage>1665</fpage>–<lpage>1678</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dubois</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>de Berker</surname>, <given-names>A. O.</given-names></string-name>, and <string-name><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Single-unit recordings in the macaque face patch system reveal limitations of fMRI MVPA</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>35</volume>(<issue>6</issue>):<fpage>2791</fpage>–<lpage>2802</lpage>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Freeman</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Brouwer</surname>, <given-names>G. J.</given-names></string-name>, <string-name><surname>Heeger</surname>, <given-names>D. J.</given-names></string-name>, and <string-name><surname>Merriam</surname>, <given-names>E. P</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Orientation Decoding Depends on Maps, Not Columns</article-title>. <source>Journal of Neuroscience</source>, <volume>31</volume>(<issue>13</issue>):<fpage>4792</fpage>–<lpage>4804</lpage>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Freeman</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Heeger</surname>, <given-names>D. J.</given-names></string-name>, and <string-name><surname>Merriam</surname>, <given-names>E. P</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Coarse-Scale Biases for Spirals and Orientation in Human Visual Cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume>(<issue>50</issue>):<fpage>19695</fpage>–<lpage>19703</lpage>. </mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ganguli</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Sompolinsky</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Compressed Sensing, Sparsity, and Dimensionality in Neuronal Information Processing and Data Analysis</article-title>. <source>Annual Review of Neuroscience</source>, <volume>35</volume>(<issue>1</issue>):<fpage>485</fpage>–<lpage>508</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150410</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haynes</surname>, <given-names>J.-D.</given-names></string-name> and <string-name><surname>Rees</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Predicting the orientation of invisible stimuli from activity in human primary visual cortex</article-title>. <source>Nature Neuroscience</source>, <volume>8</volume>(<issue>5</issue>):<fpage>686</fpage>–<lpage>691</lpage></mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name>, <string-name><surname>Bankson</surname>, <given-names>B. B.</given-names></string-name>, <string-name><surname>Harel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name>, and <string-name><surname>Cichy</surname>, <given-names>R. M</given-names></string-name></person-group>. (<year>2018</year>). <article-title>The representational dynamics of task and object processing in humans</article-title>. <source>eLife</source>, <volume>7</volume>:<elocation-id>e32816</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.32816</pub-id></mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huber</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Handwerker</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Jangraw</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Hall</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Stüber</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gonzalez-Castillo</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ivanov</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Marrett</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Guidi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Goense</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Poser</surname>, <given-names>B. A.</given-names></string-name>, and <string-name><surname>Bandettini</surname>, <given-names>P. A.</given-names></string-name></person-group> (<year>2017</year>). <article-title>High-Resolution CBV-fMRI Allows Mapping of Laminar Activity and Connectivity of Cortical Input and Output in Human M1</article-title>. <source>Neuron</source>, <volume>96</volume>(<issue>6</issue>):<fpage>1253</fpage>–<lpage>1263.</lpage> </mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname>, <given-names>W. B.</given-names></string-name>, <string-name><surname>Lindenstrauss</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Schechtman</surname>, <given-names>G</given-names></string-name></person-group>. (<year>1986</year>). <article-title>Extensions of lipschitz maps into Banach spaces</article-title>. <source>Israel Journal of Mathematics</source>, <volume>54</volume>(<issue>2</issue>):<fpage>129</fpage>–<lpage>138</lpage>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kamitani</surname>, <given-names>Y.</given-names></string-name> and <string-name><surname>Tong</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Decoding the visual and subjective contents of the human brain</article-title>. <source>Nature Neuroscience</source>, <volume>8</volume>(<issue>5</issue>):<fpage>679</fpage>–<lpage>685</lpage></mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Benna</surname>, <given-names>M. K.</given-names></string-name>, <string-name><surname>Rigotti</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Stefanini</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Fusi</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Churchland</surname>, <given-names>A. K</given-names></string-name></person-group>. (<year>2022</year>). <article-title>The implications of categorical and category-free mixed selectivity on representational geometries</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>77</volume>:<fpage>102644</fpage>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kiani</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Esteky</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Mirpour</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Tanaka</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Object Category Structure in Response Patterns of Neuronal Population in Monkey Inferior Temporal Cortex</article-title>. <source>Journal of Neurophysiology</source>, <volume>97</volume>(<issue>6</issue>):<fpage>4296</fpage>–<lpage>4309</lpage>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Kornblith</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Norouzi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Hinton</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Similarity of Neural Network Representations Revisited</article-title>. <source>arXiv</source></mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Cusack</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Bandettini</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2010</year>). <article-title>How does an fMRI voxel sample the neuronal activity pattern: Compact-kernel or complex spatiotemporal filter?</article-title> <source>NeuroImage</source>, <volume>49</volume>(<issue>3</issue>):<fpage>1965</fpage>– <lpage>1976</lpage>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> and <string-name><surname>Diedrichsen</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Inferring brain-computational mechanisms with models of activity measurements</article-title>. <source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source>, <volume>371</volume>(<issue>1705</issue>):<fpage>20160278</fpage>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> and <string-name><surname>Diedrichsen</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Peeling the Onion of Brain Representations</article-title>. <source>Annual Review of Neuroscience</source>, <volume>42</volume>(<issue>1</issue>):<fpage>407</fpage>–<lpage>432</lpage>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Formisano</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Sorger</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Goebel</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Individual faces elicit distinct response patterns in human anterior temporal cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>104</volume>(<issue>51</issue>):<fpage>20600</fpage>–<lpage>20605</lpage>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> and <string-name><surname>Kievit</surname>, <given-names>R. A</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Representational geometry: integrating cognition, computation, and the brain</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>17</volume>(<issue>8</issue>):<fpage>401</fpage>–<lpage>412</lpage>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Bandettini</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2008a</year>). <article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title>. <source>Frontiers in Systems Neuroscience</source>, <volume>2</volume>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ruff</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Kiani</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Bodurka</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Esteky</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Tanaka</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Bandettini</surname>, <given-names>P. A</given-names></string-name></person-group>. (<year>2008b</year>). <article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title>. <source>Neuron</source>, <volume>60</volume>(<issue>6</issue>):<fpage>1126</fpage>–<lpage>1141</lpage>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> and <string-name><surname>Wei</surname>, <given-names>X.-X</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Neural tuning and representational geometry</article-title>. <source>Nature Reviews. Neuroscience</source>, <volume>22</volume>(<issue>11</issue>):<fpage>703</fpage>–<lpage>718</lpage>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lally</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Lavan</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Garrido</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Tsantani</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>McGettigan</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Neural representations of naturalistic person identities while watching a feature film</article-title>. <source>Imaging Neuroscience</source>, <volume>1</volume>:<fpage>1</fpage>–<lpage>19</lpage>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Logothetis</surname>, <given-names>N. K.</given-names></string-name>, <string-name><surname>Pauls</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Augath</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Trinath</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Oeltermann</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2001</year>). <article-title>Neurophysiological investigation of the basis of the fMRI signal</article-title>. <source>Nature</source>, <volume>412</volume>(<issue>6843</issue>):<fpage>150</fpage>–<lpage>157</lpage>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Misaki</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bandettini</surname>, <given-names>P. A.</given-names></string-name>, and <string-name><surname>Kriegeskorte</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Comparison of multivariate classifiers and response normalizations for pattern-information fMRI</article-title>. <source>NeuroImage</source>, <volume>53</volume>(<issue>1</issue>):<fpage>103</fpage>–<lpage>118</lpage>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nastase</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Connolly</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>Oosterhof</surname>, <given-names>N. N.</given-names></string-name>, <string-name><surname>Halchenko</surname>, <given-names>Y. O.</given-names></string-name>, <string-name><surname>Guntupalli</surname>, <given-names>J. S.</given-names></string-name>, <string-name><given-names>Visconti</given-names> <surname>di Oleggio Castello</surname></string-name>, <string-name><given-names>M.</given-names>, <surname>Gors</surname></string-name>, <string-name><given-names>J.</given-names>, <surname>Gobbini</surname></string-name>, <string-name><given-names>M. I.</given-names>, <surname>and Haxby</surname></string-name>, <string-name><given-names>J.</given-names> <surname>V</surname></string-name></person-group>. (<year>2017</year>). <article-title>Attention Selectively Reshapes the Geometry of Distributed Semantic Representation.</article-title> <source>Cerebral Cortex (New York, N.Y.: 1991)</source>, <volume>27</volume>(<issue>8</issue>):<fpage>4277</fpage>–<lpage>4291</lpage>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nili</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Wingfield</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Walther</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Su</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Marslen-Wilson</surname>, <given-names>W.</given-names></string-name>, and <string-name><surname>Kriegeskorte</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2014</year>). <article-title>A Toolbox for Representational Similarity Analysis</article-title>. <source>PLOS Computational Biology</source>, <volume>10</volume>(<issue>4</issue>):<fpage>e1003553</fpage>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Okazawa</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Hatch</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Mancoo</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Machens</surname>, <given-names>C. K.</given-names></string-name>, and <string-name><surname>Kiani</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Representational geometry of perceptual decisions in the monkey parietal cortex</article-title>. <source>Cell</source>, <volume>184</volume>(<issue>14</issue>):<fpage>3748</fpage>–<lpage>3761.</lpage> </mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Op de Beeck</surname>, <given-names>H. P.</given-names></string-name></person-group> (<year>2010a</year>). <article-title>Against hyperacuity in brain reading: Spatial smoothing does not hurt multivariate fMRI analyses?</article-title> <source>NeuroImage</source>, <volume>49</volume>(<issue>3</issue>):<fpage>1943</fpage>–<lpage>1948</lpage>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Op de Beeck</surname>, <given-names>H. P.</given-names></string-name></person-group> (<year>2010b</year>). <article-title>Probing the mysterious underpinnings of multi-voxel fMRI analyses</article-title>. <source>NeuroImage</source>, <volume>50</volume>(<issue>2</issue>):<fpage>567</fpage>–<lpage>571</lpage>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pratte</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Sy</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Swisher</surname>, <given-names>J. D.</given-names></string-name>, and <string-name><surname>Tong</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Radial Bias Is Not Necessary For Orientation Decoding</article-title>. <source>NeuroImage</source>, <volume>127</volume>:<fpage>23</fpage>–<lpage>33</lpage>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ramírez</surname>, <given-names>F. M.</given-names></string-name>, <string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Allefeld</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Haynes</surname>, <given-names>J.-D.</given-names></string-name></person-group> (<year>2014</year>). <article-title>The neural code for face orientation in the human fusiform face area</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>34</volume>(<issue>36</issue>):<fpage>12155</fpage>–<lpage>12167</lpage>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Schütt</surname>, <given-names>H. H.</given-names></string-name>, <string-name><surname>Kipnis</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Diedrichsen</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Statistical inference on representational geometries</article-title>. <source>arXiv</source></mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stringer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Harris</surname>, <given-names>K. D</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Highdimensional geometry of population responses in visual cortex</article-title>. <source>Nature</source>, <volume>571</volume>(<issue>7765</issue>):<fpage>361</fpage>–<lpage>365</lpage>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Walther</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Nili</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ejaz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Alink</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, and <string-name><surname>Diedrichsen</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Reliability of dissimilarity measures for multi-voxel pattern analysis</article-title>. <source>NeuroImage</source>, <volume>137</volume>:<fpage>188</fpage>–<lpage>200</lpage>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weber</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Thompson-Schill</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Osherson</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Haxby</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Parsons</surname>, <given-names>L</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Predicting judged similarity of natural categories from their neural representations</article-title>. <source>Neuropsychologia</source>, <volume>47</volume>(<issue>3</issue>):<fpage>859</fpage>–<lpage>868</lpage>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yacoub</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Harel</surname>, <given-names>N.</given-names></string-name>, and <string-name><surname>Ugurbil</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2008</year>). <article-title>High-field fMRI unveils orientation columns in humans</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>105</volume>(<issue>30</issue>):<fpage>10607</fpage>–<lpage>10612</lpage>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yacoub</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Shmuel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Logothetis</surname>, <given-names>N.</given-names></string-name>, and <string-name><surname>Ŭgurbil</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Robust detection of ocular dominance columns in humans using Hahn Spin Echo BOLD functional MRI at 7 Tesla</article-title>. <source>NeuroImage</source>, <volume>37</volume>(<issue>4</issue>):<fpage>1161</fpage>–<lpage>1177</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106812.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kahnt</surname>
<given-names>Thorsten</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>National Institute on Drug Abuse Intramural Research Program</institution>
</institution-wrap>
<city>Baltimore</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This manuscript makes <bold>important</bold> contributions to the methodology commonly used to assess representational structures in human and animal brain activity recorded using various techniques (especially fMRI). The evidence in the form of mathematical analysis and simulations is <bold>solid</bold>. The impact of this contribution could be improved by extending the simulations to assess the effects of violations of explicit and implicit assumptions.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106812.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This work presents a formalism for the relationship between neural signals and pooled signals (e.g., voxel estimates in fMRI) and explores why correlation-based and mean-removed Euclidean RDMs perform well in practice. The key assumption is that the pooled estimates are weighted averages, with i.i.d. non-negative weights. Two sets of simulations are used to support the theoretical findings: one based on fully simulated neural data and another that reverse-engineers neural data from an RDM estimated from real macaque data. The authors also discuss limitations of their simulations, particularly concerning the i.i.d. assumption of the weights.</p>
<p>Strengths:</p>
<p>The strengths of this work include its mathematical rigor and the clear connection that is drawn between the derivations and empirical observations. The simulations were well-designed and easy to follow. One small suggestion: a brief explanation of what is meant by &quot;sparse&quot; in Figure 3 would help orient the reader without requiring them to jump ahead to the methods. Overall, I found the work engaging and insightful.</p>
<p>Weaknesses:</p>
<p>Although I appreciate the effort to explore *why* certain dissimilarity measures perform well, it wasn't clear how these findings would inform the practical choices of researchers conducting RDM-based analyses. Many researchers likely already use correlation-based or mean-removed Euclidean distance measures, given their popularity. In that case, how do these results provide additional value or guidance beyond current practice?</p>
<p>Another aspect that could benefit from further clarification is the core assumption underlying the work - that channel-based activity reflects a non-negative weighted average of neural activity. Is this widely accepted as the most plausible model, or are there alternative relationships that researchers should consider? While this may seem intuitive, it's not something I would expect all readers to be familiar with, and only a single reference was provided to support it (which I unfortunately didn't have time to read). That said, I did appreciate the discussion of the i.i.d. assumption in the discussion section. Can more be said to educate researchers as to when the i.i.d. assumption might be violated?</p>
<p>I didn't find the &quot;Simulations based on neural data&quot; section added much, and it risks being misinterpreted. The main difference here is that neural data were reverse-engineered from a macaque RDM and then used in simulations similar to those in the previous section. What is the added value of using a real RDM to generate simulated data? Were the earlier simulations lacking in some way? There's also a risk of readers mistakenly inferring that human dissimilarities have been reconstructed from macaque data, an assumption that goes beyond the paper's core message, which focuses on linking neural and channel-based signals from the *same* source. If this section is retained, the motivation should be clarified, and the implied parallel in Figure 6, between the human data and simulated data, should be reconsidered.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106812.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The paper is a methodological contribution to multivariate pattern analysis and, in particular, the analysis of representational geometry via pairwise representational distances, sometimes called representational dissimilarity analysis (RDA). The authors investigate through theoretical analysis and simulations how true representational distances (defined on the neural level) give rise to representational distances estimated from neurophysiological data, including fMRI and cell recordings. They demonstrate that, due to the way measurements sample neural activity, the activity common to all sampled neurons can be amplified in the representational geometry derived from these measurements, and therefore, an empirical representational geometry may deviate substantially from the true representational geometry. The authors propose to modify the obtained representational structure by removing the dimension corresponding to that common activity, and argue that such a removal of a single dimension does not relevantly affect the representational structure, again underpinned by mathematical analysis and simulation.</p>
<p>Importance:</p>
<p>The paper may at first sight be tackling a specific problem within a specific subfield of cognitive neuroscience methods. However, understanding the structure of representations is a fundamental goal of cognitive psychology and cognitive neuroscience, and the fact that methods of representational geometry are not yet routinely used by the wider community may at least partially be due to uncertainty regarding the reliability of these methods. This paper is an important step towards clarifying and improving reliability, and therefore towards more widespread adoption of representational geometry methods.</p>
<p>Strengths:</p>
<p>The paper makes its argument generally well, relying on previous work by the authors as well as others to support assumptions about neural sampling by neurophysiological measurements. Their main points are underpinned by both detailed mathematical analysis and simulations, and the latter also produces intuitively accessible illustrations of the authors' argument. The authors discuss in detail under which exact circumstances common neural activity distorts the representational geometry, and therefore, when exactly the removal of the common dimension is necessary to minimize that distortion.</p>
<p>Weaknesses:</p>
<p>(1) The argument around the Johnson-Lindenstrauss lemma on pages 5 &amp; 6 is somewhat confused, and also not really convincing.</p>
<p>First, the correct reference for the lemma seems to be not [20] = Johnson et al. (1986), but Johnson &amp; Lindenstrauss (1984). Moreover, as far as I can tell, Johnson et al. (1986) do not discuss random projections, and while they play a role in Johnson &amp; Lindenstrauss (1984), that is only as a proof device. The paper text suggests that the lemma itself is probabilistic, while actually it is a statement of existence.</p>
<p>Second, the authors correctly state that the lemma implies that &quot;the number of measurement channels required for a good approximation does not depend on the number of neurons and grows only logarithmically with the number of stimuli&quot;, but it is not clear what the relevance of this statement for this paper is, considering that distances between N points can be exactly preserved within an N − 1 dimensional subspace, irrespective of the number of dimensions of the original space, and since in cognitive neuroscience the number of measurement channels is usually (much) larger than the number of experimental stimuli.</p>
<p>The actually centrally important statement is not the Johnson-Lindenstrauss lemma, but one about the metric-preserving properties of random projections with zero-mean weights. It is this statement that needs to be backed up by the correct references, which, as far as I can tell, are neither the cited Johnson et al. (1986) nor even Johnson &amp; Lindenstrauss (1984) for the lemma.</p>
<p>(2) The detailed mathematical analyses and simulations focus on the effect of non-zero-mean sampling weights, and that is justified by the result that such sampling leads to a distorted representational geometry. However, there is another assumption which seems to be used almost everywhere in both mathematical analyses and simulations, and which I suspect may have a relevant effect on the observed representational geometry: statistical independence between weights. In particular, in fMRI, the existence of a naturally limited spatial resolution (due to MRI technology or vasculature) makes it unlikely that the weights with which a given neuron affects different voxels are independent.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106812.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This manuscript investigates the conditions under which representational distances estimated from brain-activity measurements accurately mirror the true geometry of the underlying neural representations. Using a theoretical framework and simulations, the authors show that (i) random weighted sampling of individual neurons preserves representational distances; (ii) the non-negative pooling characteristic of fMRI stretches the geometry along the population-mean dimension; and (iii) subtracting the across-channel mean from each activity pattern removes this distortion, explaining the well-known success of correlation-based RSA. They further argue that a mean-centred, squared Euclidean (or Mahalanobis) distance retains this corrective benefit while avoiding some pitfalls of variance normalisation.</p>
<p>Strengths:</p>
<p>(1) Theoretical clarity and novelty:</p>
<p>
The paper offers an elegant and convincing proof of how linear measurement models affect representational geometry and pinpoints the specific condition (non-zero-mean sampling weights) under which voxel pooling introduces a systematic bias. This quantitative explanation of why mean removal is effective in RSA is new and valuable.</p>
<p>(2) Simulations:</p>
<p>
Experiments on both synthetic high-dimensional fMRI data and macaque-IT-inspired embeddings corroborate the mathematics, providing practical insights into the theoretical reasoning outlined by the authors.</p>
<p>(3) Actionable recommendations:</p>
<p>
The work summarises the results into clear guidelines: random single-unit sampling is &quot;safe&quot; (the estimated geometry is undistorted); fMRI voxel data with unstructured or single-scale codes should be mean-centred; and multi-scale cortical maps require explicit forward modelling. These guidelines are clear, and useful for future research.</p>
<p>Weaknesses:</p>
<p>(1) Simplistic assumptions:</p>
<p>
The assumption that measurement-channel weights are drawn independently and identically distributed (i.i.d.) from a univariate distribution is a significant idealisation for fMRI data. Voxels have spatially structured responses (and noise), meaning they do not sample neurons with i.i.d. weights. The extent to which the conclusions (especially the &quot;exact recovery&quot; with mean centring) hold when this assumption is violated needs more discussion. While the paper states that the non-negative IWLCS model is a best-case scenario, the implications of deviations from this best case could be elaborated.</p>
<p>(2) Random-subpopulation model for electrophysiology:</p>
<p>
Similarly, the &quot;random subpopulation model&quot; is presented as an idealisation of single-cell recordings. In reality, electrophysiological sampling is often biased (e.g., towards larger, more active neurons or neurons in accessible locations). The paper acknowledges biased sampling as a challenge that requires separate modelling, but the gap between this idealised model and actual practice should be highlighted more strongly when interpreting the optimistic results.</p>
<p>(3) Noise as an &quot;orthogonal issue&quot;:</p>
<p>
The theoretical derivations largely ignore measurement noise, treating it as an orthogonal problem solvable by cross-validation. Although bias from noise is a well-known problem, interactions between noise and sampling-induced distortions (especially the down-scaling of orthogonal dimensions) could complicate the picture. For instance, if a dimension is already heavily down-scaled by averaging, it might become more susceptible to being obscured by noise. Addressing or highlighting these points more explicitly would make the limitations of this theoretical framework more transparent.</p>
<p>(4) Simulation parameters and generalizability:</p>
<p>
The random ground-truth geometries were generated from a Gaussian mixture in 5-D and then embedded into 1,024-D, with ≈25 % of the variance coming from the mean dimension. The sensitivity of the findings to these specific parameters (initial dimensionality, geometry complexity, proportion of mean variance, and sample size) could be discussed. How would the results change if the true neural geometry had a much higher or lower intrinsic dimensionality, or if the population-mean component were substantially smaller or larger? If the authors' claims are to generalise, more scenarios should be considered.</p>
<p>(5) Mean addition to the neural-data simulation:</p>
<p>
In simulations based on neural data from Kiani et al., a random mean was added to each pattern to introduce variation along the mean dimension. This was necessary because the original patterns had identical mean activation. However, the procedure might oversimplify how population means vary naturally and could influence the conclusions, particularly regarding the impact of the population-mean dimension. While precisely modelling how the mean varies across conditions is beyond the manuscript's scope, this point should be stated and discussed more clearly.</p>
<p>(6) Effect of mean removal on representational geometry:</p>
<p>
As noted, the benefits of mean removal hold &quot;under ideal conditions&quot;. Real data often violates these assumptions. A critical reader might ask: What if conditions differ in overall activation and in more complex ways (e.g., differing correlation structures across neurons)? Is it always desirable to remove population-mean differences? For example, if a stimulus truly causes a global increase in firing across the entire population (perhaps reflecting arousal or salience), subtracting the mean would treat this genuine effect as a nuisance and eliminate it from the geometry. Prior literature has cautioned that one should interpret RSA results after demeaning carefully. For instance, Ramírez (2017) dubbed this problem &quot;representational confusion&quot;, showing that subtracting the mean pattern can change the relationships between conditions in non-intuitive ways. These potential issues and previous results should be discussed and properly referenced by the authors.</p>
<p>Appraisal, Impact, and Utility:</p>
<p>The authors set out to identify principled conditions under which measured representational distances faithfully reflect the underlying neural geometry and to provide practical guidance for RSA across modalities. Overall, I believe they achieved their goals. Theoretical derivations identify the bias-inducing factors in linear measurement models, and the simulations verify the analytic claims, demonstrating that mean-pattern subtraction can indeed correct some mean-related geometric distortions. These conclusions strongly rely on idealised assumptions (e.g., i.i.d. sampling weights and negligible noise), but the manuscript is explicit about them, and the reasoning from evidence to claim is sound. A deeper exploration of how robust each conclusion is to violations of these assumptions, particularly correlated voxel weights and realistic noise, would make the argument even stronger.</p>
<p>Beyond their immediate aims, the authors offer contributions likely to shape future work. Its influence is likely to influence both analysis decisions and the design of future studies exploring the geometry of brain representations. By clarifying why correlation-based RSA seems to work so robustly, they help demystify a practice that has so far been adopted heuristically. Their proposal to adopt mean-centred Euclidean or Mahalanobis distances promises a straightforward alternative that better aligns representational geometry with decoding-based interpretations.</p>
<p>In sum, I see this manuscript as a significant and insightful contribution to the field. The theoretical work clarifying the impact of sampling schemes and the role of mean removal is highly valuable. However, the identified concerns, primarily regarding the idealized nature of the models (especially for fMRI), the treatment of noise, and the need for more nuanced claims, suggest that some revisions are necessary. Addressing these points would substantially strengthen the paper's conclusions and enhance its impact on the neuroscience community by ensuring the proposed methods are robustly understood and appropriately applied in real-world research settings.</p>
</body>
</sub-article>
</article>