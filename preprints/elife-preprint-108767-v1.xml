<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108767</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108767</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108767.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories><title-group>
<article-title>Where is the melody? Spontaneous attention orchestrates melody formation during polyphonic music listening</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0002-9221-9802</contrib-id>
<name>
<surname>Winchester</surname>
<given-names>Martin M</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Reynolds</surname>
<given-names>Kevin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Nebo</surname>
<given-names>Charbel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Scott</surname>
<given-names>Ian Cecil</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Di Liberto</surname>
<given-names>Giovanni M</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
    <email>gdiliber@tcd.ie</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>School of Computer Science and Statistics, ADAPT Centre, Trinity College Dublin</institution></institution-wrap>, <city>Dublin</city>, <country country="IE">Ireland</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>Trinity College Institute of Neuroscience, Trinity College Dublin</institution></institution-wrap>, <city>Dublin</city>, <country country="IE">Ireland</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03mxktp47</institution-id><institution>Applied Mathematics and Computational Biology, IBENS, Ecole Normale Superieure, PSL University</institution></institution-wrap>, <city>Paris</city>, <country country="FR">France</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04t0qbt32</institution-id><institution>Conservatoire, Technological University Dublin</institution></institution-wrap>, <city>Dublin</city>, <country country="IE">Ireland</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Peelle</surname>
<given-names>Jonathan Erik</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
    <institution-id institution-id-type="ror">https://ror.org/04t5xt781</institution-id><institution>Northeastern University</institution>
</institution-wrap>
<city>Boston</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Luo</surname>
<given-names>Huan</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n2" fn-type="con"><p>Author contributions: Conceptualisation, funding acquisition, supervision, and project administration: GDL. Experimental design, pilot data collection and preliminary analyses: KR, CN, and GDL. Data collection and data curation: MW. Methodology, formal data analysis, and visualisation: MW and GDL. Supervision and project administration: GDL. Validation of music stimuli and musicology implications: ICS. Writing of first draft: GDL and MW. Editing &amp; Review: All authors.</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-10-29">
<day>29</day>
<month>10</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP108767</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-08-25">
<day>25</day>
<month>08</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-08-27">
<day>27</day>
<month>08</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.08.26.672294"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Winchester et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Winchester et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108767-v1.pdf"/>
<abstract>
<p>Humans seamlessly process multi-voice music into a coherent perceptual whole. Yet the neural strategies supporting this experience remain unclear. One fundamental component of this process is the formation of melody, a core structural element of music. Previous work on monophonic listening has provided strong evidence for the neurophysiological basis of melody processing, for example indicating predictive processing as a foundational mechanism underlying melody encoding. However, considerable uncertainty remains about how melodies are formed during polyphonic music listening, as existing theories (e.g., divided attention, figure–ground model, stream integration) fail to unify the full range of empirical findings. Here, we combined behavioral measures with non-invasive electroencephalography (EEG) to probe spontaneous attentional bias and melodic expectation while participants listened to two-voice classical excerpts. Our uninstructed listening paradigm eliminated a major experimental constraint, creating a more ecologically valid setting. We found that attention bias was significantly influenced by both the high-voice superiority effect and intrinsic melodic statistics. We then employed transformer-based models to generate next-note expectation profiles and test competing theories of polyphonic perception. Drawing on our findings, we propose a weighted-integration framework in which attentional bias dynamically calibrates the degree of integration of the competing streams. In doing so, the proposed framework reconciles previous divergent accounts by showing that, even under free-listening conditions, melodies emerge through an attention-guided statistical integration mechanism.</p>
</abstract>
<abstract abstract-type="summary">
<title>Highlights</title>
<list list-type="order">
<list-item><p>EEG can be used to track spontaneous attention during the uninstructed listening of polyphonic music.</p></list-item><list-item><p>Behavioural and neural data indicate that spontaneous attention is influenced by both high-voice superiority and melodic contour.</p></list-item><list-item><p>Attention bias impacts the neural encoding of the polyphonic streams, with strongest effects within 200 ms after note onset.</p></list-item><list-item><p>Strong attention bias leads to melodic expectations consistent with a monophonic music transformer, in line with a Figure-ground model. Weak attention bias leads to melodic expectations consistent with a Stream Integration model.</p></list-item><list-item><p>We propose a bi-directional influence between attention and prediction mechanisms, with horizontal statistics impacting attention (i.e., salience), and attention impacting melody extraction.</p></list-item>
</list>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Music from many cultures and traditions, including Western tonal music, typically leads to the perception of melody, which is a monophonic sequence of notes that a listener would sing, whistle, or hum back to identify the music (<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>). While certain types of music are monophonic by nature (e.g., Sean-nós singing, nursery rhymes), leaving little doubt as to what the melody is, music typically involves multiple sound streams, or voices. A variety of factors, some acoustic (e.g., the particular instrument used, pitch distance) and others musical (e.g., meaningful melodic progressions), have been identified as key contributors to the perception of distinct sound streams in music. However, there remains considerable uncertainty on how those streams are processed by our brain into a coherent whole; and while melody has been proposed as a key structural element of music (<xref ref-type="bibr" rid="c3">3</xref>), the neural strategies underpinning its emergence from multi-stream compositions remains unclear.</p>
<p>While many theories have been proposed, one point of agreement is that attention mechanisms have a main role in polyphonic music perception. Much of the debate has centred around whether our brains are capable of dividing auditory attention across multiple auditory streams. In contrast with the widely studied multi-talker speech scenario, where attention can only be directed to one speaker or conversation at a time (<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c5">5</xref>), music is built so that distinct streams have strong relationships (e.g., harmonic) that can make them be perceived as coherent elements of a unified percept. Theories of polyphonic music perception have been formulated, for example explaining this phenomenon as the result of a divided attention between the streams (<xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c7">7</xref>). A second view proposes that the attentional focus is directed to a single “foreground” stream, while also processing its harmonic relationship with the “background” (figure-ground model (<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>)). As music unfolds, listeners may switch their attention to different streams, changing what is regarded as foreground and background. A third view proposes that our brains merge multiple music streams into a single complex melody, which then becomes the focus of attention (integration model (<xref ref-type="bibr" rid="c9">9</xref>)).</p>
<p>Tailored experiments have been used to test the validity of these models, often involving tasks such as error detection in which participants are asked to click a button when they hear anomalies in pitch, rhythm, or harmony (<xref ref-type="bibr" rid="c10">10</xref>). That work led to a wealth of knowledge on what our brains “can” do. However, these tailored tasks come at a cost, such as imposing tasks leading to neural processing strategies different from uninstructed listening. For example, error detection tasks may include the explicit instruction of focussing on one or all streams, leading to processing strategies (e.g., selective attention behaviour, rapid attention switching) that might or might not be employed during uninstructed listening. As such, while that work can certainly inform us on what our brains are capable of doing, it is less clear if that reflects the typical neural functioning during uninstructed listening.</p>
<p>Here, we investigate how the human brain processes two-stream polyphonic music during uninstructed listening by combining a behavioural task and non-invasive electroencephalography measurements (EEG). Participants were presented with classical music pieces synthesised with a high-quality virtual piano instrument, where each of the two voices consisted of a meaningful melody that would stand on its own (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). In that condition, the listener’s attention was expected to primarily focus on the high pitch stream due to:</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Experimental design.</title>
<p><bold>(A)</bold> Participants were presented with polyphonic music made of two monophonic streams (PolyOrig condition), with control stimuli where the average pitch heights for the two streams were inverted (PolyInv), and with the corresponding monophonic streams in isolation (Monophonic). All stimuli were synthesised from MIDI scores using high quality virtual piano sounds. <bold>(B)</bold> Each experimental session was organised into two blocks: a listening block, where EEG signals were recorded from participants during uninstructed music listening, followed by a music identification block. The music pieces were selected randomly from any condition, an a given piece was presented once, only in one of the three conditions, without repetitions. In the melody identification block, participants heard short polyphonic snippet from the same pieces, and were asked to sing back the melody and to indicate if that was the high or low pitch stream on a five-point scale. This led to a behavioural measurement of attention bias. <bold>(C)</bold> Attention bias was also measured from the neural signal. Attention decoding models were built on the monophonic condition, by fitting backward temporal response functions (TRF) on each participant to reconstruct the sound envelope from the EEG signal. TRF models were then applied to the polyphonic conditions to decode the attended melody, resulting in a neural measurement of attention bias.</p></caption>
<graphic xlink:href="672294v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<list list-type="bullet">
<list-item><p>the <underline>high-voice superiority effect</underline> (i.e., a bias towards the high-pitch stream; (<xref ref-type="bibr" rid="c11">11</xref>));</p></list-item>
<list-item><p>and <underline>the melody itself</underline>, as the more attractive statistics are often placed at the higher pitch stream (<xref ref-type="bibr" rid="c11">11</xref>-<xref ref-type="bibr" rid="c14">14</xref>).</p></list-item>
</list>
<p>To disentangle these two factors, alongside this main experimental condition involving the original polyphonic music (PolyOrig), our experiment included a control polyphonic condition where the average pitch-heights of the two-streams were inverted (PolyInv). Finally, we also included a monophonic condition including a random selection of the monophonic voices in both polyphonic conditions (Monophonic), which was used for training the models for the attention decoding analysis.</p>
<sec id="s1a">
<title>Where is the attention?</title>
<p>First, we decode the attention bias in the listeners’ brains in the polyphonic listening conditions. In doing so, we answer the question: Where is the attended melody? To that end, we derived two attention bias scores. The first score is behavioural, and it reflects which of the two sound streams the listener sung back during a listen and repeat task in a dedicated block (behavioural attention bias score; <xref rid="fig1" ref-type="fig">Figure 1</xref>). The second score was derived from the EEG signal through an attention decoding procedure (neural attention bias score). Recent methodological developments provide us with tools to carry out that decoding from EEG signals based on regression methods (<xref ref-type="bibr" rid="c15">15</xref>). Specifically, we fit attention decoders on the EEG data recorded during the monophonic condition, which contained melodies selected from high and low pitch streams, and main and support streams. That way, the resulting decoders were unbiased with regard to pitch height and voice statistics, and were then applied to decode attention on the polyphonic conditions.</p>
<p>We hypothesised both music streams to be robustly encoded in the human cortex during polyphonic music listening, primarily reflecting the cortical encoding of low-level acoustic properties (e.g., sound envelope). With regard to auditory attention, the high-voice superiority effect and melody statistics are the two possible contributors to salience – timbre was not a factor in this experiment, as the sound stimuli were synthesised using MIDI sounds from a single instrument (see <bold>Methods</bold>). In the PolyOrig condition, both contributors were expected to primarily steer attention toward the high pitch stream by construction. Avoiding generalisations, the melodic material that characterises the main motif or primary thematic element, which is distinguished by its more intriguing and expressive features, is often constructed in the more exposed higher registers. For good practice, the main motif is supported by a harmonic and rhythmic structure constructed in order to enhance the attention on the main thematic material.</p>
<p>We designed the PolyInv condition to disentangle the two key contributors to saliency – high-voice superiority and melody statistics – by placing the main motif in the low-pitch voice. The pieces in this experiment, primarily double-counterpoint compositions, were selected specifically as the inversion of voices does not compromise their structural integrity. For example, this technique is a well-established practice in fugues. The bass (low-pitch) line, even if less ornamented than the main (high-pitch) melody, provides a fundamental harmonic structure. This harmonic foundation ensures that when the lines are inverted, the resulting music retains coherence and stability, even in the absence of a third voice to fully define the harmonic context. With this premise, three hypotheses were explored for the PolyInv condition: Hp0) High-voice superiority is the main contributor to attention bias, which would lead to a high-pitch bias; Hp1) The two contributors contrast each other, leading to small (or no) attentional bias; and Hp2) Melody statistics is the main contributor, meaning that PolyInv would lead to an inverted attention bias compared with PolyOrig, as the pitch lines for the two voices were swapped (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Behavioural and neural indices of attention bias during uninstructed listening of polyphonic music.</title>
<p><bold>(A)</bold> Illustration of the hypothesised attention bias scenarios for polyphonic music listening. A high-pitch bias was expected for PolyOrig by design. Dominant high-voice superiority effect or motif attractiveness were expected to lead to a strong attention bias toward the high (Hp0) and low (Hp2) pitch streams respectively, while a substantial reduction in attention bias would reflect a comparable contribution of the two factors (Hp1). <bold>(B)</bold> Behavioural result. The behavioural attention bias metric (mean ± SEM; ***p&lt;0.001) was derived from the subjective reporting in the melody identification block. Subjective ratings indicated what stream was perceived as the main melody, from value -2 (low pitch) to value 2 (high pitch). <bold>(C)</bold> EEG decoding analysis. (Left) Envelope reconstruction correlations for the decoding analysis are reported (mean ± SEM; *p&lt;0.05, ***p&lt;0.001) for individual streams (high and low pitch) and conditions (PolyOrig and PolyInv). Colours refer to the motif (red: main melody; blue: support stream). Note that colours are inverted in the two conditions, reflecting the pitch inversion. (Right) Envelope reconstruction correlations for individual participants. <bold>(D)</bold> Neural attention bias index, which was obtained by subtracting the high and low pitch reconstruction correlations in (C) within each condition (Δenvelope reconstruction; mean ± SEM; **p&lt;0.01). <bold>(E)</bold> Forward TRF model weights at channel Cz, providing insights into the temporal dynamics of the neural response to the two streams. Lines and shaded areas indicate the mean and SEM over participants respectively. Thick black lines indicate time points with statistically significantly difference across conditions.</p></caption>
<graphic xlink:href="672294v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s1b">
<title>What is the melody?</title>
<p>After having established the attention bias of each participant and music piece, we present a second analysis to determine how the perceived melody is built by our brains. To that end, we rely on the known sensitivity of EEG signals to melodic expectations (<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c16">16</xref>-<xref ref-type="bibr" rid="c19">19</xref>). A music note may be more or less expected based on its prior context. Monophonic music listening leaves little doubt as to what that local context is, and models have been built that estimate next-note expectations, for example based on variable-order Markov Chains (<xref ref-type="bibr" rid="c20">20</xref>) and deep-learning architectures (<xref ref-type="bibr" rid="c21">21</xref>-<xref ref-type="bibr" rid="c23">23</xref>). Here, we use state-of-the-art transformer-based computational models of music to generate numerical hypotheses for our brains’ next-note expectations, matching different theories of polyphonic music perception. We then relate these simulations with neural signals recorded during the polyphonic conditions, determining which music processing strategy is most neurophysiologically plausible for uninstructed listening, and how attention bias relates with the selected strategy.</p>
<p>We considered a divided attention (<xref ref-type="bibr" rid="c6">6</xref>) and an integration model (<xref ref-type="bibr" rid="c9">9</xref>). The divided attention model would predict distinct melodic surprise for the two polyphonic streams, with equal strength. Measuring different strengths, in this two-stream listening scenario, would instead be compatible with a figure-ground model (<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c9">9</xref>), where the listening focus is directed to one particular stream (horizontal melody), which may alternate as the music unfolds, while the other is processed in function of the attended stream (vertical harmony). The other possibility that we considered was the integration model, according to which next-note expectations would be based on a single melody combining the two streams. Stream integration is in line with previous findings (<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c24">24</xref>) and compatible with the intuition that our vocal tract can only produce monophonic sequences. An auditory-motor neural pathway, condensing the auditory input into monophonic vocal motor commands, would be in line with the definition of melody itself (i.e., a monophonic sequence that a listener would hum back after hearing a music piece). Note that integration is not in contrast with the processing of other properties, such as vertical harmony, for example via a distinct neural pathway.</p>
<p>With that premise, our analysis aims to arbitrate among those three models. Measuring differences in the melody expectation encoding for the two streams in PolyOrig would contrast with the divided attention model. With regard to PolyInv, we had distinct hypotheses for the other two models. Specifically, the Figure-ground model PolyInv to impair the integration negatively, if anything, due to the alterations in vertical harmony produced by the pitch inversion. An integration model, instead, would be in line with an increased integration in PolyInv, where the attention bias was expected to be in-between the two streams (Hp1 in <xref rid="fig2" ref-type="fig">Fig. 2A</xref>).</p>
</sec>
</sec>
<sec id="s2">
<title>Results</title>
<p>Neural signals were recorded with EEG from 31 participants (16 males) during the uninstructed listening of polyphonic compositions or their monophonic components (<italic>listening block;</italic> <xref rid="fig1" ref-type="fig">Fig. 1A</xref>). The <italic>listening block</italic> involved the listening of monophonic and polyphonic pieces. In the second part of the experiment, participants undertook a melody identification task in a dedicated block, where short polyphonic music segments (∼4-8 seconds) were presented to the participants, who were asked to sing back the melody, and to indicate if they sung the high or low pitch stream on a five-point scale (see <bold>Methods</bold>). The analyses that follow integrate behavioral (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>) and neural data (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>) to address the following key questions: (a) Where was the focus of attention, and what factors influenced it? (b) What melody is encoded in the human brain during uninstructed listening to polyphonic music?</p>
<sec id="s2a">
<title>Where is the attention? Investigating attention bias and its contributors using behavioural and neural measurements</title>
<p>In PolyOrig, the listeners’ attention was expected to focus on the high pitch stream by construction. PolyInv was built to disentangle the high-pitch superiority effect and the motif attractiveness. Measuring a strong attention bias toward the high-pitch stream in PolyInv would reflect a dominance of the high-pitch superiority effect (Hp0). Conversely, an attention bias toward the low-pitch stream in PolyInv would indicate a dominance of the motif attractiveness (Hp2). Our expectation was that both factors would contribute to the attention bias after the pitch inversion, thus leading to a reduced attention bias compared with PolyOrig (Hp1; <xref rid="fig2" ref-type="fig">Fig. 2A</xref>).</p>
<p>Behavioural measurements of attention bias confirm the high-pitch attention bias in the PolyOrig condition (<italic>t</italic>-test, <italic>p</italic> = 3.5*10<sup>-9</sup>, <italic>d</italic> = 1.88). The PolyInv condition also showed a high-pitch bias (<italic>t</italic>-test, <italic>p</italic> = 6.6*10<sup>-4</sup>, <italic>d</italic> = 0.80). Crucially, while the attention bias remained toward the high-pitch voice, its magnitude decreased with the voice inversion (paired <italic>t</italic>-test: <italic>p</italic> = 1.2*10<sup>-4</sup>, <italic>d</italic> = 0.95; <xref rid="fig2" ref-type="fig">Fig. 2B</xref>), in line with Hp1.</p>
<p>Next, we tested if the neural data also pointed to the same hypothesis. Neural measurements of attention bias were derived via an attention decoding analysis. Attention decoders were fit on the low-frequency EEG data (1-8 Hz) from the monophonic condition using lagged ridge regression, with a methodology referred to as backward temporal response function (TRF; (<xref ref-type="bibr" rid="c25">25</xref>-<xref ref-type="bibr" rid="c27">27</xref>)). The model fit identifies a linear combination of all EEG channels that produces an optimal reconstruction of the sound envelope. Since the monophonic listening condition only involved one music stream, participants could only attend to that stream, meaning that the resulting TRF model serves as an attention decoder. Sound envelope reconstructions were derived using that model on the EEG data from the polyphonic conditions. Pearson’s correlations were calculated between the reconstructed signal and the envelope of both polyphonic streams. The neural attention bias index was derived as the difference between those correlations (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>).</p>
<p>Envelope reconstruction correlations showed a statistically significant attention bias (two-way ANOVA; main effect of stream: <italic>F</italic>(1,30) = 36.9, <italic>p</italic> = 1.1*10<sup>-6</sup>; <xref rid="fig2" ref-type="fig">Fig. 2C</xref>), no main effect of condition (PolyOrig vs. PolyInv: <italic>F</italic>(1,30) = 1.1, <italic>p</italic> = 0.296) and a statistically significant stream x condition interaction (<italic>F</italic>(1,30) = 8.8, <italic>p</italic> = 0.006; <xref rid="fig2" ref-type="fig">Fig. 2D</xref>). Post hoc tests confirmed, in line with the behavioural results and Hp1, a neural attention bias toward the high pitch stream in both PolyOrig (paired <italic>t</italic>-test: <italic>p</italic> = 2.4*10<sup>-6</sup>, <italic>d</italic> = 1.04) and PolyInv (<italic>p</italic> = 0.025, <italic>d</italic> = 0.42; <xref rid="fig2" ref-type="fig">Fig. 2C</xref>), and that the voice inversion led to a lower attention bias in PolyInv (paired <italic>t</italic>-test: <italic>p</italic> = 0.006, <italic>d</italic> = 0.53).</p>
<p>Further analyses were run to determine the impact of attention on the temporal unfolding of music stream encoding. Multivariate forward envelope TRFs were fit to build an optimal linear mapping from the envelope of the two input streams to the corresponding low-frequency EEG recording. Statistically significant differences were measured at the representative EEG channel Cz between the TRF weights for the two conditions (FDR corrected paired <italic>t</italic>-tests, <italic>p</italic>&lt;0.05; <xref rid="fig2" ref-type="fig">Fig. 2E</xref>) in PolyOrig, with the effects emerging for all TRF components within the first 400ms after stimulus onset. Only a very short temporal cluster of significance emerged in PolyInv instead (∼80ms). Results were comparable at neighbouring channels such as Fz, while weaker effects emerged in more occipital scalp areas, such as Pz (not shown).</p>
</sec>
<sec id="s2b">
<title>What is the melody? Investigating melody formation by probing melodic prediction mechanisms in the listener’s brain</title>
<p>When asked to sing back a polyphonic piece, listeners produce a melody that is either one of the streams (segregation) or a mixture of the two (integration). The analysis that follows aimed at determining what the listeners’ brain regards as the melody. To that end, we relied on measurements of melodic expectations, which were previously validated in the context of monophonic music listening (<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c29">29</xref>). Three models in the literature would predict different outcomes for this analysis: (a) the divided attention model predicts a simultaneous encoding of melodic expectations calculated on two streams as independent monophonic streams; (b) the figure-ground model would predict the encoding of melodic expectations for the main motif; and (c) the integration model would correspond to the encoding of melodic expectations for a melody that combines the two streams. Here, estimates of melodic expectations (surprise and entropy) for each melody stream were derived from an Anticipatory Music Transformer (<xref ref-type="bibr" rid="c23">23</xref>).</p>
<p>Forward TRFs were fit to predict the low-frequency EEG signals (1-8 Hz) based on a multivariate feature set including acoustic features (<bold>A</bold>: note onset, envelope, envelope derivative) and melodic expectation features (<bold>M</bold>: note onsets amplitude-modulated by pitch surprise and note onsets amplitude-modulated by pitch entropy for all streams in the stimulus). Melodic expectation encoding was quantified as the gain in variance explained by including M in the model (i.e., <italic>r</italic><sub><italic>AM</italic></sub> - <italic>r</italic><sub><italic>A</italic></sub>).</p>
<p>As a first step, we obtained a neural measurement of melodic expectation encoding on the monophonic condition, aiming to replicate previous findings in the literature (<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c30">30</xref>). The mapping between stimulus features and EEG is multivariate-to-multivariate, making methods such as the TRF (which is multivariate-to-univariate) suboptimal. Here we used the more appropriate Canonical Correlation Analysis (CCA) methodology instead, using match-vs-mismatch classification metrics for the evaluation (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>; (<xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c32">32</xref>)), where higher classification scores indicate a stronger encoding of a given stimulus feature-set. The match-vs-mismatch evaluation is a randomised procedure. Here, we run tests on the mean across participants, where observations are 250 repetitions of that procedure. A statistically significant cortical encoding of melodic expectations was measured in the monophonic condition (AM &gt; A, Wilcoxon rank sum: <italic>p</italic> &lt; 10<sup>-30</sup>; <xref rid="fig3" ref-type="fig">Fig. 3B</xref><bold>-top</bold>), consistent with previous EEG, MEG, and intracranial EEG work with Music Transformers and Markov models (<xref ref-type="bibr" rid="c16">16</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Low-frequency cortical encoding of melodic expectation during polyphonic music listening.</title>
<p><bold>(A)</bold> Schematics of the analysis method. Canonical Correlation Analysis (CCA) was run to study the stimulus-EEG relationship. Match-vs-mismatch classification scores were derived to quantify the strength of the neural encoding of a given stimulus feature-set. <bold>(B)</bold> The analysis was run separately for acoustic-only features (A) and acoustic+melodic expectations features (AM) in each of the three experimental conditions. The distributions in the figure indicate repetitions of the match-vs-mismatch procedure. <bold>(C)</bold> The gain in match-vs-mismatch classification after including the melodic expectation features, ΔClassification, is compared across conditions and models, informing on whether a monophonic or a polyphonic account of melodic expectations best fit the neural data. In the box-plot, the bottom and top edges mark the 25th and 75th percentiles respectively, while the mid-line indicates the median value (***p&lt;0.001).</p></caption>
<graphic xlink:href="672294v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Next, we measured the cortical encoding of melodic expectation in the polyphonic conditions using the monophonic music transformer (<xref rid="fig3" ref-type="fig">Fig. 3A, B</xref>). A statistically significant encoding of melodic expectations emerged in both PolyOrig and PolyInv (two-way ANOVA; main effect of melodic expectations: <italic>F</italic>(1,249) = 512.1, <italic>p</italic> &lt; 10<sup>-30</sup>; one-tailed <italic>post hoc</italic> Wilcoxon rank sum tests in PolyOrig and PolyInv: <italic>p</italic> &lt; 10<sup>-30</sup> and <italic>p</italic> = 1.6*10<sup>-18</sup> respectively), with a significant main effect of condition (<italic>F</italic>(1,249) = 7998.3, <italic>p</italic> &lt; 10<sup>-30</sup>). This analysis did not find a condition x expectations interaction (<italic>F</italic>(1,249) = 0.1, <italic>p</italic> = 0.765).</p>
<p>Finally, we tested if melodic expectations built with a polyphonic transformer better represented the EEG signal than for a monophonic model (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>). A two-way ANOVA indicated main effects of model (monophonic vs. polyphonic: <italic>F</italic>(1,249) = 710.4, <italic>p</italic> &lt; 10<sup>-30</sup>) and condition (PolyOrig vs. PolyInv: <italic>F</italic>(1,249) = 283.8, <italic>p</italic> &lt; 10<sup>-30</sup>), as well as a statistically significant interaction effect (model x condition: <italic>F</italic>(1,249) = 4995.8, <italic>p</italic> &lt; 10<sup>-30</sup>). Interestingly, <italic>post hoc</italic> tests indicated that the EEG data more strongly related with the monophonic model in the PolyOrig condition and with the polyphonic model in the PolyInv condition (two-tailed <italic>post hoc</italic> Wilcoxon rank sum tests with <italic>p</italic> &lt; 10<sup>-30</sup>).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>This study investigated the neural processing of melody during polyphonic music listening. Previous research indicated that listeners can detect changes or errors in simultaneous music streams in some contexts (<xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c8">8</xref>) but not others (<xref ref-type="bibr" rid="c9">9</xref>), challenging the possibility that our brains can truly divide attention between simultaneous melodic streams. Theories like the figure-ground model and stream integration (<xref ref-type="bibr" rid="c9">9</xref>) have been proposed as perceptual strategies that can compensate for the difficulty of truly dividing attention. While that work offers precious insights into what our brains “can” do, the tasks used previously for that research alter the listening experience, raising doubts with regard to how those models impact uninstructed listening. Here, we fill that gap by measuring the spontaneous attention bias in the listeners’ brains during the uninstructed listening of polyphonic music. First, we found that salience is affected by both the high-voice superiority effect and the statistical properties of the melodic lines (<xref rid="fig2" ref-type="fig">Fig. 2</xref>), providing a quantitative approach to disentangle the two effects. Second, we provide evidence for a weighted integration strategy, with the attention bias altering the stream integration rate (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). Altogether, these results shed new light on the processing of melody during the uninstructed listening of polyphonic music, pointing to an inter-relationship between attention and statistical processing mechanisms, with attention orchestrating the way melody is formed.</p>
<sec id="s3a">
<title>Investigating the contributors to attention during uninstructed polyphonic music listening</title>
<p>Attention is potentially the key mechanism for understanding how melody is processed during polyphonic music listening. To that end, we designed a paradigm where the attention focus would change across conditions, without the need for instructing the participants on where to direct their attentional focus. We opted for using double-counterpoint compositions involving two monophonic piano melodic lines, primarily from J.S. Bach (<xref rid="tbl1" ref-type="table">Table 1</xref>). As discussed by P.A. Scholes, such compositions involve melodic lines that “move in apparent independence and freedom though fitting together harmonically” (<xref ref-type="bibr" rid="c33">33</xref>), creating a scenario where both streams present attractive melodic progressions. As a counter example, let’s consider a scenario involving polyphonic music with two streams, where the support stream has infrequent notes with little melodic variation. That scenario would make the pitch-line inversion less effective, as the new high-pitch stream would not stand on its own as the main melody.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
    <caption><title>Composers and titles of musical pieces used as stimuli.</title>
    </caption>
<graphic xlink:href="672294v1_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>Previous studies demonstrated that the focus of selective auditory attention can be reliably decoded from EEG signals during sustained attention tasks (<xref ref-type="bibr" rid="c27">27</xref>) and, more recently, by studying instructed attention switches (<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>). While that work was initially carried out on speech listening tasks in multi-talker scenarios (<xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c37">37</xref>), EEG attention decoding was also shown to be effective when considering polyphonic music during instructed selective attention tasks (<xref ref-type="bibr" rid="c15">15</xref>). Hausfeld and colleagues (<xref ref-type="bibr" rid="c38">38</xref>) went one step beyond by comparing an instructed selective attention task, where attention was steered to a given music stream, with a divided attention task, where participants were asked to focus on two music streams simultaneously. EEG attention decoding showed a preference toward the attended stream in the former, while no preference was measured in the latter task. Altogether, those results strongly support the reliability of decoding attention from EEG. However, that work focused on instructed listening, thus not informing us on how polyphonic music listening unfolds naturally. Other design choices further complicate the interpretation of those results, such as the use of different instruments for the distinct streams, which introduce another possible bias that, although interesting, was considered unnecessary to answer the research question in the present investigation. For that reason, our experiment only included virtual piano emulations generated from MIDI scores.</p>
<p>Our attention decoding analysis indicates that both high-pitch and melody statistics contribute to music salience. The present study teases apart two key factors influencing uninstructed attention by utilising two conditions involving the same type of music, with the sole difference that the pitch lines were inverted. Using behavioural (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>) and EEG decoding indices (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>), we found that the listeners’ steer their attention toward the high-pitch stream in both conditions, even though there was a substantially weaker bias in PolyInv. These results, together with the forward TRF results (<xref rid="fig2" ref-type="fig">Fig. 2E</xref>), are in line with our initial hypothesis that two key contributors are at play, and that the high-voice superiority effect is, in this case, stronger than the bias due to the melody attractiveness. It should be noted that our EEG results refer to the average attention bias throughout a piece, and that the behavioural index is only derived on a subsample of the music material. As such, that result would be compatible with different attention dynamics, such as a weaker attention bias throughout the piece, an increase in attention-switching between the voices, or their integration. Another limitation was the focus on a single music culture and style, which was a design choice allowing us to compare attention bias across two conditions that were minimally different. Indeed, different cultures are characterised by unique musical features. For example, some traditions employ distinct systems of note organization, such as makams, which incorporate additional pitches like quarter tones — which are not commonly found in Western music, except in certain avant-garde applications. Likewise, the inclusion of different musical styles would introduce varying melodic features, as they are constructed using diverse techniques. Future research should consider generalising the results of this research using a more diverse set of stimuli that can capture these interesting variabilities across music style and culture, as well as investigating the impact of other related factors, such as subjective biases (e.g., personal taste) and musical training.</p>
<p>This experiment involved two-stream stimuli where the main melody is primarily on the high pitch line, while both streams could stand on their own. The pitch inversion in PolyInv allowed us to study the neural encoding of music in conditions with different attention biases, while also disentangling the effects of high-pitch superiority and melody statistics. Switching pitch lines results in an inversion of the voicing, which affects the perception of the sound but not the harmonic context. Working with only two melodic streams facilitates their interchangeability, avoiding harmonic issues. Since a chord is typically defined by three notes, the absence of a third voice does not compromise harmonic clarity. In particular, many of these melodic elements, especially within the fugue, are constructed using the double-counterpoint technique, which guarantees the interchange of voices, without disrupting the structural integrity of the composition. Although the bass line is often less ornamented than the main melody, it provides essential harmonic grounding. This foundation ensures that even when the lines are inverted, the resulting music retains coherence and stability, despite the absence of a third voice to fully articulate the harmonic framework. As such, while generalisation the results of this study to other music styles requires further work, our choice of using double-counterpoint pieces enabled manipulations that were key for testing our hypotheses.</p>
</sec>
<sec id="s3b">
<title>Stream integration mechanisms contribute to melody formation</title>
<p>Numerous studies reported that neural responses to monophonic melodies reflect the expectation of a note, leading to patterns of neural activations that are compatible with a Bayesian view of the human brain, fitting frameworks such as predictive processing ((<xref ref-type="bibr" rid="c39">39</xref>); but see (<xref ref-type="bibr" rid="c21">21</xref>)). However, the relevance of those findings to the uninstructed listening of polyphonic music processing remains unclear. Listeners can identify a music piece by singing back its melody, indicating that part of polyphonic music processing involves transforming the incoming sound into a monophonic melody. This melody might correspond to a given stream or result from the integration of two or more streams. Previous work proposed theories on how that melody might be extracted. Here, we worked under the assumption that our brains process the resulting melody according to the predictive processing framework, where melodic expectations have been shown to be measurable with EEG (<xref ref-type="bibr" rid="c16">16</xref>). Using music transformers, we built numerical hypotheses on those melodic expectations, in one case assuming the processing of each monophonic stream (monophonic model), while the other model implements an integration of the two polyphonic streams (polyphonic model; <xref rid="fig3" ref-type="fig">Fig. 3</xref>).</p>
<p>Differently from existing theories, our results suggest that that the human brain processes multiple music streams via a dynamic strategy. Specifically, a strong attention bias toward a dominant stream led to a melody processing consistent with a monophonic processing of that stream. A weak attention bias, instead, led to an integration of the two streams into a single melody, as evidenced by a stronger alignment with the polyphonic model of melody expectations. In light of this result, we propose a <italic>weighted integration model</italic>, where attention modulates the rate of stream integration. The resulting integrated melody would then be the input of a statistical predictive process, which is what was measured in <xref rid="fig3" ref-type="fig">Figure 3</xref>. It should be noted that our results only speak to melody formation, and they do not address the important issue of how the support stream is processed in presence of a strong attention bias. While we contend that the figure-ground model could be a valid possibility, our data does not address that question nor exclude other possibilities, such as the divided attention model or one of its variations.</p>
<p>With the premise that attention might orchestrate music stream integration, one might observe that PolyOrig is more closely related with everyday listening of, for example, Western music, which typically involve a dominant melody. As a result, everyday music listening might be consistent with how the corresponding melodies would be processed in a monophonic context. This is a tantalising possibility, as it would make previous findings on monophonic melody perception directly relevant to more typical everyday multi-stream music listening. It should also be noted that our results involved averaging attention bias measurements throughout entire music pieces, while that bias likely changes within each piece. As such, the rate of integration might change dynamically at a more fine-grained level than when assessed here, calling for further investigation.</p>
<p>Our findings complement the past literature on simultaneous pitch sequences. Previous work using an odd-ball paradigm measured mismatch-negativity responses to deviant notes in response to both high and low pitch streams (<xref ref-type="bibr" rid="c40">40</xref>). That work had constraints that made that listening task different from the typical music listening experience, such as relying on the processing of deviant notes, using very short stimuli, and having isochronous note timing. Nonetheless, that study brought forward important data supporting the view that melody predictions built by our brains are compared with two simultaneous streams, leading to deviant responses (i.e., surprises) on either of them. Our result goes beyond that in several ways, especially by determining what melodic context is used by our brains to build note expectations. In doing so, these data confirm that melodic expectations can be measured for two streams simultaneously, determining that stream integration can naturally occur in correspondence of a weak attention bias.</p>
</sec>
    <sec id="s4">
        <title>Conclusions</title>
        <p>Additional research is necessary to determine if the inter-relationship between attention and melody formation is specific to this specific music style or if that is a general mechanism. Our methodology could be replicated on two-stream polyphonic music from other styles and cultures, with one caveat: the choice of an appropriate computational model of music must take into account the music culture or style of the stimuli and listener. That might constitute a limiting factor for under-represented music styles, even though the rapid developments in music transformer research are promising in that regard and may close that gap in the near future. Further work should also explore factors such as repetition, musical training, and music preference, which are expected to be particularly important for uninstructed listening tasks. Repetition, for example, may alter the attention bias and has been put forward as a possible way for disentangling the figure-ground and integration accounts of polyphonic music processing (<xref ref-type="bibr" rid="c9">9</xref>). With regard to musical training, while our results did not show any effects, our definition of musical training was quite varied in this sample. Future work aiming to shed light on that specific phenomenon should consider larger and more homogeneous samples.</p>
        <p>In sum, we demonstrate that polyphonic music processing can be studied with EEG with an uninstructed listening task. Our findings include novel insights into the inter-relationship between attention and melody statistical processing. Based on these results, we propose an extension of the stream integration model of polyphonic music listening, where attention bias modulates the engagement of integration strategies. We also speculate that stream integration might be a consequence of an auditory-motor neural pathway condensing the auditory input into monophonic motor instructions that can be produced via our vocal tract. More work is necessary to validate and extend our findings and speculations, for example by considering more diverse sets of stimuli, music styles, and participant cohorts. Furthermore, while our study encapsulated melodic expectations into a single metric, our brains have been shown to encode expectation in relation to different attributes (<xref ref-type="bibr" rid="c16">16</xref>). And while this study only focussed on pitch and timing properties when studying expectations, it is possible that different results would emerge in relation with distinct properties. For example, it might be the case that expectations for low pitch stream melodies are more relevant to the rhythm, while pitch contour could be more important for high pitch streams, in line with previous results from basic auditory physiology research (<xref ref-type="bibr" rid="c41">41</xref>). Additional research with tailored tasks and stimuli should be explored to tackle those questions.</p>
    </sec></sec>

<sec id="s5">
<title>Methods</title>
<sec id="s5a">
<title>Participants</title>
<p>40 participants were recruited for this study. All participants finished the study however, nine of them were excluded due to technical issues with the EEG recording, leading to a dataset with 31 participants overall (16 males, aged between 19 and 30, mean = 23.2, std = 2.3), 8 of whom have had formal musical training or at least 2 years of professional musical experience. All participants gave their written informed consent to participate in this study. The study was undertaken in accordance with the Declaration of Helsinki and was approved by the ethics committee in the School of Psychology of Trinity College Dublin. Data collection was carried out between September 2022 and January 2024.</p>
</sec>
<sec id="s5b">
<title>Experimental design</title>
<p>Testing took place in a dark room. The experiment was organized into two experimental blocks: a listening block and a melody identification block (<xref rid="fig1" ref-type="fig">Figure 1</xref>), collecting EEG and behavioural measurements respectively. First, participants were presented to monophonic and polyphonic music pieces, with the sole instruction of listening to the music while looking at a fixation cross and minimising motor movement. After the listening block, the second part of the experiment involved a melody identification task, aided by a listen and repeat exercise. Specifically, participants were presented with short (4-8 seconds) segments of polyphonic music, extracted from previously presented PolyOrig or PolyInv pieces. After hearing a segment, participants were asked to sing it back. Then, they were presented with the two melodies as separate monophonic pieces and asked: “Which of the 2 melodies were you attempting to sing? 1: only the first melody; 2: mostly the first melody; 3: an equal mix of both; 4: mostly the second melody; 5: only the second melody”. EEG signals were not recorded during the second block.</p>
<p>Neurobs Presentation software was used for coding the experiment (<ext-link ext-link-type="uri" xlink:href="http://www.neurobs.com">http://www.neurobs.com</ext-link>), which was carried out in a single session for all participants. Audio stimuli were presented at a sampling rate of 44,100 Hz and played through Sennheiser HD 280 Pro headphones. EEG data was simultaneously acquired with a sampling rate of 250 Hz, from twenty-four electrode positions using an mBrainTrain Smarting wireless system.</p>
</sec>
<sec id="s5c">
<title>Stimuli</title>
<p>Stimuli consisted of MIDI versions of 32 classical music pieces from a corpus of Bach and other Western composers (<xref rid="tbl1" ref-type="table">Table 1</xref>). The original pieces were ∼150s long snippets of polyphonic music. Minor manual corrections were applied to ensure that each piece was the combination of two monophonic streams, ensuring no more than two notes co-occurred at any given time. These two streams were then separated based into the high-pitch stream and the low-pitch stream (corresponding to the main and support stream respectively). These original polyphonic pieces (PolyOrig) were then manipulated to generate stimuli for other two conditions: PolyInv and monophonic. The two streams were pitch-shifted (typically by one octave) to invert the pitch lines, ensuring that all notes in the low-pitched stream were below the high-pitched stream. This ensured that the high- and low-pitch streams were clearly distinguishable in terms of pitch range (<xref ref-type="bibr" rid="c42">42</xref>). This manipulation led to the PolyInv stimuli, where the melody originally built to be the main stream was now at the low pitch line. Finally, the monophonic stimuli consisted of the individual streams extracted from PolyOrig and PolyInv. Finally, the music integrity of the resulting pieces was verified by a music expert and composer (I.C.S.).</p>
</sec>
<sec id="s5d">
<title>EEG data preprocessing</title>
    <p>EEG and stimulus data were anonymised after collection and were stored using the Continuous-event Neural Data (CND) format. EEG data was preprocessed and analysed offline using MATLAB R2023a software using custom code built starting from the analysis scripts of the CNSP open science initiative (<ext-link ext-link-type="uri" xlink:href="https://github.com/CNSP-Workshop/CNSP-resources">https://github.com/CNSP-Workshop/CNSP-resources</ext-link>). EEG signals were filtered using high and low pass filters at cut off frequencies 1 and 8 Hz respectively. All filtering was done with Butterworth zero-phase filters of order two, implemented with the <italic>filtfilt</italic> function. EEG signals were downsampled to 125 Hz. EEG channels with a variance exceeding three times that of the surrounding ones were replaced by an estimate calculated using spherical spline interpolation. EEG data were re-referenced to the global average of all channels.</p>
</sec>
<sec id="s5e">
<title>Analytic procedure</title>
<p>A system identification approach was used to compute the channel-specific mapping between music features and EEG responses. This method, referred to as the temporal response function (TRF), models the neural response at time <italic>t</italic> and channel <italic>η</italic> as a linear convolution of the stimulus property <italic>s</italic>(<italic>t</italic>) with an unknown, channel-specific filter <italic>w</italic>(<italic>τ, η</italic>), plus a residual term:
<disp-formula id="eqn1">
<graphic xlink:href="672294v1_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Here, <italic>r</italic>(<italic>t</italic>,η) is the instantaneous neural response, and ε(<italic>t, η</italic>) is the residual error not explained by the model.</p>
<p>The TRF <italic>w</italic>(<italic>τ,η</italic>) was estimated using regularised linear regression (ridge regression) to minimise overfitting. The solution is given by:
<disp-formula id="eqn2">
<graphic xlink:href="672294v1_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>S</italic> is the lagged time series matrix of the stimulus features, <italic>λ</italic> is the ridge parameter, and <italic>I</italic> is the identity matrix.</p>
<p>Model performance is evaluated using leave-one-out cross-validation across trials. The quality of a prediction is quantified by calculating Pearson’s correlation between the pre-processed recorded signals and the corresponding predictions at each scalp electrode.</p>
<p>Conversely, the backward TRF is a linear filter that can be fit to decode stimulus features from the neural recording. To estimate spontaneous attention, a backward TRF model is fit on the monophonic condition, where only one stream is played at a time. Note that the monophonic stimuli in the experiment were randomly selected from all streams in any of the polyphonic conditions, constructing a balanced training set and leading to a decoder that reconstructs the envelope of the attended melody. That decoder was then applied to the polyphonic conditions, producing envelope reconstructions that best correlate with the attended stream, which is then used to compare the decoding accuracies of the two streams the participants actually heard.</p>
</sec>
<sec id="s5f">
<title>Stimuli feature extraction</title>
<p>Acoustic and melodic features were extracted from the audio files. Acoustic features included the sound envelope (<italic>Env</italic>), which was extracted from the Hilbert transform of the acoustic waveform. Second, we included the halfway rectified envelope derivative (<italic>Env’</italic>), which was calculated on the Hilbert envelope at the original sampling rate (44,100 Hz), before downsampling both <italic>Env</italic> and <italic>Env’</italic>. Melodic features were derived using the Anticipatory Music Transformer (AMT), a generative pre-trained transformer model built on Western music. Transformer architectures internally build probability distribution functions for each upcoming note based on long term memory (training set) and short term memory (context). The melodic features used were the surprise and entropy of the pitch of the notes (<italic>S, H</italic>). Surprise is the information content of the note.
<disp-formula id="eqn3">
<graphic xlink:href="672294v1_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Where <italic>n</italic><sub><italic>i</italic></sub> is the current note and <italic>n</italic><sub><italic>i-1</italic></sub>, <italic>n</italic><sub><italic>i-2</italic></sub>, … are the previous notes. The entropy represents the uncertainty at the time of the new note, calculated as the Shannon entropy over the distribution of the set of notes (<italic>N</italic>).
<disp-formula id="eqn4">
<graphic xlink:href="672294v1_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>All features were downsampled to 125 Hz for analysis.</p>
</sec>
<sec id="s5g">
<title>Statistical analysis</title>
<p>All statistical analyses were carried out in MATLAB R2023a. Analyses directly comparing the groups were performed using repeated measures two-way ANOVAs with the <italic>ranova</italic> function. One-sample <italic>t</italic>-tests were used for post hoc tests. Correction for multiple comparisons was applied where necessary via Benjamini-Hochberg Procedure with the <italic>fdr_bh</italic> function. Effect sizes are reported using Cohen’s <italic>d</italic>.</p>
</sec>
</sec>
</body>
<back>
<sec id="s6" sec-type="data-availability">
<title>Data Availability</title>
<p>Analysis code and data (both EEG signals and stimuli) will be made publicly available in a standardised format (Continuous-event Neural Data (<xref ref-type="bibr" rid="c43">43</xref>)) at the time of publication via the OSF repository.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This research was conducted with the financial support of Reserach Ireland at ADAPT, the Research Ireland Centre for AI-Driven Digital Content Technology at Trinity College Dublin and University College Dublin [13/RC/2106_P2]. I.C.S. was supported by a Government of Ireland Postgraduate Scholarship (Irish Research Council). For the purpose of Open Access, the authors have applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission. We thank the Cognition and Natural Sensory Processing (CNSP) initiative, which provided the blueprint for the analysis code and data standardisation guidelines used in this work. We thank Asena Akkaya and Amirhossein Chalehchaleh for their help with data collection.</p>
</ack>
<ref-list>
<title>References</title>
    <ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Salamon</surname> <given-names>J</given-names></string-name>, <string-name><surname>Gómez</surname> <given-names>E.</given-names></string-name></person-group> <article-title>Melody extraction from polyphonic music signals using pitch contour characteristics</article-title>. <source>IEEE transactions on audio, speech, and language processing</source>. <year>2012</year>;<volume>20</volume>(<issue>6</issue>):<fpage>1759</fpage>–<lpage>70</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poliner</surname> <given-names>GE</given-names></string-name>, <string-name><surname>Ellis</surname> <given-names>DP</given-names></string-name>, <string-name><surname>Ehmann</surname> <given-names>AF</given-names></string-name>, <string-name><surname>Gómez</surname> <given-names>E</given-names></string-name>, <string-name><surname>Streich</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ong</surname> <given-names>B.</given-names></string-name></person-group> <article-title>Melody transcription from music audio: Approaches and evaluation</article-title>. <source>IEEE Transactions on Audio, Speech, and Language Processing</source>. <year>2007</year>;<volume>15</volume>(<issue>4</issue>):<fpage>1247</fpage>–<lpage>56</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Dowling</surname> <given-names>W</given-names></string-name>, <string-name><surname>Harwood</surname> <given-names>D.</given-names></string-name></person-group> <source>Music Cognition</source> <publisher-name>Academic Press</publisher-name>. <publisher-loc>London</publisher-loc>; <year>1986</year>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raghavan</surname> <given-names>VS</given-names></string-name>, <string-name><surname>O’Sullivan</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bickel</surname> <given-names>S</given-names></string-name>, <string-name><surname>Mehta</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Mesgarani</surname> <given-names>N.</given-names></string-name></person-group> <article-title>Distinct neural encoding of glimpsed and masked speech in multitalker situations</article-title>. <source>Plos Biology</source>. <year>2023</year>;<volume>21</volume>(<issue>6</issue>):<fpage>e3002128</fpage>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Park</surname> <given-names>H</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Get the gist of the story: Neural map of topic keywords in multi-speaker environment</article-title>. <source>bioRxiv</source>. <year>2022</year>:2022.05. 05.490770.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gregory</surname> <given-names>AH</given-names></string-name></person-group>. <article-title>Listening to polyphonic music</article-title>. <source>Psychology of Music</source>. <year>1990</year>;<volume>18</volume>(<issue>2</issue>):<fpage>163</fpage>–<lpage>70</lpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dowling</surname> <given-names>WJ</given-names></string-name></person-group>. <article-title>The perception of interleaved melodies</article-title>. <source>Cognitive Psychology</source>. <year>1973</year>;<volume>5</volume>(<issue>3</issue>):<fpage>322</fpage>–<lpage>37</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sloboda</surname> <given-names>J</given-names></string-name>, <string-name><surname>Edworthy</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Attending to two melodies at once: the of key relatedness</article-title>. <source>Psychology of Music</source>. <year>1981</year>;<volume>9</volume>(<issue>1</issue>):<fpage>39</fpage>–<lpage>43</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bigand</surname> <given-names>E</given-names></string-name>, <string-name><surname>McAdams</surname> <given-names>S</given-names></string-name>, <string-name><surname>Forêt</surname> <given-names>S.</given-names></string-name></person-group> <article-title>Divided attention in music</article-title>. <source>International Journal of Psychology</source>. <year>2000</year>;<volume>35</volume>(<issue>6</issue>):<fpage>270</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crawley</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>Acker-Mills</surname> <given-names>BE</given-names></string-name>, <string-name><surname>Pastore</surname> <given-names>RE</given-names></string-name>, <string-name><surname>Weil</surname> <given-names>S.</given-names></string-name></person-group> <article-title>Change detection in multi-voice music: the role of musical structure, musical training, and task demands</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>2002</year>;<volume>28</volume>(<issue>2</issue>):<fpage>367</fpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Trainor</surname> <given-names>LJ</given-names></string-name>, <string-name><surname>Marie</surname> <given-names>C</given-names></string-name>, <string-name><surname>Bruce</surname> <given-names>IC</given-names></string-name>, <string-name><surname>Bidelman</surname> <given-names>GM</given-names></string-name></person-group>. <article-title>Explaining the high voice superiority effect in polyphonic music: Evidence from cortical evoked potentials and peripheral auditory models</article-title>. <source>Hearing Research</source>. <year>2014</year>;<volume>308</volume>:<fpage>60</fpage>–<lpage>70</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marie</surname> <given-names>C</given-names></string-name>, <string-name><surname>Fujioka</surname> <given-names>T</given-names></string-name>, <string-name><surname>Herrington</surname> <given-names>L</given-names></string-name>, <string-name><surname>Trainor</surname> <given-names>LJ</given-names></string-name></person-group>. <article-title>The high-voice superiority effect in polyphonic music is influenced by experience: A comparison of musicians who play soprano-range compared with bass-range instruments</article-title>. <source>Psychomusicology: Music, Mind, and Brain</source>. <year>2012</year>;<volume>22</volume>(<issue>2</issue>):<fpage>97</fpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marie</surname> <given-names>C</given-names></string-name>, <string-name><surname>Trainor</surname> <given-names>LJ</given-names></string-name></person-group>. <article-title>Early development of polyphonic sound encoding and the high voice superiority effect</article-title>. <source>Neuropsychologia</source>. <year>2014</year>;<volume>57</volume>:<fpage>50</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marie</surname> <given-names>C</given-names></string-name>, <string-name><surname>Trainor</surname> <given-names>LJ</given-names></string-name></person-group>. <article-title>Development of simultaneous pitch encoding: infants show a high voice superiority effect</article-title>. <source>Cerebral Cortex</source>. <year>2013</year>;<volume>23</volume>(<issue>3</issue>):<fpage>660</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Cantisani</surname> <given-names>G</given-names></string-name>, <string-name><surname>Essid</surname> <given-names>S</given-names></string-name>, <string-name><surname>Richard</surname> <given-names>G</given-names></string-name></person-group>, editors. <article-title>EEG-based decoding of auditory attention to a target instrument in polyphonic music</article-title>. <conf-name>2019 IEEE workshop on applications of signal processing to audio and acoustics (WASPAA)</conf-name>; <year>2019</year>:.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Pelofi</surname> <given-names>C</given-names></string-name>, <string-name><surname>Bianco</surname> <given-names>R</given-names></string-name>, <string-name><surname>Patel</surname> <given-names>P</given-names></string-name>, <string-name><surname>Mehta</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Herrero</surname> <given-names>JL</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Cortical encoding of melodic expectations in human temporal cortex</article-title>. <source>eLife</source>. <year>2020</year>;<volume>9</volume>:<elocation-id>e51784</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.51784</pub-id></mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pearce</surname> <given-names>MT</given-names></string-name></person-group>. <article-title>Statistical learning and probabilistic prediction in music cognition: mechanisms of stylistic enculturation</article-title>. <source>Annals of the New York Academy of Sciences</source>. <year>2018</year>;<volume>1423</volume>(<issue>1</issue>):<fpage>378</fpage>–<lpage>95</lpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bianco</surname> <given-names>R</given-names></string-name>, <string-name><surname>Harrison</surname> <given-names>PMC</given-names></string-name>, <string-name><surname>Hu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bolger</surname> <given-names>C</given-names></string-name>, <string-name><surname>Picken</surname> <given-names>S</given-names></string-name>, <string-name><surname>Pearce</surname> <given-names>MT</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Long-term implicit memory for sequential auditory patterns in humans</article-title>. <source>eLife</source>. <year>2020</year>;<volume>9</volume>:<elocation-id>e56073</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.56073</pub-id></mixed-citation></ref>
    <ref id="c19"><label>19.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Trainor</surname> <given-names>LJ</given-names></string-name>, <string-name><surname>Zatorre</surname> <given-names>RJ</given-names></string-name></person-group>. <chapter-title>The neurobiological basis of musical expectations</chapter-title>. <source>The Oxford handbook of music psychology</source>. <person-group person-group-type="editor"><string-name><surname>Hallam</surname> <given-names>S</given-names></string-name>, <string-name><surname>Cross</surname> <given-names>I</given-names></string-name>, <string-name><surname>Thaut</surname> <given-names>MH</given-names></string-name></person-group> <year>2009</year>:<fpage>171</fpage>–<lpage>83</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pearce</surname> <given-names>MT</given-names></string-name>, <string-name><surname>Wiggins</surname> <given-names>GA</given-names></string-name></person-group>. <article-title>Auditory Expectation: The Information Dynamics of Music Perception and Cognition</article-title>. <source>Topics in Cognitive Science</source>. <year>2012</year>;<volume>4</volume>(<issue>4</issue>):<fpage>625</fpage>–<lpage>52</lpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Robert</surname> <given-names>P</given-names></string-name>, <string-name><surname>Van Cang</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Mercier</surname> <given-names>M</given-names></string-name>, <string-name><surname>Trébuchon</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bartolomei</surname> <given-names>F</given-names></string-name>, <string-name><surname>Arnal</surname> <given-names>LH</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Multi-stream predictions in human auditory cortex during natural music listening</article-title>. <source>bioRxiv</source>. <year>2024</year>:2024.11.27.625704.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oore</surname> <given-names>S</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>I</given-names></string-name>, <string-name><surname>Dieleman</surname> <given-names>S</given-names></string-name>, <string-name><surname>Eck</surname> <given-names>D</given-names></string-name>, <string-name><surname>Simonyan</surname> <given-names>K.</given-names></string-name></person-group> <article-title>This time with feeling: Learning expressive musical performance</article-title>. <source>Neural Computing and Applications</source>. <year>2020</year>;<volume>32</volume>:<fpage>955</fpage>–<lpage>67</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Thickstun</surname> <given-names>J</given-names></string-name>, <string-name><surname>Hall</surname> <given-names>D</given-names></string-name>, <string-name><surname>Donahue</surname> <given-names>C</given-names></string-name>, <string-name><surname>Liang</surname> <given-names>P.</given-names></string-name></person-group> <article-title>Anticipatory music transformer</article-title>. <source>arXiv</source>. <year>2023</year>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Disbergen</surname> <given-names>NR</given-names></string-name>, <string-name><surname>Valente</surname> <given-names>G</given-names></string-name>, <string-name><surname>Formisano</surname> <given-names>E</given-names></string-name>, <string-name><surname>Zatorre</surname> <given-names>RJ</given-names></string-name></person-group>. <article-title>Assessing top-down and bottom-up contributions to auditory stream segregation and integration with polyphonic music</article-title>. <source>Frontiers in neuroscience</source>. <year>2018</year>;<volume>12</volume>:<fpage>121</fpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crosse</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Bednar</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name></person-group>. <article-title>The multivariate temporal response function (mTRF) toolbox: A MATLAB toolbox for relating neural signals to continuous stimuli</article-title>. <source>Frontiers in Human Neuroscience</source>. <year>2016</year>;<volume>10</volume>(NOV2016).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crosse</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Zuk</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Nidiffer</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Molholm</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name></person-group>. <article-title>Linear Modeling of Neurophysiological Responses to Speech and Other Continuous Stimuli: Methodological Considerations for Applied Research</article-title>. <source>Frontiers in neuroscience</source>. <year>2021</year>;<volume>15</volume>:<fpage>705621</fpage>-.</mixed-citation></ref>
    <ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Sullivan</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Power</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Mesgarani</surname> <given-names>N</given-names></string-name>, <string-name><surname>Rajaram</surname> <given-names>S</given-names></string-name>, <string-name><surname>Foxe</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG</article-title>. <source>Cerebral Cortex</source>. <year>2014</year>:.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Marion</surname> <given-names>G</given-names></string-name>, <string-name><surname>Shamma</surname> <given-names>SA</given-names></string-name></person-group>. <article-title>The Music of Silence: Part II: Music Listening Induces Imagery Responses</article-title>. <source>The Journal of Neuroscience</source>. <year>2021</year>;<volume>41</volume>(<issue>35</issue>):<fpage>7449</fpage>.</mixed-citation></ref>
    <ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marion</surname> <given-names>G</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Shamma</surname> <given-names>SA</given-names></string-name></person-group>. <article-title>The Music of Silence. Part I: Responses to Musical Imagery Accurately Encode Melodic Expectations and Acoustics</article-title>. <source>Journal of Neuroscience</source>. <year>2021</year>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kern</surname> <given-names>P</given-names></string-name>, <string-name><surname>Heilbron</surname> <given-names>M</given-names></string-name>, <string-name><surname>de Lange</surname> <given-names>FP</given-names></string-name>, <string-name><surname>Spaak</surname> <given-names>E.</given-names></string-name></person-group> <article-title>Cortical activity during naturalistic music listening reflects short-range predictions based on long-term experience</article-title>. <source>eLife</source>. <year>2022</year>;<volume>11</volume>:<elocation-id>e80935</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.80935</pub-id></mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>de Cheveigné</surname> <given-names>A</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Arzounian</surname> <given-names>D</given-names></string-name>, <string-name><surname>Wong</surname> <given-names>DDE</given-names></string-name>, <string-name><surname>Hjortkjær</surname> <given-names>J</given-names></string-name>, <string-name><surname>Fuglsang</surname> <given-names>S</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Multiway canonical correlation analysis of brain data</article-title>. <source>NeuroImage</source>. <year>2019</year>;<volume>186</volume>:<fpage>728</fpage>–<lpage>40</lpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Wong</surname> <given-names>D</given-names></string-name>, <string-name><surname>Melnik</surname> <given-names>GA</given-names></string-name>, <string-name><surname>de Cheveigne</surname> <given-names>A.</given-names></string-name></person-group> <article-title>Low-frequency cortical responses to natural speech reflect probabilistic phonotactics</article-title>. <source>NeuroImage</source>. <year>2019</year>;<volume>196</volume>:<fpage>237</fpage>–<lpage>47</lpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Scholes</surname> <given-names>PA</given-names></string-name></person-group>. <chapter-title>In true polyphonic music every line develops its own metric pattern without even a regularity of accented beats among voices</chapter-title>. <source>The uniformity is brought about in the harmony alone</source>.: <publisher-loc>London</publisher-loc>: <publisher-name>Oxford Univ. Press</publisher-name>; <year>1938</year>. <fpage>742</fpage> p.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hjortkjær</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wong</surname> <given-names>DD</given-names></string-name>, <string-name><surname>Catania</surname> <given-names>A</given-names></string-name>, <string-name><surname>Märcher-Rørsted</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ceolini</surname> <given-names>E</given-names></string-name>, <string-name><surname>Fuglsang</surname> <given-names>SA</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Real-time control of a hearing instrument with EEG-based attention decoding</article-title>. <source>Journal of Neural Engineering</source>. <year>2025</year>;<volume>22</volume>(<issue>1</issue>):<fpage>016027</fpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haro</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rao</surname> <given-names>HM</given-names></string-name>, <string-name><surname>Quatieri</surname> <given-names>TF</given-names></string-name>, <string-name><surname>Smalt</surname> <given-names>CJ</given-names></string-name></person-group>. <article-title>EEG alpha and pupil diameter reflect endogenous auditory attention switching and listening effort</article-title>. <source>Eur J Neurosci</source>. <year>2022</year>;<volume>55</volume>(<issue>5</issue>):<fpage>1262</fpage>–<lpage>77</lpage>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Cheveigné</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wong</surname> <given-names>DDE</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Hjortkjær</surname> <given-names>J</given-names></string-name>, <string-name><surname>Slaney</surname> <given-names>M</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>E.</given-names></string-name></person-group> <article-title>Decoding the auditory brain with canonical component analysis</article-title>. <source>NeuroImage</source>. <year>2018</year>;<volume>172</volume>:<fpage>206</fpage>–<lpage>16</lpage>.</mixed-citation></ref>
    <ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fuglsang</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Dau</surname> <given-names>T</given-names></string-name>, <string-name><surname>Hjortkjær</surname><given-names>J</given-names></string-name></person-group>. <article-title>Noise-robust cortical tracking of attended speech in real-world acoustic scenes</article-title>. <source>NeuroImage</source>. <year>2017</year>;<volume>156</volume>:<fpage>435</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hausfeld</surname> <given-names>L</given-names></string-name>, <string-name><surname>Disbergen</surname> <given-names>NR</given-names></string-name>, <string-name><surname>Valente</surname> <given-names>G</given-names></string-name>, <string-name><surname>Zatorre</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Formisano</surname> <given-names>E.</given-names></string-name></person-group> <article-title>Modulating cortical instrument representations during auditory stream segregation and integration with polyphonic music</article-title>. <source>Frontiers in neuroscience</source>. <year>2021</year>;<volume>15</volume>:<fpage>635937</fpage>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Clark</surname> <given-names>A.</given-names></string-name></person-group> <source>Surfing Uncertainty: Prediction, Action, and the Embodied Mind</source>: <publisher-name>Oxford University Press</publisher-name>; <year>2016</year>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fujioka</surname> <given-names>T</given-names></string-name>, <string-name><surname>Trainor</surname> <given-names>LJ</given-names></string-name>, <string-name><surname>Ross</surname> <given-names>B</given-names></string-name>, <string-name><surname>Kakigi</surname> <given-names>R</given-names></string-name>, <string-name><surname>Pantev</surname> <given-names>C.</given-names></string-name></person-group> <article-title>Automatic encoding of polyphonic melodies in musicians and nonmusicians</article-title>. <source>Journal of cognitive neuroscience</source>. <year>2005</year>;<volume>17</volume>(<issue>10</issue>):<fpage>1578</fpage>–<lpage>92</lpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hove</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Marie</surname> <given-names>C</given-names></string-name>, <string-name><surname>Bruce</surname> <given-names>IC</given-names></string-name>, <string-name><surname>Trainor</surname> <given-names>LJ</given-names></string-name></person-group>. <article-title>Superior time perception for lower musical pitch explains why bass-ranged instruments lay down musical rhythms</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2014</year>;<volume>111</volume>(<issue>28</issue>):<fpage>10383</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huron</surname> <given-names>D.</given-names></string-name></person-group> <article-title>The Avoidance of Part-Crossing in Polyphonic Music: Perceptual Evidence and Musical Practice</article-title>. <source>Music Perception: An Interdisciplinary Journal</source>. <year>1991</year>;<volume>9</volume>(<issue>1</issue>):<fpage>93</fpage>–<lpage>103</lpage>.</mixed-citation></ref>
    <ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Nidiffer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Crosse</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Zuk</surname> <given-names>N</given-names></string-name>, <string-name><surname>Haro</surname> <given-names>S</given-names></string-name>, <string-name><surname>Cantisani</surname> <given-names>G</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>A standardised open science framework for sharing and re-analysing neural data acquired to continuous stimuli</article-title>. <source>Neurons, Behavior, Data analysis, and Theory</source>. <year>2024</year>:<fpage>1</fpage>–<lpage>25</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108767.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Peelle</surname>
<given-names>Jonathan Erik</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
    <institution-id institution-id-type="ror">https://ror.org/04t5xt781</institution-id><institution>Northeastern University</institution>
</institution-wrap>
<city>Boston</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> work potentially advances our understanding of melody extraction in polyphonic music listening by identifying spontaneous attentional focus in uninstructed listening contexts. However, the evidence supporting the main conclusions is <bold>incomplete</bold>. The work will be of interest to psychologists and neuroscientists working on music listening, attention, and perception in ecological settings.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108767.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This manuscript investigates the interplay between spontaneous attention and melody formation during polyphonic music listening. The authors use EEG recordings during uninstructed listening to examine how attention bias influences melody processing, employing both behavioural measures and computational modelling with music transformers. The study introduces a very clever pitch-inversion manipulation design to dissociate high-voice superiority from melodic salience, and proposes a &quot;weighted integration&quot; model where attention dynamically modulates how multiple voices are combined into perceived melody.</p>
<p>Strengths:</p>
<p>(1) The attention bias findings (Figure 2) are compelling and methodologically sound, with convergent evidence from both behavioral and neural measures.</p>
<p>(2) The pitch-inversion manipulation appears to super elegantly dissociate two competing factors (high-voice superiority vs melodic salience), moreover, the authors claim that the chosen music lends itself perfectly to his PolyInv condition. A claim I cannot really evaluate, but which would make it even more neat.</p>
<p>(3) Nice bridge between hypotheses and operationalisations.</p>
<p>Weaknesses:</p>
<p>The results in Figure 3 are very striking, but I have a number of questions before I can consider myself convinced.</p>
<p>(1) Conceptual questions about surprisal analysis:</p>
<p>The pattern of results seems backwards to me. Since the music is inherently polyphonic in PolyOrig, I'd expect the polyphonic model to fit the brain data better - after all, that's what the music actually is. These voices were composed to interact harmonically, so modeling them as independent monophonic streams seems like a misspecification. Why would the brain match this misspecified model better?</p>
<p>Conversely, it would seem to me the pitch inversion in PolyInv disrupts (at least to some extent) the harmonic coherence, so if anywhere, I'd a priori expect that in this condition, listeners would rather be processing streams separately - making the monophonic model fit better there (or less bad), not in PolyOrig. The current pattern is exactly opposite to what seems logical to me.</p>
<p>(2) Missing computational analyses:</p>
<p>If the transformer is properly trained, it should &quot;understand&quot; (i.e., predict/compress) the polyphonic music better, right? Can the authors demonstrate this via perplexity scores, bits-per-byte, or other prediction metrics, comparing how well each model (polyphonic vs monophonic) handles the music in both conditions? Similarly, if PolyInv truly maintains musical integrity as claimed, the polyphonic model should handle it as well as PolyOrig. But if the inversion does disrupt the music, we should see this reflected in degraded prediction scores. These metrics would validate whether the experimental manipulation works as intended. Also, how strongly are the surprisal streams correlated? There are many non-trivial modelling steps that should be reported in more detail.</p>
<p>(3) Methodological inconsistencies:</p>
<p>Why are the two main questions (Figures 2 and 3) answered with completely different analytical approaches? The switch from TRF to CCA with match-vs-mismatch classification seems unmotivated. I think it's very important to provide a simpler model comparison - just TRF with acoustic features plus either polyphonic or monophonic surprisal - evaluated on relevant electrodes or the full scalp. This would make the results more comparable and interpretable.</p>
<p>(4) Presentation and methods:</p>
<p>a) Coming from outside music/music theory, I found the paper somewhat abstract and hard to parse initially. The experimental logic becomes clearer with reflection, but you're doing yourselves a disservice with the jargon-heavy presentation. It would be useful to include example stimuli.</p>
<p>b) The methods section is extremely brief - no details whatsoever are provided regarding the modelling: What specific music transformer architecture? Which implementation of this &quot;anticipatory music transformer&quot;? Pre-trained on what corpus - monophonic, polyphonic, Western classical only? What constituted &quot;technical issues&quot; for the 9 excluded participants? What were the channel rejection criteria?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108767.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors sought to understand the drivers of spontaneous attentional bias and melodic expectation generation during listening to short two-part classical pieces. They measured scalp EEG data in a monophonic condition and trained a model to reconstruct the audio envelope from the EEG. They then used this model to probe which of the two voices was best reflected in the neural signal during two polyphonic conditions. In one condition, the original piece was presented, in the other, the voices were switched in an attempt to distinguish between effects of (a) the pitch range of one voice compared to the other and (b) intrinsic melodic features. They also collected a behavioural measure of attentional bias for a subset of the stimuli in a separate study. Further modelling assessed whether expectations of how the melody would unfold were formed based on an integrated percept of melody across the two voices, or based on a single voice. The authors sought to relate the findings to different theories of how musical/auditory scene analysis occurs, based on divided attention, figure-ground perception, and stream integration.</p>
<p>Strengths:</p>
<p>(1) A clever but simple manipulation - transposing the voices such that the higher one became the lower one - allowed an assessment of different factors that might affect the allocation of attention.</p>
<p>(2) State-of-the-art analytic techniques were applied to (a) build a music attention decoder (these are more commonly encountered for speech) and (b) relate the neural data to features of the stimulus at the level of acoustics and expectation.</p>
<p>(3) The effects appeared robust across the group, not driven by a handful of participants.</p>
<p>Weaknesses:</p>
<p>(1) A key goal of the work is to establish the relative importance for the listener's attention of a voice's (a) mean pitch in the context of the two voices (high-voice superiority) and (b) intrinsic melodic statistics/motif attractiveness. The rationale of the experimental manipulation is that switching the relative height of the lines allows these to be dissociated by imparting the same high-voice benefit to the new high-voice and the same preferred intrinsic melodic statistics to the new low voice. However, previous work suggests that the high-voice superiority effect is not all-or-nothing. Electrophysiology supported by auditory nerve modelling found it to depend on the degree of voice separation in a non-monotonic way (see <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.heares.2013.07.014">https://doi.org/10.1016/j.heares.2013.07.014</ext-link> at p. 68). Although the authors keep the overall pitch of the lower (and upper) line fixed across conditions, systematically different contour patterns across the voices could give rise to a sub-optimal distribution of separations in the PolyInv versus PolyOrig condition. This could weaken the high-voice superiority effect in PolyInv and explain the pattern of results. One could argue that such contour differences are examples of the &quot;intrinsic melodic statistics&quot; put forward as the effect working in opposition to high-voice superiority, but it is their interaction across voices that matters here.</p>
<p>(2) Although melody statistics are mentioned throughout, none have been calculated. It would be helpful to see the features that presumably lead to &quot;motif attractiveness&quot; quantified, as well as how they differ across lines. The work of David Huron, such as at <ext-link ext-link-type="uri" xlink:href="https://dl.acm.org/doi/abs/10.1145/3469013.3469016">https://dl.acm.org/doi/abs/10.1145/3469013.3469016</ext-link>, provides examples that could be calculated with ease and compared across the two lines: &quot;the tendency for small over large pitch movements, for large leaps to ascend, for musical phrases to fall in pitch, and for phrases to begin with an initial pitch rise&quot;. The authors also mention differences in ornamentation. Such comparisons would make it more tangible for the reader as to what differs across the original &quot;melody&quot; and &quot;support&quot; line. In particular, as the authors themselves note, lines in double-counterpoint pieces can, to a degree, operate interchangeably. Bach's inventions in particular use a lot of direct repetition (up to octave invariance), which one would expect to minimise differences in the statistics mentioned. The references purporting to relate to melodic statistics (11-14 in original numbering) seem rather to relate to high-voice superiority.</p>
<p>(3) The exact nature of the transposition manipulation is obscured by a confusing Figure 1B, which shows an example in which the transposed line does not keep the same note-to-note interval structure as the original line.</p>
<p>(4) The transformer model is barely described in the main text. Even readers who are familiar with the Hidden Markov Models (e.g., in IDyOM) previously used by some of the authors to model melodic surprise and entropy would benefit from a brief description in the main text at least of how transformer models are different. The Methods section goes a little further but does not mention what the training set was, nor the relative weight given to long- and short-term memory models.</p>
<p>(5) The match-mismatch procedure should be explained in enough detail for readers to at least understand what value represents chance performance and why performance would be measured as an average over participants. Relatedly, there is no description at all of CCA or the match-mismatch procedure in the Methods.</p>
<p>(6) Details of how the integration model was implemented will be critical to interpreting the results relating to melodic expectations. It is not clear how &quot;a single melody combining the two streams&quot; was modelled, given that at least some notes presumably overlapped in time.</p>
<p>(7) The authors propose a weighted integration model, referring in the Discussion to dynamics and an integration rate. They do show that in the PolyOrig case, the top stream bias is highest and the monophonic model gives the best prediction, while in the PolyInv case, the top stream bias is weaker and the polyphonic model provides the best prediction. However, that doesn't seem to say anything about the temporal rate of integration, just the degree, which could be fixed over the whole stimulus. Relatedly, the terms &quot;strong attention bias&quot; and &quot;weak attention bias&quot; in Highlight 4 might give the impression of different attention modes for a given listener, or perhaps different types of listeners, but this seems to be shorthand for how attention is allocated for different types of stimuli (namely those that have or have not had their voices reversed).</p>
<p>(8) Another aspect of the presentation relating to temporal dynamics is that in places (e.g., Highlight 1), the authors suggest they are tracking attention dynamically. However, as acknowledged in the Discussion, neither the behavioural nor neural measure of attentional bias are temporally resolved. The measures indicate that on average participants attend more to the higher line (less so when it formed the lower line in the original composition).</p>
<p>(9) It is not clear whether the sung-back data were analysed (and if not why participants were asked to sing the melody back rather than just listen to the two components and report which they thought was the melody). It is also not stated whether the order in which the high and low voices were played back was randomised. If not, response biases or memory capacity might have affected the behavioural attention data.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108767.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this paper, Winchester and colleagues investigated melodic perception in natural music listening. They highlight the central role of attentional processes in identifying one particular stream in polyphonic material, and propose to compare several theoretical accounts, namely (1) divided attention, (2) figure-ground separation, and (3) stream integration. In parallel, the authors compare the relative strength of exogenous attentional effects (i.e., salience) produced by two common traits of melodies: high-pitch (compared to other voices), and attractive statistics. To ensure the generalisability of their results to real-life listening contexts, they developed a new uninstructed listening paradigm in which participants can freely attend to any part of a musical stimulus.</p>
<p>Major strengths and weaknesses of the methods and results:</p>
<p>(1) Winchester and colleagues capitalized on previous attention decoding techniques and proposed an uninstructed listening paradigm. This is an important innovation for the study of music perception in ecological settings, and it is used here to investigate the spontaneous attentional focus during listening. The EEG decoding results obtained are coherent with the behavioral data, suggesting that the paradigm is robust and relevant.</p>
<p>(2) The authors first evaluate the relative importance of high-pitch and statistics in producing an attentional bias (Figure 2). Behavioral results show a clear pattern, in which both effects are present, with a dominance of the high-pitch one. The only weakness inherent to this protocol is that behavioral responses are measured based on a second presentation of short samples, which may induce a different attentional focus than in the first uninstructed listening.</p>
<p>(3) Then, the analyses of EEG data compare the decoding results of each melody (the high or low voice, and with &quot;richer&quot; or &quot;poorer&quot; statistics), and show a similar pattern of results. However, this report leaves open the possibility of a confounding factor. In this analysis, a TRF decoding model is first trained based on the presentation of monophonic samples, and it is later used to decode the envelope of the corresponding melodies in the polyphonic scenario. The fitting scores of the training phase are not reported. If the high-pitch or richer melodies were to produce higher decoding scores during monophonic listening (due to properties of the physiological response, or to perceptual processes), a similar difference could be expected during polyphonic listening. To capture attentional biases specifically, the decoding scores in the polyphonic conditions should be compared to the scores in the monophonic conditions, and attention could be expected to increase the decoding of the attended stream or decrease the unattended one.</p>
<p>(4) Then, Winchester and colleagues investigate the processing of melodic information by evaluating the encoding of melodic surprise and uncertainty (Figure 3). They compare the surprise and uncertainty estimated from a monophonic or a polyphonic model (Anticipatory Music Transformer), and analyse the data with a CCA analysis. The results show a double dissociation, where the processing of melodies with a strong attentional bias (high-pitch, rich statistics) is better approximated with a monophonic model, while a polyphonic model better classifies the other melodies. While this global result is compelling, it remains a preliminary and intriguing finding, and the manuscript does not further investigate it. As it stands, the result appears more like a starting point for further exploration than a definitive finding that can support strong theoretical claims. First, it could be complemented by a comparison of the encoding of individual melodies (e.g., AMmono high-voice vs AMmono low-voice, in PolyOrig and PolyInv conditions) to highlight a more direct correspondence with the previous results (Figure 2) and allow a more precise interpretation. Second, additional analyses or experiments would be needed to unpack this result and provide greater explanatory power. Additionally, the CCA analysis is not described in the method. The statistical testing conducted on this analysis seems to be performed across the 250 repetitions of the evaluation rather than across the 40 participants, which may bias the resulting p-values. Moreover, the choice and working principle of the Anticipatory Music Transformer are not described in the method. Overall, these results seem at first glance solid, but the missing parts of the method do not allow for full evaluation or replication of them.</p>
<p>An appraisal of whether the authors achieved their aims, and whether the results support their conclusions:</p>
<p>(1) Winchester and colleagues aimed at identifying the melodic stream that attracts attention during the listening of natural polyphonic music, and the underlying attentional processes. Their behavioral results confirm that high-pitched and attractive statistics increase melodic salience with a greater effect size of the former, as stated in the discussion. The TRF analyses of EEG data seem to show a similar pattern, but could also be explained by confounding factors. Next, the authors interpret the CCA results as the results of stream segregation when there is a high melodic salience, and stream integration when there are weaker attentional biases. These interpretations seem to be supported by the data, but unfortunately, no additional analyses or experiments have been conducted to further evaluate this hypothesis. The authors also acknowledge that their results do not show whether stream segregation occurs via divided attention or figure-ground separation. However, the lack of information about the music model used (Anticipatory Music Model) and the way it was set up raises some questions about its relevance and limits as a model of cognition (e.g. Is this transformer a &quot;better&quot; model of the listeners' expectations than the well-established IDyOM model, and why ?), and about the validity of those results.</p>
<p>(2) Overall, the authors achieved most of the aims presented in the introduction, although they couldn't give a more precise account of the attentional processes at stake. The interpretations are sound and not overstated, with the exception of potential confounding factors that could compromise the conclusions on the neural tracking of salient melodies (EEG results, Figure 2).</p>
<p>Impact of the work on the field, and the utility of the methods and data to the community:</p>
<p>The new uninstructed listening paradigm introduced in this paper will likely have an important impact on psychologists and neuroscientists working on music perception and auditory attention, enabling them to conduct experiments in more ecological settings. While the attentional biases towards melodies with high-pitch and attractive statistics are already known, showing their relative effect is an important step in building precise models of auditory attention, and allows future paradigms to explore more fine-grained effects. Finally, the stream segregation and integration shown with this paradigm could be important for researchers working on music perception. Future work may be necessary to identify the models (Markov chains, deep learning) and setup (data analysis, stimuli, control variables) that do or do not replicate these results.</p>
</body>
</sub-article>
</article>