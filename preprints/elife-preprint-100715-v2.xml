<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">100715</article-id>
<article-id pub-id-type="doi">10.7554/eLife.100715</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.100715.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories><title-group>
<article-title>Motor biases reflect a misalignment between visual and proprioceptive reference frames</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0131-850X</contrib-id>
<name>
<surname>Wang</surname>
<given-names>Tianhe</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
    <email>tianhewang@berkeley.edu</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5724-3028</contrib-id>
<name>
<surname>Morehead</surname>
<given-names>Ryan J</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jiang</surname>
<given-names>Amber</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Ivry</surname>
<given-names>Richard B</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
    <xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
    <contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3992-9023</contrib-id>
<name>
<surname>Tsay</surname>
<given-names>Jonathan S</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
        <xref ref-type="author-notes" rid="n1">*</xref>
    <email>xiaotsay2015@gmail.com</email>
</contrib>
    <aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01an7q238</institution-id><institution>Department of Psychology, University of California, Berkeley</institution></institution-wrap>, <city>Berkeley</city>, <country country="US">United States</country></aff>
    <aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01an7q238</institution-id><institution>Department of Neuroscience, University of California, Berkeley</institution></institution-wrap>, <city>Berkeley</city>, <country country="US">United States</country></aff>
    <aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mrxd33</institution-id><institution>School of Psychology, University of Leeds</institution></institution-wrap>, <city>Leeds</city>, <country country="GB">United Kingdom</country></aff>
    <aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Department of Psychology, Carnegie Mellon University</institution></institution-wrap>, <city>Pittsburgh</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ahmed</surname>
<given-names>Alaa A</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1596-342X</contrib-id><role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02ttsq026</institution-id><institution>University of Colorado Boulder</institution>
</institution-wrap>
<city>Boulder</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Behrens</surname>
<given-names>Timothy E</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
    <fn id="n1" fn-type="equal"><label>*</label><p>Equal contribution senior author</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: RI is a co-founder with equity in Magnetic Tides, Inc.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-10-04">
<day>04</day>
<month>10</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-11-07">
<day>07</day>
<month>11</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP100715</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-07-11">
<day>11</day>
<month>07</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-07-11">
<day>11</day>
<month>07</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.03.15.585272"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-10-04">
<day>04</day>
<month>10</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.100715.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.100715.1.sa4">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.100715.1.sa3">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.100715.1.sa2">Reviewer #2 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.100715.1.sa1">Reviewer #3 (Public review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.100715.1.sa0">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Wang et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Wang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-100715-v2.pdf"/>
<abstract><p>Goal-directed movements can fail due to errors in our perceptual and motor systems. While these errors may arise from random noise within these sources, they also reflect systematic motor biases that vary with the location of the target. The origin of these systematic biases remains controversial. Drawing on data from an extensive array of reaching tasks conducted over the past 30 years, we evaluated the merits of various computational models regarding the origin of motor biases. Contrary to previous theories, we show that motor biases do not arise from systematic errors associated with the sensed hand position during motor planning or from the biomechanical constraints imposed during motor execution. Rather, motor biases are primarily caused by a misalignment between eye-centric and body-centric representations of position. This model can account for motor biases across a wide range of contexts, encompassing movements with the right versus left hand, finger versus hand movements, visible and occluded starting positions, as well as before and after sensorimotor adaptation.</p>
</abstract>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01s5ya894</institution-id>
<institution>National Institute of Neurological Disorders and Stroke</institution>
</institution-wrap>
</funding-source>
<award-id>NS116883</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Figure 3 in the last version was repeated twice, and Figure 4 was missing.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Accurate movements are crucial for everyday activities, affecting whether a glass is filled or spilled, or whether a dart hits or misses the target. Some movement errors arise from sensorimotor noise, including visual noise regarding the location of targets and effectors<sup><xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c3">3</xref></sup>, planning noise introduced when issuing a motor command, and neuromuscular noise when executing a movement<sup><xref ref-type="bibr" rid="c4">4</xref>–<xref ref-type="bibr" rid="c6">6</xref></sup>.</p>
<p>In addition, some of these errors arise from systematic biases that vary across the workspace<sup><xref ref-type="bibr" rid="c7">7</xref>–<xref ref-type="bibr" rid="c10">10</xref></sup>. The origin of these biases remains controversial (<xref rid="fig1" ref-type="fig">Fig 1a</xref>): Whereas some studies postulate that motor biases stem from systematic distortions in perception<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c10">10</xref>–<xref ref-type="bibr" rid="c13">13</xref></sup>, others posit that biases originate from inaccurate motor planning and/or biomechanical constraints associated with motor execution<sup><xref ref-type="bibr" rid="c14">14</xref>–<xref ref-type="bibr" rid="c16">16</xref></sup>. In the following section, we provide an overview of current models of systematic motor biases as well as outline a novel hypothesis, setting the stage for a re-analysis of published data and presentation of new experimental results.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Different Causes of Motor Biases.</title>
<p><bold>(a)</bold> Motor biases may originate from biases in perceiving the initial hand position (proprioceptive bias), perceiving the location of the visual target (target bias), transforming positional information from visual to proprioceptive space (transformation bias), and/or biomechanical constraints during motor execution. Previous models attribute motor biases to errors originating from the distinct contributions of visual (b) and proprioceptive biases (c). (d) Our model attributes motor biases to a transformation error between visual and proprioceptive coordinate systems. (e) A visuo-proprioceptive map showing the matching error between proprioceptive and visual space (Wang et al (2020)). Participants matched the position their hand (tip of the arrow) from a random starting location to the position of a visual target (end of the arrow). The blue dot depicts an example of a visual target in the workspace, and the red arrow indicates the corresponding matched hand position. Participants were asked to maximize spatial accuracy rather than focus on speed. (f-h) Simulated motor bias functions predicted by four models. Top: Illustration of how each model yields a biased movement, with the example shown for a movement to the 135° target in panels g and h and for the 100° target in panel f (as there is no target bias at 135°). Grey bars in panel f, g, h indicate predicted bias for all targets and/or start position based on previous measurement of visual bias (f)<sup><xref ref-type="bibr" rid="c13">13</xref></sup>, and proprioceptive/transformation bias (g-h)<sup><xref ref-type="bibr" rid="c20">20</xref></sup>. Bottom: Simulated motor bias functions differ qualitatively in terms of the number of peaks and troughs. Note that the middle panel depicts two variants of a proprioception model.</p></caption>
<graphic xlink:href="585272v4_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Starting at the input side, motor biases may arise from systematic distortions in the representation of the perceived target position (<xref rid="fig1" ref-type="fig">Fig 1b</xref>). A prominent finding in the visual cognition literature is that the remembered location of a visual stimulus is biased towards diagonal axes<sup><xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup>. That is, the reported visual location of a stimulus is shifted towards the centroid of each quadrant. This bias does not depend on the method of response, as this phenomenon can be observed when participants point to a cued location or press a key to indicate the remembered location of a briefly presented visual target<sup><xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c13">13</xref></sup>. While this literature has emphasized that this form of bias arises from processing within visual working memory, it is an open question whether it contributes to goal-directed reaching when the visual target remains visible.</p>
<p>Another potential cause of motor biases stems from proprioception (<xref rid="fig1" ref-type="fig">Fig 1c</xref>). Systematic distortions in the perceived position of the hand<sup><xref ref-type="bibr" rid="c18">18</xref>–<xref ref-type="bibr" rid="c20">20</xref></sup> and/or joint position<sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup> can influence motor planning. For example, if the perceived starting position of the hand is leftwards of its true location, a reaching movement to a forward visual target would exhibit a rightward bias and a reaching movement to a rightward visual target would fall short<sup><xref ref-type="bibr" rid="c7">7</xref></sup>. A proprioceptive perceptual bias at the starting position has been reported to be the major source of bias in many previous studies.<sup><xref ref-type="bibr" rid="c18">18</xref>–<xref ref-type="bibr" rid="c22">22</xref></sup></p>
<p>Whereas the preceding models have considered how distortions of visual or proprioceptive space might, on their own, lead to reaching biases, reaching biases could also originate from a misalignment in the mapping between perceptual and motor reference frames. Based on classic theories of motor planning<sup><xref ref-type="bibr" rid="c23">23</xref></sup>, the start position and the target position are initially encoded in an eye-centric visual coordinate frame, and then transformed to representations in a body-centric proprioceptive coordinate frame within which the movement is planned (<xref rid="fig1" ref-type="fig">Fig 1d</xref>). Motor biases could arise from systematic distortions that occur during this visuo-proprioceptive transformation process<sup><xref ref-type="bibr" rid="c24">24</xref>–<xref ref-type="bibr" rid="c26">26</xref></sup>. Indeed, when participants are required to match the position of their unseen hand with a visual target, systematic transformation biases are observed across the workspace (<xref rid="fig1" ref-type="fig">Fig 1d</xref>; also see Methods)<sup><xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup>. In the current study, we develop a novel computational model to capture how these transformation biases should result in systematic motor biases during reaching.</p>
<p>On the output side, it has been posited that reaching biases could arise from biomechanical factors that impact movement execution<sup><xref ref-type="bibr" rid="c8">8</xref></sup>. Specifically, movements may be biased toward trajectories that minimize inertial resistance and/or energetic costs<sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c30">30</xref></sup>. For example, minimizing energy expenditure would result in biases towards trajectories that minimize resistive forces or changes in joint angles<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup>. Moreover, inaccuracies in the internal model of limb dynamics could produce systematic execution biases. For example, underestimating the weight of the limb would result in reaches that overshoot the target<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c32">32</xref></sup>.</p>
<p>To determine the origin of motor biases, we formalized four computational models to capture the potential sources described above. As detailed in the Results section, the models predict distinct motor bias patterns in a center-out reaching task (<xref rid="fig1" ref-type="fig">Fig 1f-h</xref>). While prior research has focused on the impact of individual sources (e.g., vision or proprioception) on the pattern of motor errors, these studies often entail a limited set of contexts (e.g., reaching behavior only when the start position is visible or only with the right hand). However, looking across studies, the task context can result in dramatically different motor bias patterns; indeed, when plotted in polar coordinates across the workspace, the bias functions range from having single peak<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c21">21</xref></sup> to quadruple peaks<sup><xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup>. This diversity underscores the absence of a comprehensive explanation for motor bias phenomena across different experimental designs and setups. Additionally, a notable limitation of earlier work is the reliance on small participant cohorts (n&lt;10) and a restricted number of targets (typically 8). The sensitivity of such experiments is limited in terms of their capacity to discriminate between models.</p>
<p>To better evaluate sources of motor bias during reaching, we report a series of experiments involving a range of contexts, designed to test predictions of the different models. We compared movements performed with the right or left hand, finger versus hand movements, under conditions in which the start position was either visible or not visible, and before and after implicit sensorimotor adaptation. To increase the power of model comparisons, we measured the motor bias function at a higher resolution (24 targets) and in a bigger sample size (n &gt;50 per experiment).</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Motor biases across the workspace</title>
<p>To examine the pattern of motor biases during goal-directed movements, participants performed a center-out reaching task with their right hand (<xref rid="fig2" ref-type="fig">Fig 2a</xref>). We ran two versions of the study in Experiment 1. In Exp 1a, we used an 8-target version similar to that used in most previous studies<sup><xref ref-type="bibr" rid="c7">7</xref>–<xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c21">21</xref></sup>. To obtain better resolution of the motor bias pattern, we also conducted a 24-target version in Exp 1b. Within each experiment, participants first performed the task without visual feedback to establish their baseline bias and then a block with veridical continuous feedback to examine how feedback influences their biases. Motor biases were calculated as the angular difference between the target and hand when the movement amplitude reached the target distance (<xref rid="fig2" ref-type="fig">Fig 2b</xref>), with a positive error indicating a counterclockwise bias and a negative error indicating a clockwise bias.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Motor biases across different experimental contexts.</title>
<p>(a) Lab-based experimental apparatus for Exps 1-2. (b) Vectors linking the start position to the average endpoint position when reach amplitude equaled the target radius (pink lines; Exp 1a). (c) Motor biases as a function of target location. The dots indicate the mean angular error across participants during the no-feedback block (pink) and veridical feedback block (grey). The pattern of motor bias was similar in Exp 1a (8-targets; left panel) and Exp 1b (24-targets; right panel), characterized by two peaks and two troughs. Error bars denote standard error. (d) Motor biases generated during left hand reaches (left), left-hand results when the data are mirror reversed across the vertical meridian (middle), and right-hand reaches (right). (e) Left: Mirror reversal of biases observed during left hand reaches are similar to biases observed with right hand reaches. Right: Difference in RMSE when the right-hand map is compared to the original left-hand map relative to when the right-hand map is compared to the mirror reversed left-hand map. Positive values indicate better data alignment when the left-hand data are mirror-reversed. (f) Correlation of the motor bias function between the no-feedback and feedback blocks is higher in the within-participant condition compared to the between-participant condition. Gray bars indicate the 25% and 75% quartiles. White dots indicate the median and horizontal lines indicate the mean. (g) Experimental setup for Exp 3. Participants were asked to make center-out reaching movements using a trackpad or mouse. These movements predominantly involve finger and wrist movements. (h) The workspace is presumed to be closer to the reference point (e.g., left shoulder) for finger/wrist movements (Exp 3) compared to that of arm movements (Exp 1-2). The transformation maps for the in-person and online spaces were simulated from the best-fit models in Exp 1 and Exp 2, respectively. (i) The pattern of motor biases in finger/wrist movements for 8-targets (left) and 24-targets (right).</p></caption>
<graphic xlink:href="585272v4_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Across the workspace, the pattern of motor biases exhibited a two-peaked function (<xref rid="fig2" ref-type="fig">Fig 2c</xref>) characterized by two peaks and two troughs. From the 8-target experiment, larger biases were observed for the diagonal targets (45°, 135°, 225°, 335°) compared to the cardinal targets (0°, 90°, 180°, 270°)<sup><xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c34">34</xref></sup>. In terms of direction, reaches to diagonal targets were biased toward the vertical axis, and reaches for cardinal targets were biased in the counterclockwise direction. This pattern is similar to what has been observed in previous studies for right-handed movements<sup><xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c9">9</xref></sup>. With the higher resolution in the 24-target experiment, we see that the peaks of the motor bias function are not strictly aligned with the diagonal targets but are shifted towards the horizontal axis. Moreover, the upward shift of the motor bias function relative to the horizontal line suggests that clockwise biases are more prevalent compared to counterclockwise biases across the workspace.</p>
</sec>
<sec id="s2b">
<title>Motor biases primarily emerge from a misalignment between visual and hand reference frames</title>
<p>We developed a series of models to capture how systematic distortions at different sensorimotor processing stages would cause systematic motor biases. Here we consider processing associated with the perceived position of the target, the perceived position of the arm/hand, and planning processes required to transform a target defined in visual space to a movement defined in arm/proprioceptive space. Biases could also arise from biomechanical constraints. Given that biomechanical biases are not easily simulated, we will evaluate this hypothesis experimentally (see below).</p>
<p>We implemented four single-source models to simulate the predicted pattern of motor bias for a center-out reaching task (<xref rid="fig1" ref-type="fig">Fig 1f-h</xref>; see Methods). Since the task permits free viewing without enforced fixation, we assume that participants shift their gaze to the visual target; as such, an eye-centric bias is unlikely. Nonetheless, prior studies have shown a general spatial distortion that biases perceived target locations toward the diagonal axes<sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup>. Interestingly, this bias appears to be domain-general, emerging not only for visual targets but also for proprioceptive ones<sup><xref ref-type="bibr" rid="c12">12</xref></sup>. We incorporated this diagonal-axis spatial distortion into a Target Bias model. This model predicts a four-peaked motor bias pattern (<xref rid="fig1" ref-type="fig">Fig 1f</xref>).</p>
<p>For the Proprioceptive Bias model, we considered two variants building on the core idea that the perceived starting position of the hand is distorted. The first variant is a Vector-Based model in which the motor plan is a vector pointing from the perceived hand position to the target<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c10">10</xref></sup>. The second variant is a Joint-Based model in which the movement is encoded as changes in the shoulder and elbow joint angles to move the limb from a start position to a desired target location<sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup> (See <xref rid="figs1" ref-type="fig">Fig S1</xref>). Importantly, both models predict a motor bias function with a single peak (<xref rid="fig1" ref-type="fig">Fig 1g</xref>). Taken together, models that focus on systematic distortions of perceptual information do not qualitatively capture the observed two-peaked motor bias function (<xref rid="fig2" ref-type="fig">Fig 2c</xref>).</p>
<p>The fourth model, the Transformation Bias model is based on the idea that the start and target positions are initially encoded in visual space and transformed into proprioceptive space for motor planning<sup><xref ref-type="bibr" rid="c23">23</xref></sup>. Motor biases may thus arise from a transformation error between these coordinate systems. Studies in which participants match a visual stimulus to their unseen hand or vice-versa provide one way to estimate this error<sup><xref ref-type="bibr" rid="c18">18</xref>–<xref ref-type="bibr" rid="c20">20</xref>,<xref ref-type="bibr" rid="c27">27</xref></sup>. Two key features stand out in these data: First, the direction of the visuo-proprioceptive mismatch is similar across the workspace: For right-handers using their dominant limb, the hand is positioned leftward and downward from each target. Second, the magnitude of the mismatch increases with distance from the body (<xref rid="fig1" ref-type="fig">Fig 1d</xref>). Using these two empirical constraints, we simulated a visual-proprioceptive error map (<xref rid="fig1" ref-type="fig">Fig. 1h</xref>) by applying a leftward and downward error vector whose magnitude scaled with the distance from each location to a reference point. This model predicts a two-peaked motor bias function (<xref rid="fig1" ref-type="fig">Fig 1h</xref> Bottom), a shape strikingly similar to that observed in Exps 1a and 1b.</p>
    <p>Note that the Proprioceptive Bias model and the Transformation Bias model tap into the same visuo-proprioceptive error map. The key difference between the two models arises in how this error influences motor planning. For the Proprioceptive Bias model, planning is assumed to occur in visual space. As such, the perceived position of the hand (based on proprioception) is transformed into visual space. This will introduce a bias in the representation of the start position. In contrast, the Transformation Bias model assumes that visually-based representations of the start and target positions need to be transformed into proprioceptive space for motor planning. As such, both positions are biased in the transformation process. In addition to differing in terms of their representation of the target, the error introduced at the start position is in opposite directions due to the direction of the transformation (see <xref rid="fig1" ref-type="fig">fig 1g-h</xref>).</p>
<p>To quantitatively compare the models, we fit each model with the data in Exp 1b at both the group and individual level. The Transformation Bias model provided a good fit of the two-peaked motor bias function (R<sup><xref ref-type="bibr" rid="c2">2</xref></sup>=0.84, <xref rid="fig3" ref-type="fig">Fig 3a</xref>, see <xref rid="tbls1" ref-type="table">Table S1</xref> for parameters). <xref rid="fig2" ref-type="fig">Fig 2h</xref> (top) shows the recovered visual-proprioceptive bias map based on the parameters of the Transformation Bias model when fit to the group-level reaching data in Exp 1b. The simulated results are very similar to the map measured empirically in a previous study<sup><xref ref-type="bibr" rid="c20">20</xref></sup> (<xref rid="fig1" ref-type="fig">Fig 1e</xref>). In contrast, the Target Bias and Proprioceptive Bias models provide poor fits (all R<sup>2</sup>&lt;0.18, <xref rid="fig3" ref-type="fig">Fig 3b</xref>). In terms of individual fits, the Transformation Bias model provided the best fit for most of the participants (48/56, <xref rid="fig3" ref-type="fig">Fig 3c</xref>). Thus, the model fitting results suggest that motor biases observed in reaching primarily originate from a transformation between visual and proprioceptive space.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>The pattern of motor biases is best explained by assuming systematic distortions in the perceived location of the target and the transformation between visual and proprioceptive coordinate frames.</title>
<p>(a) For single-source models, the pattern of motor biases in the no feedback block of Exp 1a (pink dots) is best fit by the Transformation Bias model. (b) The three input-based models cannot explain the two-peak motor bias function. (c) Considering only the four single-source models, the data overwhelmingly favored the Transformation Bias model (48 out of 56 participants). (d) A mixture model involving transformation and target biases (TR+TG) provides the best fit to the motor bias function in Exp 1b (top). (e) Model comparison using BIC in Exp 1b. ΔBIC values are provided by subtracting the BIC from the best performing model (i.e., the TR+TG model). A smaller ΔBIC signifies better model performance. (f) For the mixture models, the data for almost all of the individuals were best explained by the TR+TG model (50 out of 56). (g-i) Same as panels d–f, but for Experiment 3b. <xref rid="figs2" ref-type="fig">Fig. S2</xref> shows representative individuals whose data are best captured by different models.</p></caption>
<graphic xlink:href="585272v4_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>A second way to evaluate the models is to compare the motor bias functions for the left and right hands. The Proprioceptive and Transformation Bias models predict that the bias function will be mirror-reversed for the two hands whereas the Target Bias model predicts that the functions will be superimposed. We compared the functions for right and left hand reaches in Exp 2 using the 8-target layout. We found that the dissimilarity (RMSE) between the pattern of motor biases across two hands significantly decreased when the left-hand data are mirror-reversed compared to when the bias patterns are compared without mirror reversal (t(78) = 2.7, p = 0.008, <xref rid="fig2" ref-type="fig">Fig 2d-e</xref>). These results are consistent with the Transformation Bias model and provide further confirmation that the Target Bias model fails to provide a comprehensive account of reaching biases across both hands.</p>
<p>While the overall pattern of biases in the visuo-proprioceptive map is similar across individuals, there are subtle individual differences (see <xref rid="figs2" ref-type="fig">Fig S2</xref> for examples)<sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c20">20</xref></sup>. As such, we would anticipate that the motor bias function would also exhibit stable individual differences due to idiosyncrasies in the visuo-proprioceptive map. To examine this, we correlated the bias functions obtained from blocks in which we either provided no visual feedback or veridical endpoint feedback. The magnitude of the biases was attenuated when endpoint feedback was provided, likely because the feedback reduced the visuo-proprioceptive mismatch. Nonetheless, the overall pattern of motor bias was largely preserved, with the within-participant correlations (Exp 1a: <italic>r</italic><sub><italic>norm</italic></sub> = 0.999, Exp 1b: <italic>r</italic><sub><italic>norm</italic></sub> = 0.974) significantly higher than the averaged between-participant correlation in both Exp 1a (0.569) and Exp 1b (0.455) (<xref rid="fig2" ref-type="fig">Fig 2f</xref>).</p>
</sec>
<sec id="s2c">
<title>Target bias also contributes to the motor bias</title>
<p>In the preceding section, we considered each model in isolation, testing the idea that motor biases arise from a single source. However, the bias might originate from multiple sources. For example, there could be a distortion in both vision and proprioception, or a visuo-proprioceptive transformation that operates on distorted inputs. To address this, we evaluated hybrid models by combining the Target Bias model with the Proprioceptive or Transformation Bias models. Although theoretically plausible, we did not consider a hybrid of the Proprioceptive and Transformation Bias models since they directly conflict in terms of whether the start position is perceived visually or proprioceptively.</p>
<p>The hybrid model that combines the Transformation and Target Bias models (TR+TG model) provided an excellent fit of the motor bias pattern in Exp 1b (R<sup>2</sup>=0.973, <xref rid="fig3" ref-type="fig">Fig 3d</xref>). Based on a comparison of BIC values, this model not only outperforms the other hybrid models, but also significantly improved the fit compared to the Transformation Bias model alone (<xref rid="fig3" ref-type="fig">Fig 3e-f</xref>). These results are especially interesting in that the assumed target bias towards the diagonal axes has only been shown in studies in which perception was tested after the target had been extinguished. The current results suggest that this bias is also operative when the target remains visible, suggesting that the target bias may reflect a general distortion in how space is represented, rather than a distortion that arises as information is processed in visual working memory.</p>
<p>To further evaluate the TR+TG model, we examined its performance in explaining the motor bias function obtained in an on-line study (Exp 3) in which participants performed the center-out task by moving a finger across a trackpad. One major difference between the in-person and on-line setups is that the workspace is much smaller and closer to the body when participants use a trackpad (<xref rid="fig2" ref-type="fig">Fig 2g</xref>). As such, the magnitude of the motor biases generated by transformation errors should be smaller with the online setup (Exp 3) compared to the in-person setup (Exp 1-2; <xref rid="fig2" ref-type="fig">Fig 2h</xref>).</p>
<p>Consistent with the prediction of the TR+TG model, we found markedly smaller motor biases with the online setup (Exp 3) compared to the in-person setup (Exp 1) (<xref rid="fig2" ref-type="fig">Fig 2c</xref>). While the overall shape of the motor bias functions was similar across experiments, we observed two small peaks between 20° and 200° in Exp 3 that were not apparent in Exp 1. When we fit this function to single source models, the Target Bias model outperformed the Transformation Bias model. This suggests that when the movements are close to the body, target biases make a relatively stronger contribution to the motor biases compared to transformation biases. Nonetheless, the TR+TG model again provides the best fit to the motor bias function (R<sup>2</sup>=0.857, <xref rid="fig3" ref-type="fig">Fig 3h</xref>, see <xref rid="tbls1" ref-type="table">Table S1</xref> for best-fit parameter values), significantly outperforming all other alternatives (<xref rid="fig3" ref-type="fig">Fig 3i</xref>).</p>
</sec>
<sec id="s2d">
<title>Motor biases in movement distance</title>
<p>To this point, we have focused on how reaching biases are manifest in angular error. However, a complete model should also account for biases measured in terms of radial error (i.e., distance). To examine the source of both angular and radial errors, we conducted a center-out reaching task and instructed the participants to rapidly move their hand, attempting to terminate the movement directly on the target rather than slice through the target (<xref rid="fig4" ref-type="fig">Fig 4a</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Motor biases in both angular and distance dimensions originate from a misalignment between visuo-proprioceptive reference frames.</title>
<p>(a) KINARM apparatus for Exp 4. (b) Assuming that participants viewed the display from a fixed angle, we would expect a perceptual bias in depth perception<sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup>. (c) Model simulations for motor biases in movement extent. The Transformation Bias model predicts a two-peaked function for distance bias, while the Proprioceptive Bias and the Target Depth Bias (TGD) models predict a one-peaked function. (d) Participants exhibited a two-peaked bias function for reach angle and extent. (e) The hybrid Transformation Bias + Target Depth Bias (TR+TGD) provides a good fit to the data for both dimensions. (f-g) Model comparisons. The TR+ TGD model outperformed alternative models in terms of averaged ΔBIC (f) and model frequency (g).</p></caption>
<graphic xlink:href="585272v4_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We again simulated each model to generate predictions, now for both angular and radial errors. In its simplest form, the Target Bias model does not make specific predictions about distance biases given that prior work has focused exclusively on angular errors and lacked data on distance-related target biases. However, in our KINARM setup, participants likely viewed targets from a fixed angle (<xref rid="fig4" ref-type="fig">Fig 4a-b</xref>). This would introduce a perceptual bias in depth<sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup>. To account for this, we simulated a variant, the Target-Depth (TGD) Bias model. This model predicts single-peaked bias functions for angular and radial errors. The Proprioceptive Bias model makes a similar prediction. (Note: We used the vector-based version, as the joint-based model does not generate predictions for movement extent.) In contrast, The Transformation Bias model predicts a two-peaked bias pattern in both the angular and distance dimensions (<xref rid="fig4" ref-type="fig">Fig 4c</xref>).</p>
<p>The reaching biases exhibited two peaks when the error is plotted for both angle and extent (<xref rid="fig4" ref-type="fig">Fig 4d</xref>). We jointly fit the 2D bias data using the three single-source models and all corresponding mixture models (<xref rid="fig4" ref-type="fig">Fig 4e</xref>). Similar to our prior results, the Transformation Bias model was the best single-source model and the combination of the Target Bias and the Transformation Bias model outperformed all other models, best explained all participants (<xref rid="fig4" ref-type="fig">Fig 4g</xref>). Together, the data from all four experiments converge on the conclusion that motor biases are primarily driven by a misalignment between visuo-proprioceptive reference frames.</p>
</sec>
<sec id="s2e">
<title>Transformation bias model accounts for qualitative changes in the motor bias function</title>
<p>The Transformation Bias model assumes that, for normal reaching, both the start and the target positions are encoded in visual space before being transformed into proprioceptive space for motor planning. However, if the start position is not visible, then the sensed start position would be directly encoded in proprioceptive space (i.e., where the hand is positioned), bypassing the need for a transformation between coordinate frames. As such, biases arising from the transformation process would only arise when the input is limited to the perceived position of the visual target. When we simulated the scenario in which the start position is not visible, the Transformation Bias model predicts a single-peaked function (<xref rid="fig5" ref-type="fig">Fig 5a</xref> right), a qualitative change from the two-peaked function predicted when both the start position and target position are visible (<xref rid="fig5" ref-type="fig">Fig 5a</xref> left).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Motor bias pattern changes when the start position is not visible.</title>
<p>(a) Schematic showing the planned movement under the Transformation Bias model when the start position is either visible (left) or not visible (right). In the latter case, only the target position is transformed from visual to proprioceptive coordinates with the start position directly encoded in proprioceptive space. The TR+TG model now predicts a single-peaked motor bias function (lower row). (b) Consistent with this prediction, a two-peaked function is predicted when the start position is visible (as in Exp 1) and a single-peaked function is predicted when start position is not displayed. Data (pink dots) are from Vindras et al (2005).</p></caption>
<graphic xlink:href="585272v4_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To test this idea, we re-examined data from previous studies in which the participant’s hand was passively moved to a start position with no visual information given about the start location or hand position<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c21">21</xref></sup>. Strikingly, the motor bias function under this condition has only have one peak (<xref rid="fig5" ref-type="fig">Fig 5b</xref>). Thus, the transformation Bias model provides a novel account of the difference in motor biases observed when the start position is visible (Exp 1-3) compared to when it is not visible.</p>
<p>We note that the one-peaked motor bias function has previously been interpreted as evidence in support of a Proprioceptive Bias model (<xref rid="fig1" ref-type="fig">Fig 1g</xref>)<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c21">21</xref></sup>. We performed a model comparison on the data from one of these studies<sup><xref ref-type="bibr" rid="c10">10</xref></sup> and the TR+TG outperformed the Proprioceptive Bias model, as well as the Prop+TG models (ΔBIC=10.9). In addition, only the TR+P model can account for the asymmetry between clockwise and counterclockwise biases. In summary, these results suggest that motor biases when reaching from an unseen start position arise when the target position is transformed from visual to proprioceptive coordinates rather than from a proprioceptive bias impacting the sensed start position. Moreover, the TR+TG model provides a parsimonious account of the bias functions, independent of the visibility of the start position.</p>
<p>Another way to evaluate the Transformation Bias model is to perturb the position of the visual start position relative to the real hand position. Under this manipulation, a single peaked motor bias function is observed (<xref rid="figs3" ref-type="fig">Fig S3</xref>) <sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup>. Interestingly, the functions exhibit opposing phase shifts when the starting position is perturbed to the left versus to the right (<xref rid="figs3" ref-type="fig">Fig S3</xref>). This qualitative change in the motor bias function can again be successfully captured by the Transformation Bias model (<xref rid="figs3" ref-type="fig">Fig S3c</xref>). Taken together, these data provide strong evidence favoring the notion that motor biases originate from a misalignment between visuo-proprioceptive reference frames.</p>
</sec>
<sec id="s2f">
<title>Biomechanical models fail to account for motor biases</title>
<p>An alternative account of motor biases is that they arise from biomechanical constraints associated with upper limb movement. For example, movement kinematics have been explained in terms of cost functions that minimize energy expenditure and/or minimize jerk<sup><xref ref-type="bibr" rid="c37">37</xref></sup>. These constraints might result in an increase in endpoint error<sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup>. To evaluate the effect of biomechanical constraints on reach accuracy, we used a state-of-the-art biomechanical model of the upper limb, the MotorNet<sup><xref ref-type="bibr" rid="c38">38</xref></sup>, to simulate motor biases in a center-out reaching task. This model is composed of two bones and six muscle actuators, with the control policy generated by a recurrent neural network (<xref rid="fig6" ref-type="fig">Fig 6a</xref>, see Methods). While the model captures basic biomechanical constraints of the upper limb<sup><xref ref-type="bibr" rid="c38">38</xref></sup>, it produces a four-peaked angular bias function (<xref rid="fig6" ref-type="fig">Fig 6b</xref>). Thus, the model fails to capture the empirically observed two-peaked function. This simulation suggests that biomechanical constraints are unlikely to be a primary source of motor biases.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Biomechanical constraints are unlikely to be a primary source of motor biases.</title>
<p>(a) Schematic of the two-skeleton, 60-muscle effector used in MotorNet and how predictions concerning reaching biases were simulated. (b) The model predicts a four-peaked motor bias function for a center-out reaching task, at odds with the two-peaked functions observed in Exps 1-3. Grey lines denote single simulations. Black line denotes the group average across runs.</p></caption>
<graphic xlink:href="585272v4_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>As a second comparison of the TR+TG and Biomechanical Bias models, we examined how motor biases change after the sensorimotor map is recalibrated following a form of motor learning, implicit sensorimotor adaptation. Here we re-analyzed the data from previous experiments that had used a perturbation technique in which the visual feedback was always rotated by 15° from the target, independent of the hand position (<xref rid="fig7" ref-type="fig">Fig 7a</xref>, non-contingent clamped feedback<sup><xref ref-type="bibr" rid="c39">39</xref></sup>). Participants adapt to this perturbation, with subsequent reaches to the same target shifted in the opposite direction (<xref rid="fig7" ref-type="fig">Fig 7b</xref>), reaching an asymptote of around 20° and showing a robust aftereffect when the perturbation is removed. Participants are unaware of their change in hand angle in response to clamped feedback, reporting their perceived hand position to be close to the target<sup><xref ref-type="bibr" rid="c40">40</xref></sup>.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><title>The pattern of motor bias is preserved after implicit sensorimotor adaptation, consistent with the Transformation + Target Bias model.</title>
    <p>(a) Illustration of the clamped perturbation. Feedback cursor is offset by a fixed angle from the target, independent of the participant’s heading direction. (b) Time course of hand angle in response to clockwise or counterclockwise clamped feedback. Vertical lines demarcate the perturbation block which was preceded and followed by no-feedback baseline and washout phases, respectively (gray areas). Shaded area indicates standard error. (c) Predictions for the bias functions after adaptation for the TR+TG (top) and Biomechanical models (bottom). See text for details. The right column shows the predicted motor bias functions following adaptation in response to a clockwise (CW) or counterclockwise (CCW) clamp. (d) Motor bias functions before and after training in a CW (left) and a CCW (right) clamp. Data taken from Morehead et al. <sup><xref ref-type="bibr" rid="c39">39</xref></sup> and Kim et al. <sup><xref ref-type="bibr" rid="c41">41</xref></sup>; the height of the colored bars indicates the standard error for each data point. The best-fit lines for the TR+TG model are shown. (e) Parameter values to capture vertical and horizontal shifts in motor bias functions before and after training. The CW and CCW conditions both showed a significant vertical shift but no horizontal shift.</p></caption>
<graphic xlink:href="585272v4_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>For the TR+TG model, the transformation between visual and proprioceptive space depends on the perceived positions of the start and target locations in a visual-based reference space, one that remains <italic>unchanged</italic> before and after adaptation. We assume that adaptation has changed a sensorimotor map that is referenced after the transformation from visual to proprioceptive space. As such, the heading angle after adaptation for each target location is obtained by summing the motor biases for that target location and the extent of implicit adaptation. This would result in a vertical shift of the motor bias function (<xref rid="fig7" ref-type="fig">Fig 7c</xref> top).</p>
<p>In contrast, the biomechanical model predicts that motor biases will be dependent on the actual movement direction rather than the target location (e.g., a bias towards a movement that is energetically efficient). Since the mapping between a target location and its corresponding reach direction is rotated after adaptation, the motor bias pattern would also be rotated (<xref rid="fig7" ref-type="fig">Fig 7c</xref> bottom). As such, the biomechanical model predicts that the motor bias function will be shifted along both the horizontal and vertical axes.</p>
<p>To arbitrate between these models, we analyzed the data from two previous studies, looking at the bias function from no-feedback trials performed before (baseline) and after adaptation (washout)<sup><xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c41">41</xref></sup>. Consistent with the prediction of the TR+TG model, the motor bias function shifted vertically after adaptation (<xref rid="fig7" ref-type="fig">Fig 7d</xref>) but did not shift horizontally.</p>
<p>To quantitatively evaluate these results, we first fit the motor bias function during the baseline phase with the TR+TG model and fixed the parameters. We then examined the heading angles during the aftereffect phase by fitting two additional parameters, one that allowed the function to shift vertically (<italic>v</italic>) and the other to allow the function to shift horizontally (<italic>h</italic>). The TR+TG model predicts that only <italic>v</italic> will be different than zero; in contrast, the Biomechanical Bias model predicts that <italic>h</italic> and v will both be different than zero and should be of similar magnitude. The results clearly favored the TR+TG model (<xref rid="fig7" ref-type="fig">Fig 7d-e</xref>). The vertical shift in the bias functions was of a similar magnitude as the aftereffect, with the shift direction depending on the direction of the clamped feedback (v: CW: 12.5°; CCW: -12.2°, p&lt;0.001). In contrast, the best fitting value for <italic>h</italic> was not significantly different from zero in both conditions. These results are consistent with the hypothesis that visual representations are first transformed into proprioceptive space for motor planning, with the recalibrated sensorimotor map altering the trajectory selected to achieve the desired movement outcome.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>While motor biases are ubiquitous in goal-directed reaching movements, the origin of these biases has been the subject of considerable debate. We addressed this issue by characterizing these biases across a range of experimental conditions and evaluated a set of computational models derived to capture different possible sources of bias. Contrary to previous theories, our results indicate that motor biases do not stem from a distortion in the sensed position of the hand<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup> or from biomechanical constraints during movement execution<sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup>. Instead, motor biases appear to arise from systematic distortions in perceiving the location of the visual target and the transformation required to translate a perceived visual target into a movement described in proprioceptive coordinates.<sup><xref ref-type="bibr" rid="c24">24</xref>–<xref ref-type="bibr" rid="c26">26</xref></sup> Strikingly, our model successfully accounts for sensorimotor biases across a wide range of contexts, encompassing movements performed with either hand as well as with finger versus hand movements. Our model also accounts for the qualitative changes in the motor bias function that are observed when vision of the starting position of the hand is occluded, and when the sensorimotor map is perturbed following implicit adaptation.</p>
<p>While motor biases have been hypothesized to reflect a mismatch across perceptual and motor coordinate systems,<sup><xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c26">26</xref></sup> it is unclear what information is transformed and what reference frame is employed for motor planning. Interestingly, many previous studies posit that movement is planned in an eye-centric visual reference frame<sup><xref ref-type="bibr" rid="c42">42</xref>–<xref ref-type="bibr" rid="c44">44</xref></sup>. While the target can be directly perceived in this reference space, the start position of the hand would need to be transformed from a proprioceptive reference frame to a visual one. Systematic error in this transformation would mean that the start position of the hand is inaccurately represented in visual space, resulting in motor biases<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c25">25</xref></sup>. This idea underlies the Proprioceptive Bias models described in this paper.</p>
<p>In contrast to these models, our Transformation Bias model posits that movement is planned in a hand-centric proprioceptive reference frame. By this view, when both the target and start position are provided in visual coordinates, the sensorimotor system transforms these positions from visual space to proprioceptive space. Systematic error in this transformation process will result in motor biases. When vision of the start position is available, the Transformation Bias model successfully accounts for the two-peaked motor bias function (Exp 1). Even more compelling, the Transformation Bias model accounts for how the pattern of motor biases change when the visibility of the start position is manipulated. When the start position is occluded, the transformation from visual to proprioceptive space is only relevant for the target position since the start position of the hand is already represented in proprioceptive space. Here the model predicts a motor bias function with a single peak, a function that has been observed in previous studies<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c10">10</xref></sup>.</p>
<p>We note that there is a third scenario, one which both the start position and target position are provided in proprioceptive space. We predict that under this condition, motor biases originating from the visuo-proprioceptive transformation would completely disappear. Indeed, when the hand is passively moved first to the target location and then to the start position, subsequent reaches to the target do not show the signature of bias from a visuo-proprioceptive transformation.<sup><xref ref-type="bibr" rid="c12">12</xref></sup> Instead, the reaches exhibited a bias towards the diagonal axes, consistent with the predicted pattern if the sole source of bias is visual.</p>
<p>Why would a sensorimotor system exhibit inherent biases during the transformation process? We propose that these biases arise from two interrelated factors. First, these systems are optimally tuned for distinct purposes: A body-centric system predominantly uses proprioceptive and vestibular inputs to determine the orientation and position of the body in space, while an eye-centric system relies on visual inputs to interpret the layout of objects in the external world, representations that should remain stable even as the agent moves about in this environment.<sup><xref ref-type="bibr" rid="c45">45</xref>,<xref ref-type="bibr" rid="c46">46</xref></sup> Second, these sensory systems consistently receive information with very different statistical distributions<sup><xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c47">47</xref></sup>, perhaps because of these distinct functions. For example, visual inputs tend to cluster around the principal axes (horizontal and vertical) <sup><xref ref-type="bibr" rid="c48">48</xref>,<xref ref-type="bibr" rid="c49">49</xref></sup>, whereas proprioceptive information during reaching is clustered around diagonal axes<sup><xref ref-type="bibr" rid="c50">50</xref></sup>. This is because these movements are often the least effortful and are the most frequently enacted directions of movement.<sup><xref ref-type="bibr" rid="c51">51</xref></sup>. The differences in computational goals and input distributions might have led to natural divergences in how each system represents space<sup><xref ref-type="bibr" rid="c52">52</xref></sup>, and consequently, result in a misalignment between the reference frames.</p>
<p>The Transformation Bias model addresses how biases arise when the information is passed along from a visual to a proprioceptive reference frame. However, the results indicate that another source of bias originates from a distortion within the visual reference frame itself, manifesting as an attractive bias towards the diagonal axes. Thus, the best fitting model posits two sources of bias, one related to the representation of the visual target and a second associated with the transformation process. This hybrid Transformation + Target Bias model outperformed all single-source and hybrid models, providing an excellent fit of the behavioral data across a wide variety of contexts.</p>
<p>What might be the source of the visual bias in the perceived location of the target? In the perception literature, a prominent theory has focused on the role of visual working memory account based on the observation that in delayed response tasks, participants exhibit a bias towards the diagonals when recalling the location of visual stimuli<sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c53">53</xref></sup>. Underscoring that the effect is not motoric, this bias is manifest regardless of whether the response is made by an eye movement, pointing movement, or keypress<sup><xref ref-type="bibr" rid="c17">17</xref></sup>. However, this bias is unlikely to be dependent on a visual input as similar diagonal bias is observed when the target is specified proprioceptively via the passive displacement of an unseen hand<sup><xref ref-type="bibr" rid="c12">12</xref></sup>. Moreover, as shown in the present study, a diagonal bias is observed even when the target is continuously visible. Thus, we hypothesize that the bias to perceive the target towards the diagonals reflects a more general distortion in spatial representation rather than being a product of visual working memory.</p>
<p>Other forms of visual bias may influence movement. Depth perception biases could contribute to biases in movement extent<sup><xref ref-type="bibr" rid="c54">54</xref>,<xref ref-type="bibr" rid="c55">55</xref></sup>. Visual biases towards the principal axes have been reported when participants are asked to report the direction of moving targets or the orientation of an object<sup><xref ref-type="bibr" rid="c56">56</xref>,<xref ref-type="bibr" rid="c57">57</xref></sup>. However, the predicted patterns of reach biases do not match the observed biases in the current experiments. We also considered a class of eye-centric models in which participants overestimate the radial distance to a target while maintaining central fixation<sup><xref ref-type="bibr" rid="c54">54</xref>,<xref ref-type="bibr" rid="c55">55</xref></sup>. At odds with this hypothesis, participants undershot rightward targets when we measured the radial bias in Exp 4. The absence of these other distortions of visual space may be accounted for by the fact that we allowed free viewing during the task.</p>
<p>Our data suggest that biomechanical factors do not significantly impact motor biases. We provided several lines of evidence suggesting the biomechanical factors have minimal influence on the pattern of motor biases. For example, it is hard to envision a biomechanical model that would account for the qualitative change in the bias function when the start position was visible (two-peak function) to when it was hidden (one-peak function). More directly, simulations with a state-of-the-art biomechanical model produced motor bias patterns that did not resemble the empirical results.</p>
<p>We also evaluated biomechanical contributions to motor biases by examining the bias pattern observed before and after implicit sensorimotor adaptation. We assume that adaptation mainly modifies a sensorimotor map<sup><xref ref-type="bibr" rid="c58">58</xref></sup> but has a relatively smaller influence on a visuo-proprioceptive map<sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c40">40</xref>,<xref ref-type="bibr" rid="c59">59</xref></sup>. That is, adaptation may change the mapping between a target represented in the proprioceptive space and the motor commands required to reach that location. Given that a biomechanical model assumes that motor biases are associated with the direction of a movement, this model would predict that the pattern of motor biases would be distorted by implicit motor adaptation. At odds with this prediction, the pattern of motor biases remained unchanged after adaptation, a result consistent with the Transformation Bias model.</p>
<p>Nonetheless, the current study does not rule out the possibility that biomechanical factors may influence motor biases in other contexts. Biomechanical constraints may have had limited influence in our experiments due to the relatively modest movement amplitudes used and minimal interaction torques involved. Moreover, while we have focused on biases that manifest at the movement endpoint, biomechanical constraints might introduce biases that are manifest in the movement trajectories.<sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup> Future studies are needed to examine the influence of context on reaching biases.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>For the lab-based study (Exp 1, 2), 236 undergraduate students (age: 18-24) were recruited from University of California, Berkeley. For the online study (Exp 3), 183 young adult participants (age: 18-30) were recruited via Prolific, a website designed to recruit participants for online behavioral testing. All participants were right-handed as assessed by the Edinburgh handedness test<sup><xref ref-type="bibr" rid="c60">60</xref></sup> with normal or corrected-to-normal vision. Each participant was paid $20/h. The protocol was approved by the institutional review board at the University of California Berkeley.</p>
</sec>
<sec id="s4b">
<title>Procedure</title>
<sec id="s4b1">
<title>Experiments 1a, 1b, and 2</title>
<p>Experiments 1a, 1b, and 2 were conducted in the lab. Participants performed a center-out reaching task, holding a digitizing pen in the right or left hand to make horizontal movements on a digitizing tablet (49.3cm x 32.7cm, sampling rate= 100 Hz; Wacom, Vancouver, WA). The stimuli were displayed on a 120 Hz, 17-in. monitor (Planar Systems, Hillsboro, OR), which was mounted horizontally above the tablet (25 cm), to preclude vision of the limb. The experiment was controlled by custom software coded in MATLAB (The MathWorks, Natick, MA), using Psychtoolbox extensions, and run on a Dell OptiPlex 7040 computer (Dell, Round Rock, TX) with Windows 7 operating system (Microsoft Co., Redmond, WA).</p>
<p>Participants made reaches from the center of the workspace to targets positioned at a radial distance of 8 cm. The start position and target location were indicated by a white annulus (1.2 cm diameter) and a filled blue circle (1.6 cm), respectively. Vision of the hand was occluded by the monitor, and the lights were extinguished in the room to minimize peripheral vision of the arm. Feedback, when provided, was in the form of a 4 mm white cursor that appeared on the computer monitor, aligned with the position of the digitizing pen.</p>
<p>To start each trial, the participant moved the cursor to the start circle (5mm diameter). After maintaining the cursor within the start circle for 500 ms, a target appeared at one of the target locations. The participant was instructed to make a rapid slicing movement through the target. We did not impose any reaction time guidelines, allowing the participant to set their own pace to initiate the movement. On no-feedback trials, the cursor was blanked when the hand left the start circle, and the target was extinguished once the radial distance of the movement reached the target distance (8 cm). On feedback trials, the cursor was visible throughout the movement until the movement amplitude reached 8 cm; at that point, its position was frozen for 1 s, providing feedback of the accuracy of the movement (angular position with respect to the target). After this interval, the target and cursor were extinguished.</p>
<p>At the end of both the no-feedback and feedback trials, a white ring appeared denoting the participant’s radial distance from the start position. This ring was displayed to guide the participant back to the start position without providing angular information about hand position. Once the participant moved within 2 cm of the start position, the ring was extinguished, and a veridical cursor appeared to allow the participant to move their hand to the start position. If the amplitude of the hand movement did not reach the target (&lt;8 cm radial distance) within 300 ms, the message “too slow” would be displayed for 500 ms before the white ring appeared.</p>
<p>For Exp 1a and Exp 2, there were 8 target locations, evenly spaced in 45° increments around the workspace (primary axes and main diagonals). For Exp 1b, there were 24 target locations, evenly spaced in 15° increments. Each experiment consisted of a no-feedback block followed by a feedback block. There were 5 trials per target (40 trials total) for each block in the Exps 1a. There were 4 trials per target (96 trials total) in Exp 1b.</p>
</sec>
<sec id="s4b2">
<title>Experiments 3a and 3b</title>
<p>Exps 3a and 3b were conducted using our web-based experimental platform (Tsay et al., 2021). Participants made center-out movements by controlling a cursor with the trackpad on their personal computers. It was not possible to occlude vision of the hand. However, since the visual stimulus was presented on a vertical monitor and the hand movement was in the horizontal plane, we assume vision of the hand was limited to the periphery (based on observations that the eyes remain directed to the screen during the trial). The size and position of visual stimuli were scaled based on each participant’s screen size (height = 239.6 ± 37.7 mm, width = 403.9 ± 69.5 mm). The experiment was controlled by custom software written with JavaScript and presented on Google Chrome. Data were collected and stored using Google Firebase.</p>
<p>The procedure was designed to mimic the lab-based experiments. On each trial, the participant made a center-out planar movement from the start position to a visual target. A white annulus (1% of screen height in diameter, 0.4 cm on average) indicated the start position, and a blue circle (1% of screen height in diameter) indicated the target location. The radial distance of the target from the start position was 40% of the screen height (5 cm on average). At the beginning of each trial, participants moved the cursor (0.6% of the screen height in diameter) to the start position, located at the center of their screen. The cursor was only visible when its distance from the start position was within 20% of the screen height. After maintaining the cursor at the start position for 500 ms, the target appeared. The participant made a rapid slicing movement through the blue target. As in the online experiments, there were feedback and no-feedback trials. For feedback trials, the cursor was visible until it reached the target distance, and then froze for 1 s at the target distance. On no-feedback trials, the cursor was extinguished after the hand exited the start position and the target disappeared once the radial distance of the movement reached the target distance. 500 ms after the end of the trial, the cursor became visible, repositioned at a random location within 10% of the screen height from the start position. The participant then moved the cursor to the start position to trigger the next trial.</p>
<p>There were 8 target locations in Exp 3a and 24 target locations in Exp 3b. As with the lab-based experiments, each experiment included a no-feedback block followed by a feedback block. We obtained larger data sets in the online studies: For each block, there were 20 trials/target (160 total trials for Exp 3a and 480 total trials for Exp 3b).</p>
</sec>
<sec id="s4b3">
<title>Experiment 4</title>
<p>To examine motor biases for both reach angle and extent, we performed a lab-based experiment with the KINARM system (BKIN Technologies). Participants performed a center-out reaching task, while holding onto the handle of a two-link robotic manipulandum. Vision of the arm and hand was occluded by a semi-silvered mirror that reflected the visual display from an LCD monitor mounted above the mirror (LG47LD452C, LG Electronics, 47 in., 1920 × 1080 pixel resolution). A black cloth was draped over the participant’s shoulder and arm, and the lights were extinguished in the room to minimize peripheral vision of the arm. The participant was seated in a comfortable chair with their forehead resting against a soft leather patch at the height of the monitor. Kinematic data were recorded at a sampling rate of 1,000 Hz, and with a spatial resolution of 0.1 mm. The experiment was controlled by a Dell OptiPlex 7040 computer (Dell, Round Rock, TX) running Dexterit-E software and coded in MATLAB Simulink™.</p>
<p>On each trial, a red target (1 cm diameter) appeared at one of 12 locations, spaced 30° apart and positioned 10 cm from the start location (green circle; 1 cm diameter). After maintaining their hand within the home position for 500 ms, the target appeared. The participant was instructed to make a rapid, straight movement to the target. Unlike in Experiment 1, they were told to attempt to stop on the target, holding the position until the target disappeared. The instructions emphasized that they should not make corrective movements but rather, that they try and reach the target with the initial movement. The target disappeared 1s after movement onset, defined as when the movement speed &lt;0.01cm/s. The robotic arm returned the hand to the central start position. No visual feedback was provided at any point during the experiment. Each block consisted of one reach to each of the 12 targets in randomized order, and the experiment was composed of 10 blocks.</p>
</sec>
<sec id="s4b4">
<title>Reanalysis of prior data sets</title>
    <p>Vindras et al <sup><xref ref-type="bibr" rid="c10">10</xref></sup>. This study used a design in which the participant did not see the start position of the movement. This was achieved by not included start position information in the visual display and passively moving the participant’s hand to a start position prior to each reach. Once positioned, a visual target would appear and the participant reached to that location. Across trials, there were two start positions, 12 target positions (spaced evenly by 30° around the workspace), and two target distances (6 and 12 cm). In modeling these data, we used the movement endpoint averaged across start positions and target distances.</p>
    <p>Morehead et al <sup><xref ref-type="bibr" rid="c39">39</xref></sup> &amp; Kim et al <sup><xref ref-type="bibr" rid="c41">41</xref></sup>. We re-analyzed the data from the 15° conditions of Exp 4 in Morehead et al <sup><xref ref-type="bibr" rid="c39">39</xref></sup> and Exps 1 and 2 in Kim et al <sup><xref ref-type="bibr" rid="c41">41</xref></sup>. These three experiments examined visuomotor adaptation using non-contingent clamped feedback. On perturbation trials, the feedback cursor was presented at the radial position of the hand but with a fixed 15° angular offset relative to the target. Participants were informed that the angular position was not contingent on their hand position and instructed to move directly to the target, ignoring the feedback. This method results in robust implicit adaptation, with the heading direction of the movement gradually shifting away from the target in the opposite direction of the cursor. Participants are unaware of this change in behavior <sup><xref ref-type="bibr" rid="c40">40</xref></sup>. In each experiment, there were three blocks: A no-feedback baseline block (10 trials/target), a clamped feedback block (60 trials/target), and a no-feedback washout block (10 trials/target).</p>
</sec>
</sec>
<sec id="s4c">
<title>Data analyses</title>
<p>Motor bias refers to the angular difference between the position of the hand and target when the hand reaches the endpoint target distance. Angular errors were plotted as a function of the target position with 0° corresponding to the rightward target (3 o’clock location) and 90° corresponding to the forward target. Positive bias values indicate a counterclockwise error, and negative values indicate a clockwise error.</p>
<p>To assess the similarity of the motor bias functions across different conditions, we calculated the normalized correlation coefficient as <inline-formula><inline-graphic xlink:href="585272v4_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. <italic>r<sub>data</sub></italic> is the Pearson correlation coefficient between the two motor bias functions. <italic>r</italic><sub><italic>max</italic></sub>is the correlation coefficient between the recorded motor bias function and the true (but unknown) underlying motor bias function from that condition. To calculate <italic>r</italic><sub><italic>max</italic></sub>, we used a method developed to measure the noise ceiling for EEG/fMRI data <sup><xref ref-type="bibr" rid="c61">61</xref></sup>:
<disp-formula>
<graphic xlink:href="585272v4_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>r</italic><sub><italic>half</italic></sub> is determined by splitting the data set (based on participants) into random halves and calculating the correlation coefficient between the first half and the second half of the data. We bootstrapped <italic>r</italic><sub><italic>half</italic></sub> by resampling the data 2000 times and used the average value. <italic>r</italic><sub><italic>max</italic></sub>is calculated separately for a pair of conditions and the smaller one is applied as the normalizer for <italic>r</italic><sub><italic>norm</italic></sub>.</p>
</sec>
<sec id="s4d">
<title>Models</title>
<p>To examine the source of motor bias, we considered five single-source models and three multiple-source models.</p>
<sec id="s4d1">
<title>Target Bias model</title>
<p>The Target Bias model postulates that movement biases arise because the perceived position of the visual target is systematically distorted (<xref rid="fig1" ref-type="fig">Fig 1b</xref>). Here we draw on the work of Huttenlocher et al<sup><xref ref-type="bibr" rid="c13">13</xref></sup>. In their study, a visual target was picked from an invisible circle, presented for 1 s and then blanked. The participant then indicated the remembered position of the target by pointing to a position on a circular digitizing pad. The results showed a bias towards the four diagonal directions (45°, 135°, 225°, 315°), with the magnitude of this bias increasing linearly as a function of the distance from the diagonals. As such, the maximum bias was observed for targets close to four cardinal target locations (0°, 90°, 180°, 270°), and the sign of bias flipped at the four cardinal target locations.</p>
<p>We used the shape of this function to model bias associated with the perception of the location of the visual targets. To obtain a continuous function, we assumed a transition zone around the cardinal targets, each with a half-width represented by the parameter <italic>a</italic> (<xref rid="fig1" ref-type="fig">Fig 1a</xref>), and the peak motor bias is represented by the parameter <italic>b</italic>. As such, the angular bias (<italic>y</italic>) at a target located at <italic>x</italic>° can be formalized as:
<disp-formula id="eqn1">
<graphic xlink:href="585272v4_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<disp-formula id="eqn2">
<graphic xlink:href="585272v4_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<p>This model has two free parameters (<italic>a</italic> and <italic>b</italic>). If participants directly reach to the perceived target location, their motor biases will directly reflect their visual biases.</p>
</sec>
<sec id="s4d2">
<title>Vector-based Proprioceptive Bias model</title>
<p>Vindras et al. <sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c10">10</xref></sup> proposed a model in which movement biases result from a misperception in estimating the initial position of the hand (<xref rid="fig1" ref-type="fig">Fig 1c</xref>). Specifically, it has been shown that the perceived position of the hand when placed near the center of the workspace is biased towards the ipsilateral side and away from the body <sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c27">27</xref></sup>). Assuming that the planned movement is formed by a vector pointing from the sensed hand position to the visual target position, this proprioceptive distortion will result in systematic motor biases around the workspace. For example, for the target at 90°, misperceiving the initial position of the right hand to the right of the start position will result in a movement that is biased in the counterclockwise (leftward) direction.</p>
<p>To simulate this Proprioceptive Bias model, we assumed the participants perceived the start position (0, 0) as a rightward bias away from the midline position, defining a proprioceptive error vector (<italic>x</italic><sub><italic>e</italic></sub>, <italic>y</italic><sub><italic>e</italic></sub>). For a target i at [<italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>], the motor plan is a vector [<italic>x</italic><sub><italic>i</italic></sub> − <italic>x</italic><sub><italic>e</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub> − <italic>y</italic><sub><italic>e</italic></sub>]. From this, we calculated the angular difference between the motor plan vector and the target position to generate the motor bias for each target. The two free parameters in this model are [<italic>x</italic><sub><italic>e</italic></sub>, <italic>y</italic><sub><italic>e</italic></sub>].</p>
</sec>
<sec id="s4d3">
<title>Joint-based Proprioceptive Bias model</title>
<p>Reaching movements may also be planned in joint coordinates rather than the hand (endpoint) position <sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup>. Based on this hypothesis, motor biases could come about if there is a misperception of the initial elbow and shoulder joint angles. To implement a Joint-Based Proprioceptive Bias model, we represent the length of the forearm and upper arm as <italic>l</italic>1 and <italic>l</italic>1, respectively. We denote the initial angles of the shoulder and elbow joints as θ<sub>0</sub> and φ<sub>0</sub>, respectively, and their associated perceived error as θ<sub><italic>e</italic></sub>, and φ<sub><italic>e</italic></sub> (See <xref rid="figs1" ref-type="fig">Fig S1</xref>).</p>
<p>By setting the origin of the coordinate system for the right shoulder at <italic>P<sub>0</sub></italic> (0, 0), the hand can be represented as:
<disp-formula id="eqn3">
<graphic xlink:href="585272v4_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
For a fixed position in the workspace, there will be a unique solution pair for θ and φ (π &gt; φ &gt; θ &gt; 0), should a solution exist. To calculate the required change in joint angle to reach a visual target, we assumed that the system plans a movement based on the perceived hand position:
<disp-formula id="eqn4">
<graphic xlink:href="585272v4_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Then we solve the following equation to decide the proper σθ<sub><italic>i</italic></sub> and σφ<sub><italic>i</italic></sub> that transfer the hand from the start position to a target i at [<italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>]:
<disp-formula id="eqn5">
<graphic xlink:href="585272v4_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We calculated the real movement direction based on the real hand position:
<disp-formula id="eqn6">
<graphic xlink:href="585272v4_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We compare the direction of σ<italic>h</italic><sub><italic>i</italic></sub> and the target direction to calculate the motor bias. For simplicity, we assume <italic>l</italic>1= <italic>l</italic>2=24 cm.<sup><xref ref-type="bibr" rid="c62">62</xref></sup> The four free parameters in this model are θ<sub>0</sub>, φ<sub>0</sub>, θ<sub><italic>e</italic></sub>, and φ<sub><italic>e</italic></sub></p>
</sec>
<sec id="s4d4">
<title>Transformation Bias model</title>
<p>The Transformation Bias model proposes attributes motor biases to systematic errors that arise during the transformation from a visual to proprioceptive-based reference frame. To implement this model, we refer to an empirically derived visuo-proprioceptive error map from a data set that sampled most of reachable space (<xref rid="fig1" ref-type="fig">Fig 1d</xref>, <sup><xref ref-type="bibr" rid="c20">20</xref></sup>). Specifically, in that study, participants were asked to move their unseen hand from a random start position to a visual target. Rather than require a discrete reaching movement, they were told to continuously adjust their hand position, focusing on accuracy in aligning the hand with the target. The direction of the error was relatively consistent across targets, with the final hand position shifted to the right and undershooting the target. The magnitude of these biases increased as the radial extent of the limb increased. This basic pattern has been observed across studies using different visuo-proprioceptive matching methods<sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c59">59</xref>,<xref ref-type="bibr" rid="c63">63</xref></sup>.</p>
<p>The matching errors provide an empirical measure of the transformation from a visual reference frame to a proprioceptive reference frame. To model these data, we defined a transformation error vector, [<italic>x</italic><sub><italic>e</italic></sub>,</p>
<p><italic>y</italic><sub><italic>e</italic></sub>], whose direction is fixed across space. We then defined a &quot;reference position&quot; with a coordinate of [<italic>x</italic><sub><italic>r</italic></sub>, <italic>y</italic><sub><italic>r</italic></sub>]. For upper-limb movements, this reference position is often considered to be positioned around the shoulder.<sup><xref ref-type="bibr" rid="c64">64</xref></sup> The transformation error vector at position <italic>i</italic> is scaled by its Euclidean distance (<italic>d</italic>) to the referent position:
<disp-formula id="eqn7">
<graphic xlink:href="585272v4_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<disp-formula id="eqn8">
<graphic xlink:href="585272v4_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<p>Movements towards a target i is planned via the vector connecting the start position to the target in proprioceptive space, denoted as:
<disp-formula id="eqn9">
<graphic xlink:href="585272v4_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>T</italic><sub>0</sub> is the transformation vector at the start position, which is set as [0,0]. Motor bias is calculated as the angular difference between the motor plan and the target. The four free parameters in the Transformation Bias model are <italic>x</italic><sub><italic>e</italic></sub>, <italic>y</italic><sub><italic>e</italic></sub>, <italic>x</italic><sub><italic>r</italic></sub>, <italic>y</italic><sub><italic>r</italic></sub>.</p>
</sec>
<sec id="s4d5">
<title>Visual Depth Bias model</title>
<p>Reaching movements were made with the KINARM system in Exp 4. With this system, the targets appear at an oblique angle (<xref rid="fig4" ref-type="fig">Fig 4a</xref>) and this may introduce perceptual biases in depth (<italic>b</italic><sub><italic>y</italic>,</sub>) <sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup>
<disp-formula id="eqn9a">
<graphic xlink:href="585272v4_eqn9a.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Where c is a consistent bias applied to all targets, and <italic>k</italic> determines the specific bias for target <italic>i</italic> based on its position on the y-axis (<italic>y</italic><sub><italic>i</italic></sub>) (<xref rid="fig4" ref-type="fig">Fig 4b</xref>). We assumed that the same rule for visual depth bias would apply to both the target and start position.</p>
</sec>
<sec id="s4d6">
<title>Hybrid models</title>
<p>The four models described above each attribute motor biases to a single source. However, the bias might originate from multiple processes. To formalize this hypothesis, we considered three hybrid models, combining the Target Bias model with the two versions of the Proprioceptive Bias model and with the Transformation Bias model. We did not create a hybrid of the Proprioceptive and Transformation Bias models since they make different assumptions about the information used to derive the motor plan.</p>
</sec>
<sec id="s4d7">
<title>Proprioceptive Bias + Target Bias (P+TG) model</title>
<p>We also created two hybrid models, combining the Target Bias model with the Vector-Based and Joint-Based Proprioceptive Bias models. The Target Bias model is used to estimate systematic error in the perceived location of the target and the proprioceptive Bias models are used to estimate systematic error in the perceived position of the hand at the start position. For these models, we calculated the biases from two models separately and then added them together:
<disp-formula id="eqn10">
<graphic xlink:href="585272v4_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>b</italic><sub><italic>i</italic></sub> refers to the bias at target i.</p>
</sec>
<sec id="s4d8">
<title>Transformation Bias + Target Bias (TR+TG) model</title>
<p>To simulate the TR+TG model, the representation of the visual target is first determined based on the Target Bias model. The biased visual target is then transformed into proprioceptive coordinates based on the Transformation Bias model (<xref rid="figs5" ref-type="fig">Figure S5</xref>).</p>
</sec>
<sec id="s4d9">
<title>Biomechanical model</title>
<p>To simulate motor biases that might arise from biomechanical constraints, we used MotorNet, a biomechanical model of the upper limb<sup><xref ref-type="bibr" rid="c38">38</xref></sup>. For upper limb reaching, MotorNet includes two bones and six muscle actuators (ReluMuscle), and a control policy dictated by simple recurrent neural networks using PyTorch. We created a policy network architecture with the recurrent layer of the model simulated as a single layer of 32 gated recurrent units (GRUs). The output layer of the model is a fully connected linear layer with sigmoid nonlinearity. The network receives input concerning target position and the current state of effectors to generate a transformation matrix as a policy.</p>
<p>We trained the model to perform point-to-point reaching where the start and target positions were randomized across the whole workspace. We then examined the directional biases produced by the model in a center-out reaching task. Following the procedure recommended by the authors of MotorNet, we trained the model using mini-batch gradient descent with Adam optimizer, a learning rate of 1000, and 6000 batches of size 32. For each batch, the policy network was trained on a random reach to minimize L1 loss between the effector’s fingertip trajectory and the goal trajectory. The model was evaluated with a center-out reaching task in which the targets were positioned at a radial distance of 10 cm from the center, mimicking the procedure in Exp 4. We used 36 target positions separated by 10°. Angular bias was measured as the angular difference between the simulated fingertip position at 80% of the trajectory distance and the intended target angle. Note that the model does not generate any prediction on movement extend in the current format. To estimate variability of the motor bias predictions, we trained and tested 50 independent instances of the model.</p>
</sec>
<sec id="s4d10">
<title>Model comparison</title>
<p>To compare the models, we fit each model with the data from Experiments 1b and 3b in which reaches were made to 24 targets. We used the fminsearchbnd function in MATLAB to minimize the sum of loglikelihood (LL) across all trials for each participant. LL were computed assuming normally distributed noise around each participant’s motor biases:
<disp-formula id="eqn11">
<graphic xlink:href="585272v4_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where x is the empirical reaching angle, b is the predicted motor bias by the model, c is motor noise, calculated as the standard deviation of x-b.</p>
<p>For model comparison, we calculated the BIC as follows:
<disp-formula id="eqn12">
<graphic xlink:href="585272v4_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>k</italic> is the number of parameters of the models. Smaller BIC values correspond to better fits. We report the sum of ΔBIC by subtracting the BIC value of the TR+TG model from all other models.</p>
<p>For illustrative purposes, we fit each model at the group level, pooling data across all participants to predict the group-averaged bias function.</p>
</sec>
<sec id="s4d11">
<title>Modeling motor bias after implicit sensorimotor adaptation</title>
<p>To examine how the motor bias function changes after visuomotor adaptation, we first used the TR+TG model to fit the motor bias function from a no-feedback baseline block tested prior to the introduction of the perturbation. We then used the best-fitted baseline model (<italic>TVb</italic>) to estimate the shift in the motor bias function from data obtained in a no-feedback aftereffect block following adaptation:
<disp-formula id="eqn13">
<graphic xlink:href="585272v4_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>b</italic>(<italic>i</italic>) is the motor bias at target i in the aftereffect; <italic>v</italic> and <italic>h</italic> indicate the vertical and horizontal shift respectively. To estimate distribution of <italic>v</italic> and <italic>h</italic>, we bootstrapped the subjects with repetition for 200 times and fitted the <italic>v</italic>, <italic>h</italic> based on the group average of each bootstrapped sample.</p>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="s8">
<title>Supplementary Information</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1.</label>
<caption><title>Schematic of a vector based and joint-based Proprioceptive Bias model.</title>
<p>Previous studies have considered two variants of the Proprioceptive Bias model. (a) A vector-based model in which the motor plan is a vector pointing from the perceived hand position to the target<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c10">10</xref></sup>. (b) A joint-Based model in which the movement is encoded as changes in the shoulder and elbow joint angles to move the limb from a start position to a desired location<sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup>. See Method, Models for details.</p></caption>
<graphic xlink:href="585272v4_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2.</label>
    <caption><title>Data and fits for four individuals.</title>
        <p>These participants were selected to represent cases in which the best fit was provided by each of the four single source models.</p></caption>
<graphic xlink:href="585272v4_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Figure S3.</label>
<caption><title>The Transformation Bias model can explain the motor bias functions when the visual information is shifted.</title>
<p>(a) In Sober and Sabes (2003)<sup><xref ref-type="bibr" rid="c22">22</xref></sup> participants performed center-out reaches to a visual target. To perturb the visual information, the start position was presented 6 cm to the left or right of the actual start position of the hand. (b) Participants showed a one-peaked motor bias functions with the shift-left and shift-right functions shifted in an antiphase relationship to one another. (c) These bias functions are quantitively captured by the Transformation Bias model.</p></caption>
<graphic xlink:href="585272v4_figs3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Figure S4.</label>
<caption><title>Model recovery.</title>
<p>The parameters in the three mixed models are recoverable. We simulated each model 50 times with all parameters randomly sampled from uniform distributions and then fit each simulated agent with all three models 200 times each. (a-c) The fitted parameters are very close to the ground truth. (d) Log-likelihood as a function of fitting iterations. Based on this curve, we determined that 150 iterations were sufficient given that the log-likelihood values were asymptotic at this point. (e) In most cases, the model fits recover the simulated model, with minimal confusion across the three models.</p></caption>
<graphic xlink:href="585272v4_figs4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Figure S5.</label>
<caption><title>Illustration of the TR+TG model.</title>
<p>(a) We assumed that participants are biased in their representation of the target position, following the Target Bias model. (b) The biased target position is transformed into proprioceptive space, following the Transformation Bias model. (c) The movement is planned in proprioceptive space.</p></caption>
<graphic xlink:href="585272v4_figs5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Table S1.</label>
<caption>
<title>Parameter estimates from best fits using the group-level data for the the TR+TG model from Exps 1b and 3b.</title>
<p>See Methods for description of each parameter. a. Participant moved on the trackpad in Exp 3b. We assumed the movement distance was 1 cm and scaled the parameters accordingly. b. The estimate of <bold><italic>y<sub>r</sub></italic></bold> is much smaller in Exp 3b compared to Exp 1b, suggesting the workspace in Exp 3b is closer to the body. This attenuates the average magnitude of the bias.</p>
</caption>
<graphic xlink:href="585272v4_tbls1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s7" sec-type="data-availability">
<title>Data availability statement</title>
<p>Data and code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/shion707/Motor-Bias">https://github.com/shion707/Motor-Bias</ext-link></p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank Zixuan Wang and Anisha Chandy for helpful discussions. We thank Anisha Chandy for data collection.</p>
</ack>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>Funding</title>
<p>RBI is funded by the NIH (grants NS116883 and NS105839). JST was supported by the NIH (F31NS120448).</p>
</sec>
</sec>
<ref-list>
<title>Reference</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burge</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ernst</surname>, <given-names>M. O.</given-names></string-name> &amp; <string-name><surname>Banks</surname>, <given-names>M. S</given-names></string-name></person-group>. <article-title>The statistical determinants of adaptation rate in human reaching</article-title>. <source>J. Vis</source>. <volume>8</volume>, <issue>20</issue>.<fpage>1</fpage>–<lpage>19</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tassinari</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Hudson</surname>, <given-names>T. E.</given-names></string-name> &amp; <string-name><surname>Landy</surname>, <given-names>M. S</given-names></string-name></person-group>. <article-title>Combining priors and noisy visual cues in a rapid pointing task</article-title>. <source>J. Neurosci</source>. <volume>26</volume>, <fpage>10154</fpage>–<lpage>10163</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Osborne</surname>, <given-names>L. C.</given-names></string-name>, <string-name><surname>Lisberger</surname>, <given-names>S. G.</given-names></string-name> &amp; <string-name><surname>Bialek</surname>, <given-names>W</given-names></string-name></person-group>. <article-title>A sensory source for motor variation</article-title>. <source>Nature</source> <volume>437</volume>, <fpage>412</fpage>– <lpage>416</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dideriksen</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Negro</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Enoka</surname>, <given-names>R. M.</given-names></string-name> &amp; <string-name><surname>Farina</surname>, <given-names>D</given-names></string-name></person-group>. <article-title>Motor unit recruitment strategies and muscle properties determine the influence of synaptic noise on force steadiness</article-title>. <source>J. Neurophysiol</source>. <volume>107</volume>, <fpage>3357</fpage>– <lpage>3369</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hamilton</surname>, <given-names>A. F.</given-names></string-name> <string-name><surname>de</surname> <given-names>C.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>K. E.</given-names></string-name> &amp; <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name></person-group> <article-title>The scaling of motor noise with muscle strength and motor unit number in humans</article-title>. <source>Exp. Brain Res</source>. <volume>157</volume>, <fpage>417</fpage>–<lpage>430</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dhawale</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>M. A.</given-names></string-name> &amp; <string-name><surname>Ölveczky</surname>, <given-names>B. P</given-names></string-name></person-group>. <article-title>The Role of Variability in Motor Learning</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>40</volume>, <fpage>479</fpage>–<lpage>498</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vindras</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Desmurget</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Prablanc</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Viviani</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>Pointing Errors Reflect Biases in the Perception of the Initial Hand Position</article-title>. <source>J. Neurophysiol</source>. <volume>79</volume>, <fpage>3290</fpage>–<lpage>3294</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gordon</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ghilardi</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Cooper</surname>, <given-names>S. E.</given-names></string-name> &amp; <string-name><surname>Ghez</surname>, <given-names>C</given-names></string-name></person-group>. <article-title>Accuracy of planar reaching movements. II. Systematic extent errors resulting from inertial anisotropy</article-title>. <source>Exp. Brain Res</source>. <volume>99</volume>, <fpage>112</fpage>–<lpage>130</lpage> (<year>1994</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ghilardi</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Gordon</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Ghez</surname>, <given-names>C</given-names></string-name></person-group>. <article-title>Learning a visuomotor transformation in a local area of work space produces directional biases in other areas</article-title>. <source>J. Neurophysiol</source>. <volume>73</volume>, <fpage>2535</fpage>–<lpage>2539</lpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vindras</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Desmurget</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Viviani</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>Error parsing in visuomotor pointing reveals independent processing of amplitude and direction</article-title>. <source>J. Neurophysiol</source>. <volume>94</volume>, <fpage>1212</fpage>–<lpage>1224</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holden</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Newcombe</surname>, <given-names>N. S.</given-names></string-name> &amp; <string-name><surname>Shipley</surname>, <given-names>T. F</given-names></string-name></person-group>. <article-title>Categorical biases in spatial memory: the role of certainty</article-title>. <source>J. Exp. Psychol. Learn. Mem. Cogn</source>. <volume>41</volume>, <fpage>473</fpage>–<lpage>481</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yousif</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Forrence</surname>, <given-names>A. D.</given-names></string-name> &amp; <string-name><surname>McDougle</surname>, <given-names>S. D</given-names></string-name></person-group>. <article-title>A common format for representing spatial location in visual and motor working memory</article-title>. <source>Psychon. Bull. Rev</source>. (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.3758/s13423-023-02366-3</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huttenlocher</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hedges</surname>, <given-names>L. V.</given-names></string-name>, <string-name><surname>Corrigan</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Crawford</surname>, <given-names>L. E</given-names></string-name></person-group>. <article-title>Spatial categories and the estimation of location</article-title>. <source>Cognition</source> <volume>93</volume>, <fpage>75</fpage>–<lpage>97</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goble</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Shimansky</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Sharma</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Dounskaia</surname>, <given-names>N. V</given-names></string-name></person-group>. <article-title>Directional biases reveal utilization of arm’s biomechanical properties for optimization of motor behavior</article-title>. <source>J. Neurophysiol</source>. <volume>98</volume>, <fpage>1240</fpage>–<lpage>1252</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alexander</surname>, <given-names>R. M</given-names></string-name></person-group>. <article-title>A minimum energy cost hypothesis for human arm trajectories</article-title>. <source>Biol. Cybern</source>. <volume>76</volume>, <fpage>97</fpage>–<lpage>105</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nishii</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Taniai</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>Evaluation of trajectory planning models for arm-reaching movements based on energy cost</article-title>. <source>Neural Comput</source>. <volume>21</volume>, <fpage>2634</fpage>–<lpage>2647</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kosovicheva</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Whitney</surname>, <given-names>D</given-names></string-name></person-group>. <article-title>Stable individual signatures in object localization</article-title>. <source>Curr. Biol</source>. <volume>27</volume>, <fpage>R700</fpage>– <lpage>R701</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rincon-Gonzalez</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Buneo</surname>, <given-names>C. A.</given-names></string-name> &amp; <string-name><surname>Helms Tillery</surname>, <given-names>S. I</given-names></string-name></person-group>. <article-title>The Proprioceptive Map of the Arm Is Systematic and Stable, but Idiosyncratic</article-title>. <source>PLoS One</source> <volume>6</volume>, <fpage>e25214</fpage>-(<year>2011</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Beers</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Sittig</surname>, <given-names>A. C.</given-names></string-name> &amp; <string-name><surname>Denier van der Gon</surname>, <given-names>J. J.</given-names></string-name></person-group> <article-title>The precision of proprioceptive position sense</article-title>. <source>Exp. Brain Res</source>. <volume>122</volume>, <fpage>367</fpage>–<lpage>377</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Accuracy of hand localization is subject-specific and improved without performance feedback</article-title>. <source>Sci. Rep</source>. <volume>10</volume>, <fpage>19188</fpage> (<issue>12/</issue><year>2020</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sober</surname>, <given-names>S. J.</given-names></string-name> &amp; <string-name><surname>Sabes</surname>, <given-names>P. N</given-names></string-name></person-group>. <article-title>Flexible strategies for sensory integration during motor planning</article-title>. <source>Nat. Neurosci</source>. <volume>8</volume>, <fpage>490</fpage>–<lpage>497</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sober</surname>, <given-names>S. J.</given-names></string-name> &amp; <string-name><surname>Sabes</surname>, <given-names>P. N</given-names></string-name></person-group>. <article-title>Multisensory integration during motor planning</article-title>. <source>J. Neurosci</source>. <volume>23</volume>, <fpage>6982</fpage>– <lpage>6992</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buneo</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Jarvis</surname>, <given-names>M. R.</given-names></string-name>, <string-name><surname>Batista</surname>, <given-names>A. P.</given-names></string-name> &amp; <string-name><surname>Andersen</surname>, <given-names>R. A</given-names></string-name></person-group>. <article-title>Direct visuomotor transformations for reaching</article-title>. <source>Nature</source> <volume>416</volume>, <fpage>632</fpage>–<lpage>636</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Soechting</surname>, <given-names>J. F.</given-names></string-name> &amp; <string-name><surname>Flanders</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Errors in pointing are due to approximations in sensorimotor transformations</article-title>. <source>J. Neurophysiol</source>. <volume>62</volume>, <fpage>595</fpage>–<lpage>608</lpage> (<year>1989</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tillery</surname>, <given-names>S. I.</given-names></string-name>, <string-name><surname>Flanders</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Soechting</surname>, <given-names>J. F</given-names></string-name></person-group>. <article-title>A coordinate system for the synthesis of visual and kinesthetic information</article-title>. <source>J. Neurosci</source>. <volume>11</volume>, <fpage>770</fpage>–<lpage>778</lpage> (<year>1991</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Flanders</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Soechting</surname>, <given-names>J. F</given-names></string-name></person-group>. <article-title>Frames of reference for hand orientation</article-title>. <source>J. Cogn. Neurosci</source>. <volume>7</volume>, <fpage>182</fpage>– <lpage>195</lpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jones</surname>, <given-names>S. A. H.</given-names></string-name>, <string-name><surname>Cressman</surname>, <given-names>E. K.</given-names></string-name> &amp; <string-name><surname>Henriques</surname>, <given-names>D. Y. P</given-names></string-name></person-group>. <article-title>Proprioceptive localization of the left and right hands</article-title>. <source>Exp. Brain Res</source>. <volume>204</volume>, <fpage>373</fpage>–<lpage>383</lpage> (18 <year>2009</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cressman</surname>, <given-names>E. K.</given-names></string-name> &amp; <string-name><surname>Henriques</surname>, <given-names>D. Y. P</given-names></string-name></person-group>. <article-title>Reach adaptation and proprioceptive recalibration following exposure to misaligned sensory input</article-title>. <source>J. Neurophysiol</source>. <volume>103</volume>, <fpage>1888</fpage>–<lpage>1895</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Balasubramanian</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Howe</surname>, <given-names>R. D.</given-names></string-name> &amp; <string-name><surname>Matsuoka</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>Task performance is prioritized over energy reduction</article-title>. <source>IEEE Trans. Biomed. Eng</source>. <volume>56</volume>, <fpage>1310</fpage>–<lpage>1317</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerside</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Courter</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Shadmehr</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Ahmed</surname>, <given-names>A. A</given-names></string-name></person-group>. <article-title>Slowing of movements in healthy aging as a rational economic response to an elevated effort landscape</article-title>. <source>J. Neurosci</source>. (<year>2024</year>) doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.1596-23.2024</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Soechting</surname>, <given-names>J. F.</given-names></string-name>, <string-name><surname>Buneo</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Herrmann</surname>, <given-names>U.</given-names></string-name> &amp; <string-name><surname>Flanders</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Moving effortlessly in three dimensions: does Donders’ law apply to arm movement?</article-title> <source>J. Neurosci</source>. <volume>15</volume>, <fpage>6271</fpage>–<lpage>6280</lpage> (<year>1995</year>).</mixed-citation></ref>
    <ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gordon</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ghilardi</surname>, <given-names>M. F.</given-names></string-name> &amp; <string-name><surname>Ghez</surname>, <given-names>C</given-names></string-name></person-group>. <article-title>Impairments of reaching movements in patients without proprioception. I. Spatial errors</article-title>. <source>J. Neurophysiol</source>. <volume>73</volume>, <fpage>347</fpage>–<lpage>360</lpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Slijper</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Richter</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Over</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Smeets</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Frens</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Statistics predict kinematics of hand movements during everyday activity</article-title>. <source>J. Mot. Behav</source>. <volume>41</volume>, <fpage>3</fpage>–<lpage>9</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van der Kooij</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Brenner</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>van Beers</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Schot</surname>, <given-names>W. D.</given-names></string-name> &amp; <string-name><surname>Smeets</surname>, <given-names>J. B. J</given-names></string-name></person-group>. <article-title>Alignment to natural and imposed mismatches between the senses</article-title>. <source>J. Neurophysiol</source>. <volume>109</volume>, <fpage>1890</fpage>–<lpage>1899</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Volcic</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fantoni</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Caudek</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Assad</surname>, <given-names>J. A.</given-names></string-name> &amp; <string-name><surname>Domini</surname>, <given-names>F</given-names></string-name></person-group>. <article-title>Visuomotor adaptation changes stereoscopic depth perception and tactile discrimination</article-title>. <source>J. Neurosci</source>. <volume>33</volume>, <fpage>17081</fpage>–<lpage>17088</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hibbard</surname>, <given-names>P. B.</given-names></string-name> &amp; <string-name><surname>Bradshaw</surname>, <given-names>M. F</given-names></string-name></person-group>. <article-title>Reaching for virtual objects: binocular disparity and the control of prehension</article-title>. <source>Exp. Brain Res</source>. <volume>148</volume>, <fpage>196</fpage>–<lpage>201</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Flash</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Hogan</surname>, <given-names>N</given-names></string-name></person-group>. <article-title>The coordination of arm movements: an experimentally confirmed mathematical model</article-title>. <source>J. Neurosci</source>. <volume>5</volume>, <fpage>1688</fpage>–<lpage>1703</lpage> (<year>1985</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Codol</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Michaels</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Kashefi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pruszynski</surname>, <given-names>J. A.</given-names></string-name> &amp; <string-name><surname>Gribble</surname>, <given-names>P. L</given-names></string-name></person-group>. <article-title>MotorNet, a Python toolbox for controlling differentiable biomechanical effectors with artificial neural networks</article-title>. <source>eLife</source> <volume>12</volume>, (<year>2024</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morehead</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Taylor</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Parvin</surname>, <given-names>D. E.</given-names></string-name> &amp; <string-name><surname>Ivry</surname>, <given-names>R. B</given-names></string-name></person-group>. <article-title>Characteristics of Implicit Sensorimotor Adaptation Revealed by Task-irrelevant Clamped Feedback</article-title>. <source>J. Cogn. Neurosci</source>. <volume>29</volume>, <fpage>1061</fpage>–<lpage>1074</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsay</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Parvin</surname>, <given-names>D. E.</given-names></string-name> &amp; <string-name><surname>Ivry</surname>, <given-names>R. B</given-names></string-name></person-group>. <article-title>Continuous reports of sensed hand position during sensorimotor adaptation</article-title>. <source>J. Neurophysiol</source>. <volume>124</volume>, <fpage>1122</fpage>–<lpage>1130</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>H. E.</given-names></string-name>, <string-name><surname>Morehead</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Parvin</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Moazzezi</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Ivry</surname>, <given-names>R. B</given-names></string-name></person-group>. <article-title>Invariant errors reveal limitations in motor correction rather than constraints on error sensitivity</article-title>. <source>Commun Biol</source> <volume>1</volume>, <fpage>19</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Batista</surname>, <given-names>A. P.</given-names></string-name>, <string-name><surname>Buneo</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>L. H.</given-names></string-name> &amp; <string-name><surname>Andersen</surname>, <given-names>R. A</given-names></string-name></person-group>. <article-title>Reach plans in eye-centered coordinates</article-title>. <source>Science</source> <volume>285</volume>, <fpage>257</fpage>–<lpage>260</lpage> (<year>1999</year>).</mixed-citation></ref>
    <ref id="c43"><label>43.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Blohm</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Khan</surname>, <given-names>A. Z.</given-names></string-name> &amp; <string-name><surname>Crawford</surname>, <given-names>J. D</given-names></string-name></person-group>. <chapter-title>Spatial Transformations for Eye–Hand Coordination</chapter-title>. In <source>Encyclopedia of Neuroscience</source> <person-group person-group-type="editor"><string-name><surname>Squire</surname><given-names>L. R.</given-names></string-name></person-group> <fpage>203</fpage>–<lpage>211</lpage> (<publisher-name>Elsevier</publisher-name>, <year>2009</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Henriques</surname>, <given-names>D. Y.</given-names></string-name>, <string-name><surname>Klier</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Lowy</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Crawford</surname>, <given-names>J. D</given-names></string-name></person-group>. <article-title>Gaze-centered remapping of remembered visual space in an open-loop pointing task</article-title>. <source>J. Neurosci</source>. <volume>18</volume>, <fpage>1583</fpage>–<lpage>1594</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Proske</surname>, <given-names>U.</given-names></string-name> &amp; <string-name><surname>Gandevia</surname>, <given-names>S. C</given-names></string-name></person-group>. <article-title>The proprioceptive senses: their roles in signaling body shape, body position and movement, and muscle force</article-title>. <source>Physiol. Rev</source>. <volume>92</volume>, <fpage>1651</fpage>–<lpage>1697</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Héroux</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Butler</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Robertson</surname>, <given-names>L. S.</given-names></string-name>, <string-name><surname>Fisher</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Gandevia</surname>, <given-names>S. C</given-names></string-name></person-group>. <article-title>Proprioception: a new look at an old concept</article-title>. <source>J. Appl. Physiol</source>. <volume>132</volume>, <fpage>811</fpage>–<lpage>814</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Daw</surname>, <given-names>N. D.</given-names></string-name> &amp; <string-name><surname>Maloney</surname>, <given-names>L. T</given-names></string-name></person-group>. <article-title>Human representation of visuo-motor uncertainty as mixtures of orthogonal basis distributions</article-title>. <source>Nat. Neurosci</source>. <volume>18</volume>, <fpage>1152</fpage>–<lpage>1158</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van den Berg</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Shin</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Chou</surname>, <given-names>W.-C.</given-names></string-name>, <string-name><surname>George</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Ma</surname>, <given-names>W. J.</given-names></string-name></person-group> <article-title>Variability in encoding precision accounts for visual short-term memory limitations</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source>. <volume>109</volume>, <fpage>8780</fpage>–<lpage>8785</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hahn</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Wei</surname>, <given-names>X.-X</given-names></string-name></person-group>. <article-title>A unifying theory explains seemingly contradictory biases in perceptual estimation</article-title>. <source>Nat. Neurosci</source>. (<year>2024</year>) doi:<pub-id pub-id-type="doi">10.1038/s41593-024-01574-x</pub-id>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mawase</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Lopez</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Celnik</surname>, <given-names>P. A.</given-names></string-name> &amp; <string-name><surname>Haith</surname>, <given-names>A. M</given-names></string-name></person-group>. <article-title>Movement Repetition Facilitates Response Preparation</article-title>. <source>Cell Rep</source>. <volume>24</volume>, <fpage>801</fpage>–<lpage>808</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shadmehr</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>H. J.</given-names></string-name> &amp; <string-name><surname>Ahmed</surname>, <given-names>A. A</given-names></string-name></person-group>. <article-title>A representation of effort in decision-making and motor control</article-title>. <source>Curr. Biol</source>. <volume>26</volume>, <fpage>1929</fpage>–<lpage>1934</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Beers</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name> &amp; <string-name><surname>Haggard</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>When Feeling Is More Important Than Seeing in Sensorimotor Adaptation</article-title>. <source>Curr. Biol</source>. <volume>12</volume>, <fpage>834</fpage>–<lpage>837</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Sheehan</surname>, <given-names>T. C.</given-names></string-name> &amp; <string-name><surname>Serences</surname>, <given-names>J. T.</given-names></string-name></person-group> <article-title>Distinguishing response from stimulus driven history biases</article-title>. <source>bioRxiv</source> (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.1101/2023.01.11.523637</pub-id>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beurze</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Van Pelt</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Medendorp</surname>, <given-names>W. P</given-names></string-name></person-group>. <article-title>Behavioral reference frames for planning human reaching movements</article-title>. <source>J. Neurophysiol</source>. <volume>96</volume>, <fpage>352</fpage>–<lpage>362</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Pelt</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Medendorp</surname>, <given-names>W. P</given-names></string-name></person-group>. <article-title>Updating target distance across eye movements in depth</article-title>. <source>J. Neurophysiol</source>. <volume>99</volume>, <fpage>2281</fpage>–<lpage>2290</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname>, <given-names>X.-X.</given-names></string-name> &amp; <string-name><surname>Stocker</surname>, <given-names>A. A</given-names></string-name></person-group>. <article-title>A Bayesian observer model constrained by efficient coding can explain “anti-Bayesian” percepts</article-title>. <source>Nat. Neurosci</source>. <volume>18</volume>, <fpage>1509</fpage>–<lpage>1517</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Patten</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Mannion</surname>, <given-names>D. J.</given-names></string-name> &amp; <string-name><surname>Clifford</surname>, <given-names>C. W. G</given-names></string-name></person-group>. <article-title>Correlates of perceptual orientation biases in human primary visual cortex</article-title>. <source>J. Neurosci</source>. <volume>37</volume>, <fpage>4744</fpage>–<lpage>4750</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsay</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Haith</surname>, <given-names>A. M.</given-names></string-name> &amp; <string-name><surname>Ivry</surname>, <given-names>R. B</given-names></string-name></person-group>. <article-title>Understanding implicit sensorimotor adaptation as a process of proprioceptive re-alignment</article-title>. <source>eLife</source> <volume>11</volume>, <elocation-id>e76639</elocation-id> (<year>2022</year>). <pub-id pub-id-type="doi">10.7554/eLife.76639</pub-id></mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cressman</surname>, <given-names>E. K.</given-names></string-name> &amp; <string-name><surname>Henriques</surname>, <given-names>D. Y. P</given-names></string-name></person-group>. <article-title>Sensory recalibration of hand position following visuomotor adaptation</article-title>. <source>J. Neurophysiol</source>. <volume>102</volume>, <fpage>3505</fpage>–<lpage>3518</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oldfield</surname>, <given-names>R. C</given-names></string-name></person-group>. <article-title>The assessment and analysis of handedness: The Edinburgh inventory</article-title>. <source>Neuropsychologia</source> <volume>9</volume>, <fpage>97</fpage>–<lpage>113</lpage> (<year>1971</year>).</mixed-citation></ref>
    <ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schoppe</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Harper</surname>, <given-names>N. S.</given-names></string-name>, <string-name><surname>Willmore</surname>, <given-names>B. D. B.</given-names></string-name>, <string-name><surname>King</surname>, <given-names>A. J.</given-names></string-name> &amp; <string-name><surname>Schnupp</surname>, <given-names>J. W. H</given-names></string-name></person-group>. <article-title>Measuring the Performance of Neural Models</article-title>. <source>Front Comput Neurosci</source> doi:<pub-id pub-id-type="doi">10.3389/fncom.2016.00010</pub-id>. <year>2016</year></mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fryar</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Gu</surname>, <given-names>Q.</given-names></string-name> &amp; <string-name><surname>Ogden</surname>, <given-names>C. L.</given-names></string-name></person-group> <article-title>Anthropometric reference data for children and adults: United States, 2007-2010</article-title>. <source>Vital Health Stat</source>. <volume>11</volume> <fpage>1</fpage>–<lpage>48</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname>, <given-names>E. O.</given-names></string-name>, <string-name><surname>Babis</surname>, <given-names>G. C.</given-names></string-name>, <string-name><surname>Soultanis</surname>, <given-names>K. C.</given-names></string-name> &amp; <string-name><surname>Soucacos</surname>, <given-names>P. N</given-names></string-name></person-group>. <article-title>Functional neuroanatomy of proprioception</article-title>. <source>J. Surg. Orthop. Adv</source>. <volume>17</volume>, <fpage>159</fpage>–<lpage>164</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haggard</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Newman</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Blundell</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Andrew</surname>, <given-names>H</given-names></string-name></person-group>. <article-title>The perceived position of the hand in space</article-title>. <source>Percept. Psychophys</source>. <volume>62</volume>, <fpage>363</fpage>–<lpage>377</lpage> (<year>2000</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100715.2.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ahmed</surname>
<given-names>Alaa A</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1596-342X</contrib-id>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02ttsq026</institution-id><institution>University of Colorado Boulder</institution>
</institution-wrap>
<city>Boulder</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study uses an original method to address the longstanding question of why reaching movements are often biased. The combination of a wide range of experimental conditions and computational modeling is a strength. <bold>Solid</bold> evidence is presented in support of the main claim that most of the biases in 2-D movement planning originate in misalignment between visuo-proprioceptive reference frames.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100715.2.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Wang et al. studied an old, still unresolved problem: Why are reaching movements often biased? Using data from a set of new experiments and from earlier studies, they identified how the bias in reach direction varies with movement direction and movement extent, and how this depends on factors such as the hand used, the presence of visual feedback, the size and location of the workspace, the visibility of the start position and implicit sensorimotor adaptation. They then examined whether a target bias, a proprioceptive bias, a bias in the transformation from visual to proprioceptive coordinates and/or biomechanical factors could explain the observed patterns of biases. The authors conclude that biases are best explained by a combination of transformation and target biases.</p>
<p>A strength of this study is that it used a wide range of experimental conditions with also a high resolution of movement directions and large numbers of participants, which produced a much more complete picture of the factors determining movement biases than previous studies did. The study used an original, powerful and elegant method to distinguish between the various possible origins of motor bias, based on the number of peaks in the motor bias plotted as a function of movement direction. The biomechanical explanation of motor biases could not be tested in this way, but this explanation was excluded in a different way using data on implicit sensorimotor adaptation. This was also an elegant method as it allowed the authors to test biomechanical explanations without the need to commit to a certain biomechanical cost function.</p>
<p>Overall, the authors have done a good job mapping out reaching biases in a wide range of conditions, revealing new patterns in one of the most basic tasks, and the evidence for the proposed origins is convincing. The study will likely have substantial impact on the field, as the approach taken is easily applicable to other experimental conditions. As such, the study can spark future research on the origin of reaching biases.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100715.2.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This work examines an important question in the planning and control of reaching movements - where do biases in our reaching movements arise and what might this tell us about the planning process. They compare several different computational models to explain the results from a range of experiments including those within the literature. Overall, they highlight that motor biases are primarily caused errors in the transformation between eye and hand reference frames. One strength of the paper is the large numbers of participants studied across many experiments. However, one weakness is that most of the experiments follow a very similar planar reaching design - with slicing movements through targets rather than stopping within a target. This is partially addressed with Exp 4. This work provides a valuable insight into the biases that govern reaching movements. While the evidence is solid for planar reaching movements, further support in the manner of 3D reaching movements would help strengthen the findings.</p>
<p>Strengths:</p>
<p>The work uses a large number of participants both with studies in the laboratory which can be controlled well and a huge number of participants via online studies. In addition, they use a large number of reaching directions allowing careful comparison across models. Together these allow a clear comparison between models which is much stronger than would usually be performed.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100715.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study makes excellent use of a uniquely large dataset of reaching movements collected over several decades to evaluate the origins of systematic motor biases. The analyses convincingly demonstrate that these biases are not explained by errors in sensed hand position or by biomechanical constraints, but instead arise from a misalignment between eye-centric and body-centric representations of position. By testing multiple computational models across diverse contexts-including different effectors, visible versus occluded start positions-the authors provide strong evidence for their transformation model. My earlier concerns have been addressed, and I find the work to be a significant and timely contribution that will be of broad interest to researchers studying visuomotor control, perception, and sensorimotor integration.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100715.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Wang</surname>
<given-names>Tianhe</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0131-850X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Morehead</surname>
<given-names>Ryan J</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5724-3028</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Jiang</surname>
<given-names>Amber</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ivry</surname>
<given-names>Richard B</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Tsay</surname>
<given-names>Jonathan S</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3992-9023</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Wang et al. studied an old, still unresolved problem: Why are reaching movements often biased? Using data from a set of new experiments and from earlier studies, they identified how the bias in reach direction varies with movement direction, and how this depends on factors such as the hand used, the presence of visual feedback, the size and location of the workspace, the visibility of the start position and implicit sensorimotor adaptation. They then examined whether a visual bias, a proprioceptive bias, a bias in the transformation from visual to proprioceptive coordinates and/or biomechanical factors could explain the observed patterns of biases. The authors conclude that biases are best explained by a combination of transformation and visual biases.</p>
<p>A strength of this study is that it used a wide range of experimental conditions with also a high resolution of movement directions and large numbers of participants, which produced a much more complete picture of the factors determining movement biases than previous studies did. The study used an original, powerful, and elegant method to distinguish between the various possible origins of motor bias, based on the number of peaks in the motor bias plotted as a function of movement direction. The biomechanical explanation of motor biases could not be tested in this way, but this explanation was excluded in a different way using data on implicit sensorimotor adaptation. This was also an elegant method as it allowed the authors to test biomechanical explanations without the need to commit to a certain biomechanical cost function.</p>
</disp-quote>
<p>We thank the reviewer for their enthusiastic comments.</p>
<disp-quote content-type="editor-comment">
<p>(1) The main weakness of the study is that it rests on the assumption that the number of peaks in the bias function is indicative of the origin of the bias. Specifically, it is assumed that a proprioceptive bias leads to a single peak, a transformation bias to two peaks, and a visual bias to four peaks, but these assumptions are not well substantiated. Especially the assumption that a transformation bias leads to two peaks is questionable. It is motivated by the fact that biases found when participants matched the position of their unseen hand with a visual target are consistent with this pattern. However, it is unclear why that task would measure only the effect of transformation biases, and not also the effects of visual and proprioceptive biases in the sensed target and hand locations. Moreover, it is not explained why a transformation bias would lead to this specific bias pattern in the first place.</p>
</disp-quote>
<p>We would like to clarify two things.</p>
<p>Frist, the measurements of the transformation bias are not entirely independent of proprioceptive and visual biases. Specifically, we define transformation bias as the misalignment between the internal representation of a visual target and the corresponding hand position. By this definition, the transformation error entails <italic>both</italic> visual and proprioceptive biases (see Author response image 1). Transformation biases have been empirically quantified in numerous studies using matching tasks, where participants either aligned their unseen hand to a visual target (Wang et al., 2021) or aligned a visual target to their unseen hand (Wilson et al., 2010). Indeed, those tasks are always considered as measuring proprioceptive biases assuming visual bias is small given the minimal visual uncertainty.</p>
<fig id="sa4fig1">
<label>Author response image 1.</label>
<graphic mime-subtype="jpg" xlink:href="elife-100715-sa4-fig1.jpg" mimetype="image"/>
</fig>
<p>Second, the critical difference between models is in how these biases influence motor planning rather than how those biases are measured. In the Proprioceptive bias model, a movement is planned in visual space. The system perceives the starting hand position in proprioceptive space and transforms this into visual space (Vindras &amp; Viviani, 1998; Vindras et al., 2005). As such, bias only affects the perceived starting position; there is no influence on the perceived target location (no visual bias).</p>
<p>In contrast, the Transformation bias model proposes that while both the starting and target positions are perceived in visual space, movement is planned in proprioceptive space. Consequently, both positions must be transformed from visual space to proprioceptive coordinates before movement planning (i.e., where is my sensed hand and where do I want it to be). Under this framework, biases can emerge from both the start and target positions. This is how the transformation model leads to different predictions compared to the perceptual models, even if the bias is based on the same measurements.</p>
<p>We now highlight the differences between the Transformation bias model and the Proprioceptive bias model explicitly in the Results section (Lines 192-200):</p>
<p>“Note that the Proprioceptive Bias model and the Transformation Bias model tap into the same visuo-proprioceptive error map. The key difference between the two models arises in how this error influences motor planning. For the Proprioceptive Bias model, planning is assumed to occur in visual space. As such, the perceived position of the hand (based on proprioception) is transformed into the visual space. This will introduce a bias in the representation of the start position. In contrast, the Transformation Bias model assumes that the visually-based representations of the start and target positions need to be transformed into proprioceptive space for motor planning. As such, both positions are biased in the transformation process. In addition to differing in terms of their representation of the target, the error introduced at the start position is in opposite directions due to the direction of the transformation (see fig 1g-h).”</p>
<p>In terms of the motor bias function across the workspace, the peaks are quantitatively derived from the model simulations. The number of peaks depends on how we formalize each model. Importantly, this is a stable feature of each model, regardless of how the model is parameterized. Thus, the number of peaks provides a useful criterion to evaluate different models.</p>
<p>Figure 1 g-h illustrates the intuition of how the models generate distinct peak patterns. We edited the figure caption and reference this figure when we introduce the bias function for each model.</p>
<disp-quote content-type="editor-comment">
<p>(2) Also, the assumption that a visual bias leads to four peaks is not well substantiated as one of the papers on which the assumption was based (Yousif et al., 2023) found a similar pattern in a purely proprioceptive task.</p>
</disp-quote>
<p>What we referred to in the original submission as “visual bias” is not an eye-centric bias, nor is it restricted to the visual system. Rather, it may reflect a domain-general distortion in the representation of position within polar space. We called it a visual bias as it was associated with the perceived location of the visual target in the current task. To avoid confusion, we have opted to move to a more general term and now refer to this as “target bias.”</p>
<p>We clarify the nature of this bias when introducing the model in the Results section (Lines 164-169):</p>
<p>“Since the task permits free viewing without enforced fixation, we assume that participants shift their gaze to the visual target; as such, an eye-centric bias is unlikely. Nonetheless, prior studies have shown a general spatial distortion that biases perceived target locations toward the diagonal axes(Huttenlocher et al., 2004; Kosovicheva &amp; Whitney, 2017). Interestingly, this bias appears to be domain-general, emerging not only for visual targets but also for proprioceptive ones(Yousif et al., 2023). We incorporated this diagonal-axis spatial distortion into a Target Bias model. This model predicts a four-peaked motor bias pattern (Fig 1f).”</p>
<p>We also added a paragraph in the Discussion to further elaborate on this model (Lines 502-511):</p>
<p>“What might be the source of the visual bias in the perceived location of the target? In the perception literature, a prominent theory has focused on the role of visual working memory account based on the observation that in delayed response tasks, participants exhibit a bias towards the diagonals when recalling the location of visual stimuli(Huttenlocher et al., 2004; Sheehan &amp; Serences, 2023). Underscoring that the effect is not motoric, this bias is manifest regardless of whether the response is made by an eye movement, pointing movement, or keypress(Kosovicheva &amp; Whitney, 2017). However, this bias is unlikely to be dependent on a visual input as similar diagonal bias is observed when the target is specified proprioceptively via the passive displacement of an unseen hand(Yousif et al., 2023). Moreover, as shown in the present study, a diagonal bias is observed even when the target is continuously visible. Thus, we hypothesize that the bias to perceive the target towards the diagonals reflects a more general distortion in spatial representation rather than being a product of visual working memory.”</p>
<disp-quote content-type="editor-comment">
<p>(3) Another weakness is that the study looked at biases in movement direction only, not at biases in movement extent. The models also predict biases in movement extent, so it is a missed opportunity to take these into account to distinguish between the models.</p>
</disp-quote>
<p>We thank the reviewer for this suggestion. We have now conducted a new experiment to assess angular and extent biases simultaneously (Figure 4a; Exp. 4; N = 30). Using our KINARM system, participants were instructed to make center-out movements that would terminate (rather than shoot past) at the visual target. No visual feedback was provided throughout the experiment.</p>
<p>The Transformation Bias model predicts a two-peaked error function in both the angular and extent dimensions (Figure 4c). Strikingly, when we fit the data from the new experiment to both dimensions simultaneously, this model captures the results qualitatively and quantitatively (Figure 4e). In terms of model comparison, it outperformed alternative models (Figure 4g) particularly when augmented with a visual bias component. Together, these results provide strong evidence that a mismatch between visual and proprioceptive space is a key source of motor bias.</p>
<p>This experiment is now reported within the revised manuscript (Lines 280-301).</p>
<disp-quote content-type="editor-comment">
<p>Overall, the authors have done a good job mapping out reaching biases in a wide range of conditions, revealing new patterns in one of the most basic tasks, but unambiguously determining the origin of these biases remains difficult, and the evidence for the proposed origins is incomplete. Nevertheless, the study will likely have a substantial impact on the field, as the approach taken is easily applicable to other experimental conditions. As such, the study can spark future research on the origin of reaching biases.</p>
</disp-quote>
<p>We thank the reviewer for these summary comments. We believe that the new experiments and analyses do a better job of identifying the origins of motor biases.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary:</p>
<p>This work examines an important question in the planning and control of reaching movements - where do biases in our reaching movements arise and what might this tell us about the planning process? They compare several different computational models to explain the results from a range of experiments including those within the literature. Overall, they highlight that motor biases are primarily caused by errors in the transformation between eye and hand reference frames. One strength of the paper is the large number of participants studied across many experiments. However, one weakness is that most of the experiments follow a very similar planar reaching design - with slicing movements through targets rather than stopping within a target. Moreover, there are concerns with the models and the model fitting. This work provides valuable insight into the biases that govern reaching movements, but the current support is incomplete.</p>
<p>Strengths:</p>
<p>The work uses a large number of participants both with studies in the laboratory which can be controlled well and a huge number of participants via online studies. In addition, they use a large number of reaching directions allowing careful comparison across models. Together these allow a clear comparison between models which is much stronger than would usually be performed.</p>
</disp-quote>
<p>We thank the reviewer for their encouraging comments.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>Although the topic of the paper is very interesting and potentially important, there are several key issues that currently limit the support for the conclusions. In particular I highlight:</p>
<p>(1) Almost all studies within the paper use the same basic design: slicing movements through a target with the hand moving on a flat planar surface. First, this means that the authors cannot compare the second component of a bias - the error in the direction of a reach which is often much larger than the error in reaching direction.</p>
</disp-quote>
<p>Reviewer 1 made a similar point, noting that we had missed an opportunity to provide a more thorough assessment of reaching biases. As described above, we conducted a new experiment in which participants made pointing movements, instructed to terminate the movements at the target. These data allow us to analyze errors in both angular and extent dimensions. The transformation bias model successfully predicts angular and extent biases, outperformed the other models at both group and individual levels. We have now included this result as Exp 4 in the manuscript. Please see response to Reviewer 1 Comment 3 for details.</p>
<disp-quote content-type="editor-comment">
<p>Second, there are several studies that have examined biases in three-dimensional reaching movements showing important differences to two-dimensional reaching movements (e.g. Soechting and Flanders 1989). It is unclear how well the authors' computational models could explain the biases that are present in these much more common-reaching movements.</p>
</disp-quote>
<p>This is an interesting issue to consider. We expect the mechanisms identified in our 2D work will generalize to 3D.</p>
<p>Soechting and Flanders (1989) quantified 3D biases by measuring errors across multiple 2D planes at varying heights (see Author response image 2 for an example from their paper). When projecting their 3-D bias data to a horizontal 2D space, the direction of the bias across the 2D plane looks relatively consistent across different heights even though the absolute value of the bias varies (Author response image 2). For example, the matched hand position is generally to the leftwards and downward of the target. Therefore, the models we have developed and tested in a specific 2D plane are likely to generalize to other 2D plane of different heights.</p>
<fig id="sa4fig2">
<label>Author response image 2.</label>
<graphic mime-subtype="jpg" xlink:href="elife-100715-sa4-fig2.jpg" mimetype="image"/>
</fig>
<p>However, we think the biases reported by Soechting and Flanders likely reflect transformation biases rather than motor biases. First, the movements in their study were performed very slowly (3–5 seconds), more similar to our proprioceptive matching tasks and much slower than natural reaching movements (&lt;500ms). Given the slow speed, we suspect that motor planning in Soechting and Flanders was likely done in a stepwise, incremental manner (closed loop to some degree). Second, the bias pattern reported in Soechting and Flanders —when projected into 2D space— closely mirrors the leftward transformation errors observed in previous visuo-proprioceptive matching task (e.g., Wang et al., 2021).</p>
<p>In terms of the current manuscript, we think that our new experiment (Exp 4, where we measure angular and radial error) provides strong evidence that the transformation bias model generalizes to more naturalistic pointing movements. As such, we expect these principles will generalize were we to examine movements in three dimensions, an extension we plan to test in future work.</p>
<disp-quote content-type="editor-comment">
<p>(2) The model fitting section is under-explained and under-detailed currently. This makes it difficult to accurately assess the current model fitting and its strength to support the conclusions. If my understanding of the methods is correct, then I have several concerns. For example, the manuscript states that the transformation bias model is based on studies mapping out the errors that might arise across the whole workspace in 2D. In contrast, the visual bias model appears to be based on a study that presented targets within a circle (but not tested across the whole workspace). If the visual bias had been measured across the workspace (similar to the transformation bias model), would the model and therefore the conclusions be different?</p>
</disp-quote>
<p>We have substantially expanded the Methods section to clarify the modeling procedures (detailed below in section “Recommendations for the Authors”). We also provide annotated code to enable others to easily simulate the models.</p>
<p>Here we address three points relevant to the reviewer’s concern about whether the models were tested on equal footing, and in particular, concern that the transformation bias model was more informed by prior literature than the visual bias model.</p>
<p>First, our center-out reaching task used target locations that have been employed in both visual and proprioceptive bias studies, offering reasonable comprehensive coverage of the workspace. For example, for a target to the left of the body’s midline, visual biases tend to be directed diagonally (Kosovicheva &amp; Whitney, 2017), while transformation biases are typically leftward and downward (Wang et al, 2021). In this sense, the models were similarly constrained by prior findings.</p>
<p>Second, while the qualitative shape of each model was guided by prior empirical findings, no previous data were directly used to quantitatively constrain the models. As such, we believe the models were evaluated on equal footing. No model had more information or, best we can tell, an inherent advantage over the others.</p>
<p>Third, reassuringly, the fitted transformation bias closely matches empirically observed bias maps reported in prior studies (Fig 2h). The strong correspondence provides convergent validity and supports the putative causality between transformation biases to motor biases.</p>
<disp-quote content-type="editor-comment">
<p>(3) There should be other visual bias models theoretically possible that might fit the experimental data better than this one possible model. Such possibilities also exist for the other models.</p>
</disp-quote>
<p>Our initial hypothesis, grounded in prior literature, was that motor biases arise from a combination of proprioceptive and visual biases. This led us to thoroughly explore a range of visual models. We now describe these alternatives below, noting that in the paper, we chose to focus on models that seemed the most viable candidates. (Please also see our response to Reviewer 3, Point 2, on another possible source of visual bias, the oblique effect.)</p>
<p>Quite a few models have described visual biases in perceiving motion direction or object orientation (e.g., Wei &amp; Stocker, 2015; Patten, Mannion &amp; Clifford, 2017). Orientation perception would be biased towards the Cartesian axis, generating a four-peak function. However, these models failed to account for the motor biases observed in our experiments. This is not surprising given that these models were not designed to capture biases related to a static location.</p>
<p>We also considered a class of eye-centric models where biases for peripheral locations are measured under fixation. A prominent finding here is that the bias is along the radial axis in which participants overshoot targets when they fixate on the start position during the movement (Beurze et al., 2006; Van Pelt &amp; Medendorp, 2008). Again, this is not consistent with the observed motor biases. For example, participants undershoot rightward targets when we measured the distance bias in Exp 4. Importantly, since most our tasks involved free viewing in natural settings with no fixation requirements, we considered it unlikely that biases arising from peripheral viewing play a major role.</p>
<p>We note, though, that in our new experiment (Exp 4), participants observed the visual stimuli from a fixed angle in the KinArm setup (see Figure 4a). This setup has been shown to induce depth-related visual biases (Figure 4b, e.g., Volcic et al., 2013; Hibbard &amp; Bradshaw, 2003). For this reason, we implemented a model incorporating this depth bias as part of our analyses of these data. While this model performed significantly worse than the transformation bias model alone, a mixed model that combined the depth bias and transformation bias provided the best overall fit. We now include this result in the main text (Lines 286-294).</p>
<p>We also note that the “visual bias” we referred to in the original submission is not restricted to the visual system. A similar bias pattern has been observed when the target is presented visually or proprioceptively (Kosovicheva &amp; Whitney, 2017; Yousif, Forrence, &amp; McDougle, 2023). As such, it may reflect a domaingeneral distortion in the representation of position within polar space. Accordingly, in the revision, we now refer to this in a more general way, using the term “target bias.” We justify this nomenclature when introducing the model in the Results section (Lines 164-169). Please also see Reviewer 1 comment 2.</p>
<p>We recognize that future work may uncover a better visual model or provide a more fine-grained account of visual biases (or biases from other sources). With our open-source simulation code, such biases can be readily incorporated—either to test them against existing models or to combine them with our current framework to assess their contribution to motor biases. Given our explorations, we expect our core finding will hold: Namely, that a combination of transformation and target biases offers the most parsimonious account, with the bias associated with the transformation process explaining the majority of the observed motor bias in visually guided movements.</p>
<p>Given the comments from the reviewer, we expanded the discussion session to address the issue of alternative models of visual bias (lines 522-529):</p>
<p>“Other forms of visual bias may influence movement. Depth perception biases could contribute to biases in movement extent(Beurze et al., 2006; Van Pelt &amp; Medendorp, 2008). Visual biases towards the principal axes have been reported when participants are asked to report the direction of moving targets or the orientation of an object(Patten et al., 2017; Wei &amp; Stocker, 2015). However, the predicted patterns of reach biases do not match the observed biases in the current experiments. We also considered a class of eye-centric models in which participants overestimate the radial distance to a target while maintaining central fixation(Beurze et al., 2006; Van Pelt &amp; Medendorp, 2008). At odds with this hypothesis, participants undershot rightward targets when we measured the radial bias in Exp 4. The absence of these other distortions of visual space may be accounted for by the fact that we allowed free viewing during the task.”</p>
<disp-quote content-type="editor-comment">
<p>(4) Although the authors do mention that the evidence against biomechanical contributions to the bias is fairly weak in the current manuscript, this needs to be further supported. Importantly both proprioceptive models of the bias are purely kinematic and appear to ignore the dynamics completely. One imagines that there is a perceived vector error in Cartesian space whereas the other imagines an error in joint coordinates. These simply result in identical movements which are offset either with a vector or an angle. However, we know that the motor plan is converted into muscle activation patterns which are sent to the muscles, that is, the motor plan is converted into an approximation of joint torques. Joint torques sent to the muscles from a different starting location would not produce an offset in the trajectory as detailed in Figure S1, instead, the movements would curve in complex patterns away from the original plan due to the non-linearity of the musculoskeletal system. In theory, this could also bias some of the other predictions as well. The authors should consider how the biomechanical plant would influence the measured biases.</p>
</disp-quote>
<p>We thank the reviewer for encouraging us on this topic and to formalize a biomechanical model. In response, we have implemented a state-of-the-art biomechanical framework, MotorNet</p>
<p>(<ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/articles/88591">https://elifesciences.org/articles/88591</ext-link>), which simulates a six-muscle, two-skeleton planar arm model using recurrent neural networks (RNNs) to generate control policies (See Figure 6a). This model captures key predictions about movement curvature arising from biomechanical constraints. We view it as a strong candidate for illustrating how motor bias patterns could be shaped by the mechanical properties of the upper limb.</p>
<p>Interestingly, the biomechanical model did not qualitatively or quantitatively reproduce the pattern of motor biases observed in our data. Specifically, we trained 50 independent agents (RNNs) to perform random point-to-point reaching movements across the workspace used in our task. We used a loss function that minimized the distance between the fingertip and the target over the entire trajectory. When tested on a center-out reaching task, the model produced a four-peaked motor bias pattern (Figure 6b), in contrast to the two-peaked function observed empirically. These results suggest that upper limb biomechanical constraints are unlikely to be a primary driver of motor biases in reaching. This holds true even though the reported bias is read out at 60% of the reaching distance, where biomechanical influences on the curvature of movement are maximal. We have added this analysis to the results (lines 367-373).</p>
<p>It may seem counterintuitive that biomechanics plays a limited role in motor planning. This could be due to several factors. First, First, task demands (such as the need to grasp objects) may lead the biomechanical system to be inherently organized to minimize endpoint errors (Hu et al., 2012; Trumbower et al., 2009). Second, through development and experience, the nervous system may have adapted to these biomechanical influences—detecting and compensating for them over time (Chiel et al., 2009).</p>
<p>That said, biomechanical constraints may make a larger contribution in other contexts; for example, when movements involve more extreme angles or span larger distances, or in individuals with certain musculoskeletal impairments (e.g., osteoarthritis) where physical limitations are more likely to come into play. We address this issue in the revised discussion.</p>
<p>“Nonetheless, the current study does not rule out the possibility that biomechanical factors may influence motor biases in other contexts. Biomechanical constraints may have had limited influence in our experiments due to the relatively modest movement amplitudes used and minimal interaction torques involved. Moreover, while we have focused on biases that manifest at the movement endpoint, biomechanical constraints might introduce biases that are manifest in the movement trajectories.(Alexander, 1997; Nishii &amp; Taniai, 2009) Future studies are needed to examine the influence of context on reaching biases.”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public review):</bold></p>
<p>The authors make use of a large dataset of reaches from several studies run in their lab to try to identify the source of direction-dependent radial reaching errors. While this has been investigated by numerous labs in the past, this is the first study where the sample is large enough to reliably characterize isometries associated with these radial reaches to identify possible sources of errors.</p>
<p>(1) The sample size is impressive, but the authors should Include confidence intervals and ideally, the distribution of responses across individuals along with average performance across targets. It is unclear whether the observed “averaged function” is consistently found across individuals, or if it is mainly driven by a subset of participants exhibiting large deviations for diagonal movements. Providing individual-level data or response distributions would be valuable for assessing the ubiquity of the observed bias patterns and ruling out the possibility that different subgroups are driving the peaks and troughs. It is possible that the Transformation or some other model (see below) could explain the bias function for a substantial portion of participants, while other participants may have different patterns of biases that can be attributable to alternative sources of error.</p>
</disp-quote>
<p>We thank the reviewer for encouraging a closer examination of the individual-level data. We did include standard error when we reported the motor bias function. Given that the error distribution is relatively Gaussian, we opted to not show confidence intervals since they would not provide additional information.</p>
<p>To examine individual differences, we now report a best-fit model frequency analysis. For Exp 1, we fit each model at the individual level and counted the number of participants that are best predicted by each model. Among the four single source models (Figure 3a), the vast majority of participants are best explained by the transformation bias model (48/56). When incorporating mixture models, the combined transformation + target bias model emerged as the best fit for almost all participants across experiments (50/56). The same pattern holds for Exp 3b, the frequency analysis is more distributed, likely due to the added noise that comes with online studies.</p>
<p>We report this new analysis in the Results. (see Fig 3. Fig S2). Note that we opted to show some representative individual fits, selecting individuals whose data were best predicted by different models (Fig S2). Given that the number of peaks characterizes each model (independent of the specific parameter values), the two-peaked function exhibited for most participants indicates that the Transformation bias model holds at the individual level and not just at the group level.</p>
<disp-quote content-type="editor-comment">
<p>(2) The different datasets across different experimental settings/target sets consistently show that people show fewer deviations when making cardinal-directed movements compared to movements made along the diagonal when the start position is visible. This reminds me of a phenomenon referred to as the oblique effect: people show greater accuracy for vertical and horizontal stimuli compared to diagonal ones. While the oblique effect has been shown in visual and haptic perceptual tasks (both in the horizontal and vertical planes), there is some evidence that it applies to movement direction. These systematic reach deviations in the current study thus may reflect this epiphenomenon that applies across modalities. That is, estimating the direction of a visual target from a visual start position may be less accurate, and may be more biased toward the horizontal axis, than for targets that are strictly above, below, left, or right of the visual start position. Other movement biases may stem from poorer estimation of diagonal directions and thus reflect more of a perceptual error than a motor one. This would explain why the bias function appears in both the in-lab and on-line studies although the visual targets are very different locations (different planes, different distances) since the oblique effects arise independent of plane, distance, or size of the stimuli. When the start position is not visible like in the Vindras study, it is possible that this oblique effect is less pronounced; masked by other sources of error that dominate when looking at 2D reach endpoint made from two separate start positions, rather than only directional errors from a single start position. Or perhaps the participants in the Vindras study are too variable and too few (only 10) to detect this rather small direction-dependent bias.</p>
</disp-quote>
<p>The potential link between the oblique effect and the observed motor bias is an intriguing idea, one that we had not considered. However, after giving this some thought, we see several arguments against the idea that the oblique effect accounts for the pattern of motor biases.</p>
<p>First, by the oblique effect, perceptual variability is greater along the diagonal axes compared to the cardinal axes. These differences in perceptual variability have been used to explain biases in visual perception through a Bayesian model under the assumption that the visual system has an expectation that stimuli are more likely to be oriented along the cardinal axes (Wei &amp; Stocker, 2015). Importantly, the model predicts low biases at targets with peak perceptual variability. As such, even though those studies observed that participants showed large variability for stimuli at diagonal orientations, the bias for these stimuli was close to zero. Given we observed a large bias for targets at locations along the diagonal axes, we do not think this visual effect can explain the motor bias function.</p>
<p>Second, the reviewer suggested that the observed motor bias might be largely explained by visual biases (or what we now refer to as target biases). If this hypothesis is correct, we would anticipate observing a similar bias pattern in tasks that use a similar layout for visual stimuli but do not involve movement. However, this prediction is not supported. For example, Kosovicheva &amp; Whitney (2017) used a position reproduction/judgment task with keypress responses (no reaching). The stimuli were presented in a similar workspace as in our task. Their results showed four-peaked bias function while our results showed a two-peaked function.</p>
<p>In summary, we don’t think oblique biases make a significant contribution to our results.</p>
<disp-quote content-type="editor-comment">
<p>A bias in estimating visual direction or visual movement vector Is a more realistic and relevant source of error than the proposed visual bias model. The Visual Bias model is based on data from a study by Huttenlocher et al where participants “point” to indicate the remembered location of a small target presented on a large circle. The resulting patterns of errors could therefore be due to localizing a remembered visual target, or due to relative or allocentric cues from the clear contour of the display within which the target was presented, or even movements used to indicate the target. This may explain the observed 4-peak bias function or zig-zag pattern of “averaged” errors, although this pattern may not even exist at the individual level, especially given the small sample size. The visual bias source argument does not seem well-supported, as the data used to derive this pattern likely reflects a combination of other sources of errors or factors that may not be applicable to the current study, where the target is continuously visible and relatively large. Also, any visual bias should be explained by a coordinates centre on the eye and should vary as a function of the location of visual targets relative to the eyes. Where the visual targets are located relative to the eyes (or at least the head) is not reported.</p>
</disp-quote>
<p>Thank you for this question. A few key points to note:</p>
<p>The visual bias model has also been discussed in studies using a similar setup to our study. Kosovicheva &amp; Whitney (2017) observed a four-peaked function in experiments in which participants report a remembered target position on a circle by either making saccades or using key presses to adjust the position of a dot. However, we agree that this bias may be attenuated in our experiment given that the target is continuously visible. Indeed, the model fitting results suggest the peak of this bias is smaller in our task (~3°) compared to previous work (~10°, Kosovicheva &amp; Whitney, 2017; Yousif, Forrence, &amp; McDougle, 2023).</p>
<p>We also agree with the reviewer that this “visual bias” is not an eye-centric bias, nor is it restricted to the visual system. A similar bias pattern is observed even if the target is presented proprioceptively (Yousif, Forrence, &amp; McDougle, 2023). As such, this bias may reflect a domain-general distortion in the representation of position within polar space. Accordingly, in the revision, we now refer to this in a more general way, using the term “target bias”, rather than visual bias. We justify this nomenclature when introducing the model in the Results section (Lines 164-169). Please also see Reviewer 1 comment 2 for details.</p>
<p>Motivated by Reviewer 2, we also examined multiple alternative visual bias models (please refer to our response to Reviewer 2, Point 3.</p>
<disp-quote content-type="editor-comment">
<p>The Proprioceptive Bias Model is supposed to reflect errors in the perceived start position. However, in the current study, there is only a single, visible start position, which is not the best design for trying to study the contribution. In fact, my paradigms also use a single, visual start position to minimize the contribution of proprioceptive biases, or at least remove one source of systematic biases. The Vindras study aimed to quantify the effect of start position by using two sets of radial targets from two different, unseen start positions on either side of the body midline. When fitting the 2D reach errors at both the group and individual levels (which showed substantial variability across individuals), the start position predicted most of the 2D errors at the individual level – and substantially more than the target direction. While the authors re-plotted the data to only illustrate angular deviations, they only showed averaged data without confidence intervals across participants. Given the huge variability across their 10 individuals and between the two target sets, it would be more appropriate to plot the performance separately for two target sets and show confidential intervals (or individual data). Likewise, even the VT model predictions should differ across the two targets set since the visual-proprioceptive matching errors from the Wang et al study that the model is based on, are larger for targets on the left side of the body.</p>
</disp-quote>
<p>To be clear, in the Transformation bias model, the vector bias at the start position is also an important source of error. The critical difference between the proprioceptive and transformation models is how bias influences motor planning. In the Proprioceptive bias model, movement is planned in visual space. The system perceives the starting hand position in proprioceptive space and transforms this into visual space (Vindras &amp; Viviani, 1998; Vindras et al., 2005). As such, the bias is only relevant in terms of the perceived start position; it does not influence the perceived target location. In contrast, the transformation bias model proposes that while both the starting and target positions are perceived in visual space, movements are planned in proprioceptive space. Consequently, when the start and target positions are visible, both positions must be transformed from visual space to proprioceptive coordinates before movement planning. Thus, bias will influence both the start and target positions. We also note that to set the transformation bias for the start/target position, we referred to studies in which bias is usually referred to as proprioception error measurement. As such, changing the start position has a similar impact on the Transformation and the Proprioceptive Bias models in principle, and would not provide a stronger test to separate them.</p>
<p>We now highlight the differences between the models in the Results section, making clear that the bias at the start position influences both the Proprioceptive bias and Transformation bias models (Lines 192200).</p>
<p>“Note that the Proprioceptive Bias model and the Transformation Bias model tap into the same visuo-proprioceptive error map. The key difference between the two models arises in how this error influences motor planning. For the Proprioceptive Bias model, planning is assumed to occur in visual space. As such, the perceived position of the hand (based on proprioception) is transformed into visual space. This will introduce a bias in the representation of the start position. In contrast, the Transformation Bias model assumes that the visually-based representations of the start and target positions need to be transformed into proprioceptive space for motor planning. As such, both positions are biased in the transformation process. In addition to differing in terms of their representation of the target, the error introduced at the start position is in opposite directions due to the direction of the transformation (see fig 1g-h).”</p>
<p>In terms of fitting individual data, we have conducted a new experiment, reported as Exp 4 in the revised manuscript (details in our response to Reviewer 1, comment 3). The experiment has a larger sample size (n=30) and importantly, examined error for both movement angle and movement distance. We chose to examine the individual differences in 2-D biases using this sample rather than Vindras’ data as our experiment has greater spatial resolution and more participants. At both the group and individual level, the Transformation bias model is the best single source model, and the Transformation + Target Bias model is the best combined model. These results strongly support the idea that the transformation bias is the main source of the motor bias.</p>
<p>As for the different initial positions in Vindras et al (2005), the two target sets have very similar patterns of motor biases. As such, we opted to average them to decrease noise. Notably, the transformation model also predicts that altering the start location should have limited impact on motor bias patterns: What matters for the model is the relative difference between the transformation biases at the start and target positions rather than the absolute bias.</p>
<fig id="sa4fig3">
<label>Author response image 3.</label>
<graphic mime-subtype="jpg" xlink:href="elife-100715-sa4-fig3.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>I am also having trouble fully understanding the V-T model and its associated equations, and whether visual-proprioception matching data is a suitable proxy for estimating the visuomotor transformation. I would be interested to first see the individual distributions of errors and a response to my concerns about the Proprioceptive Bias and Visual Bias models.</p>
</disp-quote>
<p>We apologize for the lack of clarity on this model. To generate the T+V (Now Transformation + Target bias, or TR+TG) model, we assume the system misperceives the target position (Target bias, see Fig S5a) and then transforms the start and misperceived target positions into proprioceptive space (Fig S5b). The system then generates a motor plan in proprioceptive space; this plan will result in the observed motor bias (Fig. S5c). We now include this figure as Fig S5 and hope that it makes the model features salient.</p>
<p>Regarding whether the visuo-proprioceptive matching task is a valid proxy for transformation bias, we refer the reviewer to the comments made by Public Reviewer 1, comment 1. We define the transformation bias as the discrepancy between corresponding positions in visual and proprioceptive space. This can be measured using matching tasks in which participants either aligned their unseen hand to a visual target (Wang et al., 2021) or aligned a visual target to their unseen hand (Wilson et al., 2010).</p>
<p>Nonetheless, when fitting the model to the motor bias data, we did not directly impose the visual-proprioceptive matching data. Instead, we used the shape of the transformation biases as a constraint, while allowing the exact magnitude and direction to be free parameters (e.g., a leftward and downward bias scaled by distance from the right shoulder). Reassuringly, the fitted transformation biases closely matched the magnitudes reported in prior studies (Fig. 2h, 1e), providing strong quantitative support for the hypothesized causal link between transformation and motor biases.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p>Overall, the reviewers agreed this is an interesting study with an original and strong approach. Nonetheless, there were three main weaknesses identified. First, is the focus on bias in reach direction and not reach extent. Second, the models were fit to average data and not individual data. Lastly, and most importantly, the model development and assumptions are not well substantiated. Addressing these points would help improve the eLife assessment.</p>
<p><bold>Reviewer #1 (Recommendations for the authors):</bold></p>
<p>It is mentioned that the main difference between Experiments 1 and 3 is that in Experiment 3, the workspace was smaller and closer to the shoulder. Was the location of the laptop relative to the participant in Experiment 3 known by the authors? If so, variations in this location across participants can be used to test whether the Transformation bias was indeed larger for participants who had the laptop further from the shoulder.</p>
<p>Another difference between Experiments 1 and 3 is that in Experiment 1, the display was oriented horizontally, whereas it was vertical in Experiment 3. To what extent can that have led to the different results in these experiments?</p>
</disp-quote>
<p>This is an interesting point that we had not considered. Unfortunately, for the online work we do not record the participants’ posture.</p>
<p>Regarding the influence of display orientation (horizontal vs. vertical), Author response image 4 presents three relevant data points: (1) Vandevoorde and Orban de Xivry (2019), who measured motor biases in-person across nine target positions using a tablet and vertical screen; (2) Our Experiment 1b, conducted online with a vertical setup; (3) Our in-person Experiment 3b, using a horizontal monitor. For consistency, we focus on the baseline conditions with feedback, the only condition reported in Vandevoorde. Motor biases from the two in-person studies were similar despite differing monitor orientations: Both exhibited two-peaked functions with comparable peak locations. We note that the bias attenuation in Vandevoorde may be due to their inclusion of reward-based error signals in addition to cursor feedback. In contrast, compared to the in-person studies, the online study showed reduced bias magnitude with what appears to be a four peaked function. While more data are needed, these results suggest that the difference in the workspace (more restricted in our online study) may be more relevant than monitor orientation.</p>
<fig id="sa4fig4">
<label>Author response image 4.</label>
<graphic mime-subtype="jpg" xlink:href="elife-100715-sa4-fig4.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>For the joint-based proprioceptive model, the equations used are for an arm moving in a horizontal plane at shoulder height, but the figures suggest the upper arm was more vertical than horizontal. How does that affect the predictions for this model?</p>
</disp-quote>
<p>Please also see our response to your public comment 1. When the upper limb (or the lower limb) is not horizontal, it will influence the projection of the upper limb to the 2-D space. Effectively in the joint-based proprioceptive model, this influences the ratio between L1 and L2 (see  Author response image 5b below). However, adding a parameter to vary L1/L2 ratio would not change the set of the motor bias function that can be produced by the model. Importantly, it will still generate a one-peak function. We simulated 50 motor bias function across the possible parameter space. As shown by  Author response image 5c-d, the peak and the magnitude of the motor bias functions are very similar with and without the L1/L2 term. We characterize the bias function with the peak position and the peak-to-valley distance. Based on those two factors, the distribution of the motor bias function is very similar ( Author response image 5e-f). Moreover, the L1/L2 ratio parameter is not recoverable by model fitting ( Author response image 5c), suggesting that it is redundant with other parameters. As such we only include the basic version of the joint-based proprioceptive model in our model comparisons.</p>
<fig id="sa4fig5">
<label>Author response image 5.</label>
<graphic mime-subtype="jpg" xlink:href="elife-100715-sa4-fig5.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>It was unclear how the models were fit and how the BIC was computed. It is mentioned that the models were fit to average data across participants, but the BIC values were based on all trials for all participants, which does not seem consistent. And the models are deterministic, so how can a log-likelihood be determined? Since there were inter-individual differences, fitting to average data is not desirable. Take for instance the hypothetical case that some participants have a single peak at 90 deg, and others have a single peak at 270 deg. Averaging their data will then lead to a pattern with two peaks, which would be consistent with an entirely different model.</p>
</disp-quote>
<p>We thank the reviewer for raising these issues.</p>
<p>Given the reviewers’ comments, we now report fits at both the group and individual level (see response to reviewer 3 public comment 1). The group-level fitting is for illustration purposes. Model comparison is now based on the individual-level analyses which show that the results are best explained by the transformation model when comparing single source models and best explained by the T+V (now TG+TR) model when consider all models. These new results strongly support the transformation model.</p>
<p>Log-likelihoods were computed assuming normally distributed motor noise around the motor biases predicted by each model.</p>
<p>We updated the Methods section as follows (lines 841-853):</p>
<p>“We used the fminsearchbnd function in MATLAB to minimize the sum of loglikelihood (LL) across all trials for each participant. LL were computed assuming normally distributed noise around each participant’s motor biases:</p>
<p>[11] LL = normpdf(x, b, c)</p>
<p>where x is the empirical reaching angle, b is the predicted motor bias by the model, c is motor noise, calculated as the standard deviation of (x − b). For model comparison, we calculated the BIC as follow:</p>
<p>[12] BIC = -2LL+k∗ln(n)</p>
<p>where k is the number of parameters of the models. Smaller BIC values correspond to better fits. We report the sum of ΔBIC by subtracting the BIC value of the TR+TG model from all other models.</p>
<p>For illustrative purposes, we fit each model at the group level, pooling data across all participants to predict the group-averaged bias function.”</p>
<disp-quote content-type="editor-comment">
<p>What was the delay of the visual feedback in Experiment 1?</p>
</disp-quote>
<p>The visual delay in our setup was ~30 ms, with the procedure used to estimate this described in detail in Wang et al (2024, Curr. Bio.). We note that in calculating motor biases, we primarily relied on the data from the no-feedback block.</p>
<disp-quote content-type="editor-comment">
<p>Minor corrections</p>
<p>In several places it is mentioned that movements were performed with proximal and distal effectors, but it's unclear where that refers to because all movements were performed with a hand (distal effector).</p>
</disp-quote>
<p>By 'proximal and distal effectors,' we were referring to the fact that in the online setup, “reaching movements” are primarily made by finger and/or wrist movements across a trackpad, whereas in the inperson setup, the participants had to use their whole arm to reach about the workspace. To avoid confusion, we now refer to these simply as 'finger' versus 'hand' movements.</p>
<disp-quote content-type="editor-comment">
<p>In many figures, Bias is misspelled as Bais.</p>
</disp-quote>
<p>Fixed.</p>
<disp-quote content-type="editor-comment">
<p>In Figure 3, what is meant by deltaBIC (*1000) etc? Literally, it would mean that the bars show 1,000 times the deltaBIC value, suggesting tiny deltaBIC values, but that's probably not what's meant.</p>
</disp-quote>
<p>×1000' in the original figure indicates the unit scaling, with ΔBIC values ranging from approximately 1000 to 4000. However, given that we now fit the models at the individual level, we have replaced this figure with a new one (Figure 3e) showing the distribution of individual BIC values.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations for the authors):</bold></p>
<p>I have concerns that the authors only examine slicing movements through the target and not movements that stop in the target. Biases create two major errors - errors in direction and errors in magnitude and here the authors have only looked at one of these. Previous work has shown that both can be used to understand the planning processes underlying movement. I assume that all models should also make predictions about the magnitude biases which would also help support or rule out specific models.</p>
</disp-quote>
<p>Please see our response to Reviewer 1 public review 3.</p>
<disp-quote content-type="editor-comment">
<p>As discussed above, three-dimensional reaching movements also have biases and are not studied in the current manuscript. In such studies, biomechanical factors may play a much larger role.</p>
</disp-quote>
<p>Please see our response to your public review.</p>
<disp-quote content-type="editor-comment">
<p>It may be that I am unclear on what exactly is done, as the methods and model fitting barely explain the details, but on my reading on the methods I have several major concerns.</p>
<p>First, it feels that the visual bias model is not as well mapped across space if it only results from one study which is then extrapolated across the workspace. In contrast, the transformation model is actually measured throughout the space to develop the model. I have some concerns about whether this is a fair comparison. There are potentially many other visual bias models that might fit the current experimental results better than the chosen visual bias model.</p>
</disp-quote>
<p>Please refers to our response to your public review.</p>
<disp-quote content-type="editor-comment">
<p>It is completely unclear to me why a joint-based proprioceptive model would predict curved planned movements and not straight movements (Figure S1). Changes in the shoulder and elbow joint angles could still be controlled to produce a straight movement. On the other hand, as mentioned above, the actual movement is likely much more complex if the physical starting position is offset from the perceived hand.</p>
</disp-quote>
<p>Natural movements are often curved, reflecting a drive to minimize energy expenditure or biomechanical constraints (e.g., joint and muscle configuration). This is especially the case when the task emphasizes endpoint precision (Codol et al., 2024) like ours. Trajectory curvature was also observed in a recent simulation study in which a neural network was trained to control a biomechanical model (2-limb, 6muscles) with the cost function specified to minimize trajectory error (reach to a target with as straight a movement as possible). Even under these constraints, the movements showed some curvature. To examined whether the endpoint reaching bias somehow reflects the curvature (or bias during reaching), we included the prediction of this new biomechanical model in the paper to show it does not explain the motor bias we observed.</p>
<p>To be clear, while we implemented several models (Joint-based proprioceptive model and the new biomechanical model) to examine whether motor biases can be explained by movement curvature, our goal in this paper was to identify the source of the endpoint bias. Our modeling results reveal a previously underappreciated source of motor bias—a transformation error that arises between visual and proprioceptive space—plays a dominant role in shaping motor bias patterns across a wide range of experiments, including naturalistic reaching contexts where vision and hand are aligned at the start position. While the movement curvature might be influenced by selectively manipulating factors that introduce a mismatch between the visual starting position and the actual hand position (such as Sober and Sabes, 2003), we think it will be an avenue for future work to investigate this question.</p>
<disp-quote content-type="editor-comment">
<p>The model fitting section is barely described. It is unclear how the data is fit or almost any other aspects of the process. How do the authors ensure that they have found the minimum? How many times was the process repeated for each model fit? How were starting parameters randomized? The main output of the model fitting is BIC comparisons across all subjects. However, there are many other ways to compare the models which should be considered in parallel. For example, how well do the models fit individual subjects using BIC comparisons? Or how often are specific models chosen for individual participants? While across all subjects one model may fit best, it might be that individual subjects show much more variability in which model fits their data. Many details are missing from the methods section. Further support beyond the mean BIC should be provided.</p>
</disp-quote>
<p>We fit each model 150 times and for each iteration, the initial value of each parameter was randomly selected from a uniform distribution. The range for each parameter was hand tuned for each model, with an eye on making sure the values covered a reasonable range. Please see our response to your first minor comment below for the range of all parameters and how we decide the iteration number for each model.</p>
<p>Given the reviewers’ comments in the individual difference, we now fit the models at individual level and report a frequency analysis, describing the best fitting model for each participant. In brief, the data for a vast majority of the participants was best explained by the transformation model when comparing single source models and by the T+V (TR+TG) model when consider all models. Please see response to reviewer 3 public comment 1 for the updated result.</p>
<p>We updated the method session, and it reads as follows (lines 841-853):</p>
<p>_“_We used the fminsearchbnd function in MATLAB to minimize the sum of loglikelihood (LL) across all trials for each participant. LL were computed assuming normally distributed noise around each participant’s motor biases:</p>
<p>[11]       𝐿𝐿 = 𝑛𝑜𝑟𝑚𝑝𝑑𝑓(𝑥, 𝑏, 𝑐)</p>
<p>where x is the empirical reaching angle, b is the predicted motor bias by the model, c is motor noise, calculated as the standard deviation of x-b.</p>
<p>For model comparison, we calculated the BIC as follows:</p>
<p>[12] BIC = -<italic>2LL</italic>+<italic>k</italic>∗ln(n)</p>
<p>where <italic>k</italic> is the number of parameters of the models. Smaller BIC values correspond to better fits. We report the sum of ΔBIC by subtracting the BIC value of the TR+TG model from all other models.<italic>”</italic></p>
<disp-quote content-type="editor-comment">
<p>Line 305-307. The authors state that biomechanical issues would not predict qualitative changes in the motor bias function in response to visual manipulation of the start position. However, I question this statement. If the start position is offset visually then any integration of the proprioceptive and visual information to determine the start position would contain a difference from the real hand position. A calculation of the required joint torques from such a position sent through the mechanics of the limb would produce biases. These would occur purely because of the combination of the visual bias and the inherent biomechanical dynamics of the limb.</p>
</disp-quote>
<p>We thank the reviewer for this comment. We have removed the statement regarding inferences about the biomechanical model based on visual manipulations of the start position. Additionally, we have incorporated a recently proposed biomechanical model into our model comparisons to expand our exploration of sources of bias. Please refer to our response to your public review for details.</p>
<disp-quote content-type="editor-comment">
<p>Measurements are made while the participants hold a stylus in their hand. How can the authors be certain that the biases are due to the movement and not due to small changes in the hand posture holding the stylus during movements in the workspace. It would be better if the stylus was fixed in the hand without being held.</p>
</disp-quote>
<p>Below, we have included an image of the device used in Exp 1 for reference. The digital pen was fixed in a vertical orientation. At the start of the experiment, the experimenter ensured that the participant had the proper grip alignment and held the pen at the red-marked region. With these constraints, we see minimal change in posture during the task.</p>
<fig id="sa4fig6">
<label>Author response image 6.</label>
<graphic mime-subtype="jpg" xlink:href="elife-100715-sa4-fig6.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>Minor Comments</p>
<p>Best fit model parameters are not presented. Estimates of the accuracy of these measures would also be useful.</p>
</disp-quote>
<p>In the original submission, we included a Table S1 that presented the best-fit parameters for the TR+TG (Previously T+V) model. Table S1 now shows the parameters for the other models (Exp 1b and 3b, only). We note the parameter values from these non-optimal models are hard to interpret given that core predictions are inconsistent with the data (e.g., number of peaks).</p>
<p>We assume that by &quot;accuracy of these measures,&quot; the reviewers are referring to the reliability of the model fits. To assess this, we conducted a parameter recovery analysis in which we simulated a range of model parameters for each model and then attempted to recover them through fitting. Each model was simulated 50 times, with the parameters randomly sampled from distributions used to define the initial fitting parameters. Here, we only present the results for the combined models (TR+TG, PropV+V, and PropJ+V), as the nested models would be even easier to fit.</p>
<p>As shown in Fig. S4, all parameters were recovered with high accuracy, indicating strong reliability in parameter estimation. Additionally, we examined the log-likelihood as a function of fitting iterations (Fig. S4d). Based on this curve, we determined that 150 iterations were sufficient given that the log-likelihood values were asymptotic at this point. Moreover, in most cases, the model fitting can recover the simulated model, with minimal confusion across the three models (Fig. S4e).</p>
<disp-quote content-type="editor-comment">
<p>What are the (*1000) and (*100) in the Change in BIC y-labels? I assume they indicate that the values should be multiplied by these numbers. If these indicate that the BIC is in the hundreds or thousands it would be better the label the axes clearly, as the interpretation is very different (e.g. a BIC difference of 3 is not significant).</p>
</disp-quote>
<p>×1000' in the original figure indicates the unit scaling, with ΔBIC values ranging from approximately 1000 to 4000. However, given that we now fit the models at the individual level, we have replaced this figure with a new one showing the distribution of individual BIC values.</p>
<disp-quote content-type="editor-comment">
<p>Lines 249, 312, and 315, and maybe elsewhere - the degree symbol does not display properly.</p>
</disp-quote>
<p>Corrected.</p>
<disp-quote content-type="editor-comment">
<p>Line 326. The authors mention that participants are unaware of their change in hand angle in response to clamped feedback. However, there may be a difference between sensing for perception and sensing for action. If the participants are unaware in terms of reporting but aware in terms of acting would this cause problems with the interpretation?</p>
</disp-quote>
<p>This is an interesting distinction, one that has been widely discussed in the literature. However, it is not clear how to address this in the present context. We have looked at awareness in different ways in prior work with clamped feedback. In general, even when the hand direction might have deviated by &gt;20d, participants report their perceived hand position after the movement as near the target (Tsay et al, 2020). We also have used post-experiment questionnaires to probe whether they thought their movement direction had changed over the course of the experiment (volitionally or otherwise). Again, participants generally insist they moved straight to the target throughout the experiment. So it seems that they unaware of any change in action or perception.</p>
<p>Reaction time data provide additional support that participants are unaware of any change in behavior. The RT function remains flat after the introduction of the clamp, unlike the increases typically observed when participants engage in explicit strategy use (Tsay et al, 2024).</p>
<disp-quote content-type="editor-comment">
<p>Figure 1h: The caption suggests this is from the Wang 2021 paper. However, in the text 180-182 it suggests this might be the map from the current results. Can the authors clarify?</p>
</disp-quote>
<p>Fig 1e is the data from Wang et al, 2021. We formalized an abstract map based on the spatial constrains observed in Fig 1e, and simulated the error at the start and target position based on this abstraction (Fig 1h). We have revised the text to now read (Lines 182-190):</p>
<p>“Motor biases may thus arise from a transformation error between these coordinate systems. Studies in which participants match a visual stimulus to their unseen hand or vice-versa provide one way to estimate this error(Jones et al., 2009; Rincon-Gonzalez et al., 2011; van Beers et al., 1998; Wang et al., 12/2020). Two key features stand out in these data: First, the direction of the visuo-proprioceptive mismatch is similar across the workspace: For right-handers using their dominant limb, the hand is positioned leftward and downward from each target. Second, the magnitude increases with distance from the body (Fig 1d). Using these two empirical constraints, we simulated a visual-proprioceptive error map (Fig. 1h) by applying a leftward and downward error vector whose magnitude scaled with the distance from each location to a reference point.”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations for the authors):</bold></p>
<p>The central idea behind the research seems quite promising, and I applaud the efforts put forth. However, I'm not fully convinced that the current model formulations are plausible explanations. While the dataset is impressively large, it does not appear to be optimally designed to address the complex questions the authors aim to tackle. Moreover, the datasets used to formulate the 3 different model predictions are SMALL and exhibit substantial variability across individuals, and based on average (and thus &quot;smoothed&quot;) data.</p>
</disp-quote>
<p>We hope to have addressed these concerns with the two major changes to revised manuscript: 1) The new experiment in which we examine biases in both angle and extent and 2) the inclusion in the analyses of fits based on individual data sets.</p>
</body>
</sub-article>
</article>