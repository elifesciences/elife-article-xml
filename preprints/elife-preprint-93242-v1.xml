<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">93242</article-id>
<article-id pub-id-type="doi">10.7554/eLife.93242</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.93242.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Microbiology and Infectious Disease</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>An antimicrobial drug recommender system using MALDI-TOF MS and dual-branch neural networks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0367-9699</contrib-id>
<name>
<surname>De Waele</surname>
<given-names>Gaetan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7575-2085</contrib-id>
<name>
<surname>Menschaert</surname>
<given-names>Gerben</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5950-3003</contrib-id>
<name>
<surname>Waegeman</surname>
<given-names>Willem</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Ghent University</institution></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Folkman</surname>
<given-names>Lukas</given-names>
</name>
<role>Reviewing Editor</role>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Garrett</surname>
<given-names>Wendy S</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Harvard T.H. Chan School of Public Health</institution>
</institution-wrap>
<city>Boston</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Correspondence to &lt;<email>gaetan.dewaele@ugent.be</email>&gt;.</corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-05-01">
<day>01</day>
<month>05</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP93242</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-12-14">
<day>14</day>
<month>12</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-10-10">
<day>10</day>
<month>10</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.09.28.559916"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, De Waele et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>De Waele et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-93242-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Timely and effective use of antimicrobial drugs can improve patient outcomes, as well as help safeguard against resistance development. Matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (MALDI-TOF MS) is currently routinely used in clinical diagnostics for rapid species identification. Mining additional data from said spectra in the form of antimicrobial resistance (AMR) profiles is, therefore, highly promising. Such AMR profiles could serve as a drop-in solution for drastically improving treatment efficiency, effectiveness, and costs.</p><p>This study endeavours to develop the first machine learning models capable of predicting AMR profiles for the whole repertoire of species and drugs encountered in clinical microbiology. The resulting model can be interpreted as a drug recommender system for infectious diseases. We find that our dual-branch method delivers considerably higher performance compared to previous approaches. In addition, experiments show that the models can be efficiently fine-tuned to data from other clinical laboratories. MALDI-TOF-based AMR recommender systems can, hence, greatly extend the value of MALDI-TOF MS for clinical diagnostics.</p><p>All code supporting this study is distributed on PyPI and is packaged under: <ext-link ext-link-type="uri" xlink:href="https://github.com/gdewael/maldi-nn">https://github.com/gdewael/maldi-nn</ext-link></p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Recommender systems</kwd>
<kwd>MALDI-TOF MS</kwd>
<kwd>Neural networks</kwd>
<kwd>Antimicrobial resistance</kwd>
</kwd-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Order of authors</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1.</label>
<title>Introduction</title>
<p>In diagnostic laboratories, matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (MALDI-TOF MS) is routinely used for microbial species identification (<xref ref-type="bibr" rid="c1">Hou et al., 2019</xref>). Usually, microbial samples only require an overnight culturing step before being analyzed with mass spectrometry (<xref ref-type="bibr" rid="c2">Van Veen et al., 2010</xref>; <xref ref-type="bibr" rid="c3">Cuénod et al., 2021</xref>). Consequently, the technology provides a time- and cost-efficient way to accurately identify the pathogen underlying an infection.</p>
<p>Due to the rapid evolution of antibiotic resistant strains, it is increasingly difficult to determine a treatment based on only species identity. It has been estimated that infections caused by antibiotic-resistant bacteria have caused the deaths of 1.27 million people in 2019, making AMR one of the leading causes of death on earth (<xref ref-type="bibr" rid="c4">Murray et al., 2022</xref>). Projections have estimated that this annual number could rise to 10 million by 2050 (<xref ref-type="bibr" rid="c5">O’Neill, 2016</xref>), highlighting the need for responsible antimicrobial drug use. In light of this, diagnostic laboratories will often perform various tests, such as dilution arrays or disc diffusion tests, to probe which drug will be effective (<xref ref-type="bibr" rid="c6">Khan et al., 2019</xref>). Such experiments typically require further culturing and are either costly, labor-intensive, time-intensive, or a mixture of the above (<xref ref-type="bibr" rid="c7">Humphries, 2022</xref>).</p>
<p>Given that MALDI-TOF spectra are already routinely used for identification, it is worth investigating to which extent they can contain further information regarding the resistance status of strains (<xref ref-type="bibr" rid="c8">Weis et al., 2020a</xref>). Mining this information from the spectra could help inform healthcare workers of candidate drugs. This may nullify the need for pheno-typical experiments, or (at least) direct the tests by narrowing down the choices. Furthermore, possessing a detailed resistance profile allows to treat with more specifically-working drugs (instead of broad-spectrum antibiotics) (<xref ref-type="bibr" rid="c9">Weis et al., 2022</xref>). Consequently, predicting resistance status from MALDI-TOF spectra could help towards the goals of antibiotic stewardship (<xref ref-type="bibr" rid="c10">Shlaes et al., 1997</xref>).</p>
<p>It has been described that some known resistance mechanisms are outside of the m/z range that MALDI-TOF spectrometers can accurately measure (<xref ref-type="bibr" rid="c7">Humphries, 2022</xref>). Still, it remains largely unknown to which extent co-evolved traits, such as subtle changes in metabolism caused by the resistance mechanism, can be detected by MALDI-TOF spectra. A number of studies have shown that some resistant strains can reliably be predicted from MALDI-TOF MS, either by identifying and detecting specific markers (e.g. peaks) or by learning patterns from data (see §2.1). To our knowledge, all of these studies have modeled AMR prediction for specific species-drug combinations. For this reason, they learn very specific markers of resistance, not guaranteed to extrapolate well to other drugs and species. As susceptibility rapidly evolves, it is practically impossible to perform such studies for all clinically-relevant species-drug combinations. As such, the value of aforementioned studies remains of exploratory nature with limited practical value. In addition, their performance remains limited owing to small sample sizes and, likely, the inability of MALDI-TOF spectra to fully discriminate between the characteristics of interest (<xref ref-type="bibr" rid="c11">Bai et al., 2017</xref>). The recently published DRIAMS dataset (<xref ref-type="bibr" rid="c9">Weis et al., 2022</xref>) contains phenotypic AMR data covering a wide range of species and drugs, allowing to study MALDI-TOF-based AMR prediction on an unprecedented scale.</p>
<p>We posit that the most pertinent challenge health-care workers face regarding AMR is to choose between all possible drugs given an infection, not whether one specific drug will be effective or not. For this reason, we argue that our models and evaluation metrics should be designed to optimally answer that question. In this study, a single model is proposed that can predict AMR for the whole range of pathogens and drugs encountered in clinical microbiology. The method jointly learns representations for antibiotic drugs and bacterial MALDI-TOF spectra. It can be used to recommend the most-likely drug to work for any drug-spectrum combination, irregardless of the species identity of the underlying pathogen. Consequently, the model is broadly-applicable and practical to use. Our contributions are as follows:</p>
<list list-type="order">
<list-item><p>We formulate a dual-branch neural network recommender system for the prediction of AMR profiles. The model operates on MALDI-TOF spectra, as well as a representation of the candidate drug.</p></list-item>
<list-item><p>We evaluate multiple state-of-the-art techniques for representing drug identity in the model.</p></list-item>
<list-item><p>We perform evaluations by comparing our method to non-recommender system baselines.</p></list-item>
<list-item><p>We show that the model efficiently transfers to data from diagnostic laboratories it wasn’t trained on. Making the model easy to adopt for hospitals lacking the means and/or volume to collect large data.</p></list-item>
</list>
</sec>
<sec id="s2">
<label>2.</label>
<title>Related Work</title>
<sec id="s2a">
<label>2.1.</label>
<title>MALDI-TOF-based machine learning</title>
<p>The most canonical task for MALDI-TOF-based methods is species identification. Identification solutions are usually provided by the MS manufacturers and are built on large, proprietary, in-house databases (<xref ref-type="bibr" rid="c12">Van Belkum et al., 2012</xref>). It is unclear how these closed-source identification pipelines work, but it is likely that query spectra are directly compared to the in-house database in an approach akin to nearest neighbors (<xref ref-type="bibr" rid="c13">Dauwalder et al., 2023</xref>). While this approach works excellently for identification of most species, some strains remain problematic (<xref ref-type="bibr" rid="c14">Cao et al., 2018</xref>; <xref ref-type="bibr" rid="c15">Vrioni et al., 2018</xref>). Furthermore, by presumably focusing on the presence or absence of specific peaks, a lot of spectral information stands unused (<xref ref-type="bibr" rid="c16">Florio et al., 2018</xref>).</p>
<p>For various difficult prediction cases, such as strain typing, researchers often resort to machine learning (<xref ref-type="bibr" rid="c17">Hettick et al., 2006</xref>; <xref ref-type="bibr" rid="c18">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="c19">De Bruyne et al., 2011</xref>). Stifled by a historical lack of large open data, machine learning research on MALDI-TOF data remains in its infancy. Most studies have narrow scopes and simple datasets (e.g. binary classification), only warranting standard preprocessing and off-the-shelf learning techniques (<xref ref-type="bibr" rid="c20">Yu et al., 2022</xref>; <xref ref-type="bibr" rid="c21">Zhang et al., 2023</xref>; <xref ref-type="bibr" rid="c22">Chung et al., 2023</xref>). Only a handful of examples exist of more advanced learning techniques specifically adapted to a MALDI-TOF-based task (<xref ref-type="bibr" rid="c23">Mortier et al., 2021</xref>; <xref ref-type="bibr" rid="c8">Weis et al., 2020a</xref>; <xref ref-type="bibr" rid="c24">Vervier et al., 2015</xref>). For a more thorough overview of MALDI-TOF-based machine learning, readers are referred to the review of <xref ref-type="bibr" rid="c25">Weis et al. (2020b)</xref>.</p>
</sec>
<sec id="s2b">
<label>2.2.</label>
<title>Dual-branch neural networks</title>
<p>The idea of processing and combining two separate streams of information with two neural networks is applied in many fields of machine learning, collectively referred to as deep multi-target prediction (<xref ref-type="bibr" rid="c26">Waegeman et al., 2019</xref>; <xref ref-type="bibr" rid="c27">Iliadis et al., 2022</xref>).</p>
<p>In collaborative filtering, the goal is to predict the preference of a user to items (<xref ref-type="bibr" rid="c28">He et al., 2017</xref>). In its most elementary neural form, both users and items are represented by one-hot encodings, generating a model unable to make salient predictions for new users or items without having seen them during training. To solve this, a body of works exists on trying to communicate user- and item-identity to the model via side-information encoded in features (<xref ref-type="bibr" rid="c29">Zheng et al., 2017</xref>).</p>
<p>Dual-branch neural networks are also prevalent in language and vision. Recent advances in (multi-modal) contrastive learning of image (and text) representations often rely on two neural encoders to learn a matching score between two views of the same or discordant objects (<xref ref-type="bibr" rid="c30">Radford et al., 2021</xref>; <xref ref-type="bibr" rid="c31">Chen et al., 2020</xref>). Language retrieval systems typically compare input vectors with a database of key vectors, each derived from a neural network, using approximate nearest neighbor search techniques (<xref ref-type="bibr" rid="c32">Karpukhin et al., 2020</xref>). In biology, fields of research employing dualbranch neural networks include (1) drug-target interaction (<xref ref-type="bibr" rid="c33">Lee et al., 2019</xref>), (2) single-cell multi-omics analysis (<xref ref-type="bibr" rid="c34">Lance et al., 2022</xref>), and (3) transcription factor binding prediction (<xref ref-type="bibr" rid="c35">Yang et al., 2020</xref>), among countless others.</p>
<p>Most of these applications can, to varying extents, be interpreted as (collaborative filtering) recommender systems. For example, contrastive language-image models have been used to retrieve the most-semantically similar images to a piece of text (Beaumont, 2022).</p>
</sec>
</sec>
<sec id="s3">
<label>3.</label>
<title>Methods</title>
<sec id="s3a">
<label>3.1.</label>
<title>Data</title>
<p>To train models, we use the recently published DRI-AMS database, consisting of 765 048 AMR measurements derived from 55 773 spectra across four different hospitals, spanning in total 74 different drugs<sup><xref ref-type="fn" rid="fn1">1</xref></sup> (<xref ref-type="bibr" rid="c9">Weis et al., 2022</xref>). Every drug is characterized by a canonical SMILES string obtained from PubChem (<xref ref-type="bibr" rid="c37">Kim et al., 2023</xref>). As in the original DRIAMS publication, AMR measurements are binarized according to the EUCAST norms per drug. Specifically, intermediate or resistant values are assigned a positive label, and susceptible samples a negative one. Furthermore, spectra are identically processed as in the original publication. Briefly, the following steps are performed: (1) square-root transformation of the intensities, (2) smoothing using a Savitzky-Golay filter with half-window size of 10, (3) baseline correction using 20 iterations of the SNIP algorithm, (4) trimming to the 2000-20000 Da range, (5) intensity calibration so that the total intensity sums to 1, and (6) binning the intensities by summing all values in intervals of 3 Da. After preprocessing, every spectrum is represented as a 6000-dimensional vector.</p>
<p>The main experiments concern models that are trained on data from one hospital only (DRIAMS-A, University Hospital Basel). All spectra and measurements derived from the other three hospitals in DRI-AMS are left out for transfer learning experiments (see §4.3). Within DRIAMS-A, all spectra from before 2018 are allocated to the training set, and all spectra measured during 2018 are evenly split between validation and test set. This split in time reflects a realistic evaluation scenario, as models trained on historical data need to generalize to new patients possibly infected by newly-evolved strains. The final sizes of all splits are as follows: 409 395 labels across 28 331 spectra for the training set, 76 431 labels across 4 994 spectra for the validation set, and 76 133 labels across 4 999 spectra for the test set.</p>
</sec>
<sec id="s3b">
<label>3.2.</label>
<title>Model architecture</title>
<p>We formulate AMR prediction as a multi-target classification problem with side-information for both instances and targets, also referred to as dyadic prediction (<xref ref-type="bibr" rid="c26">Waegeman et al., 2019</xref>). In this context, let us denote a sample in the dataset <italic>𝒟</italic> by a triplet(<bold><italic>s</italic></bold><sub><italic>i</italic></sub>, <bold><italic>d</italic></bold> <sub><italic>j</italic></sub>, <italic>y</italic><sub><italic>i j</italic></sub>), where <italic>y</italic><sub><italic>i j</italic></sub> denotes the resistance label of a microbial spectrum <bold><italic>s</italic></bold><sub><italic>i</italic> ∈ {1,…,<italic>n</italic>}</sub> w.r.t. a drug <bold><italic>d</italic></bold> <sub><italic>j</italic> ∈ {1,…,<italic>m</italic>}</sub>. This dataset can be arranged in an incomplete score matrix<bold><italic>Y</italic></bold> ∈ {0, 1} <sup><italic>n</italic>×<italic>m</italic></sup>. In what follows, the final architectural set-ups used to present the results are described. For details on hyperparameter tuning, readers are referred to <xref ref-type="sec" rid="s7">Appendix B</xref>.</p>
<p>The model consists of two separate neural network embedders <italic>E</italic><sub><italic>s</italic></sub> (⋅) and <italic>E</italic><sub><italic>d</italic></sub> (⋅) for processing the spectra and drugs, respectively. The resulting instance and target embeddings <bold><italic>x</italic></bold><sub><italic>i</italic></sub> and <bold><italic>t</italic></bold> <sub><italic>j</italic></sub> are then combined into a single score by their scaled dot product<inline-formula><inline-graphic xlink:href="559916v3_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (<xref ref-type="bibr" rid="c38">Rendle et al., 2020</xref>). The scaling factor<inline-formula><inline-graphic xlink:href="559916v3_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, with ℎ the dimensionality of both embeddings, is inspired by the formulation of self-attention (<xref ref-type="bibr" rid="c39">Vaswani et al., 2017</xref>). It ensures the dot products to be of manageable magnitudes, even for large values of ℎ. This score can be used together with the sigmoid function and the cross-entropy loss to optimize the two-branch neural network to map a spectrum-drug pair to a resistance label (<xref ref-type="bibr" rid="c27">Iliadis et al., 2022</xref>). An overview of the model is visualized in <xref rid="fig1" ref-type="fig">Figure 1</xref>.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1</label>
<caption><p>Architectural overview of the proposed model. AMR labels of spectrum-drug pairs can be represented in an incomplete matrix. A microbial sample that is susceptible to a drug is denoted by a negative label (orange), whereas positive labels (blue) signify an intermediate or resistant combination. Instance (spectrum) and target (drug) embeddings <bold><italic>x</italic></bold><sub><italic>i</italic></sub> and <bold><italic>t</italic></bold> <sub><italic>j</italic></sub> are obtained from their respective input representations passed through their respective neural network branch. The two resulting embeddings are aggregated to a single score by their (scaled) dot product. The cross-entropy loss optimizes this score to be maximal or minimal for positive or negative combinations of microbial spectra and drugs, respectively. On the upper right-hand side, different metrics are visualized. Whereas micro ROC-AUC takes all prediction-label pairs together, the instance-wise and macro ROC-AUC compute their score per spectrum or drug, respectively, and then average.</p></caption>
<graphic xlink:href="559916v3_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The representations of the instance vectors <bold><italic>x</italic></bold><sub><italic>i</italic></sub> are extracted from a neural network <italic>E</italic><sub><italic>s</italic></sub> (⋅) operating on the processed and binned MALDI-TOF spectra <bold><italic>s</italic></bold><sub><italic>i</italic></sub>. <italic>E</italic><sub><italic>s</italic></sub> (⋅) is parameterized by a multi-layer perceptron (MLP), consisting of a series of fully-connected layers. Between every two such layers, a series of operations consisting of (1) a GeLU activation (<xref ref-type="bibr" rid="c40">Hendrycks and Gimpel, 2016</xref>), (2) a dropout rate of 0.2 (<xref ref-type="bibr" rid="c41">Srivastava et al., 2014</xref>), and (3) layer normalization (<xref ref-type="bibr" rid="c42">Ba et al., 2016</xref>), is applied. We include multiple model sizes in our final results (<xref rid="tbl1" ref-type="table">Table 1</xref>). To make comparisons easier, all models output the same number of hidden dimensions that are used in the dot product, <bold><italic>x</italic></bold><sub><italic>i</italic></sub> ∈ ℝ<sup>64</sup>.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1</label>
<caption><title>All tested model sizes for the (instance) spectrum branch.</title>
<p>Hidden sizes represent the evolution of the hidden state dimensionality as it goes through the model, with every hyphen defining one fully connected layer. The listed number of parameters only include those of the instance (spectrum) branch.</p></caption>
<graphic xlink:href="559916v3_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>Drug identity can be communicated to the model in a number of ways. In this work, we study the following different input representations <bold><italic>d</italic></bold> <sub><italic>j</italic></sub> and embedder <italic>E</italic><sub><italic>d</italic></sub>(⋅) combinations:</p>
<list list-type="order">
<list-item><p>As indices in a one-hot encoding paired with a single linear layer.</p></list-item>
<list-item><p>As Extended Connectivity Fingerprints paired with a single linear layer.</p></list-item>
<list-item><p>As DeepSMILES strings (<xref ref-type="bibr" rid="c43">O’Boyle and Dalke, 2018</xref>) paired with a 1D convolutional neural network (CNN).</p></list-item>
<list-item><p>As DeepSMILES strings paired with a gated recurrent unit neural network (GRU).</p></list-item>
<list-item><p>As DeepSMILES strings paired with a transformer neural network.</p></list-item>
<list-item><p>As images paired with a 2D CNN.</p></list-item>
<list-item><p>As rows of a pre-computed string kernel on the SMILES strings (LINGO (<xref ref-type="bibr" rid="c44">Vidal et al., 2005</xref>)), paired with a single linear layer.</p></list-item>
</list>
<p>For all these combinations, the embedder outputs target embeddings <bold><italic>t</italic></bold> <sub><italic>j</italic></sub> ∈ ℝ<sup>64</sup>. For more details on the different drug embedders and their hyperparam-\ eters (as well as their tuning), see <xref ref-type="sec" rid="s7">Appendix B</xref>. For every combination of spectrum embedder (four sizes: S, M, L, and XL) and drug embedder (seven types), four different learning rates ({1e-4, 5e-4, 1e-3, 5e-3}) are tested. 1e−10 For all these different combinations, three models are trained (using different random seeds for model initialization and batching of data). For every spectrum and drug embedder combination, only results from the best learning rate are presented; that is, the learning rate resulting in the best average validation micro ROC-AUC for that combination.</p>
<p>All models are trained with the Adam optimizer (<xref ref-type="bibr" rid="c45">Kingma and Ba, 2014</xref>) for a maximum of 50 epochs with a batch size of 128. A linear learning rate warm-up over the first 250 steps is applied, after which the rate is kept constant. As every epoch constitutes one pass over every label and, hence, multiple passes over every individual drug and spectrum, a branch can technically already be overfitting before the end of the first epoch. Because of this, performance on the validation set is checked every tenth of an epoch. Training is halted early when validation micro ROC-AUC hasn’t improved for 10 validation set checks. The checkpoint of the best performing model (in terms of validation micro ROC-AUC) is used as the final model.</p>
</sec>
</sec>
<sec id="s4">
<label>4.</label>
<title>Results</title>
<p>The following section will first relay the results of the different dual-branch model configurations. After, a comprehensive comparison is made with baselines consisting of specialist models trained on specific species-drug combinations, as is now common practice in existing works. Finally, the models’ capabilities and representations are examined through transfer learning and embeddings.</p>
<sec id="s4a">
<label>4.1.</label>
<title>Encoding species and drugs effectively</title>
<p><xref rid="fig2" ref-type="fig">Figure 2</xref> shows the performance of all trained models in terms of their areas under the receiver operating characteristic curve (ROC-AUC). The ROC-AUC measures the probability that any positive (resistant or intermediate) sample is assigned a higher predicted probability of being positive as compared to any negative (susceptible) sample. The micro ROC-AUC computes this probability for any pair of positive and negative samples, whereas the instance-wise ROC-AUC only compares pairs within the same spectrum, respectively<sup><xref ref-type="fn" rid="fn2">2</xref></sup> (<xref ref-type="bibr" rid="c46">Waegeman and Dembczynski, 2018</xref>). In a realistic scenario, an AMR recommender system, such as this one, would be used to evaluate which drugs work for new patients. Instance-wise metrics reflect on the average performance across drugs for a single patient. The instance-wise ROC-AUC, for instance, measures the average quality of the ranking of suggested drugs to a patient.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2</label>
<caption><p>Barplots showing test performance results for all trained models. Errorbars represent the standard deviation over three random seeds. The x-axis and colors show the different drug and spectrum embedders, respectively.</p></caption>
<graphic xlink:href="559916v3_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>From <xref rid="fig2" ref-type="fig">Figure 2</xref> it can be seen that, in general, performance differences between model configurations occupy a small margin. However, trends can still be found. One-hot drug embedders typically outperform those using molecular structure information. We offer two hypotheses for this observation: (1) the number of drugs in the training dataset is too small to learn salient representations based on molecular information, and/or (2) wildly different resistance profiles may be obtained from very similar molecular structures. In the latter case, including molecular structure in the model may actually hinder the model, rather than helping it. Among the other drug embedders, the best performance is found in models using Morgan fingerprints. On the spectrum embedder side, it is observed that the small or medium-sized variants typically perform best.</p>
<p>Performance in terms of Macro ROC-AUC and Instance-wise Prec@1 of the negative class can be found in <xref ref-type="sec" rid="s8">Appendix C</xref> <xref rid="fig8" ref-type="fig">Figure 8</xref>. The Macro ROC-AUC averages the ROC-AUC for every individual drug. The Prec@1 evaluates how often the top-ranked prediction is correct. The Prec@1 of the negative class, hence, reports the proportion of cases for which the “most-likely susceptible drug” prediction is actually an effective one. In a scenario where the top recommended drug is always administered, it corresponds to the percentage of correctly-suggested treatments. It can be observed that, in terms of Macro ROC-AUC, Morgan fingerprints (and sometimes other drug embedders) outperform one-hot drug embedders. Hence, molecular structures are a useful inductive bias to learn salient representations for drugs not encountered much in the data. For the Instance-wise Prec@1, a similar observation can be made. The full list of performances can be found in <xref ref-type="sec" rid="s8">Appendix C</xref> <xref rid="tbl4" ref-type="table">Table 4</xref>.</p>
<p>In <xref ref-type="sec" rid="s8">Appendix C</xref> <xref rid="fig9" ref-type="fig">Figure 9</xref>, the performance of the spectrum embedder sizes is compared against a linear baseline. The linear baseline uses the same pre-processed input spectrum representation, but only uses a single linear combination to produce an embedding. For this comparison, only the one-hot and Morgan fingerprint drug embedders are used, as they produce the best-performing models overall. Models using non-linear multi-layer spectrum embedders obtain considerably better performance over linear embedders.</p>
</sec>
<sec id="s4b">
<label>4.2.</label>
<title>Dual-branch modeling improves AMR prediction</title>
<p>Previous studies have studied AMR prediction in specific species-drug combinations (<xref ref-type="bibr" rid="c25">Weis et al., 2020b</xref>). To the best of our knowledge, this study represents the first attempt at modeling MALDI-TOF-based AMR prediction across a wide array of species and drugs using a single model. For this reason, it is useful to compare how the dual-branch setup weighs up against training separate models for separate species and drugs. To test this, binary logistic regression, XGBoost (<xref ref-type="bibr" rid="c47">Chen and Guestrin, 2016</xref>) and MLPs are trained for the 200 combinations of species and drugs for which most training labels are present. The tested MLPs come in the same four sizes as the spectrum branches of the dual-branch models. Other than having an output node of size 1 for binary classification, they share all hyperparameters with the tested spectrum branches.</p>
<p>There exist many species-drug combinations for which there are either only positive or only negative labels. As it is impossible to train and evaluate models for these cases, models are trained only for the 200 most-occurring combinations for which both labels are present in the training, validation and test set. We refer to these models as “specialists”, as they are optimized to recognize patterns for specific cases of AMR. For details on the training and tuning procedure of all baselines, see <xref ref-type="sec" rid="s7b2">Appendix B.2.2</xref>.</p>
<p>The performances of specialist models are compared to two dual-branch recommenders (<xref rid="tbl2" ref-type="table">Table 2</xref>). Two representative dual-branch models are chosen based on their validation micro ROC-AUC. One model is chosen among those using one-hot drug embedders, and another is chosen among those not using a one-hot drug embedder. Performance is computed on the subset of labels spanning the 200 most-common species-drug combinations. This subset spans 4017 spectra, 35 drugs, and 53503 labels (covering 70.28% of the original test set). Dual-branch recommenders outperform specialist baselines on all but one metric. Logistic regression baselines result in the best average ROC-AUC for individual species-drug combinations. By all other metrics, dual-branch recommenders outshine a collection of specialist models. These results are perhaps not surprising, as one could argue that either type of models is designed to be optimal for the metric they outperform the other in. It’s illustrated that, when the question is to choose between drugs for a patient (i.e. instance-wise metrics), a model designed as a recommender will out-perform binary classification models trained to predict AMR for specific drugs. On the other hand, when the pathogen is already known, and only one drug is in consideration, a binary classification model will be optimal.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2</label>
<caption><title>Test performance of selected dual-branch models, compared to the performance of a collection of models — each trained on only one species-drug combination — coined “specialists”.</title>
<p>Performance is computed on the subset of labels spanning the 200 most-common species-drug combinations.</p></caption>
<graphic xlink:href="559916v3_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>Multi-task learning can explain why dual-branch models outperform a collection of specialists (<xref ref-type="bibr" rid="c26">Waegeman et al., 2019</xref>; <xref ref-type="bibr" rid="c48">Caruana, 1997</xref>). As species will often share some common biological processes causing AMR, representations learned on one species may strengthen those for another. In doing so, the combination of data from multiple species in one model helps to produce more expressive spectrum representations overall. Similarly, many antimicrobial drugs share common structure and mechanisms. As such, combining the data from different drugs into one dataset and model may similarly lead to performance gains.</p>
</sec>
<sec id="s4c">
<label>4.3.</label>
<title>Efficient transfer learning to new hospitals</title>
<p>An AMR prediction model trained using data from one hospital may not be suitable for use in other hospitals for several reasons. First, protocols such as sample preparation and culturing media differ from hospital to hospital, resulting in systematic differences in MALDI-TOF spectra (<xref ref-type="bibr" rid="c9">Weis et al., 2022</xref>). Second, epidemiology is spatially varied. Drug-resistant clades may be prevalent in one region or country, but absent in another (<xref ref-type="bibr" rid="c7">Humphries, 2022</xref>). Finally, the MALDI-TOF instruments themselves may also be specific to the hospital and influence the readout. This influences prediction models, as a hospital-specific effect is reported by the study introducing the DRIAMS dataset (<xref ref-type="bibr" rid="c9">Weis et al., 2022</xref>). They find that models typically perform best when trained with data from the same hospital. Here, hospital transferability is studied in the context of transfer learning.</p>
<p>Data from DRIAMS-B, -C and -D, are split into training, validation and test set. The train set for these hospitals consists of 1000 randomly-drawn spectra, simulating a small data scenario where the hospital has not spent considerable efforts in data collection. The remaining spectra for all three hospitals are evenly split among validation and test set.</p>
<p>For all three hospitals, we train models in the same way as previously (see §3.2). A comparison is made between fine-tuning starting from models trained on DRIAMS-A (i.e. models from previous sections) and dual-branch models trained from scratch (<xref rid="fig3" ref-type="fig">Figure 3</xref>). Over all three hospitals, models fine-tuned from a DRIAMS-A checkpoint generally outperform models trained from scratch. This trend holds true over different numbers of spectra available in the fine-tuning set. In general, it can be seen that pre-trained models require very little fine-tuning spectra to obtain performances in the same order of magnitude as with DRIAMS-A (§4.1). Curiously, whereas the one-hot drug embedders typically outperform Morgan fingerprints-based models on DRIAMS-A, here, we see an opposite trend. This effect may be explained by the fact that not all hospitals use the same set of drugs. Whereas a Morgan fingerprint-based drug embedder may (to an extent) instantly generalize to new drugs, a one-hot drug embedder needs to learn a representation from scratch for every drug it hasn’t seen before. Performance comparisons of the same models in terms of other metrics are shown in <xref ref-type="sec" rid="s3">Appendix C</xref> <xref rid="fig10" ref-type="fig">Figure 10</xref></p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3</label>
<caption><p>Transfer learning of DRIAMS-A models to other hospitals. Errorbands show the standard deviation over three runs. Results in terms of other evaluation metrics are shown in <xref ref-type="sec" rid="s3">Appendix C</xref> <xref rid="fig10" ref-type="fig">Figure 10</xref>.</p></caption>
<graphic xlink:href="559916v3_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Lowering the amount of data required is paramount to expedite the uptake of AMR models in clinical diagnostics. The transfer learning qualities of dual-branch models may be ascribed to multiple properties. First of all, since different hospitals use much of the same drugs, transferred drug embedders allow for expressively representing drugs out of the box. Secondly, owing to multi-task learning, even with a limited number of spectra, a considerable fine-tuning dataset may be obtained, as all available data is “thrown on one pile”.</p>
</sec>
<sec id="s4d">
<label>4.4.</label>
<title>MALDI-TOF spectra embeddings</title>
<p>To investigate what the dual-branch models have learned to represent, MALDI-TOF spectra embeddings are examined. For this purpose, the model with the best validation micro ROC-AUC is used, which is a model using a medium-size spectrum embedder and one-hot drug embedder. Here, we visualize the embeddings <bold><italic>x</italic></bold><sub><italic>i</italic></sub> ∈ ℝ<sup>64</sup> of all test set spectra from the 25 most-occurring pathogens. To visualize in a 2-dimensional space, UMAP is applied (using default parameters apart from <monospace>min_dist=0.75</monospace><sup><xref ref-type="fn" rid="fn3">3</xref></sup>). <xref rid="fig4" ref-type="fig">Figure 4</xref> shows the resulting embeddings, colored by species identity, as well as by their AMR status to a selection of drugs.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4</label>
<caption><p>UMAP scatterplots of test set MALDI-TOF spectra embeddings <bold><italic>x</italic></bold><sub><italic>i</italic></sub>. Only embeddings belonging to the 25 most-occurring species in the test set are shown. The panels on the right show the same embeddings as on the left, but colored according to its AMR status to a certain drug. The four displayed drugs are selected based on a ranking of the product of the number of positive and negative labels <inline-formula><inline-graphic xlink:href="559916v3_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. In this way, the drugs that have a lot of observed labels, both positives and negatives, are displayed.</p></caption>
<graphic xlink:href="559916v3_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The MALDI-TOF embeddings are grouped primarily per species. This shows that, without being instructed to discriminate between species, the model has learned to group spectra of the same species together. Furthermore, species under the same genus are typically grouped close together, illustrating that the model can pick up hierarchical relations in the tree of life from the data. Within species clusters, the AMR status subplots show that samples are often grouped according to their resistance. For example, for <italic>S. epidermidis</italic> and <italic>S. aureus</italic>, multidrug resistant variants clearly form subclusters. In addition, the cluster of <italic>E. coli</italic> spectra shows a clear tail with samples resistant to ciprofloxacin. UMAP embedding plots for other drugs are shown in <xref ref-type="sec" rid="s3">Appendix C</xref> <xref rid="fig11" ref-type="fig">Figure 11</xref>.</p>
</sec>
</sec>
<sec id="s5">
<label>5.</label>
<title>Discussion</title>
<p>Prior work on AMR prediction has always modeled within the boundaries of one clade and drug(class), using standard machine learning practices. This work differentiates itself from others by constructing one model for the whole range of pathogens and drugs encountered in clinical diagnostics. We propose to model AMR prediction via dual-branch neural networks, producing a novel MALDI-TOF-based AMR recommender system. The proposed models come with improved performance over the approaches taken in previous works. Only when evaluated on a perpathogen-and-drug basis, it makes more sense to have separate models. It is worth noting, though, that the dual-branch models can also be fine-tuned to similar “specialist” models on a certain of subset of data (spanning, for example, one species and drug). Aside from this point, we argue that, in any case, instance-wise metrics are more appropriate for AMR prediction. They judge the quality of predictions on a patient-per-patient basis, as it would happen in usage of such models in clinical diagnostics.</p>
<p>We postulate that the performance of the proposed models is still limited due to (1) lacking a MALDI-TOF-specific learning architecture, (2) collection of more data, especially on rarely-encountered species and drugs, and (3) inherent technological limitations of MALDI-TOF MS. Whilst the former is the subject of further machine learning research, the latter two can be considered by equipping the model with some notion of uncertainty, epistemic and aleatoric, respectively (<xref ref-type="bibr" rid="c49">Hüllermeier and Waegeman, 2021</xref>). In medical decision-making applications, effective uncertainty estimates would be an invaluable tool to aid understanding the models’ predictions. A fourth factor to consider is that perfect test set performance may also be unattainable due to labeling errors. This comes as a consequence of (1) error-prone laboratory measurements of MIC values, and (2) the fact that EUCAST norms change over time, resulting in outdated label thresholds for historical data.</p>
<p>As bacterial strains readily adapt resistance to new and frequently-used antibiotics, it is impossible for a single machine learning model to maintain its performance over time. Due to this obvious need for continual data collection and online machine learning, it is clear that ML for AMR prediction will prove most valuable when integrated tightly in the inner workings of healthcare (<xref ref-type="bibr" rid="c50">Lee and Lee, 2020</xref>).</p>
<p>It stands to reason that blindly following the recommender system’s predictions spells misery. For example, healthcare practitioners should additionally take into account host-specific factors such as patient age, medical history, and concurrent medication. Additionally, as the model is trained on the whole repertoire of antimicrobial drugs, it will have learnt that broad-spectrum antibiotics are typically effective. Hence, it may overrecommend their use. As a consequence, the model’s proposed treatment strategies may not be aligned with antibiotic stewardship, instead exacerbating the very issue it is designed to mitigate. To tackle this problem, one could downweigh the prediction probabilities of undesirable drugs, or, alternatively, train a dual-branch model on only more-specifically-working drugs.</p>
<p>In summary, this study serves as the first proof-of-concept for large MALDI-TOF-based antimicrobial drug recommenders. In this context, we highlight the need for appropriate metrics, proposing that instance-wise metrics are most suitable. Extensive experiments on our proposed dual-branch model allow us to assemble some conclusions w.r.t. its use. Firstly, given the present data at hand, medium-sized MLP spectrum embedders (counting 3.2M weights) generally perform best. Second, embedding drugs without structural information (by one-hot encoding) works best for the larger DRIAMS-A dataset. For the smaller datasets used in the transfer learning experiments, the structural inductive bias lent to the model via Morgan fingerprints delivers best results. Our experiments demonstrate that dual-branch recommenders outperform non-recommender baselines on relevant metrics. In the above discussion, some considerations are listed w.r.t. its practical implementation in health-care. Taken together, this work demonstrates the potential of AMR recommenders to greatly extend the value of MALDI-TOF MS for clinical diagnostics.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by Research Foundation - Flanders (FWO) [PhD Fellowship fundamental research grant 1153024N to G.D.W.]. W.W. also received funding from the Flemish Government under the “Onderzoeksprogramma Artificiële Intelligentie (AI) Vlaanderen” Programme</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><given-names>Tsung-Yun</given-names> <surname>Hou</surname></string-name>, <string-name><given-names>Chuan</given-names> <surname>Chiang-Ni</surname></string-name>, and <string-name><given-names>Shih-Hua</given-names> <surname>Teng</surname></string-name>. <article-title>Current status of maldi-tof mass spectrometry in clinical microbiology</article-title>. <source>Journal of food and drug analysis</source>, <volume>27</volume>(<issue>2</issue>):<fpage>404</fpage>–<lpage>414</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><given-names>SQ</given-names> <surname>Van Veen</surname></string-name>, <string-name><given-names>ECJ</given-names> <surname>Claas</surname></string-name>, and <string-name><given-names>Ed J</given-names> <surname>Kuijper</surname></string-name>. <article-title>Highthroughput identification of bacteria and yeast by matrix-assisted laser desorption ionization-time of flight mass spectrometry in conventional medical microbiology laboratories</article-title>. <source>Journal of clinical microbiology</source>, <volume>48</volume>(<issue>3</issue>):<fpage>900</fpage>–<lpage>907</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><given-names>Aline</given-names> <surname>Cuénod</surname></string-name>, <string-name><given-names>Frédéric</given-names> <surname>Foucault</surname></string-name>, <string-name><surname>Valentin</surname> <given-names>Pflüger</given-names></string-name>, and <string-name><given-names>Adrian</given-names> <surname>Egli</surname></string-name>. <article-title>Factors associated with maldi-tof mass spectral quality of species identification in clinical routine diagnostics</article-title>. <source>Frontiers in Cellular and Infection Microbiology</source>, <volume>11</volume>:<issue>646648</issue>, <year>2021</year>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><given-names>Christopher JL</given-names> <surname>Murray</surname></string-name>, <string-name><given-names>Kevin Shunji</given-names> <surname>Ikuta</surname></string-name>, <string-name><given-names>Fablina</given-names> <surname>Sharara</surname></string-name>, <string-name><given-names>Lucien</given-names> <surname>Swetschinski</surname></string-name>, <string-name><given-names>Gisela Robles</given-names> <surname>Aguilar</surname></string-name>, <string-name><given-names>Authia</given-names> <surname>Gray</surname></string-name>, <string-name><given-names>Chieh</given-names> <surname>Han</surname></string-name>, <string-name><given-names>Catherine</given-names> <surname>Bisignano</surname></string-name>, <string-name><given-names>Puja</given-names> <surname>Rao</surname></string-name>, <string-name><given-names>Eve</given-names> <surname>Wool</surname></string-name>, <etal>et al.</etal> <article-title>Global burden of bacterial antimicrobial resistance in 2019: a systematic analysis</article-title>. <source>The Lancet</source>, <volume>399</volume>(<issue>10325</issue>):<fpage>629</fpage>–<lpage>655</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="other"><string-name><given-names>Jim</given-names> <surname>O’Neill</surname></string-name>. <source>Tackling drug-resistant infections globally: final report and recommendations</source>. <year>2016</year>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><given-names>Zeeshan A</given-names> <surname>Khan</surname></string-name>, <string-name><given-names>Mohd F</given-names> <surname>Siddiqui</surname></string-name>, and <string-name><given-names>Seungkyung</given-names> <surname>Park</surname></string-name>. <article-title>Current and emerging methods of antibiotic susceptibility testing</article-title>. <source>Diagnostics</source>, <volume>9</volume>(<issue>2</issue>):<fpage>49</fpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><given-names>Romney M</given-names> <surname>Humphries</surname></string-name>. <article-title>Ad hoc antimicrobial susceptibility testing from maldi-tof ms spectra in the clinical microbiology laboratory</article-title>. <source>Clinical Chemistry</source>, <volume>68</volume> (<issue>9</issue>):<fpage>1118</fpage>–<lpage>1120</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><given-names>Caroline</given-names> <surname>Weis</surname></string-name>, <string-name><given-names>Max</given-names> <surname>Horn</surname></string-name>, <string-name><given-names>Bastian</given-names> <surname>Rieck</surname></string-name>, <string-name><given-names>Aline</given-names> <surname>Cuénod</surname></string-name>, <string-name><given-names>Adrian</given-names> <surname>Egli</surname></string-name>, and <string-name><given-names>Karsten</given-names> <surname>Borgwardt</surname></string-name>. <article-title>Topological and kernel-based microbial phenotype prediction from maldi-tof mass spectra</article-title>. <source>Bioinformatics</source>, <volume>36</volume>(<issue>Supplement_1</issue>):<fpage>i30</fpage>–<lpage>i38</lpage>, <year>2020a</year>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><given-names>Caroline</given-names> <surname>Weis</surname></string-name>, <string-name><given-names>Aline</given-names> <surname>Cuénod</surname></string-name>, <string-name><given-names>Bastian</given-names> <surname>Rieck</surname></string-name>, <string-name><given-names>Olivier</given-names> <surname>Dubuis</surname></string-name>, <string-name><given-names>Susanne</given-names> <surname>Graf</surname></string-name>, <string-name><given-names>Claudia</given-names> <surname>Lang</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Oberle</surname></string-name>, <string-name><given-names>Maximilian</given-names> <surname>Brackmann</surname></string-name>, <string-name><given-names>Kirstine K</given-names> <surname>Søgaard</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Osthoff</surname></string-name>, <etal>et al.</etal> <article-title>Direct antimicrobial resistance prediction from clinical maldi-tof mass spectra using machine learning</article-title>. <source>Nature Medicine</source>, <volume>28</volume>(<issue>1</issue>):<fpage>164</fpage>–<lpage>174</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><given-names>David M</given-names> <surname>Shlaes</surname></string-name>, <string-name><given-names>Dale N</given-names> <surname>Gerding</surname></string-name>, <string-name><given-names>Joseph F</given-names> <surname>John</surname></string-name>, <string-name><given-names>William A</given-names> <surname>Craig</surname></string-name>, <string-name><given-names>Donald L</given-names> <surname>Bornstein</surname></string-name>, <string-name><given-names>Robert A</given-names> <surname>Duncan</surname></string-name>, <string-name><given-names>Mark R</given-names> <surname>Eckman</surname></string-name>, <string-name><given-names>William E</given-names> <surname>Farrer</surname></string-name>, <string-name><given-names>William H</given-names> <surname>Greene</surname></string-name>, <string-name><given-names>Victor</given-names> <surname>Lorian</surname></string-name>, <etal>et al.</etal> <article-title>Society for healthcare epidemiology of america and infectious diseases society of america joint committee on the prevention of antimicrobial resistance guidelines for the prevention of antimicrobial resistance in hospitals</article-title>. <source>Infection Control &amp; Hospital Epidemiology</source>, <volume>18</volume>(<issue>4</issue>):<fpage>275</fpage>–<lpage>291</lpage>, <year>1997</year>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="other"><string-name><given-names>J</given-names> <surname>Bai</surname></string-name>, <string-name><given-names>ZC</given-names> <surname>Fan</surname></string-name>, <string-name><given-names>LP</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>XY</given-names> <surname>Xu</surname></string-name>, and <string-name><given-names>ZL</given-names> <surname>Zhang</surname></string-name>. <article-title>Classification of methicillin-resistant and methicillinsusceptible staphylococcus aureus using an improved genetic algorithm for feature selection based on mass spectra</article-title>. <source>In Proceedings of the 9th International Conference on Bioinformatics and Biomedical Technology</source>, pages <fpage>57</fpage>–<lpage>63</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><given-names>Alex</given-names> <surname>Van Belkum</surname></string-name>, <string-name><given-names>Martin</given-names> <surname>Welker</surname></string-name>, <string-name><given-names>Marcel</given-names> <surname>Erhard</surname></string-name>, and <string-name><given-names>Sonia</given-names> <surname>Chatellier</surname></string-name>. <article-title>Biomedical mass spectrometry in today’s and tomorrow’s clinical microbiology laboratories</article-title>. <source>Journal of clinical microbiology</source>, <volume>50</volume>(<issue>5</issue>): <fpage>1513</fpage>–<lpage>1517</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><given-names>Olivier</given-names> <surname>Dauwalder</surname></string-name>, <string-name><given-names>Tiphaine</given-names> <surname>Cecchini</surname></string-name>, <string-name><given-names>Jean Philippe</given-names> <surname>Rasigade</surname></string-name>, and <string-name><given-names>François</given-names> <surname>Vandenesch</surname></string-name>. <article-title>Matrix assisted laser desorption ionisation/time of flight (maldi/tof) mass spectrometry is not done revolutionizing clinical microbiology diagnostic</article-title>. <source>Clinical Microbiology and Infection</source>, <volume>29</volume>(<issue>2</issue>):<fpage>127</fpage>–<lpage>129</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><given-names>Yan</given-names> <surname>Cao</surname></string-name>, <string-name><given-names>Lei</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Ping</given-names> <surname>Ma</surname></string-name>, <string-name><given-names>Wenting</given-names> <surname>Fan</surname></string-name>, <string-name><given-names>Bing</given-names> <surname>Gu</surname></string-name>, and <string-name><given-names>Shaoqing</given-names> <surname>Ju</surname></string-name>. <article-title>Accuracy of matrix-assisted laser desorption ionization–time of flight mass spectrometry for identification of mycobacteria: a systematic review and meta-analysis</article-title>. <source>Scientific reports</source>, <volume>8</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>9</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><given-names>Georgia</given-names> <surname>Vrioni</surname></string-name>, <string-name><given-names>Constantinos</given-names> <surname>Tsiamis</surname></string-name>, <string-name><given-names>George</given-names> <surname>Oikonomidis</surname></string-name>, <string-name><given-names>Kalliopi</given-names> <surname>Theodoridou</surname></string-name>, <string-name><given-names>Violeta</given-names> <surname>Kapsimali</surname></string-name>, and <string-name><given-names>Athanasios</given-names> <surname>Tsakris</surname></string-name>. <article-title>Maldi-tof mass spectrometry technology for detecting biomarkers of antimicrobial resistance: current achievements and future perspectives</article-title>. <source>Annals of translational medicine</source>, <volume>6</volume> (<issue>12</issue>), <year>2018</year>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><given-names>Walter</given-names> <surname>Florio</surname></string-name>, <string-name><given-names>Arianna</given-names> <surname>Tavanti</surname></string-name>, <string-name><given-names>Simona</given-names> <surname>Barnini</surname></string-name>, <string-name><given-names>Emilia</given-names> <surname>Ghelardi</surname></string-name>, and <string-name><given-names>Antonella</given-names> <surname>Lupetti</surname></string-name>. <article-title>Recent advances and ongoing challenges in the diagnosis of microbial infections by maldi-tof mass spectrometry</article-title>. <source>Frontiers in microbiology</source>, <volume>9</volume>:<issue>1097</issue>, <year>2018</year>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><given-names>Justin M</given-names> <surname>Hettick</surname></string-name>, <string-name><given-names>Michael L</given-names> <surname>Kashon</surname></string-name>, <string-name><given-names>James E</given-names> <surname>Slaven</surname></string-name>, <string-name><given-names>Yan</given-names> <surname>Ma</surname></string-name>, <string-name><given-names>Janet P</given-names> <surname>Simpson</surname></string-name>, <string-name><given-names>Paul D</given-names> <surname>Siegel</surname></string-name>, <string-name><given-names>Gerald N</given-names> <surname>Mazurek</surname></string-name>, and <string-name><given-names>David N</given-names> <surname>Weissman</surname></string-name>. <article-title>Discrimination of intact mycobacteria at the strain level: a combined maldi-tof ms and biostatistical analysis</article-title>. <source>Proteomics</source>, <volume>6</volume>(<issue>24</issue>):<fpage>6416</fpage>–<lpage>6425</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><given-names>Hsin-Yao</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Tzong-Yi</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>Yi-Ju</given-names> <surname>Tseng</surname></string-name>, <string-name><given-names>Tsui-Ping</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Kai-Yao</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>Yung-Ta</given-names> <surname>Chang</surname></string-name>, <string-name><given-names>Chun-Hsien</given-names> <surname>Chen</surname></string-name>, and <string-name><given-names>Jang-Jih</given-names> <surname>Lu</surname></string-name>. <article-title>A new scheme for strain typing of methicillin-resistant staphylococcus aureus on the basis of matrix-assisted laser desorption ionization time-of-flight mass spectrometry by using machine learning approach</article-title>. <source>PloS one</source>, <volume>13</volume>(<issue>3</issue>): <fpage>e0194289</fpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><given-names>Katrien</given-names> <surname>De Bruyne</surname></string-name>, <string-name><given-names>Bram</given-names> <surname>Slabbinck</surname></string-name>, <string-name><given-names>Willem</given-names> <surname>Waegeman</surname></string-name>, <string-name><given-names>Paul</given-names> <surname>Vauterin</surname></string-name>, <string-name><given-names>Bernard</given-names> <surname>De Baets</surname></string-name>, and <string-name><given-names>Peter</given-names> <surname>Vandamme</surname></string-name>. <article-title>Bacterial species identification from maldi-tof mass spectra through data analysis and machine learning</article-title>. <source>Systematic and applied microbiology</source>, <volume>34</volume>(<issue>1</issue>):<fpage>20</fpage>–<lpage>29</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><given-names>Jiaxin</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>Ni</given-names> <surname>Tien</surname></string-name>, <string-name><given-names>Yu-Ching</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Der-Yang</given-names> <surname>Cho</surname></string-name>, <string-name><given-names>JiaWen</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Yin-Tai</given-names> <surname>Tsai</surname></string-name>, <string-name><given-names>Yu-Chen</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>Huei-Jen</given-names> <surname>Chao</surname></string-name>, and <string-name><given-names>Chao-Jung</given-names> <surname>Chen</surname></string-name>. <article-title>Rapid identification of methicillin-resistant staphylococcus aureus using maldi-tof ms and machine learning from over 20,000 clinical isolates</article-title>. <source>Microbiology Spectrum</source>, <volume>10</volume> (<issue>2</issue>):<fpage>e00483</fpage>–<lpage>22</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><given-names>Yu-Ming</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Mei-Fen</given-names> <surname>Tsao</surname></string-name>, <string-name><given-names>Ching-Yu</given-names> <surname>Chang</surname></string-name>, <string-name><given-names>Kuan-Ting</given-names> <surname>Lin</surname></string-name>, <string-name><given-names>Joseph Jordan</given-names> <surname>Keller</surname></string-name>, and <string-name><given-names>Hsiu-Chen</given-names> <surname>Lin</surname></string-name>. <article-title>Rapid identification of carbapenem-resistant klebsiella pneumoniae based on matrix-assisted laser desorption ionization time-of-flight mass spectrometry and an artificial neural network model</article-title>. <source>Journal of Biomedical Science</source>, <volume>30</volume>(<issue>1</issue>):<fpage>25</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="other"><string-name><given-names>Chia-Ru</given-names> <surname>Chung</surname></string-name>, <string-name><given-names>Hsin-Yao</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Chun-Han</given-names> <surname>Yao</surname></string-name>, <string-name><given-names>LiChing</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>Jang-Jih</given-names> <surname>Lu</surname></string-name>, <string-name><given-names>Jorng-Tzong</given-names> <surname>Horng</surname></string-name>, and <string-name><given-names>Tzong-Yi</given-names> <surname>Lee</surname></string-name>. <article-title>Data-driven two-stage framework for identification and characterization of different antibiotic-resistant escherichia coli isolates based on mass spectrometry data</article-title>. <source>Microbiology Spectrum</source>, pages <fpage>e03479</fpage>–<lpage>22</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><given-names>Thomas</given-names> <surname>Mortier</surname></string-name>, <string-name><given-names>Anneleen D</given-names> <surname>Wieme</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Vandamme</surname></string-name>, and <string-name><given-names>Willem</given-names> <surname>Waegeman</surname></string-name>. <article-title>Bacterial species identification using maldi-tof mass spectrometry and machine learning techniques: A large-scale benchmarking study</article-title>. <source>Computational and Structural Biotechnology Journal</source>, <volume>19</volume>:<fpage>6157</fpage>–<lpage>6168</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="other"><string-name><given-names>Kévin</given-names> <surname>Vervier</surname></string-name>, <string-name><given-names>Pierre</given-names> <surname>Mahé</surname></string-name>, <string-name><given-names>Jean-Baptiste</given-names> <surname>Veyrieras</surname></string-name>, and <string-name><given-names>Jean-Philippe</given-names> <surname>Vert</surname></string-name>. <article-title>Benchmark of structured machine learning methods for microbial identification from mass-spectrometry data</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1506.07251</pub-id>, <year>2015</year>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><given-names>Caroline V</given-names> <surname>Weis</surname></string-name>, <string-name><given-names>Catherine R</given-names> <surname>Jutzeler</surname></string-name>, and <string-name><given-names>Karsten</given-names> <surname>Borgwardt</surname></string-name>. <article-title>Machine learning for microbial identification and antimicrobial susceptibility testing on maldi-tof mass spectra: a systematic review</article-title>. <source>Clinical Microbiology and Infection</source>, <volume>26</volume>(<issue>10</issue>):<fpage>1310</fpage>–<lpage>1317</lpage>, <year>2020b</year>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><given-names>Willem</given-names> <surname>Waegeman</surname></string-name>, <string-name><given-names>Krzysztof</given-names> <surname>Dembczyński</surname></string-name>, and <string-name><given-names>Eyke</given-names> <surname>Hüllermeier</surname></string-name>. <article-title>Multi-target prediction: a unifying view on problems and methods</article-title>. <source>Data Mining and Knowledge Discovery</source>, <volume>33</volume>:<fpage>293</fpage>–<lpage>324</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="other"><string-name><given-names>Dimitrios</given-names> <surname>Iliadis</surname></string-name>, <string-name><given-names>Bernard</given-names> <surname>De Baets</surname></string-name>, and <string-name><given-names>Willem</given-names> <surname>Waegeman</surname></string-name>. <article-title>Multi-target prediction for dummies using two-branch neural networks</article-title>. <source>Machine Learning</source>, pages <fpage>1</fpage>–<lpage>34</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="other"><string-name><given-names>Xiangnan</given-names> <surname>He</surname></string-name>, <string-name><given-names>Lizi</given-names> <surname>Liao</surname></string-name>, <string-name><given-names>Hanwang</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Liqiang</given-names> <surname>Nie</surname></string-name>, <string-name><given-names>Xia</given-names> <surname>Hu</surname></string-name>, and <string-name><given-names>Tat-Seng</given-names> <surname>Chua</surname></string-name>. <article-title>Neural collaborative filtering</article-title>. <source>In Proceedings of the 26th international conference on world wide web</source>, pages <fpage>173</fpage>–<lpage>182</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="other"><string-name><given-names>Lei</given-names> <surname>Zheng</surname></string-name>, <string-name><given-names>Vahid</given-names> <surname>Noroozi</surname></string-name>, and <string-name><given-names>Philip S</given-names> <surname>Yu</surname></string-name>. <article-title>Joint deep modeling of users and items using reviews for recommendation</article-title>. <source>In Proceedings of the tenth ACM international conference on web search and data mining</source>, pages <fpage>425</fpage>–<lpage>434</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="book"><string-name><given-names>Alec</given-names> <surname>Radford</surname></string-name>, <string-name><given-names>Jong Wook</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>Chris</given-names> <surname>Hallacy</surname></string-name>, <string-name><given-names>Aditya</given-names> <surname>Ramesh</surname></string-name>, <string-name><given-names>Gabriel</given-names> <surname>Goh</surname></string-name>, <string-name><given-names>Sandhini</given-names> <surname>Agarwal</surname></string-name>, <string-name><given-names>Girish</given-names> <surname>Sastry</surname></string-name>, <string-name><given-names>Amanda</given-names> <surname>Askell</surname></string-name>, <string-name><given-names>Pamela</given-names> <surname>Mishkin</surname></string-name>, <string-name><given-names>Jack</given-names> <surname>Clark</surname></string-name>, <etal>et al.</etal> <chapter-title>Learning transferable visual models from natural language supervision</chapter-title>. <source>In International conference on machine learning</source>, pages <fpage>8748</fpage>–<lpage>8763</lpage>. <publisher-name>PMLR</publisher-name>, <year>2021</year>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="book"><string-name><given-names>Ting</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Simon</given-names> <surname>Kornblith</surname></string-name>, <string-name><given-names>Mohammad</given-names> <surname>Norouzi</surname></string-name>, and <string-name><given-names>Geoffrey</given-names> <surname>Hinton</surname></string-name>. <chapter-title>A simple framework for contrastive learning of visual representations</chapter-title>. <source>In International conference on machine learning</source>, pages <fpage>1597</fpage>–<lpage>1607</lpage>. <publisher-name>PMLR</publisher-name>, <year>2020</year>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="other"><string-name><given-names>Vladimir</given-names> <surname>Karpukhin</surname></string-name>, <string-name><given-names>Barlas</given-names> <surname>Oğuz</surname></string-name>, <string-name><given-names>Sewon</given-names> <surname>Min</surname></string-name>, <string-name><given-names>Patrick</given-names> <surname>Lewis</surname></string-name>, <string-name><given-names>Ledell</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>Sergey</given-names> <surname>Edunov</surname></string-name>, <string-name><given-names>Danqi</given-names> <surname>Chen</surname></string-name>, and <string-name><given-names>Wen-tau</given-names> <surname>Yih</surname></string-name>. <article-title>Dense passage retrieval for open-domain question answering</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2004.04906</pub-id>, <year>2020</year>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><given-names>Ingoo</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>Jongsoo</given-names> <surname>Keum</surname></string-name>, and <string-name><given-names>Hojung</given-names> <surname>Nam</surname></string-name>. <article-title>Deepconv-dti: Prediction of drug-target interactions via deep learning with convolution on protein sequences</article-title>. <source>PLoS computational biology</source>, <volume>15</volume> (<issue>6</issue>):<fpage>e1007129</fpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="other"><string-name><given-names>Christopher</given-names> <surname>Lance</surname></string-name>, <string-name><given-names>Malte D</given-names> <surname>Luecken</surname></string-name>, <string-name><given-names>Daniel B</given-names> <surname>Burkhardt</surname></string-name>, <string-name><given-names>Robrecht</given-names> <surname>Cannoodt</surname></string-name>, <string-name><given-names>Pia</given-names> <surname>Rautenstrauch</surname></string-name>, <string-name><given-names>Anna Christine</given-names> <surname>Laddach</surname></string-name>, <string-name><given-names>Aidyn</given-names> <surname>Ubingazhibov</surname></string-name>, <string-name><given-names>Zhi-Jie</given-names> <surname>Cao</surname></string-name>, <string-name><given-names>Kaiwen</given-names> <surname>Deng</surname></string-name>, <string-name><given-names>Sumeer</given-names> <surname>Khan</surname></string-name>, <etal>et al.</etal> <article-title>Multimodal single cell data integration challenge: results and lessons learned</article-title>. <source>bioRxiv</source>, pages <fpage>2022</fpage>–<lpage>04</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><given-names>Shu</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>Xiaoxi</given-names> <surname>Liu</surname></string-name>, and <string-name><given-names>Raymond T</given-names> <surname>Ng</surname></string-name>. <article-title>Proberating: a recommender system to infer binding profiles for nucleic acid-binding proteins</article-title>. <source>Bioinformatics</source>, <volume>36</volume>(<issue>18</issue>):<fpage>4797</fpage>–<lpage>4804</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="web"><collab>Romain Beaumont</collab>. <source>Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with them</source>. <ext-link ext-link-type="uri" xlink:href="https://github.com/rom1504/clip-retrieval">https://github.com/rom1504/clip-retrieval</ext-link>, <year>2022</year>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><given-names>Sunghwan</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>Jie</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Tiejun</given-names> <surname>Cheng</surname></string-name>, <string-name><given-names>Asta</given-names> <surname>Gindulyte</surname></string-name>, <string-name><given-names>Jia</given-names> <surname>He</surname></string-name>, <string-name><given-names>Siqian</given-names> <surname>He</surname></string-name>, <string-name><given-names>Qingliang</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Benjamin A</given-names> <surname>Shoemaker</surname></string-name>, <string-name><given-names>Paul A</given-names> <surname>Thiessen</surname></string-name>, <string-name><given-names>Bo</given-names> <surname>Yu</surname></string-name>, <etal>et al.</etal> <article-title>Pubchem 2023 update</article-title>. <source>Nucleic acids research</source>, <volume>51</volume>(<issue>D1</issue>):<fpage>D1373</fpage>– <lpage>D1380</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="other"><string-name><given-names>Steffen</given-names> <surname>Rendle</surname></string-name>, <string-name><given-names>Walid</given-names> <surname>Krichene</surname></string-name>, <string-name><given-names>Li</given-names> <surname>Zhang</surname></string-name>, and <string-name><given-names>John</given-names> <surname>Anderson</surname></string-name>. <article-title>Neural collaborative filtering vs. matrix factorization revisited</article-title>. <source>In Proceedings of the 14th ACM Conference on Recommender Systems</source>, pages <fpage>240</fpage>–<lpage>248</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><given-names>Ashish</given-names> <surname>Vaswani</surname></string-name>, <string-name><given-names>Noam</given-names> <surname>Shazeer</surname></string-name>, <string-name><given-names>Niki</given-names> <surname>Parmar</surname></string-name>, <string-name><given-names>Jakob</given-names> <surname>Uszkoreit</surname></string-name>, <string-name><given-names>Llion</given-names> <surname>Jones</surname></string-name>, <string-name><given-names>Aidan N</given-names> <surname>Gomez</surname></string-name>, Ł<string-name><given-names>ukasz</given-names> <surname>Kaiser</surname></string-name>, and <string-name><given-names>Illia</given-names> <surname>Polosukhin</surname></string-name>. <article-title>Attention is all you need</article-title>. <source>Advances in neural information processing systems</source>, <volume>30</volume>, <year>2017</year>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="other"><string-name><given-names>Dan</given-names> <surname>Hendrycks</surname></string-name> and <string-name><given-names>Kevin</given-names> <surname>Gimpel</surname></string-name>. <article-title>Gaussian error linear units (gelus)</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1606.08415</pub-id>, <year>2016</year>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><given-names>Nitish</given-names> <surname>Srivastava</surname></string-name>, <string-name><given-names>Geoffrey</given-names> <surname>Hinton</surname></string-name>, <string-name><given-names>Alex</given-names> <surname>Krizhevsky</surname></string-name>, <string-name><given-names>Ilya</given-names> <surname>Sutskever</surname></string-name>, and <string-name><given-names>Ruslan</given-names> <surname>Salakhutdinov</surname></string-name>. <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>. <source>The journal of machine learning research</source>, <volume>15</volume> (<issue>1</issue>):<fpage>1929</fpage>–<lpage>1958</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="other"><string-name><given-names>Jimmy Lei</given-names> <surname>Ba</surname></string-name>, <string-name><given-names>Jamie Ryan</given-names> <surname>Kiros</surname></string-name>, and <string-name><given-names>Geoffrey E</given-names> <surname>Hinton</surname></string-name>. <article-title>Layer normalization</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1607.06450</pub-id>, <year>2016</year>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="other"><string-name><given-names>Noel</given-names> <surname>O’Boyle</surname></string-name> and <string-name><given-names>Andrew</given-names> <surname>Dalke</surname></string-name>. <source>Deepsmiles: an adaptation of smiles for use in machine-learning of chemical structures</source>. <year>2018</year>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><given-names>David</given-names> <surname>Vidal</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Thormann</surname></string-name>, and <string-name><given-names>Miquel</given-names> <surname>Pons</surname></string-name>. <article-title>Lingo, an efficient holographic text based method to calculate biophysical properties and intermolecular similarities</article-title>. <source>Journal of chemical information and modeling</source>, <volume>45</volume>(<issue>2</issue>):<fpage>386</fpage>–<lpage>393</lpage>, <year>2005</year>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="other"><string-name><given-names>Diederik P</given-names> <surname>Kingma</surname></string-name> and <string-name><given-names>Jimmy</given-names> <surname>Ba</surname></string-name>. <article-title>Adam: A method for stochastic optimization</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1412.6980</pub-id>, <year>2014</year>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="other"><string-name><given-names>Willem</given-names> <surname>Waegeman</surname></string-name> and <string-name><given-names>Krzysztof</given-names> <surname>Dembczynski</surname></string-name>. <article-title>Multi-target prediction: a unifying view on problems and methods</article-title>. <source>Tutorial presented at ECML/PKDD 2018</source>, <year>2018</year>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="other"><string-name><given-names>Tianqi</given-names> <surname>Chen</surname></string-name> and <string-name><given-names>Carlos</given-names> <surname>Guestrin</surname></string-name>. <article-title>Xgboost: A scalable tree boosting system</article-title>. <source>In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</source>, pages <fpage>785</fpage>–<lpage>794</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><given-names>Rich</given-names> <surname>Caruana</surname></string-name>. <article-title>Multitask learning</article-title>. <source>Machine learning</source>, <volume>28</volume>:<fpage>41</fpage>–<lpage>75</lpage>, <year>1997</year>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><given-names>Eyke</given-names> <surname>Hüllermeier</surname></string-name> and <string-name><given-names>Willem</given-names> <surname>Waegeman</surname></string-name>. <article-title>Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods</article-title>. <source>Machine Learning</source>, <volume>110</volume>:<fpage>457</fpage>–<lpage>506</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><given-names>Cecilia S</given-names> <surname>Lee</surname></string-name> and <string-name><given-names>Aaron Y</given-names> <surname>Lee</surname></string-name>. <article-title>Clinical applications of continual learning machine learning</article-title>. <source>The Lancet Digital Health</source>, <volume>2</volume>(<issue>6</issue>):<fpage>e279</fpage>–<lpage>e281</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><given-names>Alice</given-names> <surname>Capecchi</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Probst</surname></string-name>, and <string-name><given-names>Jean-Louis</given-names> <surname>Reymond</surname></string-name>. <article-title>One molecular fingerprint to rule them all: drugs, biomolecules, and the metabolome</article-title>. <source>Journal of cheminformatics</source>, <volume>12</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>15</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><given-names>Greg</given-names> <surname>Landrum</surname></string-name>. <article-title>Rdkit documentation</article-title>. <source>Release</source>, <volume>1</volume>(<issue>1-79</issue>): <fpage>4</fpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><given-names>David</given-names> <surname>Weininger</surname></string-name>. <article-title>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules</article-title>. <source>Journal of chemical information and computer sciences</source>, <volume>28</volume>(<issue>1</issue>):<fpage>31</fpage>–<lpage>36</lpage>, <year>1988</year>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="other"><string-name><given-names>Noam</given-names> <surname>Shazeer</surname></string-name>. <article-title>Glu variants improve transformer</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2002.05202</pub-id>, <year>2020</year>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="other"><string-name><given-names>Zhuang</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Hanzi</given-names> <surname>Mao</surname></string-name>, <string-name><given-names>Chao-Yuan</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>Christoph</given-names> <surname>Feichtenhofer</surname></string-name>, <string-name><given-names>Trevor</given-names> <surname>Darrell</surname></string-name>, and <string-name><given-names>Saining</given-names> <surname>Xie</surname></string-name>. <article-title>A convnet for the 2020s</article-title>. <source>In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</source>, pages <fpage>11976</fpage>–<lpage>11986</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="other"><string-name><given-names>Kyunghyun</given-names> <surname>Cho</surname></string-name>, <string-name><given-names>Bart Van</given-names> <surname>Merriënboer</surname></string-name>, <string-name><given-names>Caglar</given-names> <surname>Gulcehre</surname></string-name>, <string-name><given-names>Dzmitry</given-names> <surname>Bahdanau</surname></string-name>, <string-name><given-names>Fethi</given-names> <surname>Bougares</surname></string-name>, <string-name><given-names>Holger</given-names> <surname>Schwenk</surname></string-name>, and <string-name><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name>. <article-title>Learning phrase representations using rnn encoder-decoder for statistical machine translation</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1406.1078</pub-id>, <year>2014</year>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><given-names>Hakime</given-names> <surname>Öztürk</surname></string-name>, <string-name><given-names>Elif</given-names> <surname>Ozkirimli</surname></string-name>, and <string-name><given-names>Arzucan</given-names> <surname>Özgür</surname></string-name>. <article-title>A comparative study of smiles-based compound similarity functions for drug-target interaction prediction</article-title>. <source>BMC bioinformatics</source>, <volume>17</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>11</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><given-names>Mario</given-names> <surname>Krenn</surname></string-name>, <string-name><given-names>Florian</given-names> <surname>Häse</surname></string-name>, <string-name><given-names>AkshatKumar</given-names> <surname>Nigam</surname></string-name>, <string-name><given-names>Pascal</given-names> <surname>Friederich</surname></string-name>, and <string-name><given-names>Alan</given-names> <surname>Aspuru-Guzik</surname></string-name>. <article-title>Selfreferencing embedded strings (selfies): A 100% robust molecular string representation</article-title>. <source>Machine Learning: Science and Technology</source>, <volume>1</volume>(<issue>4</issue>):<fpage>045024</fpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="other"><string-name><given-names>Jacob</given-names> <surname>Devlin</surname></string-name>, <string-name><given-names>Ming-Wei</given-names> <surname>Chang</surname></string-name>, <string-name><given-names>Kenton</given-names> <surname>Lee</surname></string-name>, and <string-name><given-names>Kristina</given-names> <surname>Toutanova</surname></string-name>. <article-title>Bert: Pre-training of deep bidirectional transformers for language understanding</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1810.04805</pub-id>, <year>2018</year>.</mixed-citation></ref>
</ref-list>
<sec id="s6">
<label>A.</label>
<title>DRIAMS processing</title>
<p>As our models require every target to correspond to one specific drug (for which a SMILES string can be obtained), data provided by <xref ref-type="bibr" rid="c9">Weis et al. (2022)</xref> is further cleaned up. First, as “Quinolones”, and “Aminoglycosides” constitute classes of drugs rather than single ones, these drugs and their corresponding measurements are removed from the dataset. Second, some drug names in DRIAMS that refer to the same chemical structure are merged to a single drug. As this merging of drugs also combines their labels, care is taken so that no conflicting labels are combined. If, for a single spectrum, labels exist for both of the merging drugs in question, the label is only kept if both measurements are congruent (either both resistant, intermediate or susceptible). Otherwise, the merged label is discarded. Finally, some drugs are renamed such that there is less ambiguity as to exactly which compound is referred to by their name. The full list of modifications to drug names is listed in <xref rid="tbl3" ref-type="table">Table 3</xref>.</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3</label>
<caption><title>Full list of modifications made to drug names in DRIAMS.</title>
<p>Modifications consist of (1) removal of drugs, (2) merging of drugs, and (3) renaming drugs.</p></caption>
<graphic xlink:href="559916v3_tbl3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>To present drugs to the model, all names of drugs are converted to SMILES strings. In this work, Pub-Chem’s canonical SMILES strings of every compound are used. In PubChem, canonical SMILES are not isomeric, which means that stereochemistry is ignored. As such, two drugs that are stereoisomers are treated as a single drug, this is the case for Ofloxacin and Lev-ofloxacin. Furthermore, many drugs in the dataset refer to the co-administration of two compounds (such as, for example, Ampicillin-Sulbactam or Amoxicillin-Clavulanic acid). These cases are treated as a single drug with a SMILES string consisting of the strings of both constituent compounds separated by a “.” character, as is common practice with SMILES strings.</p>
</sec>
<sec id="s7">
<label>B.</label>
<title>Modeling set-up</title>
<sec id="s7a">
<label>B.1.</label>
<title>Drug embedders</title>
<p>In this paper, seven ways to encode drugs in a model are tested out. In this section, those seven drug embedders are described in detail. All descriptions correspond to the final set-up used to present results, hyperparameter tuning results are presented in <xref ref-type="sec" rid="s7b1">Appendix B.2.1</xref>.</p>
<p>All drug embedders encode drugs to a vector <bold><italic>t</italic></bold> <sub><italic>j</italic></sub> ∈ ℝ<sup>64</sup>. The most simple way to obtain a dense vector of that size for every drug is via a <bold>one-hot embedding</bold>. Every drug gets assigned an index in a vector, and the resulting vectors are embedded to a dense representation via a single linear layer. Encoding drugs in this way is the most straightforward, but no structural information of the underlying active compound is included. No inductive bias is presented to the model that will give structurally-similar drugs comparable embeddings. As such, all this information must be learnt from data. Similarly, such drug embedders can not be generalized out-of-the-box to drugs it hasn’t seen in the training data, as there are no indices - and learnt embeddings - for them.</p>
<p>The (local) structure of drugs can be encoded via fingerprints. A molecular fingerprint corresponds to a bit-vector in which every bit corresponds to the presence or absence of a substructure (<xref ref-type="bibr" rid="c51">Capecchi et al., 2020</xref>). In this paper, <bold>Morgan fingerprints</bold> with a diameter of 4 and consisting of 512 bits are derived from RDKit (<xref ref-type="bibr" rid="c52">Landrum, 2013</xref>). The resulting vector is embedded with a linear layer to get a dense drug representation. Embedding drugs using such structural features overcomes the aforementioned drawbacks with one-hot embeddings.</p>
<p>Similarly, the identity of a drug can be communicated via the textual representation known as SMILES strings (<xref ref-type="bibr" rid="c53">Weininger, 1988</xref>). Here, an adaptation of SMILES for machine and deep learning applications is used, called DeepSMILES (<xref ref-type="bibr" rid="c43">O’Boyle and Dalke, 2018</xref>). All the different letters in the alphabet are assigned an index in a one-hot vector. Hence, every molecule can be encoded to a matrix <bold><italic>S</italic></bold> <sub><italic>j</italic></sub> ∈ ℝ<sup><bold><italic>v</italic></bold>×<italic>l</italic></sup>, with <bold><italic>v</italic></bold> the vocabulary size of the DeepSMILES alphabet and <italic>l</italic> the string length of the molecule. This representation can be processed to a vector embedding using any neural network type that is appropriate for variable-length sequences.</p>
<p>A <bold>1D CNN</bold> detects and composes local patterns in the DeepSMILES string to a final drug embedding. Every input channel corresponds to a specific letter in the SMILES alphabet. The convolutional network used here consists of a position-wise linear layer to embed the channels to 64 dimensions, four convolutional blocks placed in sequence, followed by a global max-pooling operation across the length axis and a final linear layer to return a vector <bold><italic>t</italic></bold> <sub><italic>j</italic></sub> ∈ ℝ<sup>64</sup>. The global max-pooling layer allows the same network to be used for variable-length inputs. Each convolutional block consists of a structure similar to the one found in transformers (<xref ref-type="bibr" rid="c39">Vaswani et al., 2017</xref>). A first layer normalization is followed by a (padded) convolutional layer with a kernel size of 5, a first residual connection is wrapped around these two operations. After this, a position-wise feedforward makes up the second half of the convolutional block. The positionwise feedforward consists of a layer normalization, after which a GeLU-based gated linear unit identical to the one introduced by <xref ref-type="bibr" rid="c54">Shazeer (2020)</xref> is employed: <italic>z</italic> = <italic>Dropout</italic><sub>0.2</sub> ((<italic>GeLU</italic> (<bold><italic>xW</italic></bold>) <bold><italic>⊙ xV</italic></bold>)) <bold><italic>W</italic></bold> <sub>2</sub>. First, the input is sent to two position-wise linear layers via <bold><italic>W</italic></bold> and <bold><italic>V</italic></bold>, each of them exploding the hidden dimensions of the input by a factor of four. By sending the result of the first linear layer to a GeLU activation and multiplying element-wise with the result of the second linear layer, a gated linear unit structure is obtained. The output of this gated linear unit is sent to a dropout layer with rate 0.2 and then returned to the original dimension size via a final linear layer <bold><italic>W</italic></bold> <sub>2</sub>. Around this second LayerNorm and feedforward structure, a residual connection is again wrapped. All residual blocks have an input and output hidden dimension of 64. <xref rid="fig5" ref-type="fig">Figure 5</xref> shows the structure of this convolutional block. Its design adopts the current state-of-the-art practices in transformers, which are increasingly being used in convolutional networks (<xref ref-type="bibr" rid="c55">Liu et al., 2022</xref>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5</label>
<caption><p>Structure used for the residual blocks, used in the 1D CNN, 2D CNN, and transformer. In the case of convolutions, the output is zero padded so as to produce the same output dimensions as in the input.</p></caption>
<graphic xlink:href="559916v3_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>A <bold>transformer</bold> can be used to learn and compose signals in the DeepSMILES strings that occur sequence-wide, as opposed to the local pattern detection with a 1D CNN. The DeepSMILES strings are similarly embedded to 64 dimensions per character. After this, sinusoidal positional encodings (<xref ref-type="bibr" rid="c39">Vaswani et al., 2017</xref>) are added, and a CLS token embedding is prepended to the sequence. Four transformer blocks are employed, each with 64 as hidden dimension. The structure of the blocks are identical as with the 1D CNN (<xref rid="fig5" ref-type="fig">Figure 5</xref>), but using scaled dot-product self-attention instead of 1D convolutions. The self-attention operation uses 8 heads. The output at the CLS token is used as a “summary” of the content in the sequence (as opposed to the global max pooling with the CNN). A final linear layer on the output of the CLS token returns the drug embedding <bold><italic>t</italic></bold> <sub><italic>j</italic></sub> ∈ ℝ<sup>64</sup>.</p>
<p>A <bold>recurrent neural network</bold> (RNN) is used to process the DeepSMILES strings sequentially. The RNN used here consists of a bidirectional GRU with 64 hidden dimensions (<xref ref-type="bibr" rid="c56">Cho et al., 2014</xref>). The two final hidden states of the GRU are used as “summaries” of the content in the sequence. These two final states are averaged (element-wise) and sent to a final linear layer returning <bold><italic>t</italic></bold> <sub><italic>j</italic></sub> ∈ ℝ<sup>64</sup>.</p>
<p>All three aforementioned neural network structures work on variable-length (Deep)SMILES strings. With mini-batches, input drugs are (zero) padded so that everything fits into a tensor S ∈ ℝ<sup><italic>b</italic>×<bold><italic>v</italic></bold>×<italic>l</italic></sup>, with <italic>b</italic> the batch size, <bold><italic>v</italic></bold> the vocabulary size, and <italic>l</italic> the longest length of a drug in the batch. The three aforementioned neural nets are adapted so that no information can flow from masked tokens to actual drug tokens (through masking after convolutions or in the attention matrices).</p>
<p>As (Deep)SMILES are a 1D representation of a 3D molecular structure, a more-detailed view of the drug may be obtained by permitting an extra dimension into its input representation. Drawings of drugs achieve this 2D view of the molecule. Here, 128 × 128 drawings of drugs are obtained through RDKit. The RGB values are inverted so as to make the parts of the image containing molecule “activated”. Also, the RGB values are scaled to the range of 0 to 1 by dividing by 255. A <bold>2D CNN</bold> processes the images to a drug embedding. The CNN consists of an input convolutional layer with kernel size and stride of 2. The input layer takes the three input channels and returns 32 hidden dimensions. After, two convolutional blocks of the same structure as with the transformer and 1D CNN are placed in tandem (<xref rid="fig5" ref-type="fig">Figure 5</xref>). The 2D convolutional operation used in the convolutional operation has a kernel size of 5. Hereafter, a global max-pooling operation across the height and width of the image is performed, followed by a final linear layer producing the drug embedding <bold><italic>t</italic></bold> <sub><italic>j</italic></sub> ∈ ℝ<sup>64</sup>.</p>
<p>A final way to obtain a numerical representation of drugs tested here is through similarity matrices. A <bold>string kernel</bold> is used to create a Gram matrix of all drugs in the training set. The input representation of a drug is then simply a row in said Gram matrix. This approach is generalizable to unseen drugs at inference time, as obtaining a representation for them involves running the kernel function of the new compound to all training drugs. In this work, the LINGO string kernel (using 4-mers) is used (<xref ref-type="bibr" rid="c44">Vidal et al., 2005</xref>), as this kernel performed well in a recent benchmark (<xref ref-type="bibr" rid="c57">Öztürk et al., 2016</xref>). Note that here, SMILES strings are used instead of DeepSMILES (as with the 1D CNN, RNN, and transformer). A linear layer produces the final drug embedding <bold><italic>t</italic></bold> <sub><italic>j</italic></sub> ∈ ℝ<sup>64</sup> from a row in the Gram matrix.</p>
<p>A visual overview of all seven drug embedders in given in <xref rid="fig6" ref-type="fig">Figure 6</xref></p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6</label>
<caption><p>Overview of all different drug embedders tested in this work. One-hot embeddings are the only technique not incorporating prior knowledge of the structure of the compound. Hence, they are the only technique incapable of directly transferring to new compounds. Morgan fingerprints produce a bit-vector containing information on the presence of certain substructures. DeepSMILES strings are encoded and processed with a 1D CNN, GRU, or transformer. Drawings of molecules are processed with a 2D CNN. A string kernel on SMILES strings produces a numerical vector for every drug (taken as the row in the resulting Gram matrix).</p></caption>
<graphic xlink:href="559916v3_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s7b">
<label>B.2.</label>
<title>Hyperparameter tuning</title>
<sec id="s7b1">
<label>B.2.1.</label>
<title>Dual-branch models</title>
<p>Due to the complexity of tuning two branches and the size of the dataset, tuning is mostly done in an <italic>ad hoc</italic> fashion, relying on knowledge of current best practices in deep learning. Only some hyperparameters of interest are tuned on the validation set. Here, we present validation model results of those experiments. All results presented here concern models that are trained with a medium-sized spectrum embedder, with hyperparameters otherwise as described in <xref ref-type="sec" rid="s7a">Appendix B.1</xref>. All numbers indicate an average over three runs, similarly choosing the best average out of four tested learning rates (1e-4, 5e-4, 1e-3, 5e-3).</p>
<p><xref rid="fig7" ref-type="fig">Figure 7A</xref> shows validation set performances for a grid of different kernel sizes and hidden dimensionalities for the SMILES 1-D CNN. The best-performing hidden dimensionality (64) is copied to the (Deep)SMILES Transformer and GRU without further tuning. In <xref rid="fig7" ref-type="fig">Figure 7B</xref>, a similar grid is shown for the Image 2-D CNN, where it is found that a smaller hidden size is favored. <xref rid="fig7" ref-type="fig">Figure 7C</xref> shows the performance for using different molecular string representations as input to the 1-D CNN model: SMILES, DeepSMILES (<xref ref-type="bibr" rid="c43">O’Boyle and Dalke, 2018</xref>), and SELFIES (<xref ref-type="bibr" rid="c58">Krenn et al., 2020</xref>). While all techniques perform competitively, DeepSMILES strings outperform the other two by a small margin. Similarly, DeepSMILES are thus selected as input representations for the Transformer and GRU, without further tuning. <xref rid="fig7" ref-type="fig">Figure 7D</xref> shows how sinusoidal positional encodings outperform learned positional encodings (as in <xref ref-type="bibr" rid="c59">Devlin et al. (2018)</xref>). It is found that a bidirectional GRU considerably outpeforms a unidirectional one (<xref rid="fig7" ref-type="fig">Figure 7E</xref>). Finally, the number of bits in the Morgan fingerprint encoding is also tuned (<xref rid="fig7" ref-type="fig">Figure 7F</xref>). It is seen that including lower than 512 bits degenerates performance, but including more than 512 introduces instabilities in model training, as the model becomes prone to overfitting the drug branch.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7</label>
<caption><p>All hyperparameter tuning experiments. All evaluations are listed in terms of validation Micro ROC-AUCs. All numbers are averages of three model runs, with errorbars showing standard deviations. In every experiment, the highest average is chosen to use in the final models.</p></caption>
<graphic xlink:href="559916v3_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s7b2">
<label>B.2.2.</label>
<title>Specialist baselines</title>
<p>All baselines are trained using the same data splits as used with the dual-branch model. In essence: all DRIAMS-A spectra before the year 2018 are in the training set. The remaining spectra from 2018 are evenly divided among validation and test (with which belonging to which corresponding with the splits used for the dual-branch experiments). The same preprocessed 6000-dimensional spectrum representations are used as input.</p>
<p>Logistic regression baselines are trained with the L-BFGS solver for a maximum of 500 training iterations. For every species-drug combination, a grid search is performed on various hyperparameters, selecting the best based on validation ROC-AUC. The hyperparameters that are tuned are the scaling method on the features (either none, or standard scaling), and the L2 regularization strength (<italic>C</italic> ∈ {10<sup>−3</sup>, 10<sup>−2</sup>, …, 10<sup>2</sup>, 10<sup>3</sup>})</p>
<p>For XGBoost, default parameters are used apart from those tuned. For every species-drug combination, a grid is run, testing different numbers of trees (n_estimators ∈ {25, 50, 100, 200}) and learning rate (learning_rate ∈ {10<sup>−3</sup>, 10<sup>−2</sup>, 10<sup>−1</sup>, 10<sup>0</sup>}).</p>
<p>For the MLP baselines, the same hyperparameters are used as for the spectrum branch. Briefly recapitulated: between every two fully-connected layers, a series of operations consisting of (1) a GeLU activation, (2) a dropout rate of 0.2, and (3) layer normalization, is applied. The sizes of the models are as in <xref rid="tbl1" ref-type="table">Table 1</xref>, but then ending in 1 node instead of 64. For every species-drug combination, models are trained using the crossentropy loss and the Adam optimizer for a maximum of 250 epochs. A batch size of 128 is employed. A linear learning rate warm-up is applied over the first 250 steps. Early stopping based on validation ROC-AUC is applied with a patience of 10 epochs. The model with the best validation ROC-AUC during training is kept as final model. the best model out of four different learning rates (learning_rate ∈ {1e-4, 5e-4, 1e-3, 5e-3}) is chosen based on their validation ROC-AUC.</p>
</sec>
</sec>
</sec>
<sec id="s8">
<label>C.</label>
<title>Tables and figures supporting the results section</title>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4</label>
<caption><title>Full table of test results.</title>
<p>The listed averages and standard deviations are calculated over three independent runs of the same model. The best models for every metric per drug embedder are underlined. The overall best model for every metric is in bold face.</p></caption>
<graphic xlink:href="559916v3_tbl4.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8</label>
<caption><p>Barplots showing test performance results for all trained models. Colors represent the different spectrum embedder model sizes. Performance is shown in terms of Macro ROC-AUC (computed per drug and averaged) and in terms of Instance-wise Prec@1 of the negative class. The Prec@1 evaluates how often the top-ranked prediction is correct. The Prec@1 of the negative class, hence, reports the proportion of cases for which the “most-likely susceptible drug” prediction is actually an effective one. In a scenario where the top recommended drug is always administered, it corresponds to the percentage of correctly-suggested treatments. Errorbars represent the standard deviation over three random seeds.</p></caption>
<graphic xlink:href="559916v3_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9</label>
<caption><p>Performance of models compared against a linear spectrum embedder baseline. The comparison is only shown for the two best-performing drug embedders (in terms of Micro ROC-AUC). Errorbars represent the standard deviation over three random seeds.</p></caption>
<graphic xlink:href="559916v3_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10</label>
<caption><p>Transfer learning of DRIAMS-A models to other hospitals. Errorbands show the standard deviation over three runs.</p></caption>
<graphic xlink:href="559916v3_fig10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig11" position="float" fig-type="figure">
<label>Figure 11</label>
<caption><p>UMAP scatterplots of test set MALDI-TOF spectra embeddings <bold><italic>x</italic></bold><sub><italic>i</italic></sub>. Only embeddings belonging to the 25 most-occurring species in the test set are shown. Spectra are colored according to its AMR status to a certain drug. The twenty displayed drugs are selected based on a ranking of the product of the number of positive and negative labels<inline-formula><inline-graphic xlink:href="559916v3_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. In this way, the drugs that have a lot of observed labels, both positives and negatives, are displayed. The drugs here are ranked 5-24 (the first four are shown in <xref rid="fig4" ref-type="fig">Figure 4</xref>).</p></caption>
<graphic xlink:href="559916v3_fig11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<fn-group>
<fn id="fn1">
<label><sup>1</sup></label>
<p>These figures reflect the size of the dataset as downloaded from the original Dryad repository <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.bzkh1899q">https://doi.org/10.5061/dryad.bzkh1899q</ext-link>, and after processing. For example, the number of spectra listed here corresponds to all spectra in DRIAMS for which there exists at least one AMR measurement. The total number of spectra in DRIAMS counts 250 070, but no labels are associated with these extra spectra. Further, the naming of drugs was further preprocessed such that every drug can be linked to a single chemical identifier. For more information on which drugs were merged and how this was performed, see <xref ref-type="sec" rid="s6">Appendix A</xref></p></fn>
<fn id="fn2">
<label><sup>2</sup></label>
<p>An instance-wise metric can be seen as a “macro”-averaged metric. But, whereas macro-averaged metrics usually compute an average performance per class (here, drug), with instance-wise metrics, the performance is averaged per instance (here, a spectrum).</p></fn>
<fn id="fn3">
<label><sup>3</sup></label>
<p>Increasing this parameter helps reduce UMAP packing points too tightly together, hence, making for a more-legible plot.</p></fn>
</fn-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93242.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Folkman</surname>
<given-names>Lukas</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study presents a machine learning model to recommend effective antimicrobial drugs from patients' samples analysed with mass spectrometry. While the proposed approach of training a single model across different bacterial species and drugs seems promising, the comparison with baselines and related work is <bold>incomplete</bold>. With the evaluation part strengthened, this paper would be of interest to computational biologists, microbiologist, and clinicians.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93242.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>De Waele et al. reported a dual-branch neural network model for predicting antibiotic resistance profiles using matrix-assisted laser desorption/ionization time-of-flight (MALDI-TOF) mass spectrometry data. Neural networks were trained on the recently available DRIAMS database of MALDI-TOF mass spectrometry data and their associated antibiotic susceptibility profiles. The authors used a dual branch neural network approach to simultaneously represent information about mass spectra and antibiotics for a wide range of species and antibiotic combinations. The authors showed consistent performance of their strategy to predict antibiotic susceptibility for different spectrums and antibiotic representations (i.e., embedders). Remarkably, the authors showed how small datasets collected at one location can improve the performance of a model trained with limited data collected at a second location. Despite these promising results, there are several analyses that the authors could incorporate to offer additional support to some of their claims (see weaknesses). In particular, this work would benefit from a more comprehensive comparison of the author's single recommender model vs an ensemble of specialist models, and the inclusion of 1-2 examples that showcase how their model could be translated into the clinic.</p>
<p>Strengths:</p>
<p>• A single AMR recommender system could potentially facilitate the adoption of MALDI-TOF-based antibiotic susceptibility profiling into clinical practices by reducing the number of models to be considered, and the efforts that may be required to periodically update them.</p>
<p>• Authors tested multiple combinations of embedders for the mass spectra and antibiotics while using different metrics to evaluate the performance of the resulting models. Models trained using different spectrum embedder-antibiotic embedder combinations had remarkably good performance for all tested metrics. The average ROC AUC scores for global and spectrum-level evaluations were above 0.9. Average ROC AUC scores for antibiotic-level evaluations were greater than 0.75.</p>
<p>• Authors showed that data collected in one location can be leveraged to improve the performance of models generated using a smaller number of samples collected at a different location. This result may encourage researchers to optimize data integration to reduce the burden of data generation for institutions interested in testing this method.</p>
<p>Weaknesses:</p>
<p>• Although ROC AUC is a widely used metric. Other metrics such as precision, recall, sensitivity, and specificity are not reported in this work. The last two metrics would help readers understand the model's potential implications in the context of clinical research.</p>
<p>• The authors did not hypothesize or describe in any way what an acceptable performance of their recommender system should be in order to be adopted by clinicians.</p>
<p>• Related to the previous comment, this work would strongly benefit from the inclusion of 1-2 real-life applications of their method that could showcase the benefits of their strategy for designing antibiotic treatment in a clinical setting.</p>
<p>• The authors do not offer information about the model features associated with resistance. This information may offer insights about mechanisms of antimicrobial resistance and how conserved they are across species.</p>
<p>• Comparison of AUC values across models lacks information regarding statistical significance. Without this information it is hard for a reader to figure out which differences are marginal and which ones are meaningful (for example, it is unclear if a difference in average AUC of 0.02 is significant). This applied to Figure 2, Figure 3, and Table 2 (and the associated supplementary figures).</p>
<p>• One key claim of this work was that their single recommender system outperformed specialist (single species-antibiotic) models. However, in its current status, it is not possible to determine that in fact that is the case (see comment above). Moreover, comparisons to species-level models (that combine all data and antibiotic susceptibility profiles for a given species) would help to illustrate the putative advantages of the dual branch neural network model over species-based models. This analysis will also inform the species (and perhaps datasets) for which specialist models would be useful to consider.</p>
<p>• Taking into account that the clustering of spectra embeddings seemed to be species-driven (Figure 4), one may hypothesize that there is limited transfer of information between species, and therefore the neural network model may be working as an ensemble of species models. Thus, this work would deeply benefit from a comparison between the authors' general model and an ensemble model in which the species is first identified and then the relevant species recommender is applied. If authors had identified cases to illustrate how data from one species positively influence the results for another species, they should include some of those examples.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93242.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors frame the MS-spectrum-based prediction of antimicrobial resistance prediction as a drug recommendation task. Weis et al introduced the dataset this model is tested on and benchmark models which take as input a single species and are trained to predict resistance to a single drug. Instead here, a pair of drug and spectrum are fed to 2 neural network models to predict a resistance probability. In this manner, knowledge from different drugs and species can be shared through the model parameters. Three questions are asked: 1. what is the best way to encode the drugs? 2. does the dual NN outperform the single-spectrum drug?</p>
<p>Overall the paper is well-written and structured. It presents a novel framework for a relevant problem. The work would benefit from more work on evaluation.</p>
</body>
</sub-article>
</article>