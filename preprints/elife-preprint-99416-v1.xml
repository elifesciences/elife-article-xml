<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">99416</article-id>
<article-id pub-id-type="doi">10.7554/eLife.99416</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.99416.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Progressive neural engagement within the IFG-pMTG circuit as gesture and speech entropy and MI advances</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6979-940X</contrib-id>
<name>
<surname>Zhao</surname>
<given-names>Wanying</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>zhaowy@psych.ac.cn</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Zhouyi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Xiang</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4512-5221</contrib-id>
<name>
<surname>Du</surname>
<given-names>Yi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<email>duyi@psych.ac.cn</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/034t30j35</institution-id><institution>CAS Key Laboratory of Behavioral Science, Institute of Psychology, Chinese Academy of Sciences</institution></institution-wrap>, <city>Beijing</city>, <country>China</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qbk4x57</institution-id><institution>Department of Psychology, University of Chinese Academy of Sciences</institution></institution-wrap>, <city>Beijing</city>, <country>China</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00vpwhm04</institution-id><institution>CAS Center for Excellence in Brain Science and Intelligence Technology</institution></institution-wrap>, <city>Shanghai</city>, <country>China</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/029819q61</institution-id><institution>Chinese Institute for Brain Research</institution></institution-wrap>, <city>Beijing</city>, <country>China</country> 102206</aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03x1jna21</institution-id><institution>School of Psychology, Central China Normal University</institution></institution-wrap>, <city>Wuhan</city>, <country>China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Martin</surname>
<given-names>Andrea E</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Max Planck Institute for Psycholinguistics</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="original-publication" iso-8601-date="2024-12-13">
<day>13</day>
<month>12</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP99416</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-05-28">
<day>28</day>
<month>05</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-06-04">
<day>04</day>
<month>06</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.11.23.517759"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Zhao et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Zhao et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-99416-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Semantic representation emerges from distributed multisensory modalities, yet a comprehensive understanding of the functional changing pattern within convergence zones or hubs integrating multisensory semantic information remains elusive. In this study, employing information-theoretic metrics, we quantified gesture and speech information, alongside their interaction, utilizing entropy and mutual information (MI). Neural activities were assessed via interruption effects induced by High-Definition transcranial direct current stimulation (HD-tDCS). Additionally, chronometric double-pulse transcranial magnetic stimulation (TMS) and high-temporal event-related potentials were utilized to decipher dynamic neural changes resulting from various information contributors. Results showed gradual inhibition of both inferior frontal gyrus (IFG) and posterior middle temporal gyrus (pMTG) as degree of gesture-speech integration, indexed by MI, increased. Moreover, a time-sensitive and staged progression of neural engagement was observed, evidenced by distinct correlations between neural activity patterns and entropy measures of speech and gesture, as well as MI, across early sensory and lexico-semantic processing stages. These findings illuminate the gradual nature of neural activity during multisensory gesture-speech semantic processing, shaped by dynamic gesture constraints and speech encoding, thereby offering insights into the neural mechanisms underlying multisensory language processing.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>gesture-speech integration</kwd>
<kwd>pMTG-IFG circuit</kwd>
<kwd>information theory</kwd>
<kwd>multisensory</kwd>
<kwd>semantic</kwd>
<kwd>dual-stage modal</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>New tDCS data are added to support the findings. All figures are revised. A new author is added.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Semantic representation, distinguished by its cohesive conceptual nature, emerges from distributed modality-specific regions. Consensus acknowledges the presence of ’convergence zones’ within the temporal and inferior parietal areas <sup><xref ref-type="bibr" rid="c1">1</xref></sup>, or the ’semantic hub’ located in the anterior temporal lobe<sup><xref ref-type="bibr" rid="c2">2</xref></sup>, pivotal for integrating, converging, or distilling multimodal inputs. Contemporary perspectives on semantic processing portray it as a sequence of quantitatively functional mental states defined by a specific parser<sup><xref ref-type="bibr" rid="c3">3</xref></sup>, unified by statistical regularities among multiple sensory inputs<sup><xref ref-type="bibr" rid="c4">4</xref></sup> through hierarchical prediction and multimodal interactions<sup><xref ref-type="bibr" rid="c5">5</xref>–<xref ref-type="bibr" rid="c9">9</xref></sup>. Hence, proposals suggest that the coherent semantic representation emerges from statistical learning mechanisms within these ’convergence zones’ or ’semantic hub’ <sup><xref ref-type="bibr" rid="c10">10</xref>–<xref ref-type="bibr" rid="c12">12</xref></sup>, potentially functioning in a graded manner<sup><xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c13">13</xref></sup>. However, the exact nature of the graded structure within these integration hubs, along with their temporal dynamics, remains elusive.</p>
<p>Among the many kinds of multimodal extralinguistic information, representational gesture is the one that is related to the semantic content of co-occurring speech<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c15">15</xref></sup>. Representational gesture is regarded as ‘part of language’<sup><xref ref-type="bibr" rid="c16">16</xref></sup> or functional equivalents of lexical units that alternate and integrate with speech into a ‘single unification space’ to convey a coherent meaning<sup><xref ref-type="bibr" rid="c17">17</xref>–<xref ref-type="bibr" rid="c19">19</xref></sup>. Empirical studies have investigated the semantic integration between representational gesture (gesture in short hereafter) and speech by manipulating their semantic relationship<sup><xref ref-type="bibr" rid="c20">20</xref>–<xref ref-type="bibr" rid="c23">23</xref></sup> and revealed a mutual interaction between them<sup><xref ref-type="bibr" rid="c24">24</xref>–<xref ref-type="bibr" rid="c26">26</xref></sup> as reflected by the N400 latency and amplitude<sup><xref ref-type="bibr" rid="c19">19</xref></sup> as well as common neural underpinnings in the left inferior frontal gyrus (IFG) and posterior middle temporal gyrus (pMTG)<sup><xref ref-type="bibr" rid="c20">20</xref>,<xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup>. Quantifying the amount of information from both sources and their interaction, the present study delved into cortical engagement and temporal dynamics during multisensory gesture-speech integration, with a specific focus on the IFG and pMTG, alongside various ERP components.</p>
<p>To this end, we developed an analytic approach to directly probe the contribution of gesture and speech during multisensory semantic integration, while adopting the information-theoretic complexity metrics of <italic>entropy</italic> and <italic>mutual information</italic> (MI). Entropy captures the disorder or randomness of information and is used as a measurement of the uncertainty of representation activated when an event occurs<sup><xref ref-type="bibr" rid="c29">29</xref></sup>. MI illustrates the mutual constraint that the two variables impose on each other<sup><xref ref-type="bibr" rid="c30">30</xref></sup>. Herein, during gesture-speech integration, entropy measures the uncertainty of information of gesture or speech, while MI indexes the degree of integration.</p>
<p>Three experiments were conducted to unravel the intricate neural processes underlying gesture-speech semantic integration. In Experiment 1, High-Definition Transcranial Direct Current Stimulation (HD-tDCS) was utilized to administer Anodal, Cathodal and Sham stimulation to either the IFG or the pMTG. HD-tDCS induces membrane depolarization with anodal stimulation and membrane hyperpolarisation with cathodal stimulation<sup><xref ref-type="bibr" rid="c31">31</xref></sup>, thereby respectively increasing or decreasing cortical excitability in the targeted brain area. Hence, Experiment 1 aimed to determine whether the facilitation effect (Anodal-tDCS minus Sham-tDCS) and/or the inhibitory effect (Cathodal-tDCS minus Sham-tDCS) on the integration hubs of IFG and/or pMTG were modulated by the degree of gesture-speech integration, indexed with MI. Considering the different roles of IFG and pMTG during integration<sup><xref ref-type="bibr" rid="c28">28</xref></sup>, as well as the various ERP components reported in prior investigations, such as the early sensory effect as P1 and N1–P2<sup><xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c34">34</xref></sup>, the N400 semantic conflict effect<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup>, and the late positive component (LPC) reconstruction effect<sup><xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c37">37</xref></sup>. Experiment 2 employed chronometric double-pulse transcranial magnetic stimulation (TMS) to target short time windows along the gesture-speech integration period<sup><xref ref-type="bibr" rid="c32">32</xref></sup>. In parallel, Experiment 3 utilized high-temporal event-related potentials to explore whether the various neural engagements were temporally and progressively modulated by distinct information contributors during gesture-speech integration.</p>
</sec>
<sec id="s2">
<title>Material and methods</title>
<sec id="s2a">
<title>Participants</title>
<p>Ninety-eight young Chinese participants signed written informed consent forms and took part in the present study (Experiment 1: 29 females, 23 males, age = 20 ± 3.40 years; Experiment 2: 11 females, 13 males, age = 23 ± 4.88 years; Experiment 3: 12 females, 10 males, age = 21 ± 3.53 years). All of the participants were right-handed (Experiment 1: laterality quotient (LQ)<sup><xref ref-type="bibr" rid="c38">38</xref></sup> = 88.71 ± 13.14; Experiment 2: LQ = 89.02 ± 13.25; Experiment 3: LQ = 88.49 ± 12.65), had normal or corrected-to-normal vision and were paid ¥100 per hour for their participation. All experiments were approved by the Ethics Committee of the Institute of Psychology, Chinese Academy of Sciences.</p>
</sec>
<sec id="s2b">
<title>Stimuli</title>
<p>Twenty gestures (<xref rid="atbl1" ref-type="table">Appendix Table 1</xref>) with 20 semantically congruent speech signals taken from previous study<sup><xref ref-type="bibr" rid="c28">28</xref></sup> were used. The stimuli set were recorded from two native Chinese speakers (1 male, 1 female) and validated by replicating the semantic congruency effect with 30 participants. Results showed a significantly (<italic>t</italic>(29) = 7.16, <italic>p</italic> &lt; 0.001) larger reaction time when participants were asked to judge the gender of the speaker if gesture contained incongruent semantic information with speech (a ‘cut’ gesture paired with speech word ‘喷 pen1 (spray)’: mean = 554.51 ms, SE = 11.65) relative to when they were semantically congruent (a ‘cut’ gesture paired with ‘剪 jian3 (cut)’ word: mean = 533.90 ms, SE = 12.02)<sup><xref ref-type="bibr" rid="c28">28</xref></sup>.</p>
<p>Additionally, two separate pre-tests with 30 subjects in each (pre-test 1: 16 females, 14 males, age = 24 ± 4.37 years; pre-test 2: 15 females, 15 males, age = 22 ± 3.26 years) were conducted to determine the comprehensive values of gesture and speech. Participants were presented with segments of increasing duration, beginning at 40 ms, and were prompted to provide a single verb to describe either the isolated gesture they observed (pre-test 1) or the isolated speech they heard (pre-test 2). For each pre-test, the response consistently provided by participants for four to six consecutive instances was considered the comprehensive answer for the gesture or speech. The initial instance duration was marked as the discrimination point (DP) for gesture (mean = 183.78 ± 84.82ms) or the identification point (IP) for speech (mean = 176.40 ± 66.21ms) (<xref rid="fig1" ref-type="fig">Figure 1A top</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Experimental design, and stimulus characteristics.</title>
<p><bold>(A) Experimental stimuli.</bold> Twenty gestures were paired with 20 relevant speech stimuli. Two gating studies were executed to define the minimal length of each gesture and speech required for semantic identification, namely, the discrimination point (DP) of gesture and the identification point (IP) of speech. Overall, a mean of 183.78 ms (SD = 84.82) was found for the DP of gestures and the IP of speech was 176.40 ms (SD = 66.21). The onset of speech was set at the gesture DP. Responses for each item were assessed utilizing information-theoretic complexity metrics to quantify the information content of both gesture and speech during integration, employing entropy and MI.</p>
<p><bold>(B) Procedure of Experiment 1.</bold> HD-tDCS, including Anodal, Cathodal, or Sham conditions, was administered to the IFG or pMTG) using a 4 * 1 ring-based electrode montage. Electrode F7 targeted the IFG, with return electrodes placed on AF7, FC5, F9, and FT9. For pMTG stimulation, TP7 was targeted, with return electrodes positioned on C5, P5, T9, and P9. Sessions lasted 20 minutes, with a 5-second fade-in and fade-out, while the Sham condition involved only 30 seconds of stimulation.</p>
<p><bold>(C) Procedure of Experiment 2.</bold> Eight time windows (TWs, duration = 40 ms) were segmented in relative to the speech IP. Among the eight TWs, five (TW1, TW2, TW3, TW6, and TW7) were chosen based on the significant results in our prior study<sup><xref ref-type="bibr" rid="c28">28</xref></sup>. Double-pulse TMS was delivered over each of the TW of either the pMTG or the IFG.</p>
<p><bold>(D) Procedure of Experiment 3.</bold> Semantically congruent gesture-speech pairs were presented randomly with Electroencephalogram (EEG) recorded simultaneously. Epochs were time locked to the onset of speech and lasted for 1000 ms. A 200 ms pre-stimulus baseline correction was applied before the onset of gesture stoke. Various elicited components were hypothesized.</p>
<p><bold>(E-F) Proposed gradations in cortical engagements during gesture-speech information changes.</bold> Stepwise variations in the quantity of gesture and speech information during integration, as characterized by information theory metrics (<bold>E</bold>), are believed to the underpinned by progressive neural engagement within the IFG-pMTG gesture-speech integration circuit (<bold>F</bold>).</p></caption>
<graphic xlink:href="517759v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To quantify information content, responses for each item were converted into Shannon’s entropy (H) as a measure of information richness (<xref rid="fig1" ref-type="fig">Figure 1A bottom</xref>). With no significant gender differences observed in both gesture (<italic>t</italic>(20) = 0.21, <italic>p</italic> = 0.84) and speech (<italic>t</italic>(20) = 0.52, <italic>p</italic> = 0.61), responses were aggregated across genders, resulting in 60 answers per item (<xref rid="atbl2" ref-type="table">Appendix Table 2</xref>). Here, p(xi) and p(yi) represent the distribution of 60 answers for a given gesture (<xref rid="atbl2" ref-type="table">Appendix Table 2B</xref>) and speech (<xref rid="atbl2" ref-type="table">Appendix Table 2A</xref>), respectively. High entropy indicates diverse answers, reflecting broad representation, while low entropy suggests focused lexical recognition for a specific item (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). The joint entropy computation for gesture and speech, represented by H(xi, yi), involved amalgamating datasets of gesture and speech responses to depict their combined distributions. For specific gesture-speech combinations, equivalence between the joint entropy and the sum of individual entropies (gesture or speech) indicates absence of overlap in response sets. Conversely, significant overlap, denoted by a considerable number of shared responses between gesture and speech datasets, leads to a noticeable discrepancy between joint entropy and the sum of gesture and speech entropies. This quantification of gesture-speech overlap was operationalized by subtracting the joint entropy of gesture-speech from the combined entropies of gesture and speech, indexed by Mutual Information (MI) (see <xref rid="atbl2" ref-type="table">Appendix Table 2C</xref>). Elevated MI values thus signify substantial overlap, indicative of a robust mutual interaction between gesture and speech. The quantitative information for each stimulus, including gesture entropy, speech entropy, joint entropy, and MI are displayed in <xref rid="atbl3" ref-type="table">Appendix Table 3</xref>.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Quantification formulas (A) and distributions of each stimulus in Shannon’s entropy (B).</title><p>Two separate pre-tests (N = 30) were conducted to assign a single verb for describing each of the isolated 20 gestures and 20 speech items. Responses provided for each item were transformed into Shannon’s entropy using a relative quantification formula. Gesture (<bold>B left</bold>) and speech (<bold>B right</bold>) entropy quantify the randomness of gestural or speech information, representing the uncertainty of probabilistic representation activated when a specific stimulus occurs. Joint entropy (<bold>B middle</bold>) captures the widespread nature of the two sources of information combined. Mutual information (MI) was calculated as the difference between joint entropy with gesture entropy and speech entropy combined (<bold>A</bold>), thereby capturing the overlap of gesture and speech and representing semantic integration.</p></caption>
<graphic xlink:href="517759v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To accurately assess whether entropy/MI corresponds to stepped neural changes, the current study aggregated neural responses (Non-invasive brain stimulation (NIBS) inhibition effect or ERP amplitude) with identical entropy or MI values prior to conducting correlational analyses.</p>
</sec>
<sec id="s2c">
<title>Experimental procedure</title>
<p>Adopting a semantic priming paradigm of gestures onto speech<sup><xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c32">32</xref></sup>, speech onset was set to be at the DP of each accompanying gesture. An irrelevant factor of gender congruency (e.g., a man making a gesture combined with a female voice) was created<sup><xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c39">39</xref></sup>. This involved aligning the gender of the voice with the corresponding gender of the gesture in either a congruent (e.g., male voice paired with a male gesture) or incongruent (e.g., male voice paired with a female gesture) manner. This approach served as a direct control mechanism, facilitating the investigation of the automatic and implicit semantic interplay between gesture and speech<sup><xref ref-type="bibr" rid="c39">39</xref></sup>. In light of previous findings indicating a distinct TMS-disruption effect on the semantic congruency of gesture-speech interactions<sup><xref ref-type="bibr" rid="c28">28</xref></sup>, both semantically congruent and incongruent pairs were included in Experiment 1 and Experiment 2. Experiment 3, conversely, exclusively utilized semantically congruent pairs to elucidate ERP metrics indicative of nuanced semantic progression.</p>
<p>Gesture–speech pairs were presented randomly using Presentation software (<ext-link ext-link-type="uri" xlink:href="http://www.neurobs.com">www.neurobs.com</ext-link>). Participants underwent Experiment 1, comprising 480 gesture-speech pairs, across three separate sessions spaced one week apart for each participant. In each session, participants received one of three stimulation types (Anodal, Cathodal, or Sham). Experiment 2 consisted of 800 pairs and was conducted across 15 blocks over three days, with one week between sessions. The order of stimulation site and time window (TW) was counterbalanced using a Latin square design. Experiment 3, comprising 80 gesture-speech pairs, was completed in a single-day session. Participants were asked to look at the screen but respond with both hands as quickly and accurately as possible merely to the gender of the voice they heard. The RT and the button being pressed were recorded. The experiment started with a fixation cross presented on the center of the screen, which lasted for 0.5-1.5 sec.</p>
</sec>
<sec id="s2d">
<title>Experiment 1: HD-tDCS protocol and data analysis</title>
<p>HD-tDCS protocol employed a constant current stimulator (The Starstim 8 system) delivering stimulation at an intensity of 2000mA. A 4 * 1 ring-based electrode montage was utilized, comprising a central electrode (stimulation) positioned directly over the target cortical area and four return electrodes encircling it to provide focused stimulation. For targeting the left IFG at Montreal Neurological Institute (MNI) coordinates (-62, 16, 22), electrode F7 was selected as the optimal cortical projection site<sup><xref ref-type="bibr" rid="c40">40</xref></sup>, with the four return electrodes placed on AF7, FC5, F9, and FT9. For stimulation of the pMTG at coordinates (-50, -56, 10), TP7 was identified as the cortical projection site<sup><xref ref-type="bibr" rid="c40">40</xref></sup>, with return electrodes positioned on C5, P5, T9, and P9. The stimulation parameters included a 20-minute duration with a 5-second fade-in and fade-out for both Anodal and Cathodal conditions. The Sham condition involved a 5-second fade-in followed by only 30 seconds of stimulation, then 19’20 minutes of no stimulation, and finally a 5-second fade-out (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). Stimulation was controlled using NIC software, with participants blinded to the stimulation conditions.</p>
<p>All incorrect responses (702 out of the total number of 24960, 2.81% of trials) were excluded. To eliminate the influence of outliers, a 2SD trimmed mean for every participant in each session was also calculated. Our present analysis focused on Pearson correlations between the interruption effects of HD-tDCS (active tDCS minus sham tDCS) on the semantic congruency effect (difference in reaction time between semantic incongruent and semantic congruent pairs) and the variables of gesture entropy, speech entropy, or MI. This methodology seeks to determine whether the neural activity within the left IFG and pMTG is gradually affected by varying levels of gesture and speech information during integration, as quantified by entropy and MI.</p>
</sec>
<sec id="s2e">
<title>Experiment 2: TMS protocol and data analysis</title>
<p>At an intensity of 50% of the maximum stimulator output, double-pulse TMS was delivered via a 70 mm figure-eight coil using a Magstim Rapid² stimulator (Magstim, UK) over either the left IFG in TW3 (-40∼0 ms in relative to speech identification point (IP)) and TW6 (80∼120 ms,) or the left pMTG in TW1 (-120 ∼ -80 ms), TW2 (-80 ∼ -40 ms) and TW7 (120∼160 ms). Among the TWs that covering the period of gesture-speech integration, those that showed a TW-selective disruption of gesture-speech integration were selected<sup><xref ref-type="bibr" rid="c28">28</xref></sup> (<xref rid="fig1" ref-type="fig">Figure 1C</xref>).</p>
<p>High-resolution (1 × 1 × 0.6 mm) T1-weighted MRI scans were obtained using a Siemens 3T Trio/Tim Scanner for image-guided TMS navigation. Frameless stereotaxic procedures (BrainSight 2; Rogue Research) allowed real-time stimulation monitoring. To ensure precision, individual anatomical images were manually registered by identifying the anterior and posterior commissures. Subject-specific target regions were defined using trajectory markers in the MNI coordinate system. Vertex was used as control.</p>
<p>All incorrect responses (922 out of the total number of 19200, 4.8% of trials) were excluded. We focused our analysis on Pearson correlations of the TMS interruption effects (active TMS minus vertex TMS) of the semantic congruency effect with the gesture entropy, speech entropy or MI. By doing this, we can determine how the time-sensitive contribution of the left IFG and pMTG to gesture–speech integration was affected by gesture and speech information distribution. FDR correction was applied for multiple comparisons.</p>
</sec>
<sec id="s2f">
<title>Experiment 3: Electroencephalogram (EEG) recording and data analysis</title>
<p>EEG were recorded from 48 Ag/AgCl electrodes mounted in a cap according to the 10-20 system<sup><xref ref-type="bibr" rid="c41">41</xref></sup>, amplified with a PORTI-32/MREFA amplifier (TMS International B.V., Enschede, NL) and digitized online at 500 Hz (bandpass, 0.01-70 Hz). EEGLAB, a MATLAB toolbox, was used to analyze the EEG data<sup><xref ref-type="bibr" rid="c42">42</xref></sup>. Vertical and horizontal eye movements were measured with 4 electrodes placed above the left eyebrow, below the left orbital ridge and at bilateral external canthus. All electrodes were referenced online to the left mastoid. Electrode impedance was maintained below 5 KΩ. The average of the left and right mastoids was used for re-referencing. A high-pass filter with a cutoff of 0.05 Hz and a low-pass filter with a cutoff of 30 Hz were applied. Semi-automated artifact removal, including independent component analysis (ICA) for identifying components of eye blinks and muscle activity, was performed (<xref rid="fig1" ref-type="fig">Figure 1D</xref>). Participants with rejected trials exceeding 30% of their total were excluded from further analysis.</p>
<p>All incorrect responses were excluded (147 out of 1760, 8.35% of trials). To eliminate the influence of outliers, a 2 SD trimmed mean was calculated for every participant in each condition. Data were epoched from the onset of speech and lasted for 1000 ms. To ensure a clean baseline with no stimulus presented, a 200 ms pre-stimulus baseline correction was applied before gesture onset.</p>
<p>To objectively identify the time windows of activated components, grand-average ERPs at electrode Cz were compared between the higher (≥50%) and lower (&lt;50%) halves for gesture entropy (<xref rid="fig5" ref-type="fig">Figure 5A1</xref>), speech entropy (<xref rid="fig5" ref-type="fig">Figure 5B1</xref>), and MI (<xref rid="fig5" ref-type="fig">Figure 5C1</xref>). Consequently, four ERP components were predetermined: the P1 effect observed within the time window of 0-100 ms<sup><xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c34">34</xref></sup>, the N1-P2 effect observed between 150-250ms<sup><xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c34">34</xref></sup>, the N400 within the interval of 250-450ms<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup>, and the LPC spanning from 550-1000ms<sup><xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c37">37</xref></sup>. Additionally, seven regions-of-interest (ROIs) were defined in order to locate the modulation effect on each ERP component: left anterior (LA): F1, F3, F5, FC1, FC3, and FC5; left central (LC): C1, C3, C5, CP1, CP3, and CP5; left posterior (LP): P1, P3, P5, PO3, PO5, and O1; right anterior (RA): F2, F4, F6, FC2, FC4, and FC6; right central (RC): C2, C4, C6, CP2, CP4, and CP6; right posterior (RP): P2, P4, P6, PO4, PO6, and O2; and midline electrodes (ML): Fz, FCz, Cz, Pz, Oz, and CPz<sup><xref ref-type="bibr" rid="c43">43</xref></sup>.</p>
<p>Subsequently, cluster-based permutation tests<sup><xref ref-type="bibr" rid="c44">44</xref></sup> in Fieldtrip was further used to determine the significant clusters of adjacent time points and electrodes of ERP amplitude between the higher and lower halves of gesture entropy, speech entropy and MI, respectively. The electrode-level type I error threshold was set to 0.025. Cluster-level statistic was estimated through 5000 Monte Carlo simulations, where the cluster-level statistic is the sum of T-values for each stimulus within a cluster. The cluster-level type I error threshold was set to 0.05. Clusters with a p-value less than the critical alpha-level are considered to be conditionally different.</p>
<p>Paired t-tests were conducted to compare the lower and upper halves of each information model for the averaged amplitude within each ROI or cluster across the four ERP time windows, separately. Pearson correlations were calculated between each model value and each averaged ERP amplitude in each ROI or cluster, individually. False discovery rate (FDR) correction was applied for multiple comparisons.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Experiment 1: Modulation of left pMTG and IFG engagement by gradual changes in gesture-speech semantic information</title>
<p>In the IFG, one-way ANOVA examining the effects of three tDCS conditions (Anodal, Cathodal, or Sham) on semantic congruency (RT (semantic incongruent) – RT (semantic congruent)) demonstrated a significant main effect of stimulation condition (<italic>F</italic>(2, 75) = 3.673, <italic>p</italic> = 0.030, ηp2 = 0.089). Post hoc paired t-tests indicated a significantly reduced semantic congruency effect between the Cathodal condition and the Sham condition (<italic>t</italic>(26) = -3.296, <italic>p</italic> = 0.003, 95% CI = [-11.488, 4.896]) (<xref rid="fig3" ref-type="fig">Figure 3A left</xref>). Subsequent Pearson correlation analysis revealed that the reduced semantic congruency effect was progressively associated with the MI, evidenced by a significant correlation between the Cathodal-tDCS effect (Cathodal-tDCS minus Sham-tDCS) and MI (<italic>r</italic> = -0.595, <italic>p</italic> = 0.007, 95% CI = [-0.995, -0.195]) (<xref rid="fig3" ref-type="fig">Figure 3B</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>tDCS effect over semantic congruency.</title>
<p><bold>(A)</bold> tDCS effect was defined as active-tDCS minus sham-tDCS. The semantic congruency effect was calculated as the reaction time (RT) difference between semantically incongruent and semantically congruent pairs.</p>
<p><bold>(B)</bold> Correlations of the tDCS effect over the semantic congruency effect with three information models (gesture entropy, speech entropy and MI) are displayed with best-fitting regression lines. Significant correlations are marked in red. * <italic>p</italic> &lt; 0.05, ** <italic>p</italic> &lt; 0.01 after FDR correction.</p>
</caption>
<graphic xlink:href="517759v3_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Similarly, in the pMTG, a one-way ANOVA assessing the effects of three tDCS conditions on semantic congruency also revealed a significant main effect of stimulation condition (<italic>F</italic>(2, 75) = 3.250, <italic>p</italic> = 0.044, ηp2 = 0.080). Subsequent paired t-tests identified a significantly reduced semantic congruency effect between the Cathodal condition and the Sham condition (<italic>t</italic>(25) = -2.740, <italic>p</italic> = 0.011, 95% CI = [-11.915, 6.435]) (<xref rid="fig3" ref-type="fig">Figure 3A right</xref>). Moreover, a significant correlation was observed between the Cathodal-tDCS effect and MI (<italic>r</italic> = -0.457, <italic>p</italic> = 0.049, 95% CI = [-0.900, -0.014]) (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). RTs of congruent and incongruent trials of IFG and pMTG in each of the stimulation conditions were shown in <xref rid="atbl4" ref-type="table">Appendix Table 4A</xref>.</p>
</sec>
<sec id="s3b">
<title>Experiment 2: Time-sensitive modulation of left pMTG and IFG engagements by gradual changes in gesture-speech semantic information</title>
<p>A 2 (TMS effect: active - Vertex) × 5 (TW) ANOVA on semantic congruency revealed a significant interaction between TMS effect and TW (<italic>F</italic>(3.589, 82.538) = 3.273, <italic>p</italic> = 0.019, ηp2 = 0.125). Further t-tests identified a significant TMS effect over the pMTG in TW1 (<italic>t</italic>(23) = -3.068, <italic>p</italic> = 0.005, 95% CI = [-6.838, 0.702]), TW2 (<italic>t</italic>(23) = -2.923, <italic>p</italic> = 0.008, 95% CI = [-6.490, 0.644]), and TW7 (<italic>t</italic>(23) = -2.005, <italic>p</italic> = 0.047, 95% CI = [-5.628, 1.618]). In contrast, a significant TMS effect over the IFG was found in TW3 (<italic>t</italic>(23) = -2.335, <italic>p</italic> = 0.029, 95% CI = [-5.928, 1.258]), and TW6 (<italic>t</italic>(23) = -4.839, <italic>p</italic> &lt; 0.001, 95% CI = [-7.617, -2.061]) (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). Raw RTs of congruent and incongruent trials were shown in <xref rid="atbl4" ref-type="table">Appendix Table 4B</xref>.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>TMS effect over semantic congruency.</title>
<p><bold>(A)</bold> TMS effect was defined as active-TMS minus vertex-TMS. The semantic congruency effect was calculated as the reaction time (RT) difference between semantically incongruent and semantically congruent pairs.</p>
<p><bold>(B)</bold> Correlations of the TMS effect over the semantic congruency effect with three information models (gesture entropy, speech entropy and MI) are displayed with best-fitting regression lines. Significant correlations are marked in red. * <italic>p</italic> &lt; 0.05, ** <italic>p</italic> &lt; 0.01, *** <italic>p</italic> &lt; 0.001 after FDR correction.</p></caption>
<graphic xlink:href="517759v3_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Additionally, a significant negative correlation was found between the TMS effect (a more negative TMS effect represents a stronger interruption of the integration effect) and speech entropy when the pMTG was inhibited in TW2 (<italic>r</italic> = -0.792, <italic>p</italic> = 0.004, 95% CI = [-1.252, -0.331]). Meanwhile, when the IFG activity was interrupted in TW6, a significant negative correlation was found between the TMS effect and gesture entropy (<italic>r</italic> = -0.539, <italic>p</italic> = 0.014, 95% CI = [-0.956, -0.122]), speech entropy (<italic>r</italic> = -0.664, <italic>p</italic> = 0.026, 95% CI = [-1.255, -0.073]), and MI (<italic>r</italic> = -0.677, <italic>p</italic> = 0.001, 95% CI = [-1.054, -0.300]) (<xref rid="fig4" ref-type="fig">Figure 4B</xref>).</p>
</sec>
<sec id="s4">
<title>Experiment 3: Temporal modulation of P1, N1-P2, N400 and LPC components by gradual changes in gesture-speech semantic information</title>
<p>Topographical maps illustrating amplitude differences between the lower and higher halves of speech entropy demonstrate a central-posterior P1 amplitude (0-100 ms, <xref rid="fig5" ref-type="fig">Figure 5B2 middle</xref>). Aligning with prior findings<sup><xref ref-type="bibr" rid="c33">33</xref></sup>, the paired t-tests demonstrated a significantly larger P1 amplitude within the ML ROI (<italic>t</italic>(22) = 2.510, <italic>p</italic> = 0.020, 95% confidence interval (CI) = [1.66, 3.36]) when contrasting stimuli with higher 50% speech entropy against those with lower 50% speech entropy (<xref rid="fig5" ref-type="fig">Figure 5B2 left</xref>). Subsequent correlation analyses unveiled a significant increase in the P1 amplitude with the rise in speech entropy within the ML ROI (<italic>r</italic> = 0.609, <italic>p</italic> = 0.047, 95% CI = [0.039, 1.179], <xref rid="fig5" ref-type="fig">Figure 5B2 right</xref>). Furthermore, a cluster of neighboring time-electrode samples exhibited a significant contrast between the lower 50% and higher 50% of speech entropy, revealing a P1 effect spanning 16 to 78 ms at specific electrodes (FC2, FCz, C1, C2, Cz, and CPz, <xref rid="fig5" ref-type="fig">Figure 5B3 middle</xref>) (<italic>t</italic>(22) = 2.754, <italic>p</italic> = 0.004, 95% confidence interval (CI) = [1.65, 3.86], <xref rid="fig5" ref-type="fig">Figure 5B3 left</xref>), with a significant correlation with speech entropy (<italic>r</italic> = 0.636, <italic>p</italic> = 0.035, 95% CI = [0.081, 1.191], <xref rid="fig5" ref-type="fig">Figure 5B3 right</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>ERP results of gesture entropy (A), speech entropy (B) or MI (C).</title><p>Four ERP components were identified from grand-average ERPs at the Cz electrode, contrasting trials with the lower 50% (red lines) and the higher 50% (blue lines) of gesture entropy, speech entropy or MI (<bold>Top panels</bold>). Clusters of adjacent time points and electrodes were subsequently identified within each component using a cluster-based permutation test (<bold>Bottom right</bold>). Topographical maps depict amplitude differences between the lower and higher halves of each information model, with significant ROIs or electrode clusters highlighted in black. Solid rectangles delineating the ROIs that exhibited the maximal correlation and paired t-values (<bold>Bottom left</bold>). T-test comparisons with normal distribution lines and correlations with best-fitting regression lines are calculated and illustrated between the average ERP amplitude within the rectangular ROI (<bold>Bottom left</bold>) or the elicited clusters (<bold>Bottom right</bold>) and the three information models individually. * p &lt; 0.05, ** p &lt; 0.01 after FDR correction.</p></caption>
<graphic xlink:href="517759v3_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Additionally, topographical maps comparing the lower 50% and higher 50% gesture entropy revealed a frontal N1-P2 amplitude (150-250 ms, <xref rid="fig5" ref-type="fig">Figure 5A2 middle</xref>). In accordance with previous findings on bilateral frontal N1-P2 amplitude<sup><xref ref-type="bibr" rid="c33">33</xref></sup>, paired t-tests displayed a significantly larger amplitude for stimuli with lower 50% gesture entropy than with higher 50% entropy in both ROIs of LA (<italic>t</italic>(22) = 2.820, <italic>p</italic> = 0.011, 95% CI = [2.21, 3.43]) and RA (<italic>t</italic>(22) = 2.223, <italic>p</italic> = 0.038, 95% CI = [1.56, 2.89]) (<xref rid="fig5" ref-type="fig">Figure 5A2 left</xref>). Moreover, a negative correlation was found between N1-P2 amplitude and gesture entropy in both ROIs of LA (<italic>r</italic> = -0.465, <italic>p</italic> = 0.039, 95% CI = [-0.87, -0.06]) and RA (<italic>r</italic> = -0.465, <italic>p</italic> = 0.039, 95% CI = [-0.88, -0.05]) (<xref rid="fig5" ref-type="fig">Figure 5A2 right</xref>). Additionally, through a cluster-permutation test, the N1-P2 effect was identified between 184 to 202 ms at electrodes FC4, FC6, C2, C4, C6, and CP4 (<xref rid="fig5" ref-type="fig">Figure 5A3 middle</xref>) (<italic>t</italic>(22) = 2.638, <italic>p</italic> = 0.015, 95% CI = [1.79, 3.48], (<xref rid="fig5" ref-type="fig">Figure 5A3 left</xref>)), exhibiting a significant correlation with gesture entropy (<italic>r</italic> = -0.485, <italic>p</italic> = 0.030, 95% CI = [-0.91, -0.06], <xref rid="fig5" ref-type="fig">Figure 5A3 right</xref>).</p>
<p>Furthermore, in line with prior research<sup><xref ref-type="bibr" rid="c45">45</xref></sup>, a left-frontal N400 amplitude (250-450 ms) was discerned from topographical maps of both gesture entropy (<xref rid="fig5" ref-type="fig">Figure 5A4 middle</xref>) and MI (<xref rid="fig5" ref-type="fig">Figure 5C2 middle</xref>). Notably, a larger N400 amplitude in the LA ROI was consistently observed for stimuli with lower 50% values compared to those with higher 50% values, both for gesture entropy (<italic>t</italic>(22) = 2.455, <italic>p</italic> = 0.023, 95% CI = [1.95, 2.96], <xref rid="fig5" ref-type="fig">Figure 5A4 left</xref>) and MI (<italic>t</italic>(22) = 3.00, <italic>p</italic> = 0.007, 95% CI = [2.54, 3.46], <xref rid="fig5" ref-type="fig">Figure 5C2 left</xref>). Concurrently, a negative correlation was noted between the N400 amplitude and both gesture entropy (<italic>r</italic> = -0.480, <italic>p</italic> = 0.032, 95% CI = [-0.94, -0.03], <xref rid="fig5" ref-type="fig">Figure 5A4 right</xref>) and MI (<italic>r</italic> = -0.504, <italic>p</italic> = 0.028, 95% CI = [-0.97, -0.04], <xref rid="fig5" ref-type="fig">Figure 5C2 right</xref>) in the LA ROI.</p>
<p>The identified clusters with the N400 effect for gesture entropy (282 – 318 ms at electrodes FC1, FCz, C1, and Cz, <xref rid="fig5" ref-type="fig">Figure 5A5 middle</xref>) (<italic>t</italic>(22) = 2.828, <italic>p</italic> = 0.010, 95% CI = [2.02, 3.64], <xref rid="fig5" ref-type="fig">Figure 5A5 left</xref>) exhibited significant correlation between the N400 amplitude and gesture entropy (<italic>r</italic> = -0.445, <italic>p</italic> = 0.049, 95% CI = [-0.88, -0.01], <xref rid="fig5" ref-type="fig">Figure 5A5 right</xref>). Similarly, the cluster with the N400 effect for MI (294 – 306 ms at electrodes F1, F3, Fz, FC1, FC3, FCz, and C1, <xref rid="fig5" ref-type="fig">Figure 5C3 middle</xref>) (<italic>t</italic>(22) = 2.461, <italic>p</italic> = 0.023, 95% CI = [1.62, 3.30], <xref rid="fig5" ref-type="fig">Figure 5C3 left</xref>) also exhibited significant correlation (<italic>r</italic> = -0.569, <italic>p</italic> = 0.011, 95% CI = [-0.98, -0.16], <xref rid="fig5" ref-type="fig">Figure 5C5 right</xref>).</p>
<p>Finally, consistent with previous findings<sup><xref ref-type="bibr" rid="c33">33</xref></sup>, an anterior LPC effect (550-1000 ms) was observed in topographical maps comparing stimuli with lower and higher 50% speech entropy (<xref rid="fig5" ref-type="fig">Figure 5B4 middle</xref>). The reduced LPC amplitude was evident in the paired t-tests conducted in ROIs of LA (<italic>t</italic>(22) = 2.614, <italic>p</italic> = 0.016, 95% CI = [1.88, 3.35]); LC (<italic>t</italic>(22) = 2.592, <italic>p</italic> = 0.017, 95% CI = [1.83, 3.35]); RA (<italic>t</italic>(22) = 2.520, <italic>p</italic> = 0.020, 95% CI = [1.84, 3.24]); and ML (<italic>t</italic>(22) = 2.267, <italic>p</italic> = 0.034, 95% CI = [1.44, 3.10]) (<xref rid="fig5" ref-type="fig">Figure 5B4 left</xref>). Simultaneously, a marked negative correlation with speech entropy was evidenced in ROIs of LA (<italic>r</italic> = -0.836, <italic>p</italic> = 0.001, 95% CI = [-1.26, -0.42]); LC (<italic>r</italic> = -0.762, <italic>p</italic> = 0.006, 95% CI = [-1.23, -0.30]); RA (<italic>r</italic> = -0.774, <italic>p</italic> = 0.005, 95% CI = [-1.23, -0.32]) and ML (<italic>r</italic> = -0.730, <italic>p</italic> = 0.011, 95% CI = [-1.22, -0.24]) (<xref rid="fig5" ref-type="fig">Figure 5B4 right</xref>). Additionally, a cluster with the LPC effect (644 - 688 ms at electrodes Cz, CPz, P1, and Pz, <xref rid="fig5" ref-type="fig">Figure 5B5 middle</xref>) (<italic>t</italic>(22) = 2.754, <italic>p</italic> = 0.012, 95% CI = [1.50, 4.01], <xref rid="fig5" ref-type="fig">Figure 5B5 left</xref>) displayed a significant correlation with speech entropy (<italic>r</italic> = -0.699, <italic>p</italic> = 0.017, 95% CI = [-1.24, -0.16], <xref rid="fig5" ref-type="fig">Figure 5B5 right</xref>).</p>
</sec>
</sec>
<sec id="s5">
<title>Discussion</title>
<p>Through mathematical quantification of gesture and speech information using entropy and mutual information (MI), we examined the functional pattern and dynamic neural structure underlying multisensory semantic integration. Our results, for the first time, unveiled a progressive inhibition of IFG and pMTG by HD-tDCS as the degree of gesture-speech interaction, indexed by MI, advanced (Experiment 1). Additionally, the gradual neural engagement was found to be time-sensitive and staged, as evidenced by the selectively interrupted time windows (Experiment 2) and the distinct correlated ERP components (Experiment 3), which were modulated by top-down gesture constrain (gesture entropy) and bottom-up speech. These findings significantly expand our understanding of the cortical foundations of statistically regularized multisensory semantic information.</p>
<p>It is widely acknowledged that a single, amodal system mediates the interactions among perceptual representations of different modalities<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c46">46</xref></sup>. Moreover, observations have suggested that semantic dementia patients experience increasing overregularization of their conceptual knowledge due to the progressive deterioration of this amodal system<sup><xref ref-type="bibr" rid="c47">47</xref></sup>. Consequently, a graded function and structure of the transmodal ’hub’ representational system has been proposed<sup><xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c48">48</xref>,<xref ref-type="bibr" rid="c49">49</xref></sup>. In line with this, through the use of NIBS techniques such as HD-tDCS and TMS, the present study provides compelling evidence that the integration hubs of gesture and speech, namely the pMTG and IFG, function in a graded manner. This is supported by the progressive inhibition effect observed in these brain areas as the entropy and mutual information of gesture and speech advances.</p>
<p>Moreover, by dividing the potential integration period into eight TWs relative to the speech IP and administering inhibitory double-pulse TMS across each TW, the current study attributed the gradual TMS-selective regional inhibition to distinct information sources. In the early pre-lexical TW2 of gesture-speech integration, the suppression effect observed in the pMTG was correlated with speech entropy. Conversely, in the later post-lexical TW6, the IFG interruption effect was influenced by both gesture entropy, speech entropy, and their MI. A dual-stage pMTG-IFG-pMTG neurocircuit loop during gesture-speech integration has been proposed previous<sup><xref ref-type="bibr" rid="c28">28</xref></sup>. As an extension, the present study unveils a staged accumulation of engagement within the neurocircuit linking the transmodal regions of pMTG and IFG, arising from distinct contributors of information.</p>
<p>Furthermore, we disentangled the sub-processes of integration with high-temporal ERPs, when representations of gesture and speech were variously presented. Early P1-N1 and P2 sensory effects linked to perception and attentional processes<sup><xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c50">50</xref></sup> was comprehended as a reflection of the early audiovisual gesture-speech integration in the sensory-perceptual processing chain<sup><xref ref-type="bibr" rid="c51">51</xref></sup>. Note that a semantic priming paradigm was adopted here to create a top-down prediction of gesture over speech. The observed positive correlation of the P1 effect with speech entropy and the negative correlation of the N1-P2 effect with gesture entropy suggest that the early interaction of gesture-speech information was modulated by both top-down gesture prediction and bottom-up speech processing. Additionally, the lexico-semantic effect of the N400 and the LPC were differentially mediated by top-down gesture prediction, bottom-up speech encoding and their interaction: the N400 was negatively correlated with both the gesture entropy and MI, but the LPC was negatively correlated only with the speech entropy. Nonetheless, activation of representation is modulated progressively. The input stimuli would activate a dynamically distributed neural landscape, the state of which constructs gradually as measured by entropy and MI and correlates with the electrophysiological signals (N400 and LPC) which indicate the change of lexical representation. Consistent with recent account in multisensory information processing<sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c52">52</xref></sup>, our findings further confirm that the changed activation pattern can be induced from directions of both top-down and bottom-up gesture-speech processing.</p>
<p>Considering the close alignment of the ERP components with the TWs of TMS effect, it is reasonable to speculate the ERP components with the cortical involvements (<xref rid="fig6" ref-type="fig">Figure 6</xref>). Consequently, referencing the recurrent neurocircuit connecting the left IFG and pMTG for semantic unification<sup><xref ref-type="bibr" rid="c53">53</xref></sup>, we extended the previously proposed two-stage gesture-speech integration circuit<sup><xref ref-type="bibr" rid="c28">28</xref></sup> into sequential steps. First, bottom-up speech processing mapping acoustic signal to its lexical representation was performed from the STG/S to the pMTG. The larger speech entropy was, the greater effort was made during the matching of the acoustic input with its stored lexical representation, thus leading to a larger involvement of the pMTG at pre-lexical stage (TW2) and a larger P1 effect (<xref rid="fig6" ref-type="fig">Figure 6</xref> ①). Second, the gesture representation was activated in the pMTG and further exerted a top-down modulation over the phonological processing of speech in the STG/S<sup><xref ref-type="bibr" rid="c54">54</xref></sup>. The higher certainty of gesture, a larger modulation of gesture would be made upon speech, as indexed by a smaller gesture entropy with an enhanced N1-P2 amplitude (<xref rid="fig6" ref-type="fig">Figure 6</xref>②). Third, information was relayed from the pMTG to the IFG for sustained activation, during which a semantic constraint from gesture has been made on the semantic retrieval of speech. Greater TMS effect over the IFG at post-lexical stage (TW6) accompanying with a reduced N400 amplitude were found with the increase of gesture entropy, when the representation of gesture was wildly distributed and the constrain over the following speech was weak (<xref rid="fig6" ref-type="fig">Figure 6</xref>③). Fourth, the activated speech representation was compared with that of the gesture in the IFG. At this stage, the larger overlapped neural populations activated by gesture and speech as indexed by a larger MI, a greater TMS disruption effect of the IFG and a reduced N400 amplitude indexing easier integration and less semantic conflict were observed (<xref rid="fig6" ref-type="fig">Figure 6</xref>④). Last, the activated speech representation would disambiguate and reanalyze the semantic information that was stored in the IFG and further unify into a coherent comprehension in the pMTG<sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c55">55</xref></sup>. The more uncertain information being provided by speech, as indicated by an increased speech entropy, a stronger reweighting effect was made over the activated semantic information, resulting in a strengthened involvement of the IFG as well as a reduced LPC amplitude (<xref rid="fig6" ref-type="fig">Figure 6</xref>⑤).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Progressive processing stages of gesture–speech information within the pMTG-IFG loop.</title><p>Correlations between the TMS disruption effect of pMTG and IFG with three information models are represented by the orange line and the green lines, respectively. Black lines denote the strongest correlations of ROI averaged ERP components with three information models. * p &lt; 0.05, ** p &lt; 0.01 after FDR correction.</p></caption>
<graphic xlink:href="517759v3_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Note that the sequential cortical involvement and ERP components discussed above are derived from a deliberate alignment of speech onset with gesture DP, creating an artificial priming effect with gesture semantically preceding speech. Caution is advised when generalizing these findings to the spontaneous gesture-speech relationships, although gestures naturally precede speech<sup><xref ref-type="bibr" rid="c56">56</xref></sup>.</p>
<p>Limitations exist. ERP components and cortical engagements were linked through intermediary variables of entropy and MI. Dissociations were observed between ERP components and cortical engagement. Importantly, there is no direct evidence of the brain structures underpinning the corresponding ERPs, necessitating clarification in future studies. Additionally, not all influenced TWs exhibited significant associations with entropy and MI. While HD-tDCS and TMS may impact functionally and anatomically connected brain regions<sup><xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c44">44</xref></sup>, the graded functionality of every disturbed period is not guaranteed. Caution is warranted in interpreting the causal relationship between NIBS inhabitation effects and information-theoretic metrics (entropy and MI). Finally, the current study incorporated a restricted set of entropy and MI measures. The generalizability of the findings should be assessed in future studies using a more extensive range of matrices.</p>
<p>In summary, utilizing information-theoretic complexity metrics such as entropy and mutual information (MI), our study demonstrates that multisensory semantic processing, involving gesture and speech, gives rise to dynamically evolving representations through the interplay between gesture-primed prediction and speech presentation. This process correlates with the progressive engagement of the pMTG-IFG-pMTG circuit and various ERP components. These findings significantly advancing our understanding of the neural mechanisms underlying multisensory semantic integration.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>This research was supported by grants from the STI 2030—Major Projects 2021ZD0201500, the National Natural Science Foundation of China (31822024, 31800964), the Scientific Foundation of Institute of Psychology, Chinese Academy of Sciences (E2CX3625CX), and the Strategic Priority Research Program of Chinese Academy of Sciences (XDB32010300).</p>
</ack>
<sec id="s6">
<title>Additional information</title>
<sec id="s6a">
<title>Author contributions</title>
<p>Conceptualization, W.Y.Z. and Y.D.; Investigation, W.Y.Z. and Z.Y.L.; Formal Analysis, W.Y.Z. and Z.Y.L.; Methodology, W.Y.Z. and Z.Y.L.; Validation, Z.Y.L. and X.L.; Visualization, W.Y.Z. and Z.Y.L. and X.L.; Funding Acquisition, W.Y.Z. and Y.D.; Supervision, Y.D.; Project administration, Y.D.; Writing – Original Draft, W.Y.Z.; Writing – Review &amp; Editing, W.Y.Z., Z.Y.L., X.L., and Y.D.</p>
</sec>
<sec id="s7">
<title>Competing interests</title>
<p>The authors declare no competing interests.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Damasio</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Grabowski</surname>, <given-names>T.J.</given-names></string-name>, <string-name><surname>Tranel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hichwa</surname>, <given-names>R.D.</given-names></string-name>, and <string-name><surname>Damasio</surname>, <given-names>A.R</given-names></string-name></person-group>. (<year>1996</year>). <article-title>A neural basis for lexical retrieval</article-title>. <source>Nature</source> <volume>380</volume>, <fpage>499</fpage>–<lpage>505</lpage>. DOI <pub-id pub-id-type="doi">10.1038/380499a0</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Patterson</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Nestor</surname>, <given-names>P.J.</given-names></string-name>, and <string-name><surname>Rogers</surname>, <given-names>T.T</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Where do you know what you know? The representation of semantic knowledge in the human brain</article-title>. <source>Nature Reviews Neuroscience</source> <volume>8</volume>, <fpage>976</fpage>–<lpage>987</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2277</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brennan</surname>, <given-names>J.R.</given-names></string-name>, <string-name><surname>Stabler</surname>, <given-names>E.P.</given-names></string-name>, <string-name><surname>Van Wagenen</surname>, <given-names>S.E.</given-names></string-name>, <string-name><surname>Luh</surname>, <given-names>W.M.</given-names></string-name>, and <string-name><surname>Hale</surname>, <given-names>J.T.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Abstract linguistic structure correlates with temporal activity during naturalistic comprehension</article-title>. <source>Brain and Language</source> <volume>157</volume>, <fpage>81</fpage>–<lpage>94</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandl.2016.04.008</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benetti</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ferrari</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Pavani</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Multimodal processing in face-to-face interactions: A bridging link between psycholinguistics and sensory neuroscience</article-title>. <source>Front Hum Neurosci</source> <volume>17</volume>, <fpage>1108354</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2023.1108354</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Armeni</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Willems</surname>, <given-names>R.M.</given-names></string-name>, and <string-name><surname>Frank</surname>, <given-names>S.L</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Probabilistic language models in cognitive neuroscience: Promises and pitfalls</article-title>. <source>Neurosci Biobehav R</source> <volume>83</volume>, <fpage>579</fpage>–<lpage>588</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.09.001</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leonard</surname>, <given-names>M.K.</given-names></string-name>, <string-name><surname>Bouchard</surname>, <given-names>K.E.</given-names></string-name>, <string-name><surname>Tang</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Chang</surname>, <given-names>E.F</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Dynamic encoding of speech sequence probability in human temporal cortex</article-title>. <source>Journal of Neuroscience</source> <volume>35</volume>, <fpage>7203</fpage>–<lpage>7214</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4100-14.2015</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goldstein</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zada</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Buchnik</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Schain</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Price</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Aubrey</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Nastase</surname>, <given-names>S.A.</given-names></string-name>, <string-name><surname>Feder</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Emanuel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Cohen</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2022</year>). <article-title>Shared computational principles for language processing in humans and deep language models</article-title>. <source>Nature Neuroscience</source> <volume>25</volume>, <fpage>369</fpage>-+. <pub-id pub-id-type="doi">10.1038/s41593-022-01026-4</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heilbron</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Armeni</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Schoffelen</surname>, <given-names>J.-M.</given-names></string-name>, <string-name><surname>Hagoort</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>de Lange</surname>, <given-names>F.P.</given-names></string-name></person-group> (<year>2022</year>). <article-title>A hierarchy of linguistic predictions during natural language comprehension</article-title>. <source>P Natl Acad Sci USA</source> <volume>119</volume>, <fpage>e2201968119</fpage>–<lpage>e2201968119</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.2201968119</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kuperberg</surname>, <given-names>G.R.</given-names></string-name>, and <string-name><surname>Jaeger</surname>, <given-names>T.F</given-names></string-name></person-group>. (<year>2016</year>). <article-title>What do we mean by prediction in language comprehension?</article-title> <source>Lang Cogn Neurosci</source> <volume>31</volume>, <fpage>32</fpage>–<lpage>59</lpage>. <pub-id pub-id-type="doi">10.1080/23273798.2015.1102299</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ralph</surname>, <given-names>M.A.L</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Neurocognitive insights on conceptual knowledge and its breakdown</article-title>. <source>Philos T R Soc B</source> <volume>369</volume>. <elocation-id>20120392</elocation-id> <pub-id pub-id-type="doi">10.1098/rstb.2012.0392</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rogers</surname>, <given-names>T.T.</given-names></string-name>, <string-name><surname>Ralph</surname>, <given-names>M.A.L.</given-names></string-name>, <string-name><surname>Garrard</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Bozeat</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>McClelland</surname>, <given-names>J.L.</given-names></string-name>, <string-name><surname>Hodges</surname>, <given-names>J.R.</given-names></string-name>, and <string-name><surname>Patterson</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Structure and deterioration of semantic memory: A neuropsychological and computational investigation</article-title>. <source>Psychological Review</source> <volume>111</volume>, <fpage>205</fpage>–<lpage>235</lpage>. <pub-id pub-id-type="doi">10.1037/0033-295x.111.1.205</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ralph</surname>, <given-names>M.A.L.</given-names></string-name>, <string-name><surname>Jefferies</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Patterson</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Rogers</surname>, <given-names>T.T</given-names></string-name></person-group>. (<year>2017</year>). <article-title>The neural and computational bases of semantic cognition</article-title>. <source>Nature Reviews Neuroscience</source> <volume>18</volume>, <fpage>42</fpage>–<lpage>55</lpage>. <pub-id pub-id-type="doi">10.1038/nrn.2016.150</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holler</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Levinson</surname>, <given-names>S.C</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Multimodal language processing in human communication</article-title>. <source>Trends in Cognitive Sciences</source> <volume>23</volume>, <fpage>639</fpage>–<lpage>652</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2019.05.006</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hostetter</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Mainela-Arnold</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Gestures occur with spatial and Motoric knowledge: It’s more than just coincidence</article-title>. <source>Perspectives on Language Learning and Education</source> <volume>22</volume>, <fpage>42</fpage>–<lpage>49</lpage>. doi:<pub-id pub-id-type="doi">10.1044/lle22.2.42</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>McNeill</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2005</year>). <source>Gesture and though</source> (<publisher-name>University of Chicago Press</publisher-name>). <pub-id pub-id-type="doi">10.7208/chicago/9780226514642.001.0001</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kendon</surname>, <given-names>A</given-names></string-name></person-group>. (<year>1997</year>). <article-title>Gesture</article-title>. <source>Annu Rev Anthropol</source> <volume>26</volume>, <fpage>109</fpage>–<lpage>128</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.anthro.26.1.109</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2005</year>). <article-title>On broca, brain, and binding: a new framework</article-title>. <source>Trends in Cognitive Sciences</source> <volume>9</volume>, <fpage>416</fpage>–<lpage>423</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2005.07.004</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hagoort</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Hald</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Bastiaansen</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Petersson</surname>, <given-names>K.M</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Integration of word meaning and world knowledge in language comprehension</article-title>. <source>Science</source> <volume>304</volume>, <fpage>438</fpage>–<lpage>441</lpage>. <pub-id pub-id-type="doi">10.1126/science.1095455</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ozyurek</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Willems</surname>, <given-names>R.M.</given-names></string-name>, <string-name><surname>Kita</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2007</year>). <article-title>On-line integration of semantic information from speech and gesture: Insights from event-related brain potentials</article-title>. <source>J Cognitive Neurosci</source> <volume>19</volume>, <fpage>605</fpage>–<lpage>616</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2007.19.4.605</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Willems</surname>, <given-names>R.M.</given-names></string-name>, <string-name><surname>Ozyurek</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Differential roles for left inferior frontal and superior temporal cortex in multimodal integration of action and language</article-title>. <source>Neuroimage</source> <volume>47</volume>, <fpage>1992</fpage>–<lpage>2004</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.05.066</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Drijvers</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Jensen</surname>, <given-names>O.</given-names></string-name>, and <string-name><surname>Spaak</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Rapid invisible frequency tagging reveals nonlinear integration of auditory and visual information</article-title>. <source>Human Brain Mapping</source> <volume>42</volume>, <fpage>1138</fpage>–<lpage>1152</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.25282</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Drijvers</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Ozyurek</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Native language status of the listener modulates the neural integration of speech and iconic gestures in clear and adverse listening conditions</article-title>. <source>Brain and Language</source> <volume>177</volume>, <fpage>7</fpage>–<lpage>17</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandl.2018.01.003</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Drijvers</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>van der Plas</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ozyurek</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Jensen</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Native and non-native listeners show similar yet distinct oscillatory dynamics when using gestures to access speech in noise</article-title>. <source>Neuroimage</source> <volume>194</volume>, <fpage>55</fpage>–<lpage>67</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.032</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holle</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Gunter</surname>, <given-names>T.C</given-names></string-name></person-group>. (<year>2007</year>). <article-title>The role of iconic gestures in speech disambiguation: ERP evidence</article-title>. <source>J Cognitive Neurosci</source> <volume>19</volume>, <fpage>1175</fpage>–<lpage>1192</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2007.19.7.1175</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kita</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Ozyurek</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2003</year>). <article-title>What does cross-linguistic variation in semantic coordination of speech and gesture reveal?: Evidence for an interface representation of spatial thinking and speaking</article-title>. <source>J Mem Lang</source> <volume>48</volume>, <fpage>16</fpage>–<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1016/S0749-596x(02)00505-3</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bernardis</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Gentilucci</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Speech and gesture share the same communication system</article-title>. <source>Neuropsychologia</source> <volume>44</volume>, <fpage>178</fpage>–<lpage>190</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.05.007</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhao</surname>, <given-names>W.Y.</given-names></string-name>, <string-name><surname>Riggs</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Schindler</surname>, <given-names>I.</given-names></string-name>, and <string-name><surname>Holle</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Transcranial magnetic stimulation over left inferior frontal and posterior temporal cortex disrupts gesture-speech integration</article-title>. <source>Journal of Neuroscience</source> <volume>38</volume>, <fpage>1891</fpage>–<lpage>1900</lpage>. <pub-id pub-id-type="doi">10.1523/Jneurosci.1748-17.2017</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhao</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Du</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2021</year>). <article-title>TMS reveals dynamic interaction between inferior frontal gyrus and posterior middle temporal gyrus in gesture-speech semantic integration</article-title>. <source>The Journal of Neuroscience</source>, 10356-10364. <pub-id pub-id-type="doi">10.1523/jneurosci.1355-21.2021</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shannon</surname>, <given-names>C.E</given-names></string-name></person-group>. (<year>1948</year>). <article-title>A mathematical theory of communication</article-title>. <source>Bell Syst Tech J</source> <volume>27</volume>, <fpage>379</fpage>–<lpage>423</lpage>. <pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb01338.x</pub-id>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tremblay</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Deschamps</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Baroni</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Hasson</surname>, <given-names>U</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Neural sensitivity to syllable frequency and mutual information in speech perception and production</article-title>. <source>Neuroimage</source> <volume>136</volume>, <fpage>106</fpage>–<lpage>121</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.05.018</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bikson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Inoue</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Akiyama</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Deans</surname>, <given-names>J.K.</given-names></string-name>, <string-name><surname>Fox</surname>, <given-names>J.E.</given-names></string-name>, <string-name><surname>Miyakawa</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Jefferys</surname>, <given-names>J.G.R</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Effects of uniform extracellular DC electric fields on excitability in rat hippocampal slices</article-title>. <source>J Physiol-London</source> <volume>557</volume>, <fpage>175</fpage>–<lpage>190</lpage>. <pub-id pub-id-type="doi">10.1113/jphysiol.2003.055772</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pitcher</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Walsh</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Yovel</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Duchaine</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2007</year>). <article-title>TMS evidence for the involvement of the right occipital face area in early face processing</article-title>. <source>Current Biology</source> <volume>17</volume>, <fpage>1568</fpage>–<lpage>1573</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2007.07.063</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Federmeier</surname>, <given-names>K.D.</given-names></string-name>, <string-name><surname>Mai</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Kutas</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Both sides get the point: hemispheric sensitivities to sentential constraint</article-title>. <source>Memory &amp; Cognition</source> <volume>33</volume>, <fpage>871</fpage>–<lpage>886</lpage>. <pub-id pub-id-type="doi">10.3758/bf03193082</pub-id>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kelly</surname>, <given-names>S.D.</given-names></string-name>, <string-name><surname>Kravitz</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Hopkins</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Neural correlates of bimodal speech and gesture comprehension</article-title>. <source>Brain and Language</source> <volume>89</volume>, <fpage>253</fpage>–<lpage>260</lpage>. <pub-id pub-id-type="doi">10.1016/s0093-934x(03)00335-3</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname>, <given-names>Y.C.</given-names></string-name>, and <string-name><surname>Coulson</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Meaningful gestures: Electrophysiological indices of iconic gesture comprehension</article-title>. <source>Psychophysiology</source> <volume>42</volume>, <fpage>654</fpage>–<lpage>667</lpage>. <pub-id pub-id-type="doi">10.1111/j.1469-8986.2005.00356.x</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fritz</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Kita</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Littlemore</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Krott</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Multimodal language processing: How preceding discourse constrains gesture interpretation and affects gesture integration when gestures do not synchronise with semantic affiliates</article-title>. <source>J Mem Lang</source> <volume>117</volume>, <fpage>104191</fpage>. <pub-id pub-id-type="doi">10.1016/j.jml.2020.104191</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gunter</surname>, <given-names>T.C.</given-names></string-name>, and <string-name><surname>Weinbrenner</surname>, <given-names>J.E.D</given-names></string-name></person-group>. (<year>2017</year>). <article-title>When to take a gesture seriously: On how we use and prioritize communicative cues</article-title>. <source>J Cognitive Neurosci</source> <volume>29</volume>, <fpage>1355</fpage>–<lpage>1367</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_01125</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oldfield</surname>, <given-names>R.C</given-names></string-name></person-group>. (<year>1971</year>). <article-title>The assessment and analysis of handedness: the Edinburgh inventory</article-title>. <source>Neuropsychologia</source> <volume>9</volume>, <fpage>97</fpage>–<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1016/0028-3932(71)90067-4</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kelly</surname>, <given-names>S.D.</given-names></string-name>, <string-name><surname>Creigh</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Bartolotti</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Integrating speech and iconic gestures in a Stroop-like task: Evidence for automatic processing</article-title>. <source>Journal of Cognitive Neuroscience</source> <volume>22</volume>, <fpage>683</fpage>–<lpage>694</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2009.21254</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koessler</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Maillard</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Benhadid</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Vignal</surname>, <given-names>J.P.</given-names></string-name>, <string-name><surname>Felblinger</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Vespignani</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Braun</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Automated cortical projection of EEG sensors: Anatomical correlation via the international 10-10 system</article-title>. <source>Neuroimage</source> <volume>46</volume>, <fpage>64</fpage>–<lpage>72</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.02.006</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nuwer</surname>, <given-names>M.R.</given-names></string-name>, <string-name><surname>Comi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Emerson</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fuglsang-Frederiksen</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Guerit</surname>, <given-names>J.M.</given-names></string-name>, <string-name><surname>Hinrichs</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ikeda</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Luccas</surname>, <given-names>F.J.</given-names></string-name>, and <string-name><surname>Rappelsberger</surname>, <given-names>P</given-names></string-name></person-group>. (<year>1999</year>). <article-title>IFCN standards for digital recording of clinical EEG. The International Federation of Clinical Neurophysiology</article-title>. <source>Electroencephalogr Clin Neurophysiol Suppl</source> <volume>52</volume>, <fpage>11</fpage>–<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1016/S0013-4694(97)00106-5</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Delorme</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Makeig</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2004</year>). <article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title>. <source>J Neurosci Methods</source> <volume>134</volume>, <fpage>9</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Habets</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Kita</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Shao</surname>, <given-names>Z.S.</given-names></string-name>, <string-name><surname>Ozyurek</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2011</year>). <article-title>The Role of Synchrony and Ambiguity in Speech-Gesture Integration during Comprehension</article-title>. <source>J Cognitive Neurosci</source> <volume>23</volume>, <fpage>1845</fpage>–<lpage>1854</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2010.21462</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fries</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Schoffelen</surname>, <given-names>J.-M</given-names></string-name></person-group>. (<year>2011</year>). <article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title>. <source>Computational Intelligence and Neuroscience</source> <volume>2011</volume>, <issue>156869</issue>. <pub-id pub-id-type="doi">10.1155/2011/156869</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kutas</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Federmeier</surname>, <given-names>K.D</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Thirty Years and Counting: Finding Meaning in the N400 Component of the Event-Related Brain Potential (ERP)</article-title>. <source>Annual Review of Psychology</source>, Vol <volume>62</volume> <issue>62</issue>, <fpage>621</fpage>–<lpage>647</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Noppeney</surname>, <given-names>U</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Perceptual Inference, Learning, and Attention in a Multisensory World</article-title>. <source>Annual Review of Neuroscience</source>, Vol <volume>44</volume>, 2021 44, <fpage>449</fpage>–<lpage>473</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-neuro-100120-085519</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rogers</surname>, <given-names>T.T.</given-names></string-name>, <string-name><surname>Hodges</surname>, <given-names>J.R.</given-names></string-name>, <string-name><surname>Ralph</surname>, <given-names>M.A.L.</given-names></string-name>, and <string-name><surname>Patterson</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Object recognition under semantic impairment: The effects of conceptual regularities on perceptual decisions</article-title>. <source>Lang Cognitive Proc</source> <volume>18</volume>, <fpage>625</fpage>–<lpage>662</lpage>. <pub-id pub-id-type="doi">10.1080/01690960344000053</pub-id>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krieger-Redwood</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Souter</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Gonzalez Alam</surname>, <given-names>T.R.d.J.</given-names></string-name>, <string-name><surname>Smallwood</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Jackson</surname>, <given-names>R.L.</given-names></string-name>, and <string-name><surname>Jefferies</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Graded and sharp transitions in semantic function in left temporal lobe</article-title>. <source>Brain and Language</source> <volume>251</volume>, <fpage>105402</fpage>. <pub-id pub-id-type="doi">10.1016/j.bandl.2024.105402</pub-id>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Plaut</surname>, <given-names>D.C</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Graded modality-specific specialisation in semantics: A computational account of optic aphasia</article-title>. <source>Cognitive Neuropsychology</source> <volume>19</volume>, <fpage>603</fpage>–<lpage>639</lpage>. <pub-id pub-id-type="doi">10.1080/02643290244000112</pub-id>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fadiga</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Craighero</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Olivier</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Human motor cortex excitability during the perception of others’ action</article-title>. <source>Current Opinion in Neurobiology</source> <volume>15</volume>, <fpage>213</fpage>–<lpage>218</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2005.03.013</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giard</surname>, <given-names>M.H.</given-names></string-name>, and <string-name><surname>Peronnet</surname>, <given-names>F</given-names></string-name></person-group>. (<year>1999</year>). <article-title>Auditory-visual integration during multimodal object recognition in humans: A behavioral and electrophysiological study</article-title>. <source>J Cognitive Neurosci</source> <volume>11</volume>, <fpage>473</fpage>–<lpage>490</lpage>. <pub-id pub-id-type="doi">10.1162/089892999563544</pub-id>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Trujillo</surname>, <given-names>J.P.</given-names></string-name>, and <string-name><surname>Holler</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Interactionally Embedded Gestalt Principles of Multimodal Human Communication</article-title>. <source>Perspect Psychol Sci</source> <volume>18</volume>, <fpage>1136</fpage>–<lpage>1159</lpage>. <pub-id pub-id-type="doi">10.1177/17456916221141422</pub-id>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2013</year>). <article-title>MUC (Memory, Unification</article-title>, <source>Control) and beyond. Frontiers in Psychology</source> <volume>4</volume>, <fpage>416</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2013.00416</pub-id>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bizley</surname>, <given-names>J.K.</given-names></string-name>, <string-name><surname>Maddox</surname>, <given-names>R.K.</given-names></string-name>, and <string-name><surname>Lee</surname>, <given-names>A.K.C</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Defining auditory-visual objects: Behavioral tests and physiological mechanisms</article-title>. <source>Trends in Neurosciences</source> <volume>39</volume>, <fpage>74</fpage>–<lpage>85</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2015.12.007</pub-id>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tesink</surname>, <given-names>C.M.J.Y.</given-names></string-name>, <string-name><surname>Petersson</surname>, <given-names>K.M.</given-names></string-name>, <string-name><surname>van Berkum</surname>, <given-names>J.J.A.</given-names></string-name>, <string-name><surname>van den Brink</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Buitelaar</surname>, <given-names>J.K.</given-names></string-name>, and <string-name><surname>Hagoort</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Unification of speaker and meaning in language comprehension: An fMRI study</article-title>. <source>J Cognitive Neurosci</source> <volume>21</volume>, <fpage>2085</fpage>–<lpage>2099</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2008.21161</pub-id>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>McNeill</surname>, <given-names>D.</given-names></string-name></person-group> (<year>1992</year>). <source>Hand and mind : what gestures reveal about thought</source> (<publisher-name>University of Chicago Press</publisher-name>). <pub-id pub-id-type="doi">10.2307/1576015</pub-id>.</mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1">
<table-wrap id="atbl1" orientation="portrait" position="float">
<label>Appendix Table 1.</label>
<caption><title>Gesture description and paring with incongruent and congruent speech.</title></caption>
<graphic xlink:href="517759v3_atbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="atbl2" orientation="portrait" position="float">
<label>Appendix Table 2.</label>
<caption><title>Examples of ‘an4 (press)’ for the calculation of speech entropy, gesture entropy and mutual information (MI)</title></caption>
<graphic xlink:href="517759v3_atbl2.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="atbl3" orientation="portrait" position="float">
<label>Appendix Table 3.</label>
<caption><title>Quantitative information for each stimulus.</title></caption>
<graphic xlink:href="517759v3_atbl3.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="atbl4" orientation="portrait" position="float">
<label>Appendix Table 4.</label>
<caption><title>Raw RT of semantic congruent (Sc) and semantic incongruent (Si) in Experiment 1 and Experiment 2.</title></caption>
<graphic xlink:href="517759v3_atbl4.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="517759v3_atbl4a.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99416.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Martin</surname>
<given-names>Andrea E</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Max Planck Institute for Psycholinguistics</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>useful</bold> study investigates the role of frontotemporal regions in integrating linguistic and extra-linguistic information during communication, focusing on the inferior frontal gyrus and posterior middle temporal gyrus. It uses brain stimulation and electroencephalography to study speech-gesture integration. While the research question is interesting, the methods are insufficient for studying tightly-coupled brain regions over short timescales, leading to <bold>incomplete</bold> support for the claims due to conceptual and methodological limitations.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99416.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors quantified information in gesture and speech, and investigated the neural processing of speech and gestures in pMTG and LIFG, depending on their informational content, in 8 different time-windows, and using three different methods (EEG, HD-tDCS and TMS). They found that there is a time-sensitive and staged progression of neural engagement that is correlated with the informational content of the signal (speech/gesture).</p>
<p>Strengths:</p>
<p>A strength of the paper is that the authors attempted to combine three different methods to investigate speech-gesture processing.</p>
<p>Weaknesses:</p>
<p>(1) One major issue is that there is a tight anatomical coupling between pMTG and LIFG. Stimulating one area could therefore also result in stimulation of the other area (see Silvanto and Pascual-Leone, 2008). I therefore think it is very difficult to tease apart the contribution of these areas to the speech-gesture integration process, especially considering that the authors stimulate these regions in time windows that are very close to each other in both time and space (and the disruption might last longer over time).</p>
<p>(2) Related to this point, it is unclear to me why the HD-TDCS/TMS is delivered in set time windows for each region. How did the authors determine this, and how do the results for TMS compare to their previous work from 2018 and 2023 (which describes a similar dataset+design)? How can they ensure they are only targeting their intended region since they are so anatomically close to each other?</p>
<p>(3) As the EEG signal is often not normally distributed, I was wondering whether the authors checked the assumptions for their Pearson correlations. The authors could perhaps better choose to model the different variables to see whether MI/entropy could predict the neural responses. How did they correct the many correlational analyses that they have performed?</p>
<p>(4) The authors use ROIs for their different analyses, but it is unclear why and on the basis of what these regions are defined. Why not consider all channels without making them part of an ROI, by using a method like the one described in my previous comment?</p>
<p>(5) The authors describe that they have divided their EEG data into a &quot;lower half&quot; and a &quot;higher half&quot; (lines 234-236), based on entropy scores. It is unclear why this is necessary, and I would suggest just using the entropy scores as a continuous measure.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99416.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The study is an innovative and fundamental study that clarified important aspects of brain processes for integration of information from speech and iconic gesture (i.e., gesture that depicts action, movement, and shape), based on tDCS, TMS, and EEG experiments. They evaluated their speech and gesture stimuli in information-theoretic ways and calculated how informative speech is (i.e., entropy), how informative gesture is, and how much shared information speech and gesture encode. The tDCS and TMS studies found that the left IFG and pMTG, the two areas that were activated in fMRI studies on speech-gesture integration in the previous literature, are causally implicated in speech-gesture integration. The size of tDC and TMS effects are correlated with the entropy of the stimuli or mutual information, which indicates that the effects stem from the modulation of information decoding/integration processes. The EEG study showed that various ERP (event-related potential, e.g., N1-P2, N400, LPC) effects that have been observed in speech-gesture integration experiments in the previous literature, are modulated by the entropy of speech/gesture and mutual information. This makes it clear that these effects are related to information decoding processes. The authors propose a model of how the speech-gesture integration process unfolds in time, and how IFG and pMTG interact with each other in that process.</p>
<p>Strengths:</p>
<p>The key strength of this study is that the authors used information theoretic measures of their stimuli (i.e., entropy and mutual information between speech and gesture) in all of their analyses. This made it clear that the neuro-modulation (tDCS, TMS) affected information decoding/integration and ERP effects reflect information decoding/integration. This study used tDCS and TMS methods to demonstrate that left IFG and pMTG are causally involved in speech-gesture integration. The size of tDCS and TMS effects are correlated with information-theoretic measures of the stimuli, which indicate that the effects indeed stem from disruption/facilitation of the information decoding/integration process (rather than generic excitation/inhibition). The authors' results also showed a correlation between information-theoretic measures of stimuli with various ERP effects. This indicates that these ERP effects reflect the information decoding/integration process.</p>
<p>Weaknesses:</p>
<p>The &quot;mutual information&quot; cannot fully capture the interplay of the meaning of speech and gesture. The mutual information is calculated based on what information can be decoded from speech alone and what information can be decoded from gesture alone. However, when speech and gesture are combined, a novel meaning can emerge, which cannot be decoded from a single modality alone. When example, a person produces a gesture of writing something with a pen, while saying &quot;He paid&quot;. The speech-gesture combination can be interpreted as &quot;paying by signing a cheque&quot;. It is highly unlikely that this meaning is decoded when people hear speech only or see gestures only. The current study cannot address how such speech-gesture integration occurs in the brain, and what ERP effects may reflect such a process. Future studies can classify different types of speech-gesture integration and investigate neural processes that underlie each type. Another important topic for future studies is to investigate how the neural processes of speech-gesture integration change when the relative timing between the speech stimulus and the gesture stimulus changes.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99416.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this useful study, Zhao et al. try to extend the evidence for their previously described two-step model of speech-gesture integration in the posterior Middle Temporal Gyrus (pMTG) and Inferior Frontal Gyrus (IFG). They repeat some of their previous experimental paradigms, but this time quantifying Information-Theoretical (IT) metrics of the stimuli in a stroop-like paradigm purported to engage speech-gesture integration. They then correlate these metrics with the disruption of what they claim to be an integration effect observable in reaction times during the tasks following brain stimulation, as well as documenting the ERP components in response to the variability in these metrics.</p>
<p>The integration of multiple methods, like tDCS, TMS, and ERPs to provide converging evidence renders the results solid. However, their interpretation of the results should be taken with care, as some critical confounds, like difficulty, were not accounted for, and the conceptual link between the IT metrics and what the authors claim they index is tenuous and in need of more evidence. In some cases, the difficulty making this link seems to arise from conceptual equivocation (e.g., their claims regarding 'graded' evidence), whilst in some others it might arise from the usage of unclear wording in the writing of the manuscript (e.g. the sentence 'quantitatively functional mental states defined by a specific parser unified by statistical regularities'). Having said that, the authors' aim is valuable, and addressing these issues would render the work a very useful approach to improve our understanding of integration during semantic processing, being of interest to scientists working in cognitive neuroscience and neuroimaging.</p>
<p>The main hurdle to achieving the aims set by the authors is the presence of the confound of difficulty in their IT metrics. Their measure of entropy, for example, being derived from the distribution of responses of the participants to the stimuli, will tend to be high for words or gestures with multiple competing candidate representations (this is what would presumptively give rise to the diversity of responses in high-entropy items). There is ample evidence implicating IFG and pMTG as key regions of the semantic control network, which is critical during difficult semantic processing when, for example, semantic processing must resolve competition between multiple candidate representations, or when there are increased selection pressures (Jackson et al., 2021). Thus, the authors' interpretation of Mutual Information (MI) as an index of integration is inextricably contaminated with difficulty arising from multiple candidate representations. This casts doubt on the claims of the role of pMTG and IFG as regions carrying out gesture-speech integration as the observed pattern of results could also be interpreted in terms of brain stimulation interrupting the semantic control network's ability to select the best candidate for a given context or respond to more demanding semantic processing.</p>
<p>In terms of conceptual equivocation, the use of the term 'graded' by the authors seems to be different from the usage commonly employed in the semantic cognition literature (e.g., the 'graded hub hypothesis', Rice et al., 2015). The idea of a graded hub in the controlled semantic cognition framework (i.e., the anterior temporal lobe) refers to a progressive degree of abstraction or heteromodal information as you progress through the anatomy of the region (i.e., along the dorsal-to-ventral axis). The authors, on the other hand, seem to refer to 'graded manner' in the context of a correlation of entropy or MI and the change in the difference between Reaction Times (RTs) of semantically congruent vs incongruent gesture-speech. The issue is that the discourse through parts of the introduction and discussion seems to conflate both interpretations, and the ideas in the main text do not correspond to the references they cite. This is not overall very convincing. What is it exactly the authors are arguing about the correlation between RTs and MI indexes? As stated above, their measure of entropy captures the spread of responses, which could also be a measure of item difficulty (more diverse responses imply fewer correct responses, a classic index of difficulty). Capturing the diversity of responses means that items with high entropy scores are also likely to have multiple candidate representations, leading to increased selection pressures. Regions like pMTG and IFG have been widely implicated in difficult semantic processing and increased selection pressures (Jackson et al., 2021). How is this MI correlation evidence of integration that proceeds in a 'graded manner'? The conceptual links between these concepts must be made clearer for the interpretation to be convincing.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99416.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhao</surname>
<given-names>Wanying</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6979-940X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Zhouyi</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Xiang</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Du</surname>
<given-names>Yi</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4512-5221</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p><bold>Responses to Editors:</bold></p>
<p>We appreciate Reviewer 1’s first concern regarding the difficulty of disentangling the contributions of tightly-coupled brain regions to the speech-gesture integration process—particularly due to the close temporal and spatial proximity of the stimulation windows and the potential for prolonged disruption. We would like to provide clarification and evidence supporting the validity of our methodology.</p>
<p>Our previous study (Zhao et al., 2021, J. Neurosci) employed the same experimental protocol—using inhibitory double-pulse transcranial magnetic stimulation (TMS) over the inferior frontal gyrus (IFG) and posterior middle temporal gyrus (pMTG) in one of eight 40-ms time windows. The findings from that study demonstrated a time-window-selective disruption of the semantic congruency effect (i.e., reaction time costs driven by semantic conflict), with no significant modulation of the gender congruency effect (i.e., reaction time costs due to gender conflict). This result establishes that double-pulse TMS provides sufficient temporal precision to independently target the left IFG and pMTG within these 40-ms windows during gesture-speech integration. Importantly, by comparing the distinctively inhibited time windows for IFG and pMTG, we offered clear evidence of distinct engagement and temporal dynamics between these regions during different stages of gesture-speech semantic processing.</p>
<p>Furthermore, we reviewed prior studies utilizing double-pulse TMS on structurally and functionally connected brain regions to explore neural contributions across timescales as brief as 3–60 ms. These studies, which encompass areas from the tongue and lip areas of the primary motor cortex (M1) to high-level semantic regions such as the pMTG and ATL (Author response table 1), consistently demonstrate the methodological rigor and precision of double-pulse TMS in disentangling the neural dynamics of different regions within these short temporal windows.</p>
<table-wrap id="sa4table1">
<label>Author response table 1.</label>
<caption>
<title>Double-pulse TMS studies on brain regions over 3-60 ms time interval</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa4-table1.jpg" mimetype="image"/>
</table-wrap>
<p><bold>Response to Reviewer #1:</bold></p>
<disp-quote content-type="editor-comment">
<p>(1) For concern on the difficulty of disentangling the contributions of tightly-coupled brain regions to the speech-gesture integration process:</p>
</disp-quote>
<p>We trust that the explanation provided above has clarified this issue.</p>
<disp-quote content-type="editor-comment">
<p>(2) For concern on the rationale for delivering HD-tDCS/TMS in set time windows for each region, as well as how these time windows were determined and how the current results compare to our previous studies from 2018 and 2023:</p>
</disp-quote>
<p>The current study builds on a series of investigations that systematically examined the temporal and spatial dynamics of gesture-speech integration. In our earlier work (Zhao et al., 2018, J. Neurosci), we demonstrated that interrupting neural activity in the IFG or pMTG using TMS selectively disrupted the semantic congruency effect (reaction time costs due to semantic incongruence), without affecting the gender congruency effect (reaction time costs due to gender incongruence). These findings identified the IFG and pMTG as critical hubs for gesture-speech integration. This informed the brain regions selected for subsequent studies.</p>
<p>In Zhao et al. (2021, J. Neurosci), we employed a double-pulse TMS protocol, delivering stimulation within one of eight 40-ms time windows, to further examine the temporal involvement of the IFG and pMTG. The results revealed time-window-selective disruptions of the semantic congruency effect, confirming the dynamic and temporally staged roles of these regions during gesture-speech integration.</p>
<p>In Zhao et al. (2023, Frontiers in Psychology), we investigated the semantic predictive role of gestures relative to speech by comparing two experimental conditions: (1) gestures preceding speech by a fixed interval of 200 ms, and (2) gestures preceding speech at its semantic identification point. We observed time-window-selective disruptions of the semantic congruency effect in the IFG and pMTG only in the second condition, leading to the conclusion that gestures exert a semantic priming effect on co-occurring speech. These findings underscored the semantic advantage of gesture in facilitating speech integration, further refining our understanding of the temporal and functional interplay between these modalities.</p>
<p>The design of the current study—including the choice of brain regions and time windows—was directly informed by these prior findings. Experiment 1 (HD-tDCS) targeted the entire gesture-speech integration process in the IFG and pMTG to assess whether neural activity in these regions, previously identified as integration hubs, is modulated by changes in informativeness from both modalities (i.e., entropy) and their interactions (mutual information, MI). The results revealed a gradual inhibition of neural activity in both areas as MI increased, evidenced by a negative correlation between MI and the tDCS inhibition effect in both regions. Building on this, Experiments 2 and 3 employed double-pulse TMS and event-related potentials (ERPs) to further assess whether the engaged neural activity was both time-sensitive and staged. These experiments also evaluated the contributions of various sources of information, revealing correlations between information-theoretic metrics and time-locked brain activity, providing insights into the ‘gradual’ nature of gesture-speech integration.</p>
<p>We acknowledge that the rationale for the design of the current study was not fully articulated in the original manuscript. In the revised version, we will provide a more comprehensive and coherent explanation of the logic behind the three experiments, ensuring clear alignment with our previous findings.</p>
<disp-quote content-type="editor-comment">
<p>(3) For concern about the use of Pearson correlation and the normality of EEG data.</p>
</disp-quote>
<p>We appreciate the reviewer’s thoughtful consideration. In Figure 5 of the manuscript, we have already included normal distribution curves that illustrate the relationships between the average ERP amplitudes within each ROI or elicited clusters and the three information models. Additionally, multiple comparisons were addressed using FDR correction, as outlined in the manuscript.</p>
<p>To further clarify the data, we will calculate the Shapiro-Wilk test, a widely accepted method for assessing bivariate normality, for both the MI/entropy and averaged ERP data. The corresponding p-values will be provided in the following-up point-to-point responses.</p>
<disp-quote content-type="editor-comment">
<p>(4) For concern about the ROI selection, and the suggestion of using whole-brain electrodes to build models of different variables (MI/entropy) to predict neural responses:</p>
</disp-quote>
<p>For the EEG data, we conducted both a traditional region-of-interest (ROI) analysis, with ROIs defined based on a well-established work (Habets et al., 2011), and a cluster-based permutation approach, which utilizes data-driven permutations to enhance robustness and address multiple comparisons. The latter method complements the hypothesis-driven ROI analysis by offering an exploratory, unbiased perspective. Notably, the results from both approaches were consistent, reinforcing the reliability of our findings.</p>
<p>To make the methods more accessible to a broader audience, we will provide a clear description of the methods used and how they relate to each other in the revised manuscript.</p>
<p>Reference:</p>
<p>Habets, B., Kita, S., Shao, Z.S., Ozyurek, A., and Hagoort, P. (2011). The Role of Synchrony and Ambiguity in Speech-Gesture Integration during Comprehension. J Cognitive Neurosci 23, 1845-1854. 10.1162/jocn.2010.21462</p>
<disp-quote content-type="editor-comment">
<p>(5) For concern about the median split of the data:</p>
</disp-quote>
<p>To identify ERP components or spatiotemporal clusters that demonstrated significant semantic differences, we split each model into higher and lower halves, focusing on indexing information changes reflected by entropy or mutual information (MI). To illustrate the gradual activation process, the identified components and clusters were further analyzed for correlations with each information matrix. Remarkably, consistent results were observed between the ERP components and clusters, providing robust evidence that semantic information conveyed through gestures and speech significantly influenced the amplitude of these components or clusters. Moreover, the semantic information was shown to be highly sensitive, varying in tandem with these amplitude changes.</p>
<p>We acknowledge that the rationale behind this approach may not have been sufficiently clear in the initial manuscript. In our revision, we will ensure a more detailed and precise explanation to enhance the clarity and coherence of this logical framework.</p>
<p><bold>Response to Reviewer #2:</bold></p>
<p>We greatly appreciate Reviewer2 ’s concern regarding whether &quot;mutual information&quot; adequately captures the interplay between the meanings of speech and gesture. We would like to clarify that the materials used in the present study involved gestures performed without actual objects, paired with verbs that precisely describe the corresponding actions. For example, a hammering gesture was paired with the verb “hammer”, and a cutting gesture was paired with the verb “cut”. In this design, all gestures conveyed redundant meaning relative to the co-occurring speech, creating significant overlap between the information derived from speech alone and that from gesture alone.</p>
<p>We understand the reviewer’s concern about cases where gestures and speech may provide complementary rather than redundant information. To address this, we have developed an alternative metric for quantifying information gains contributed by supplementary multisensory cues, which will be explored in a subsequent study. However, for the present study, we believe that the observed overlap in information serves as an indicator of the degree of multisensory convergence, a central focus of our investigation.</p>
<p>Regarding the reviewer’s concern about how the neural processes of speech-gesture integration may change with variations in the relative timing between speech and gesture stimuli, we would like to highlight findings from our previous study (Zhao, 2023, Frontiers in Psychology). In that study, we explored the semantic predictive role of gestures relative to speech under two conditions: (1) gestures preceding speech by a fixed interval of 200 ms, and (2) gestures preceding speech of its semantic identification point. Interestingly, only in the second condition did we observe time-window-selective disruptions of the semantic congruency effect in the IFG and pMTG. This led us to conclude that gestures play a semantic priming role for co-occurring speech. Building on this, we designed the present study with gestures preceding speech of its semantic identification point to reflect this semantic priming relationship. Additionally, ongoing research is exploring gesture and speech interactions in natural conversational settings to investigate whether the neural processes identified here are consistent across varying contexts.</p>
<p>To prevent any similar concerns from causing doubt among the audience and to ensure clarity regarding the follow-up study, we will provide a detailed discussion of the two issues in the revised manuscript.</p>
<p><bold>Response to Reviewer #3:</bold></p>
<p>The primary aim of this study is to investigate whether the degree of activity in the established integration hubs, IFG and pMTG, is influenced by the information provided by gesture-speech modalities and/or their interactions. While we provided evidence for the differential involvement of the IFG and pMTG by delineating their dynamic engagement across distinct time windows of gesture-speech integration and associating these patterns with unisensory information and their interaction, we acknowledge that the mechanisms underlying these dynamics remain open to interpretation. Specifically, whether the observed effects stem from difficulties in semantic control processes, as suggested by Reviewer 3, or from resolving information uncertainty, as quantified by entropy, falls outside the scope of the current study. Importantly, we view these two interpretations as complementary rather than mutually exclusive, as both may be contributing factors. Nonetheless, we agree that addressing this question is a compelling avenue for future research. In the revised manuscript, we will include an exploratory analysis to investigate whether the confounding difficulty, stemming from the number of lexical or semantic representations, is limited to high-entropy items. Additionally, we will address and discuss alternative interpretations.</p>
<p>Regarding the concern of conceptual equivocation, we would like to emphasize that this study represents the first attempt to focus on the relationship between information quantity and neural engagement. In our initial presentation, we inadvertently conflated the commonly used term &quot;graded hub,&quot; which refers to anatomical distribution, with its usage in the present context. We sincerely apologize for this oversight and are grateful for the reviewer’s careful critique. In the revised manuscript, we will clearly articulate the study’s objectives, clarify the representations of entropy and mutual information, and accurately describe their association with neural engagement.</p>
<p>Reference</p>
<p>Teige, C., Mollo, G., Millman, R., Savill, N., Smallwood, J., Cornelissen, P. L., &amp; Jefferies, E. (2018). Dynamic semantic cognition: Characterising coherent and controlled conceptual retrieval through time using magnetoencephalography and chronometric transcranial magnetic stimulation. Cortex, 103, 329-349.</p>
<p>Amemiya, T., Beck, B., Walsh, V., Gomi, H., &amp; Haggard, P. (2017). Visual area V5/hMT+ contributes to perception of tactile motion direction: a TMS study. Scientific reports, 7(1), 40937.</p>
<p>Muessgens, D., Thirugnanasambandam, N., Shitara, H., Popa, T., &amp; Hallett, M. (2016). Dissociable roles of preSMA in motor sequence chunking and hand switching—a TMS study. Journal of Neurophysiology, 116(6), 2637-2646.</p>
<p>Vernet, M., Brem, A. K., Farzan, F., &amp; Pascual-Leone, A. (2015). Synchronous and opposite roles of the parietal and prefrontal cortices in bistable perception: a double-coil TMS–EEG study. Cortex, 64, 78-88.</p>
<p>Pitcher, D. (2014). Facial expression recognition takes longer in the posterior superior temporal sulcus than in the occipital face area. Journal of Neuroscience, 34(27), 9173-9177.</p>
<p>Bardi, L., Kanai, R., Mapelli, D., &amp; Walsh, V. (2012). TMS of the FEF interferes with spatial conflict. Journal of cognitive neuroscience, 24(6), 1305-1313.</p>
<p>D’Ausilio, A., Bufalari, I., Salmas, P., &amp; Fadiga, L. (2012). The role of the motor system in discriminating normal and degraded speech sounds. Cortex, 48(7), 882-887.</p>
<p>Pitcher, D., Duchaine, B., Walsh, V., &amp; Kanwisher, N. (2010). TMS evidence for feedforward and feedback mechanisms of face and body perception. Journal of Vision, 10(7), 671-671.</p>
<p>Gagnon, G., Blanchet, S., Grondin, S., &amp; Schneider, C. (2010). Paired-pulse transcranial magnetic stimulation over the dorsolateral prefrontal cortex interferes with episodic encoding and retrieval for both verbal and non-verbal materials. Brain Research, 1344, 148-158.</p>
<p>Kalla, R., Muggleton, N. G., Juan, C. H., Cowey, A., &amp; Walsh, V. (2008). The timing of the involvement of the frontal eye fields and posterior parietal cortex in visual search. Neuroreport, 19(10), 1067-1071.</p>
<p>Pitcher, D., Garrido, L., Walsh, V., &amp; Duchaine, B. C. (2008). Transcranial magnetic stimulation disrupts the perception and embodiment of facial expressions. Journal of Neuroscience, 28(36), 8929-8933.</p>
</body>
</sub-article>
</article>