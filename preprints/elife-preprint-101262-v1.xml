<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">101262</article-id>
<article-id pub-id-type="doi">10.7554/eLife.101262</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.101262.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Prediction tendency, eye movements, and attention in a unified framework of neural speech tracking</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2536-6522</contrib-id>
<name>
<surname>Schubert</surname>
<given-names>Juliane</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
<email>juliane.schubert@plus.ac.at</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9407-2763</contrib-id>
<name>
<surname>Gehmacher</surname>
<given-names>Quirin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9839-1614</contrib-id>
<name>
<surname>Schmidt</surname>
<given-names>Fabian</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8298-8125</contrib-id>
<name>
<surname>Hartmann</surname>
<given-names>Thomas</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7816-0037</contrib-id>
<name>
<surname>Weisz</surname>
<given-names>Nathan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Paris-Lodron-University of Salzburg, Department of Psychology, Centre for Cognitive Neuroscience</institution>, Salzburg, <country>Austria</country></aff>
<aff id="a2"><label>2</label><institution>Neuroscience Institute, Christian Doppler University Hospital, Paracelsus Medical University</institution>, Salzburg, <country>Austria</country></aff>
<aff id="a3"><label>3</label><institution>Department of Experimental Psychology, University College London</institution>, <country>United Kingdom</country></aff>
<aff id="a4"><label>4</label><institution>Wellcome Centre for Human Neuroimaging, University College London</institution>, <country>United Kingdom</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Bottini</surname>
<given-names>Roberto</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Trento</institution>
</institution-wrap>
<city>Trento</city>
<country>Italy</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes><fn id="n1" fn-type="equal"><label>*</label><p>contributed equally</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-10-14">
<day>14</day>
<month>10</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP101262</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-08-13">
<day>13</day>
<month>08</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-08-27">
<day>27</day>
<month>08</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.06.27.546746"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Schubert et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Schubert et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-101262-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Auditory speech comprehension is a multi-faceted process in which attention, prediction, and sensorimotor integration (via active sensing) interact with or complement each other. Although different conceptual models that focus on one of these aspects exist, we still lack a unified understanding of their role in speech processing. Here, we first replicated two recently published studies from our lab, confirming 1) a positive relationship between individual prediction tendencies and neural speech tracking, and 2) the phenomenon of ocular speech tracking - the tracking of attended speech by eye movements - and its shared contribution with neural activity to speech processing. In addition, we extended these findings with complementary analyses and investigated these phenomena in relation to each other in a multi-speaker paradigm with continuous, narrative speech. Importantly, prediction tendency and ocular speech tracking seem to be unrelated. In contrast to the shared contributions of oculomotor and neural activity to speech processing over a distributed set of brain regions that are critical for attention, individual prediction tendency and its relation to neural speech tracking seem to be largely independent of attention. Based on these findings, we propose a framework that aims to bridge the gaps between attention, prediction, and active (ocular) sensing in order to contribute to a holistic understanding of neural speech processing. In this speculative framework for listening, auditory inflow is, on a basic level, temporally modulated via active ocular sensing, and incoming information is interpreted based on probabilistic assumptions.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>active auditory sensing</kwd>
<kwd>speech processing</kwd>
<kwd>predictive processing</kwd>
<kwd>selective attention</kwd>
<kwd>eye movements</kwd>
<kwd>magnetoencephalography</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Acknowledgements statement was updated to comply with the new funding guidelines</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Listening challenges the brain to infer meaning from vastly overlapping spectrotemporal information. For example, in complex, everyday environments, we need to select and segregate streams of speech from different speakers for further processing. To accomplish this task, (mainly independent) research lines suggested contributions of predictive and attentional processes to speech perception.</p>
<p>Predictive brain accounts (<xref ref-type="bibr" rid="c13">K. Friston, 2010</xref>; <xref ref-type="bibr" rid="c14">K. J. Friston et al., 2021</xref>; <xref ref-type="bibr" rid="c20">Knill &amp; Pouget, 2004</xref>; <xref ref-type="bibr" rid="c43">Yon et al., 2019</xref>) suggest an active engagement in speech perception. In this view, experience-based internal models constantly generate and continuously compare top-down predictions with bottom-up input, thus inferring sound sources from neural activity patterns. This idea is supported by the influential role of speech predictability (e.g. semantic context and word surprisal) on speech processing in naturalistic contexts (<xref ref-type="bibr" rid="c3">Broderick et al., 2019</xref>; <xref ref-type="bibr" rid="c10">Donhauser &amp; Baillet, 2020</xref>; <xref ref-type="bibr" rid="c41">Weissbart et al., 2020</xref>).</p>
<p>Selective attention describes the process by which the brain prioritises information to focus our limited cognitive capacities on relevant inputs while ignoring distracting, irrelevant ones. Its beneficial role in the processing of complex acoustic scenes, like the cocktail-party scenario (<xref ref-type="bibr" rid="c44">Zion Golumbic et al., 2013</xref>; <xref ref-type="bibr" rid="c45">Zion-Golumbic &amp; Schroeder, 2012</xref>), supports the key role of attentional and predictive processes in natural listening. It is important to note that even “normal hearing” individuals vary greatly in everyday speech comprehension and communication (<xref ref-type="bibr" rid="c30">Ruggles et al., 2011</xref>), which consequently promotes interindividual variability in predictive processes (<xref ref-type="bibr" rid="c35">Siegelman &amp; Frost, 2015</xref>) or selective attention (<xref ref-type="bibr" rid="c28">Oberfeld &amp; Klöckner-Nowotny, 2016</xref>) as underlying modulators.</p>
<p>In a recent study (<xref ref-type="bibr" rid="c34">Schubert et al., 2023</xref>), we quantified individual differences in predictive processes as the tendency to anticipate low-level acoustic features (i.e. prediction tendency). We established a positive relationship of this “trait” with speech processing, demonstrating that speech tracking is enhanced in individuals with stronger prediction tendency, independent of selective attention. Furthermore, we found an increased tracking of words of high surprisal, demonstrating the importance of predictive processes in speech perception.</p>
<p>Another important aspect with regard to speech processing - that has been vastly overlooked so far in neuroscience - is active auditory sensing. The potential benefit of active sensing - the engagement of covert motor routines in acquiring sensory inputs - was outlined for other sensory modalities (<xref ref-type="bibr" rid="c33">Schroeder et al., 2010</xref>). In the auditory domain, and especially for speech perception, motor contributions to an increased weighting of temporal precision were suggested to modulate auditory processing gain in support of (speech) segmentation (<xref ref-type="bibr" rid="c26">Morillon et al., 2015</xref>). Along these lines, It has been shown that covert, mostly blink related eye activity aligns with higher-order syntactic structures of temporally predictable, artificial speech (i.e. monosyllabic words). In support of ideas that the motor system is actively engaged in speech perception (<xref ref-type="bibr" rid="c15">Galantucci et al., 2006</xref>; <xref ref-type="bibr" rid="c23">Liberman &amp; Mattingly, 1985</xref>), the authors suggest a global entrainment across sensory and (oculo)motor areas which implements temporal attention.</p>
<p>In another recent study from our lab (<xref ref-type="bibr" rid="c16">Gehmacher et al., 2024</xref>), we showed that eye movements continuously track intensity fluctuations of attended natural speech, a phenomenon we termed ocular speech tracking. We further established links to increased intelligibility with stronger ocular tracking and provided evidence that eye movements and neural activity share contributions to speech tracking. Considering the aforementioned study by <xref ref-type="bibr" rid="c34">Schubert and colleagues (2023)</xref>, the two recently uncovered predictors of neural tracking (individual prediction tendency and ocular tracking) raise several questions regarding their potential relationship with speech processing: Are predictive processes similarly related to active ocular sensing as to neural tracking in natural listening, i.e. are stronger prediction tendencies related to increased ocular speech tracking? Or vice versa? Or not at all? To what extent is their relationship to neural speech tracking affected by selective attention? Are they related to behavioural outcomes? While all these concepts have been shown to contribute to successful listening in challenging environments, a thorough investigation of potential interactions and their consequences on speech perception has been lacking.</p>
<p>Here, we set out to answer these questions by a) replicating aforementioned findings in a single experiment, and b) combining prediction tendency, eye movements, and attention into a unified framework of neural speech tracking. We therefore repeated the study protocol of <xref ref-type="bibr" rid="c34">Schubert et al. (2023)</xref> with slight modifications: Again, participants performed a separate, passive listening paradigm (also see (<xref ref-type="bibr" rid="c9">Demarchi et al., 2019</xref>)) that allowed us to quantify individual prediction tendency. Afterward, they listened to sequences of audiobooks (using the same stimulus material), either in a clear speech (0 distractors) or a multi-speaker (1 distractor) condition. Simultaneously recorded magnetoencephalographic (MEG) and Eye Tracking data confirmed the previous findings of <xref ref-type="bibr" rid="c34">Schubert et al. (2023)</xref> and <xref ref-type="bibr" rid="c16">Gehmacher et al. (2024)</xref>: 1) Individuals with a stronger prediction tendency showed an increased neural speech tracking over left frontal areas, 2) eye movements track acoustic speech in selective attention, and 3) further mediate neural speech tracking effects over widespread, but mostly auditory regions. Additionally, we found an increased neural tracking of semantic violations (compared to their lexically identical controls), indicating that surprisal evoked responses indeed encode information about the stimulus. Interestingly, we could not find this difference in semantic processing for ocular speech tracking. Finally, we behaviorally assessed speech comprehension by probing participants on story content. Responses indicate that weaker performance in comprehension was related to increased ocular speech tracking while we did not find a significant relation to neural speech tracking. The findings suggest a differential role of prediction tendency, eye movements, and attention in speech processing. Behavioural responses further indicate substantial differences in ocular and neural engagement and perceptual outcomes. Based on these findings, we propose a unified framework of neural speech tracking where anticipatory predictions support the interpretation of auditory input along the perceptual hierarchy while active ocular sensing increases the temporal precision of peripheral auditory responses to facilitate bottom-up processing of selectively attended input.</p>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>Subjects</title>
<p>In total, 39 subjects were recruited to participate in the experiment. For 3 participants, eye tracker calibration failed and they had to abort the experiment. We further controlled for excessive blinking (&gt; 50% of data samples) and eye movements away from fixation cross (&gt; 50% of data samples exceeded ⅓ of screen size on the horizontal or vertical plane) that suggest a lack of commitment to the experiment instructions. 7 subjects had to be excluded according to these criteria. Thus, a final sample size of 29 subjects (12 female, 17 male; mean age = 25.70, range = 19 - 43) was used for the analysis of brain and gaze data.</p>
<p>All participants reported normal hearing and had normal, or corrected to normal, vision. They gave written, informed consent and reported that they had no previous neurological or psychiatric disorders. The experimental procedure was approved by the ethics committee of the University of Salzburg and was carried out in accordance with the declaration of Helsinki. All participants received either a reimbursement of 10 € per hour or course credits.</p>
</sec>
<sec id="s2b">
<title>Experimental Procedure</title>
<p>The current study is a replication of a previous experiment (for details see <xref ref-type="bibr" rid="c34">Schubert et al., 2023</xref>), using the same experimental structure and the same auditory stimuli. The current design only differs in that one condition was dropped and the whole experiment was shortened (participants spent approximately 2 hours in the MEG including preparation time). In addition to the previous study, this time we recorded ocular movements (also see Data acquisition and Preprocessing). Before the start of the experiment, participants’ head shapes were assessed using cardinal head points (nasion and pre-auricular points), digitised with a Polhemus Fastrak Digitiser (Polhemus), and around 300 points on the scalp. For every participant, MEG sessions started with a 5-minute resting-state recording, after which the individual hearing threshold was determined using a pure tone of 1043 Hz. This was followed by 2 blocks of passive listening to tone sequences of varying entropy levels to quantify individual prediction tendencies (see Quantification of individual prediction tendency and <xref rid="fig1" ref-type="fig">Figure 1A-C</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Quantification of individual prediction tendency and the multi-speaker paradigm</title>
<p><bold>A)</bold> Participants passively listened to sequences of pure tones in different conditions of entropy (ordered vs. random). Four tones of different fundamental frequencies were presented with a fixed stimulation rate of 3 Hz, their transitional probabilities varied according to respective conditions. <bold>B)</bold> Expected classifier decision values contrasting the brains’ prestimulus tendency to predict a forward transition (ordered vs. random). The purple shaded area represents values that were considered as prediction tendency <bold>C)</bold> Exemplary excerpt of a tone sequence in the ordered condition. An LDA classifier was trained on forward transition trials of the ordered condition (75% probability) and tested on all repetition trials to decode sound frequency from brain activity across time. <bold>D)</bold> Participants either attended to a story in clear speech, i.e. 0 distractor condition, or to a target speaker with a simultaneously presented distractor (blue), i.e. 1 distractor condition. <bold>E)</bold> The speech envelope was used to estimate neural and ocular speech tracking in respective conditions with temporal response functions (TRF). <bold>F)</bold> The last noun of some sentences was replaced randomly with an improbable candidate to measure the effect of envelope encoding on the processing of semantic violations. Adapted from <xref ref-type="bibr" rid="c34">Schubert et al., 2023</xref>.</p></caption>
<graphic xlink:href="546746v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Participants were instructed to look at a black fixation cross at the centre of a grey screen. In the main task, 4 different stories were presented in separate blocks in random order and with randomly balanced selection of the target speaker (male vs. female voice). Each block consisted of 2 trials with a continuous storyline, with each trial corresponding to one of 2 experimental conditions: a single speaker and a multi-speaker condition (see also <xref rid="fig1" ref-type="fig">Figure 1D</xref>). The distractor speaker was always of the opposite sex of the target speaker (and was identical to the target speaker in a different run). Distracting speech was presented exactly 20 s after target speaker onset, and all stimuli were presented binaurally at equal volume (40db above individual hearing threshold) for the left and right ear (i.e. at phantom centre). Participants were instructed to attend to the first speaker and their understanding was tested using comprehension questions (true vs. false statements) at the end of each trial (e.g.: “Das Haus, in dem Sofie lebt, ist rot” <italic>(The house Sofie lives in is red)</italic>, “Ein gutes Beispiel für unterschiedliche Dialekte sind die Inuit aus Alaska und Grönland” <italic>(A good example of different dialects are the Inuit from Alaska and Greenland)</italic>…). Furthermore, participants indicated their task engagement and their perceived task-difficulty on a 5-point Likert scale at the end of every trial. During the audiobook presentation, participants were again instructed to look at the fixation-cross on the screen and to blink as little as (still comfortably) possible. The experiment was coded and conducted with the Psychtoolbox-3 (<xref ref-type="bibr" rid="c1">Brainard &amp; Vision, 1997</xref>; <xref ref-type="bibr" rid="c19">Kleiner et al., 2007</xref>), with an additional class-based library (‘Objective Psychophysics Toolbox’, o_ptb) on top of it (<xref ref-type="bibr" rid="c17">Hartmann &amp; Weisz, 2020</xref>).</p>
</sec>
<sec id="s2c">
<title>Stimuli</title>
<p>We used the same stimulus material as in <xref ref-type="bibr" rid="c34">Schubert et al. (2023)</xref>, recorded with a t.bone SC 400 studio microphone at a sampling rate of 44100 Hz. In total, we used material from 4 different, consistent stories (see <bold>Supplementary Table 1</bold>). These stories were split into 3 separate trials of approximately 3 - 4 min. The first two parts of each story were always narrated by a target speaker, whereas the last part served as distractor material. Additionally, we randomly selected half of the nouns that ended a sentence and replaced them with the other half to induce unexpected semantic violations within each trial, resulting in two sets of lexically identical words (<italic>N</italic> = 79) that differed greatly in their contextual probabilities (see <xref rid="fig1" ref-type="fig">Figure 1F</xref> for an example). All trials were recorded twice, narrated by a different speaker (male vs. female). Stimuli were presented in 4 blocks containing 2 trials each (a single and a multi-speaker trial), resulting in 2 male and 2 female target speaker blocks for every participant.</p>
</sec>
<sec id="s2d">
<title>Data Acquisition and Preprocessing</title>
<p>Brain activity was recorded using a whole head MEG system (Elekta Neuromag Triux, Elekta Oy, Finland), placed within a standard passive magnetically shielded room (AK3b, Vacuumschmelze, Germany). We used a sampling frequency of 1 kHz (hardware filters: 0.1 - 330 Hz). The signal was recorded with 102 magnetometers and 204 orthogonally placed planar gradiometers at 102 different positions. In a first step, a signal space separation algorithm, implemented in the Maxfilter program (version 2.2.15) provided by the MEG manufacturer, was used to clean the data from external noise and realign data from different blocks to a common standard head position. Data preprocessing was performed using Matlab R2020b (The MathWorks, Natick, Massachusetts, USA) and the FieldTrip Toolbox (<xref ref-type="bibr" rid="c29">Oostenveld et al., 2011</xref>). All data was filtered between 0.1 Hz and 30 Hz (Kaiser windowed finite impulse response filter) and downsampled to 100 Hz. To identify eye-blinks and heart rate artefacts, 50 independent components were identified from filtered (0.1 - 100 Hz), downsampled (1000 Hz) continuous data of the recordings from the entropy modulation paradigm, and on average 3 components were removed for every subject. All data was filtered between 0.1 Hz and 30 Hz (Kaiser windowed finite impulse response filter) and downsampled to 100 Hz. Data of the entropy modulation paradigm was epoched into segments of 1200 ms (from 400 ms before sound onset to 800 ms after onset). Multivariate pattern analysis (see quantification of individual prediction tendency) was carried out using the MVPA-Light package (<xref ref-type="bibr" rid="c38">Treder, 2020</xref>). Data of the listening task was temporally aligned with the corresponding speech envelope, which was extracted from the audio files using the Chimera toolbox (<xref ref-type="bibr" rid="c36">Smith et al., 2002</xref>) over a broadband frequency range of 100 Hz - 10 kHz (in 9 steps, equidistant on the tonotopic map of auditory cortex; see also <xref rid="fig1" ref-type="fig">Figure 1E</xref>).</p>
<p>Eye tracking data from both eyes were acquired using a Trackpixx3 binocular tracking system (Vpixx Technologies, Canada) at a sampling rate of 2 kHz with a 50 mm lens. Participants were seated in the MEG at a distance of 82 cm from the screen. Their chin rested on a chinrest to reduce head movements. Each experimental block started with a 13-point calibration / validation procedure that was used throughout the block. Blinks were automatically detected by the Trackpixx3 system and excluded from horizontal and vertical eye movement data. We additionally excluded 100 ms of data around blinks to control for additional blink artefacts that were not automatically detected by the eye tracker. We then averaged gaze position data from the left and right eye to increase the accuracy of gaze estimation (<xref ref-type="bibr" rid="c7">Cui &amp; Hondzinski, 2006</xref>). Missing data due to blink removal were then interpolated with a piecewise cubic Hermite interpolation. Afterwards, data was imported into the FieldTrip Toolbox, bandpass filtered between 0.1 - 40 Hz (zero-phase finite impulse response (FIR) filter, order: 33000, hamming window), and cut to the length of respective speech segments of a block. Data were then downsampled to 1000 Hz to match the sampling frequency of neural and speech envelope data and further corrected for a 16 ms delay between trigger onset and actual stimulation. Finally, gaze data was temporally aligned with the corresponding speech envelope and downsampled to 100 Hz for TRF analysis after an antialiasing low-pass filter at 20 Hz was applied (zero-phase FIR filter, order: 662, hamming window).</p>
</sec>
<sec id="s2e">
<title>Quantification of Prediction Tendency</title>
<p>The quantification of individual prediction tendency was the same as in <xref ref-type="bibr" rid="c34">Schubert et al. (2023)</xref>: We used an entropy modulation paradigm where participants passively listened to sequences of 4 different pure tones (f1: 440 Hz, f2: 587 Hz, f3: 782 Hz, f4: 1043 Hz, each lasting 100 ms) during two separate blocks, each consisting of 1500 tones presented with a temporally predictable rate of 3 Hz. Entropy levels (ordered / random) changed pseudorandomly every 500 trials within each block, always resulting in a total of 1500 trials per entropy condition. While in an “ordered” context certain transitions (hereinafter referred to as forward transitions, i.e. f1→f2, f2→f3, f3→f4, f4→f1) were to be expected with a high probability of 75%, self repetitions (e.g., f1→f1, f2→f2,…) were rather unlikely with a probability of 25%. However, in a “random” context all possible transitions (including forward transitions and self repetitions) were equally likely with a probability of 25% (see <xref rid="fig1" ref-type="fig">Figure 1A</xref>). To estimate the extent to which individuals anticipate auditory features (i.e. sound frequencies) according to their contextual probabilities, we used a multiclass linear discriminant analyser (LDA) to decode sound frequency (f1 - f4) from brain activity (using data from the 102 magnetometers) between −0.3 and 0 s in a time-resolved manner. Based on the resulting classifier decision values (i.e. d1 - d4 for every test-trial and time-point), we calculated individual prediction tendency. We define individual prediction tendency as the tendency to pre-activate sound frequencies of high probability (i.e. a forward transition from one stimulus to another: f1→f2, f2→f3, f3→f4, f4→f1). In order to capture any prediction-related neural activity, we trained the classifier exclusively on ordered forward trials (see <xref rid="fig1" ref-type="fig">Figure 1B</xref>). Afterwards, the classifier was tested on all self-repetition trials, providing classifier decision values for every possible sound frequency, which were then transformed into corresponding transitions (e.g. d1(t) | f1(t-1) “dval for 1 at trial t, given that 1 was presented at trial t-1” → repetition, d2(t) | f1(t-1) → forward,…). The tendency to represent a forward vs. repetition transition was contrasted for both ordered and random trials (see <xref rid="fig1" ref-type="fig">Figure 1C</xref>). Using self-repetition trials for testing, we ensured a fair comparison between the ordered and random contexts (with an equal number of trials and the same preceding bottom-up input). Thus, we quantified “prediction tendency” as the classifier’s pre-stimulus tendency to a forward transition in an ordered context exceeding the same tendency in a random context (which can be attributed to carry-over processing of the preceding stimulus). Then, using the summed difference across pre-stimulus times, one value can be extracted per subject (also see <xref rid="fig1" ref-type="fig">Figure 1B</xref>).</p>
</sec>
<sec id="s2f">
<title>Encoding Models</title>
<p>To quantify the neural representations corresponding to the acoustic envelope, we calculated a multivariate temporal response function (TRF) using the Eelbrain toolkit (<xref ref-type="bibr" rid="c2">Brodbeck et al., 2023</xref>). A deconvolution algorithm (boosting; (<xref ref-type="bibr" rid="c8">David et al., 2007</xref>)) was applied to the concatenated trials to estimate the optimal TRF to predict the brain response from the speech envelope, separately for each condition (single vs. multi-speaker). Before model fitting, MEG data of 102 magnetometers were normalised by subtracting the mean and dividing by the standard deviation (i.e. z-scoring) across all channels (as recommended by (<xref ref-type="bibr" rid="c5">Crosse et al., 2016</xref>, <xref ref-type="bibr" rid="c6">2021</xref>)). Similarly, the speech envelope was also z-scored, however, after the transformation the negative of the minimum value (which naturally would be zero) was added to the time-series to retain zero values (z’ = z+(min(z)*-1). The defined time-lags to train the model were from −0.4 s to 0.8 s. To evaluate the model, the data was split into 4 folds, and a cross-validation approach was used to avoid overfitting (<xref ref-type="bibr" rid="c42">Ying, 2019</xref>). The resulting predicted channel responses (for all 102 magnetometers) were then correlated with the true channel responses to quantify the model fit and the degree of speech envelope tracking at a particular sensor location.</p>
<p>To investigate the effect of semantic violations, the true as well as the predicted data was segmented into single word epochs of 2 seconds starting at word onset (using a forced-aligner; (<xref ref-type="bibr" rid="c18">Kisler et al., 2017</xref>; <xref ref-type="bibr" rid="c32">Schiel &amp; Ohala, 1999</xref>)). We selected semantic violations as well as their lexically identical controls and correlated true with predicted responses for every word. We then averaged the result within each condition (i.e. single vs. multi-speaker) and word type (i.e. high vs. low suprisal).</p>
<p>The same TRF approach was also used to estimate ocular speech tracking, separately predicting eye movements in the horizontal and vertical direction using the same time-lags (from −0.4 s to 0.8 s). The same z-scoring was applied to the speech envelope. However, horizontal and vertical eye channel responses were normalised within channels.</p>
</sec>
<sec id="s2g">
<title>Mediation Analysis</title>
<p>To investigate the contribution of eye movements to neural speech tracking, we approached a mediation analysis similar to <xref ref-type="bibr" rid="c16">Gehmacher et al. (2024)</xref>. The TRFs that we obtained from these encoding models can be interpreted as time-resolved weights for a predictor variable that aims to explain a dependent variable (very similar to beta-coefficients in classic regression analyses). We simply compared the plain effect of the speech envelope on neural activity to its direct (residual) effect by including an indirect effect via eye movements into our model. In order to account for a reduction in speech envelope weights simply due to the inclusion of this additional (eye-movement) predictor, we obtained a control model by including a time-shuffled version of the eye-movement predictor in addition to the unchanged speech envelope. Thus, the plain effect (i.e. speech envelope predicting neural responses) is represented in the absolute weights (i.e. TRFs) obtained from this control model with the speech envelope and shuffled eye movement data as the predictor of neural activity. The direct (residual) effect (not mediated by eye movements) is obtained from the model including the speech envelope as well as true eye movements and is represented in the exclusive weights (c’) of the former predictor (i.e. speech envelope). If model weights are significantly reduced by the inclusion of true eye movements into the model in comparison to a model with a time-shuffled version of the same predictor (i.e. c’ &lt; c), this indicates that a meaningful part of the relationship between the speech envelope and neural responses was mediated by eye movements (for further details see also <xref ref-type="bibr" rid="c16">Gehmacher et al., 2024</xref>).</p>
</sec>
<sec id="s2h">
<title>Source and Principal Component Analysis</title>
<p>In order to estimate the location along with the temporal profile of this mediation effect and at the same time minimise the number of comparisons, we computed the main components of the effect, projected into source space. For this, we used all 306 MEG channels for our models as described in the previous section (note that here MEG responses were z-scored within channel type, i.e. within magnetometers and gradiometers separately). The resulting envelope model weights were then projected into source-space using an LCMV beamforming approach (<xref ref-type="bibr" rid="c39">Van Veen et al., 1997</xref>). Spatial filters were computed by warping anatomical template images to the individual head shape and further brought into a common space by co-registering them based on the three anatomical landmarks (nasion, left and right preauricular points) with a standard brain from the Montreal Neurological Institute (MNI, Montreal, Canada; (<xref ref-type="bibr" rid="c25">Mattout et al., 2007</xref>)). For each participant, a single-shell head model (<xref ref-type="bibr" rid="c27">Nolte, 2003</xref>) was computed. Finally, as a source model, a grid with 1 cm resolution and 2982 voxels based on an MNI template brain was morphed into the brain volume of each participant. This allowed group-level averaging and statistical analysis as all the grid points in the warped grid belong to the same region across subjects.</p>
<p>Afterwards, envelope TRF edges of both plain and direct models were cropped to time-lags from −0.3 - 0.7 s to exclude potential regression artefacts. We then subtracted the absolute direct from the absolute plain TRF to obtain the ‘abs’ indirect (mediation) effect. We transformed the 2982 voxel space into an orthogonal component space with a principal component analysis (PCA) based on the grand average mediation effect. The number of components for further analysis was visually determined by plotting the ranked cumulative explained variance and estimating the ‘elbow’. Based on this inspection, we extracted weight matrices of the first three components and multiplied individual ‘abs’ plain TRFs (from the control model with time-shuffled eye movements) and ‘abs’ direct TRFs (from the test model with true eye-movements) with this weight matrix to obtain individual source location and temporal profile of the mediation components. The projected, single subject data was then used for statistical analysis.</p>
</sec>
<sec id="s2i">
<title>Statistical Analysis and Bayesian Models</title>
<p>To calculate the statistics, we used Bayesian multilevel regression models with Bambi (<xref ref-type="bibr" rid="c4">Capretto et al., 2022</xref>), a python package built on top of the PyMC3 package (<xref ref-type="bibr" rid="c31">Salvatier et al., 2016</xref>), for probabilistic programming. First, in replication of <xref ref-type="bibr" rid="c34">Schubert et al. (2023)</xref>, we investigated the effect of neural speech tracking and its relation to individual prediction tendency as well as the influence of increasing noise (i.e. adding a distractor speaker). Separate models were calculated for all 102 magnetometers using the following model formula:
<disp-formula id="ueqn1">
<graphic xlink:href="546746v3_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
(Note that “speech envelope tracking” refers to the correlation between predicted and true responses from the aforementioned encoding model and prediction tendency was always z-scored before entering the models). Similarly, we investigated the effect of ocular speech tracking under different conditions of attention (i.e. attended single speaker, attended multi-speaker and unattended multi-speaker). In addition to replicating the findings from <xref ref-type="bibr" rid="c16">Gehmacher et al. (2024)</xref>, we extended this analysis for a detailed investigation of horizontal and vertical ocular speech envelope tracking and further included prediction tendency as a predictor:
<disp-formula id="ueqn2">
<graphic xlink:href="546746v3_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
To investigate the effect of semantic violations, we compared envelope tracking between target words (high surprisal) and lexically matched controls (low surprisal), both for neural as well as ocular speech tracking:
<disp-formula id="ueqn3">
<graphic xlink:href="546746v3_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
For the mediation analysis, we compared weights (TRFs) for the speech envelope between a plain (control) model that included shuffled eye-movements as second predictor and a residual (test) model that included true eye movements as a second predictor (i.e. c’ &lt; c). In order to investigate the temporal dynamics of the mediation, we included time-lags (−0.3 - 0.7 s) as a fixed effect into the model. To investigate potential null effects (neural speech tracking that is definitely independent of eye movements), we used a region of practical equivalence (ROPE) approach (see for example <xref ref-type="bibr" rid="c21">Kruschke, 2018</xref>). The dependent variable (absolute weights) was z-scored across time-lags and models to get standardised betas for the mediation effect (i.e. c’ &lt; c):
<disp-formula id="ueqn4">
<graphic xlink:href="546746v3_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
As suggested by (<xref ref-type="bibr" rid="c21">Kruschke, 2018</xref>), a null effect was considered for betas ranging between - 0.1 and 0.1. Accordingly, it was considered a significant mediation effect if betas were above 0.1 and at minimum two neighbouring time-points also showed a significant result.</p>
<p>Finally, we investigated the relationship between neural as well as ocular speech tracking and behavioural data using the averaged accuracy from the questions on story content that were asked at the end of each trial (in the following: “comprehension”) and averaged subjective ratings of difficulty. In order to avoid calculating separate models for all magnetometers again, we selected 10% of the channels that showed the strongest speech encoding effect and used the averaged speech tracking (z-scored within condition before entering the model) as predictor:
<disp-formula id="ueqn5">
<graphic xlink:href="546746v3_ueqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
To investigate the relation between ocular speech tracking and behavioural performance, we used the following model (again speech tracking was z-scored within condition) separately for horizontal and vertical gaze direction:
<disp-formula id="ueqn6">
<graphic xlink:href="546746v3_ueqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
For all models (as in <xref ref-type="bibr" rid="c16">Gehmacher et al, 2024</xref> and <xref ref-type="bibr" rid="c34">Schubert et al., 2023</xref>), we used the weakly- or non-informative default priors of Bambi (<xref ref-type="bibr" rid="c4">Capretto et al., 2022</xref>) and specified a more robust Student-T response distributions instead of the default gaussian distribution. To summarise model parameters, we report regression coefficients and the 94% high density intervals (HDI) of the posterior distribution (the default HDI in Bambi). Given the evidence provided by the data, the prior and the model assumptions, we can conclude from the HDIs that there is a 94% probability that a respective parameter falls within this interval. We considered effects as significantly different from zero if the 94%HDI did not include zero (with the exception of the mediation analysis where the 94%HDI had to fall above the ROPE). Furthermore, we ensured the absence of divergent transitions (r^ &lt; 1.05 for all relevant parameters) and an effective sample size &gt; 400 for all models (an exhaustive summary of Bayesian model diagnostics can be found in (<xref ref-type="bibr" rid="c40">Vehtari et al., 2021</xref>)). Finally, when we estimated an effect on brain sensor level (using all 102 magnetometers), we defined clusters for which an effect was only considered as significant if at minimum two neighbouring channels also showed a significant result.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Individual prediction tendency is related to neural speech tracking</title>
<p>In the first step, we wanted to investigate the relationship between individual prediction tendency and neural speech tracking under different conditions of noise. We quantified prediction tendency as the individual tendency to represent auditory features (i.e. pure tone frequency) of high probability in advance of an event (i.e. pure tone onset). Importantly, this prediction tendency was assessed in an independent entropy modulation paradigm (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>). Replicating previous findings (<xref ref-type="bibr" rid="c34">Schubert et al., 2023</xref>), we found widespread encoding of clear speech, predominantly over auditory processing regions (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>), that was decreased in a multi-speaker condition (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>). Furthermore, a stronger prediction tendency was associated with increased neural speech tracking over left frontal sensors (see <xref rid="fig2" ref-type="fig">Fig. 2C</xref>). We found no interaction between prediction tendency and condition (see <xref rid="fig2" ref-type="fig">Fig. 2D</xref>). These findings indicate that the relationship between individual prediction tendency and neural speech tracking is largely unaffected by demands on selective attention.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Neural speech tracking is related to prediction tendency and word surprisal independent of selective attention</title>
<p><bold>A)</bold> In a single speaker condition, neural tracking of the speech envelope was significant for widespread areas, most pronounced over auditory processing regions. <bold>B)</bold> The condition effect indicates a decrease of neural speech tracking with increasing noise (1 distractor). <bold>C)</bold> Stronger prediction tendency was associated with increased neural speech tracking over left frontal areas. <bold>D)</bold> However, there was no interaction between prediction tendency and conditions of selective attention. <bold>E)</bold> There was an increased neural tracking of semantic violations over left temporal areas. <bold>F)</bold> There was no interaction between word surprisal and speaker condition, suggestive of a representation of surprising words independent of background noise. Statistics were performed using Bayesian regression models. Marked sensors show ‘significant’ clusters where at minimum two neighbouring channels showed a significant result. <italic>N</italic> = 29.</p></caption>
<graphic xlink:href="546746v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Additionally, we wanted to investigate how semantic violations affect neural speech tracking. For this reason, we introduced rare words of high surprisal into the story by randomly replacing half of the nouns at the end of a sentence with the other half. In a direct comparison with lexically identical controls, we found an increased neural tracking of semantic violations over left temporal areas (see <xref rid="fig2" ref-type="fig">Fig. 2E</xref>). Furthermore, we found no interaction between word surprisal and speaker condition (see <xref rid="fig2" ref-type="fig">Fig. 2F</xref>). These findings indicate an increased representation of surprising words independent of background noise.</p>
<p>In sum, we found that individual prediction tendency as well as semantic predictability affect neural speech tracking.</p>
</sec>
<sec id="s3b">
<title>Eye movements track acoustic speech in selective attention</title>
<p>In a second step, we aimed to replicate previous findings from <xref ref-type="bibr" rid="c16">Gehmacher and colleagues (2024)</xref>, showing that eye movements track the acoustic features of speech in absence of visual information. For this, we separately predicted horizontal and vertical eye movements from the acoustic speech envelope. For vertical eye movements, we found evidence for attended speech tracking in a single speaker condition (<italic>β</italic> = 0.012, 94%HDI = [0.001, 0.0023]) but not in a multi-speaker condition (<italic>β</italic> = 0.006, 94%HDI = [−0.005, 0.016]; see <xref rid="fig3" ref-type="fig">Figure 3A</xref>). There was no evidence for tracking of the distracting speech stream (<italic>β</italic> = −0.008, 94%HDI = [−0.020, 0.003]). On the contrary, horizontally directed eye movements selectively track attended (<italic>β</italic> = 0.014, 94%HDI = [0.005, 0.024]), but not unattended (<italic>β</italic> = −0.002, 94%HDI = [−0.011, 0.007]) acoustic speech in a multi-speaker condition (see <xref rid="fig3" ref-type="fig">Figure 3B</xref>). Speech tracking in a single speaker condition did not reach significance (<italic>β</italic> = 0.009, 94%HDI = [−0.001, 0.017]) for horizontal eye movements. These findings indicate that eye movements selectively track attended, but not unattended acoustic speech. Furthermore, there seems to be a dissociation between horizontal and vertical ocular speech tracking, indicating that horizontal movements track attended speech in a multi-speaker condition, whereas vertical movements track attended speech in a single speaker condition (see <bold>Supplementary Table 2</bold> for a summary of ocular speech tracking effects).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Ocular speech tracking is dependent on selective attention</title>
<p><bold>A)</bold> Vertical eye movements ‘significantly’ track attended clear speech, but not in a multi-speaker condition. Temporal profiles of this effect show a downward pattern (negative TRF weights). <bold>B)</bold> Horizontal eye movements ‘significantly’ track attended speech in a multi-speaker condition. Temporal profiles of this effect show a left-rightwards (negative to positive TRF weights) pattern. Statistics were performed using Bayesian regression models. A ‘*’ within posterior distributions depicts a significant difference from zero (i.e. the 94%HDI does not include zero). Shaded areas in TRF weights represent 95% confidence intervals. <italic>N</italic> = 29.</p></caption>
<graphic xlink:href="546746v3_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Additionally, we wanted to investigate if predictive processes are related to ocular speech tracking using the same approach as for neural speech tracking (see previous section). Crucially, we found no evidence for a relationship between individual prediction tendency and ocular speech tracking on a vertical (<italic>β</italic> = 0.001, 94%HDI = [−0.005, 0.008]) or horizontal (<italic>β</italic> = - 0.001, 94%HDI = [−0.007, 0.004]) plane. Similarly, we found no difference in ocular speech tracking between words of high surprisal and their lexically matched controls (see <bold>Supplementary Table 3</bold>). These findings indicate that individuals engage in ocular speech tracking independent of their individual prediction tendency or overall semantic probability.</p>
</sec>
<sec id="s3c">
<title>Neural speech tracking is mediated by eye movements</title>
<p>Additionally, we performed a similar, but more detailed mediation analysis compared to <xref ref-type="bibr" rid="c16">Gehmacher and colleagues (2024)</xref> in order to separately investigate the contribution of horizontal and vertical eye movements to neural speech tracking across different time lags. Following mediation analysis requirements, only significant ocular speech tracking effects were further considered, i.e. vertical eye movements in the clear speech condition and horizontal eye movements in response to a target in the multi-speaker condition. We compared the plain effect (c) of neural speech tracking (using a simple stimulus model with speech envelope and a time-shuffled version of the respective eye movements as the predictors for neural responses) to its direct (residual) effect (c’) by including true horizontal or vertical eye movements as a second predictor into the stimulus model. The decrease in predictor weights from the plain to the residual stimulus model indicates the extent of the mediation effect. To establish a time-resolved mediation analysis in source-space, we computed the main components of the effect (via PCA, see <xref rid="fig4" ref-type="fig">Figure 4</xref>). We found significant mediation effects for all three principal components (PC) for both vertical eye movements in the clear speech condition (see <xref rid="fig4" ref-type="fig">Figure 4A</xref>) and horizontal eye movements in response to a target in the multi-speaker condition (see <xref rid="fig4" ref-type="fig">Figure 4B</xref>). PC1 reached significance (nearly) across the whole time-window from −0.3 - 0.7 s over widespread auditory regions in both conditions with a right-hemispheric dominance, peaking at ∼ 0.18 s. Interestingly, PC2 instead loaded mostly on left auditory regions, with a slight anticipation effect for both vertical and horizontal eye movements. Additionally, PC2 loaded over left parietal areas for the mediation via horizontal eye movements in the multi-speaker condition. In both conditions, PC2 showed another early peak at ∼80 ms. The mediation effect remained significant almost entirely over positive time-lags for both conditions. We found less contributions of vertical eye movements to neural clear speech tracking for PC3 with significant clustering ∼ 0.2 - 0.4 s over scattered cortical regions. For horizontal eye movements, PC3 still showed an effect in left auditory areas at several short peaks (∼ −0.2 s, ∼ 0.05 s, and ∼ 0.4 s). Taken together, this suggests that eye movements contribute considerably to neural speech tracking over widespread cortical areas that are commonly related to speech processing and attention.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Ocular speech tracking and selective attention to speech share underlying neural computations</title>
<p><bold>A)</bold> Vertical eye movements significantly mediate neural clear speech tracking throughout the time-lags from −0.3 - 0.7 s for principal component 1 (PC1) over right-lateralized auditory regions.This mediation effect propagates to more leftwards lateralized auditory areas over later time-lags for PC2 and PC3. <bold>B)</bold> Horizontal eye movements similarly contribute to neural speech tracking of a target in a multi-speaker condition over right-lateralized auditory processing regions for PC1, also with significant anticipatory contributions and a clear peak at ∼ 0.18 s. PC2 shows a clear left-lateralization, however not only over auditory, but also parietal areas almost entirely throughout the time-window of interest with a clear anticipatory effect starting at −0.3 s. For PC3, there still remained a small anticipatory cluster ∼ −0.2 s again over mostly left-lateralized auditory regions. Colour bars represent PCA weights for the group-averaged mediation effect. Shaded areas on time-resolved model-weights represent regions of practical equivalence (ROPE) according to Kruschke et al. (2018). Solid lines show ‘significant’ clusters where at minimum two neighbouring time-points showed a significant mediation effect. Statistics were performed using Bayesian regression models. <italic>N</italic> = 29.s</p></caption>
<graphic xlink:href="546746v3_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s3d">
<title>Neural and ocular speech tracking are differently related to comprehension</title>
<p>In a final step, we addressed the behavioural relevance of neural as well as ocular speech tracking respectively. At the end of every trial, participants were asked to evaluate 4 different true or false statements about the target story. The accuracy of these responses was averaged within condition (single vs. multi-speaker) and served as an approximation for semantic speech comprehension. Additionally, we evaluated the averaged subjective ratings of difficulty (which were given on a 5-point likert scale).</p>
<p>To avoid calculating separate models for all 102 magnetometers, neural encoding was averaged over selected channels (10% showing the strongest encoding effect). We found no significant relationship between neural speech tracking and comprehension (<italic>β</italic> = 0.138, 94%HDI = [−0.050, 0.330]), no interaction between neural speech tracking and condition (<italic>β</italic> = −0.088, 94%HDI = [−0.390, 0.201]), but a significant effect for condition (<italic>β</italic> = −0.438, 94%HDI = [−0.714, −0.178]), indicating that comprehension was decreased in the multi-speaker condition (see <xref rid="fig5" ref-type="fig">Figure 5A</xref>). Similarly, we found no effect of prediction tendency on comprehension (<italic>β</italic> = 0.050, 94%HDI = [−0.092, 0.197]). When investigating subjective ratings of task difficulty, we also found no effect for neural speech tracking (<italic>β</italic> = 0.156, 94%HDI = [−0.079, 0.382]), no interaction between neural speech tracking and condition (<italic>β</italic> = 0.041, 94%HDI = [−0.250, 0.320]), nor individual prediction tendency (<italic>β</italic> = −0.058, 94%HDI = [−0.236, 0.127]). There was, however, a significant difference between conditions (<italic>β</italic> = 1.365, 94%HDI = [1.128, 1.612]), indicating that the multi-speaker condition was rated more difficult than the single speaker condition.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Ocular, but not neural speech tracking is related to semantic speech comprehension</title>
<p><bold>A)</bold> There was no significant relationship between neural speech tracking (10% sensors with strongest encoding effect) and comprehension, however, a condition effect indicated that comprehension was generally decreased in the multi-speaker condition. <bold>B &amp; C)</bold> A ‘significant’ negative relationship between comprehension and vertical as well as horizontal ocular speech tracking shows that participants with weaker comprehension increasingly engaged in ocular speech tracking. Statistics were performed using Bayesian regression models. Shaded areas represent 94% HDIs. <italic>N</italic> = 29.</p></caption>
<graphic xlink:href="546746v3_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In contrast to neural findings, we found a negative relationship between ocular speech tracking and comprehension for vertical as well as horizontal eye movements irrespective of condition (see <xref rid="fig5" ref-type="fig">Figure 5B-C</xref> and <bold>Supplementary Table 4</bold>). This suggests that participants with weaker performance in semantic comprehension increasingly engaged in ocular speech tracking. There was, however, no significant relationship between subjectively rated difficulty and ocular speech tracking (see <bold>Supplementary Table 5</bold>). Presumably, subjective ratings are less comparable between participants, which might be one reason why we did find an effect for objective but not subjective measures. In sum, the current findings show that ocular speech tracking is differently related to comprehension than neural speech tracking, suggesting that they might not refer to the same underlying concept (e.g. improved representations vs. increased attention). A mediation analysis, however, suggests that they are related and that ocular speech tracking contributes to neural speech tracking (or vice versa).</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>In the current study, we aimed to replicate and extend findings from two recent studies from our lab in order to integrate them into a comprehensive view of speech processing. In <xref ref-type="bibr" rid="c34">Schubert and colleagues (2023)</xref>, we found that individual prediction tendencies are related to cortical speech tracking. In <xref ref-type="bibr" rid="c16">Gehmacher and colleagues (2024)</xref>, we found that eye movements track acoustic speech in selective attention (a phenomenon that we termed “ocular speech tracking”). In the present study, we were able to replicate both findings, providing further details and insight into these phenomena.</p>
<p>In a first step, we confirmed that individuals with a stronger prediction tendency (which was inferred from anticipatory probabilistic representations in an independent paradigm) showed an increased neural speech tracking over left frontal areas. Thus, the finding that prediction tendencies generalise across different listening situations seems to be robust. This stresses the importance of research focusing on the beneficial aspects of individual “traits” in predictive processing. As suggested in a comparative study (<xref ref-type="bibr" rid="c34">Schubert et al., 2023</xref>), these predictive tendencies or traits do not generalise across modalities but seem to be reliable within the auditory modality. We suggest that they could serve as an independent predictor of linguistic abilities, complementing previous research on statistical learning (e.g. (<xref ref-type="bibr" rid="c35">Siegelman &amp; Frost, 2015</xref>)).</p>
<p>In a second step, we were able to support the assumption that eye movements track prioritised acoustic modulations in a continuous speech design without visual input. With the current (extended) replication, we were able to address an important consideration from Gemacher and colleagues (2024): Ocular speech tracking is not restricted to a simplistic 5-word sentence structure but can also be found for continuous, narrative speech. In line with previous results, we found that horizontal ocular speech tracking is increased in a multi-speaker (compared to a single speaker) condition in response to an attended target but not a distractor speaker. In contrast, we found that vertical eye movements solely track auditory information in a single speaker condition. The differential contribution of vertical vs. horizontal eye movements to auditory processing is not a widely studied topic in neuroscience and further replications are necessary to establish the robustness of this dissociation. One possible explanation for these findings is, that in the multi-speaker condition, the demand for spatial segregation was increased compared to the single speaker condition. Although speech was always presented at phantom centre even for both speakers during the multi-speaker condition, multiple speakers are distributed mostly horizontally in natural human environments. The observed effect might resemble a residual of this learned association between a spatially segregated target speaker’s acoustic output and a lateral shift of gaze (and attention) on the horizontal plane towards the target’s source location, maximising information and thereby minimising uncertainty about the targeted auditory object.</p>
<p>Irrespective of the potential differences in gaze direction and condition, we found that ocular movements contribute to neural speech tracking over widespread auditory and parietal areas, replicating the sensor-level findings of <xref ref-type="bibr" rid="c16">Gehmacher et al. (2024)</xref>. In addition, time-resolved analyses of this mediation effect further extend these findings and suggest even anticipatory contributions. It is important to note that our current findings do not allow for inference on directionality. Hence, it is possible that ocular mediation of speech tracking may reflect a) active (ocular) sensing for information driven by (top-down) selective attention or b) improved neural representations as a consequence of temporally aligned increase of sensory gain or c) (not unlikely) both.</p>
<p>Despite the finding that eye movements mediate neural speech tracking, the behavioural relevance for semantic comprehension seems to differ between ocular and neural speech tracking. To be more specific, we found a negative association between ocular speech tracking and comprehension, indicating that subjects with weaker performance in comprehension increasingly engaged in ocular speech tracking. Interestingly, we did not find a relationship between neural tracking and comprehension. This suggests that speech envelope tracking may reflect different underlying (cognitive) processes depending on where the response was measured (eyes vs. brain). It should be noted that our questions on story content (see section Experimental Procedure for an example) targeted a broad range of cognitive processes from intelligibility to memory encoding / retrieval. Thus, it can be argued that the inverse relationship between ocular tracking and semantic comprehension might be the result of individual differences in the trade-off between the focus on lower level acoustics (with temporal sharpening) and the engagement in higher-level cognitive processes such as memory (requiring the integration of information over longer timescales). This interpretation would also explain why we did not find a relationship with neural speech tracking, as sensors that show the highest acoustic tracking might not be relevant for more extensive semantic processing. However, a more detailed investigation would require carefully graduated measures of intelligibility and comprehension, which was beyond the scope of the current analysis.</p>
<p>Crucially, as we found no difference in ocular speech tracking between words of high surprisal and their lexically matched control, we argue that ocular speech tracking may not be indicative of “qualia” in higher level (linguistic or semantic) stimulus representations. Instead, we propose that it reflects an attentional (and - also in relation to the point above - maybe even a compensatory) mechanism. For example, it has been found that attention modulates phase in the auditory brainstem response, locking it to the pitch structure of attended but not unattended speech (<xref ref-type="bibr" rid="c12">Forte et al., 2017</xref>). Similarly, it is possible that increased ocular tracking indicates increased selective attention in the auditory periphery, rather than improved representations of stimulus content on a higher level.</p>
<p>Interestingly, we were not able to relate individual prediction tendency to ocular speech tracking. This raises the question whether ocular speech tracking is modulated not by predictive, but other (for example attentional) processes. Indeed, the current findings suggest that prediction tendencies and active ocular sensing are related to different aspects of neural speech processing. We propose a perspective in which active sensing is guided by selective attention, whereas anticipatory prediction tendency is an independent correlate of neural speech tracking. Even though predictive processing as well as attention might be considered as the two pillars on which perception rests, research investigating their selective contributions and interactions is rare. <xref ref-type="bibr" rid="c37">Summerfield and de Lange (2014)</xref> have argued that predictions and attention are distinctive mechanisms as the former refers to the probability and the latter to the relevance of an event, arguing that events can be conditionally probable but irrelevant for current goals and vice versa. Similarly, it has been proposed that attention can be integrated into the predictive coding framework in reference to the optimization of sensory precision (<xref ref-type="bibr" rid="c11">Feldman &amp; Friston, 2010</xref>). In this view, prediction tendency encodes probabilistic representations of a feature, whereas selective attention determines the precision of sensory inflow that leads to internal model updating. This interpretation is in line with our finding that a) anticipatory feature-specific predictions can be found in a passive listening task, b) prediction tendencies are not increasingly linked to speech tracking with increasing demands on selective attention, whereas on the other hand c) ocular speech tracking seems to increase with selective attention (at least on a horizontal plane), d) remains unaffected by semantic probability and e) contributes to neural speech processing over widespread auditory and parietal areas with a potential overlap with attentional networks. For this reason, we refer to ocular speech tracking as an “active sensing” mechanism that implements the attentional optimization of sensory precision. Instead of passively transducing any input into neural activity, we actively engage with our environment to maximise information, i.e. reduce uncertainty. It has been suggested that motor routines contribute to temporal precision of selective attention, largely determining sensory inflow and hence perception (<xref ref-type="bibr" rid="c26">Morillon et al., 2015</xref>; <xref ref-type="bibr" rid="c33">Schroeder et al., 2010</xref>). In the current framework, we refer to active sensing via eye movements as temporally aligned shifts of gaze at exact points in time aligned with its intensity (information content) fluctuations. In particular, active ocular sensing may even help to increase sensory gain already in the auditory periphery at specific intervals, synchronised with the temporal modulation of attended (but not unattended) speech. As a consequence of this “sensory gating” already at early stages, eye movements potentially affect auditory processing from the ear to the cortex (also see (<xref ref-type="bibr" rid="c22">Leszczynski et al., 2023</xref>) for saccadic modulations of cortical excitability in auditory areas), contributing to neural speech representations rather than encoding them. Again, this idea is supported by our finding that ocular speech tracking seems to be unaffected by semantic violations.</p>
<p>Indeed, similar active ocular sensing mechanisms have already been suggested to facilitate sound localization (<xref ref-type="bibr" rid="c24">Lovich et al., 2023</xref>). Since the current paradigm did not allow for spatial segregation (as competing speech streams have all been presented at phantom centre), it required a different strategy such as (spectro-)temporal differentiation. We argue that active ocular sensing increases temporal precision of complex acoustic input (in order to parse speech into its components of rhythmic, temporally predictable patterns within a particular speaker).</p>
<p>Based on the joint findings of the present as well as its preceding studies (<xref ref-type="bibr" rid="c16">Gehmacher et al., 2024</xref>, <xref ref-type="bibr" rid="c34">Schubert et al., 2023</xref>), we propose a unified working model in which anticipatory predictions as well as active sensing work (independently) together to support auditory speech perception (see <xref rid="fig6" ref-type="fig">Figure 6</xref> for a schematic illustration). We suggest that anticipatory predictions about a feature help to interpret auditory information at different levels along the perceptual hierarchy. Accordingly, these predictions carry high feature-specificity but low temporal precision (as they are anticipatory in nature). It should be noted that the representational content is likely to be different at different levels of the perceptual hierarchy (e.g. encoding of acoustics vs. semantics). However, the probability of a feature at a higher level affects the interpretation (and thus the representation and prediction) of a feature at lower levels. Furthermore, individuals differ in their general tendency to create such anticipatory predictions, which leads to differences in neural speech tracking. Active sensing, on the other hand, increases temporal precision - potentially already at the early stages of sound transduction - to facilitate bottom-up processing of selectively attended input. Crucially, we suggest that active (ocular) sensing does not necessarily convey feature- or content-specific information, it is merely used to boost (and conversely filter) sensory input at specific timescales (similar to neural oscillations). Our research suggests that this active ocular sensing mechanism or “filter” is driven by internal (attentional) goals rather than prior beliefs (stressing the distinction to other active sensing mechanisms).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><title>A schematic illustration of the framework</title>
<p>Anticipatory predictions help to interpret auditory information at different levels along the perceptual hierarchy (purple) with high feature-specificity but low temporal precision. Active sensing (green) increases the temporal precision already at early stages of the auditory system to facilitate bottom-up processing of selectively attended input (blue). We suggest that auditory inflow, on a basic level, and speech segmentation, on a more complex level, is temporally modulated via active ocular sensing, and incoming information is interpreted based on probabilistic assumptions.</p></caption>
<graphic xlink:href="546746v3_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In this speculative framework for listening, auditory inflow is, at a basic level, temporally modulated via active ocular sensing, and incoming information is interpreted based on probabilistic assumptions. Optimal speech processing requires a careful balance between the weighting of internal models and the allocation of resources based on current goals. We suggest that in the case of speech processing, this challenge results in an independent adaptation of feature-based precision-weighting by predictions on the one hand and temporal precision-weighting by selective attention on the other.</p>
<p>We suggest that future research on auditory perception should integrate conceptual considerations on predictive processing, active (crossmodal) sensing, and selective attention. In particular, it would be interesting whether ocular speech tracking can be observed for unfamiliar languages with unpredictable prosodic rate. Furthermore, the relationship between neural oscillations in selective attention and active sensing should be further investigated using experimental modulations to address the important, pending question of causality. Brain stimulation (such as tACS; transcranial alternating current stimulation) could be used in an attempt to alter temporal processing frames and / or ocular speech tracking. With regards to the latter, future studies should focus on the potential consequences of inhibited active sensing (e.g. actively disrupting natural gaze behaviour) for neural speech tracking. Our interpretation suggests that increased tracking of unexpected input (i.e. semantic violations) should be affected by active ocular sensing if sensory gain in the periphery is indeed dependent on this mechanism. Currently, the findings propose that active ocular sensing, with its substantial contribution to neural speech tracking, is driven by selective attention, and not by individual differences in prediction tendency.</p>
</sec>
<sec id="d1e978" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1068">
<label>Supplementary Material</label>
<media xlink:href="supplements/546746_file02.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>This research was funded in whole or in part by the Austrian Science Fund (FWF) [10.55776/W1233-B]. For open access purposes, the author has applied a CC BY public copyright license to this work. Q.G. was also supported by the Austrian Research Promotion Agency (FFG; BRIDGE 1 project “SmartCIs”; 871232). Thanks to the whole research team. Special thanks to Manfred Seifter for his support in conducting the MEG measurements.</p>
</ack>
<sec id="s5">
<title>Author contributions</title>
<p>J.S. and Q.G. designed the experiment, analysed the data, generated the figures, and wrote the manuscript. T.H. recruited participants and supported the data analysis. F.S. supported the data analysis and edited the manuscript. N.W. acquired the funding, supervised the project, and edited the manuscript.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brainard</surname>, <given-names>D. H.</given-names></string-name>, &amp; <string-name><surname>Vision</surname>, <given-names>S</given-names></string-name></person-group>. (<year>1997</year>). <article-title>The psychophysics toolbox</article-title>. <source>Spatial Vision</source>, <volume>10</volume>(<issue>4</issue>), <fpage>433</fpage>– <lpage>436</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brodbeck</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Das</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Gillis</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kulasingham</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Bhattasali</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gaston</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Resnik</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Simon</surname>, <given-names>J. Z</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Eelbrain, a Python toolkit for time-continuous analysis with temporal response functions</article-title>. <source>eLife</source>, <volume>12</volume>, <fpage>e85012</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.85012</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Broderick</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>A. J.</given-names></string-name>, &amp; <string-name><surname>Lalor</surname>, <given-names>E. C</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Semantic Context Enhances the Early Auditory Encoding of Natural Speech</article-title>. <source>Journal of Neuroscience</source>, <volume>39</volume>(<issue>38</issue>), <fpage>7564</fpage>– <lpage>7575</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0584-19.2019</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Capretto</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Piho</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kumar</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Westfall</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Yarkoni</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Martin</surname>, <given-names>O. A</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Bambi: A Simple Interface for Fitting Bayesian Linear Models in Python</article-title>. <source>Journal of Statistical Software</source>, <volume>103</volume>, <fpage>1</fpage>–<lpage>29</lpage>. <pub-id pub-id-type="doi">10.18637/jss.v103.i15</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crosse</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Di Liberto</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>Bednar</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Lalor</surname>, <given-names>E. C.</given-names></string-name></person-group> (<year>2016</year>). <article-title>The multivariate temporal response function (mTRF) toolbox: A MATLAB toolbox for relating neural signals to continuous stimuli</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>10</volume>, <fpage>604</fpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crosse</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Zuk</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Di Liberto</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>Nidiffer</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Molholm</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Lalor</surname>, <given-names>E. C.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Linear modeling of neurophysiological responses to speech and other continuous stimuli: Methodological considerations for applied research</article-title>. <source>Frontiers in Neuroscience</source>, <volume>15</volume>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cui</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Hondzinski</surname>, <given-names>J. M</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Gaze tracking accuracy in humans: Two eyes are better than one</article-title>. <source>Neuroscience Letters</source>, <volume>396</volume>(<issue>3</issue>), <fpage>257</fpage>–<lpage>262</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>David</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Mesgarani</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Shamma</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Estimating sparse spectro-temporal receptive fields with natural stimuli. <italic>Network (Bristol</italic></article-title>, <source>England</source><italic>)</italic>, <volume>18</volume>, <fpage>191</fpage>–<lpage>212</lpage>. <pub-id pub-id-type="doi">10.1080/09548980701609235</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Demarchi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Sanchez</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Weisz</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Automatic and feature-specific prediction-related neural activity in the human auditory system</article-title>. <source>Nature Communications</source>, <volume>10</volume>(<fpage>1</fpage>), <elocation-id>3440</elocation-id>. <pub-id pub-id-type="doi">10.1038/s41467-019-11440-1</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Donhauser</surname>, <given-names>P. W.</given-names></string-name>, &amp; <string-name><surname>Baillet</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Two Distinct Neural Timescales for Predictive Speech Processing</article-title>. <source>Neuron</source>, <volume>105</volume>(<issue>2</issue>), <fpage>385</fpage>–<lpage>393.e9.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2019.10.019</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feldman</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Friston</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Attention, Uncertainty, and Free-Energy</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>4</volume>. <pub-id pub-id-type="doi">10.3389/fnhum.2010.00215</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Forte</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Etard</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Reichenbach</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2017</year>). <article-title>The human auditory brainstem response to running speech reveals a subcortical mechanism for selective attention</article-title>. <source>eLife</source>, <volume>6</volume>, <fpage>e27203</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.27203</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2010</year>). <article-title>The free-energy principle: A unified brain theory?</article-title> <source>Nature Reviews Neuroscience</source>, <volume>11</volume>(<fpage>2</fpage>), <fpage>127</fpage>–<lpage>138</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2787</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Sajid</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Quiroga-Martinez</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Parr</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Price</surname>, <given-names>C. J.</given-names></string-name>, &amp; <string-name><surname>Holmes</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Active listening</article-title>. <source>Hearing Research</source>, <volume>399</volume>, <fpage>107998</fpage>. <pub-id pub-id-type="doi">10.1016/j.heares.2020.107998</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Galantucci</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Fowler</surname>, <given-names>C. A.</given-names></string-name>, &amp; <string-name><surname>Turvey</surname>, <given-names>M. T</given-names></string-name></person-group>. (<year>2006</year>). <article-title>The motor theory of speech perception reviewed</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>13</volume>(<issue>3</issue>), <fpage>361</fpage>–<lpage>377</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gehmacher</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Schubert</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Hartmann</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Reisinger</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Rösch</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Schwarz</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Popov</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Chait</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Weisz</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Eye movements track prioritized auditory features in selective attention to natural speech</article-title>. <source>Nature Communications</source>, <volume>15</volume>(<issue>1</issue>), <fpage>3692</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-024-48126-2</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hartmann</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Weisz</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2020</year>). <article-title>An introduction to the Objective Psychophysics Toolbox</article-title>. <source>Frontiers in Psychology</source>, <volume>11</volume>. <pub-id pub-id-type="doi">10.3389/fpsyg.2020.585437</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kisler</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Reichel</surname>, <given-names>U.</given-names></string-name>, &amp; <string-name><surname>Schiel</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Multilingual processing of speech via web services</article-title>. <source>Computer Speech &amp; Language</source>, <volume>45</volume>, <fpage>326</fpage>–<lpage>347</lpage>. <pub-id pub-id-type="doi">10.1016/j.csl.2017.01.005</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kleiner</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brainard</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Pelli</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2007</year>). <article-title>What’s new in Psychtoolbox-3?</article-title> <source>Perception</source>, <volume>36</volume>, <fpage>1</fpage>–<lpage>16</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Knill</surname>, <given-names>D. C.</given-names></string-name>, &amp; <string-name><surname>Pouget</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2004</year>). <article-title>The Bayesian brain: The role of uncertainty in neural coding and computation</article-title>. <source>Trends in Neurosciences</source>, <volume>27</volume>(<issue>12</issue>), <fpage>712</fpage>–<lpage>719</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2004.10.007</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kruschke</surname>, <given-names>J. K.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Rejecting or Accepting Parameter Values in Bayesian Estimation</article-title>. <source>Advances in Methods and Practices in Psychological Science</source>, <volume>1</volume>(<issue>2</issue>), <fpage>270</fpage>–<lpage>280</lpage>. <pub-id pub-id-type="doi">10.1177/2515245918771304</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leszczynski</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bickel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Nentwich</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Russ</surname>, <given-names>B. E.</given-names></string-name>, <string-name><surname>Parra</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mehta</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Schroeder</surname>, <given-names>C. E</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Saccadic modulation of neural excitability in auditory areas of the neocortex</article-title>. <source>Current Biology</source>, <volume>33</volume>(<issue>7</issue>), <fpage>1185</fpage>–<lpage>1195.e6.</lpage> <pub-id pub-id-type="doi">10.1016/j.cub.2023.02.018</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liberman</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name><surname>Mattingly</surname>, <given-names>I. G</given-names></string-name></person-group>. (<year>1985</year>). <article-title>The motor theory of speech perception revised</article-title>. <source>Cognition</source>, <volume>21</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>36</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lovich</surname>, <given-names>S. N.</given-names></string-name>, <string-name><surname>King</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Murphy</surname>, <given-names>D. L. K.</given-names></string-name>, <string-name><surname>Landrum</surname>, <given-names>R. E.</given-names></string-name>, <string-name><surname>Shera</surname>, <given-names>C. A.</given-names></string-name>, &amp; <string-name><surname>Groh</surname>, <given-names>J. M</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Parametric information about eye movements is sent to the ears</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>120</volume>(<issue>48</issue>), <fpage>e2303562120</fpage>. <pub-id pub-id-type="doi">10.1073/pnas.2303562120</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mattout</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Henson</surname>, <given-names>R. N.</given-names></string-name>, &amp; <string-name><surname>Friston</surname>, <given-names>K. J</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Canonical Source Reconstruction for MEG</article-title>. <source>Computational Intelligence and Neuroscience</source>, <volume>2007</volume>, <fpage>e67613</fpage>. <pub-id pub-id-type="doi">10.1155/2007/67613</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morillon</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Hackett</surname>, <given-names>T. A.</given-names></string-name>, <string-name><surname>Kajikawa</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Schroeder</surname>, <given-names>C. E</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Predictive motor control of sensory dynamics in auditory active sensing</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>31</volume>, <fpage>230</fpage>–<lpage>238</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2014.12.005</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nolte</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2003</year>). <article-title>The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors</article-title>. <source>Physics in Medicine &amp; Biology</source>, <volume>48</volume>(<issue>22</issue>), <fpage>3637</fpage>. <pub-id pub-id-type="doi">10.1088/0031-9155/48/22/002</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oberfeld</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Klöckner-Nowotny</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Individual differences in selective attention predict speech identification at a cocktail party</article-title>. <source>eLife</source>, <volume>5</volume>, <fpage>e16747</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.16747</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fries</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Schoffelen</surname>, <given-names>J.-M</given-names></string-name></person-group>. (<year>2011</year>). <article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>. <source>Computational Intelligence and Neuroscience</source>, <volume>2011</volume>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruggles</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bharadwaj</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Shinn-Cunningham</surname>, <given-names>B. G</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Normal hearing is not enough to guarantee robust encoding of suprathreshold features important in everyday communication</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>108</volume>(<issue>37</issue>), <fpage>15516</fpage>–<lpage>15521</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1108912108</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Salvatier</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wiecki</surname>, <given-names>T. V.</given-names></string-name>, &amp; <string-name><surname>Fonnesbeck</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Probabilistic programming in Python using PyMC3</article-title>. <source>PeerJ Computer Science</source>, <volume>2</volume>, <fpage>e55</fpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schiel</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Ohala</surname>, <given-names>J. J</given-names></string-name></person-group>. (<year>1999</year>). <article-title><italic>Automatic Phonetic Transcription of Non-Prompted Speech</italic> (J. J. Ohala</article-title>, <source>Ed</source>.; pp. <fpage>607</fpage>–<lpage>610</lpage>). <pub-id pub-id-type="doi">10.5282/ubm/epub.13682</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schroeder</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Radman</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Scharfman</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Lakatos</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Dynamics of Active Sensing and perceptual selection</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>20</volume>(<issue>2</issue>), <fpage>172</fpage>–<lpage>176</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2010.02.010</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schubert</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Gehmacher</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Bresgen</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Weisz</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Cortical speech tracking is related to individual prediction tendencies</article-title>. <source>Cerebral Cortex</source>, <volume>33</volume>(<issue>11</issue>), <fpage>6608</fpage>– <lpage>6619</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhac528</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Siegelman</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Frost</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Statistical learning as an individual ability: Theoretical perspectives and empirical evidence</article-title>. <source>Journal of Memory and Language</source>, <volume>81</volume>, <fpage>105</fpage>–<lpage>120</lpage>. <pub-id pub-id-type="doi">10.1016/j.jml.2015.02.001</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>Z. M.</given-names></string-name>, <string-name><surname>Delgutte</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Oxenham</surname>, <given-names>A. J</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Chimaeric sounds reveal dichotomies in auditory perception</article-title>. <source>Nature</source>, <volume>416</volume> <fpage>87</fpage>–<lpage>90</lpage>. <pub-id pub-id-type="doi">10.1038/416087a</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Expectation in perceptual decision making: Neural and computational mechanisms</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>15</volume>(<issue>11</issue>), <fpage>745</fpage>– <lpage>756</lpage>. <pub-id pub-id-type="doi">10.1038/nrn3838</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Treder</surname>, <given-names>M. S</given-names></string-name></person-group>. (<year>2020</year>). <article-title>MVPA-Light: A Classification and Regression Toolbox for Multi-Dimensional Data</article-title>. <source>Frontiers in Neuroscience</source>, <volume>14</volume>. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fnins.2020.00289">https://www.frontiersin.org/articles/10.3389/fnins.2020.00289</ext-link></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Veen</surname>, <given-names>B. D.</given-names></string-name>, <string-name><surname>Van Drongelen</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Yuchtman</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Suzuki</surname>, <given-names>A.</given-names></string-name></person-group> (<year>1997</year>). <article-title>Localization of brain electrical activity via linearly constrained minimum variance spatial filtering</article-title>. <source>IEEE Transactions on Biomedical Engineering</source>, <volume>44</volume>(<issue>9</issue>), <fpage>867</fpage>–<lpage>880</lpage>. <pub-id pub-id-type="doi">10.1109/10.623056</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vehtari</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gelman</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Simpson</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Carpenter</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Bürkner</surname>, <given-names>P.-C</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Rank-normalization, folding, and localization: An improved R for assessing convergence of MCMC (with discussion)</article-title>. <source>Bayesian Analysis</source>, <volume>16</volume>(<issue>2</issue>), <fpage>667</fpage>–<lpage>718</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weissbart</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Kandylaki</surname>, <given-names>K. D.</given-names></string-name>, &amp; <string-name><surname>Reichenbach</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Cortical Tracking of Surprisal during Continuous Speech Comprehension</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>32</volume>(<issue>1</issue>), <fpage>155</fpage>–<lpage>166</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_01467</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ying</surname>, <given-names>X</given-names></string-name></person-group>. (<year>2019</year>). <article-title>An Overview of Overfitting and its Solutions</article-title>. <source>Journal of Physics: Conference Series</source>, <volume>1168</volume>(<issue>2</issue>), <fpage>022022</fpage>. <pub-id pub-id-type="doi">10.1088/1742-6596/1168/2/022022</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yon</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lange</surname>, <given-names>F. P. de</given-names></string-name>, &amp; <string-name><surname>Press</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2019</year>). <article-title>The Predictive Brain as a Stubborn Scientist</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>23</volume>(<issue>1</issue>), <fpage>6</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2018.10.003</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zion Golumbic</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Bickel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Schevon</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>McKhann</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>Goodman</surname>, <given-names>R. R.</given-names></string-name>, <string-name><surname>Emerson</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Mehta</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name>, <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Schroeder</surname>, <given-names>C. E.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Mechanisms Underlying Selective Neuronal Tracking of Attended Speech at a “Cocktail Party”</article-title>. <source>Neuron</source>, <volume>77</volume>(<issue>5</issue>), <fpage>980</fpage>–<lpage>991</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2012.12.037</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zion-Golumbic</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Schroeder</surname>, <given-names>C. E</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Attention modulates ‘speech-tracking’ at a cocktail party</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>16</volume>(<issue>7</issue>), <fpage>363</fpage>–<lpage>364</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2012.05.004</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101262.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bottini</surname>
<given-names>Roberto</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Trento</institution>
</institution-wrap>
<city>Trento</city>
<country>Italy</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>These are <bold>valuable</bold> findings for those interested in how neural signals reflect auditory speech streams, and in understanding the roles of prediction, attention, and eye movements in this tracking. However, the evidence as it stands is <bold>incomplete</bold>. Further details are needed on how the observed quantities relate to the relevant theoretical claims and mathematical models. Moreover, additional motivation is required for several analytical choices.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101262.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study aimed at replicating two previous findings that showed (1) a link between prediction tendencies and neural speech tracking, and (2) that eye movements track speech. The main findings were replicated which supports the robustness of these results. The authors also investigated interactions between prediction tendencies and ocular speech tracking, but the data did not reveal clear relationships. The authors propose a framework that integrates the findings of the study and proposes how eye movements and prediction tendencies shape perception.</p>
<p>Strengths:</p>
<p>This is a well-written paper that addresses interesting research questions, bringing together two subfields that are usually studied in separation: auditory speech and eye movements. The authors aimed at replicating findings from two of their previous studies, which was overall successful and speaks for the robustness of the findings. The overall approach is convincing, methods and analyses appear to be thorough, and results are compelling.</p>
<p>Weaknesses:</p>
<p>Linking the new to the previous studies could have been done in more detail, and the extent to which results were replicated could have been discussed more thoroughly.</p>
<p>Eye movement behavior could have been presented in more detail and the authors could have attempted to understand whether there is a particular component in eye movement behavior (e.g., microsaccades) that drives the observed effects.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101262.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary</p>
<p>Schubert et al. recorded MEG and eye-tracking activity while participants were listening to stories in single-speaker or multi-speaker speech. In a separate task, MEG was recorded while the same participants were listening to four types of pure tones in either structured (75% predictable) or random (25%) sequences. The MEG data from this task was used to quantify individual 'prediction tendency': the amount by which the neural signal is modulated by whether or not a repeated tone was (un)predictable, given the context. In a replication of earlier work, this prediction tendency was found to correlate with 'neural speech tracking' during the main task. Neural speech tracking is quantified as the multivariate relationship between MEG activity and speech amplitude envelope. Prediction tendency did not correlate with 'ocular speech tracking' during the main task. Neural speech tracking was further modulated by local semantic violations in the speech material, and by whether or not a distracting speaker was present. The authors suggest that part of the neural speech tracking is mediated by ocular speech tracking. Story comprehension was negatively related to ocular speech tracking.</p>
<p>Strengths</p>
<p>This is an ambitious study, and the authors' attempt to integrate the many reported findings related to prediction and attention in one framework is laudable. The data acquisition and analyses appear to be done with great attention to methodological detail (perhaps even with too much focus on detail-see below). Furthermore, the experimental paradigm used is more naturalistic than was previously done in similar setups (i.e. stories instead of sentences).</p>
<p>Weaknesses</p>
<p>For many of the key variables and analysis choices (e.g. neural/ocular speech tracking, prediction tendency, mediation) it is not directly clear how these relate to the theoretical entities under study, and why they were quantified in this particular way. Relatedly, while the analysis pipeline is outlined in much detail, an overarching rationale and important intermediate results are often missing, which makes it difficult to judge the strength of the evidence presented. Furthermore, some analysis choices appear rather ad-hoc and should be made uniform and/or better motivated.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101262.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this paper, the authors measured neural activity (using MEG) and eye gaze while individuals listened to speech from either one or two speakers, which sometimes contained semantic incongruencies.</p>
<p>The stated aim is to replicate two previous findings by this group: (1) that there is &quot;ocular speech tracking&quot; (that eye-movements track the audio of the speech), (2) that individual differences in neural response to tones that are predictable vs. not-predictable in their pitch is linked to neural response to speech. In addition, here they try to link the above two effects to each other, and to link &quot;attention, prediction, and active sensing&quot;.</p>
<p>Strengths:</p>
<p>This is an ambitious project, that tackles an important issue and combines different sources of data (neural data, eye-movements, individual differences in another task) in order to obtain a comprehensive &quot;model&quot; of the involvement of eye-movements in sensory processing.</p>
<p>The authors use many adequate methods and sophisticated data-analysis tools (including MEG source analysis and multivariate statistical models) in order to achieve this.</p>
<p>Weaknesses:</p>
<p>Although I sympathize with the goal of the paper and agree that this is an interesting and important theoretical avenue to pursue, I am unfortunately not convinced by the results and find that many of the claims are very weakly substantiated in the actual data.</p>
<p>Since most of the analyses presented here are derivations of statistical models and very little actual data is presented, I found it very difficult to assess the reliability and validity of the results, as they currently stand. I would be happy to see a thoroughly revised version, where much more of the data is presented, as well as control analyses and rigorous and well-documented statistical testing (including addressing multiple comparisons).</p>
<p>These are the main points of concern that I have regarding the paper, in its current format.</p>
<p>(1) Prediction tendencies - assessed by listening to sequences of rhythmic tones, where the pitch was either &quot;predictable&quot; (i.e., followed a fixed pattern, with 25% repetition) or &quot;unpredictable&quot; (no particular order to the sounds). This is a very specific type of prediction, which is a general term that can operate along many different dimensions. Why was this specific design selected? Is there theoretical reason to believe that this type of prediction is also relevant to &quot;semantic&quot; predictions or other predictive aspects of speech processing?</p>
<p>(2) On the same point - I was disappointed that the results of &quot;prediction tendencies&quot; were not reported in full, but only used later on to assess correlations with other metrics. Even though this is a &quot;replication&quot; of previous work, one would like to fully understand the results from this independent study. On that note, I would also appreciate a more detailed explanation of the method used to derive the &quot;prediction tendency&quot; metric (e.g, what portion of the MEG signal is used? Why use a pre-stimulus and not a post-stimulus time window? How is the response affected by the 3Hz steady-state response that it is riding on? How are signals integrated across channels? Can we get a sense of what this &quot;tendency&quot; looks like in the actual neural signal, rather than just a single number derived per participant (an illustration is provided in Figure 1, but it would be nice to see the actual data)? How is this measure verified statistically? What is its distribution across the sample? Ideally, we would want enough information for others to be able to replicate this finding).</p>
<p>(3) Semantic violations - half the nouns ending sentences were replaced to create incongruent endings. Can you provide more detail about this - e.g., how were the words selected? How were the recordings matched (e.g., could they be detected due to audio editing?)? What are the &quot;lexically identical controls that are mentioned&quot;? Also, is there any behavioral data to know how this affected listeners? Having so many incongruent sentences might be annoying/change the nature of listening. Were they told in advance about these?</p>
<p>(4) TRF in multi-speaker condition: was a univariate or multivariate model used? Since the single-speaker condition only contains one speech stimulus - can we know if univariate and multivariate models are directly comparable (in terms of variance explained)? Was any comparison to permutations done for this analysis to assess noise/chance levels?</p>
<p>(5) TRF analysis at the word level: from my experience, 2-second segments are insufficient for deriving meaningful TRFs (see for example the recent work by Mesik &amp; Wojtczak). Can you please give further details about how the analysis of the response to semantic violations was conducted? What was the model trained on (the full speech or just the 2-second long segments?) Is there a particular advantage to TRFs here, relative - say - to ERPs (one would expect a relatively nice N400 response, not)? In general, it would be nice to see the TRF results on their own (and not just the modulation effects).</p>
<p>(6) Another related point that I did not quite understand - is the dependent measure used for the regression model &quot;neural speech envelope tracking&quot; the r-value derived just from the 2sec-long epochs? Or from the entire speech stimulus? The text mentions the &quot;effect of neural speech tracking&quot; - but it's not clear if this refers to the single-speaker vs. two-speaker conditions or to the prediction manipulation. Or is it different in the different analyses? Please spell out exactly what metric was used in each analysis.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101262.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Schubert</surname>
<given-names>Juliane</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2536-6522</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Gehmacher</surname>
<given-names>Quirin</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9407-2763</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Schmidt</surname>
<given-names>Fabian</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9839-1614</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Hartmann</surname>
<given-names>Thomas</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8298-8125</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Weisz</surname>
<given-names>Nathan</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7816-0037</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We appreciate all the reviewers for their encouraging comments and thoughtful feedback. We are confident that we can incorporate many of the suggestions to provide a clearer overall picture in the revised manuscript. In particular, we agree with the reviewers' concern that some of our methodological decisions, including our choice of metrics, require further clarification. We will focus on revising the methods section to make these decisions more transparent and to address any misunderstandings related to the analysis.</p>
<p>We also value the request to include more data, such as intermediate results and additional control analyses. We will carefully assess which results to include in the main manuscript and which to provide in an extended supplementary section.</p>
<p>To offer a more detailed understanding of our quantification of &quot;prediction tendency,&quot; we refer to our previous work (Schubert et al., 2023, 2024), where we elaborate on our analytical choices in great detail and provide additional control analyses (e.g., ensuring that the relationship with speech tracking is not driven by participants' signal-to-noise ratio; Schubert et al., 2023).</p>
<p>Additionally, we would like to clarify that the aim of this manuscript is not to analyze viewing behavior in depth but to replicate the general finding of ocular speech tracking, as presented in Gehmacher et al. (2024). A thorough investigation of specific ocular contributions (e.g., microsaccades or blinks) would require a separate research question and distinct analysis approaches, given the binary nature of such events.</p>
<p>Nevertheless, we share the reviewers' interest in independent results from the current study, and we plan to carefully select and present the most relevant findings in the revised manuscript.</p>
</body>
</sub-article>
</article>