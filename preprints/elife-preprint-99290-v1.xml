<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">99290</article-id>
<article-id pub-id-type="doi">10.7554/eLife.99290</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.99290.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Neural dynamics of visual working memory representation during sensory distraction</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0003-8682-3920</contrib-id>
<name>
<surname>Degutis</surname>
<given-names>Jonas Karolis</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8440-1156</contrib-id>
<name>
<surname>Weber</surname>
<given-names>Simon</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8879-5666</contrib-id>
<name>
<surname>Soch</surname>
<given-names>Joram</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="aff" rid="a7">7</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1786-6954</contrib-id>
<name>
<surname>Haynes</surname>
<given-names>John-Dylan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a8">8</xref>
<xref ref-type="aff" rid="a9">9</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Bernstein Center for Computational Neuroscience Berlin and Berlin Center for Advanced Neuroimaging, Charité Universitätsmedizin Berlin, corporate member of the Freie Universität Berlin, Humboldt-Universität zu Berlin, and Berlin Institute of Health</institution>, <city>Berlin</city>, <country>Germany</country></aff>
<aff id="a2"><label>2</label><institution>Max Planck School of Cognition</institution>, <city>Leipzig</city>, <country>Germany</country></aff>
<aff id="a3"><label>3</label><institution>Department of Psychology, Humboldt-Universität zu Berlin</institution>, <city>Berlin</city>, <country>Germany</country></aff>
<aff id="a4"><label>4</label><institution>Research Training Group “Extrospection” and Berlin School of Mind and Brain, Humboldt-Universität zu Berlin</institution>, <city>Berlin</city>, <country>Germany</country></aff>
<aff id="a5"><label>5</label><institution>Institute of Psychology, Otto von Guericke University</institution>, <city>Mageburg</city>, <country>Germany</country></aff>
<aff id="a6"><label>6</label><institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution>, <city>Leipzig</city>, <country>Germany</country></aff>
<aff id="a7"><label>7</label><institution>German Center for Neurodegenerative Diseases</institution>, <city>Göttingen</city>, <country>Germany</country></aff>
<aff id="a8"><label>8</label><institution>Research Cluster of Excellence “Science of Intelligence”, Technische Universität Berlin</institution>, <city>Berlin</city>, <country>Germany</country></aff>
<aff id="a9"><label>9</label><institution>Collaborative Research Center “Volition and Cognitive Control”, Technische Universität Dresden</institution>, <city>Dresden</city>, <country>Germany</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Xue</surname>
<given-names>Gui</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Moore</surname>
<given-names>Tirin</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University, Howard Hughes Medical Institute</institution>
</institution-wrap>
<city>Stanford</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Correspondence to: <email>j.karolis.degutis@maxplanckschools.de</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-08-27">
<day>27</day>
<month>08</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP99290</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-05-23">
<day>23</day>
<month>05</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-05-23">
<day>23</day>
<month>05</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.04.12.589170"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Degutis et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Degutis et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-99290-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Recent studies have provided evidence for the concurrent encoding of sensory percepts and visual working memory contents (VWM) across visual areas; however, it has remained unclear how these two types of representations are concurrently present. Here, we reanalyzed an open-access fMRI dataset where participants memorized a sensory stimulus while simultaneously being presented with sensory distractors. First, we found that the VWM code in several visual regions did not generalize well between different time points, suggesting a dynamic code. A more detailed analysis revealed that this was due to shifts in coding spaces across time. Second, we collapsed neural signals across time to assess the degree of interference between VWM contents and sensory distractors, specifically by testing the alignment of their encoding spaces. We find that VWM and feature-matching sensory distractors are encoded in separable coding spaces. Together, these results indicate a role of dynamic coding and temporally stable coding spaces in helping multiplex perception and VWM within visual areas.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>visual working memory</kwd>
<kwd>dynamic coding</kwd>
<kwd>neural subspace</kwd>
<kwd>sensory distractor</kwd>
<kwd>multiplexing</kwd>
</kwd-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>A link to the code and data (OSF).
Additional funding statement.
Line numbers of the main text.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>To successfully achieve behavioral goals, humans rely on the ability to remember, update, and ignore information. Visual working memory (VWM) allows for a brief maintenance of visual stimuli that are no longer present within the environment (<xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c3">3</xref>). Previous studies have revealed that the contents of VWM are present throughout multiple visual areas, starting from V1 (<xref ref-type="bibr" rid="c4">4</xref>–<xref ref-type="bibr" rid="c12">12</xref>). These findings raised the question of how areas that are primarily involved in visual perception can also maintain VWM information without interference between the two contents. Recent studies that had participants remember a stimulus while simultaneously being presented with sensory stimuli during the delay period have found supporting evidence that both VWM contents and sensory percepts are multiplexed in occipital and parietal regions (<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c14">14</xref>). However, the mechanism employed in order to segregate bottom-up visual input from VWM contents remains poorly understood.</p>
<p>One proposed mechanism to achieve the separation between sensory and memory representations is dynamic coding (<xref ref-type="bibr" rid="c15">15</xref>–<xref ref-type="bibr" rid="c17">17</xref>): the change of the population code encoding VWM representations across time. Recent work has shown that the format of VWM might not be as persistent and stable throughout the delay as previously thought (<xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c19">19</xref>). Frontal regions display dynamic population coding across the delay during the maintenance of category (<xref ref-type="bibr" rid="c20">20</xref>) and spatial contents in the absence of interference (<xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref>), and also shows dynamic recoding of the memoranda after sensory distraction (<xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c24">24</xref>). The visual cortex in humans displays dynamic coding of contents during high load trials (<xref ref-type="bibr" rid="c25">25</xref>) and during a spatial VWM task (<xref ref-type="bibr" rid="c26">26</xref>). However, it is not yet clear whether dynamic coding of VWM might help evade sensory distraction in human visual areas.</p>
<p>Another line of evidence suggests that perception could potentially be segregated from VWM representations using stable non-overlapping coding spaces (<xref ref-type="bibr" rid="c27">27</xref>). For example, evidence from neuroanatomy indicates that the sensory bottom-up visual pathway primarily projects to the cytoarchitectonic Layer 4 in V1, while feedback projections culminate in superficial and deep layers of the cortex (<xref ref-type="bibr" rid="c28">28</xref>). Functional results are in line with neuroanatomy by showing that VWM signals preferentially activate the superficial and deep layers in humans (<xref ref-type="bibr" rid="c29">29</xref>) and non-human primates (<xref ref-type="bibr" rid="c30">30</xref>), while perceptual signals are more prevalent in the middle layers (<xref ref-type="bibr" rid="c31">31</xref>). In addition to laminar separation, regional multiplexing of multiple items could potentially rely on rotated representations, as seen in memory and sensory representations orthogonally coded in the auditory cortex (<xref ref-type="bibr" rid="c32">32</xref>) and in the storage of a sequence of multiple spatial locations in the prefrontal cortex (PFC) (<xref ref-type="bibr" rid="c33">33</xref>). Non-overlapping orthogonal representations have also been seen in both humans and trained recurrent neural networks as a way of segregating attended and unattended VWM representations (<xref ref-type="bibr" rid="c34">34</xref>–<xref ref-type="bibr" rid="c36">36</xref>).</p>
<p>Here we investigated whether the concurrent presence of VWM and sensory information is compatible with predictions offered by dynamic coding or by stable non-aligned coding spaces. For this, we reanalyzed an open-access fMRI dataset by Rademaker et al. (<xref ref-type="bibr" rid="c8">8</xref>) where participants performed a delayed-estimation VWM task with and without sensory distraction. To investigate dynamic coding we employed a temporal cross-decoding analysis that assessed how well the multivariate code encoding VWM generalizes from one time point to another (<xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c37">37</xref>–<xref ref-type="bibr" rid="c39">39</xref>), and a temporal neural subspace analysis that examined a sensitive way of looking at alignment of neural populations coding for VWM at different time points. To assess the non-overlapping coding hypothesis, we used neural subspaces (<xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c32">32</xref>) to see whether temporally stable representations of the VWM target and the sensory distractor are coded in separable neural populations. Finally, we examined the multivariate VWM code changes during distractor trials when compared to the no-distractor VWM format.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Temporal cross-decoding in distractor and no-distractor trials</title>
<p>In the previously published study (<xref ref-type="bibr" rid="c8">8</xref>) participants completed a VWM task where on a given trial they were asked to remember an orientation of a grating, which they had to then recall at the end of the trial. The delay period was either left blank (no-distractor) or a noise or randomly oriented grating distractor was presented (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). To investigate the dynamics of the VWM code, we examined how the multivariate pattern of activity encoding VWM memoranda changed across the duration of the delay period. To do so, we ran a temporal cross-decoding analysis where we trained a decoder (periodic support vector regression, see (<xref ref-type="bibr" rid="c40">40</xref>)) on the target orientation, separately for each time point and tested on all time points in turn in a cross-validated fashion. If the information encoding VWM memoranda were to have the same code, the trained decoder would generalize to other time points, indicated by similar decoding accuracies on the diagonal and off-diagonal elements of the matrix. However, if the code exhibited dynamic properties, despite information about the memonda being present (above-chance decoding on the diagonal of the matrix), both off-diagonal elements corresponding to a given on-diagonal element would have lower decoding accuracies (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). Such off-diagonal elements are considered an indication of a dynamic code.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Task and temporal cross-decoding.</title>
<p><bold>a)</bold> On each trial an oriented grating was presented for the 0.5 s followed by a delay period of 13 s (<xref ref-type="bibr" rid="c8">8</xref>). In a third of the trials a noise distractor was presented for 11 s during the middle of the delay; in another third another orientation grating was presented; one third of trials had no distractor during the delay. <bold>b)</bold> Illustration of dynamic coding elements. An off-diagonal element had to have a lower decoding accuracy compared to both corresponding diagonal elements (see Methods for details). <bold>c)</bold> Temporal generalization of the multivariate code encoding VWM representations in three conditions across occipital and parietal regions. Across-participant mean temporal cross-decoding of no-distractor trials. Black outlines: matrix elements showing above-chance decoding (cluster-based permutation test; <italic>p</italic> &lt; 0.05). Blue outlines with dots: dynamic coding elements; parts of the cross-decoding matrix where the multivariate code fails to generalize (off-diagonal elements having lower decoding accuracy than their corresponding two diagonal elements; conjunction between two cluster-based permutation tests; <italic>p</italic> &lt; 0.05). <bold>d)</bold> Same as c), but noise distractor trials. <bold>e)</bold> Same as c), but orientation distractor trials. <bold>f)</bold> Dynamicism index; the proportion of dynamic coding elements across time. High values indicate a dynamic non-generalizing code, while low values indicate a generalizing code. Time indicates the time elapsed since the onset of the delay period.</p></caption>
<graphic xlink:href="589170v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We ran the temporal cross-decoding analysis for the three VWM delay conditions: no-distractor, noise distractor and orientation distractor (feature-matching distractor). First, we examined each element of the cross-decoding matrix to test whether decoding accuracies were above chance. In all three conditions and throughout all ROIs, we found clusters where decoding was above chance (<xref rid="fig1" ref-type="fig">Fig. 1c-e</xref>, black outline; nonparametric cluster-permutation test against null; all clusters <italic>p</italic> &lt; 0.05) from as early as 4 s after the onset of the delay period. We found that decoding on the diagonal was highest during no-distractor compared to noise and orientation distractor trials in most regions of interest (ROI; <xref rid="fig4" ref-type="fig">Fig. 4a</xref>).</p>
<p>Second, we examined off-diagonal elements to assess whether there was any indication that they reflected a non-generalizing dynamic code (see Methods for full details). Despite a high degree of temporal generalization, we found dynamic coding clusters in all three conditions. Some degree of dynamic coding was observed in all ROIs but LO2 in the noise distractor and no-distractor trials, while it was only present in V1, V2, V3, V4, and IPS in the orientation distractor condition (<xref rid="fig1" ref-type="fig">Fig. 1c-e</xref>, blue outline). The difference between noise and orientation distractor conditions could not be explained by the amount of information present in each ROI, as the decoding accuracy of the diagonal was similar across all ROIs in both the noise and orientation distractor conditions (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>). We saw a nominally larger number of dynamic coding elements in V1, V2 and V3AB during the noise distractor condition and in V3 during the no-distractor condition (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>).</p>
<p>To qualitatively compare the amount of dynamic coding in the three conditions across the delay period, we calculated a dynamicism index (<xref ref-type="bibr" rid="c22">22</xref>) (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>; see Methods), which measured the multivariate code’s uniqueness at each time point; more precisely, the proportion of dynamic elements corresponding to each diagonal element. High values indicate dynamic code and low values indicate a generalizing code. Across all conditions, most dynamic elements occurred between the encoding and early delay periods (4-8 s), and the late delay and retrieval (14.4-16.8 s). Interestingly, during the noise distractor trials in V1 we also saw dynamic coding during the middle of the delay period; the multivariate code not only changed during the onset and offset of the noise stimulus, but also during its presentation and throughout the extent of the delay.</p>
</sec>
<sec id="s2b">
<title>Dynamics of VWM neural subspaces across time</title>
<p>The temporal cross-decoding analysis revealed more dynamic coding in the early visual cortex primarily during the early and late delay phase and a more generalized coding throughout the delay in higher-order regions. In order to understand the nature of these effects in more detail, we conducted a separate series of analyses that directly assessed the neural subspaces in which the orientations were encoded and how these potentially changed across time. Specifically, we followed a previous methodological framework (<xref ref-type="bibr" rid="c26">26</xref>) and applied a principal component analysis (PCA) to the high-dimensional activity patterns at each time point to identify the two axes that explained maximal variance across orientations (see <xref rid="fig2" ref-type="fig">Fig. 2</xref> and Methods).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Assessing the dynamics of neural subspaces in V1-V3AB.</title>
<p><bold>a)</bold> Schematic illustration of the neural subspace analysis. A given data matrix (voxels x orientation bins) was subjected to a principal components analysis and the first two dimensions were used to define a neural subspace onto which a left-out test data matrix was projected. This resulted in a matrix of two coordinates for each orientation bin and was visualized (see right). The x and y axes indicate the first two principal components. Each color depicts an angular bin. <bold>b)</bold> Schematic illustration of the calculation of an above-baseline principal angle (aPA). A principal angle (PA) is the angle between the 2D PCA-based neural subspaces (as in <bold>a</bold>) for two different time points t<sub>1</sub>, t<sub>2</sub>. A small angle would indicate alignment of coding spaces; an angle of above-baseline would indicate a shift in the coding space. The above-baseline principle angle (aPA) is the angle for a comparison between two time points (t<sub>1</sub>, t<sub>2</sub>) minus the angle between cross-validated pairs of the same time points. <bold>c)</bold> Each row shows a projection that was estimated for one of two time ranges (middle and late delay) and then applied to all time points (using independent, split-half cross-validated data). Opacity increases from early to late time points. For visualization purposes the subspaces were estimated on a participant-aggregated ROI (<xref ref-type="bibr" rid="c26">26</xref>). <xref rid="figs1" ref-type="fig">Fig. S1</xref> depicts the same projections as neural trajectories. <bold>d)</bold> aPA between all pairwise time point comparisons (nonparametric permutation test against null; FDR-corrected <italic>p</italic> &lt; 0.05) averaged across 1,000 split-half iterations. Corresponding <italic>p</italic>-values found in <xref rid="tbls1" ref-type="table">Supplementary Table 1</xref>.</p></caption>
<graphic xlink:href="589170v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>First, we visualized the consistency of the neural subspaces across time. For this, we computed low-dimensional 2D neural subspaces for a given time point and projected left-out data from six time points during the delay onto this subspace (<xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c32">32</xref>). A projection of data from a single time point resulted in four orientation bin values placed within the subspace (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>, colored circles indicate orientation). Taking into account projected data from all timepoints, if the VWM code were generalizing, we would see a clustering of orientation points in a subspace; however, if orientation points were scattered around the neural subspace, this would show a non-generalizing code.</p>
<p>We examined the projections in a combined ROI spanning V1-V3AB aggregated across participants. We projected left-out data from all six time point bins onto subspaces generated from the early (7.2 s), middle (12 s), and late (16.8 s) time point data for each of the three conditions. Overall, the results showed generalization across time with some exceptions (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>, <xref rid="figs1" ref-type="fig">Fig. S1</xref>). The clustering of orientation bins in the no-distractor condition was most pronounced (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>). In contrast, the noise distractor trials showed a resemblance of some degree of dynamic coding, as seen by less variance explained by early time points projected onto the middle subspace and the early and middle time points projected onto late subspace (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>, <xref rid="figs1" ref-type="fig">Fig. S1</xref>).</p>
<p>To quantify the visualized changes, we measured the alignment between each pair of subspaces by calculating the above-baseline principal angle (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>) within the combined V1-V3AB ROI. The above-baseline principal angle (aPA) measures the alignment between the 2D subspaces encoding the VWM representations: the higher the angle, the smaller the alignment between two subspaces and an indication of a changed neural coding space. Unlike in the projection of data from time points, the aPA was calculated participant-wise. Using a split-half approach, we measured the aPA between each split-pair of subspaces and subtracted the angles measured within each of the subspaces with the latter acting as a null baseline.</p>
<p>All three conditions showed significant aPAs (<xref rid="fig2" ref-type="fig">Fig. 2d</xref>; cyan stars; permutation test; <italic>p</italic> &lt; 0.05, FDR-corrected). Corresponding to the results from the cross-decoding analysis, the early (4.8s) and late (16.8) delay subspaces showed the highest number of significant pairwise aPAs in all conditions, with noise distractor trials having all pairwise aPAs including the early and late subspaces being significant. The three conditions each had two significant aPAs between timepoints in the middle of the delay period.</p>
</sec>
<sec id="s2c">
<title>Alignment between distractor and target subspaces in orientation distractor trials</title>
<p>Next, we assessed any similarity in encoding between the memorized orientation targets and the orientation distractors by focusing on those trials where both occurred. First, we examined whether the encoding of the sensory distractor is stable across its entire presentation duration (1.5 s - 12.5 s after target onset) using the same approach as for the VWM target (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>). We found stable coding of the distractor in all ROIs with only a few dynamic elements in V2 and V3 (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>, <xref rid="figs2" ref-type="fig">Fig. S2</xref>). We then assessed whether the sensory distractor had a similar code to the VWM target by examining whether the multivariate code across time generalizes from the target to the distractor and vice versa. When cross-decoded, the sensory distractor (<xref rid="fig3" ref-type="fig">Fig. 3c</xref>) and target orientation (<xref rid="fig3" ref-type="fig">Fig. 3b</xref>) had lower decoding accuracies in the early visual cortex compared to when trained and tested on the same label-type, indicative of a non-generalizing code. Such a difference was not seen in higher-order visual regions, as the decoding of the sensory distractor was low to begin with (<xref rid="figs2" ref-type="fig">Fig. S2</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Generalization between target and distractor codes in orientation distractor VWM trials in V1-V3AB.</title>
<p><bold>a)</bold> Across-participant mean temporal cross-decoding of the sensory distractor. Black outlines: matrix elements showing above-chance decoding (cluster-based permutation test; <italic>p</italic> &lt; 0.05). Blue outlines with dots: dynamic coding element (conjunction between two cluster-based permutation tests; <italic>p</italic> &lt; 0.05). <bold>b)</bold> Same as a), but the decoder was trained on the target and tested on the sensory distractor in orientation VWM trials. <bold>c)</bold> Same as a), but trained on the sensory distractor and tested on the target. See <xref rid="figs2" ref-type="fig">Fig. S2</xref> for ROIs from V4-LO2. <bold>d)</bold> Left: projection of left-out target (green) and sensory distractor (gray) onto an orientation VWM target neural subspace. Right: same as left, but the projections are onto the sensory distractor subspace. <bold>e)</bold> Principal angle between the sensory distractor and orientation VWM target subspaces (<italic>p</italic> = 0.0297, one-tailed permutation test of sample mean). Average across 1,000 split-half iterations. Errorbars indicate ± SEM across participants.</p></caption>
<graphic xlink:href="589170v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Since we found minimal dynamics in the encoding of the distractor (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>) and target (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>), we focused on temporally stable neural subspaces that encoded the target and sensory distractor. We computed stable neural subspaces where we disregarded the temporal variance by averaging across the whole delay period and binned the trials either based on the target orientation (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>, left subpanel) or the distractor orientation (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>, right subpanel). We then projected left-out data binned based on the target (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>, green quadrilateral) or the distractor (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>, gray quadrilateral). This projection provided us with both a baseline (as when training and testing on the same label) and a cross-generalization. Unsurprisingly, the target subspace explained the left-out target data well (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>, left subpanel, green quadrilateral); however, the target subspace explained less variance of the left-out distractor data (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>, left subpanel, gray quadrilateral), as qualitatively seen from the smaller spread of the sensory distractor orientations. A similar but less pronounced dissociation between projections was seen in the distractor subspace (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>, left, quadrilateral in green) with the distractor subspace better explaining the left-out distractor data. We quantified the difference between the target and distractor subspaces and found a significant aPA between them (<italic>p</italic> = 0.0297, one-tailed nonparametric permutation test; <xref rid="fig3" ref-type="fig">Fig. 3e</xref>). These results provide evidence for the presence of separable stable neural subspaces that might enable the multiplexing of VWM and perception across the extent of the delay period.</p>
</sec>
<sec id="s2d">
<title>Impact of distractors on VWM multivariate code</title>
<p>To further assess the impact of distractors on the available VWM information, we examined the decoding accuracies of distractor and no-distractor trials across time. Decoding accuracy was higher in the no-distractor trials compared to both orientation and noise distractor trials across all ROIs, but IPS (<xref rid="fig4" ref-type="fig">Fig. 4a, red and blue lines</xref>, <italic>p</italic> &lt; 0.05, cluster permutation test) across several stages of the delay period. To further assess how distractors affected the delay period information, we increased sensitivity by collapsing across it, because time courses were comparable in all conditions (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>). To assess to which degree VWM encoding generalized from no-distractor to distractor trials, we trained a decoder on no-distractor trials and tested it on both types of distractor trials (<xref rid="fig4" ref-type="fig">Fig. 4b</xref> noise- and orientation-cross). We expressed the decoding accuracy of each distractor condition as a proportion of the decoding accuracy in the no-distractor condition. Values close to one indicate comparable information, while values below one mean the decoder does not generalize well. We found that the cross-decoding accuracies were significantly lower than the no-distractor in all ROIs but V4 (in both noise and orientation) and LO2 (only noise). Thus, in most areas the decoder did not generalize well from the no-distractor to distractor conditions. However, the total amount of information in distractor trials was generally slightly lower (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>). Thus, we also compared the generalization to a decoder trained and tested on the same distractor condition (<xref rid="fig4" ref-type="fig">Fig. 4b</xref> noise- and orientation-within), which might thus be able to extract more information. We found that indeed information recovered in areas V2 and V3AB in the noise distractor condition (<xref rid="fig4" ref-type="fig">Fig. 4b</xref>, pairwise permutation test). Thus, there was more information in the noise distractor condition, but it was not accessible to a decoder trained only on no-distractor trials. Additionally, a temporal cross-decoding analysis where all training time points were no-distractor trials had less dynamic coding in early visual regions (<xref rid="figs3" ref-type="fig">Fig. S3</xref>) when compared to the temporal cross-decoding matrix when trained and tested on noise distractor trials (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>). These results indicate a change in the VWM format between the noise distractor and no-distractor trials.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Cross-decoding between distractor and no-distractor conditions.</title>
<p><bold>a)</bold> Decoding accuracy (feature continuous accuracy; FCA) across time for train and test on no-distractor trials (purple), train and test on noise distractor trials (dark green) and train and test on orientation distractor trials (light green). Horizontal lines indicate clusters where there is a difference between two time courses (all clusters <italic>p</italic> &lt; 0.05; nonparametric cluster permutation test, see color code on the right). <bold>b)</bold> Decoding accuracy as a proportion of no-distractor decoding estimated on the averaged delay period (4-16.8s). Nonparametric permutation tests compared the decoding accuracy of each analysis to the no-distractor decoding baseline (indicated as a dashed line) and between a decoder trained and tested on distractor trials (noise- or orientation-within) and a decoder trained on no-distractor trials and tested on distractor trials (noise or orientation-cross). FDR-corrected across ROIs. * <italic>p</italic> &lt; 0.05, *** <italic>p</italic> &lt; 0.001. Corresponding <italic>p</italic>-values found in <xref rid="tbls2" ref-type="table">Supplementary Table 2</xref>.</p></caption>
<graphic xlink:href="589170v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We examined the dynamics of visual working memory (VWM) with and without distractors and explored the impact of sensory distractors on the coding spaces of VWM contents in visual areas by reanalyzing previously published data (<xref ref-type="bibr" rid="c8">8</xref>). Participants completed a task during which they had to maintain an orientation stimulus in VWM. During the delay period either no distractor, an orientation distractor, or a noise distractor were presented. We assessed two potential mechanisms that could help concurrently maintain the superimposed sensory and memory representations. First, we examined whether changes were observable in the multivariate code for memory contents across time, which we term dynamic coding. For this we used two different analyses: temporal cross-classification and a direct assessment of angles between coding spaces. We found evidence for dynamic coding in all conditions, but there were differences in these dynamics between conditions and regions. Dynamic coding was most pronounced during the noise distractor trials in early visual regions. Second, we assessed the complementary question of temporally stable coding spaces. We computed the stable neural subspaces by averaging across the delay period. We saw that coding of the VWM target and concurrent sensory distractors occurred in different stable neural subspaces. Finally, we observed that the format of the multivariate VWM code during the noise distraction differs from the VWM code when distractors were not present.</p>
<p>Dynamic encoding of VWM contents has been previously repeatedly examined. Temporal cross-decoding analyses have been used in a number of non-human primate electrophysiology and human fMRI studies (<xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c41">41</xref>). Spaak et al. (<xref ref-type="bibr" rid="c22">22</xref>) found dynamic coding in the non-human primate PFC during a spatial VWM task. They observed a change in the multivariate code between different stages; specifically a first shift between the encoding and maintenance periods, and also a second shift between the maintenance and retrieval periods. The initial transformation between the encoding and maintenance periods might recode the percept of the target into a stable VWM representation, whereas the second might transform the stable memoranda into a representation suited for initiation of motor output. A similar dynamic coding pattern was also observed in human visual regions using neuroimaging (<xref ref-type="bibr" rid="c26">26</xref>). In this study, in all three conditions we find a comparable pattern of results, where the multivariate code changes between the early delay and middle delay, and middle delay and late delay periods.</p>
<p>When noise distractors are added to the delay period we find evidence of additional coding shifts in V1 during the middle of the delay. Previous research in non-human primates has shown that the presentation of a distractor induces a change in multivariate encoding for VWM in lateral PFC (lPFC) (<xref ref-type="bibr" rid="c23">23</xref>). More precisely, a lack of generalization was observed between the population code encoding VWM before the presentation of a distractor (first half of the delay) and after its presentation (the second half). Additionally, continuous shifts in encoding have been observed in the extrastriate cortex throughout the extent of the delay period when decoding multiple remembered items at high VWM load (<xref ref-type="bibr" rid="c25">25</xref>). The dynamic code has been interpreted to enable multiplexing of representations when the visual cortex is overloaded by the maintenance of multiple stimuli at once. Future research could examine how properties of the distractor and of the target stimulus could interact to lead to dynamic coding. One intriguing hypothesis is that distractors that perturb the activity of feature channels that are used to encode VWM representations induce changes in its coding space over time. It is important to note that in this experiment, the activation of the encoded target features was highest for the noise stimulus. Thus the shared spatial frequencies between noise distractor and the VWM contents potentially contribute to a more pronounced dynamic coding effect.</p>
<p>In a complementary analysis we directly assessed subspaces in which orientations were encoded in VWM. We defined the subspaces for three different time windows, early, middle and late. We find no evidence that the identity of orientations is confusable across time, e.g. we do not observe 45° at one given time point being recoded as 90° from a different time point. Such dynamics have been previously observed in the rotation of projected angles within a fixed neural subspace (<xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c34">34</xref>). Rather, we find a decreased generalization between neural subspaces at different time points, as previously observed in a spatial VWM task (<xref ref-type="bibr" rid="c26">26</xref>). These results suggest that the temporal dynamics across the VWM trial periods are driven by changes in the coding subspace of VWM. We do observe a preservation of the topology of the projected angles, as more similar angles remained closer together (e.g. the bin containing 45° was always closer to the bin containing 0° and 90°). Such a topology has been seen in V4 during a color perception task (<xref ref-type="bibr" rid="c42">42</xref>).</p>
<p>We also find evidence that the VWM contents are encoded in a different way depending on whether a noise distractor is presented or not. The decoder trained on no-distractor trials does not generalize well, presumably because it fails to fully access all the information present in noise distractor trials. If the decoders are trained directly on the distractor conditions the VWM related information is much higher. Additionally, we see that the code generalizes better across time when training on no-distractor trial time points and testing on noise distractor trials. This may imply that by training our decoder on the no-distractor trials we are able to uncover an underlying stable population code encoding VWM in noise distractor trials. Consistent with this finding, Murray et al. (<xref ref-type="bibr" rid="c21">21</xref>) demonstrated that subspaces derived on the delay period could still generalize to the more dynamic encoding and retrieval periods, albeit not perfectly.</p>
<p>Interestingly, we found limited dynamic coding in the orientation distractor condition; primarily a change in the code between the early delay and middle delay periods was observed. Nonetheless, we find distinct temporally stable coding spaces in which sensory distractors and memory targets are encoded. These results correspond to prior research demonstrating a rotated format between perception and memory representations (<xref ref-type="bibr" rid="c32">32</xref>), attended and unattended VWM representations in both humans and recurrent-neural networks trained on a 2-back VWM (<xref ref-type="bibr" rid="c34">34</xref>) and serial retro-cueing tasks (<xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c43">43</xref>). Additionally, similar rotation dynamics have been observed between multiple spatial VWM locations stored in the non-human primate lPFC (<xref ref-type="bibr" rid="c33">33</xref>). Considering the consistency of these results across different paradigms, we speculate that separate coding spaces might be a general mechanism of how feature-matching items can be concurrently multiplexed within visual regions. With growing evidence of the relationship between VWM capacity and neural resources available within the visual cortex (<xref ref-type="bibr" rid="c44">44</xref>–<xref ref-type="bibr" rid="c46">46</xref>), further research could examine the number of feature-matching items that can be stored in non-aligned coding spaces.</p>
<p>It remains to be seen whether the degree of change or rotation between subspaces correlates with behavior. In this experiment, we do not observe a behavioral deficit in the feature-matching orientation distractor trials (<xref ref-type="bibr" rid="c8">8</xref>). Yet there is evidence from behavioral and neural studies that show interactions between perception and VWM: feature-matching distractors behaviorally bias retrieved VWM contents (<xref ref-type="bibr" rid="c47">47</xref>,<xref ref-type="bibr" rid="c48">48</xref>); VWM representations influence perception (<xref ref-type="bibr" rid="c49">49</xref>–<xref ref-type="bibr" rid="c52">52</xref>); neural visual VWM representations in the early visual cortices are biased towards distractors (<xref ref-type="bibr" rid="c53">53</xref>); and the fidelity of VWM neural representations within the visual cortex negatively correlates with behavioral errors when recalling VWM during a sensory distraction task (<xref ref-type="bibr" rid="c54">54</xref>). In cases where a distractor does induce a drop in recall accuracy or biases the recalled VWM target, VWM and the sensory distractor neural subspaces might overlap more.</p>
<p>To our surprise, we did not observe a significant difference in the coding format of VWM between orientation distractor and no-distractor trials. Our initial expectation was that the VWM coding might undergo changes due to the target representation avoiding the distractor stimulus. However, the presence of a generalizing code between no-distractor and orientation distractor trials, along with the non-aligned coding spaces between the target and distractor in the orientation trials, suggests an alternative explanation. We suggest that the sensory distractor stimulus occupies a distinct coding space throughout its presentation during the delay, while the coding space of the target remains the same in both orientation and no-distractor trials. Layer-specific coding differences in perception and VWM might explain these findings (<xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c55">55</xref>). Specifically, the sensory distractor neural subspace might predominantly reside in the bottom-up middle layers of early visual cortices, while the neural subspace encoding VWM might primarily occupy the superficial and deep layers.</p>
<p>We provide evidence for two types of mechanisms found in visual areas during the presence of both VWM and sensory distractors. First, our findings show dynamic coding of VWM within the human visual cortex during sensory distraction and indicate that such activity is not only present within the lPFC. Second, we find that VWM and feature-matching sensory distractors are encoded in shifted coding spaces. Taking into account previous findings, we posit that different coding spaces within the same region might be a more general mechanism of segregating feature-matching stimuli. In sum, these results provide possible mechanisms of how VWM and perception are concurrently present within visual areas.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Participants, stimuli, procedure, and preprocessing</title>
<p>The following section is a brief explanation of parts of the methods covered in Rademaker et al. (<xref ref-type="bibr" rid="c8">8</xref>). Readers may refer to that paper for details. We reanalyzed data from Experiment 1.</p>
<p>Six participants performed two tasks while in the scanner: a VWM task and a perceptual localizer task. In the perceptual localizer task, either a donut-shaped or a circle-shaped grating was presented in 9 second blocks. The participants had to respond whenever the grating dimmed. There were a total of 20 donut-shaped and 20 circle-shaped gratings in one run. Participants completed a total of 15-17 runs.</p>
<p>The visual VWM task began with the presentation of a colored 100% valid cue which indicated the type of trial: no-distractor, orientation distractor, or noise distractor. Following the cue, the target orientation grating was presented centrally for 500 ms, followed by a 13 s delay period. In the trials with the distractor, a stimulus of the same shape and size as the target grating was presented centrally for 11 s in the middle of the delay period (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). The orientation and noise distractors reversed contrast at 4 Hz. At the end of the delay, a probe stimulus bar appeared at a random orientation. The participants had to align the bar to the target orientation and had to respond in 3 s.</p>
<p>The orientations for the VWM sample were pseudo-randomly chosen from six orientation bins each consisting of 30 orientations. The orientation distractor and sample were counterbalanced in order not to have the same orientation presented as a distractor. Each run consisted of four trials of each condition. Across three sessions participants completed 27 runs of the task resulting in a total of 108 trials per condition.</p>
<p>The data were acquired using a simultaneous multi-slice EPI sequence with a TR of 800 ms, TE of 35 ms, flip angle of 52°, and isotropic voxels of 2 mm. The data were preprocessed using FreeSurfer and FSL and time-series were z-scored across time for each voxel.</p>
</sec>
<sec id="s4b">
<title>Voxel selection</title>
<p>We used the same regions of interest (ROI) as in Rademaker et al. (<xref ref-type="bibr" rid="c8">8</xref>), which were derived using retinotopic mapping. In contrast to the original study, we reduced the size of our ROIs by selecting voxels that reliably responded to both the donut-shaped orientation perception task and the no-distractor VWM task. In order to select reliably activating voxels, we calculated four tuning functions for each voxel: two from the perceptual localizer and two from the no-distractor VWM task. The tuning functions spanned the continuous feature space in bins of 30°. Thus, to calculate the tuning functions, we ran a split-half analysis using stratified sampling where we binned all trials into six bins (of 30°). For both halves, tuning functions were estimated using a GLM that included six orientation regressors (one for each bin) and assumed an additive noise component independent and identically distributed across trials. We calculated Pearson correlations between the no-distractor memory and the perception tuning functions across the six parameter estimates extracted from the GLM, thus generating one memory-memory and one perception-perception correlation coefficient for each voxel.</p>
<p>The same analysis was additionally performed 1,000 times on randomly permuted orientation labels to generate a null distribution for each participant and each ROI. These distributions were used to check for the reliability of voxel activation to perception and no-distractor VWM. After performing Fisher z-transformation on the correlations, we selected voxels that had a value above the 75th percentile of the null distributions in both the memory-memory and perception-perception correlations. This population of voxels was then used for all subsequent analyses. IPS included reliable voxels from retinotopically derived IPS0, IPS1, and IPS2.</p>
</sec>
<sec id="s4c">
<title>Periodic support vector regression</title>
<p>We used periodic support vector regression (pSVR) to predict the target orientation from the multivariate BOLD activity (<xref ref-type="bibr" rid="c40">40</xref>). PSVR uses a regression approach to estimate the sine and cosine components of a given orientation independently and therefore accounts for the circular nature of stimuli. In order to have a proper periodic function, orientation labels from the range [0°, 180°) were projected into the range [0, 2π).</p>
<p>We used the support vector regression algorithm using a non-linear radial basis function (RBF) kernel implemented in LIBSVM (<xref ref-type="bibr" rid="c56">56</xref>) for orientation decoding. Specifically, sine and cosine components of the presented orientations were predicted based on multivariate fMRI signals from a set of voxels at specific time points within a trial (see <italic>Temporal Generalization</italic>). In each cross-validation fold, we rescaled the training data voxel activation into the range [0, 1] and applied the training data parameters to rescale the test data. For each participant we had a total of three iterations in our cross-validation, where we trained on two thirds (i.e. two sessions) and tested on one third of the data (i.e. the left-out session). We selected three iterations in order to mitigate training and test data leakage (see <italic>Temporal Generalization</italic>).</p>
<p>After pSVR-based analysis, reconstructed orientations were obtained by plugging the predicted sine and cosine components into the four-quadrant inverse tangent:
<disp-formula id="eqn1">
<graphic xlink:href="589170v2_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>x<sub>p</sub></italic> and <italic>y<sub>p</sub></italic> are pSVR outputs in the test set. Prediction accuracy was measured as the trial-wise absolute angular deviation between predicted orientation and actual orientation:
<disp-formula id="eqn2">
<graphic xlink:href="589170v2_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where θ is the labeled orientation and <italic>θ<sub>p</sub></italic> is the predicted orientation. This measure was then transformed into a trial-wise feature continuous accuracy (FCA) (<xref ref-type="bibr" rid="c57">57</xref>) as follows:
<disp-formula id="eqn3">
<graphic xlink:href="589170v2_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The final across-trial accuracy was the mean of the trial-wise FCAs. Mean FCA was calculated across predicted orientations from all test sets after cross-validation was complete. The FCA is an equivalent measurement to standard accuracy measured in decoding analyses falling into the range between 0 and 100%, but extended to the continuous domain. In the case of random guessing, the expected angular deviation is π/2, resulting in chance-level FCA at 50%.</p>
</sec>
<sec id="s4d">
<title>Temporal cross-decoding</title>
<p>To determine the underlying stability of the VWM code, we ran a temporal cross-decoding analysis using pSVR (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). We trained on data from a given time point and then predicted orientations for all time points, using the presented targets as labels. We trained on two-thirds of the trials per iteration and tested on the left-out third. Training and test data were never taken from the same trials, both when testing on the same and different time points.</p>
<p>We used a cluster-based approach to test for significance for above-chance decoding clusters (<xref ref-type="bibr" rid="c58">58</xref>). To determine whether the size of the cluster of the above-chace values was significantly larger than chance, we calculated a summed t-value for each cluster. We then generated a null distribution by randomly permuting the sign of the estimated above-chance accuracy (each FCA value was subtracted by 50%, such that 0 corresponds to chance level) of all components within the temporal cross-decoding matrix. We calculated the summed t-value for the largest randomly occurring above-chance cluster. This procedure was repeated 1000 times to estimate a null distribution. The empirical summed t-value of each cluster was then compared to the null distribution to determine significance (<italic>p</italic> &lt; 0.05; without control of multiple cluster comparisons).</p>
<p>Dynamic coding clusters were defined as elements within the temporal cross-decoding matrix where the multivariate code at a given time point did not fully generalize to another time point; in other words, an off-diagonal element was significantly smaller in accuracy compared to its two corresponding on-diagonal elements (<italic>a<sub>ij</sub> &lt; a<sub>ii</sub></italic> and <italic>a<sub>ij</sub> &lt; a<sub>jj</sub></italic>, <xref rid="fig1" ref-type="fig">Fig. 1b</xref>). In order to test for significance of these clusters, we ran two cluster-permutation tests as done in previous studies to define dynamic clusters (<xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c26">26</xref>). In each test, we subtracted one or the other corresponding diagonal elements from the off-diagonal elements (<italic>a<sub>ij</sub> – a<sub>ii</sub></italic> and <italic>a<sub>ij</sub> – a<sub>jj</sub></italic>). We then ran the same sign permutation test as for the above-chance decoding cluster for both comparisons. An off-diagonal element was deemed dynamic, if both tests were significant (<italic>p</italic> &lt; 0.05) and it was part of the above-chance decoding cluster.</p>
<p>Following (<xref ref-type="bibr" rid="c22">22</xref>), we also computed the dynamicism index as a proportion of elements across time that were dynamic. Specifically, we calculated the proportion of (off-diagonal) dynamic elements corresponding to a diagonal time point in both columns (corresponding to the test time points) and rows (corresponding to the train time points) of the temporal cross-decoding matrix.</p>
</sec>
<sec id="s4e">
<title>Neural subspaces</title>
<p>We adapted the method from (<xref ref-type="bibr" rid="c26">26</xref>) to calculate two-dimensional neural subspaces encoding VWM information at a given time point. To do so, we used principal component analysis (PCA). To maximize power, we binned trial-wise fMRI activations into four equidistant bins of 45 degrees and averaged the signal across all trials within a bin (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). The data matrix <bold>X</bold> was defined as a p×ν matrix where <italic>p</italic> = 4 was the four orientation bins, and <italic>ν</italic> was the number of voxels. We mean-centered the columns (i.e. each voxel) of the data matrix.</p>
<p>This analysis focused on the time points from 4 s to 17.6 s after delay onset. The first TRs were not used since the temporal cross-decoding results showed no above-chance decoding. We averaged across every three TRs leading to six non-overlapping temporal bins resulting in six <bold>X</bold> matrices. We calculated the principal components (PCs) using eigendecomposition of the covariance matrix for each <bold>X</bold> and defined the matrix <bold>V</bold> using the two largest eigenvalues as a <italic>ν</italic> × 2 matrix, resulting in six neural subspaces, one for each non-overlapping temporal bin.</p>
</sec>
<sec id="s4f">
<title>Neural subspaces across time</title>
<p>For visualization purposes, we used three out of the total of six neural subspaces from the following time points: early (7.2 s), middle (12 s), and late (16.8 s). Following the aforementioned procedure, these subspaces were calculated on half of the trials, as we projected the left-out data onto the subspaces. The left-out data were binned into six temporal bins between 4 s and 17.6 s after target onset with no overlap just like in the calculation of the six subspaces. The projection resulted in a <italic>p</italic> × 2 matrix <bold>P</bold> for each projected time bin (resulting in a total of six <bold>P</bold> matrices). We use distinct colors to plot the temporal trajectories of each orientation bin across time in a 2D subspace flattened (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>) and not flattened (<xref rid="figs1" ref-type="fig">Fig. S1</xref>) across the time dimension. Importantly, the visualization analysis was done on a combined participant-aggregated V1-V3AB region, which included all reliable voxels across the four regions and all six participants (see <italic>Voxel Selection</italic>).</p>
<p>To measure the alignment between coding spaces at different times, we calculated an above-baseline principal angle (aPA) between all subspaces (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>). We used the MATLAB function <bold><monospace>subspace</monospace></bold> for an implementation of the method proposed by (<xref ref-type="bibr" rid="c59">59</xref>) to measure the angle between two <bold>V</bold> matrices. This provided us with a possible principal angle between 0-90°; the higher the angle, the larger the difference between the two subspaces. In order to avoid overfitting and as in the visualization analysis, we used a split-half approach to compute the aPA between subspaces. Half of the binned trials were used to calculate <bold>V<sub>i,A</sub></bold> and <bold>V<sub>j,A</sub></bold> and half for <bold>V<sub>i,B</sub> V<sub>j,B</sub></bold>, where <bold>A</bold> and <bold>B</bold> refer to the two halves of the split and and <bold>i</bold> and <bold>j</bold> refer to the two time bins compared. For significance testing, the within-subspace angle (the angle between two splits of the data within a given temporal bin (i.e. <bold>V<sub>i,A</sub></bold> and <bold>V<sub>i,B</sub></bold>)) was subtracted from the between-subspace PA (the angle between two different temporal bins (e.g. <bold>V<sub>i,A</sub></bold> and <bold>V<sub>j,B</sub></bold>)). Unlike the visualization analysis, the PA was calculated per participant 1,000 times using different splits of the data on a combined V1-V3AB region that included the reliable voxels across the four regions (see <italic>Voxel Selection</italic>). The final aPA value was an average across all iterations for each participant.</p>
</sec>
<sec id="s4g">
<title>Sensory distractor and orientation VWM target neural subspaces</title>
<p>For the orientation VWM target and sensory distractor neural subspace, we followed the aforementioned subspace analysis, but instead of calculating subspaces on six temporal bins, we averaged across the 4-17.6 s delay period and calculated a single subspace. As in the previous analysis, we split the orientation VWM trials in half. We then binned the trials either based on the target orientation or the sensory distractor. For visualization purposes, we projected the left-out data averaged based on the sensory distractor and the target onto subspaces derived from both the sensory distractor and target subspaces. As in the previous visualization, the analysis was run on a participant-aggregated V1-V3AB region.</p>
<p>To calculate the aPA we had the following subspaces: <bold>V<sub>Target,A</sub></bold>, <bold>V<sub>Dist,A</sub></bold>, <bold>V<sub>Target,B</sub></bold> and <bold>V<sub>Dist,B</sub></bold>, where the subspaces were calculated on trials binned either based on the target orientation or the sensory distractor. The aPA was calculated by subtracting the within-subspace angle (<bold>V<sub>Target,A</sub></bold> and <bold>V<sub>Target,B</sub></bold>, <bold>V<sub>Dist,A</sub></bold> and <bold>V<sub>Dist,B</sub></bold>) from the sensory distractor and working memory angle (<bold>V<sub>Target,A</sub></bold> and <bold>V<sub>Dist,B</sub></bold>, <bold>V<sub>Target,B</sub></bold> and <bold>V<sub>Dist,A</sub></bold>). The split-half aPA analysis was performed 1,000 times and the final value was an average across these iterations for each participant.</p>
</sec>
</sec>
<sec id="s5">
<title>Data availability</title>
<p>The preprocessed data are shared open-access <ext-link ext-link-type="uri" xlink:href="https://osf.io/dkx6y/">https://osf.io/dkx6y/</ext-link>. The analysis scripts and results are shared <ext-link ext-link-type="uri" xlink:href="https://osf.io/jq3ma/?view_only=8fcef0ef33a047e693c4102e3794319f">https://osf.io/jq3ma/?view_only=8fcef0ef33a047e693c4102e3794319f</ext-link> (the link will become public (e.g. not view only) when the manuscript is published).</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>J.K.D. was funded by the Max Planck Society and BMBF (as part of the Max Planck School of Cognition). J.D.H. was supported by the Deutsche Forschungsgemeinschaft (DFG, Exzellenzcluster Science of Intelligence); SFB 940 “Volition and Cognitive Control”; and SFB-TRR 295 “Retuning dynamic motor network disorders using neuromodulation”. S.W. was supported by Deutsche Forschungsgemeinschaft (DFG) Research Training Group 2386 451 and EXC 2002/1 “Science of Intelligence.” Open access funding provided by Max Planck Society. We thank Rosanne Rademaker, Chaipat Chunharas, and John Serences for collecting and sharing their data open access, without which this reanalysis would not have been possible. We also thank Rosanne Rademaker, Michael Wolff, Amir Rawal, and Maria Servetnik for extensive discussions of the results. We also thank Vivien Chopurian and Thomas Christophel for their feedback on the manuscript.</p>
</ack>
<sec id="s6">
<title>Author contributions</title>
<p>Conceptualization, J.K.D.; Methodology, J.K.D, S.W., J.S., J.-D.H.; Formal Analysis, J.K.D.; Software, J.K.D, S.W., J.S.; Visualization, J.K.D.; Funding Acquisition, J.K.D., J.-D.H.; Writing - Original Draft Preparation, J.K.D.; Writing – Review &amp; Editing, J.K.D., S.W., J.S., J.-D.H. Supervision, J.-D.H.</p>
</sec>
<sec id="s7">
<title>Declaration of interests</title>
<p>The authors declare no competing interests.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Curtis</surname> <given-names>CE</given-names></string-name>, <string-name><surname>D’Esposito</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Persistent activity in the prefrontal cortex during working memory</article-title>. <source>Trends Cogn Sci</source>. <year>2003</year> <month>Sep</month>;<volume>7</volume>(<issue>9</issue>):<fpage>415</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></string-name></person-group>. <article-title>Cellular basis of working memory</article-title>. <source>Neuron</source>. <year>1995</year> <month>Mar</month> 1;<volume>14</volume>(<issue>3</issue>):<fpage>477</fpage>–<lpage>85</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>D’Esposito</surname> <given-names>M</given-names></string-name>, <string-name><surname>Postle</surname> <given-names>BR</given-names></string-name></person-group>. <article-title>The Cognitive Neuroscience of Working Memory</article-title>. <source>Annu Rev Psychol</source>. <year>2015</year> <month>Jan</month> 3;<volume>66</volume>(<issue>1</issue>):<fpage>115</fpage>–<lpage>42</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fuster</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Alexander</surname> <given-names>GE</given-names></string-name></person-group>. <article-title>Neuron Activity Related to Short-Term Memory</article-title>. <source>Science</source>. <year>1971</year> <month>Aug</month> <day>13</day>;<volume>173</volume>(<issue>3997</issue>):<fpage>652</fpage>–<lpage>4</lpage>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harrison</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Tong</surname> <given-names>F</given-names></string-name></person-group>. <article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title>. <source>Nature</source>. <year>2009</year> Apr;<volume>458</volume>(<issue>7238</issue>):<fpage>632</fpage>–<lpage>5</lpage>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Christophel</surname> <given-names>TB</given-names></string-name>, <string-name><surname>Hebart</surname> <given-names>MN</given-names></string-name>, <string-name><surname>Haynes</surname> <given-names>JD</given-names></string-name></person-group>. <article-title>Decoding the Contents of Visual Short-Term Memory from Human Visual and Parietal Cortex</article-title>. <source>J Neurosci</source>. <year>2012</year> <month>Sep</month> 19;<volume>32</volume>(<issue>38</issue>):<fpage>12983</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ester</surname> <given-names>EF</given-names></string-name>, <string-name><surname>Sprague</surname> <given-names>TC</given-names></string-name>, <string-name><surname>Serences</surname> <given-names>JT</given-names></string-name></person-group>. <article-title>Parietal and Frontal Cortex Encode Stimulus-Specific Mnemonic Representations during Visual Working Memory</article-title>. <source>Neuron</source>. <year>2015</year> <month>Aug</month>;<volume>87</volume>(<issue>4</issue>):<fpage>893</fpage>– <lpage>905</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rademaker</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Chunharas</surname> <given-names>C</given-names></string-name>, <string-name><surname>Serences</surname> <given-names>JT</given-names></string-name></person-group>. <article-title>Coexisting representations of sensory and mnemonic information in human visual cortex</article-title>. <source>Nat Neurosci</source>. <year>2019</year> <month>Aug</month>;<volume>22</volume>(<issue>8</issue>):<fpage>1336</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Riggall</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Postle</surname> <given-names>BR</given-names></string-name></person-group>. <article-title>The Relationship between Working Memory Storage and Elevated Activity as Measured with Functional Magnetic Resonance Imaging</article-title>. <source>J Neurosci</source>. <year>2012</year> <month>Sep</month> 19;<volume>32</volume>(<issue>38</issue>):<fpage>12990</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Serences</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Ester</surname> <given-names>EF</given-names></string-name>, <string-name><surname>Vogel</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Awh</surname> <given-names>E</given-names></string-name></person-group>. <article-title>Stimulus-Specific Delay Activity in Human Primary Visual Cortex</article-title>. <source>Psychol Sci</source>. <year>2009</year> <month>Feb</month>;<volume>20</volume>(<issue>2</issue>):<fpage>207</fpage>–<lpage>14</lpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Christophel</surname> <given-names>TB</given-names></string-name>, <string-name><surname>Klink</surname> <given-names>PC</given-names></string-name>, <string-name><surname>Spitzer</surname> <given-names>B</given-names></string-name>, <string-name><surname>Roelfsema</surname> <given-names>PR</given-names></string-name>, <string-name><surname>Haynes</surname> <given-names>JD</given-names></string-name></person-group>. <article-title>The Distributed Nature of Working Memory</article-title>. <source>Trends Cogn Sci</source>. <year>2017</year> <month>Feb</month>;<volume>21</volume>(<issue>2</issue>):<fpage>111</fpage>–<lpage>24</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Curtis</surname> <given-names>CE</given-names></string-name>, <string-name><surname>Sprague</surname> <given-names>TC</given-names></string-name></person-group>. <article-title>Persistent Activity During Working Memory From Front to Back</article-title>. <source>Front Neural Circuits</source>. <year>2021</year> <month>Jul</month> 21;<volume>15</volume>:<issue>696060</issue>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Iamshchinina</surname> <given-names>P</given-names></string-name>, <string-name><surname>Christophel</surname> <given-names>TB</given-names></string-name>, <string-name><surname>Gayet</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rademaker</surname> <given-names>RL</given-names></string-name></person-group>. <article-title>Essential considerations for exploring visual working memory storage in the human brain</article-title>. <source>Vis Cogn</source>. <year>2021</year> <month>Aug</month> 9;<volume>29</volume>(<issue>7</issue>):<fpage>425</fpage>–<lpage>36</lpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bettencourt</surname> <given-names>KC</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>Y</given-names></string-name></person-group>. <article-title>Decoding the content of visual short-term memory under distraction in occipital and parietal areas</article-title>. <source>Nat Neurosci</source>. <year>2016</year> <month>Jan</month>;<volume>19</volume>(<issue>1</issue>):<fpage>150</fpage>–<lpage>7</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Muhle-Karbe</surname> <given-names>PS</given-names></string-name>, <string-name><surname>Myers</surname> <given-names>NE</given-names></string-name></person-group>. <article-title>Theoretical distinction between functional states in working memory and their corresponding neural states</article-title>. <source>Vis Cogn</source>. <year>2020</year> <month>Sep</month> 13;<volume>28</volume>(<issue>5– 8</issue>):<fpage>420</fpage>–<lpage>32</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname> <given-names>MG</given-names></string-name></person-group>. <article-title>‘Activity-silent’ working memory in prefrontal cortex: a dynamic coding framework</article-title>. <source>Trends Cogn Sci</source>. <year>2015</year> <month>Jul</month>;<volume>19</volume>(<issue>7</issue>):<fpage>394</fpage>–<lpage>405</lpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stroud</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Duncan</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lengyel</surname> <given-names>M</given-names></string-name></person-group>. <article-title>The computational foundations of dynamic coding in working memory</article-title>. <source>Trends Cogn Sci</source>. <year>2024</year> <month>Apr</month>;<fpage>S1364661324000536</fpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miller</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Lundqvist</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bastos</surname> <given-names>AM</given-names></string-name></person-group>. <article-title>Working Memory 2.0</article-title>. <source>Neuron</source>. <year>2018</year> <month>Oct</month> 24;<volume>100</volume>(<issue>2</issue>):<fpage>463</fpage>– <lpage>75</lpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sreenivasan</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Curtis</surname> <given-names>CE</given-names></string-name>, <string-name><surname>D’Esposito</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Revisiting the role of persistent neural activity during working memory</article-title>. <source>Trends Cogn Sci</source>. <year>2014</year> <month>Feb</month>;<volume>18</volume>(<issue>2</issue>):<fpage>82</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meyers</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Freedman</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Kreiman</surname> <given-names>G</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Poggio</surname> <given-names>T</given-names></string-name></person-group>. <article-title>Dynamic Population Coding of Category Information in Inferior Temporal and Prefrontal Cortex</article-title>. <source>J Neurophysiol</source>. <year>2008</year> <month>Sep</month>;<volume>100</volume>(<issue>3</issue>):<fpage>1407</fpage>–<lpage>19</lpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murray</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Bernacchia</surname> <given-names>A</given-names></string-name>, <string-name><surname>Roy</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Constantinidis</surname> <given-names>C</given-names></string-name>, <string-name><surname>Romo</surname> <given-names>R</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name></person-group>. <article-title>Stable population coding for working memory coexists with heterogeneous neural dynamics in prefrontal cortex</article-title>. <source>Proc Natl Acad Sci</source>. <year>2017</year> <month>Jan</month> 10;<volume>114</volume>(<issue>2</issue>):<fpage>394</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spaak</surname> <given-names>E</given-names></string-name>, <string-name><surname>Watanabe</surname> <given-names>K</given-names></string-name>, <string-name><surname>Funahashi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Stokes</surname> <given-names>MG</given-names></string-name></person-group>. <article-title>Stable and Dynamic Coding for Working Memory in Primate Prefrontal Cortex</article-title>. <source>J Neurosci</source>. <year>2017</year> <month>Jul</month> 5;<volume>37</volume>(<issue>27</issue>):<fpage>6503</fpage>–<lpage>16</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parthasarathy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Herikstad</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bong</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Medina</surname> <given-names>FS</given-names></string-name>, <string-name><surname>Libedinsky</surname> <given-names>C</given-names></string-name>, <string-name><surname>Yen</surname> <given-names>SC</given-names></string-name></person-group>. <article-title>Mixed selectivity morphs population codes in prefrontal cortex</article-title>. <source>Nat Neurosci</source>. <year>2017</year> <month>Dec</month>;<volume>20</volume>(<issue>12</issue>):<fpage>1770</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parthasarathy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Tang</surname> <given-names>C</given-names></string-name>, <string-name><surname>Herikstad</surname> <given-names>R</given-names></string-name>, <string-name><surname>Cheong</surname> <given-names>LF</given-names></string-name>, <string-name><surname>Yen</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Libedinsky</surname> <given-names>C</given-names></string-name></person-group>. <article-title>Time-invariant working memory representations in the presence of code-morphing in the lateral prefrontal cortex</article-title>. <source>Nat Commun</source>. <year>2019</year> <month>Nov</month> 1;<volume>10</volume>(<issue>1</issue>):<fpage>4995</fpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sreenivasan</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Vytlacil</surname> <given-names>J</given-names></string-name>, <string-name><surname>D’Esposito</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Distributed and Dynamic Storage of Working Memory Stimulus Information in Extrastriate Cortex</article-title>. <source>J Cogn Neurosci</source>. <year>2014</year> <month>May</month> 1;<volume>26</volume>(<issue>5</issue>):<fpage>1141</fpage>–<lpage>53</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname> <given-names>HH</given-names></string-name>, <string-name><surname>Curtis</surname> <given-names>CE</given-names></string-name></person-group>. <article-title>Neural population dynamics of human working memory</article-title>. <source>Curr Biol</source>. <year>2023</year> <month>Sep</month>;<volume>33</volume>(<issue>17</issue>):<fpage>3775</fpage>–<lpage>3784.</lpage> </mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lorenc</surname> <given-names>ES</given-names></string-name>, <string-name><surname>Mallett</surname> <given-names>R</given-names></string-name>, <string-name><surname>Lewis-Peacock</surname> <given-names>JA</given-names></string-name></person-group>. <article-title>Distraction in Visual Working Memory: Resistance is Not Futile</article-title>. <source>Trends Cogn Sci</source>. <year>2021</year> <month>Mar</month>;<volume>25</volume>(<issue>3</issue>):<fpage>228</fpage>–<lpage>39</lpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Felleman</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Van Essen</surname> <given-names>DC</given-names></string-name></person-group>. <article-title>Distributed Hierarchical Processing in the Primate Cerebral Cortex</article-title>. <source>Cereb Cortex</source>. <year>1991</year> <month>Jan</month> 1;<volume>1</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>47</lpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lawrence</surname> <given-names>SJD</given-names></string-name>, <string-name><surname>van Mourik</surname> <given-names>T</given-names></string-name>, <string-name><surname>Kok</surname> <given-names>P</given-names></string-name>, <string-name><surname>Koopmans</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Norris</surname> <given-names>DG</given-names></string-name>, <string-name><surname>de Lange</surname> <given-names>FP</given-names></string-name></person-group>. <article-title>Laminar Organization of Working Memory Signals in Human Visual Cortex</article-title>. <source>Curr Biol</source>. <year>2018</year> <month>Nov</month>;<volume>28</volume>(<issue>21</issue>):<fpage>3435</fpage>–<lpage>3440.</lpage> </mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Kerkoerle</surname> <given-names>T</given-names></string-name>, <string-name><surname>Self</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Roelfsema</surname> <given-names>PR</given-names></string-name></person-group>. <article-title>Layer-specificity in the effects of attention and working memory on activity in primary visual cortex</article-title>. <source>Nat Commun</source>. <year>2017</year> <month>Apr</month> 28;<volume>8</volume>(<issue>1</issue>):<fpage>13804</fpage>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lawrence</surname> <given-names>SJD</given-names></string-name>, <string-name><surname>Norris</surname> <given-names>DG</given-names></string-name>, <string-name><surname>de Lange</surname> <given-names>FP</given-names></string-name></person-group>. <article-title>Dissociable laminar profiles of concurrent bottom-up and top-down modulation in the human visual cortex</article-title>. <source>eLife</source>. <year>2019</year> <month>May</month> 7;<volume>8</volume>:<fpage>e44422</fpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Libby</surname> <given-names>A</given-names></string-name>, <string-name><surname>Buschman</surname> <given-names>TJ</given-names></string-name></person-group>. <article-title>Rotational dynamics reduce interference between sensory and memory representations</article-title>. <source>Nat Neurosci</source>. <year>2021</year> <month>May</month>;<volume>24</volume>(<issue>5</issue>):<fpage>715</fpage>–<lpage>26</lpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xie</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Hu</surname> <given-names>P</given-names></string-name>, <string-name><surname>Li</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Song</surname> <given-names>W</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Geometry of sequence working memory in macaque prefrontal cortex</article-title>. <source>Science</source>. <year>2022</year> <month>Feb</month> 11;<volume>375</volume>(<issue>6581</issue>):<fpage>632</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wan</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Menendez</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Postle</surname> <given-names>BR</given-names></string-name></person-group>. <article-title>Priority-based transformations of stimulus representation in visual working memory. Buschman T, editor</article-title>. <source>PLOS Comput Biol</source>. <year>2022</year> Jun 2;<volume>18</volume>(<issue>6</issue>):<fpage>e1009062</fpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Wan</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Ardalan</surname> <given-names>A</given-names></string-name>, <string-name><surname>Fulvio</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Postle</surname> <given-names>BR</given-names></string-name></person-group>. <article-title>Representing context and priority in working memory</article-title>. <source>bioRxiv</source>; <year>2023</year>. doi: <pub-id pub-id-type="doi">10.1101/2023.10.24.563608</pub-id></mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Loon</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Olmos-Solis</surname> <given-names>K</given-names></string-name>, <string-name><surname>Fahrenfort</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Olivers</surname> <given-names>CN</given-names></string-name></person-group>. <article-title>Current and future goals are represented in opposite patterns in object-selective cortex</article-title>. <source>eLife</source>. <year>2018</year> <month>Nov</month> 6;<volume>7</volume>:<fpage>e38677</fpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Kusunoki</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sigala</surname> <given-names>N</given-names></string-name>, <string-name><surname>Nili</surname> <given-names>H</given-names></string-name>, <string-name><surname>Gaffan</surname> <given-names>D</given-names></string-name>, <string-name><surname>Duncan</surname> <given-names>J</given-names></string-name></person-group>. <article-title>Dynamic Coding for Cognitive Control in Prefrontal Cortex</article-title>. <source>Neuron</source>. <year>2013</year> <month>Apr</month>;<volume>78</volume>(<issue>2</issue>):<fpage>364</fpage>–<lpage>75</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Degutis</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Chaimow</surname> <given-names>D</given-names></string-name>, <string-name><surname>Haenelt</surname> <given-names>D</given-names></string-name>, <string-name><surname>Assem</surname> <given-names>M</given-names></string-name>, <string-name><surname>Duncan</surname> <given-names>J</given-names></string-name>, <string-name><surname>Haynes</surname> <given-names>JD</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Dynamic layer-specific processing in the prefrontal cortex during working memory</article-title>. <source>bioRxiv</source>; <year>2023</year>. <pub-id pub-id-type="doi">10.1101/2023.10.27.564330</pub-id></mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anders</surname> <given-names>S</given-names></string-name>, <string-name><surname>Heinzle</surname> <given-names>J</given-names></string-name>, <string-name><surname>Weiskopf</surname> <given-names>N</given-names></string-name>, <string-name><surname>Ethofer</surname> <given-names>T</given-names></string-name>, <string-name><surname>Haynes</surname> <given-names>JD</given-names></string-name></person-group>. <article-title>Flow of affective information between communicating brains</article-title>. <source>NeuroImage</source>. <year>2011</year> <month>Jan</month>;<volume>54</volume>(<issue>1</issue>):<fpage>439</fpage>–<lpage>46</lpage>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weber</surname> <given-names>S</given-names></string-name>, <string-name><surname>Christophel</surname> <given-names>T</given-names></string-name>, <string-name><surname>Görgen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Soch</surname> <given-names>J</given-names></string-name>, <string-name><surname>Haynes</surname> <given-names>J</given-names></string-name></person-group>. <article-title>Working memory signals in early visual cortex are present in weak and strong imagers</article-title>. <source>Hum Brain Mapp</source>. <year>2024</year> <month>Feb</month> 15;<volume>45</volume>(<issue>3</issue>):<fpage>e26590</fpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cavanagh</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Towers</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Wallis</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Hunt</surname> <given-names>LT</given-names></string-name>, <string-name><surname>Kennerley</surname> <given-names>SW</given-names></string-name></person-group>. <article-title>Reconciling persistent and dynamic hypotheses of working memory coding in prefrontal cortex</article-title>. <source>Nat Commun</source>. <year>2018</year> <month>Dec</month>;<volume>9</volume>(<issue>1</issue>):<fpage>3498</fpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brouwer</surname> <given-names>GJ</given-names></string-name>, <string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name></person-group>. <article-title>Decoding and Reconstructing Color from Responses in Human Visual Cortex</article-title>. <source>J Neurosci</source>. <year>2009</year> <month>Nov</month> 4;<volume>29</volume>(<issue>44</issue>):<fpage>13992</fpage>–<lpage>4003</lpage>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Piwek</surname> <given-names>EP</given-names></string-name>, <string-name><surname>Stokes</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Summerfield</surname> <given-names>C</given-names></string-name></person-group>. <article-title>A recurrent neural network model of prefrontal brain activity during a working memory task. Cai MB, editor</article-title>. <source>PLOS Comput Biol</source>. <year>2023</year> Oct 18;<volume>19</volume>(<issue>10</issue>):<fpage>e1011555</fpage>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Konkle</surname> <given-names>T</given-names></string-name>, <string-name><surname>Rhee</surname> <given-names>JY</given-names></string-name>, <string-name><surname>Nakayama</surname> <given-names>K</given-names></string-name>, <string-name><surname>Alvarez</surname> <given-names>GA</given-names></string-name></person-group>. <article-title>Processing multiple visual objects is limited by overlap in neural channels</article-title>. <source>Proc Natl Acad Sci</source>. <year>2014</year> <month>Jun</month> 17;<volume>111</volume>(<issue>24</issue>):<fpage>8955</fpage>–<lpage>60</lpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Franconeri</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Alvarez</surname> <given-names>GA</given-names></string-name>, <string-name><surname>Cavanagh</surname> <given-names>P</given-names></string-name></person-group>. <article-title>Flexible cognitive resources: competitive content maps for attention and memory</article-title>. <source>Trends Cogn Sci</source>. <year>2013</year> <month>Mar</month>;<volume>17</volume>(<issue>3</issue>):<fpage>134</fpage>–<lpage>41</lpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sprague</surname> <given-names>TC</given-names></string-name>, <string-name><surname>Ester</surname> <given-names>EF</given-names></string-name>, <string-name><surname>Serences</surname> <given-names>JT</given-names></string-name></person-group>. <article-title>Reconstructions of Information in Visual Spatial Working Memory Degrade with Memory Load</article-title>. <source>Curr Biol</source>. <year>2014</year> <month>Sep</month>;<volume>24</volume>(<issue>18</issue>):<fpage>2174</fpage>–<lpage>80</lpage>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rademaker</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Bloem</surname> <given-names>IM</given-names></string-name>, <string-name><surname>De Weerd</surname> <given-names>P</given-names></string-name>, <string-name><surname>Sack</surname> <given-names>AT</given-names></string-name></person-group>. <article-title>The impact of interference on short-term memory for visual orientation</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>2015</year>;<volume>41</volume>(<issue>6</issue>):<fpage>1650</fpage>–<lpage>65</lpage>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mallett</surname> <given-names>R</given-names></string-name>, <string-name><surname>Mummaneni</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lewis-Peacock</surname> <given-names>JA</given-names></string-name></person-group>. <article-title>Distraction biases working memory for faces</article-title>. <source>Psychon Bull Rev</source>. <year>2020</year> <month>Apr</month>;<volume>27</volume>(<issue>2</issue>):<fpage>350</fpage>–<lpage>6</lpage>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teng</surname> <given-names>C</given-names></string-name>, <string-name><surname>Kravitz</surname> <given-names>DJ</given-names></string-name></person-group>. <article-title>Visual working memory directly alters perception</article-title>. <source>Nat Hum Behav</source>. <year>2019</year> <month>Aug</month>;<volume>3</volume>(<issue>8</issue>):<fpage>827</fpage>–<lpage>36</lpage>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kang</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Hong</surname> <given-names>SW</given-names></string-name>, <string-name><surname>Blake</surname> <given-names>R</given-names></string-name>, <string-name><surname>Woodman</surname> <given-names>GF</given-names></string-name></person-group>. <article-title>Visual working memory contaminates perception</article-title>. <source>Psychon Bull Rev</source>. <year>2011</year> <month>Oct</month>;<volume>18</volume>(<issue>5</issue>):<fpage>860</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gayet</surname> <given-names>S</given-names></string-name>, <string-name><surname>Paffen</surname> <given-names>CLE</given-names></string-name>, <string-name><surname>Van Der Stigchel</surname> <given-names>S</given-names></string-name></person-group>. <article-title>Information Matching the Content of Visual Working Memory Is Prioritized for Conscious Access</article-title>. <source>Psychol Sci</source>. <year>2013</year> <month>Dec</month>;<volume>24</volume>(<issue>12</issue>):<fpage>2472</fpage>– <lpage>80</lpage>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gayet</surname> <given-names>S</given-names></string-name>, <string-name><surname>Guggenmos</surname> <given-names>M</given-names></string-name>, <string-name><surname>Christophel</surname> <given-names>TB</given-names></string-name>, <string-name><surname>Haynes</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Paffen</surname> <given-names>CLE</given-names></string-name>, <string-name><surname>Van Der Stigchel</surname> <given-names>S</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Visual Working Memory Enhances the Neural Response to Matching Visual Input</article-title>. <source>J Neurosci</source>. <year>2017</year> <month>Jul</month> 12;<volume>37</volume>(<issue>28</issue>):<fpage>6638</fpage>–<lpage>47</lpage>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lorenc</surname> <given-names>ES</given-names></string-name>, <string-name><surname>Sreenivasan</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Nee</surname> <given-names>DE</given-names></string-name>, <string-name><surname>Vandenbroucke</surname> <given-names>ARE</given-names></string-name>, <string-name><surname>D’Esposito</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Flexible Coding of Visual Working Memory Representations during Distraction</article-title>. <source>J Neurosci</source>. <year>2018</year> <month>Jun</month> 6;<volume>38</volume>(<issue>23</issue>):<fpage>5267</fpage>–<lpage>76</lpage>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hallenbeck</surname> <given-names>GE</given-names></string-name>, <string-name><surname>Sprague</surname> <given-names>TC</given-names></string-name>, <string-name><surname>Rahmati</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sreenivasan</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Curtis</surname> <given-names>CE</given-names></string-name></person-group>. <article-title>Working memory representations in visual cortex mediate distraction effects</article-title>. <source>Nat Commun</source>. <year>2021</year> <month>Aug</month> 5;<volume>12</volume>(<issue>1</issue>):<fpage>4714</fpage>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Iamshchinina</surname> <given-names>P</given-names></string-name>, <string-name><surname>Kaiser</surname> <given-names>D</given-names></string-name>, <string-name><surname>Yakupov</surname> <given-names>R</given-names></string-name>, <string-name><surname>Haenelt</surname> <given-names>D</given-names></string-name>, <string-name><surname>Sciarra</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mattern</surname> <given-names>H</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Perceived and mentally rotated contents are differentially represented in cortical depth of V1</article-title>. <source>Commun Biol</source>. <year>2021</year> <month>Dec</month>;<volume>4</volume>(<issue>1</issue>):<fpage>1069</fpage>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname> <given-names>CC</given-names></string-name>, <string-name><surname>Lin</surname> <given-names>CJ</given-names></string-name></person-group>. <article-title>LIBSVM: A library for support vector machines</article-title>. <source>ACM Trans Intell Syst Technol</source>. <year>2011</year> <month>Apr</month>;<volume>2</volume>(<issue>3</issue>):<fpage>1</fpage>–<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pilly</surname> <given-names>PK</given-names></string-name>, <string-name><surname>Seitz</surname> <given-names>AR</given-names></string-name></person-group>. <article-title>What a difference a parameter makes: A psychophysical comparison of random dot motion algorithms</article-title>. <source>Vision Res</source>. <year>2009</year> <month>Jul</month>;<volume>49</volume>(<issue>13</issue>):<fpage>1599</fpage>–<lpage>612</lpage>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maris</surname> <given-names>E</given-names></string-name>, <string-name><surname>Oostenveld</surname> <given-names>R</given-names></string-name></person-group>. <article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title>. <source>J Neurosci Methods</source>. <year>2007</year> <month>Aug</month>;<volume>164</volume>(<issue>1</issue>):<fpage>177</fpage>–<lpage>90</lpage>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bjorck</surname> <given-names>A</given-names></string-name>, <string-name><surname>Golub</surname> <given-names>GH</given-names></string-name></person-group>. <article-title>Numerical Methods for Computing Angles Between Linear Subspaces</article-title>. <source>Math Comput</source>. <year>1973</year> <month>Jul</month>;<volume>27</volume>(<issue>123</issue>):<fpage>579</fpage>–<lpage>94</lpage>.</mixed-citation></ref>
</ref-list>
<sec>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 1.</label>
<caption><title>Neural trajectories across time.</title><p>Same as <xref rid="fig2" ref-type="fig">Figure 2c</xref>), but the time dimension is on the z-axis.</p></caption>
<graphic xlink:href="589170v2_figs1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 2.</label>
<caption><title>Extension of Figure 3 for V4-LO2.</title></caption>
<graphic xlink:href="589170v2_figs2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 3.</label>
<caption><title>Temporal cross-decoding generalization between distractor and no-distractor VWM trials.</title>
<p><bold>a)</bold> Across-participant mean temporal cross-decoding of noise distractor trials when trained on no-distractor trials. <bold>b)</bold> Same as a), but orientation distractor trials trained on no-distractor trials.</p></caption>
<graphic xlink:href="589170v2_figs3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Supplementary Table 1.</label>
<caption><title>FDR-corrected <italic>p</italic>-values corresponding to <xref rid="fig2" ref-type="fig">Figure 2d</xref>.</title></caption>
<graphic xlink:href="589170v2_tbls1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tbls2" orientation="portrait" position="float">
<label>Supplementary Table 2.</label>
<caption><title>FDR-corrected <italic>p</italic>-values corresponding to <xref rid="fig4" ref-type="fig">Figure 4b</xref>.</title></caption>
<graphic xlink:href="589170v2_tbls2.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99290.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Xue</surname>
<given-names>Gui</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>useful</bold> study reports a reanalysis of one experiment of a previously published report to characterize the dynamics of neural population codes during visual working memory in the presence of distracting information. The evidence supporting the claims of dynamic codes is <bold>incomplete</bold>, as only a subset of the original data is analyzed, there is only modest evidence for dynamic coding in the results, and the result might be affected by the signal-to-noise ratio. This research will be of interest to cognitive neuroscientists working on the neural bases of visual perception and memory.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99290.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this study, the authors re-analyzed Experiment 1 of a public dataset (Rademaker et al, 2019, Nature Neuroscience) which includes fMRI and behavioral data recorded while participants held an oriented grating in visual working memory (WM) and performed a delayed recall task at the end of an extended delay period. In that experiment, participants were pre-cued on each trial as to whether there would be a distracting visual stimulus presented during the delay period (filtered noise or randomly oriented grating). In this manuscript, the authors focused on identifying whether the neural code in the retinotopic cortex for remembered orientation was 'stable' over the delay period, such that the format of the code remained the same, or whether the code was dynamic, such that information was present, but encoded in an alternative format. They identify some time points - especially towards the beginning/end of the delay - where the multivariate activation pattern fails to generalize to other time points and interpret this as evidence for a dynamic code. Additionally, the authors compare the representational format of remembered orientation in the presence vs absence of a distracting stimulus, averaged over the delay period. This analysis suggested a 'rotation' of the representational subspace between distracting orientations and remembered orientations, which may help preserve simultaneous representations of both remembered and viewed stimuli.</p>
<p>Strengths:</p>
<p>(1) Direct comparisons of coding subspaces/manifolds between time points and task conditions is an innovative and useful approach for understanding how neural representations are transformed to support cognition.</p>
<p>(2) Re-use of existing datasets substantially goes beyond the authors' previous findings by comparing the geometry of representational spaces between conditions and time points, and by looking explicitly for dynamic neural representations</p>
<p>Weaknesses:</p>
<p>(1) Only Experiment 1 of Rademaker et al (2019) is reanalyzed. The previous study included another experiment (Expt 2) using different types of distractors which did result in distractor-related costs to neural and behavioral measures of working memory. The Rademaker et al (2019) study uses these two results to conclude that neural WM representations are protected from distraction when distraction does not impact behavior, but conditions that do impact behavior also impact neural WM representations. Considering this previous result is critical for relating the present manuscript's results to the previous findings, it seems necessary to address Experimentt 2's data in the present work</p>
<p>(2) Primary evidence for 'dynamic coding', especially in the early visual cortex, appears to be related to the transition between encoding/maintenance and maintenance/recall, but the delay period representations seem overall stable, consistent with previous findings</p>
<p>(3) Dynamicism index used in Figure 1f quantifies the proportion of off-diagonal cells with significant differences in decoding performance from the diagonal cell. It's unclear why the proportion of time points is the best metric, rather than something like a change in decoding accuracy. This is addressed in the subsequent analysis considering coding subspaces, but the utility of the Figure 1f analysis remains weakly justified.</p>
<p>(4) There is no report of how much total variance is explained by the two PCs defining the subspaces of interest in each condition, and timepoint. It could be the case that the first two principal components in one condition (e.g., sensory distractor) explain less variance than the first two principal components of another condition.</p>
<p>(5) Converting a continuous decoding metric (angular error) to &quot;% decoding accuracy&quot; serves to obfuscate the units of the actual results. Decoding precision (e.g., sd of decoding error histogram) would be more interpretable and better related to both the previous study and behavioral measures of WM performance.</p>
<p>(6) This report does not make use of behavioral performance data in the Rademaker et al (2019) dataset.</p>
<p>(7) Given there were observed differences between individual retinotopic ROIs in the temporal cross-decoding analyses shown in Figure 1, the lack of data presented for the subspace analyses for the corresponding individual ROIs is a weakness</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99290.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this work, Degutis and colleagues addressed an interesting issue related to the concurrent coding of sensory percepts and visual working memory contents in visual cortices. They used generalization analyses to test whether working memory representations change over time, diverge from sensory percepts, and vary across distraction conditions. Temporal generalization analysis demonstrated that off-diagonal decoding accuracies were lower than on-diagonal decoding accuracies, regardless of the presence of intervening distractions, implying that working memory representations can change over time. They further showed that the coding space for working memory contents showed subtle but statistically significant changes over time, potentially explaining the impaired off-diagonal decoding performance. The neural coding of sensory distractions instead remained largely stable. Generalization analyses between target and distractor codes showed overlaps but were not identical. Cross-condition decodings had lower accuracies compared to within-condition decodings. Finally, within-condition decoding revealed more reliable working memory representations in the condition with intervening random noises compared to cross-condition decoding using a trained classifier on data from the no-distraction condition, indicating a change in the VWM format between the noise distractor and no-distractor trials.</p>
<p>Strengths:</p>
<p>This paper demonstrates a clever use of generalization analysis to show changes in the neural codes of working memory contents across time and distraction conditions. It provides some insights into the differences between representations of working memory and sensory percepts, and how they can potentially coexist in overlapping brain regions.</p>
<p>Weaknesses:</p>
<p>(1) An alternative interpretation of the temporal dynamic pattern is that working memory representations become less reliable over time. As shown by the authors in Figure 1c and Figure 4a, the on-diagonal decoding accuracy generally decreased over time. This implies that the signal-to-noise ratio was decreasing over time. Classifiers trained with data of relatively higher SNR and lower SNR may rely on different features, leading to poor generalization performance. This issue should be addressed in the paper.</p>
<p>(2) The paper tests against a strong version of stable coding, where neural spaces representing WM contents must remain identical over time. In this version, any changes in the neural space will be evidence of dynamic coding. As the paper acknowledges, there is already ample evidence arguing against this possibility. However, the evidence provided here (dynamic coding cluster, angle between coding spaces) is not as strong as what prior studies have shown for meaningful transformations in neural coding. For instance, the principal angle between coding spaces over time was smaller than 8 degrees, and around 7 degrees between sensory distractors and WM contents. This suggests that the coding space for WM was largely overlapping across time and with that for sensory distractors. Therefore, the major conclusion that working memory contents are dynamically coded is not well-supported by the presented results.</p>
<p>(3) Relatedly, the main conclusions, such as &quot;VWM code in several visual regions did not generalize well between different time points&quot; and &quot;VWM and feature-matching sensory distractors are encoded in separable coding spaces&quot; are somewhat subjective given that cross-condition generalization analyses consistently showed above chance-level performance. These results could be interpreted as evidence of stable coding. The authors should use more objective descriptions, such as 'temporal generalization decoding showed reduced decoding accuracy in off-diagonals compared to on-diagonals.</p>
</body>
</sub-article>
</article>