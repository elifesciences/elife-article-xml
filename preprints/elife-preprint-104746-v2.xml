<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">104746</article-id>
<article-id pub-id-type="doi">10.7554/eLife.104746</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.104746.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Efficient coding explains neural response homeostasis and stimulus-specific adaptation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0884-3195</contrib-id>
<name>
<surname>Young</surname>
<given-names>Edward James</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>ey245@cam.ac.uk</email>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5942-0697</contrib-id>
<name>
<surname>Ahmadian</surname>
<given-names>Yashar</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>ya311@cam.ac.uk</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>Computational and Biological Learning Lab, Department of Engineering, University of Cambridge</institution></institution-wrap>, <city>Cambridge</city>, <country country="GB">United Kingdom</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Sharpee</surname>
<given-names>Tatyana O</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Salk Institute for Biological Studies</institution>
</institution-wrap>
<city>La Jolla</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country country="US">United States</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-03-04">
<day>04</day>
<month>03</month>
<year>2025</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-10-03">
<day>03</day>
<month>10</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP104746</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-11-20">
<day>20</day>
<month>11</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-10-16">
<day>16</day>
<month>10</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.10.29.564616"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2025-03-04">
<day>04</day>
<month>03</month>
<year>2025</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.104746.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.104746.1.sa2">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.104746.1.sa1">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.104746.1.sa0">Reviewer #2 (Public review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>¬© 2025, Young &amp; Ahmadian</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Young &amp; Ahmadian</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-104746-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>In the absence of adaptation, the average firing rate of neurons would rise or drop when changes in the environment make their preferred stimuli more or less prevalent. However, by adjusting the responsiveness of neurons, adaptation can yield firing rate homeostasis and stabilise the average rates of neurons at fixed levels, despite changes in stimulus statistics. In sensory cortex, adaptation is typically also stimulus specific, in that neurons reduce their responsiveness to over-represented stimuli, but maintain or even increase their responsiveness to stimuli far from over-represented ones. Here, we present a normative explanation of firing rate homeostasis grounded in the efficient coding principle. Specifically, we show that this homeostasis can arise when neurons adapt their responsiveness to optimally mitigate the effect of neural noise on population coding fidelity, at minimal metabolic cost. Unlike previous efficient coding theories, we formulate the problem in a computation-agnostic manner, enabling our framework to apply far from the sensory periphery. We then apply this general framework to Distributed Distributional Codes, a specific computational theory of neural representations serving Bayesian inference. We demonstrate how homeostatic coding, combined with such Bayesian neural representations, provides a normative explanation for stimulus-specific adaptation, widely observed across the brain, and how this coding scheme can be accomplished by divisive normalisation with adaptive weights. Further, we develop a model within this combined framework, and, by fitting it to previously published experimental data, quantitatively account for measures of stimulus-specific and homeostatic adaption in the primary visual cortex.</p>
</abstract>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00cwqg982</institution-id>
<institution>Biotechnology and Biological Sciences Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>BB/X013235/1</award-id>
</award-group>
<award-group id="funding-2">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/0439y7842</institution-id>
<institution>Engineering and Physical Sciences Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>EP/T517847/1</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Revision in response to reviews in eLife</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Neural population responses, and thus the computations on sensory inputs represented by them, are corrupted by noise. The extent of this corruption can, however, be modulated by changing neural gains. When the gain of a neuron increases, the strength of its response noise, or trial-to-trial variability, typically grows sublinearly and more slowly than its average response; for example, for Poisson-like firing, response noise grows as the square root of the mean response. The neuron can therefore increase its signal-to-noise ratio by increasing its gain or responsiveness. However, this increase in coding fidelity comes at the cost of an elevated average firing rate, and thus higher metabolic energy expenditure. From a normative perspective, coding fidelity and metabolic cost are thus two conflicting forces.</p>
<p>We start this study by asking what is the optimal way of adjusting neural gains in order to combat noise with minimal metabolic cost, <italic>irrespective</italic> of the computations represented by the neural population. As we will see, the answer depends on the prevailing stimulus statistics. We will thus address the following more specific question: given a neural population with <italic>arbitrary</italic> tuning curve shapes and general noise distribution, how should neurons optimally adjust their gains depending on the stimulus statistics prevailing in the environment? To address this question, we use efficient coding theory as our normative framework (<xref ref-type="bibr" rid="c2">Attneave, 1954</xref>; <xref ref-type="bibr" rid="c39">Nadal and Parga, 1999</xref>; <xref ref-type="bibr" rid="c27">Laughlin, 1981</xref>; <xref ref-type="bibr" rid="c3">Barlow, 2012</xref>; <xref ref-type="bibr" rid="c30">Linsker, 1988</xref>; <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="c64">Wei and Stocker, 2015</xref>; <xref ref-type="bibr" rid="c1">Atick and Redlich, 1990</xref>). Efficient coding theories formalise the problem of optimal coding subject to biological constraints and costs, and by finding the optimal solution make predictions about the behavior of nervous systems (which are postulated to have been approximately optimised, <italic>e</italic>.<italic>g</italic>., via natural selection). Concretely, the Infomax Principle (<xref ref-type="bibr" rid="c30">Linsker, 1988</xref>; <xref ref-type="bibr" rid="c1">Atick and Redlich, 1990</xref>; <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="c64">Wei and Stocker, 2015</xref>) states that sensory systems optimise the mutual information between a noisy neural representation and an external stimulus, subject to metabolic constraints, <italic>e</italic>.<italic>g</italic>., on the energy cost of spiking activity. To that end, the neural system should exploit the statistics of stimuli in its local environment (<xref ref-type="bibr" rid="c47">Simoncelli and Olshausen, 2001</xref>). Efficient coding theories can therefore predict how sensory systems should optimally <italic>adapt</italic> to changes in environmental stimulus statistics. There is, however, a key difference between the approach we will adopt and classic applications of the efficient coding principle. In most applications, this principle has been applied to systems located early in the sensory stream, and information transmission is taken to be the exclusive goal of the system. Accordingly, the entire input-output transformations performed by the system is assumed to have been optimised (under metabolic or anatomical constraints) purely for this goal. Based on this assumption, such theories typically make predictions about patterns of neural selectivity, including the shapes and arrangement of neural tuning curves. However, for general brain regions located deeper within the sensory-motor pathway, information transmission is far from an adequate charactrisation of the system‚Äôs computational goals. For example, many neural systems perform computations which require systematically discarding information contained in the stimulus input. The discarded information may correspond to nuisance variables, or be irrelevant to the behavioural tasks faced by the organism. Here, we make the key assumption that the shapes and arrangement of tuning curves are determined by computational goals ‚Äî beyond mitigating the effect of noise on coding fidelity ‚Äî to which our efficient coding framework remains agnostic. We therefore do not optimise these aspects of the population code and allow them to be arbitrary. On the other hand, we assume the neural gains (which set the scale of the tuning curves) are adapted and optimised purely for the purpose of fighting noise corruption at minimal metabolic cost, regardless of the system‚Äôs computational goals. Conceptually, this division of labour is akin to the separation, in digital computers, between error correction (which is indispensable in practice) and general computations, and the need for the former to be performed in a manner agnostic to the ongoing computations.</p>
<p>Suppose an environmental shift makes a stimulus feature more prevalent. Neurons sensitive to that feature will then see their preferred feature more often, and thus their average firing rate will initially increase. However, typically, those neurons gradually adapt by reducing their responsiveness or gain (<xref ref-type="bibr" rid="c50">Solomon and Kohn, 2014</xref>; <xref ref-type="bibr" rid="c10">Clifford et al., 2007</xref>; <xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>). Such gain adaptation can lead to <italic>firing rate homeostasis</italic> (<xref ref-type="bibr" rid="c11">Desai, 2003</xref>; <xref ref-type="bibr" rid="c59">Turrigiano and Nelson, 2004</xref>; <xref ref-type="bibr" rid="c31">Maffei and Turrigiano, 2008</xref>; <xref ref-type="bibr" rid="c21">Hengen et al., 2013</xref>), whereby adaptation brings the average firing rate back to the level prior to the environmental shift (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>). Thus, under such homeostatic adaptation, neuronal populations maintain a constant stimulus-averaged firing rate in spite of changes to the environment (<xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>; <xref ref-type="bibr" rid="c21">Hengen et al., 2013</xref>; <xref ref-type="bibr" rid="c31">Maffei and Turrigiano, 2008</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Firing rate homeostasis via gain adaptation (cartoon examples).</title>
<p>(A) Gain adaptation after a shift in environmental statistics, without changes in tuning curve shapes. The bottom row shows the stimulus distributions in the first (left column) and the second(middle and right columns) environments. The top row shows the tuning curves of the same four neurons (main plots) and their mean firing rates (insets) when adapted to the first (left) and the second (right) environments, as well as immediately after the transition, but before adaptation to the new stimulus statistics (middle). Immediately after the transition the mean firing rates differ from their values in the first environment (middle and left insets). Neurons adapt to the new environment solely by independently adjusting their gains, <italic>i</italic>.<italic>e</italic>., by scaling their tuning curves up or down (correspondingly, the tuning curves of the same color in the middle and right columns ‚Äî which belong to the same neuron ‚Äî are scaled versions of each other and have identical shape), to a degree appropriate for returning their stimulus averaged firing rates to the level before the environmental shift (right and left insets). (B) Gain adaptation after a change in tuning curve shapes, due, <italic>e</italic>.<italic>g</italic>., to a change in the system‚Äôs computational goals. The plots have the same format as in (A). In this case, the stimulus distribution (bottom row) does not change between the two contexts, but the tuning curves have different shapes in the second context (top row, second and third columns vs. the first). Again, the average firing rates change immediately after the transition (middle vs. left insets), but the neurons adapt their gains to the right degree to return their rates to the level before the context changed (right inset).</p></caption>
<graphic xlink:href="564616v4_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>There are multiple levels at which homeostatic adaptation can be observed. Firstly, there is population homeostasis, in which the stimulus-average firing rate of an entire population of neurons remains constant, without individual neurons necessarily holding their rates constant (<xref ref-type="bibr" rid="c48">Slomowitz et al., 2015</xref>; <xref ref-type="bibr" rid="c44">Sanzeni et al., 2023</xref>). Secondly, there is what we term <italic>cluster homeostasis</italic>. In this form of homeostasis, stimulus-average firing rate of groups or clusters of neurons with similar stimulus tuning remains stable under environmental shifts (<xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>), but the firing rate of individual neurons within a cluster can change. Lastly, in the strongest form, homeostasis can occur at the level of individual neurons, in which case the firing rate of each individual neuron is kept constant under changes in environment statistics (<xref ref-type="bibr" rid="c32">Marder and Prinz, 2003</xref>). Previous normative explanations for firing rate homeostasis often focus on the necessity of preventing the network from becoming hypo- or hyperactive (<xref ref-type="bibr" rid="c59">Turrigiano and Nelson, 2004</xref>; <xref ref-type="bibr" rid="c31">Maffei and Turrigiano, 2008</xref>; <xref ref-type="bibr" rid="c23">Keck et al., 2013</xref>; <xref ref-type="bibr" rid="c21">Hengen et al., 2013</xref>). Although this might explain homeostasis at the population level, it does not adequately explain homeostasis at a more fine-grained level. As we will show in this paper, the answer to the normative question that we posed above provides an explanation for firing rate homeostasis at such a finer level. Specifically, we will show that, for a wide variety of neural noise models (and under conditions that we argue do obtain in the visual cortex), optimal adjustment of neural gains to combat the effect of coding noise predicts firing rate homeostasis at the level of clusters of similarly-tuned neurons.</p>
<p>Having shown the optimality of homeostatic gain adaptation regardless of the computational goals of the population (which we have assumed determine its tuning curve shapes), we then apply our framework to <italic>Distributed Distributional Codes (DDC)</italic> (<xref ref-type="bibr" rid="c61">Vertes and Sahani, 2018</xref>), as a specific computational theory for neural implementation of Bayesian inference. Combining DDC theory (which determines tuning curve shapes) with our normative results (on optimal adjustments of neural gains) yields what we call <italic>homeostatic DDC</italic>. We will show that homeostatic DDCs explain the typical finding that sensory adaptations are stimulus specific (<xref ref-type="bibr" rid="c25">Kohn, 2007</xref>; <xref ref-type="bibr" rid="c45">Schwartz et al., 2007</xref>; <xref ref-type="bibr" rid="c54">Taaseh et al., 2011</xref>). In stimulus specific adaptation (SSA), the suppression of response is greater for test stimuli that are closer to an over-represented adaptor stimulus than for stimuli further away (to which the neuron may even respond more strongly after adaptation). In particular, this causes a repulsion of tuning curves away from the adaptor (<xref ref-type="bibr" rid="c36">Movshon and Lennie, 1979</xref>; <xref ref-type="bibr" rid="c37">M√ºller et al., 1999</xref>; <xref ref-type="bibr" rid="c14">Dragoi et al., 2000</xref>, <xref ref-type="bibr" rid="c13">2002</xref>; <xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>). We will show that homeostatic DDC are able to account for SSA effects which cannot be fully accounted for in previous efficient coding frameworks (<xref ref-type="bibr" rid="c64">Wei and Stocker, 2015</xref>; <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="c49">Snow et al., 2016</xref>), and we will use them to quantitatively model homeostatic SSA as observed in V1, by re-analysing previously published data (<xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>). This model relies on a special class of homeostatic DDC which we term <italic>Bayes-ratio coding</italic>. We show that Bayes-ratio coding has attractive computational properties: it can be propagated between populations without synaptic weight adjustments, and it can be achieved by divisive normalisation (which has been called a canonical neural operation) with adaptive weights (<xref ref-type="bibr" rid="c9">Carandini and Heeger, 2012</xref>; <xref ref-type="bibr" rid="c66">Westrick et al., 2016</xref>).</p>
<p>We start the next section by introducing our efficient coding framework for addressing the normative question posed above. We show numerically that our framework predicts that the firing rate of clusters of similarly tuned neurons should remain constant despite shifts in environmental stimulus statistics, with individual neurons free to shuffle their average rates. This result is shown to hold across a diverse set of noise models, demonstrating the robustness of the optimality of homeostasis to the precise nature of neural response noise. We additionally demonstrate that our framework can account for a wide distribution of stimulus-averaged single-neuron firing rates as observed in cortex. We then provide an analytic argument that demonstrates why, within a certain parameter regime, homeostasis arises from our optimisation problem, and show that cortical areas, in particular the primary visual cortex (V1), are likely to be within this parameter regime. We then numerically validate the quality of our analytic approximations to the optimal solution. Lastly, we apply our theory to DDC representational codes, showing how homeostatic DDC can account for stimulus specific adaptation effects observed experimentally.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<sec id="s2a">
<label>2.1</label>
<title>Theoretical framework</title>
<p>The central question we seek to address is how the presence of response noise affects optimal neural coding strategies. Specifically, we consider how a neural population should distribute activity in order to best mitigate the effect of neural noise on coding fidelity, subject to metabolic constraints, irrespective of the computations it performs on sensory inputs (which we do not optimise, but take as given). We consider a population of <italic>K</italic> neuronal units (We will use ‚Äúunits‚Äù instead of ‚Äúneurons‚Äù to refer to the tuning curves because we will apply the theoretical framework to both cases where the units correspond to single neurons or clusters of similarly tuned neurons.), responding to the (possibly high-dimensional) stimulus <bold><italic>s</italic></bold> according to their tuning curves <bold><italic>h</italic></bold>(<bold><italic>s</italic></bold>). More precisely, we assume that our population engages in rate coding using time bins of a fixed duration, and denote the vector of joint population spike counts in a coding interval by <bold><italic>n</italic></bold> = (<italic>n</italic><sub>1</sub>, ‚Ä¶, <italic>n</italic><sub><italic>K</italic></sub>). Single-trial neural responses, <bold><italic>n</italic></bold>, are taken to be noisy emissions from the neural tuning curves, <bold><italic>h</italic></bold>(<bold><italic>s</italic></bold>), according to a noise distributions <bold><italic>n|s</italic></bold> <italic>~ P</italic><sub>noise</sub>(<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>), subject to the constraint that the trial-averaged response ùîº[<bold><italic>n</italic></bold> | <bold><italic>s</italic></bold>] equals <bold><italic>h</italic></bold>(<bold><italic>s</italic></bold>). We consider a number of different noise models, including correlated and power-law noise, showing that our main results are robust to specific assumptions about the nature of noise in neural systems. The sensory environment is characterised by a stimulus distribution <italic>P</italic> (<bold><italic>s</italic></bold>); accordingly, an ‚Äúenvironmental shift‚Äù corresponds to a change in <italic>P</italic> (<bold><italic>s</italic></bold>), making certain stimuli more or less prevalent.</p>
<p>The central assumption of our efficient coding framework is that, while the shapes of neural tuning curves are determined by the computational goals of the circuit, the gains of those tuning curves are adapted purely for the purpose of fighting noise corruption at minimal metabolic cost, regardless of the computational goals. To this end, we adopt a shape-amplitude decomposition of the neural tuning curves. The tuning curve of the <italic>a</italic>-th unit, <italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>), is factorised into a <italic>representational curve</italic>, Œ©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>), and a <italic>gain, g</italic><sub><italic>a</italic></sub>:
<disp-formula id="eqn1">
<graphic xlink:href="564616v4_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<xref rid="fig3" ref-type="fig">Fig. 3A</xref> demonstrates the effect of changing the gain while keeping the representational curve, and therefore the tuning curve shape, fixed (Note that adjusting the gains does not mathematically restrict the computations which can be performed by downstream populations, as adjustments in gains can in principle ‚Äî and if needed ‚Äî be compensated for by reciprocal adjustments in readout weights. More formally, if a function, <italic>F</italic>, of the input <bold><italic>s</italic></bold>, can be computed from the unmodulated Œ©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>), say by a linear combination <italic>F</italic> (<bold><italic>s</italic></bold>) <italic>‚âà</italic> ‚àë <sub><italic>a</italic></sub> <italic>w</italic><sub><italic>a</italic></sub>Œ©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>), then it can equally be represented as a linear combination of the gain-modulated tuning curves, <inline-formula><inline-graphic xlink:href="564616v4_inline99.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, with the adjusted weights <inline-formula><inline-graphic xlink:href="564616v4_inline100.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.). The representational curve can be thought of as a normalised tuning curve; in fact, to make the definition of gain unambiguous, we define Œ©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>) to be the tuning curve normalised by its peak value, <italic>i</italic>.<italic>e</italic>., Œ©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>) <italic>‚â° h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>)<italic>/</italic> max <italic>h</italic><sub><italic>a</italic></sub> (<xref rid="fig3" ref-type="fig">Fig. 3A</xref> inset). Correspondingly, the gain of a tuning curve is simply its maximum value, <italic>g</italic><sub><italic>a</italic></sub> = max<sub><bold><italic>s</italic></bold></sub> <italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>).</p>
<p>We take the shape of the tuning curve, captured by Œ©<sub><italic>a</italic></sub>, to encode the computations represented by the neural population, and thus to be determined by those computational goals. These computational goals, and hence the Œ©<sub><italic>a</italic></sub> may change across environments or contexts (<italic>e</italic>.<italic>g</italic>., corresponding to different tasks), but our efficient coding theory will not dictate how they should change (see <xref rid="fig1" ref-type="fig">Fig. 1B</xref>, and <xref ref-type="sec" rid="s2g">Section 2.7</xref> for an example). Rather, we treat the representational curves, Œ©<sub><italic>a</italic></sub>, as given, and do not optimise them within our efficient coding framework. In this respect, we deviate from the assumption of many classic studies based on the efficient coding principle, wherein information transmission is taken to be the sole computational goal of the system (usually located early in the sensory stream), and therefore the input-output transformation performed by the system is assumed to have been optimised purely for this purpose. General computations, however, often involve discarding information contained in the stimulus, that, <italic>e</italic>.<italic>g</italic>., constitute nuisance variables or may be irrelevant to behavioural tasks; this is the rationale for taking the Œ©<sub><italic>a</italic></sub> as given and not optimising them purely based on information transmission criteria. In line with this rationale, we do not make any prior assumptions about or put any constraints on the shapes of the representational curves: the Œ©<sub><italic>a</italic></sub> can be any complex (<italic>e</italic>.<italic>g</italic>., multi-modal, discontinuous, or heterogeneous) set of functions of the possibly high-dimensional stimulus, and can thus represent any computation. This makes our treatment more generally applicable than most other efficient coding frameworks which place tight constraints on the shapes and configuration of the tuning curves (see the Discussion for a more detailed comparison of our approach with those studies). In particular, this generality enables our theory to apply to populations located deep in the processing pathway, and not just to primary sensory neurons. For example, the representational curves, Œ©<sub><italic>a</italic></sub>, could map the normalised response of face-detecting neurons in the inferotemporal cortex as a function of the stimulus, <bold><italic>s</italic></bold>, which could be taken as the high-dimensional raster of RGB values in an image shown to the animal.</p>
<p>Our framework, relating environmental stimuli to neural tuning curves and finally to noisy neural responses, and the respective computational roles of the gains and representational curves, is summarised by the diagram in <xref rid="fig2" ref-type="fig">Fig. 2</xref>.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>The assumptions and the generative model underlying the theoretical framework.</title>
<p>The environment gives rise to a stimulus distribution <italic>P</italic> from which the stimulus <bold><italic>s</italic></bold> is drawn. Conceptually, the brain performs some computation on <bold><italic>s</italic></bold>, the result of which are represented by neurons via their <italic>normalised</italic> tuning curves, Œ©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>), which we thus refer to as representational curves. These are multiplied by adaptive gains, <italic>g</italic><sub><italic>a</italic></sub>, to yields the actual tuning curves <italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>). The single-trial neural responses are noisy emissions based on <italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>). We assume the only function of gain modulation is to mitigate the eventual corruption of computational results by neural noise, subject to metabolic energy constraints.</p></caption>
<graphic xlink:href="564616v4_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We theorise that units adapt their gains to maximise an objective function that trades off the metabolic cost of neural activity with the information conveyed by the responses (<xref ref-type="bibr" rid="c29">Levy and Baxter, 1996</xref>; <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli, 2014</xref>). Informally
<disp-formula id="eqn2">
<graphic xlink:href="564616v4_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
As is common in efficient coding frameworks (see <italic>e</italic>.<italic>g</italic>., <xref ref-type="bibr" rid="c1">Atick and Redlich (1990)</xref>; <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref>) we assume the main contributor to metabolic cost is the energy cost of emitting action potentials. Thus we take the metabolic cost term in the objective to be proportional to <inline-formula><inline-graphic xlink:href="564616v4_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, the total average spike count fired by the population. On the other hand, a natural and common choice (up to approximations) for the coding fidelity is the mutual information between the stimulus and response, <italic>I</italic>(<bold><italic>n</italic></bold>; <bold><italic>s</italic></bold>). However, since in general the analytic maximisation of mutual information is intractable, in the tradition of efficient coding theory (<xref ref-type="bibr" rid="c7">Brunel and Nadal, 1998</xref>; <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="c30">Linsker, 1988</xref>), we will instead optimise an approximate surrogate for mutual information. We decompose the mutual information as <italic>I</italic>(<bold><italic>s</italic></bold>; <bold><italic>n</italic></bold>) = <italic>H</italic>[<bold><italic>n</italic></bold>] ‚àí <italic>H</italic>[<bold><italic>n</italic></bold> | <bold><italic>s</italic></bold>]. The marginal entropy term <italic>H</italic>[<bold><italic>n</italic></bold>] can be upper bounded by the entropy of a Gaussian with the same covariance,
<disp-formula id="eqn3">
<graphic xlink:href="564616v4_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Substituting the right hand side for the marginal entropy term in the mutual information, gives us an upper bound for the latter. Using this upper bound on mutual information as our quantification of ‚Äúcoding fidelity‚Äù, yields the objective function
<disp-formula id="eqn4">
<graphic xlink:href="564616v4_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the constant <italic>¬µ &gt;</italic> 0 controls the information-energy trade-off. This trade-off is illustrated in <xref rid="fig3" ref-type="fig">Fig. 3B</xref>. To summarise, we assume that the optimal gains maximise the objective <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref>
<disp-formula id="eqn5">
<graphic xlink:href="564616v4_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>The effect of changing the gain on tuning curves and different components of the objective function.</title>
<p>(A) As the gain, <italic>g</italic>, of a neuron is increased, the shape of its tuning curve remains the same, but all firing rates are scaled up. The inset shows the gain-independent representational curve, which we define as the tuning curve normalised to have a peak value of 1. (This cartoon shows a one-dimensional example, but our theory applies to general tuning curve shapes and joint configurations of population tuning curves, on high-dimensional stimulus spaces.) (B) Cartoon representation of the efficient coding objective function. Optimal neural gains maximise an objective function, ‚Ñí, which is the weighted difference between mutual information, <italic>I</italic>, capturing coding fidelity, and metabolic cost, <italic>E</italic>, given by average population firing rate.</p></caption>
<graphic xlink:href="564616v4_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Note that because the Œ©<sub><italic>a</italic></sub> depend deterministically on the stimulus <bold><italic>s</italic></bold>, and in turn fully determine the statistics of <italic>n</italic><sub><italic>a</italic></sub>, we have the mutual information identity <italic>I</italic>(<bold><italic>n</italic></bold>; <bold><italic>s</italic></bold>) = <italic>I</italic>(<bold><italic>n</italic></bold>; <bold>Œ©</bold>) (this identity also holds for the upper bound on the mutual information that we are using). This means that the coding fidelity term in the objective function can equivalently be interpreted, without any reference to the low-level stimulus, as (an upper bound on) the mutual information between the population‚Äôs noisy output and its ideal, noise-free outputs, <bold>Œ©</bold>, as specified by the computational goals of the circuit.</p>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Optimal adaptation of gains leads to rate homeostasis across a variety of noise models</title>
<p>We now investigate the consequences of optimal gain adaptation, according to <xref ref-type="disp-formula" rid="eqn4">Eqs. (4)</xref>‚Äì<xref ref-type="disp-formula" rid="eqn5">(5)</xref>, for the statistics of population response. As our hypothesis is that gains adapt in order to optimally mitigate the effect of neural noise on coding fidelity (with minimal energy expenditure), ideally we would like our conclusions to be robust to the model of noise. We thus consider a broad family of noise models which allow for general noise correlation structure, as well as different (sub- or super-Poissonian) scalings of noise strength with mean response. Specifically, we assume that conditioned on the stimulus, the noisy population response, <bold><italic>n</italic></bold>, has a normal distribution, with mean <bold><italic>h</italic></bold>(<bold><italic>s</italic></bold>), and stimulus-dependent covariance matrix
<disp-formula id="eqn6">
<graphic xlink:href="564616v4_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>h</italic>(<bold><italic>s</italic></bold>) is the diagonal matrix with the components of the tuning curve vector, <bold><italic>h</italic></bold>(<bold><italic>s</italic></bold>), on its diagonal (more generally, we adopt the convention that the non-bold and index-free version of a bold symbol, representing a vector, denotes a diagonal matrix made from that vector), the noise strength <italic>œÉ</italic> scales the overall strength of noise, and 0 <italic>&lt; Œ± &lt;</italic> 1 is a parameter that controls the power-law scaling of noise power with mean response <bold><italic>h</italic></bold>(<bold><italic>s</italic></bold>). Finally, Œ£(<bold><italic>s</italic></bold>) is the noise correlation matrix which can depend on the stimulus, but is independent of the gains. The noise model <xref ref-type="disp-formula" rid="eqn6">Eq. (6)</xref> can equivalently be characterised by (1) the noise variance of spike count <italic>n</italic><sub><italic>a</italic></sub> is given by <italic>œÉ</italic><sup>2</sup><italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>)<sup>2<italic>Œ±</italic></sup>, and (2) the noise correlation coefficient between <italic>n</italic><sub><italic>a</italic></sub> and <italic>n</italic><sub><italic>b</italic></sub> is given by Œ£<sub><italic>ab</italic></sub>(<bold><italic>s</italic></bold>), independently of the neural gains. Thus the strength (standard deviation) of noise scales sublinearly, as gain to the power <italic>Œ±</italic>. Note that the case <italic>œÉ</italic> = 2<italic>Œ±</italic> = 1 corresponds to Poisson-like noise with variance of <italic>n</italic><sub><italic>a</italic></sub> equal to its mean; in App. B.2, we show that, under mild conditions, a true Poisson noise model (with no noise correlations) leads to the same results as we derive below for the Gaussian noise model, <xref ref-type="disp-formula" rid="eqn6">Eq. (6)</xref>, with <italic>œÉ</italic> = 2<italic>Œ±</italic> = 1 and Œ£(<bold><italic>s</italic></bold>) = <italic>I</italic>.</p>
<p>In addition to the gains, the objective ‚Ñí (<bold><italic>g</italic></bold>), <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref>, also depends tacitly on (<italic>i</italic>.<italic>e</italic>., is a functional of) the stimulus distribution <italic>P</italic>, the representational curves <bold>Œ©</bold>, and the stimulus-dependent noise correlation matrix Œ£. As shown in App. B.1, for our class of noise models, <xref ref-type="disp-formula" rid="eqn6">Eq. (6)</xref>, ‚Ñí depends on <italic>P</italic>, <bold>Œ©</bold>, and Œ£ only through the following collection of signal and noise statistics. (By ‚Äúsignal‚Äù we refer to the noise-free trial-averaged population response, <bold><italic>h</italic></bold>(<bold><italic>s</italic></bold>); under variations of stimulus, <bold><italic>h</italic></bold>(<bold><italic>s</italic></bold>) can be thought of as a multivariate random variable with associated statistics.)</p>
<list list-type="order">
<list-item><p>The units‚Äô normalised average responses
<disp-formula id="eqn7">
<graphic xlink:href="564616v4_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
These are related to the units‚Äô mean responses, <italic>r</italic><sub><italic>a</italic></sub>, via <italic>r</italic><sub><italic>a</italic></sub> := ùîº[<italic>n</italic><sub><italic>a</italic></sub>] = ùîº[<italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>)] = <italic>g</italic><sub><italic>a</italic></sub><italic>œâ</italic><sub><italic>a</italic></sub>.</p></list-item>
<list-item><p>The relative size of stimulus-dependent signal variations, as quantified by the coefficients of variation of the units‚Äô trial-average responses, CV<sub><italic>a</italic></sub> := SD[<italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>)]<italic>/</italic>ùîº[<italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>)].</p></list-item>
<list-item><p>Signal correlation structure, as quantified by the matrix, <italic>œÅ</italic>, of correlation coefficients of the trial-averaged responses, <italic>œÅ</italic><sub><italic>ab</italic></sub> := Cov[<italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>), <italic>h</italic><sub><italic>b</italic></sub>(<bold><italic>s</italic></bold>)] <italic>/</italic> (SD[<italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>)]SD[<italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>)]).</p></list-item>
<list-item><p>Noise correlation structure, as captured by the average normalised noise covariance matrix (Note that for Poisson-like noise scaling, <italic>Œ±</italic> = 1<italic>/</italic>2, this matrix is actually a correlation matrix, in the sense that it is positive-definite with diagonal elements equal to 1.), <italic>W</italic>, defined by
<disp-formula id="eqn8">
<graphic xlink:href="564616v4_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="564616v4_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> denotes the tuning curve normalised by its stimulus average.</p></list-item>
</list>
<p>Note that all these statistics are invariant with respect to arbitrary rescalings of the tuning curves, and are thus independent of the gains ‚Äî in particular, in all the definitions above, the <italic>h</italic><sub><italic>a</italic></sub> could be replaced with the Œ©<sub><italic>a</italic></sub>. In terms of these quantities, the objective function, <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref>, is given by
<disp-formula id="eqn9">
<graphic xlink:href="564616v4_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and ‚Äî in line with the convention introduced above ‚Äî CV and <italic>r</italic> denote <italic>K √ó K</italic> diagonal matrices with CV<sub><italic>a</italic></sub> and <italic>r</italic><sub><italic>a</italic></sub> = <italic>œâ</italic><sub><italic>a</italic></sub><italic>g</italic><sub><italic>a</italic></sub> on their diagonals, respectively. Thus the objective depends on the gains, <bold><italic>g</italic></bold>, only via the average responses <bold><italic>r</italic></bold>. In addition, it also depends on the gain-independent quantities, CV, <italic>œÅ</italic>, and <italic>W</italic>, which capture aspects of signal and noise statistics.</p>
<p>We now present the results of numerical optimisation of the objective <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref> for different noise distributions within the general family of noise models, <xref ref-type="disp-formula" rid="eqn6">Eq. (6)</xref>, for a population of <italic>K</italic> = 10, 000 units. To capture the notion of adaptation to changes in the environment or context, we simulated a family of environment models smoothly parameterised by a single parameter <italic>ŒΩ</italic> ‚àà [0, 1]. As we have just seen, the objective depends only on <italic>œâ, œÅ, W</italic>, and CV. Changes in these quantities can arise due to changes in the stimulus density, <italic>P</italic> (<bold><italic>s</italic></bold>), the representational curves of the population, Œ©(<bold><italic>s</italic></bold>), or both. We specify environment models only through the parameters on which the objective depends (<italic>œâ, œÅ, W</italic>, and CV), remaining agnostic to the underlying cause of changes to these quantities.</p>
<p>For simplicity, we fixed the response coefficient of variation CV<sub><italic>a</italic></sub> = 1, the noise scale <italic>œÉ</italic><sup>2</sup> = 1, and the information-energy trade-off parameter <italic>¬µ</italic> = 5<italic>/</italic>(1 ‚àí <italic>Œ±</italic>) in all environments. (Below, in <xref ref-type="sec" rid="s2e">Sec. 2.5</xref>, we will discuss the biological basis for these parameter choices.) On the other hand, the normalised average response, <italic>œâ</italic><sub><italic>a</italic></sub>, and the signal correlation matrix, <italic>œÅ</italic>, depended smoothly on the environment parameter <italic>ŒΩ</italic>, as follows. For each unit <italic>a</italic> = 1, ‚Ä¶, <italic>K</italic>, the normalised average responses in the extreme environments, <italic>œâ</italic><sub><italic>a</italic></sub>(<italic>ŒΩ</italic> = 0) and <italic>œâ</italic><sub><italic>a</italic></sub>(<italic>ŒΩ</italic> = 1), were drawn independently from a wide distribution (specifically the beta distribution Beta(6, 1)). The normalised average response at intermediate values of <italic>ŒΩ, œâ</italic><sub><italic>a</italic></sub>(<italic>ŒΩ</italic>), was then obtained by linear interpolation between the independently sampled boundary values. This method for sampling environments is illustrated in <xref rid="fig4" ref-type="fig">Fig. 4</xref>. Similarly, to construct the signal correlation matrix <italic>œÅ</italic>(<italic>ŒΩ</italic>), we first constructed the boundary values <italic>œÅ</italic>(<italic>ŒΩ</italic> = 0) and <italic>œÅ</italic>(<italic>ŒΩ</italic> = 1) with independent random structures and smoothly interpolated between them (see <xref ref-type="sec" rid="s4a">Sec. 4.1</xref> for the details). We did this in such a way that, for all <italic>ŒΩ</italic>, the correlation matrices, <italic>œÅ</italic>(<italic>ŒΩ</italic>), were derived from covariance matrices with a 1<italic>/n</italic> power-law eigenspectrum (<italic>i</italic>.<italic>e</italic>., the ranked eigenvalues of the covariance matrix fall off inversely with their rank), in line with the findings of <xref ref-type="bibr" rid="c52">Stringer et al. (2019)</xref> in the primary visual cortex. Finally, the stimulus-averaged noise correlation matrix, <italic>W</italic>, is determined by our choice of the noise model subfamily used in each simulation. We now present simulation results for three 1-parameter subfamilies of the general family of noise models, <xref ref-type="disp-formula" rid="eqn6">Eq. (6)</xref>, in turn.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>The structure of pre-modulated average population responses in the simulated environments.</title>
<p>We simulated a one-parameter family of environments parameterised by <italic>ŒΩ</italic> ‚àà [0, 1]. In the environments on the two extremes, for each unit, the gain-independent average normalised responses <italic>œâ</italic><sub><italic>a</italic></sub>(<italic>ŒΩ</italic> = 0) and <italic>œâ</italic><sub><italic>a</italic></sub>(<italic>ŒΩ</italic> = 1) are randomly and independently sampled from a Beta(1, 6) distribution, with density shown on the left and right panels. For intermediate environments (<italic>i</italic>.<italic>e</italic>., for 0 <italic>&lt; ŒΩ &lt;</italic> 1), <italic>œâ</italic><sub><italic>a</italic></sub>(<italic>ŒΩ</italic>) is then obtained by linearly interpolating these values (middle panel). The middle panel shows <italic>K</italic> = 10000 such interpolations, with every 500th interpolation (ordered by initial value) coloured black.</p></caption>
<graphic xlink:href="564616v4_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s2b1">
<title>Uncorrelated power-law noise</title>
<p>Consider first the case of no average noise correlations, in the sense that <italic>W</italic> = <italic>I</italic> (In terms of the objective function, <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref>, this is mathematically equivalent to having zero stimulus-conditioned noise correlations, <italic>i</italic>.<italic>e</italic>., Œ£(<bold><italic>s</italic></bold>) = 1, but with CV‚Äôs redefined not to denote the standard coefficient of variation, but rather <inline-formula><inline-graphic xlink:href="564616v4_inline101.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where <inline-formula><inline-graphic xlink:href="564616v4_inline102.gif" mimetype="image" mime-subtype="gif"/></inline-formula>; for Poisson-like scaling, <italic>Œ±</italic> = 1<italic>/</italic>2, this reduces to the standard coefficient of variation.). We investigated the adaptation behavior of optimally gain-modulated population responses as a function of the noise scaling exponent <italic>Œ±</italic>. <xref rid="fig5" ref-type="fig">Fig. 5A</xref> shows the scaling of spike count variance as a function of mean spike count for different values of the noise scaling exponent <italic>Œ±</italic>. By construction, in the absence of adaptation, an environmental shift (<italic>ŒΩ</italic> = 0 <italic>‚Üí ŒΩ</italic> = 1) results in a wide range of average firing rates (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>, middle panel). But following adaptation, the firing rates become tightly concentrated around a single value (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>, bottom panel), which matches the value they were concentrated around in the original environment (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>, top panel). Thus, adaptation results in (1) <italic>uniformisation</italic> of average firing rates across the population in a fixed environment, and (2) firing rate <italic>homeostasis</italic>, where following adaptation the mean firing rates of the units returns to their value before the environmental shift. These same uniformisation and homeostatic effects are seen for other values of <italic>Œ±</italic> (<xref rid="fig5" ref-type="fig">Fig. 5B</xref> and Supp. Fig. 14). In <xref rid="fig5" ref-type="fig">Fig. 5C</xref> we quantify deviation from perfect homeostasis as the average % difference in the units‚Äô mean firing rates between the initial environment (<italic>ŒΩ</italic> = 0) and environments at increasing <italic>ŒΩ</italic>. As shown, the faster the spike count variance grows with mean spike count (<italic>i</italic>.<italic>e</italic>., the greater the noise scaling exponent <italic>Œ±</italic> is) the greater the deviation from firing rate homeostasis. But even for super-poissonian noise with <italic>Œ±</italic> = 0.75 the average relative change in firing rates between the environments never exceeds 3% (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Optimal gains under uncorrelated power-law noise lead to homeostasis of rates.</title>
<p>(A) Illustration of power-law noise. The trial-to-trial variance of spike counts, as a function of the trial-averaged spike count, is shown for different values of the noise scaling parameter <italic>Œ±</italic>. (B) Distribution of average firing rates before and after a discrete environmental shift from <italic>ŒΩ</italic> = 0 to <italic>ŒΩ</italic> = 1 for the case <italic>Œ±</italic> = 1<italic>/</italic>2. Top: the histogram of firing rates adapted to the original environment (<italic>ŒΩ</italic> = 0) before the shift, as determined by the optimal gains <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref> in this environment, <inline-formula><inline-graphic xlink:href="564616v4_inline106.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Middle: the histogram of firing rates immediately following the environmental shift to <italic>ŒΩ</italic> = 1, but before gain adaptation; these are proportional to <inline-formula><inline-graphic xlink:href="564616v4_inline107.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Bottom: the histogram of firing rates after adaptation to the new environment, proportional to <inline-formula><inline-graphic xlink:href="564616v4_inline108.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We have normalised firing rates such that the pre-adaptation firing rate distribution has mean 1. (C) Deviation from firing rate homeostasis for different values of <italic>Œ±</italic> as a function of environmental shift. For each environment (parametrised by <italic>ŒΩ</italic>), we compute the optimal gains and find the adapted firing rate under these gains. The average relative deviation of the (post-adaptation) firing rates from their value before the environmental shift, <inline-formula><inline-graphic xlink:href="564616v4_inline109.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, is plotted as a function of <italic>ŒΩ</italic>.</p></caption>
<graphic xlink:href="564616v4_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Even under super-poissonian noise (<italic>Œ±</italic> = 0.75 in <xref rid="fig5" ref-type="fig">Fig. 5C</xref>), the average relative change in firing rates between environments never gets above 3%. <xref rid="fig5" ref-type="fig">Fig. 5C</xref> additionally shows that the slower the spike count variance grows with mean spike count (<italic>i</italic>.<italic>e</italic>., the smaller the noise scaling exponent <italic>Œ±</italic> is) the more precise the firing rate homeostasis.</p>
</sec>
<sec id="s2b2">
<title>Uniform noise correlations</title>
<p>In the second sub-family of noise models we allow for noise correlations, but consider only Poisson-like scaling of noise (<italic>i</italic>.<italic>e</italic>., a noise scaling exponent <italic>Œ±</italic> = 1<italic>/</italic>2). More precisely, we assume that the effective noise correlation coefficients between all pairs of units (corresponding to off-diagonal elements of the matrix <italic>W</italic>, <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>) are the same, equal to <italic>p</italic> (<xref rid="fig6" ref-type="fig">Fig. 6A</xref>; The positivity of the noise correlation matrix, <italic>W</italic>, requires that <inline-formula><inline-graphic xlink:href="564616v4_inline103.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Assuming large <italic>K</italic>, the right side of the inequality goes to zero; hence we only simulated cases with positive <italic>p</italic>.). As shown in <xref rid="fig6" ref-type="fig">Fig. 6B</xref>, optimal gain adaptation leads to uniformisation and homeostasis of mean firing rates in this noise model as well. In fact, in the presence of uniform noise correlations, homeostasis is even stronger compared to the previous noise model with zero noise correlations; in this case, mean relative deviations in average firing rates never exceed 1% (<xref rid="fig6" ref-type="fig">Fig. 6C</xref>). Further, as noise correlations get stronger (<italic>i</italic>.<italic>e</italic>., as <italic>p</italic> increases) we see tighter homeostasis (see also Supp. Fig. 15).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Firing rate homeostasis under Poissonian noise with uniform noise correlations.</title>
<p>(A) Illustration of an effective noise correlation matrix with uniform correlation coefficients; the (green) off-diagonal elements have the same value, <italic>p &lt;</italic> 1, while the (black) diagonal elements are 1. Panels B and C have the same format as in <xref ref-type="fig" rid="fig5">Fig. 5</xref>.</p></caption>
<graphic xlink:href="564616v4_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b3">
<title>Aligned correlated noise</title>
<p>In the final noise model we considered, we once again fixed <italic>Œ±</italic> = 1<italic>/</italic>2 (Poisson-like scaling), but allowed for heterogeneous effective noise correlation coefficients. Specifically, we took the effective noise correlation matrix <italic>W</italic> to have approximately the same eigenbasis as <italic>œÅ</italic>, the signal correlation matrix ‚Äî hence ‚Äúaligned noise‚Äù‚Äî, but with a different eigenvalue spectrum. As described above, for all noise sub-families, the signal correlation structure <italic>œÅ</italic>(<italic>ŒΩ</italic>) was obtained by normalising a covariance matrix with 1<italic>/n</italic> spectrum (corresponding to the findings of <xref ref-type="bibr" rid="c52">Stringer et al. (2019)</xref> in V1); in the current case, we obtained the noise correlation structure <italic>W</italic> (<italic>ŒΩ</italic>) by normalising a covariance matrix with the same eigenbasis but with a 1<italic>/n</italic><sup><italic>Œ≥</italic></sup> spectrum (<italic>i</italic>.<italic>e</italic>., with the <italic>n</italic>-th largest eigenvalue of <italic>W</italic> scaling as 1<italic>/n</italic><sup><italic>Œ≥</italic></sup>); see <xref rid="fig7" ref-type="fig">Fig. 7A</xref>, and <xref ref-type="sec" rid="s4a">Sec. 4.1</xref> for further details. <xref rid="fig7" ref-type="fig">Fig. 7B</xref> demonstrates that, as with the other noise sub-families, optimal gain adaptation in the presence of aligned correlated noise also leads to homeostasis and uniformisation of mean firing rates (see Supp. Fig. 16 for rate histograms for other values of <italic>Œ≥</italic>). Across different values of <italic>Œ≥</italic>, homeostasis is again tighter compared to the zero noise correlation case, with the average firing rate shift never exceeding 1% (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>). Further, with the exception of <italic>Œ≥</italic> = 1, we see more homeostasis for higher values of <italic>Œ≥</italic>, corresponding to lower dimensional noise. The special case of <italic>Œ≥</italic> = 1 corresponds to so-called information-limiting noise (<xref ref-type="bibr" rid="c34">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="c22">Kanitscheider et al., 2015</xref>), where noise and signal correlation structures are fully aligned: <italic>W</italic> = <italic>œÅ</italic>. In this case, homeostasis is perfect, and the mean relative error in firing rates is 0. We provide an analytic proof that in this case we have perfect homeostasis and uniformisation in App. B.5, and discuss this situation further in <xref ref-type="sec" rid="s2e">Sec. 2.5</xref> and <xref ref-type="sec" rid="s2f">Sec. 2.6</xref>.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7:</label>
<caption><title>Firing rate homeostasis under signal-aligned noise.</title>
<p>(A) Eigenspectra of noise covariance matrix for the aligned noise model for various values of <italic>Œ≥</italic>; in this model, the <italic>n</italic>-th eigenvalue of the average noise correlation matrix, <italic>W</italic>, is proportional to 1<italic>/n</italic><sup><italic>Œ≥</italic></sup>. Panels B and C have the same format as in <xref ref-type="fig" rid="fig5">Fig. 5</xref>.</p></caption>
<graphic xlink:href="564616v4_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We have shown that, according to our efficient coding framework, optimal adaptation of response gains (with the goal of combating neural noise at minimal metabolic cost) can robustly account for the firing rate homeostasis of individual units within a population, despite environmental shifts or changes in neural representations (tuning curve shapes), under a diverse family of noise models. To shed light on these results, in <xref ref-type="sec" rid="s2d">Sec. 2.4</xref>, we analytically investigate sufficient conditions on the parameters, <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>‚Äì<xref ref-type="disp-formula" rid="eqn8">(8)</xref>, of our model under which we expect homeostasis to emerge. Following that, in <xref ref-type="sec" rid="s2e">Sec. 2.5</xref>, we will show that biological estimates for these parameters obtained in cortex (which are determined by natural stimulus statistics, as well as the stimulus dependence of high-dimensional population responses) are consistent with these conditions.</p>
</sec>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>A clustered population accounts for the diversity of firing rates seen in cortex</title>
<p>In addition to homeostasis, the above simulation results also predict uniformisation of average firing rates across the population, in any given environment. By contrast, cortical neurons display a wide range of average firing rates spanning a few orders of magnitude (<xref ref-type="bibr" rid="c8">Buzs√°ki and Mizuseki, 2014</xref>). But a cortical neuron can have very similar response tuning to other neurons in the same area, and neurons can therefore be grouped based on the similarity of their tuning curve shapes. We can thus interpret the units in our preceding numerical experiments not as single neurons, but as clusters of similarly tuned neurons. In this interpretation, uniformisation occurs at the cluster level, with potentially a wide distribution of single-cell average firing rates within each cluster. We found that our model is indeed consistent with such an interpretation. In particular, in the case of uncorrelated noise with Poisson-like mean-variance scaling, we show that a simple extension of our model gives rise to both diversity of firing rates as seen in cortex, as well as homeostasis and uniformisation at the level of clusters of similarly tuned neurons.</p>
<p>To gain intuition and allow for analytic solutions, we study a toy model in which the <italic>N</italic> neurons in the population are sorted into <italic>K</italic> such clusters. In this toy model, neurons within a cluster have very similar stimulus tuning, but neurons in different clusters are tuned differently (<xref rid="fig8" ref-type="fig">Fig. 8A</xref>); the model thus exaggerates a more realistic situation in which there is a more gradual transition from similar to dissimilar tuning curves within the entire population. A perturbation strength parameter, <italic>œµ ‚â•</italic> 0, controls how much the tuning curve shapes of single neurons in a cluster deviate from the stereotype tuning curve shape for that cluster (see <xref ref-type="disp-formula" rid="eqn25">Eq. (25)</xref> in Methods). When <italic>œµ</italic> = 0, we have perfect clusters, <italic>i</italic>.<italic>e</italic>., neurons in a cluster have identical representational curves. As the perturbation strength <italic>œµ</italic> increases, within-cluster deviations become stronger.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8:</label>
<caption><title>Optimal gain adaptation in a clustered population accounts for the broad distribution of cortical single-neuron firing rates.</title>
<p>(A) A cartoon description of the clustered neural population model. Top: The single neurons in the population belong to different functional groups or clusters (shown by different colors). Bottom: The normalised tuning curves of the single neurons belonging to the same cluster (thin lines of the same color) are very similar, and close to their cluster‚Äôs stereotype normalised tuning curve (solid lines) which can be very different across clusters. (B) Each panel shows the distribution of log firing rates of the active single neurons for different choices of cluster size <italic>k</italic> and effective dimensionality, <italic>D</italic>, of the within-cluster tuning curve variations. The distributions shown are for the active neurons with non-zero firing rate, with their percentage of the total population given in each panel (purple text). For all simulations we fixed the cluster firing rate to <italic>k √ó</italic> 5 Hz (corresponding to <italic>¬µ</italic> = 10, if we assume a coding interval of 0.1 s), such that the mean rate of single neurons is 5 Hz on average.</p></caption>
<graphic xlink:href="564616v4_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We assume single-neuron gains adapt to optimise the same efficient coding objective defined above in <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref>, but with units now clearly interpreted as single neurons. Note that optimisation in terms of the neural gains is equivalent to optimisation in terms of the neurons‚Äô mean firing rates (because the objective depends on the neurons‚Äô gains only via their mean fire rates each neuron‚Äôs mean rate is proportional to its gain by a fixed constant of proportionality, given by the mean value of its fixed and unoptimised representational curve, <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>). For <italic>œµ</italic> = 0 (identical tuning curve shapes within each cluster), the efficient coding objective function is blind to the precise distribution of single-cell firing rates among the neurons of a cluster, as long as the total rate of the cluster is held fixed. This is because for clusters of identically-tuned and independently-firing neurons with Poisson-like noise, the cluster can be considered as a single coherent unit whose firing rate is given by the sum of the firing rates of its neurons. Thus at zeroth order, the distribution of individual neuron rates are completely free and undetermined by the optimisation, as long as they give rise to optimal total cluster rates.</p>
<p>For small but non-zero perturbation strength <italic>œµ</italic> (corresponding to non-identical but sufficiently similar within-cluster tuning curve shapes), we showed that the efficient coding optimisation breaks down into two optimisation problems: one at the level of clusters, wherein the mean cluster rates are optimised, and one at the level of individual neurons within each cluster (Methods <xref ref-type="sec" rid="s4b">Sec. 4.2</xref> and App. B.3). The cluster-level problem remains decoupled from the within-cluster distribution of single-neuron rates; thus the optimisation of cluster firing rates reduces to the same optimisation problem considered in the previous sections, <xref ref-type="disp-formula" rid="eqn4">Eqs. (4)</xref>‚Äì<xref ref-type="disp-formula" rid="eqn5">(5)</xref>, with the units now interpreted not as neurons but as clusters of similarly tuned neurons. On the other hand, within-cluster optimisation of individual neuron rates decouples across clusters. Within each cluster, we show that this optimisation is approximately given by a quadratic program, subject to the constraint that the individual neuron rates sum up to the optimal total cluster rate found by solving the cluster-level problem (<xref ref-type="disp-formula" rid="eqn28">Eqs. (28)</xref>‚Äì<xref ref-type="disp-formula" rid="eqn30">(30)</xref> in <xref ref-type="sec" rid="s4b">Sec. 4.2</xref>).</p>
<p>The within-cluster quadratic program optimisation is specified by the signal covariance structure of the normalised tuning curves in the cluster, or more precisely, the covariance matrix of their deviations from the cluster stereotype normalised tuning curve (see <xref ref-type="disp-formula" rid="eqn30">Eq. (30)</xref>). A key characteristic of any covariance matrix is its effective dimensionality (corresponding roughly to the number of principal components with significant variance). Instead of making arbitrary choices about the signal covariance matrix defining the within-cluster optimisation, in our simulations we sampled them from a simple random ensemble which was controlled only by an effective dimensionality parameter, <italic>D</italic>, and the cluster size, <italic>k</italic> (see <xref ref-type="sec" rid="s4c">Sec. 4.3</xref> for details). <xref rid="fig8" ref-type="fig">Figure 8</xref> (see also Supp. Fig. 18) shows the resulting firing rate distributions for neurons aggregated across all clusters (corresponding to optimisations with independently sampled covariance matrices), for different values of <italic>k</italic> and <italic>D</italic>. Firstly, note that, depending on these parameters, a significant fraction of neurons can be silent, <italic>i</italic>.<italic>e</italic>., with zero firing rate in the optimal solution; the figure shows the percentage of and the rate distributions for the active neurons. Our theory therefore predicts that a significant fraction of cortical neurons are silent in any given sensory environment, and that which neurons are silent can shuffle following shifts in environmental stimulus statistics (specifically shifts that result in significant changes in the signal correlation structure of the cluster neurons). Secondly, note that the distributions span multiple orders of magnitude, in agreement with empirical observations in the cortex (see <italic>e</italic>.<italic>g</italic>. <xref ref-type="bibr" rid="c48">Slomowitz et al. (2015)</xref>; <xref ref-type="bibr" rid="c21">Hengen et al. (2013)</xref>; <xref ref-type="bibr" rid="c31">Maffei and Turrigiano (2008)</xref>), and is approximately log-normal (albeit with a skewed tail towards small log-rates), also consistent with empirical findings (<xref ref-type="bibr" rid="c8">Buzs√°ki and Mizuseki, 2014</xref>). Lastly, for fixed <italic>k</italic>, as the effective tuning curve dimension, <italic>D</italic>, increases, the fraction of silent neurons decreases (Supp. Fig. 18).</p>
<p>To summarise, we have shown that the objective ‚Ñí considered in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref>, reinterpreted to correspond to the cluster level, can arise from optimisation of a similar objective, defined at the level of single neurons in a functionally clustered population (<xref ref-type="disp-formula" rid="eqn26">Eqs. (26)</xref>‚Äì<xref ref-type="disp-formula" rid="eqn27">(27)</xref>). Furthermore, the correction to the cluster-level objective (arising from within-cluster variations in neural tuning) serves to break the symmetry within clusters, generating a diverse range of firing rates which matches the distribution of cortical single-neuron firing rates. In the rest of the paper, we will pursue the problem at the coarser level of clusters.</p>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Analytical insight on the optimality of homeostasis</title>
<p>The numerical results of <xref ref-type="sec" rid="s2b">Sec. 2.2</xref> showed that, across a range of neural noise models, optimal gain modulation for combating noise can give rise to firing rate homeostasis and uniformisation across neural clusters defined based on similarity of stimulus tuning. To shed light on these results, we analytically investigated how and in what parameter regimes these results arise. We found that homeostasis and uniformisation emerge in the regime where the matrix
<disp-formula id="eqn10">
<graphic xlink:href="564616v4_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
is small (according to an appropriate matrix norm); here <italic>Œ≤</italic> = 2 ‚àí 2<italic>Œ±</italic>. The intuitive meaning of this matrix is that its inverse, Œî<sup>‚àí1</sup>, characterises the strength of the signal-to-noise ratio (SNR) along different directions in the code space, <italic>i</italic>.<italic>e</italic>., the space of cluster responses. Therefore the above condition can be roughly interpreted as the requirement that signal-to-noise ratio is strong (This is a self-consistent statement, as Œî<sup>‚àí1</sup>, more precisely, encodes the high-dimensional SNR structure (which in general depends on the gains) <italic>under the approximate homeostatic solution</italic> <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref>. But since largeness of Œî<sup>‚àí1</sup> justifies that approximation, the largeness of SNR in a population with the homeostatic gains <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref> self-consistently justifies that homeostatic approximation.). We will discuss this condition, and the constraints it imposes on various model parameters, further in <xref ref-type="sec" rid="s2e">Sec. 2.5</xref>, and will provide evidence that it is likely to hold in cortex. Here, we focus on its consequences.</p>
<p>We developed a perturbative expansion in Œî, to obtain approximate analytical solutions for the cluster gains that maximise our objective ‚Ñí, <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref> (see App. B.4). In the leading approximation, <italic>i</italic>.<italic>e</italic>., to zeroth order in Œî, this expansion yields
<disp-formula id="eqn11">
<graphic xlink:href="564616v4_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We denote this approximate solution by <inline-formula><inline-graphic xlink:href="564616v4_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. To see the significance of this result, note that the average response of cluster <italic>a, i</italic>.<italic>e</italic>., its stimulus-averaged total spike count is given by ùîº[<italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>)] = <italic>g</italic><sub><italic>a</italic></sub><italic>œâ</italic><sub><italic>a</italic></sub>. Thus, in the leading approximation, the stimulus-average spike count of cluster <italic>a</italic> is given by the constant <inline-formula><inline-graphic xlink:href="564616v4_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Thus, since <italic>Œ≤¬µ</italic> is constant across both clusters and environments, the zeroth order solution yields homeostasis and uniformisation of cluster firing rates.</p>
<p>In <xref ref-type="sec" rid="s2f">Sec. 2.6</xref>, we numerically quantify the performance of the homeostatic approximation, <bold><italic>g</italic></bold><sup>(0)</sup>, in terms of maximising the efficient coding objective, under different noise models. Here, we also note the following exact analytical result regarding homeostasis that holds even for non-small Œî. It follows form the structure of the objective function, <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref>, that, irrespective of how pre-modulation mean rates, <italic>œâ</italic><sub><italic>a</italic></sub>, change between two stimulus environments, as long as Œî and <italic>W</italic>, the effective noise covariance, remain invariant across the environments, optimal gains imply exact homeostasis of each cluster‚Äôs average firing rate across the two environments. However, even in this case, unless Œî is small, in general the optimal solution does not result in uniformisation of firing rates across clusters.</p>
</sec>
<sec id="s2e">
<label>2.5</label>
<title>Conditions for the validity of the homeostatic solution hold in cortex</title>
<p>We asked whether cortical populations satisfy the conditions under which the homeostatic solution, <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref>, is expected to provide a good approximation to optimal gains according to efficient coding. As noted above, the general mathematical condition is that the matrix Œî, defined in <xref ref-type="disp-formula" rid="eqn10">Eq. (10)</xref>, is in some sense small (recall that through its inverse, Œî represents the high-dimensional structure of signal-to-noise ratio in the coding space, under the homeostatic solution <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref>). We can make this statement more precise by looking at the size of the leading-order correction to the approximate homeostatic solution <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref>, in the perturbative expansion. We found that the first-order correction modifies the approximate homeostatic solution for the gain <italic>g</italic><sub><italic>a</italic></sub> by a multiplicative factor 1 ‚àí Œî<sub><italic>aa</italic></sub> (see <xref ref-type="disp-formula" rid="eqn106">Eq. (106)</xref> in App. B.4). In other words, to leading order, the relative error in the homeostatic approximation to the optimal gain (and thus the optimal mean response) for unit <italic>a</italic> is given by |Œî<sub><italic>aa</italic></sub>|; we can thus reformulate the condition for the accuracy of the homeostatic approximation as
<disp-formula id="eqn12">
<graphic xlink:href="564616v4_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
for all <italic>a</italic>. Conversely, in the first approximation, deviations from homeostasis depend on the SNR structure, as captured by Œî<sub><italic>aa</italic></sub>, and how that structure shifts between environments.</p>
<p>Before proceeding to empirical estimates of Œî<sub><italic>aa</italic></sub>, let us first delineate the dependence of Œî on different model parameters. Firstly, note that Œî scales linearly with the noise strength <italic>œÉ</italic><sup>2</sup>; thus, if noise strength is scaled up, so is Œî. Next, consider increasing the noise scaling exponent <italic>Œ±</italic>. This decreases (<italic>Œ≤¬µ</italic>)<sup><italic>Œ≤</italic></sup> provided <italic>Œ±</italic> 1 <italic>‚â§</italic> ‚àí <italic>e/</italic>2<italic>¬µ</italic> (which holds in our simulations above). Moreover, increasing <italic>Œ±</italic> increases the diagonal elements of <italic>W</italic> (see <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>). Intuitively, this increases the magnitude of <italic>W</italic>. Through these two mechanisms, increasing the noise scaling exponent <italic>Œ±</italic> will generally increase the size of Œî. Next, note that Œî has an inverse relationship with the coefficients of variation, CV, which increase as neurons become more stimulus selective; accordingly, the more selective the tuning curves, the smaller the Œî. Lastly, Œî has a complex dependency on the spectra of both CV<italic>œÅ</italic> CV and the noise correlation structure <italic>W</italic> as well as the alignment of their eigenbases (which represent the geometric structure of signal and noise in the neural coding space, respectively). In <xref ref-type="sec" rid="s2f">Sec. 2.6</xref>, we will numerically investigate these dependencies, and their effects on the validity of the homeostatic approximation.</p>
<p>Translated to biological quantities, it follows from the above dependencies that the smallness of Œî (or Œî<sub><italic>aa</italic></sub>) requires that (1) the cluster firing rates are sufficiently high, (2) noise variance scales sufficiently slowly with mean response, (3) individual clusters are sufficiently selective in responding to stimuli, and (4) the neural representation has a high-dimensional geometry in the population response space. We used the available biological data to estimate the size of these variables for cortical populations; see <xref ref-type="sec" rid="s4d">Sec. 4.4</xref>. Here we summarise the resulting estimates.</p>
<sec id="s2e1">
<title>Sufficiently high firing rate</title>
<p>In the homeostatic approximation, <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref>, <italic>Œ≤¬µ</italic> is equal to the average total spike count of a cluster over the rate-coding time interval. Condition <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref> thus requires that the stimulus-averaged firing rate of all clusters are sufficiently high. Based on the available data (see <xref ref-type="sec" rid="s4d">Sec. 4.4</xref>), we estimate the mean firing rate in rodent V1 to be around 5 Hz. Assuming a coding interval of 0.1 seconds, and a cluster size of <italic>k</italic> = 20, this yields <italic>Œ≤¬µ ~</italic> 10.</p>
</sec>
<sec id="s2e2">
<title>Sufficiently slow scaling of noise variance</title>
<p>We can obtain an estimate for the prefactor <italic>œÉ</italic><sup>2</sup><italic>/</italic>(<italic>Œ≤¬µ</italic>)<sup><italic>Œ≤</italic></sup>, by fitting the following mean-variance relationship for cluster spike counts: variance = <italic>œÉ</italic><sup>2</sup>mean<sup>2‚àí<italic>Œ≤</italic></sup>. Empirical estimates of this relationship lead to estimates of <italic>œÉ</italic><sup>2</sup><italic>/</italic>(<italic>Œ≤¬µ</italic>)<sup><italic>Œ≤</italic></sup> ranging from 0.05 (<xref ref-type="bibr" rid="c46">Shadlen and Newsome, 1998</xref>) to 0.31 (<xref ref-type="bibr" rid="c26">Koyama, 2015</xref>), with a median value 0.14 (see <xref ref-type="sec" rid="s4d">Sec. 4.4</xref> for details). This is close to the value for Poissonian noise (<italic>œÉ</italic><sup>2</sup> = 1, <italic>Œ≤</italic> = 1), which is 0.1.</p>
</sec>
<sec id="s2e3">
<title>Sufficiently selective responses</title>
<p>The coefficient of variation CV for neural responses is a measure of neural selectivity. Accordingly, it is related to another measure of stimulus selectivity, namely the coding level (<italic>i</italic>.<italic>e</italic>., the fraction of stimuli to which a neuron responds). Using empirical estimates of the latter quantity for cortical neurons (<xref ref-type="bibr" rid="c28">Lennie, 2003</xref>), we estimated CV<sup>2</sup> <italic>‚âà</italic> 20. Below we take CV<sup>2</sup> <italic>‚âà</italic> 10 as a more conservative estimate.</p>
</sec>
<sec id="s2e4">
<title>High-dimensional signal geometry</title>
<p>Assuming, for simplicity, that the coefficients of variation are approximately the same for all clusters, we see from <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref> that Œî<sub><italic>aa</italic></sub> are proportional to the diagonal elements of the matrix <italic>œÅ</italic><sup>‚àí1</sup><italic>W</italic>. Assuming further, that these elements are all comparable in size (This is expected to be valid when the eigenbasis of <italic>œÅ</italic><sup>‚àí1</sup><italic>W</italic> is not aligned with the standard (cluster) basis, which, in turn, corresponds to distributed coding.), we estimated their order of magnitude based on the known structure of the signal correlation matrix, <italic>œÅ</italic>, in V1 (see below), and for the aligned noise family of average noise covariance matrices, <italic>W</italic>, studied in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref>. This family includes two biologically relevant cases: (1) an uncorrelated noise model corresponding to <italic>W</italic> = <italic>I</italic>, and (2) a <italic>W</italic> with so-called information-limiting noise correlations (<xref ref-type="bibr" rid="c34">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="c22">Kanitscheider et al., 2015</xref>). In the first case Œî<sub><italic>aa</italic></sub> is proportional to [<italic>œÅ</italic><sup>‚àí1</sup>]<sub><italic>aa</italic></sub>. It can be shown that [<italic>œÅ</italic><sup>‚àí1</sup>]<sub><italic>aa</italic></sub> <italic>‚â•</italic> 1, with equality if and only if cluster <italic>a</italic> has zero signal signal correlation with every other cluster. [<italic>œÅ</italic><sup>‚àí1</sup>]<sub><italic>aa</italic></sub> can thus be seen as a measure of representational redundancy. Many traditional efficient coding accounts predict zero signal correlation between neurons, at least in the low noise limit (<xref ref-type="bibr" rid="c3">Barlow, 2012</xref>; <xref ref-type="bibr" rid="c1">Atick and Redlich, 1990</xref>; <xref ref-type="bibr" rid="c38">Nadal and Parga, 1994</xref>, <xref ref-type="bibr" rid="c39">1999</xref>), providing a normative justification for low signal correlations. However, complete lack of signal correlations is not necessary for [<italic>œÅ</italic><sup>‚àí1</sup>]<sub><italic>aa</italic></sub> to be small. Geometrically, small signal correlations correspond to neural responses forming a representation with high intrinsic dimensionality. Stringer <italic>et al</italic>. (<xref ref-type="bibr" rid="c52">Stringer et al., 2019</xref>) found that the signal covariance matrix of mouse V1 neurons responding to natural stimuli possesses an approximately 1<italic>/n</italic> eigen-spectrum. In this case, for <italic>K</italic> large, we obtain the estimate [<italic>œÅ</italic><sup>‚àí1</sup>]<sub><italic>aa</italic></sub> <italic>‚âà</italic> ln(<italic>K</italic>)<italic>/</italic>2 (see App. B.6). Given the slow logarithmic increase with <italic>K</italic>, we expect that our condition can hold even for very large neural populations. For example, suppose we take the entire human V1 as our neural population, to obtain an arguably generous upper bound for <italic>K</italic>. This contains roughly 1.5 <italic>√ó</italic> 10<sup>8</sup> neurons (<xref ref-type="bibr" rid="c62">Wandell, 1995</xref>), leading to 7.5 <italic>√ó</italic> 10<sup>6</sup> clusters of 20 neurons, and therefore an average value of [<italic>œÅ</italic><sup>‚àí1</sup>]<sub><italic>aa</italic></sub> just under 8. This, together with our other estimates above, yields the following overall estimate for Œî<sub><italic>aa</italic></sub>
<disp-formula id="eqn13">
<graphic xlink:href="564616v4_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We found that in the case of <italic>information-limiting noise correlations</italic> the |Œî<sub><italic>aa</italic></sub>| tend to be even smaller than the above estimate. Information-limiting noise correlations refer to the case where directions of strong noise in the coding space align with those of strong signal variation. The extreme case, <italic>i</italic>.<italic>e</italic>., perfect alignment of noise and signal geometry, corresponds to <italic>œÅ</italic> = <italic>W</italic>, or equivalently <italic>œÅ</italic><sup>‚àí1</sup><italic>W</italic> = <italic>I</italic>, for which Œî<sub><italic>aa</italic></sub> <italic>‚àù</italic> [<italic>œÅ</italic><sup>‚àí1</sup><italic>W</italic>]<sub><italic>aa</italic></sub> = 1 does not grow with <italic>K</italic> at all. In App. B.6 we analysed less extreme forms of alignment (corresponding to the model numerically studied in <xref rid="fig7" ref-type="fig">Fig. 7</xref>, <xref ref-type="sec" rid="s2b">Sec. 2.2</xref>), and found (unsurprisingly) that for those cases, the estimates of Œî<sub><italic>aa</italic></sub> lie between the estimates for the two above cases. Accordingly, even partial signal-noise alignment can (potentially strongly) reduce the size of Œî below the estimate <xref ref-type="disp-formula" rid="eqn13">Eq. (13)</xref>, further encouraging homeostatic coding.</p>
<p>The above analysis makes it clear when we should expect homeostasis in general ‚Äì when cluster firing rates are high enough, the noise scaling of cluster responses is sufficiently slow, responses are highly selective, and signal correlation structure corresponds to a high-dimensional geometry (<xref ref-type="bibr" rid="c52">Stringer et al., 2019</xref>), possibly with information-limiting noise correlations (<xref ref-type="bibr" rid="c34">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="c22">Kanitscheider et al., 2015</xref>). On the other hand, when these conditions are violated, the optimal gain configuration can potentially deviate strongly from the homeostatic solution. Moreover, from the above estimates, leading to <xref ref-type="disp-formula" rid="eqn13">Eq. (13)</xref>, we see that in V1 and possibly other cortical areas, we can expect the leading corrections to the homeostatic solution to not exceed 10%, in relative terms.</p>
</sec>
</sec>
<sec id="s2f">
<label>2.6</label>
<title>Numerical comparison of homeostatic gains to optimal gains</title>
<p>We now return to the numerical simulations performed in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref>, to evaluate the performance and accuracy of the approximate homeostatic solution, <inline-formula><inline-graphic xlink:href="564616v4_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref>, and the first-order correction to it, <inline-formula><inline-graphic xlink:href="564616v4_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, relative to the true optimal solution of <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref>. We also evaluate the performance and accuracy of the best possible homeostatic solution, <italic>i</italic>.<italic>e</italic>., the best configuration of cluster gains, <inline-formula><inline-graphic xlink:href="564616v4_inline6a.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, that, by construction, are guaranteed to yield homeostasis and uniformisation. In other words, under such gains the mean responses <italic>r</italic><sub><italic>a</italic></sub> = <italic>g</italic><sub><italic>a</italic></sub><italic>œâ</italic><sub><italic>a</italic></sub> must equal a constant (<italic>i</italic>.<italic>e</italic>., independent of <italic>a</italic>, the cluster index, and constant across environments). Denoting this <italic>a priori</italic> unknown constant by <italic>œá</italic>, we thus have
<disp-formula id="eqn14">
<graphic xlink:href="564616v4_eqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
under which the average spike count of clusters in the coding interval is given by <italic>œá</italic>. The variable <italic>œá</italic> is then chosen to optimise the expectation of ‚Ñí, <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref>, across a relevant family of environments (note that we need to optimise the average objective across environments, since otherwise the optimal <italic>œá</italic> will in general depend on the environment, invalidating homeostasis). With such an optimal <italic>œá</italic>, <bold><italic>g</italic></bold><sup>hom</sup> will, by definition, perform better than <bold><italic>g</italic></bold><sup>(0)</sup> in terms of our objective function, ‚Ñí (Further, it can be shown (see App. B.7) that <italic>œá &lt; Œ≤¬µ, i</italic>.<italic>e</italic>., the mean cluster firing rate under the optimal homeostatic solution is strictly smaller than that predicted by <bold><italic>g</italic></bold><sup>(0)</sup>, and that the average spike-count of the entire population of clusters is approximately the same under this solution as it is under the first-order approximation <italic>g</italic><sup>(1)</sup>.). In fact, as we will show below, in our simulations (with model parameters within the biologically plausible ranges identified in the previous section), the homeostatic <bold><italic>g</italic></bold><sup>hom</sup> performed very close to the true optimal gains.</p>
<p>For each of the different noise models considered in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref> we compare the performance of the homeostatic gains <bold><italic>g</italic></bold><sup>(0)</sup> and <bold><italic>g</italic></bold><sup>hom</sup>, as well as the perturbative correction, <bold><italic>g</italic></bold><sup>(1)</sup>, to the numerically optimised gains. To examine the adaptation properties of these gains, we once again work with the sequence of environments indexed by <italic>ŒΩ</italic> ‚àà [0, 1], with the same specifications as in the numerical simulations of <xref ref-type="sec" rid="s2b">Sec. 2.2</xref>. To measure the accuracy of the approximations to the optimal gains, for each environment <italic>ŒΩ</italic>, we calculated the mean relative errors
<disp-formula id="eqn15">
<graphic xlink:href="564616v4_eqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
for each of the approximate solutions, denoted by <bold><italic>g</italic></bold><sup>app</sup>(<italic>ŒΩ</italic>), with <bold><italic>g</italic></bold><sup>opt</sup>(<italic>ŒΩ</italic>), the numerically optimised solution in environment <italic>ŒΩ</italic>. To measure the performance of an approximate solution, we use the following <italic>relative improvement</italic> measure, defined by
<disp-formula id="eqn16">
<graphic xlink:href="564616v4_eqn16.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
which can be interpreted as the improvement in the objective ‚Ñí (<italic>¬∑</italic>; <italic>ŒΩ</italic>) (the efficient coding objective <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref> according to the statistics of environment <italic>ŒΩ</italic>) achieved by the adaptive gains <bold><italic>g</italic></bold><sup>app</sup>(<italic>ŒΩ</italic>) over the unadapted gains from the original environment <bold><italic>g</italic></bold><sup>app</sup>(0), relative to the improvement obtained by the optimally adaptive gains <bold><italic>g</italic></bold><sup>opt</sup>(<italic>ŒΩ</italic>). The better the approximation the closer <italic>C</italic><sup>app</sup>(<italic>ŒΩ</italic>) will be to 1. Note that, in general, the relative improvement worsens when <italic>ŒΩ</italic> goes to 0 (see <xref rid="fig9" ref-type="fig">Fig. 9B, D</xref> and <xref ref-type="fig" rid="fig9">F</xref>); this follows from the definition of <italic>C</italic>(<italic>ŒΩ</italic>) and the fact that when environmental change is weak, using gains optimised in the original environment without adapting them in the new environment is superior to adaptive but only approximately optimal gains. We now summarise the results for the different noise models discussed in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref> (see that section for the detailed specification of the three noise models).</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9:</label>
<caption><title>Accuracy of homeostatic approximation to optimal gains.</title>
<p>(A) The relative error, <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref>, of different approximate solutions for the optimal gains, averaged across environments (the standard deviations of these values across environments were negligible relative to the average), as a function of the noise scaling parameter <italic>Œ±</italic>. To give a sense of the scale of variation of the optimal gains across environments, the black line shows the relative change in the optimal gains between the most extreme environments (a measure of effect size), <inline-formula><inline-graphic xlink:href="564616v4_inline110.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. (B) Relative improvement in the objective for the homeostatic approximation <bold><italic>g</italic></bold><sup>hom</sup> as measured by <xref ref-type="disp-formula" rid="eqn16">Eq. (16)</xref>. Panels C and E (panels F and D) are the same as panel A (panel B), but for the uniformly correlated noise and the aligned noise models, respectively.</p></caption>
<graphic xlink:href="564616v4_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s2f1">
<title>Uncorrelated power-law noise</title>
<p>First, we consider the uncorrelated noise model with general power-law scaling of noise strength with mean responses. In this case, the homeostatic solution <bold><italic>g</italic></bold><sup>hom</sup>, and the first order correction, <bold><italic>g</italic></bold><sup>(1)</sup>, had very low (<italic>&lt;</italic> 3%) relative errors for all noise scaling exponents, <italic>Œ±</italic>, while the relative error in <bold><italic>g</italic></bold><sup>(0)</sup> becomes large for significantly super-Poissonian noise scaling, <italic>i</italic>.<italic>e</italic>., <italic>Œ±</italic> = 0.75 (<xref rid="fig9" ref-type="fig">Fig. 9A</xref>; For the case of uncorrelated power-law noise model, as well as the aligned correlated noise model, we used an analytical approximation to the optimal <italic>œá</italic> (see App. B.7) used in the homeostatic approximation <inline-formula><inline-graphic xlink:href="564616v4_inline104.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, <xref ref-type="disp-formula" rid="eqn14">Eq. (14)</xref>. This means that the performance measures in the plots of <xref rid="fig9" ref-type="fig">Fig. 9B and F</xref> are lower-bounds for the performance of <inline-formula><inline-graphic xlink:href="564616v4_inline105.gif" mimetype="image" mime-subtype="gif"/></inline-formula> with the truly optimal <italic>œá</italic>.). The relative improvement measure <italic>C</italic><sup>hom</sup>(<italic>ŒΩ</italic>) for the homeostatic solution is also close to one, the optimal value, for all <italic>Œ±</italic>, except for 0.75 (<xref rid="fig9" ref-type="fig">Fig. 9B</xref>). For the other approximate solutions too (Supp. Fig. 17A,B), the relative improvement decreases with increasing noise scaling. This is consistent with our analysis, since increasing <italic>Œ±</italic> increases the size of Œî (see <xref ref-type="sec" rid="s2d">Sec. 2.4</xref>) and hence the influence of terms of higher order in Œî.</p>
</sec>
<sec id="s2f2">
<title>Uniform noise correlation coefficients</title>
<p>We now examine the effect of noise correlations on the accuracy of the approximate homeostatic solutions, first within the noise family with uniform noise correlation coefficients, denoted by <italic>p</italic>, and Poisson-like scaling (<italic>Œ±</italic> = 1<italic>/</italic>2). For all <italic>p</italic>, the relative deviation of the homeostatic solution <bold><italic>g</italic></bold><sup>hom</sup> from optimal gains was below 1% (<xref rid="fig9" ref-type="fig">Fig. 9C</xref>), while <bold><italic>g</italic></bold><sup>(0)</sup> and <bold><italic>g</italic></bold><sup>(1)</sup> were less accurate overall. Note that the accuracy of the homeostatic solutions <bold><italic>g</italic></bold><sup>hom</sup> and <bold><italic>g</italic></bold><sup>(0)</sup> increases with <italic>p</italic>, suggesting that positive noise correlations improve the optimality of homeostatic gains. The same trend can be seen in the relative improvement on the objective, <italic>C</italic><sup>hom</sup>, for homeostatic gain adaptation, which increases with <italic>p</italic> (<xref rid="fig9" ref-type="fig">Fig. 9D</xref>).</p>
</sec>
<sec id="s2f3">
<title>Aligned noise</title>
<p>Lastly, we considered the algined noise model, in which noise has Poisson-like scaling (<italic>i</italic>.<italic>e</italic>., <italic>Œ±</italic> = 1<italic>/</italic>2), and is correlated across clusters with the noise and signal covariance matrices sharing the same eigenbasis. In this noise family, the parameter, <italic>Œ≥</italic>, controls the scaling of the eigenvalues of the noise covariance matrix with their rank, <italic>n</italic>, as 1<italic>/n</italic><sup><italic>Œ≥</italic></sup>. As in the previous cases, for all <italic>Œ≥</italic>, the homeostatic solution <bold><italic>g</italic></bold><sup>hom</sup> was very close to the optimal gains <bold><italic>g</italic></bold><sup>opt</sup> (<xref rid="fig9" ref-type="fig">Fig. 9E</xref>), and preformed nearly equally (<xref rid="fig9" ref-type="fig">Fig. 9F</xref>). The case <italic>Œ≥</italic> = 1, which corresponds to perfect alignment between signal and noise correlations, <italic>W</italic> = <italic>œÅ</italic>, is particularly interesting. As noted in the previous section, this case corresponds to that of so-called information-limiting noise correlations (<xref ref-type="bibr" rid="c34">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="c22">Kanitscheider et al., 2015</xref>). As we show in App. B.5, in this case the optimal gains, without any approximation, lead to homeostasis and uniformisation of cluster firing rates. Thus, in this case <bold><italic>g</italic></bold><sup>opt</sup> = <bold><italic>g</italic></bold><sup>hom</sup> exactly, leading to zero relative error (<xref rid="fig9" ref-type="fig">Fig. 9E</xref>) for <bold><italic>g</italic></bold><sup>hom</sup>, and perfect relative improvement <italic>C</italic><sup>hom</sup> (<xref rid="fig9" ref-type="fig">Fig. 9F</xref>). For other cases, we can see that for increasing <italic>Œ≥</italic> (at least for the values we simulated), which corresponds to more correlated and lower-dimensional noise (as noise power falls off faster as a function of eigenvalue rank), the homeostatic gains <bold><italic>g</italic></bold><sup>hom</sup> tend to perform better.</p>
<p>Our numerical simulations across the three noise subfamilies (with varying noise power scaling, noise correlations, and noise spectrum) demonstrate that homeostatic strategies can - robustly and across a wide range of noise distributions - combat coding noise by effectively navigating the trade-off between energetic costs and coding fidelity.</p>
<p>Finally, note that adaptation in <bold><italic>g</italic></bold><sup>hom</sup> uses only information local to each neuron, <italic>i</italic>.<italic>e</italic>., its firing rate. This implies that good performance does not require the use of complex regulation mechanisms which take into account how the encoding of each cluster relates to the population at large; homeostatic regulation, implemented by purely local mechanisms, is sufficient. In fact, homeostatic regulation can be implemented via a simple mechanism of synaptic scaling (<xref ref-type="bibr" rid="c58">Turrigiano et al., 1998</xref>; <xref ref-type="bibr" rid="c57">Turrigiano, 2008</xref>). Synaptic scaling involves the normalisation of synaptic weights onto a neuron in order to keep the total synaptic mass constant. In Appendix B.8, we demonstrate that, in a linear feedforward network, synaptic normalisation, via synaptic scaling, is both necessary and sufficient to allow for the propagation of homeostatic coding from the first layer to the subsequent ones. Thus, homeostatic coding in feedforward networks provides an additional normative interpretation of synaptic normalisation.</p>
</sec>
</sec>
<sec id="s2g">
<label>2.7</label>
<title>Homeostatic noisy DDCs</title>
<p>When exposed to over- or under-represented stimuli, neurons across sensory systems exhibit stimulus-specific adaptation whereby they decrease or increase their gains only for a limited subset of stimuli (<xref ref-type="bibr" rid="c36">Movshon and Lennie, 1979</xref>; <xref ref-type="bibr" rid="c37">M√ºller et al., 1999</xref>; <xref ref-type="bibr" rid="c14">Dragoi et al., 2000</xref>, <xref ref-type="bibr" rid="c13">2002</xref>; <xref ref-type="bibr" rid="c40">Nelken, 2014</xref>). In particular, homeostatic adaptation in the primary visual cortex (V1) has been observed to be stimulus-specific (<xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>). In this section and the next (<xref ref-type="sec" rid="s2h">Sec. 2.8</xref>), we combine our efficient coding theory of homeostatic adaptation with a specific computational theory of neural representations. Then, in <xref ref-type="sec" rid="s2i">Sec. 2.9</xref>, we show that the resulting theory is able to account for stimulus-specific adaptation effects observed in V1, which have been difficult to account for under other efficient coding frameworks (see our discussion of previous attempts in <xref ref-type="sec" rid="s3">Sec. 3</xref>). This theory thus provides a unified normative explanation for these two types of adaptation.</p>
<p>Recall that up to this point we have made no assumptions about the nature of cortical representations (beyond rate coding, and the condition described by <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref>), and thus our framework has been independent of the computational goals of the circuit. We now apply our framework to a specific theory of neural representation and computation, namely the distributive distributional code (DDC) (<xref ref-type="bibr" rid="c43">Sahani and Dayan, 2003</xref>; <xref ref-type="bibr" rid="c61">Vertes and Sahani, 2018</xref>), introducing what we term a <italic>homeostatic DDC</italic>.</p>
<p>While there is ample behavioural and psychophysical evidence for the hypothesis that animal perception and decision making approximate optimal Bayesian probabilistic inference (<xref ref-type="bibr" rid="c24">Kersten et al., 2004</xref>; <xref ref-type="bibr" rid="c51">Stocker and Simoncelli, 2006</xref>; <xref ref-type="bibr" rid="c60">van Bergen et al., 2015</xref>; <xref ref-type="bibr" rid="c17">Geurts et al., 2022</xref>), our knowledge of the neural implementation of computations serving Bayesian inference (such as computations of posterior distributions and posterior expectations) are rudimentary at best. The DDC framework is one proposal for neural implementations of Bayesian computations underlying perception. In a DDC model, neural responses directly encode posterior expectations of the inferred latent causes of sensory inputs according to an internal generative model. This internal model mathematically relates the latent causes or latent variables, which we denote by <bold><italic>z</italic></bold>, to the sensory inputs or stimuli, <bold><italic>s</italic></bold>, via a family of conditional distributions, <italic>f</italic> (<bold><italic>s</italic></bold> | <bold><italic>z</italic></bold>), as well as a prior distribution, <italic>œÄ</italic>(<bold><italic>z</italic></bold>), over the latent variables. The sensory system is assumed to implement a so-called <italic>recognition model</italic> corresponding to this generative model. Namely, given a stimulus <bold><italic>s</italic></bold>, the task of the sensory system is to invert the generative process by calculating and representing the posterior distribution of latent variables given the current sensory input (see the schema in <xref rid="fig10" ref-type="fig">Fig. 10</xref>).</p>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10:</label>
<caption><title>Schema of the generative and recognition models underlying a DDC.</title>
<p>According to the internal generative model, sensory inputs, <bold><italic>s</italic></bold>, are caused by latent causes, <bold><italic>z</italic></bold>, that occur in the environment according to a prior distribution <italic>œÄ</italic>(<bold><italic>z</italic></bold>). A conditional distribution <italic>f</italic> (<bold><italic>s</italic></bold> | <bold><italic>z</italic></bold>), the so-called likelihood function, describes how the latent causes generate or give rise to the sensory inputs <bold><italic>s</italic></bold>. The task of the brain is to invert this generative process by inferring the latent causes based on the current sensory input, which is done by computing the posterior distribution Œ†(<bold><italic>z</italic></bold>|<bold><italic>s</italic></bold>).</p></caption>
<graphic xlink:href="564616v4_fig10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In a DDC, neural responses are specifically postulated to directly represent the posterior expectations of the latent variables, <bold><italic>z</italic></bold>, or rather, the posterior expectations of a sufficiently rich (and fixed) class of so-called kernel functions of <bold><italic>z</italic></bold>. Specifically, in the standard DDC, each neuron, say neuron <italic>a</italic>, is assigned to a so-called kernel function of the latent variables, <italic>œï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>), and its response is given by the posterior expectation ùîº[<italic>œï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>) | <bold><italic>s</italic></bold>]. Here, we modify the standard DDC scheme in two ways. First, we assume single-trial neuron responses are corrupted by noise and represent the posterior expectations of the kernel functions only on average. Second, we adopt the key premise of our general efficient coding framework, namely that neural gains adapt in order to optimally mitigate the effect of neural noise on representational fidelity. Specifically, we assume this gain adaptation takes the homeostatic form, <xref ref-type="disp-formula" rid="eqn14">Eq. (14)</xref> (which we found approximates the optimal solution of the efficient coding problem for noise mitigation). These modifications lead to a new coding scheme which we call <italic>noisy homeostatic DDC</italic>. In this scheme, as in the standard DDC, the tuning curve of a neuron, with assigned kernel function <italic>œï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>), is proportional to the posterior expectation ùîº[<italic>œï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>) | <bold><italic>s</italic></bold>]. However, unlike the standard DDC, the corresponding constants of proportionality are not absolute constants but adapt (independently for different neurons) in such a way as to give rise to firing rate homeostasis. In other words, we assume the representational curve of neuron <italic>a</italic> is proportional to ùîº[<italic>œï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>) | <bold><italic>s</italic></bold>], while its gain adapts across contexts or environments so as to maintain homeostasis. Since (under an ideal-observer internal model) the posterior expectation of a quantity averaged over stimuli is equal to that quantity‚Äôs prior expectation, we have ùîº[ùîº[<italic>œï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>) | <bold><italic>s</italic></bold>]] = ùîº[<italic>œï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>)]. Therefore, keeping average firing rates constant at level <italic>œá</italic> (independent of the kernel function or the environmental statistics) requires the gains to be inversely proportional to the prior expectations, ùîº[<italic>œï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>)], and thus the tuning curve of neuron <italic>a</italic> to be given by
<disp-formula id="eqn17">
<graphic xlink:href="564616v4_eqn17.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In other words, in noisy homeostatic DDCs, the neural tuning curves are (up to the absolute constant <italic>œá</italic>) the ratio of the posterior to the prior expectations of the neurons‚Äô assigned kernel functions. Note that the prior expectation in the denominator, which by definition does not depend on the stimulus, <bold><italic>s</italic></bold>, does depend implicitly on the environmental stimulus statistics. Indeed, this is also true of the posterior expectation in the numerator (as it combines information about the current stimulus with prior expectations about the latent causes of the stimulus input which can depend on the environment). Thus, in homeostatic DDCs, not only the gains, but also the shapes of tuning curves can adapt to changes in stimulus statistics, and thus vary across environments. This corresponds to a mixture of the two special scenarios sketched in the two panels of <xref rid="fig1" ref-type="fig">Fig. 1</xref>: as the stimulus distribution changes (<xref rid="fig1" ref-type="fig">Fig. 1A</xref> second row), (1) the tuning curve shapes change (<xref rid="fig1" ref-type="fig">Fig. 1B</xref> top row), as dictated by the computational goal of representing posterior expectations of latent variables under the changed statistics, and (2) the gains adapt in order to mitigate the effect of noise at minimal metabolic cost, resulting in mean response homeostasis (right columns). As we will see in <xref ref-type="sec" rid="s2i">Sec. 2.9</xref>, this combined adaptation can explain stimulus-specific adaptation.</p>
</sec>
<sec id="s2h">
<label>2.8</label>
<title>Homeostatic DDCs can be implemented by divisive normalisation</title>
<p>We now show that for the special case when the DDC kernel function of neuron <italic>a</italic> is approximately a delta function, <italic>œï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>) <italic>‚âà Œ¥</italic>(<bold><italic>z</italic></bold> ‚àí <bold><italic>z</italic></bold><sub><italic>a</italic></sub>), the homeostatic DDC can be implemented by divisive normalisation with adaptive weights (<xref ref-type="bibr" rid="c66">Westrick et al., 2016</xref>). For <italic>œï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>) <italic>‚âà Œ¥</italic>(<bold><italic>z</italic></bold> ‚àí <bold><italic>z</italic></bold><sub><italic>a</italic></sub>), <xref ref-type="disp-formula" rid="eqn17">Eq. (17)</xref> becomes
<disp-formula id="eqn18">
<graphic xlink:href="564616v4_eqn18.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where in the last equality we used Bayes‚Äô rule: Œ†(<bold><italic>z</italic></bold> | <bold><italic>s</italic></bold>) = <italic>f</italic> (<bold><italic>s</italic></bold> | <bold><italic>z</italic></bold>)<italic>œÄ</italic>(<bold><italic>z</italic></bold>)<italic>/P</italic> (<bold><italic>s</italic></bold>). Thus, in this case, the tuning curve of neuron <italic>a</italic> is the ratio of the posterior distribution to the prior distribution both evaluated at that neuron‚Äôs preferred latent variable value, <bold><italic>z</italic></bold><sub><italic>a</italic></sub>. Accordingly, we call this coding scheme <italic>Bayes-ratio coding</italic>.</p>
<p>Divisive normalisation has been dubbed a canonical neural operation (<xref ref-type="bibr" rid="c9">Carandini and Heeger, 2012</xref>), as it appears in multiple brain regions serving different computational goals. Given the feedforward inputs, <italic>F</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>), the divisive normalisation model computes the tuning curve of neuron <italic>a</italic> as
<disp-formula id="eqn19">
<graphic xlink:href="564616v4_eqn19.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here, <italic>w</italic><sub><italic>b</italic></sub> are a collection of so-called normalisation weights, and <italic>Œ± ‚â•</italic> 0 and <italic>n ‚â•</italic> 1 are constants. To show that Bayes-ratio coding can be implemented by a divisive normalisation model, we note that <italic>P</italic> (<bold><italic>s</italic></bold>), which appears in <xref ref-type="disp-formula" rid="eqn18">Eq. (18)</xref>, is given by the marginalisation of the joint density <italic>P</italic> (<bold><italic>s</italic></bold>) = ‚à´ <italic>P</italic> (<bold><italic>s, z</italic></bold>)<italic>d</italic><bold><italic>z</italic></bold> = ‚à´ <italic>f</italic> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold>)<italic>œÄ</italic>(<bold><italic>z</italic></bold>)<italic>d</italic><bold><italic>z</italic></bold>. We approximate the integral by a sum as <italic>P</italic> (<bold><italic>s</italic></bold>) <italic>‚âà</italic> ‚àë<sub><italic>b</italic></sub> <italic>f</italic> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold><sub><italic>b</italic></sub>)<italic>œÄ</italic>(<bold><italic>z</italic></bold><sub><italic>b</italic></sub>)Œî<bold><italic>z</italic></bold><sub><italic>b</italic></sub> =<sub><italic>b</italic></sub> <italic>w</italic><sub><italic>b</italic></sub><italic>f</italic> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold><sub><italic>b</italic></sub>), where we defined the weights by <italic>w</italic><sub><italic>b</italic></sub> = <italic>œÄ</italic>(<bold><italic>z</italic></bold><sub><italic>b</italic></sub>)<italic>Œ¥</italic><bold><italic>z</italic></bold><sub><italic>b</italic></sub> (We have assumed the latent variable space is partitioned into cells, one for each neuron, with <italic>Œ¥</italic><bold><italic>z</italic></bold><sub><italic>b</italic></sub> denoting the volume of the cell centred on neuron <italic>b</italic>‚Äôs preferred latent variable <bold><italic>z</italic></bold><sub><italic>b</italic></sub>.). Substituting this approximation in <xref ref-type="disp-formula" rid="eqn18">Eq. (18)</xref>, and regularising the denominator by an additive small constant <italic>Œ± &gt;</italic> 0, we obtain
<disp-formula id="eqn20">
<graphic xlink:href="564616v4_eqn20.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
By <xref ref-type="disp-formula" rid="eqn19">Eq. (19)</xref> this corresponds to a divisive normalisation model with <italic>n</italic> = 1. Not only does this result show that implementing Bayes-ratio coding is biologically plausible, it also provides a normative computational interpretation to both the feedforward inputs and the normalisation weights of the divisive normalisation model, as the internal generative model‚Äôs likelihoods and prior probabilities, respectively. Note also that the normalisation weights of this model are adaptive, and vary between environments that have different latent variable distributions, <italic>œÄ</italic>(<bold><italic>z</italic></bold>).</p>
<p>Finally, we consider the propagation of Bayes-ratio coding between populations. Suppose the generative model has the hierarchical structure <bold><italic>z</italic></bold><sup>(2)</sup> <bold><italic>z</italic></bold><sup>(1)</sup> ‚Üí <bold><italic>s</italic></bold>, and consider a feedforward network with two populations or layers that receives the stimulus and computes a Bayes-ratio encoding of the posterior distributions over <bold><italic>z</italic></bold><sup>(1)</sup> and <bold><italic>z</italic></bold><sup>(2)</sup>. Assume that the first layer is implementing Bayes-ratio coding for the lower-level latent variables <bold><italic>z</italic></bold><sup>(1)</sup> (<italic>e</italic>.<italic>g</italic>., using divisive normalisation <xref ref-type="disp-formula" rid="eqn20">Eq. (20)</xref>). We show in App. B.9 that if the feedforward synaptic weights into the second layer are given by <inline-formula><inline-graphic xlink:href="564616v4_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, the downstream population implements Bayes-ratio coding for <bold><italic>z</italic></bold><sup>(2)</sup> with linear neurons. This result is significant for three reasons. Firstly, while the downstream population is part of a <italic>recognition</italic> model that represents the posterior distribution of <bold><italic>z</italic></bold><sup>(2)</sup>, the synaptic weights needed to implement this representation only require knowledge of the <italic>generative</italic> probabilities (of <bold><italic>z</italic></bold><sup>(1)</sup> given <bold><italic>z</italic></bold><sup>(2)</sup>). Secondly, the synaptic weights make no reference to the prior distribution over <bold><italic>z</italic></bold><sup>(2)</sup>. There is therefore no need to adapt the weights upon environmental changes that only affect the statistics of the higher-level latent causes <bold><italic>z</italic></bold><sup>(2)</sup> but not the generative process (as encoded by <italic>f</italic>). Thirdly, this result can clearly be extended to a multi-level generative model by induction.</p>
</sec>
<sec id="s2i">
<label>2.9</label>
<title>Homeostatic DDCs account for stimulus specific adaptation</title>
<p>We found that homeostatic DDCs can provide a normative account of stimulus specific adaptation (SSA). Here, we start by showing this in the special case of Bayes-ratio coding, which is mathematically simpler and provides good intuitions for a more general subset of homeostatic DDCs. We then discuss this more general case and build a homeostatic DDC model of empirically observed SSA in the primary visual cortex (V1) (<xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>).</p>
<p>According to <xref ref-type="disp-formula" rid="eqn18">Eq. (18)</xref>, for Bayes-ratio codes the tuning curve of neuron <italic>a</italic> is given by <italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>) = <italic>œáf</italic> (<bold><italic>s</italic></bold> | <bold><italic>z</italic></bold><sub><italic>a</italic></sub>)<italic>/P</italic> (<bold><italic>s</italic></bold>). Suppose now that the stimulus distribution in the environment, <italic>P</italic> (<bold><italic>s</italic></bold>), changes due to a change in the statistics of the latent causes, <italic>œÄ</italic>(<bold><italic>z</italic></bold>), while the causal processes linking the latent causes and observed stimuli, as captured by <italic>f</italic> (<bold><italic>s</italic></bold> | <bold><italic>z</italic></bold>), remain stable. In this case, by <xref ref-type="disp-formula" rid="eqn18">Eq. (18)</xref>, the tuning curve of <italic>all</italic> neurons are modified by a multiplicative factor given by the ratio of the <italic>P</italic> (<bold><italic>s</italic></bold>) in the old environment to that in the new environment. This will lead to a multiplicative suppression of the response of all neurons to stimuli that are over-represented in the new environment relative to the old one, in a way that is independent of the neurons‚Äô identity or preferred stimulus. Such a suppression thus constitutes a pure form of stimulus specific adaptation. In particular, neurons do not reduce their responsiveness to stimuli that are not over-represented in the new environment. Thus, tuning curves are suppressed only near over-represented stimuli (and may in fact be enhanced near under-represented stimuli), leading to a repulsion of tuning curve peak from over-represented stimuli. This repulsion is a typical manifestation of stimulus specific adaptation; in V1, for example, orientation tuning curves display a repulsion from an over-represented orientation (<xref ref-type="bibr" rid="c36">Movshon and Lennie, 1979</xref>; <xref ref-type="bibr" rid="c37">M√ºller et al., 1999</xref>; <xref ref-type="bibr" rid="c14">Dragoi et al., 2000</xref>, <xref ref-type="bibr" rid="c13">2002</xref>; <xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>).</p>
<p>Heretofore we have implicitly made a so-called ideal observer assumption by assuming that the internal generative model underlying the DDC or Bayes-ratio code is a veridical model of the stimulus distribution in the environment. We now provide a significant generalization of the above results to the more general and realistic case of a non-ideal-observer internal model. As we will see, in this case, the effects of adaptation on tuning curves are captured by both a stimulus-specific factor as well as a neuron-specific factor. For a non-ideal observer model, there will be a discrepancy between the environment‚Äôs true stimulus distribution, <italic>P</italic><sup><italic>E</italic></sup>(<bold><italic>s</italic></bold>), and the marginal stimulus distribution predicted by the internal generative model, <italic>i</italic>.<italic>e</italic>.,
<disp-formula id="eqn21">
<graphic xlink:href="564616v4_eqn21.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In App. B.10, we demonstrate that the Bayes-ratio tuning curves in this case are given by
<disp-formula id="eqn22">
<graphic xlink:href="564616v4_eqn22.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
(instead of <xref ref-type="disp-formula" rid="eqn18">Eq. (18)</xref>) where
<disp-formula id="eqn23">
<graphic xlink:href="564616v4_eqn23.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Note that, for ideal-observer internal models, <italic>P</italic><sup><italic>E</italic></sup> = <italic>P</italic><sup><italic>I</italic></sup>, in which case the integral yields <italic>F</italic><sub><italic>a</italic></sub> = 1 due to the normalisation of <italic>f</italic> (<bold><italic>s</italic></bold> | <bold><italic>z</italic></bold>), and we recover <xref ref-type="disp-formula" rid="eqn18">Eq. (18)</xref>. As we can see, the tuning curves neatly decompose into a ‚Äúbase tuning curve‚Äù <italic>œáf</italic> (<bold><italic>s</italic></bold> | <bold><italic>z</italic></bold><sub><italic>a</italic></sub>) multiplied by a stimulus-specific factor <italic>P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>)<sup>‚àí1</sup> and a neuron-specific factor <inline-formula><inline-graphic xlink:href="564616v4_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Consequently, if we consider a pair of environments indexed by <italic>ŒΩ</italic> = 0, 1, and assume the likelihood function of the internal model does not change between environments, the transformation of the tuning curves due to adaptation will be given by <xref ref-type="disp-formula" rid="eqn24">Eq. (24)</xref>,
<disp-formula id="eqn24">
<graphic xlink:href="564616v4_eqn24.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Thus, adaptation from environment <italic>ŒΩ</italic> = 0 to environment <italic>ŒΩ</italic> = 1 causes the tuning curves to transform via multiplication by a <italic>stimulus specific factor P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>; <italic>ŒΩ</italic> = 0)<italic>/P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>; <italic>ŒΩ</italic> = 1) and a <italic>neuron specific factor F</italic><sub><italic>a</italic></sub>(<italic>ŒΩ</italic> = 0)<italic>/F</italic><sub><italic>a</italic></sub>(<italic>ŒΩ</italic> = 1). Now suppose an environmental change results in an increase in the prevalence of an adaptor stimulus. As long as the internal model is a sufficiently good (if not perfect) model of <italic>P</italic><sup><italic>E</italic></sup>(<bold><italic>s</italic></bold>), it should adapt its prior distribution, <italic>œÄ</italic>(<bold><italic>z</italic></bold>), in a way that results in an increase of its predicted stimulus distribution, <italic>P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>), around the adaptor stimulus. This results in a stimulus specific factor that goes below 1 around the adaptor <bold><italic>s</italic></bold>, thus causing suppression and repulsion of tuning curves away from the adaptor. Now consider a neuron whose preferred stimulus (roughly represented by <bold><italic>z</italic></bold><sub><italic>a</italic></sub>) is close to the adaptor stimulus. We further make the assumption that the change in the internal model, in response to the environmental change, is sufficiently conservative such that it leads to a smaller increase in <italic>P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>) around the adaptor, compared with the increase in the veridical <italic>P</italic><sup><italic>E</italic></sup>(<bold><italic>s</italic></bold>) (this is a reasonable assumption especially for artificially extreme adaptation protocols used in experiments). In this case, we expect the ratio <italic>P</italic><sup><italic>E</italic></sup>(<bold><italic>s</italic></bold>)<italic>/P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>) to increase in the support of <italic>f</italic> (<bold><italic>s</italic></bold> | <bold><italic>z</italic></bold><sub><italic>a</italic></sub>), which (by our assumption about the preferred stimulus <bold><italic>z</italic></bold><sub><italic>a</italic></sub>) is near the adaptor. According to <xref ref-type="disp-formula" rid="eqn23">Eq. (23)</xref> this will lead to an increase in <italic>F</italic><sub><italic>a</italic></sub>, and hence a suppressive neuron-specific factor <italic>F</italic><sub><italic>a</italic></sub>(<italic>v</italic> = 0)<italic>/F</italic><sub><italic>a</italic></sub>(<italic>v</italic> = 1) for neurons with preferred stimulus near the adaptor. In accordance with this picture, <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref> found that homeostatic and stimulus-specific adaptation in V1 can indeed be modelled via separable stimulus-specific and neuron-specific factors.</p>
<p>Note that the assumption of constancy of the likelihood across the two environments need not be framed as a veridical assumption about the objective environmental change, but rather as an assumption about the inductive biases of the internal generative model, according to which it tends to model changes in the observed stimulus distribution, <italic>P</italic><sup><italic>E</italic></sup>(<bold><italic>s</italic></bold>), as primarily resulting from changes in the statistics of the latent causes in the environment (which the model captures in <italic>œÄ</italic>(<bold><italic>z</italic></bold>)), rather than in the causal process itself, as modelled by <italic>f</italic> (<bold><italic>s</italic></bold> | <bold><italic>z</italic></bold>). As long as this is good enough an assumption to result in a <italic>P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>) which changes in similar ways to <italic>P</italic><sup><italic>E</italic></sup>(<bold><italic>s</italic></bold>) (and in particular exhibits an increase around adaptor stimuli), the conclusions reached above will hold.</p>
<p>In the case of DDC codes that use more general kernel functions, other than the delta functions underlying Bayes-ratio codes, we arrive at an analogous transformation, given by <xref ref-type="disp-formula" rid="eqn140">Eq. (140)</xref> in App. B.10. However, in this case, neuron-specific and stimulus-specific effects are mixed, and the wider the DDC kernel the stronger the mixing. Nevertheless, for kernels that are unimodal and sufficiently narrow, we expect an approximate factorisation of the effect of adaptation into a stimulus-specific and a neuron-specific suppression factor, as seen in <xref ref-type="disp-formula" rid="eqn24">Eq. (24)</xref>.</p>
<p>We now apply this result to the experiments performed by <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>. These experiments examined the effects of adaptation on orientation-tuned neurons in the cat primary visual cortex. Anaesthetised cats were shown a sequence of oriented gratings chosen randomly from a distribution that had an increased prevalence (3 to 5 times more likely) of one particular orientation (arbitrarily defined to be to 0<sup>¬∞</sup>). A control group was exposed to a uniform distribution of gratings. After about 2 seconds (on the order of 50 stimulus presentations), V1 tuning curves had adapted, and exhibited both the suppressive and repulsive effects mentioned above.</p>
<p>To model these findings we constructed a homeostatic DDC model as follows, and fit it to the data from <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>. We took the model‚Äôs stimulus and latent variable spaces to be the circular orientation space [‚àí90, 90), and we took the model‚Äôs generative likelihoods to be given by translation invariant functions <italic>f</italic> (<italic>s</italic> | <italic>z</italic>) = <italic>f</italic> (<italic>s</italic> ‚àí <italic>z</italic>), proportional to the Gaussian distribution with standard deviation <italic>œÉ</italic><sub><italic>f</italic></sub>, normalised over the circle (More rigorously, we ought to use a wrapped (periodic) normal distribution. However, since the standard deviations are small compared to the length of the circle, the normalisation constant is approximately 1, and we can treat the density as a normal Gaussian.). Likewise, we take the DDC kernel functions <italic>œï</italic><sub><italic>a</italic></sub>(<italic>z</italic>) = <italic>œï</italic> (<italic>z</italic> ‚àí <italic>z</italic><sub><italic>a</italic></sub>) proportional to the Gaussian distribution with standard deviation <italic>œÉ</italic><sub><italic>œï</italic></sub>.</p>
<p>The distributions of stimulus orientations used in the experiment is highly artificial, in the way they jump discontinuously near the adaptor orientation. The internal prior distribution employed by the brain is more likely to be smooth, reflecting the brain‚Äôs inductive bias adapted to natural environments. Thus in our model we chose the internal prior to be a smooth distribution. The orientation distribution used in the experiment can be understood as a mixture of a uniform distribution, and a spike concentrated at the adaptor (<xref rid="fig11" ref-type="fig">Fig. 11A</xref>, blue curve). To obtain the smooth internal prior in our model, we replace the spike component of the experimental orientation distribution with a Gaussian distribution centred at the adaptor with standard deviation <italic>œÉ</italic><sub><italic>œÄ</italic></sub> (<xref rid="fig11" ref-type="fig">Fig. 11A</xref>, red curve). Thus, our model has only three free parameters, <italic>œÉ</italic><sub><italic>f</italic></sub>, <italic>œÉ</italic><sub><italic>œï</italic></sub> and <italic>œÉ</italic><sub><italic>œÄ</italic></sub>, <italic>i</italic>.<italic>e</italic>., the widths of the generative likelihood, kernel functions, and the internal orientation prior, respectively.</p>
<fig id="fig11" position="float" fig-type="figure">
<label>Figure 11:</label>
<caption><title>Components of the model for stimulus-specific adaptation in V1.</title>
<p>(A) The distribution of stimulus orientations used in the experiments (<xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>), for an adaptor probability of 30% is shown in blue. We assume V1‚Äôs internal model use continuous prior distributions and thus we used instead a mixture of the uniform distribution with a smooth Gaussian, with standard deviation <italic>œÉ</italic><sub><italic>œÄ</italic></sub>, centered at the adaptor orientation (red curve). (B) The baseline tuning curves, <italic>i</italic>.<italic>e</italic>., the tuning curves adapted to the uniform orientation distribution. The blue curve was obtained by averaging the experimentally measured tuning curves adapted to the uniform orientation distribution, after centering those curves at 0<sup>¬∞</sup>. The model‚Äôs baseline tuning curves (red) are given by the convolution of the Gaussian likelihood function, with width <italic>œÉ</italic><sub><italic>f</italic></sub>, of the postulated internal generative model, and the Gaussian DDC kernel, with width <italic>œÉ</italic><sub><italic>œï</italic></sub>, and thus are themselves Gaussian with width <inline-formula><inline-graphic xlink:href="564616v4_inline111.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We fit this quantity to the experimentally observed baseline curves as described in <xref ref-type="sec" rid="s4e">Sec. 4.5</xref>.</p></caption>
<graphic xlink:href="564616v4_fig11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Using the dataset from <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>, we found the baseline tuning curves, <italic>i</italic>.<italic>e</italic>., the tuning curves of the population adapted to the uniform orientation distribution, by assuming that neural response is a function only of the difference between the preferred orientation and stimulus orientation (see <xref ref-type="sec" rid="s4e">Sec. 4.5</xref>). We then calculate the changes in the neurons‚Äô preferred orientations (due to adaptation and the resulting tuning curve repulsion) in each of the experimental conditions (corresponding to different levels of over-representation of the adaptor stimulus). We then fit our model‚Äôs three parameters to both the unadapted tuning curves (<xref rid="fig11" ref-type="fig">Fig. 11B</xref>) and the changes in preferred orientation (see <xref ref-type="sec" rid="s4e">Sec. 4.5</xref> for further details). The best fit values for the model parameters were found to be <italic>œÉ</italic><sub><italic>œÄ</italic></sub> = 18<sup>¬∞</sup>, <italic>œÉ</italic><sub><italic>f</italic></sub> = 14.2<sup>¬∞</sup>, and <italic>œÉ</italic><sub><italic>œï</italic></sub> = 20<sup>¬∞</sup>. The predictions of our model are compared to experimental data in <xref rid="fig12" ref-type="fig">Fig. 12</xref>. We found that the adaptive tuning curves within our model display suppression and repulsion away from the adaptor stimulus (<xref rid="fig12" ref-type="fig">Fig. 12A,D</xref>). In both the experimental data and our model, unadapted tuning curves exhibit a heightened average firing rate near the adaptor stimulus (<xref rid="fig12" ref-type="fig">Fig. 12B,E</xref>, blue curves). However, adaptation suppresses responses near the adaptor (<xref rid="fig12" ref-type="fig">Fig. 12A,D</xref>, red curves) to just the right degree to return the firing rates to the value before the environmental shift, leading to homeostasis of firing rates (as demonstrated by the uniformity of the red curves in <xref rid="fig12" ref-type="fig">Fig. 12B,E</xref>). Lastly, our model recapitulates the repulsion of preferred orientations found experimentally (<xref rid="fig12" ref-type="fig">Fig. 12C,F</xref>). Repulsion here is reflected by the change in preferred orientation having the same sign as the difference between the pre-adaptation preferred orientation and the adaptor orientation. As seen, in both the experiment and the model, repulsion is strongest for neurons with pre-adaptation preferred orientation about 30<sup>¬∞</sup> from the adaptor.</p>
<fig id="fig12" position="float" fig-type="figure">
<label>Figure 12:</label>
<caption><title>A homeostatic DDC model accounts for the observations of stimulus-specific adaptation in V1 <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>.</title>
<p>Panels A-C (left column) and E-F (right column) correspond to the experimental results and the predictions of our model, respectively. (A, D) The tuning curves for the adapted (red) and unadapted (blue) populations, averaged across experimental conditions. (B, E) The trial-averaged firing rates of the adapted (red) and unadapted (blue) populations exposed to the non-uniform stimulus ensemble. The trial-averaged population responses to the uniform stimulus ensemble were normalised to 1. (C, F) The repulsion of preferred orientations, obtained from the average tuning curves in panel A, as a function of the deviation of the neurons‚Äô preferred orientation from the adaptor orientation.</p></caption>
<graphic xlink:href="564616v4_fig12.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In sum, the application of our homeostatic coding framework to a specific DDC model of Bayesian representations (which approximates what we have termed Bayes-ratio coding) explains quantitatively the empirical observations of both stimulus-specific and homeostatic adaptation in V1 (<xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>).</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<p>We developed a theory of optimal gain modulation for combating noise in neural representations serving arbitrary computations. We demonstrated that ‚Äî when mean neural firing rates are not too small; neurons are sufficiently selective and sparsely activated; and their responses form a high-dimensional geometry ‚Äî the trade-off between coding fidelity and metabolic cost is optimised by gains that react to shifts in environmental stimulus statistics to yield firing rate homeostasis. Specifically, our theory predicts the homeostasis of the mean firing rate for clusters of similarly tuned neurons, while, at the same time, it accounts for the optimality of a broad distribution of firing rates within such clusters. By examining empirical estimates of parameters characterising cortical representations, we argued that the conditions necessary for the homeostatic adaptation to be optimal are expected to hold in the visual cortex and potentially other cortical areas. We further validated our approximation by demonstrating numerically that it performs well compared to the calculated optimal gains. Having developed a normative theory of neural homeostasis divorced from the computational aims of the neural population, we next showed how our theory can account for stimulus specific adaptation when coupled with distributed distributional codes (DDC). In particular, we focused on a special case of homeostatic DDC codes which we termed Bayes-ratio coding and showed that this model can account for stimulus specific adaptation effects empirically observed in the primary visual cortex. In the following, we will situate our work within the broader context of efficient coding theory, Bayesian neural representations, and alternative theories of stimulus specific adaptation. We will also discuss the shortcomings of the present work and possible directions for extension.</p>
<p>Within efficient coding theory, our work is closely related to that of <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref>, but differs from it in important ways. <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref> considered a population of tuning curves, defined on a one-dimensional stimulus space, which was homogeneous and translationally invariant up to a warping (<italic>i</italic>.<italic>e</italic>., reparametrization) of the stimulus space and possible variations in gain. Their normative framework then optimised the neural gains as well as the stimulus warping (<italic>i</italic>.<italic>e</italic>., the reparametrization mapping from the original stimulus parameter to one over which the tuning curve population is homogeneous) to maximise a similar objective function to ours. Specifically, their objective employed the Fisher Information lower bound on the mutual information (<xref ref-type="bibr" rid="c7">Brunel and Nadal, 1998</xref>), as opposed to the Gaussian upper bound we used; our choice of the Gaussian upper bound was motivated by increased analytic tractability in the multidimensional stimulus case covered by our theory. In the case of unimodal tuning curves, they showed that the optimal warping amounts to a concentration and sharpening of tuning curves around stimulus values that are over-represented in the environment. On the other hand, in their setting, the optimal gains stay constant and do not adapt. Nevertheless, their solution also exhibits homeostasis of single-neuron firing rates.</p>
<p>Our framework extends and generalises that of <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref> in different ways. In their framework, the stimulus space is one-dimensional, and rigid constraints are placed on the shape and arrangement of the tuning curve population (they consider only unimodal or sigmoidal tuning curves, and assume that, up to a reparameterization of the stimulus space, all tuning curves have the same shape and are homogeneously placed). <xref ref-type="bibr" rid="c63">Wang et al. (2016)</xref> and <xref ref-type="bibr" rid="c68">Yerxa et al. (2020)</xref> have generalised the framework of <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref> to the case of alternative coding fidelity objective functions and high-dimensional stimuli, respectively; however, both of these studies maintain the same rigid constraint over the shape and arrangement of the tuning curve population. In contrast, our framework is applicable to tuning curves with heterogeneous (in particular multi-modal) shapes and arbitrary arrangement over a high-dimensional stimulus space; our homeostatic solution only requires sufficiently high signal-to-noise ratio (which is also the condition for the tightness of the mutual information lower-bound employed in <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref> or the approximation to the reconstruction-error loss used by <xref ref-type="bibr" rid="c63">Wang et al. (2016)</xref>). On the other hand, we only optimised the neural gains, and not the shapes or arrangement of the tuning curves, as we assumed the latter are determined by the computational goals of the circuit and not by the aim of optimally combating coding noise; in this way, our framework is agnostic to the computational goals of the population. Additionally, <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref> only considered the case of uncorrelated Poissonian noise. In contrast, our framework can handle correlated noise with general (non-Poissonian) power-law scaling. Furthermore, we analytically calculated first-order corrections to the homeostatic solution that arises in the high signal-to-noise ratio limit.</p>
<p>As noted above, the theory of <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref> predicts that tuning curves optimised to provide an efficient code under a stimulus distribution with over-represented stimuli cluster around those stimuli. This has been used to explain asymmetries in neural selectivities, such as the over-abundance of V1 neurons with preferred orientations near cardinal orientations, which are more abundant in natural scenes; or more generally, the aggregation of tuning curves around stimuli that are more prevalent in the natural environment. Many of these asymmetries have likely resulted from adaptation at very long (<italic>e</italic>.<italic>g</italic>., ontogenetic, or possibly evolutionary) timescales; however, long-term exposure on the order of few minutes to adaptor stimuli has also been found to result in the attraction of tuning curves towards adaptor stimuli (<xref ref-type="bibr" rid="c18">Ghisovan et al., 2009</xref>). This is in contrast to the repulsive effects seen in short-term adaptation (<xref ref-type="bibr" rid="c36">Movshon and Lennie, 1979</xref>; <xref ref-type="bibr" rid="c37">M√ºller et al., 1999</xref>; <xref ref-type="bibr" rid="c14">Dragoi et al., 2000</xref>, <xref ref-type="bibr" rid="c13">2002</xref>; <xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>). It is likely that the effects of long-term exposure are the results of mechanisms which operate in parallel to those underlying short-term adaptation, with both types co-existing at different timescales. We can therefore interpret <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref> as a model of adaptation to natural stimulus statistics at long timescales. Our results obtained here are applicable to adaptation on shorter timescales (<xref ref-type="bibr" rid="c37">M√ºller et al., 1999</xref>; <xref ref-type="bibr" rid="c14">Dragoi et al., 2000</xref>, <xref ref-type="bibr" rid="c13">2002</xref>; <xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>). In particular, when married with Bayesian theories of neural representation, our framework predicts repulsive effects around an adaptor stimulus, as shown in <xref ref-type="sec" rid="s2i">Sec. 2.9</xref>.</p>
<p><xref ref-type="bibr" rid="c66">Westrick et al. (2016)</xref> also propose a model of the experimental findings of <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>. Their model (which is not explicitly normative or based on efficient coding) uses divisive normalisation (<xref ref-type="bibr" rid="c9">Carandini and Heeger, 2012</xref>) with adaptive weights to achieve homeostasis and stimulus specific repulsion. As discussed above in <xref ref-type="sec" rid="s2h">Sec. 2.8</xref>, Bayes-ratio coding (a special case of homeostatic DDCs, which we showed can account for the findings of <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>) can be accomplished by such a divisive normalisation scheme. Our framework therefore yields a normative interpretation of the model of <xref ref-type="bibr" rid="c66">Westrick et al. (2016)</xref>, and links divisive normalisation with Bayesian representations and efficient coding theory. In particular, our theory provides an interpretation of the normalisation weights of the divisive normalisation model as the internal prior over latent causes of sensory inputs. This prior should naturally adapt as stimulus statistics change across environments. Additionally, the feedforward inputs to the divisive normalisation model are interpreted as the likelihood of stimuli given latent causes.</p>
<p><xref ref-type="bibr" rid="c49">Snow et al. (2016)</xref> developed two normative models of temporal adaptation effects in V1, based on statistics of dynamic natural visual scenes (natural movies). These models are based on generative models of natural movies in the class of mixtures of Gaussian scale mixture (MGSM) distributions. In MGSMs the outputs of linear oriented filters applied to video frames are assumed to be Gaussian variables, multiplied by positive scale variables. These scale variables can be shared within pools of filter outputs at different times (video frames) and orientations. The two models of <xref ref-type="bibr" rid="c49">Snow et al. (2016)</xref> differ according to which pools of filter outputs are able to share scale variables. In both models, V1 neural responses are assumed to represent the inferred Gaussian latent variables of the corresponding MGSM. While each model was able to account for some of the findings of <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>, neither one could account for all. In particular, each of their two models could only account for either stimulus-specific or neuron-specific adaptation factors found by <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>. In particular, only the model that accounted for neuron-specific adaptation was able to produce firing rate homeostasis. As we showed in <xref ref-type="sec" rid="s2i">Sec. 2.9</xref>, our framework, which combines homeostatic gain modulation with the DDC theory of representation, robustly accounts both for stimulus-specific adaptation and firing rate homeostasis. Furthermore, when there is a discrepancy between the internal model‚Äôs stimulus prior and the true environmental stimulus distribution, it additionally accounts for a neuron-specific adaptation factor as well (see <xref ref-type="disp-formula" rid="eqn24">Eq. (24)</xref>).</p>
<p><xref ref-type="bibr" rid="c56">Tring et al. (2023)</xref> replicates the experiment performed in <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>, this time in awake mice (rather than anesthetised cats). However, in contrast to the results of <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>, <xref ref-type="bibr" rid="c56">Tring et al. (2023)</xref> do not find firing rate homeostasis in mouse V1. Additionally, <xref ref-type="bibr" rid="c56">Tring et al. (2023)</xref> find that adaptation mainly changes the scale of the population response vector, while minimally affecting its direction. Mathematically, this amounts to pure stimulus-specific adaptation without the neuron-specific factor found in by <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref> - see <xref ref-type="disp-formula" rid="eqn24">Eq. (24)</xref>. As we discuss in <xref ref-type="sec" rid="s2g">Sec. 2.7</xref>, a Bayes-ratio code based on an ideal-observer generative model also displays pure stimulus-specific adaptation with no neuron-specific factor. Our final model for Benucci‚Äôs data did contain a neuronal factor, because we used a non-ideal observer DDC (specifically, we assumed a smoother prior distribution over orientations compared to the distribution used in the experiment ‚Äî which has a very sharp peak ‚Äî as it is more natural given the inductive biases we expect in the brain). The resultant neuron-specific factor suppresses the tuning curves tuned to the adaptor stimulus. Interestingly, when gain adaptation is incomplete, and happens to a weaker degree compared to what is necessary for firing rate homeostasis, an additional neuronal factor emerges that is greater than one (<italic>i</italic>.<italic>e</italic>., facilitatory) for neurons tuned to the adaptor stimulus. These two multiplicative neural factors can potentially cancel each other approximately; such a theory would thus predict both deviation from homeostasis and approximately pure stimulus-specific adaptation. We plan to explore this possibility in future work.</p>
<p><xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref> find that in cases where the adaptor probability is particularly extreme (<italic>i</italic>.<italic>e</italic>., 50%), homeostasis is imperfect, and neurons tuned to the adaptor display higher firing rates. These results cannot be accounted for within our homeostatic DDC framework, since we take as a starting point homeostatic regulation of firing rates in our model. However, note that our efficient coding framework does predict deviation from homeostasis when signal-to-noise ratio falls (see <xref ref-type="sec" rid="s2d">Sec. 2.4</xref>). Thus, a more complete model in which we both specify DDC representational curves and numerically optimise gains according to our efficient coding objective may be able to account for the observed deviation from homeostasis.</p>
<p>Our approach has relied on a number of assumptions and simplifications; relaxing or generalising these assumptions provide opportunities for future research. Firstly, our toy model (see <xref ref-type="sec" rid="s2c">Sec. 2.3</xref>) imposed a cluster structure on the neural tuning curves, whereby neurons within a cluster are similarly tuned, while neurons in different clusters have distinct tuning. We found that total firings rates are equalised across clusters (<xref ref-type="sec" rid="s2b">Sec. 2.2</xref>), but the optimal rates of individual neurons within clusters span a broad range (<xref ref-type="sec" rid="s2c">Sec. 2.3</xref>). This results from the fact that the efficient coding objective function depends sharply on total cluster firing rates, but changes only slightly when the partition of the cluster firing rate among the cluster‚Äôs neurons changes. In reality, tuning similarity has a more graded distribution. As the distinction between clusters with different tuning (or similarity of tuning within clusters) weakens, higher order terms in our expansion of the objective <xref ref-type="disp-formula" rid="eqn27">Eq. (27)</xref> cannot be neglected, and hence the justification for dividing the problem into two separate, cluster-level vs. within-cluster, problems breaks down. However, arguably, in a more realistic model with a more graded variation in neural signal correlations, the optimisation landscape will still contain shallow directions (<italic>i</italic>.<italic>e</italic>., directions over which the objective changes slowly), corresponding to partitions of firing among similarly tuned neurons, and non-shallow directions corresponding to total firings of such groups of neurons; if so, we would expect a similar solution to emerge, whereby similarly tuned neurons display heterogeneous firing rates, with the total firing rate of such a group of neurons displaying approximate homeostasis.</p>
<p>Secondly, we showed analytically that, at the level of cluster responses, homeostasis emerges universally in the high signal-to-noise regime. In general, firing rate homeostasis is no longer optimal in the low signal-to-noise ratio limit, as correction terms to our solutions become large. However, as we argued in <xref ref-type="sec" rid="s2e">Sec. 2.5</xref>, based on empirical values we expect the signal-to-noise ratio within cortex to be sufficiently high for our results to hold. Generally, within efficient coding theory (except for simple cases such as linear Gaussian models), the regime of arbitrary signal-to-noise ratio is analytically intractable. Many approaches are therefore limited to the regime of high signal-to-noise ratio. For example, the Fisher Information Lower Bound used frequently within efficient coding theory breaks down outside of this regime (<xref ref-type="bibr" rid="c67">Yarrow et al., 2012</xref>; <xref ref-type="bibr" rid="c6">Bethge et al., 2002</xref>; <xref ref-type="bibr" rid="c65">Wei and Stocker, 2016</xref>). Thus, inevitably, exploring the low signal-to-noise ratio regime would require numerical simulation. However, such numerical simulations require concrete models of signal and noise structure. The manner in which optimal coding deviates from homeostatic coding will not be universal across these models, but depend on each model‚Äôs specific details. This limits our ability to draw general conclusions from numerical simulations, which is why we have chosen not to pursue that strategy extensively here.</p>
<p>Thirdly, our analysis here considered a specific class of noise models. These noise models include those of particular biological relevance, such as information-limiting noise correlations (<xref ref-type="bibr" rid="c34">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="c22">Kanitscheider et al., 2015</xref>), signatures of which have been found in the cortex (<xref ref-type="bibr" rid="c42">Rumyantsev et al., 2020</xref>), and power-law variability (<xref ref-type="bibr" rid="c19">Goris et al., 2014</xref>) (see <xref ref-type="sec" rid="s2f">Sec. 2.6</xref>). As we noted, our homeostatic solution is particularly well suited to the case of information-limiting noise correlations, performing optimally in that case (see App. B.5). The core ideas of our framework and derivation could potentially be applied to an even wider class of noise models.</p>
<p>Fourthly, up to <xref ref-type="sec" rid="s2g">Sec. 2.7</xref>, we make no assumptions about what the neural population is attempting to represent via the representational curves. We then apply our framework to a specific theory of Bayesian encoding (namely DDCs), and develop the new idea of Bayes-ratio coding. There is therefore an opportunity to apply our framework to other theories of neural representation, and in particular to alternative theories of Bayesian representations, such as Probabilistic Population Codes (PPCs) (<xref ref-type="bibr" rid="c4">Beck et al., 2007</xref>).</p>
<p>Finally, it is possible to generalise our theoretical framework (both the general efficient coding framework and its combination with frameworks for Bayesian inference) to the case of temporally varying stimuli and dynamic Bayesian inference. In particular, <xref ref-type="bibr" rid="c33">M≈Çynarski and Tkacik (2022)</xref> have applied optimal gain modulation, according to efficient coding (to reduce metabolic cost), to sparse coding (an example of a probabilistic model with latent variable representation), in order to model gain modulation by attention. Accordingly, their efficient coding objective was task-dependent (<italic>i</italic>.<italic>e</italic>., depended on the choice of latent variables that are desired to be in the focus of attention). However, we are not aware of similar work that has applied such a combination to model the dynamics of task-independent (<italic>i</italic>.<italic>e</italic>., bottom up, rather than top-down) adaptation.</p>
<p>In summary, we showed that homeostatic coding can arise from optimal gain modulation for fighting corruption by noise in neural representations. Based on this coding scheme, we derived a novel implementation of probabilistic inference in neural populations, known as Bayes-ratio coding, which can be achieved by divisive normalisation with adaptive weights to invert generative probabilistic models in an adaptive manner. This coding scheme accounts for adaptation effects that are otherwise hard to explain exclusively based on efficient coding theory. These contributions provide important connections between Bayesian representation, efficient coding theory, and neural adaptation.</p>
</sec>
<sec id="s4">
<label>4</label>
<title>Methods</title>
<sec id="s4a">
<label>4.1</label>
<title>Specification of environments for simulations</title>
<p>For our numerical simulations, we used <italic>K</italic> = 10, 000 units. To generate the <italic>K √ó K</italic> signal correlation matrix, <italic>œÅ</italic>(<italic>ŒΩ</italic>), for the different environments in our numerical simulations, in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref>, we first generate a covariance matrix Œ£(<italic>ŒΩ</italic>), and let <italic>œÅ</italic>(<italic>ŒΩ</italic>) be the corresponding correlation matrix. The covariance matrix Œ£(<italic>ŒΩ</italic>) is in turn constructed such that it has (1) the same eigenspectrum for all <italic>ŒΩ</italic>, with eigenvalues that decrease inversely with rank, as observed in V1 (<xref ref-type="bibr" rid="c52">Stringer et al., 2019</xref>), and (2) an eigenbasis that varies smoothly with <italic>ŒΩ</italic>. Thus we defined Œ£(<italic>ŒΩ</italic>) = <italic>U</italic> (<italic>ŒΩ</italic>)Œõ<sub>1</sub><italic>U</italic> (<italic>ŒΩ</italic>)<sup>T</sup>, where Œõ<sub>1</sub> = diag(1, 1<italic>/</italic>2, 1<italic>/</italic>3, ‚Ä¶, 1<italic>/K</italic>) and <italic>U</italic> (<italic>ŒΩ</italic>) is an orthogonal matrix that was generated as follows. We randomly and independently sample two <italic>K√óK</italic> random iid Gaussian matrices <italic>R</italic><sub>0</sub>, <italic>R</italic><sub>1</sub> <italic>~</italic> ùí© <sub><italic>K√óK</italic></sub>(0, <italic>I</italic><sub><italic>K√óK</italic></sub>) and obtain symmetric matrices <inline-formula><inline-graphic xlink:href="564616v4_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="564616v4_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. As is well known, the eigen-basis (represented by an orthogonal matrix) of such a random Gaussian matrix is distributed uniformly (<italic>i</italic>.<italic>e</italic>., according to the corresponding Haar measure) over the orthogonal group <italic>O</italic>(<italic>K</italic>). For the extreme environments, <italic>ŒΩ</italic> = 0 and 1, we let <italic>U</italic> (<italic>ŒΩ</italic>) be the matrix of eigenvectors of <italic>S</italic><sub>0</sub> and <italic>S</italic><sub>1</sub>, when ordered according to eigenvalue rank. For 0 <italic>&lt; ŒΩ &lt;</italic> 1, we let <italic>U</italic> (<italic>ŒΩ</italic>) be the matrix of the eigenvectors (again ordered according to eigenvalue rank) of the interpolated symmetric matrix <italic>S</italic>(<italic>ŒΩ</italic>) = (1 ‚àí <italic>ŒΩ</italic>)<italic>S</italic><sub>0</sub> + <italic>ŒΩS</italic><sub>1</sub>. Thus, for the extreme environments the eigenbases of the covariance matrices Œ£(0) and Œ£(1) are sampled independently and uniformly (<italic>i</italic>.<italic>e</italic>., from the Haar measure), and the eigenbases of Œ£(<italic>ŒΩ</italic>) for intermediate environments (<italic>i</italic>.<italic>e</italic>., for 0 <italic>&lt; ŒΩ &lt;</italic> 1) smoothly interpolate between these.</p>
<p>In the case of simulations for aligned noise, we also generate a noise correlation matrix <italic>W</italic> (<italic>ŒΩ</italic>) which has an approximately 1<italic>/n</italic><sup><italic>Œ≥</italic></sup> spectrum. Ideally, <italic>W</italic> and <italic>œÅ</italic> would have the same eigen-basis. However, this is impossible, since <italic>W</italic> and <italic>œÅ</italic> are both correlation matrices. Instead, we generate <italic>W</italic> (<italic>ŒΩ</italic>) by normalising the positive definite matrix <italic>U</italic> (<italic>ŒΩ</italic>)Œõ<sub><italic>Œ≥</italic></sub><italic>U</italic> (<italic>ŒΩ</italic>)<sup>T</sup> where Œõ<sub><italic>Œ≥</italic></sub> = diag(1, 1<italic>/</italic>2<sup><italic>Œ≥</italic></sup>, 1<italic>/</italic>3<sup><italic>Œ≥</italic></sup>, ‚Ä¶, 1<italic>/K</italic><sup><italic>Œ≥</italic></sup>)</p>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Analysis of the clustered population</title>
<p>Here, we provide a summary of the analysis of the clustered model introduced in <xref ref-type="sec" rid="s2c">Sec. 2.3</xref> (for derivations and a more detailed account see App. B.3). In this model, a population of <italic>N</italic> neurons are sorted into <italic>K</italic> clusters. Neurons within a cluster have very similar stimulus tuning, but neurons in different clusters are tuned differently. More concretely, we will assume that the representational curves of the neurons, indexed by <italic>i</italic>, belonging to cluster <italic>a</italic>, are small perturbations to the same representational curve characterising that cluster; <italic>i</italic>.<italic>e</italic>.,
<disp-formula id="eqn25">
<graphic xlink:href="564616v4_eqn25.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>œµ</italic> is the perturbation strength parameter controlling the deviation from perfect within-cluster similarity. Below, hatted symbols <inline-formula><inline-graphic xlink:href="564616v4_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula> denote variables defined analogously to <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>‚Äì<xref ref-type="disp-formula" rid="eqn8">(8)</xref> but for single neurons, with Œ©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>) replaced by <inline-formula><inline-graphic xlink:href="564616v4_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p>
<p>We assume single-neuron gains adapt to optimise an objective ‚Ñí <sub>pop</sub>, defined analogously to the objective ‚Ñí, <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref>, but for individual neurons rather than clusters. In the case of uncorrelated noise with Poisson-like scaling, this objective is given by
<disp-formula id="eqn26">
<graphic xlink:href="564616v4_eqn26.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="564616v4_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are the mean firing rates of individual neurons, given by <inline-formula><inline-graphic xlink:href="564616v4_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula>; note that maximising ‚Ñí<sub>pop</sub> in terms of the single-neuron gains is equivalent to maximising it in terms of <inline-formula><inline-graphic xlink:href="564616v4_inline15.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. In App. B.3, we show that this objective can be expanded in <italic>œµ</italic> as
<disp-formula id="eqn27">
<graphic xlink:href="564616v4_eqn27.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here, the leading order term, ‚Ñí (<bold><italic>r</italic></bold> = <italic>œâ</italic><bold><italic>g</italic></bold>), is a cluster-level objective and is given byL<xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref>, now viewed as a function of the cluster rates <italic>r</italic><sub><italic>a</italic></sub>, <italic>i</italic>.<italic>e</italic>., the total mean firing rate of neurons in a cluster; thus <inline-formula><inline-graphic xlink:href="564616v4_inline16.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where <italic>C</italic><sub><italic>a</italic></sub> is the set of neurons in cluster <italic>a</italic>. Note that this term is a function only of the cluster firing rates. Thus, for <italic>œµ</italic> = 0 (perfect similarity within clusters), the efficient coding objective function is blind to the precise distribution of single-cell firing rates, <inline-formula><inline-graphic xlink:href="564616v4_inline17.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, among the neurons of a cluster, as long as the total rate of the cluster is held fixed. This is because for clusters of identically-tuned and independently-firing neurons with Poisson-like noise, the cluster can be considered as a single coherent unit whose firing rate is given by the sum of the firing rates of its neurons. Thus at zeroth order, the distribution of individual neuron rates are free and undetermined by the optimisation, as long as they give rise to optimal cluster rates.</p>
<p>According to <xref ref-type="disp-formula" rid="eqn27">Eq. (27)</xref>, at small but non-zero <inline-formula><inline-graphic xlink:href="564616v4_inline18.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is still dominated by ‚Ñí; we thus expect this term to play the dominant role in determining the cluster rates, with terms of order <italic>œµ</italic><sup>2</sup> and higher having negligible effect. However, the perturbation term <inline-formula><inline-graphic xlink:href="564616v4_inline19.gif" mimetype="image" mime-subtype="gif"/></inline-formula> breaks the invariance of the loss with respect to changes in the distribution of single neuron gains (or rates) within clusters. We will therefore approximate the maximisation of the total objective function ‚Ñí<sub>pop</sub> as follows. We first specify the cluster rates by maximising ‚Ñí (<bold><italic>r</italic></bold>), obtaining the optimal cluster rates <inline-formula><inline-graphic xlink:href="564616v4_inline20.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We then specify the rates of individual neurons within the population by maximising the leading correction term in the objective, <inline-formula><inline-graphic xlink:href="564616v4_inline21.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, subject to the constraint that the single-neuron rates in each cluster sum to that cluster‚Äôs optimal rate. We reinterpret the optimisation in the previous sections to correspond to the first stage of this two-stage optimisation, with population units there corresponding to clusters of similarly tuned neurons. As we have seen in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref>, optimisation of the cluster-level objective, ‚Ñí, can result in homeostasis and uniformisation across clusters.</p>
<p>As for the distribution of individual neuron rates, we show in App. B.3.5 (see the derivation of <xref ref-type="disp-formula" rid="eqn87">Eq. (87)</xref>) that, in the parameter regime (see Secs. 2.4 and 2.5) in which cluster-level homeostasis and uniformisation occurs, the perturbation objective ‚Ñí <sub>pert</sub> decomposes into a sum of objectives over different clusters. Thus, in the second stage of optimisation the problem decouples across clusters, and (as shown in App. B.3.5) the optimal single-neuron rates in cluster <italic>a</italic> maximise the objective
<disp-formula id="eqn28">
<graphic xlink:href="564616v4_eqn28.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
subject to the constraint
<disp-formula id="eqn29">
<graphic xlink:href="564616v4_eqn29.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here ŒîŒ©<sub><italic>i</italic></sub> is defined to be the centered, zero-mean version of <italic>Œ¥</italic>Œ©<sub><italic>i</italic></sub>:<sup/>
<disp-formula id="eqn30">
<graphic xlink:href="564616v4_eqn30.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
(We can interpret ŒîŒ©<sub><italic>i</italic></sub> as the direction of the perturbed tuning curve in excess of the cluster tuning curve Œ©<sub><italic>a</italic></sub>.) Note that the optimisation problem, <xref ref-type="disp-formula" rid="eqn28">Eqs. (28)</xref>‚Äì<xref ref-type="disp-formula" rid="eqn29">(29)</xref>, is a quadratic program. Moreover, the terms in the objective for this quadratic program are homogeneous in the sense that if the information-energy trade-off parameter <italic>¬µ</italic> is scaled a factor, the optimal solutions for <inline-formula><inline-graphic xlink:href="564616v4_inline22.gif" mimetype="image" mime-subtype="gif"/></inline-formula> scale with the same factor. In our cluster-level simulations <xref ref-type="sec" rid="s2b">Sec. 2.2</xref> we used <italic>¬µ</italic> = 10 (in agreement with our biological estimates in <xref ref-type="sec" rid="s4d">Sec. 4.4</xref> for mean spike count of clusters of 20 V1 neurons in a coding interval of 0.1 seconds). That dimensionless <italic>¬µ</italic> (which in the homeostatic approximation determines the mean spike count of clusters over the coding interval) thus corresponds to a total cluster rate of 100 Hz, and using this value for <italic>¬µ</italic> in <xref ref-type="disp-formula" rid="eqn28">Eqs. (28)</xref>‚Äì<xref ref-type="disp-formula" rid="eqn29">(29)</xref> the solutions for <inline-formula><inline-graphic xlink:href="564616v4_inline23.gif" mimetype="image" mime-subtype="gif"/></inline-formula> can be interpreted as being firing rates in units of Hz.</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Within-cluster optimisation</title>
<p>To calculate the single-neuron firing rate distributions in <xref rid="fig8" ref-type="fig">Fig. 8</xref>, we solved the optimisation problem <xref ref-type="disp-formula" rid="eqn28">Eqs. (28)</xref>‚Äì<xref ref-type="disp-formula" rid="eqn29">(29)</xref> numerically for 10,000 randomly generated covariance matrices (note that different instances of the optimisation can be thought of as characterising different clusters within the same population). In general, the covariance matrix, <inline-formula><inline-graphic xlink:href="564616v4_inline24.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (which defines this optimisation problem), depends on the stimulus distribution, the cluster tuning curve, and properties (<italic>e</italic>.<italic>g</italic>., degree of smoothness) of the intra-cluster variations in single-cell tuning curves. In our simulations, instead of making specific arbitrary choices for these different factors, we used a minimal toy model where the covariance matrix was generated as the matrix of inner products of <italic>k</italic> random vectors in a <italic>D</italic>-dimensional space; specifically, we generated this matrix as the the Gram matrix of <italic>k</italic> random <italic>D</italic>-dimensional vectors with isotropic Gaussians distributions, <italic>i</italic>.<italic>e</italic>., <inline-formula><inline-graphic xlink:href="564616v4_inline25.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>Œ¥</italic><bold><italic>w</italic></bold><sub><italic>i</italic></sub> <italic>~</italic> ùí© (0, <italic>I</italic><sub><italic>D</italic></sub>). In App. B.3.6 we show that <italic>D</italic> can be interpreted as the dimensionality of the space of independent perturbations to the cluster-wide tuning curve Œ©<sub><italic>a</italic></sub> which vary significantly on the portion of stimulus space to which the cluster tuning curve responds (below, we will refer to <italic>D</italic> as the effective tuning curve dimension). Additionally, we fixed the cluster rate to <italic>¬µ</italic> = <italic>k √ó</italic> 5Hz, such that the average firing rate per single neuron is 5Hz, and fixed <italic>œÉ</italic><sup>2</sup> = 1 (corresponding to a Fano factor of 1). For various values of <italic>k</italic> and <italic>D</italic>, we used the above method to generate 10, 000 samples of the covariance matrix Cov(ŒîŒ©). For each such sample, we then solved the problem (28‚Äì29) numerically using the <monospace>cvxpy</monospace> package in Python (<xref ref-type="bibr" rid="c12">Diamond and Boyd, 2016</xref>). Neurons with firing rates less than 10<sup>‚àí4</sup>Hz had their firing rate set to zero. We aggregated the set of non-zero rates obtained from each of these optimisations to create the histograms in <xref rid="fig8" ref-type="fig">Fig. 8</xref>.</p>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>Estimation of biological parameters justifying the homeostatic approximation</title>
<p>In this section we provide the details for our estimates of cortical mean firing rates and noise scaling used in <xref ref-type="sec" rid="s2e">Sec. 2.5</xref>.</p>
<sec id="s4d1">
<title>Firing rate estimates</title>
<p>A wide range of mean firing rates for individual neurons have been reported in cortex. Here we focus on firing rates in rodent V1 during free behaviour (which tends to be lower compared to rates in cat or monkey cortex). Reported values tend to range from 0.4 Hz (<xref ref-type="bibr" rid="c20">Greenberg et al., 2008</xref>) to 14 Hz (<xref ref-type="bibr" rid="c41">Parker et al., 2022</xref>), depending on the cortical layer or area or the brain state, with other values lying in a tighter range of 4-7 Hz (<xref ref-type="bibr" rid="c21">Hengen et al., 2013</xref>; <xref ref-type="bibr" rid="c53">Szuts et al., 2011</xref>; <xref ref-type="bibr" rid="c55">Torrado Pacheco et al., 2019</xref>). Therefore, for rodent V1, we take a mean firing rate of 5 Hz to be a reasonable rough estimate.</p>
</sec>
<sec id="s4d2">
<title>Scaling of noise variance with mean response</title>
<p>A number of studies (<xref ref-type="bibr" rid="c46">Shadlen and Newsome, 1998</xref>; <xref ref-type="bibr" rid="c16">Gershon et al., 1998</xref>; <xref ref-type="bibr" rid="c35">Moshitch and Nelken, 2014</xref>; <xref ref-type="bibr" rid="c26">Koyama, 2015</xref>) have characterised the relationship between the trial-to-trial mean and variance of neural spike counts as a power law of the form
<disp-formula id="ueqn1">
<graphic xlink:href="564616v4_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and have provided best fit estimates of <inline-formula><inline-graphic xlink:href="564616v4_inline26.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <italic>Œ≤</italic> in different species and cortical areas. We have summarised these in <xref rid="tbl1" ref-type="table">Table 1</xref>.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>Table summarising empirical findings from four papers which estimate a scaled power-law relationship relationship between mean and variance of cortical spike count responses.</title>
<p>To find the values in the last column we adjust the reported <inline-formula><inline-graphic xlink:href="564616v4_inline112.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to account for different coding intervals and cluster sizes used in each experiment (see <xref ref-type="disp-formula" rid="eqn31">Eq. (31)</xref>), and assume <italic>Œ≤¬µ</italic> = 10.</p></caption>
<graphic xlink:href="564616v4_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>While the power-law exponent <italic>Œ≤</italic> is directly identifiable with the same parameter in our noise model, we need to adjust the empirical estimates of <inline-formula><inline-graphic xlink:href="564616v4_inline27.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in <xref rid="tbl1" ref-type="table">Table 1</xref> in order to obtain estimates for our model parameter <italic>œÉ</italic>, for two reasons. First, the duration of the time interval in which spikes were counted in the above papers differs from our assumed coding interval of 0.1<italic>s</italic>. We therefore must adjust the reported values to be for an interval of this size. We do this by assuming temporal homogeneity over the timescales of interest; specifically we assume that mean and variance are linear in the time interval. Second, we must also adjust the data to account for the fact that our responses are for clusters of <italic>k</italic> = 20 similarly tuned neurons. We do this by assuming uncorrelated response noise within each cluster, making mean and variance both linear functions of cluster size. Our adjustments do not affect the value of <italic>Œ≤</italic>, but do change the value of the pre-factor <inline-formula><inline-graphic xlink:href="564616v4_inline28.gif" mimetype="image" mime-subtype="gif"/></inline-formula> Specifically, we perform the transformation:
<disp-formula id="eqn31">
<graphic xlink:href="564616v4_eqn31.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>Œ¥t</italic> is the reported time interval, and Œî<italic>t</italic> is our coding interval, Œî<italic>t</italic> = 0.1s. <xref rid="fig13" ref-type="fig">Fig. 13</xref> shows the adjusted values on a scatter plot. Using the adjusted values, <italic>œÉ</italic><sup>2</sup>, and assuming that <italic>¬µŒ≤</italic> = 10 (to maintain a firing rate of 5Hz per neuron), we can find the value of <italic>œÉ</italic><sup>2</sup><italic>/</italic>(<italic>Œ≤¬µ</italic>)<sup><italic>Œ≤</italic></sup>. These are reported in the furthest right column of <xref rid="tbl1" ref-type="table">Table 1</xref>.</p>
<fig id="fig13" position="float" fig-type="figure">
<label>Figure 13:</label>
<caption><title>Empirically estimated values of the noise scaling parameter <italic>Œ≤</italic> and the base noise-level <italic>œÉ</italic><sup>2</sup>, after adjusting for coding interval duration and cluster size using the formula <xref ref-type="disp-formula" rid="eqn31">Eq. (31)</xref>.</title>
<p>The dashed lines show the values for Poissonian noise (<italic>œÉ</italic><sup>2</sup> = 1, <italic>Œ≤</italic> = 1).</p></caption>
<graphic xlink:href="564616v4_fig13.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s4d3">
<title>Sufficiently selective responses</title>
<p>The coefficient of variation CV is a measure of neural selectivity, and therefore is related also to measures of sparseness of neural responses, or so-called coding level. To see this, consider a toy model in which the cluster responds at a fixed level to a fraction <italic>p</italic> of stimuli and is silent otherwise. In this case, CV<sup>2</sup> = (1‚àí<italic>p</italic>)<italic>/p</italic> ‚âà1<italic>/p</italic> for small <italic>p</italic>. Our condition therefore requires that neurons are sufficiently selective in their responses and respond only to a small fraction of stimuli. <xref ref-type="bibr" rid="c28">Lennie (2003)</xref> places the fraction of simultaneously active neurons (which we use as a proxy for the response probability of a single cluster) at under 5%. For our toy model, this yields the estimate CV<sup>2</sup> <italic>‚âà</italic> 20. The binary distribution in the toy model is particularly sparse, and so we take CV<sup>2</sup> <italic>‚âà</italic> 10 as a more conservative estimate.</p>
</sec>
<sec id="s4d4">
<title>High-dimensional signal geometry</title>
<p>Assuming, for simplicity, that the coefficients of variation are approximately the same for all clusters, we see that the expression for Œî<sub><italic>aa</italic></sub> are proportional to the diagonal elements of <italic>œÅ</italic><sup>‚àí1</sup><italic>W</italic>. To estimate the latter, we will assume that these elements are all comparable in size (This is expected to be valid when the eigenbasis of <italic>œÅ</italic><sup>‚àí1</sup><italic>W</italic> is not aligned with the standard (cluster) basis, which, in turn, corresponds to distributed coding.), and will therefore estimate their average value, which is given by the normalised trace <inline-formula><inline-graphic xlink:href="564616v4_inline29.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. By the invariance of trace, this can equally be characterised as the mean eigenvalue of <italic>œÅ</italic><sup>‚àí1</sup><italic>W</italic>. We estimated this trace for the aligned noise family (corresponding to information-limiting noise correlations, but also including uncorrelated noise) which we numerically studied in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref> and <xref ref-type="sec" rid="s2f">Sec. 2.6</xref>, and a biologically-motivated form of <italic>œÅ</italic> used there. In the aligned noise model, <italic>W</italic> and <italic>œÅ</italic> share the same eigenbasis, but have different eigenvalue spectra. Our choice of <italic>œÅ</italic> has spectrum that scales like 1<italic>/n</italic> with the rank of the eigenvalue (based on the findings of <xref ref-type="bibr" rid="c52">Stringer et al. (2019)</xref> that the signal covariance matrix of mouse V1 neural populations responding to natural stimuli possesses an approximately 1<italic>/n</italic> spectrum), while the spectrum of <italic>W</italic> scales like 1<italic>/n</italic><sup><italic>Œ≥</italic></sup> for a positive exponent parameter <italic>Œ≥</italic>.</p>
<p>We show in App. B.6 that, for 0 <italic>‚â• Œ≥ ‚â•</italic> 1 (relatively high-dimensional and weakly correlated noise), we have <inline-formula><inline-graphic xlink:href="564616v4_inline30.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The upper bound is saturated in the case of uncorrelated noise (which correspond to <italic>Œ≥</italic> = 0 yielding <italic>W</italic> = <italic>I</italic>). In the other extreme where <italic>Œ≥</italic> = 1, we have <italic>W</italic> = <italic>œÅ</italic> or <italic>œÅ</italic><sup>‚àí1</sup><italic>W</italic>, which yields a normalised trace of 1; which does not scale with <italic>K</italic>. On the other hand, for <italic>Œ≥ &gt;</italic> 1 (relatively low-dimensional and strongly correlated noise), tr(<italic>œÅ</italic><sup>‚àí1</sup><italic>W</italic>) decays like <italic>K</italic><sup>‚àí min(<italic>Œ≥</italic>‚àí1,1)</sup> as <italic>K</italic> grows. Thus, in this case, the estimate of Œî<sub><italic>aa</italic></sub> is smaller than that of <xref ref-type="disp-formula" rid="eqn13">Eq. (13)</xref>. However, this case is arguably not biologically relevant, due to the fact that neural responses always contain some amount of ‚Äúprivate‚Äù uncorrelated noise. Mathematically this would contribute an additive correction to <italic>W</italic> proportional to the identity matric. In the case <italic>Œ≥ &gt;</italic> 1 (where noise eigenvalues decay faster than signal eigenvalues) this term, even when the size of the correction is small, prevents the inverse power-law scaling with <italic>K</italic>. Therefore we did not use this case in the Results section in our estimates of Œî<sub><italic>aa</italic></sub>.</p>
</sec>
</sec>
<sec id="s4e">
<label>4.5</label>
<title>Fitting a homeostatic DDC to <xref ref-type="bibr" rid="c5">Benucci <italic>et al</italic>. 2013</xref></title>
<p>The data we obtained from <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref> comprised 11 datasets corresponding to different experimental sessions. In each dataset, the recorded neural population was clustered into 12 groups based on preferred orientation. The datasets contained the trial-averaged responses of these clusters to gratings of different orientation (which took one of 6 or 12 possible values depending on the dataset). In each experimental context, the grating orientations were randomly drawn from a distribution that was either uniform or biased; in the latter case, one particular grating (arbitrarily defined as 0¬∞) had higher prevalence, by either 30%, 35%, 40%, or 50%. We discarded all datasets with a 50% prevalence, since (Benucciet al., 2013) report that homeostasis was less perfect in this case, and they similarly discarded it from their main analysis. After exposure to the distribution of gratings, the responses (<italic>i</italic>.<italic>e</italic>., spike count) of each cluster to a test set of 20 oriented gratings were then measured. In each dataset, the trial-averaged tuning curves had been normalised by an affine transform (see <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref> for details) such that tuning curves in the uniform stimulus ensemble context ranged from 0 to 1. We further interpolated and up-sampled all normalised tuning curves using a cubic spline (<xref rid="fig12" ref-type="fig">Fig. 12A</xref>).</p>
<p>To fit our model parameters (<italic>œÉ</italic><sub><italic>f</italic></sub>, <italic>œÉ</italic><sub><italic>œÄ</italic></sub>, and <italic>œÉ</italic><sub><italic>œï</italic></sub>), we started by centering the cluster tuning curves in the uniform ensemble context at 0¬∞ and averaged across clusters with different preferred orientations, yielding a single average tuning curve (<xref rid="fig11" ref-type="fig">Fig. 11B</xref>, blue). We fit the resultant curve with a Gaussian (<xref rid="fig11" ref-type="fig">Fig. 11B</xref>, red) with a standard deviation matched to 1/4 the width of this curve at height <italic>e</italic><sup>‚àí2</sup>. Subsequently, <inline-formula><inline-graphic xlink:href="564616v4_inline31.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (which is the model‚Äôs prediction for the standard deviation of the Gaussian tuning curves in the uniform ensemble context) was constrained to equal this standard deviation. This reduces the number of free model parameters to 2, which we choose to be <italic>œÉ</italic><sub><italic>œÄ</italic></sub> and <italic>œÉ</italic><sub><italic>f</italic></sub>. We found the best value of these parameters by fitting the curves for repulsion of tuning curves (adaptation-induced change in preferred orientation vs. pre-adaptation preferred orientation ‚Äì <xref rid="fig12" ref-type="fig">Fig. 12C</xref> and <xref ref-type="fig" rid="fig12">F</xref>) between the model and experimental data. Specifically, we performed a grid search over values of <italic>œÉ</italic><sub><italic>œÄ</italic></sub> and <italic>œÉ</italic><sub><italic>f</italic></sub> and chose the pair of values which minimised the sum of the absolute differences between the empirical and model curves.</p>
<p>We obtained the empirical repulsion curves (<xref rid="fig12" ref-type="fig">Fig. 12C</xref>) as follows. For each preferred orientation cluster and context (<italic>i</italic>.<italic>e</italic>., exposure to a uniform or biased ensemble), a smoothing kernel was applied before using cubic interpolation to generate up-sampled tuning curves. The smoothing kernel was applied to ensure that none of the tuning curves were multimodal (multimodality was a minor effect ‚Äî reasonably attributable to finite-trial averaging noise ‚Äî but could have nevertheless introduced relatively large noise in the estimated preferred orientations). The argmax of these tuning curves was found to give the preferred orientation. The preferred orientation was then compared across conditions (uniform ensemble vs. biased ensemble) to give a change in preferred orientation. These were then averaged across contexts with the same adaptor probability to obtain the repulsion curve for each adaptor probability.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank M√°t√© Lengyel for helpful discussions and comments on the manuscript. EY was supported by the UKRI Engineering and Physical Sciences Research Council Doctoral Training Program grant EP/T517847/1. YA was supported by UKRI Biotechnology and Biological Sciences Research Council research grant BB/X013235/1.</p>
</ack>
<app-group>
<app id="s5">
<label>A</label>
<title>Extended data figures</title>
<fig id="fig14" position="float" fig-type="figure">
<label>Figure 14:</label>
<caption><title>Distribution of average firing rates before and after a discrete environmental shift for the uncorrelated power-law noise subfamily.</title>
<p>Each panel has the same format as in <xref ref-type="fig" rid="fig5">Fig. 5B</xref>, but for different values of the noise scaling parameter <italic>Œ±</italic>. The case <italic>Œ±</italic> = 0.5 is shown in <xref ref-type="fig" rid="fig5">Fig. 5B</xref>.</p></caption>
<graphic xlink:href="564616v4_fig14.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig15" position="float" fig-type="figure">
<label>Figure 15:</label>
<caption><title>Distribution of average firing rates before and after a discrete environmental shift for the Poissonian noise subfamily with uniform noise correlations.</title>
<p>Each panel has the same format as in <xref ref-type="fig" rid="fig6">Fig. 6B</xref>, but for different values of the noise correlation coefficient <italic>p</italic>. The case <italic>p</italic> = 0.2 is shown in <xref ref-type="fig" rid="fig6">Fig. 6B</xref>.</p></caption>
<graphic xlink:href="564616v4_fig15.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig16" position="float" fig-type="figure">
<label>Figure 16:</label>
<caption><title>Distribution of average firing rates before and after a discrete environmental shift for the aligned Poissonian noise subfamily.</title>
<p>Each panel has the same format as in <xref ref-type="fig" rid="fig7">Fig. 7B</xref>, but for different values of the noise spectrum decay parameter <italic>Œ≥</italic>. The case <italic>Œ≥</italic> = 1.5 is shown in <xref ref-type="fig" rid="fig7">Fig. 7B</xref>.</p></caption>
<graphic xlink:href="564616v4_fig16.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig17" position="float" fig-type="figure">
<label>Figure 17:</label>
<caption><title>Accuracy of homeostatic approximation to optimal gains.</title>
<p>The plots in the panels A, C, and E show the relative improvement in the efficient coding objective for the zeroth-order approximation for the optimal gains <bold><italic>g</italic></bold><sup>(0)</sup>, <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref>, for the three noise sub-families; see the caption of <xref ref-type="fig" rid="fig9">Fig. 9</xref> for a description of panels B, D, and F there, respectively. Plots in panels B, D, and F here similarly show the relative improvement in the efficient coding objective for the first-order approximation <bold><italic>g</italic></bold><sup>(1)</sup>, <xref ref-type="disp-formula" rid="eqn106">Eq. (106)</xref>, for the three noise sub-families.</p></caption>
<graphic xlink:href="564616v4_fig17.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig18" position="float" fig-type="figure">
<label>Figure 18:</label>
<caption><title>Systematic exploration of how the firing rate distribution of individual neurons varies as the statistical properties of cluster responses change.</title>
<p>The panels here have the same description as those in <xref ref-type="fig" rid="fig8">Fig. 8</xref>, but show the single-neuron firing rate distributions for different choices of cluster size <italic>k</italic> and effective tuning curve dimension, <italic>D</italic>. (Note that the upper-right panels corresponding to cases in which <italic>D &lt; k</italic> are left empty; this is because in such cases the covariance matrix is singular, and therefore the quadratic program does not have a unique solution. Such cases are also less relevant biologically given realistic estimates for the size of the similarly tuned neurons vs. the effective dimensionality of tuning curves, noting that the nominal dimension of the tuning curve function space is infinite.)</p></caption>
<graphic xlink:href="564616v4_fig18.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</app>
<app id="s6">
<label>B</label>
<title>Supplementary Information: mathematical derivations</title>
<sec id="s6a">
<label>B.1</label>
<title>Analytic expression for ‚Ñí</title>
<p>In this appendix we derive an analytic expression for the upper bound objective ‚Ñí. We adopt the noise model <xref ref-type="disp-formula" rid="eqn6">Eq. (6)</xref>, repeated here for convenience:
<disp-formula id="eqn32">
<graphic xlink:href="564616v4_eqn32.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where Œ£(<bold><italic>s</italic></bold>) is the stimulus-dependent noise correlation matrix, and 0 <italic>&lt; Œ± &lt;</italic> 2 is a scaling parameter. Here we have made use of our convention that the non-bold version of a vector (without indices) denotes the diagonal matrix formed from that vector, so that, for example, <italic>h</italic>(<bold><italic>s</italic></bold>) is the diagonal matrix with <italic>aa</italic>-th entry equal to <italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>).</p>
<p>We decompose the mutual information between the spike counts and the stimulus as
<disp-formula id="ueqn2">
<graphic xlink:href="564616v4_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and then obtain ‚Ñí by replacing the term <italic>H</italic>[<bold><italic>n</italic></bold>] in <inline-formula><inline-graphic xlink:href="564616v4_inline32.gif" mimetype="image" mime-subtype="gif"/></inline-formula> with the entropy of a Gaussian with equal covariance to <bold><italic>n</italic></bold>. Doing so gives us the expression
<disp-formula id="eqn33">
<graphic xlink:href="564616v4_eqn33.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We consider ‚Ñí as a function of the rates <italic>r</italic><sub><italic>a</italic></sub> = <italic>œâ</italic><sub><italic>a</italic></sub><italic>g</italic><sub><italic>a</italic></sub> in order to keep the derivation clean.</p>
<p>We start by deriving an expression for the noise entropy <italic>H</italic>[<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>]. We use <italic>‚âê</italic> to denote equality up to additive pure constants or terms independent of <italic>r</italic> and <italic>g</italic>. We have
<disp-formula id="ueqn3">
<graphic xlink:href="564616v4_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
(where in deriving the third line we used <italic>h</italic> = <italic>g</italic>Œ© = <italic>rœâ</italic><sup>‚àí1</sup>Œ©), hence
<disp-formula id="eqn34">
<graphic xlink:href="564616v4_eqn34.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Next, we find an expression for Cov(<bold><italic>n</italic></bold>), using the orthogonal decomposition Cov(<bold><italic>n</italic></bold>) = ùîº[Cov(<bold><italic>n</italic></bold> |<bold><italic>s</italic></bold>)] + Cov(ùîº[<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>]). For the first term have
<disp-formula id="ueqn4">
<graphic xlink:href="564616v4_ueqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we used <italic>r</italic> = ùîº[<italic>h</italic>(<bold><italic>s</italic></bold>)]. Thus
<disp-formula id="eqn35">
<graphic xlink:href="564616v4_eqn35.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we have defined (c.f. <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref> of main text) the normalised stimulus-averaged noise covariance matrix <italic>W</italic> as
<disp-formula id="ueqn5">
<graphic xlink:href="564616v4_ueqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Next, we find an expression for Cov(ùîº[<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>]). Using <bold><italic>h</italic></bold>(<bold><italic>s</italic></bold>) = ùîº[<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>], we find
<disp-formula id="ueqn6">
<graphic xlink:href="564616v4_ueqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
or
<disp-formula id="eqn36">
<graphic xlink:href="564616v4_eqn36.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we defined
<disp-formula id="eqn37">
<graphic xlink:href="564616v4_eqn37.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and introduced the coefficients of variation (CV) of trial-averaged responses, given by <italic>h</italic>(<bold><italic>s</italic></bold>) (or equivalently, the CV‚Äôs of the representational curves)
<disp-formula id="eqn38">
<graphic xlink:href="564616v4_eqn38.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and the Pearson‚Äôs correlation matrix of <italic>h</italic> (or equivalently of the Œ©‚Äôs), defined by
<disp-formula id="eqn39">
<graphic xlink:href="564616v4_eqn39.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Thus <italic>œÅ</italic> is the matrix of signal correlations.</p>
<p>Putting <xref ref-type="disp-formula" rid="eqn35">Eq. (35)</xref> and <xref ref-type="disp-formula" rid="eqn36">Eq. (36)</xref> together, we obtain
<disp-formula id="ueqn7">
<graphic xlink:href="564616v4_ueqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Finally, plugging this result and <xref ref-type="disp-formula" rid="eqn34">Eq. (34)</xref> into <xref ref-type="disp-formula" rid="eqn33">Eq. (33)</xref>, up to an additive constant, we obtain
<disp-formula id="eqn40">
<graphic xlink:href="564616v4_eqn40.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
as required. Substituting back in <italic>r</italic><sub><italic>a</italic></sub> = <italic>œâ</italic><sub><italic>a</italic></sub><italic>g</italic><sub><italic>a</italic></sub> gets us <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref>.</p>
</sec>
<sec id="s6b">
<label>B.2</label>
<title>Upper bound for Poissonian noise</title>
<p>In this appendix, we consider the following model for cluster spike counts.
<disp-formula id="eqn41">
<graphic xlink:href="564616v4_eqn41.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here we derive a Gaussian upper bound to the mutual information, and show that an approximation to it leads to the same expression <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref> for ‚Ñí derived for the Gaussian noise case, for <italic>œÉ</italic><sup>2</sup><italic>W</italic> = <italic>I</italic> and <italic>Œ±</italic> = 1<italic>/</italic>2 (<italic>i</italic>.<italic>e</italic>., uncorrelated unit-Fano-factor noise). We start with the objective
<disp-formula id="eqn42">
<graphic xlink:href="564616v4_eqn42.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We decompose <italic>I</italic>(<bold><italic>n</italic></bold>; <bold><italic>s</italic></bold>) = <italic>H</italic>[<bold><italic>n</italic></bold>] ‚àí <italic>H</italic>[<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>]. We will once again upper bound the marginal entropy <italic>H</italic>[<bold><italic>n</italic></bold>]. A difficulty arises from the fact that a Poisson random variable is discrete and the Gaussian upper bound we previously used is for a continuous random variables. We address this problem as follows: Consider a random variable <bold><italic>U</italic></bold> which is uniformly distributed on [0, 1)<sup><italic>K</italic></sup>, independent of <bold><italic>n</italic></bold>. Then <bold><italic>n</italic></bold> + <bold><italic>U</italic></bold> is a continuous random variable. We apply the Gaussian bound to this. Let <italic>p</italic> be the p.d.f. of <bold><italic>n</italic></bold> + <bold><italic>U</italic></bold>, <italic>P</italic> the p.m.f. of <bold><italic>n</italic></bold>, and <italic>u</italic> the p.d.f. of <bold><italic>U</italic></bold> .
<disp-formula id="ueqn8">
<graphic xlink:href="564616v4_ueqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Putting this together gives us the upper bound
<disp-formula id="eqn43">
<graphic xlink:href="564616v4_eqn43.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We now address the problem of the marginal entropy <italic>H</italic>[<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>]. By conditional independence, we have that
<disp-formula id="eqn44">
<graphic xlink:href="564616v4_eqn44.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We now make use of an additional assumption, namely that the representational curves Œ©<sub><italic>a</italic></sub> have a baseline, and in particular
<disp-formula id="eqn45">
<graphic xlink:href="564616v4_eqn45.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
everywhere. Under this condition we can make the approximation that for fixed <bold><italic>s</italic></bold>,
<disp-formula id="eqn46">
<graphic xlink:href="564616v4_eqn46.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
This means we can obtain the approximate upper bound:
<disp-formula id="ueqn9">
<graphic xlink:href="564616v4_ueqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where ‚Ñí (<bold><italic>g</italic></bold>) is the same functional that we defined earlier for Gaussian random variables, in the case of uncorrelated, unit-Fano-factor noise, <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref>. We have made an additional approximation by neglecting the <italic>I</italic><sub><italic>K</italic></sub><italic>/</italic>12 term, which relevant parameter conditions is negligible compared the covariance.</p>
</sec>
<sec id="s6c">
<label>B.3</label>
<title>Mathematical treatment of the clustered population</title>
<p>In this section we consider a clustered population in which the representational curves are a small perturbation to cluster-wide representational curves. Let <italic>C</italic><sub><italic>a</italic></sub> denote cluster <italic>a</italic> (or rather the set of indices of single neurons belonging to that cluster). Then for <italic>i</italic> ‚àà <italic>C</italic><sub><italic>a</italic></sub>,
<disp-formula id="eqn47">
<graphic xlink:href="564616v4_eqn47.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>œµ</italic> is a small parameter controlling the deviation from perfect within-cluster similarity. We use hatted symbols <inline-formula><inline-graphic xlink:href="564616v4_inline33.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to denote single-neuron quantities (defined analogously to <xref ref-type="disp-formula" rid="eqn7">Eqs. (7)</xref>‚Äì<xref ref-type="disp-formula" rid="eqn8">(8)</xref> but for single neurons, with Œ©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>) replaced by <inline-formula><inline-graphic xlink:href="564616v4_inline34.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and corresponding no-hat symbols for either cluster averages or zeroth-order values of tuning curves, etc, that are uniform across neurons in a cluster and hence only depend on cluster indices. In particular, we use <italic>P</italic> = CV<italic>œÅ</italic>CV, defined in <xref ref-type="disp-formula" rid="eqn37">Eq. (37)</xref>, at cluster level (hence a <italic>K √ó K</italic> matrix), and use <inline-formula><inline-graphic xlink:href="564616v4_inline35.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to denote the equivalent matrix defined for individual neurons (an <italic>N √ó N</italic> matrix). As we noted in the main text <xref ref-type="sec" rid="s2c">Sec. 2.3</xref>, for this analysis, we limit ourselves to the case of uncorrelated noise, Œ£(<bold><italic>s</italic></bold>) = <italic>I</italic>, with Poisson-like scaling <italic>Œ±</italic> = 1<italic>/</italic>2. Mirroring <xref ref-type="disp-formula" rid="eqn40">Eq. (40)</xref>, the population level objective in this case can be written as
<disp-formula id="eqn48">
<graphic xlink:href="564616v4_eqn48.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In the rest of this section we will set <italic>œÉ</italic> to 1 by absorbing it into <inline-formula><inline-graphic xlink:href="564616v4_inline36.gif" mimetype="image" mime-subtype="gif"/></inline-formula>; at the end, we can replace all <inline-formula><inline-graphic xlink:href="564616v4_inline37.gif" mimetype="image" mime-subtype="gif"/></inline-formula>‚Äîor factors proportional to them‚Äî by <inline-formula><inline-graphic xlink:href="564616v4_inline38.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to recover the general case).</p>
<sec id="s6c1">
<label>B.3.1</label>
<title>Expansion in <italic>œµ</italic></title>
<p>To simplify the mathematical derivation, we will assume that clusters are the same size <italic>k</italic> = <italic>N/K</italic>, and that w.l.o.g. the population is sorted so that neurons in the same cluster appear adjacent to each other in the ordering. However, the assumption of equal size clusters is not essential, and our final results are valid for the case of clusters of variable size as well. To zeroth order in <italic>œµ</italic>, the elements of <inline-formula><inline-graphic xlink:href="564616v4_inline39.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are constant over blocks corresponding to clusters; in other words
<disp-formula id="eqn49">
<graphic xlink:href="564616v4_eqn49.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
for some deviation <inline-formula><inline-graphic xlink:href="564616v4_inline40.gif" mimetype="image" mime-subtype="gif"/></inline-formula> that is <italic>O</italic>(<italic>œµ</italic>) (here <bold>1</bold><sub><italic>k</italic></sub> denotes the <italic>k</italic>-dimensional vector with all components equal to 1). However, we will find that the first non-zero corrections to the loss (for <italic>œµ &gt;</italic> 0) arise from <italic>O</italic>(<italic>œµ</italic><sup>2</sup>) corrections to <inline-formula><inline-graphic xlink:href="564616v4_inline41.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (or equivalently to <inline-formula><inline-graphic xlink:href="564616v4_inline42.gif" mimetype="image" mime-subtype="gif"/></inline-formula> as defined by <xref ref-type="disp-formula" rid="eqn49">Eq. (49)</xref>). We will therefore (1) expand ‚Ñí to second order in <inline-formula><inline-graphic xlink:href="564616v4_inline43.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, (2) (in the next section) expand <inline-formula><inline-graphic xlink:href="564616v4_inline44.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to second order in <italic>œµ</italic>, and (3) substitute <inline-formula><inline-graphic xlink:href="564616v4_inline45.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in the loss and eventually neglect terms that are <italic>o</italic>(<italic>œµ</italic><sup>2</sup>).</p>
<p>We start by expanding ‚Ñí. Using <xref ref-type="disp-formula" rid="eqn56">Eq. (56)</xref>, and plugging <xref ref-type="disp-formula" rid="eqn49">Eq. (49)</xref> into <xref ref-type="disp-formula" rid="eqn48">Eq. (48)</xref> and expanding to second order in <inline-formula><inline-graphic xlink:href="564616v4_inline46.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, we obtain
<disp-formula id="eqn50">
<graphic xlink:href="564616v4_eqn50.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where
<disp-formula id="eqn51">
<graphic xlink:href="564616v4_eqn51.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn52">
<graphic xlink:href="564616v4_eqn52.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn53">
<graphic xlink:href="564616v4_eqn53.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and we defined the following matrices:
<disp-formula id="eqn54">
<graphic xlink:href="564616v4_eqn54.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn55">
<graphic xlink:href="564616v4_eqn55.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here and in the rest of this appendix we use <bold>1</bold>, instead of <bold>1</bold><sub><italic>k</italic></sub>, to denote the <italic>k</italic>-dimensional vector with all components equal to 1.</p>
</sec>
<sec id="s6c2">
<label>B.3.2</label>
<title>Cluster level problem at the zeroth order</title>
<p>We now show that the zeroth order loss, <xref ref-type="disp-formula" rid="eqn51">Eq. (51)</xref>, depends only on the cluster rates <inline-formula><inline-graphic xlink:href="564616v4_inline47.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. This objective has two components: the information term and the energetic term. For the energetic term the claim is obvious, as we have
<disp-formula id="eqn56">
<graphic xlink:href="564616v4_eqn56.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>r</italic><sub><italic>a</italic></sub> denotes the total rate of cluster <italic>a</italic>. As for the information term, and will show that
<disp-formula id="ueqn10">
<graphic xlink:href="564616v4_ueqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
can be re-written (up to a constant) as
<disp-formula id="eqn57">
<graphic xlink:href="564616v4_eqn57.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
reducing the problem to a cluster-level one (the second determinant is of a <italic>K √ó K</italic> matrix, rather than <italic>N √ó N</italic>, and only depends on cluster rates <italic>r</italic><sub><italic>a</italic></sub> and the zeroth-order signal correlations encoded in <italic>P</italic>). Let us denote the Cholesky factor of (the positive definite) <italic>P</italic> by <italic>U</italic>, such that <italic>P</italic> = <italic>UU</italic><sup>T</sup>. We thus have
<disp-formula id="ueqn11">
<graphic xlink:href="564616v4_ueqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Further defining <inline-formula><inline-graphic xlink:href="564616v4_inline48.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to be the <italic>N √ó K</italic> matrix <inline-formula><inline-graphic xlink:href="564616v4_inline49.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, we find
<disp-formula id="eqn58">
<graphic xlink:href="564616v4_eqn58.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
By the matrix determinant lemma we then obtain
<disp-formula id="eqn59">
<graphic xlink:href="564616v4_eqn59.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Finally,
<disp-formula id="eqn60">
<graphic xlink:href="564616v4_eqn60.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Using this and making use of the matrix determinant lemma one more time, we see that the determinant on the right hand side of <xref ref-type="disp-formula" rid="eqn59">Eq. (59)</xref> can be written as <xref ref-type="disp-formula" rid="eqn57">Eq. (57)</xref>.</p>
<p>Putting together the information and energetic terms (and momentarily returning the <italic>œÉ</italic><sup>2</sup>), we obtain that (up to an additive constant)
<disp-formula id="eqn61">
<graphic xlink:href="564616v4_eqn61.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s6c3">
<label>B.3.3</label>
<title>Expansion of <inline-formula><inline-graphic xlink:href="564616v4_inline50.gif" mimetype="image" mime-subtype="gif"/></inline-formula> <bold>in</bold> <italic>œµŒ¥</italic>Œ©</title>
<p>We will denote <italic>œµŒ¥</italic>Œ©<sub><italic>i</italic></sub> above by <italic>V</italic><sub><italic>i</italic></sub> in this subsections (so <italic>V</italic> = <italic>O</italic>(<italic>œµ</italic>)), and will use <italic>Œ¥</italic>‚Äôs to denote deviation from expectation: <italic>Œ¥X</italic> = <italic>X</italic> ‚àí ùîº[<italic>X</italic>]. We have
<disp-formula id="eqn62">
<graphic xlink:href="564616v4_eqn62.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn63">
<graphic xlink:href="564616v4_eqn63.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="564616v4_inline51.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We also have
<disp-formula id="eqn64">
<graphic xlink:href="564616v4_eqn64.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
or (keeping cluster indices explicit but hiding witihn-cluster ones)
<disp-formula id="eqn65">
<graphic xlink:href="564616v4_eqn65.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we defined the vector <bold><italic>c</italic></bold><sub><italic>ab</italic></sub> to have components
<disp-formula id="eqn66">
<graphic xlink:href="564616v4_eqn66.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and
<disp-formula id="eqn67">
<graphic xlink:href="564616v4_eqn67.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Since
<disp-formula id="eqn68">
<graphic xlink:href="564616v4_eqn68.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
we also expand the diagonal factors to second order in <italic>V</italic> :
<disp-formula id="eqn69">
<graphic xlink:href="564616v4_eqn69.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we defined
<disp-formula id="eqn70">
<graphic xlink:href="564616v4_eqn70.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
(note that <italic>œâ</italic><sub><italic>a</italic></sub> are numbers, but <italic>≈µ</italic> <sub><italic>a</italic></sub>, <italic>v</italic><sub><italic>a</italic></sub> and <italic>u</italic><sub><italic>a</italic></sub> are <italic>k √ó k</italic> diagonal matrices).</p>
<p>First we consider the first order (in <italic>V</italic> or equivalently in <italic>œµ</italic>) contributions to <inline-formula><inline-graphic xlink:href="564616v4_inline52.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Using <xref ref-type="disp-formula" rid="eqn65">Eq. (65)</xref> and <xref ref-type="disp-formula" rid="eqn69">Eq. (69)</xref> in <xref ref-type="disp-formula" rid="eqn68">Eq. (68)</xref> we have
<disp-formula id="eqn71">
<graphic xlink:href="564616v4_eqn71.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn72">
<graphic xlink:href="564616v4_eqn72.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we defined
<disp-formula id="eqn73">
<graphic xlink:href="564616v4_eqn73.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Next, we obtain the second order contributions to <inline-formula><inline-graphic xlink:href="564616v4_inline53.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Using <xref ref-type="disp-formula" rid="eqn65">Eq. (65)</xref> and <xref ref-type="disp-formula" rid="eqn69">Eq. (69)</xref> in <xref ref-type="disp-formula" rid="eqn68">Eq. (68)</xref>, we obtain
<disp-formula id="eqn74">
<graphic xlink:href="564616v4_eqn74.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and in particular
<disp-formula id="eqn75">
<graphic xlink:href="564616v4_eqn75.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s6c4">
<label>B.3.4</label>
<title><italic>Œ¥</italic>Œ©<sub><italic>i</italic></sub> corrections to the loss</title>
<p>We now consider the contributions of the first and second order corrections to <inline-formula><inline-graphic xlink:href="564616v4_inline54.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to the loss, via <xref ref-type="disp-formula" rid="eqn52">Eqs. (52)</xref> and (53): to calculate corrections to the loss to <italic>O</italic>(<italic>œµ</italic><sup>2</sup>), we need to plug <inline-formula><inline-graphic xlink:href="564616v4_inline55.gif" mimetype="image" mime-subtype="gif"/></inline-formula> into <italic>‚Ñí</italic> <sup>(1)</sup>, and plug in <inline-formula><inline-graphic xlink:href="564616v4_inline56.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (only) in ‚Ñí <sup>(2)</sup>. Firstly, consider the matrix inverse that appears in expressions <xref ref-type="disp-formula" rid="eqn52">Eqs. (52)</xref>‚Äì<xref ref-type="disp-formula" rid="eqn53">(53)</xref> for ‚Ñí <sup>(1)</sup> and ‚Ñí <sup>(2)</sup>. Using <inline-formula><inline-graphic xlink:href="564616v4_inline57.gif" mimetype="image" mime-subtype="gif"/></inline-formula> from <xref ref-type="disp-formula" rid="eqn58">Eq. (58)</xref> and the Woodbury matrix identity, we can write
<disp-formula id="ueqn12">
<graphic xlink:href="564616v4_ueqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Using <inline-formula><inline-graphic xlink:href="564616v4_inline58.gif" mimetype="image" mime-subtype="gif"/></inline-formula> ‚Äìsee <xref ref-type="disp-formula" rid="eqn60">Eq. (60)</xref>‚Äì we find
<disp-formula id="ueqn13">
<graphic xlink:href="564616v4_ueqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Assuming <italic>P</italic> is full-rank (the generic case), we have <italic>U</italic> [<italic>I</italic><sub><italic>K</italic></sub> + <italic>U rU</italic>]<sup>‚àí1</sup><italic>U</italic> = ((<italic>U</italic><sup>‚àí1</sup>)<sup>T</sup> <italic>U</italic><sup>‚àí1</sup> + <italic>r</italic>) <sup>‚àí1</sup> = (<italic>P</italic><sup>‚àí1</sup> + <italic>r</italic>)<sup>‚àí1</sup> yielding
<disp-formula id="eqn76">
<graphic xlink:href="564616v4_eqn76.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Up until this point, all expressions we have obtained have been exact (apart from the <italic>œµ</italic>-expansion itself). We now make an approximation to the full perturbation objective which is valid in the high signal-to-noise ratio regime as outlined in <xref ref-type="sec" rid="s2d">Sec. 2.4</xref>. Specifically, we will take the matrix
<disp-formula id="ueqn14">
<graphic xlink:href="564616v4_ueqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
to be small, and expand to zeroth order in this matrix. Œî represents the high-dimensional structure of signal-to-noise ratio. As we show in App. B.4, to zeroth order in this matrix, <italic>r</italic> = <italic>¬µI</italic><sub><italic>K</italic></sub>. Using the approximation
<disp-formula id="eqn77">
<graphic xlink:href="564616v4_eqn77.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
valid to zeroth-order in Œî, in <xref ref-type="disp-formula" rid="eqn76">Eq. (76)</xref> we obtain
<disp-formula id="eqn78">
<graphic xlink:href="564616v4_eqn78.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we defined
<disp-formula id="eqn79">
<graphic xlink:href="564616v4_eqn79.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Note that in writing <xref ref-type="disp-formula" rid="eqn77">Eq. (77)</xref> we have assumed that cluster rates are <italic>O</italic>(<italic>¬µ</italic>), so that the second term in the parenthesis is <italic>O</italic>(1) and dominates Œî; this will be justified <italic>a posteriori</italic>, by the homeostasis result for the inter-cluster problem (as the approximate optimiser of <xref ref-type="disp-formula" rid="eqn61">Eq. (61)</xref>, in the small Œî regime).</p>
<p>We will show that ùíû and hence <italic>I</italic> ‚àí ùíû are projection operators and we will characterise the latter‚Äôs kernel (<italic>i</italic>.<italic>e</italic>., the vectors annihilated by it). First ùíû is clearly a symmetric matrix. So we just need to show that ùíû<sup>2</sup> = ùíû. To prove this (and other statements), we note that products of <italic>N √ó N</italic> matrices (or their products with <italic>N</italic>-dimensional vectors) can be written in terms of products of their blocks (corresponding to the clustering of neurons) as follows: <inline-formula><inline-graphic xlink:href="564616v4_inline59.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where the subscripts index blocks, with <italic>a, b</italic> ‚àà {1, ‚Ä¶ <italic>K</italic>}, and <italic>A</italic><sub><italic>ac</italic></sub> and <italic>B</italic><sub><italic>cb</italic></sub> are multiplying as matrices. As we will see our results do not truly rely on the tensor product structure we have assumed for various matrices (here ùíú and ‚Ñ¨); but only on their constancy within blocks defined by clusters. <italic>Thus our results generalise to the case where clusters contain different number of neurons</italic>. For simplicity, however, we will stick to the tensor structure, corresponding to the same number of neurons in different clusters. Using the above observation, we can write (note that below <italic>r</italic><sub><italic>a</italic></sub> and <italic>Œ¥</italic><sub><italic>ab</italic></sub> are scalars, while <inline-formula><inline-graphic xlink:href="564616v4_inline60.gif" mimetype="image" mime-subtype="gif"/></inline-formula> denotes the <italic>a</italic>-th diagonal block of <inline-formula><inline-graphic xlink:href="564616v4_inline61.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and is a diagonal matrix)
<disp-formula id="ueqn15">
<graphic xlink:href="564616v4_ueqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we used <inline-formula><inline-graphic xlink:href="564616v4_inline62.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p>
<p>Similarly for vectors of the form <bold><italic>≈µ</italic></bold> = <bold><italic>w</italic></bold> ‚äó <bold>1</bold> (namely vectors with components that are constant over each cluster), we have <inline-formula><inline-graphic xlink:href="564616v4_inline63.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, as
<disp-formula id="ueqn16">
<graphic xlink:href="564616v4_ueqn16.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
(note that <italic>w</italic><sub><italic>a</italic></sub> are scalars). Thus if <bold><italic>≈µ</italic></bold> is any such vector, then <inline-formula><inline-graphic xlink:href="564616v4_inline64.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is left invariant by <inline-formula><inline-graphic xlink:href="564616v4_inline65.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Finally, this means that <inline-formula><inline-graphic xlink:href="564616v4_inline66.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is in the kernel of <italic>I</italic> ‚àí <italic>ùíû</italic> = (<italic>I</italic> + <italic>ùíú</italic>)<sup>‚àí1</sup> and is annihilated by it. Moreover, since ùíû is symmetric, row vectors of the form <bold><italic>w</italic></bold><sup>T</sup> ‚äó<bold>1</bold><sup>T</sup> are annihilated when multiplied by <inline-formula><inline-graphic xlink:href="564616v4_inline67.gif" mimetype="image" mime-subtype="gif"/></inline-formula> on the right. Finally, matrices with cluster blocks that have uniform rows (columns) are annihilated when multiplied by <inline-formula><inline-graphic xlink:href="564616v4_inline67a.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (by <inline-formula><inline-graphic xlink:href="564616v4_inline68.gif" mimetype="image" mime-subtype="gif"/></inline-formula> on the left (right).</p>
<p>It follows immediately that, given the structure of <xref ref-type="disp-formula" rid="eqn52">Eqs. (52)</xref>‚Äì<xref ref-type="disp-formula" rid="eqn54">(54)</xref>, any of the terms in the expressions <xref ref-type="disp-formula" rid="eqn72">Eqs. (72)</xref>‚Äì<xref ref-type="disp-formula" rid="eqn85">(85)</xref> for blocks of <inline-formula><inline-graphic xlink:href="564616v4_inline69.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="564616v4_inline70.gif" mimetype="image" mime-subtype="gif"/></inline-formula> that have <bold>1</bold> as an outer product factor contribute nothing to <italic>Œ¥</italic>‚Ñí. In particular, the <italic>O</italic>(<italic>œµ</italic>) part, <inline-formula><inline-graphic xlink:href="564616v4_inline71.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, makes no contribution to the loss. Since we are interested in leading order corrections, it thus suffices to only consider the contribution of <inline-formula><inline-graphic xlink:href="564616v4_inline72.gif" mimetype="image" mime-subtype="gif"/></inline-formula>‚Äîafter dropping terms involving <bold>1</bold> in <xref ref-type="disp-formula" rid="eqn85">Eq. (85)</xref>‚Äî as it enters ‚Ñí <sup>(1)</sup> (in the Œî ‚â™ 1 limit). Denoting this correction by <italic>Œ¥L</italic>, from <xref ref-type="disp-formula" rid="eqn52">Eqs. (52)</xref>‚Äì<xref ref-type="disp-formula" rid="eqn54">(54)</xref> and <xref ref-type="disp-formula" rid="eqn78">Eq. (78)</xref> we find
<disp-formula id="eqn80">
<graphic xlink:href="564616v4_eqn80.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn81">
<graphic xlink:href="564616v4_eqn81.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Traces of <italic>N √ó N</italic> matrix products can be written in terms of traces of products of their blocks as Tr(<italic>AB</italic>) = ‚àë<sub><italic>ab</italic></sub> tr(<italic>A</italic><sub><italic>ab</italic></sub><italic>B</italic><sub><italic>ab</italic></sub>) where tr denotes trace over <italic>k √ó k</italic> blocks. We get a further simplification because <inline-formula><inline-graphic xlink:href="564616v4_inline73.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is diagonal and <inline-formula><inline-graphic xlink:href="564616v4_inline74.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is block-diagonal (due to the diagonality of <inline-formula><inline-graphic xlink:href="564616v4_inline75.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <italic>r</italic>), with the diagonal blocks given by <inline-formula><inline-graphic xlink:href="564616v4_inline76.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. But if <italic>D</italic> is a block-diagonal matrix, then Tr(<italic>DB</italic>) = ‚àë <sub><italic>a</italic></sub> tr(<italic>D</italic><sub><italic>aa</italic></sub><italic>B</italic><sub><italic>aa</italic></sub>). Thus
<disp-formula id="eqn82">
<graphic xlink:href="564616v4_eqn82.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn83">
<graphic xlink:href="564616v4_eqn83.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn84">
<graphic xlink:href="564616v4_eqn84.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In other words, the correction to the loss is a sum of terms, each of which depends only on the rates in one of the cluster only. As a result, given the total cluster rates <italic>r</italic><sub><italic>a</italic></sub>, the optimisation of rates of single neurons decouples across clusters. Which means we can optimise the rates in cluster <italic>a</italic> under the constraint that they sum up to <italic>r</italic><sub><italic>a</italic></sub>, to maximise <xref ref-type="disp-formula" rid="eqn84">Eq. (84)</xref>. Using <xref ref-type="disp-formula" rid="eqn85">Eq. (85)</xref> we now express <italic>Œ¥</italic> ‚Ñí <sub><italic>a</italic></sub> explicitly.</p>
</sec>
<sec id="s6c5">
<label>B.3.5</label>
<title>Within-cluster loss to leading order in <italic>œµ</italic></title>
<p>From <xref ref-type="disp-formula" rid="eqn75">Eq. (75)</xref>, after dropping the terms with factors of <bold>1</bold>, we have
<disp-formula id="eqn85">
<graphic xlink:href="564616v4_eqn85.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We further ignored the factor <inline-formula><inline-graphic xlink:href="564616v4_inline77.gif" mimetype="image" mime-subtype="gif"/></inline-formula> which multiplies both terms in the loss and is thus irrelevant to the optimisation; we will similarly ignore a factor of <italic>¬µ</italic> in the loss below. The right hand side of the above expression is the covariance of the zero-mean vector (of random variables)
<disp-formula id="eqn86">
<graphic xlink:href="564616v4_eqn86.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
(as can be easily checked:<inline-formula><inline-graphic xlink:href="564616v4_inline78.gif" mimetype="image" mime-subtype="gif"/></inline-formula>).</p>
<p>Since the problem has decoupled across clusters we will now drop the cluster index <italic>a</italic>; in particular we will denote the unperturbed Œ©<sub><italic>a</italic></sub> by Œ©<sub>0</sub>, and we will also denote the given/fixed total cluster rate, <italic>r</italic><sub><italic>a</italic></sub>, by <italic>R</italic><sub>0</sub>. Substituting the above in <xref ref-type="disp-formula" rid="eqn84">Eq. (84)</xref> and ignoring irrelevant overall prefactors, we find that the single-neuron rates in a cluster with total rate <italic>R</italic><sub>0</sub> are solutions of the following quadratic programming problem:
<disp-formula id="eqn87">
<graphic xlink:href="564616v4_eqn87.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where
<disp-formula id="eqn88">
<graphic xlink:href="564616v4_eqn88.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
This is equivalent (up to replacement of <italic>R</italic><sub>0</sub> with <italic>¬µ</italic>) to the optimisation problem, <xref ref-type="disp-formula" rid="eqn28">Eq. (28)</xref>, of the main text. Finally, note the scaling property of this quadratic programming problem: if we scale the cluster rate, <italic>R</italic><sub>0</sub>, by some <italic>Œ±</italic>, the solution <inline-formula><inline-graphic xlink:href="564616v4_inline79.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and thus the rates of individual neurons in the cluster, scale by the same factor. Note also that the scaling of ŒîŒ©<sup><italic>i</italic></sup> by a constant (within a cluster) factor does not make a difference, and we can make it ‚Äúdimensionless‚Äù by dividing it by <italic>œâ</italic><sub>0</sub> if helpful (in fact this amounts to returning the prefactor <inline-formula><inline-graphic xlink:href="564616v4_inline80.gif" mimetype="image" mime-subtype="gif"/></inline-formula> that we dropped after <xref ref-type="disp-formula" rid="eqn85">Eq. (85)</xref>).</p>
</sec>
<sec id="s6c6">
<label>B.3.6</label>
<title>A toy model for a cluster‚Äôs tuning curves</title>
<p>Here we will develop a toy model for Cov(Œî<bold>Œ©</bold>), <italic>i</italic>.<italic>e</italic>., for the statistics of deviations of representational curves of neurons in the same cluster from the unperturbed curve Œ©<sub>0</sub>. Assuming a degree of smoothness for Œ©<sup><italic>i</italic></sup>(<bold><italic>s</italic></bold>) (as a function on the stimulus space), we will adopt a basis of smooth functions <italic>b</italic><sub><italic>¬µ</italic></sub>(<bold><italic>s</italic></bold>) for 1 ‚â§ <italic>¬µ</italic> ‚â§ <italic>N</italic><sub><italic>b</italic></sub>, and assume that the log tuning curves are in the span of this set of functions. In other words, we assume
<disp-formula id="eqn89">
<graphic xlink:href="564616v4_eqn89.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
for some vector of coefficients <inline-formula><inline-graphic xlink:href="564616v4_inline81.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Similar to <xref ref-type="disp-formula" rid="eqn47">Eq. (47)</xref> we assume <inline-formula><inline-graphic xlink:href="564616v4_inline82.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (we absorb <italic>œµ</italic> into <italic>Œ¥</italic><bold><italic>w</italic></bold><sup><italic>i</italic></sup>). Expanding the above equation we get
<disp-formula id="ueqn17">
<graphic xlink:href="564616v4_ueqn17.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
or equivalently
<disp-formula id="eqn90">
<graphic xlink:href="564616v4_eqn90.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
It follows that <italic>v</italic><sup><italic>i</italic></sup> = ùîº[Œ©<sub>0</sub><bold><italic>b</italic></bold>] <italic>¬∑ Œ¥</italic><bold><italic>w</italic></bold><sup><italic>i</italic></sup>, and thus (from <xref ref-type="disp-formula" rid="eqn88">Eq. (88)</xref>)
<disp-formula id="eqn91">
<graphic xlink:href="564616v4_eqn91.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we defined
<disp-formula id="eqn92">
<graphic xlink:href="564616v4_eqn92.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We thus obtain:
<disp-formula id="eqn93">
<graphic xlink:href="564616v4_eqn93.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn94">
<graphic xlink:href="564616v4_eqn94.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We think of <italic>G</italic> as a metric defining the inner product of the vectors <italic>Œ¥</italic><bold><italic>w</italic></bold><sub><italic>i</italic></sub>. Note also that given the final comment in the previous subsection, we can replace Œ©<sub>0</sub>(<bold><italic>s</italic></bold>) inside the integral expression for <italic>G</italic> with Œ©<sub>0</sub>(<bold><italic>s</italic></bold>)<italic>/œâ</italic><sub>0</sub>; this is nice as it means that if it happens that the cluster tuning curve Œ©<sub>0</sub> is mostly supported on regions with small stimulus frequency, the above metric does not go to zero. Alternatively, since the scaling of Cov(Œî<bold>Œ©</bold>) is irrelevant to the within-cluster rate optimisation, we can assume that the expectation in the definition of <italic>G</italic> is taken, not with respect to <italic>P</italic> (<bold><italic>s</italic></bold>), but with respect to the <italic>normalised</italic> measure with density proportional to <inline-formula><inline-graphic xlink:href="564616v4_inline82a.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. If we denote expectation under this ‚Äútilted‚Äù measure by <inline-formula><inline-graphic xlink:href="564616v4_inline83.gif" mimetype="image" mime-subtype="gif"/></inline-formula> we can also write:
<disp-formula id="eqn95">
<graphic xlink:href="564616v4_eqn95.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Similarly we could have written
<disp-formula id="eqn96">
<graphic xlink:href="564616v4_eqn96.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
(note that this last expectation is tilted by Œ©<sub>0</sub> and not by <inline-formula><inline-graphic xlink:href="564616v4_inline84.gif" mimetype="image" mime-subtype="gif"/></inline-formula>). To summarise, we have found that the quadratic form for our quadratic programming optimisation is given by the Gram matrix of the neurons‚Äô coefficients <italic>Œ¥</italic><bold><italic>w</italic></bold><sub><italic>i</italic></sub> with respect to the inner product defined by <italic>G</italic>.</p>
<p>For our minimal toy model, we will assume that the perturbations <italic>Œ¥</italic><bold><italic>w</italic></bold><sub><italic>i</italic></sub>are isotropic, mean-zero Gaussians, <italic>i.e.</italic>, <inline-formula><inline-graphic xlink:href="564616v4_inline85.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We will additionally assume that the positive-definite matrix <italic>G</italic> has <italic>D</italic> eigenvalues which are roughly equal, and all significantly larger than the other eigenvalues. In this case, (up to scaling) we can approximate <italic>G</italic> as a projection matrix onto <italic>D</italic>-dimensions. This allows us to generate <inline-formula><inline-graphic xlink:href="564616v4_inline86.gif" mimetype="image" mime-subtype="gif"/></inline-formula> simply as <inline-formula><inline-graphic xlink:href="564616v4_inline87.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where the vectors <inline-formula><inline-graphic xlink:href="564616v4_inline88.gif" mimetype="image" mime-subtype="gif"/></inline-formula> were drawn independently from a <italic>D</italic>-dimensional isotropic Gaussian, <italic>Œ¥</italic><bold><italic>w</italic></bold><sub><italic>i</italic></sub> ~ ùí© (0, <italic>I</italic><sub><italic>D</italic></sub>).</p>
</sec>
</sec>
<sec id="s6d">
<label>B.4</label>
<title>First order maximisation of ‚Ñí</title>
<p>In this section we compute the maximiser of ‚Ñí, to first order in the parameter Œî, defined by <bold>??</bold>:
<disp-formula id="ueqn18">
<graphic xlink:href="564616v4_ueqn18.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
To simplify the derivation we will work with as a function of the rates <italic>r</italic><sub><italic>a</italic></sub> = <italic>g</italic><sub><italic>a</italic></sub><italic>œâ</italic><sub><italic>a</italic></sub>, rather than the gains. Writing <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref> in terms of <italic>r</italic><sub><italic>a</italic></sub>, we have
<disp-formula id="ueqn19">
<graphic xlink:href="564616v4_ueqn19.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We will also introduce the matrix
<disp-formula id="eqn97">
<graphic xlink:href="564616v4_eqn97.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
so that
<disp-formula id="eqn98">
<graphic xlink:href="564616v4_eqn98.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Note that
<disp-formula id="eqn99">
<graphic xlink:href="564616v4_eqn99.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>E</italic><sub><italic>a</italic></sub> is the diagonal matrix which is zero everywhere except having 1 as the <italic>a</italic>-th element along the diagonal.</p>
<p>Using <xref ref-type="disp-formula" rid="eqn98">Eq. (98)</xref> and <xref ref-type="disp-formula" rid="eqn99">Eq. (99)</xref>, we obtain that
<disp-formula id="eqn100">
<graphic xlink:href="564616v4_eqn100.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn101">
<graphic xlink:href="564616v4_eqn101.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn102">
<graphic xlink:href="564616v4_eqn102.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn103">
<graphic xlink:href="564616v4_eqn103.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we used the cyclicity of trace and the symmetry of <italic>M</italic> and <italic>W</italic>. Setting the derivative in <xref ref-type="disp-formula" rid="eqn100">Eq. (100)</xref> to zero and rearranging, and defining <italic>Œ≤</italic> = 2(1 ‚àí <italic>Œ±</italic>), we obtain
<disp-formula id="eqn104">
<graphic xlink:href="564616v4_eqn104.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We now apply the ansatz
<disp-formula id="eqn105">
<graphic xlink:href="564616v4_eqn105.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Plugging this into <xref ref-type="disp-formula" rid="eqn97">Eq. (97)</xref> we have that
<disp-formula id="ueqn20">
<graphic xlink:href="564616v4_ueqn20.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
which after substitution in the right hand side of <xref ref-type="disp-formula" rid="eqn104">Eq. (104)</xref> yields
<disp-formula id="ueqn21">
<graphic xlink:href="564616v4_ueqn21.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where on the penultimate line we have used the Neumann expansion of <italic>I</italic> [+ Œî + ùí™ (Œî<sup>2</sup>)] <sup>‚àí1</sup>. Thus to first order in Œî, the optimal solution is given by <italic>r</italic><sub><italic>a</italic></sub> = <italic>¬µŒ≤</italic>(1 ‚àí Œî<sub><italic>aa</italic></sub>). In terms of the gains, this corresponds to the solution
<disp-formula id="eqn106">
<graphic xlink:href="564616v4_eqn106.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
which also yields <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref> in the limit Œî ‚Üí 0.</p>
</sec>
<sec id="s6e">
<label>B.5</label>
<title>Aligned noise leads to perfect homeostasis and uniformisation</title>
<p>In this appendix we demonstrate that (for constant coefficient of variation) if <italic>œÅ</italic> and <italic>W</italic> are perfectly aligned <italic>i</italic>.<italic>e</italic>.,, <italic>œÅ</italic> = <italic>W</italic>, then the optimal rates are all equal and invariant of the environment, <italic>i</italic>.<italic>e</italic>., we have perfect homeostasis and uniformisation.</p>
<p>In App. B.4 we derived an equation for the maximiser of the objective function ‚Ñí (<bold><italic>r</italic></bold>), <xref ref-type="disp-formula" rid="eqn104">Eq. (104)</xref>,
<disp-formula id="eqn107">
<graphic xlink:href="564616v4_eqn107.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We now show that, when <italic>W</italic> = <italic>œÅ</italic> and CV is constant, this equation is satisfied by constant <bold><italic>r</italic></bold> = <italic>œá</italic><bold>1</bold><sub><italic>k</italic></sub> for some <italic>œá</italic>, and find an equation for <italic>œá</italic>.
<disp-formula id="ueqn22">
<graphic xlink:href="564616v4_ueqn22.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We will now show that this equation has a solution, provided certain conditions on the parameters hold. We divide into two cases. If <italic>Œ≤</italic> ‚â§ 1 then <italic>f</italic> (<italic>œá</italic>) is strictly increasing and limits to 0 as <italic>œá</italic> ‚Üí 0 and infinity as <italic>œá</italic> ‚Üí ‚àû. By continuity we must have <italic>f</italic> (<italic>œá</italic>) = <italic>¬µŒ≤</italic> at some point.</p>
<p>If <italic>œá &gt;</italic> 1 then we still have <italic>f</italic> (<italic>œá</italic>) ‚Üí ‚àû as <italic>œá</italic> ‚Üí ‚àû. We now find the minimum of <italic>f</italic>. Taking derivatives, we see that this occurs at
<disp-formula id="eqn108">
<graphic xlink:href="564616v4_eqn108.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
At this point, we have
<disp-formula id="eqn109">
<graphic xlink:href="564616v4_eqn109.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
This is less than <italic>¬µŒ≤</italic> provided that
<disp-formula id="eqn110">
<graphic xlink:href="564616v4_eqn110.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
So provided <xref ref-type="disp-formula" rid="eqn110">Eq. (110)</xref> holds, there is a solution by continuity. Recall that <italic>¬µŒ≤</italic> is approximately the average firing rate of a cluster. Typically we are interested in holding the product of these parameters constant at a value <italic>r</italic>. If we do so, we can take the derivative of the right hand side of <xref ref-type="disp-formula" rid="eqn110">Eq. (110)</xref> in <italic>Œ≤</italic>, obtaining that this expression is minimised when <italic>Œ≤</italic> = <italic>r/</italic>(<italic>r</italic> ‚àí 1), where it attains a value of <italic>r</italic> ‚àí 1. Putting this together, we can say that there exists a uniform, homeostatic solution provided
<disp-formula id="eqn111">
<graphic xlink:href="564616v4_eqn111.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>r</italic> is the product <italic>Œ≤¬µ</italic>.</p>
</sec>
<sec id="s6f">
<label>B.6</label>
<title>Estimates for <inline-formula><inline-graphic xlink:href="564616v4_inline89.gif" mimetype="image" mime-subtype="gif"/></inline-formula></title>
<p>We define <italic>A</italic><sub><italic>Œ≥</italic></sub> to be the normalisation constant of a correlation matrix with an eigenspectrum proportional to 1<italic>/n</italic><sup><italic>Œ≥</italic></sup>. Since a <italic>K √ó K</italic> correlation matrix has trace <italic>K</italic>, we have
<disp-formula id="eqn112">
<graphic xlink:href="564616v4_eqn112.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We start with the case <italic>Œ≥</italic>‚â† 1. Then we have that
<disp-formula id="ueqn23">
<graphic xlink:href="564616v4_ueqn23.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Now, taking the limit as <italic>Œ≥</italic> ‚Üí 1, we obtain that
<disp-formula id="eqn113">
<graphic xlink:href="564616v4_eqn113.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Now when <italic>Œ≥ &lt;</italic> 1 and <italic>K</italic> is large, we obtain
<disp-formula id="eqn114">
<graphic xlink:href="564616v4_eqn114.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Lastly, when <italic>Œ≥ &gt;</italic> 1 and <italic>K</italic> is large, we obtain
<disp-formula id="eqn115">
<graphic xlink:href="564616v4_eqn115.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
These approximations allow us to arrive at the following estimates. Firstly, consider tr(<italic>œÅ</italic><sup>‚àí1</sup>)<italic>/K</italic>.
<disp-formula id="ueqn24">
<graphic xlink:href="564616v4_ueqn24.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Next, consider the case of aligned noise, when <italic>W</italic> has a <italic>A</italic><sub><italic>Œ≥</italic></sub><italic>/n</italic><sup><italic>Œ≥</italic></sup> eigenspectrum and aligned eigenbases with <italic>œÅ</italic>. In this case,
<disp-formula id="ueqn25">
<graphic xlink:href="564616v4_ueqn25.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We can then derive the following approximations for tr(<italic>œÅ</italic><sup>‚àí1</sup><italic>W</italic>)<italic>/K</italic> using <xref ref-type="disp-formula" rid="eqn113">Eqs. (113)</xref>‚Äì<xref ref-type="disp-formula" rid="eqn115">(115)</xref>.
<disp-formula id="ueqn26">
<graphic xlink:href="564616v4_ueqn26.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s6g">
<label>B.7</label>
<title>Optimal homeostatic gains</title>
<p>We now consider the case where we enforce homeostasis on the gains. Prior to taking expectations across environments, our objective, considered as a function of <italic>r</italic><sub><italic>a</italic></sub> = <italic>g</italic><sub><italic>a</italic></sub><italic>œâ</italic><sub><italic>a</italic></sub> is (up to additive constants)
<disp-formula id="ueqn27">
<graphic xlink:href="564616v4_ueqn27.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>P</italic> = CV<italic>œÅ</italic>CV. Enforcing homeostasis at the cluster level means setting <inline-formula><inline-graphic xlink:href="564616v4_inline90.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for all clusters. Substituting this in, we obtain the function
<disp-formula id="ueqn28">
<graphic xlink:href="564616v4_ueqn28.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <italic>Œª</italic><sub><italic>n</italic></sub> are the eigenvalues of <italic>œÉ</italic><sup>‚àí2</sup><italic>W</italic><sup>‚àí1</sup><italic>P</italic>. Note that this is only a function of the spectrum, and not of the eigenbasis. Therefore, under the assumption that the spectrum remains fixed across environments, we can drop the need to take expectations, and work with ‚Ñí as a function only of the spectrum. We work under this assumption going forward.</p>
<p>The optimal <italic>œá</italic>, within this family of approximate solutions, obeys
<disp-formula id="ueqn29">
<graphic xlink:href="564616v4_ueqn29.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn116">
<graphic xlink:href="564616v4_eqn116.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We now demonstrate that, under appropriate conditions, <inline-formula><inline-graphic xlink:href="564616v4_inline91.gif" mimetype="image" mime-subtype="gif"/></inline-formula>
<disp-formula id="ueqn30">
<graphic xlink:href="564616v4_ueqn30.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Now, under the assumption that (<italic>Œ≤¬µ</italic>)<sup><italic>Œ≤</italic></sup><italic>Œª</italic><sub><italic>n</italic></sub> ‚â´ 1 for each <italic>n</italic>, we can say that <italic>œá</italic> ‚âà <italic>¬µŒ≤</italic> and (<italic>Œ≤¬µ</italic>)<sup><italic>Œ≤</italic></sup><italic>Œª</italic><sub><italic>n</italic></sub> ‚âà <italic>œá</italic><sup><italic>Œ≤</italic></sup><italic>Œª</italic><sub><italic>n</italic></sub> + 1. Substituting this in gives us the required result. Note that this condition is equivalent to the high signal-to-noise ratio condition we have used throughout.</p>
<p>We now consider the special cases discussed in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref> in which more precise solutions can be obtained. Note that in all three cases the analytics correspond to idealisations of the numerical simulations we actually perform in <xref ref-type="sec" rid="s2f">Sec. 2.6</xref>.</p>
<sec id="s6g1">
<title>Uncorrelated power-law noise</title>
<p>In the first special case under consideration, <italic>W</italic> = <italic>I</italic><sub><italic>K</italic></sub>, CV<sup>2</sup> is constant, and <italic>œÅ</italic> has approximately a <italic>A</italic><sub>1</sub><italic>/n</italic> eigenspectrum, where <italic>A</italic><sub>1</sub> is chosen to normalise the trace of <italic>œÅ</italic> to be equal to <italic>K, i</italic>.<italic>e</italic>.,
<disp-formula id="ueqn31">
<graphic xlink:href="564616v4_ueqn31.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The spectrum of <italic>œÉ</italic><sup>‚àí2</sup><italic>W</italic><sup>‚àí1</sup><italic>P</italic> is therefore <italic>Œª</italic><sub><italic>n</italic></sub> = <italic>b/n</italic> where
<disp-formula id="eqn117">
<graphic xlink:href="564616v4_eqn117.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In this case, (defining <italic>u</italic> = <italic>n/K</italic>) we can make the following approximation to the right hand side of <xref ref-type="disp-formula" rid="eqn116">Eq. (116)</xref>:
<disp-formula id="ueqn32">
<graphic xlink:href="564616v4_ueqn32.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
This gives the new equation
<disp-formula id="eqn118">
<graphic xlink:href="564616v4_eqn118.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Approximating <italic>A</italic><sub>1</sub> <italic>‚âà K/</italic> ln(<italic>K</italic>) (see App. B.6) and using <xref ref-type="disp-formula" rid="eqn117">Eq. (117)</xref>, we obtain that
<disp-formula id="eqn119">
<graphic xlink:href="564616v4_eqn119.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Substituting <xref ref-type="disp-formula" rid="eqn119">Eq. (119)</xref> into <xref ref-type="disp-formula" rid="eqn118">Eq. (118)</xref> gives us
<disp-formula id="eqn120">
<graphic xlink:href="564616v4_eqn120.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s6g2">
<title>Aligned noise</title>
<p>In the aligned noise case, we approximate <italic>œÅ</italic> and <italic>W</italic> as having the same eigenbasis. <italic>œÅ</italic> still has an eigenspectrum of <italic>A</italic><sub>1</sub><italic>/n</italic>, and we take <italic>W</italic> to have an eigenspectrum of <italic>A</italic><sub><italic>Œ≥</italic></sub><italic>/n</italic><sup><italic>Œ≥</italic></sup> where <italic>A</italic><sub><italic>Œ≥</italic></sub> normalises the trace of <italic>W</italic> to be equal to <italic>K</italic>,
<disp-formula id="ueqn33">
<graphic xlink:href="564616v4_ueqn33.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The matrix <italic>œÉ</italic><sup>2</sup><italic>W</italic><sup>‚àí1</sup><italic>P</italic> therefore has eigenspectrum <italic>bn</italic><sup><italic>Œ≥</italic>‚àí1</sup> where <italic>b</italic> = <italic>œÉ</italic><sup>‚àí2</sup><italic>A</italic><sub>1</sub>CV<sup>2</sup><italic>/A</italic><sub><italic>Œ≥</italic></sub>. Inserting this into <xref ref-type="disp-formula" rid="eqn116">Eq. (116)</xref> gives us
<disp-formula id="eqn121">
<graphic xlink:href="564616v4_eqn121.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
See App. B.6 for estimates of <italic>A</italic><sub><italic>Œ≥</italic></sub>.</p>
</sec>
<sec id="s6g3">
<title>Constant correlation noise</title>
<p>The next special case occurs when <italic>Œ≤</italic> = 1, and <italic>W</italic> = (1 ‚àí <italic>p</italic>)<italic>I</italic><sub><italic>K</italic></sub> + <italic>p</italic><bold>11</bold> <sup>T</sup>. Using the Sherman-Morrison formula, we obtain
<disp-formula id="ueqn34">
<graphic xlink:href="564616v4_ueqn34.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Since <inline-formula><inline-graphic xlink:href="564616v4_inline92.gif" mimetype="image" mime-subtype="gif"/></inline-formula> we neglect this term and approximate <inline-formula><inline-graphic xlink:href="564616v4_inline93.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. This therefore has the same effect as making the replacement <italic>œÉ</italic><sup>2</sup> ‚Ü¶ (1 ‚àí <italic>p</italic>)<italic>œÉ</italic><sup>2</sup>. Substituting this into <xref ref-type="disp-formula" rid="eqn120">Eq. (120)</xref>, and using <italic>Œ≤</italic> = 1, we obtain
<disp-formula id="eqn122">
<graphic xlink:href="564616v4_eqn122.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We define
<disp-formula id="ueqn35">
<graphic xlink:href="564616v4_ueqn35.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and rearrange to get
<disp-formula id="ueqn36">
<graphic xlink:href="564616v4_ueqn36.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
</sec>
<sec id="s6h">
<label>B.8</label>
<title>Synaptic normalisation allows for propagation of homeostasis between populations</title>
<p>In this appendix, we derive how the weights between neural populations must change in order for homeostasis to be propagated between them (see the last paragraph of <xref ref-type="sec" rid="s2f">Section 2.6</xref>). We work in a linear (or linearised) rate model.</p>
<p>Consider two neural populations, with an upstream population providing feedforward input to a downstream population. We will suppose that the upstream population is engaged in homeostatic coding, and ask what is necessary for the downstream population to be also engaged in homeostatic coding. We will denote the tuning and representational curves of the upsteam population by <inline-formula><inline-graphic xlink:href="564616v4_inline94.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="564616v4_inline95.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, respectively, where <italic>‚Ñì</italic> = 1 (<italic>‚Ñì</italic> = 2) for the upstream (downstream) population or layer. We drop corrections to the zeroth order solution for optimal gains, and work within the homeostatic coding regime; that is we assume the gains of the upstream population are given by
<disp-formula id="eqn123">
<graphic xlink:href="564616v4_eqn123.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The downstream population of tuning curves will be given by <inline-formula><inline-graphic xlink:href="564616v4_inline95a.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for <italic>m</italic> = 1, ‚Ä¶, <italic>M</italic>. Let <italic>W</italic><sub><italic>ma</italic></sub> be the synaptic weight from neuron <italic>a</italic> in the upstream population to neuron <italic>m</italic> in the downstream population. Working in a linearised rate model, this gives us that the tuning curve of neuron <italic>m</italic> is
<disp-formula id="eqn124">
<graphic xlink:href="564616v4_eqn124.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Suppose that, as dictated by the computational goals of the circuit, the downstream population has representational curves <inline-formula><inline-graphic xlink:href="564616v4_inline95b.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for <italic>m</italic> = 1, ‚Ä¶, <italic>M</italic>. We similarly write each representational curve as a linear combination of the upstream cluster representational curves,
<disp-formula id="eqn125">
<graphic xlink:href="564616v4_eqn125.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We call these weights <italic>w</italic><sub><italic>ma</italic></sub> the <italic>computational weights</italic>, since they are determined by the computational goals of the downstream population. In line with our approach so far, we treat these computational goals, and hence also the computational weights, as given (<italic>i</italic>.<italic>e</italic>., set independently of optimal gain adaptation).</p>
<p>If the downstream population is also implementing homeostatic coding, we know that
<disp-formula id="eqn126">
<graphic xlink:href="564616v4_eqn126.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="564616v4_inline96.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We asked how the synaptic weights <italic>W</italic><sub><italic>ma</italic></sub> should depend on the computational weights <italic>w</italic><sub><italic>ma</italic></sub>, and how they should adapt as stimulus statistics change in order for <xref ref-type="disp-formula" rid="eqn126">Eq. (126)</xref> to hold, <italic>i</italic>.<italic>e</italic>., for the downstream population to also engage in homeostatic coding. But then
<disp-formula id="ueqn37">
<graphic xlink:href="564616v4_ueqn37.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Substituting this in, we get
<disp-formula id="eqn127">
<graphic xlink:href="564616v4_eqn127.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Comparing coefficients, we can see that
<disp-formula id="eqn128">
<graphic xlink:href="564616v4_eqn128.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In other words, this scheme requires that the synaptic weights are normalised such that the total synaptic weights received by each downstream neuron remains constant, independently of adjustments of the gains of the pre-synaptic neurons or of possible changes in the computational weights (Different normalisation factors are equivalent to different values of <italic>œá</italic> for different populations. Differences in the optimal choice of <italic>œá</italic> may arise from <italic>e</italic>.<italic>g</italic>., different noise correlation statistics or different rate coding intervals between populations.). Thus, homeostatic coding, applied sequentially to two populations, provides an additional normative interpretation of synaptic normalisation, in which synapses onto a neuron are jointly scaled to keep total input weights constant (<xref ref-type="bibr" rid="c58">Turrigiano et al., 1998</xref>; <xref ref-type="bibr" rid="c57">Turrigiano, 2008</xref>).</p>
</sec>
<sec id="s6i">
<label>B.9</label>
<title>Hierarchical Bayes-ratio coding</title>
<p>In this appendix, we consider homeostatic propagation (as defined in the Appendix B.8) in the specific case of a feedforward net implementing Bayes-ratio coding. In particular, we calculate and interpret the feedforward synaptic weights needed for this propagation. We start with a two-layer generative model <bold><italic>z</italic></bold><sup>2</sup> ‚Üí <bold><italic>z</italic></bold><sup>1</sup> ‚Üí <bold><italic>s</italic></bold> (with joint density given by <italic>P</italic> (<bold><italic>s, z</italic></bold><sup>1</sup>, <bold><italic>z</italic></bold><sup>2</sup>) = <italic>œÄ</italic><sub>2</sub> (<bold><italic>z</italic></bold><sup>2</sup>)<italic>f</italic><sub>2‚Üí1</sub>(<bold><italic>z</italic></bold><sup>1</sup>|<bold><italic>z</italic></bold><sup>2</sup>)<italic>f</italic><sub>1‚Üí0</sub> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold><sup>1</sup>)), and consider a two-layer feedforward recognition network that inverts that generative model. Recall that in Bayes-ratio coding, the representational curves are given by the posterior distribution; in particular, in the second layer of the recognition network these are given by posteriors of the higher level variable <bold><italic>z</italic></bold><sup>2</sup>:
<disp-formula id="ueqn38">
<graphic xlink:href="564616v4_ueqn38.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We first show that the second layer‚Äôs representational curves can be written as a linear combination of the first layer‚Äôs representational curves, <italic>i.e.</italic>, <inline-formula><inline-graphic xlink:href="564616v4_inline97.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. To find the proper value of <italic>w</italic><sub><italic>ma</italic></sub> (which would lead to the correct inversion of the generative model), note that due the Markov structure of the generative model, the correct posterior factorises as Œ†(<bold><italic>z</italic></bold><sup>1</sup>, <bold><italic>z</italic></bold><sup>2</sup>|<bold><italic>s</italic></bold>) = Œ†(<bold><italic>z</italic></bold><sup>2</sup>|<bold><italic>z</italic></bold><sup>1</sup>)Œ†(<bold><italic>z</italic></bold><sup>1</sup>|<bold><italic>s</italic></bold>). Thus Œ†<bold><italic>z</italic></bold><sup>2</sup>|<bold><italic>s</italic></bold> = Œ†(<bold><italic>z</italic></bold><sup>2</sup>|<bold><italic>z</italic></bold><sup>1</sup>)Œ†(<bold><italic>z</italic></bold><sup>1</sup>|<bold><italic>s</italic></bold>)<italic>d</italic><bold><italic>z</italic></bold><sup>1</sup>, and
<disp-formula id="ueqn39">
<graphic xlink:href="564616v4_ueqn39.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
hence
<disp-formula id="eqn129">
<graphic xlink:href="564616v4_eqn129.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Recall from Appendix B.8 that the feedforward synaptic weights, <italic>W</italic><sub><italic>ma</italic></sub>, needed for homeostatic propagation are given by <xref ref-type="disp-formula" rid="eqn128">Eq. (128)</xref>, in terms of the ‚Äúcomputational weights‚Äù, <italic>w</italic><sub><italic>ma</italic></sub>. Using <xref ref-type="disp-formula" rid="eqn129">Eq. (129)</xref> and that (in an ideal observer model) <inline-formula><inline-graphic xlink:href="564616v4_inline98.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (<italic>i</italic>.<italic>e</italic>., the average posterior over <bold><italic>z</italic></bold><sup>1</sup> is equal to their prior distribution) we obtain
<disp-formula id="ueqn40">
<graphic xlink:href="564616v4_ueqn40.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and
<disp-formula id="ueqn41">
<graphic xlink:href="564616v4_ueqn41.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Plugging these in <xref ref-type="disp-formula" rid="eqn128">Eq. (128)</xref>, and using Œ†(<bold><italic>z</italic></bold><sup>2</sup>|<bold><italic>z</italic></bold><sup>1</sup>) <italic>œÄ</italic> (<bold><italic>z</italic></bold><sup>1</sup>) = <italic>f</italic> <sub>2‚Üí1</sub>(<bold><italic>z</italic></bold><sup>1</sup>|<bold><italic>z</italic></bold><sup>2</sup>) <italic>œÄ</italic><sub>2</sub> (<bold><italic>z</italic></bold><sup>2</sup>), we obtain
<disp-formula id="eqn130">
<graphic xlink:href="564616v4_eqn130.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Thus a linear feedforward network with these feedforward weights correctly propagates Bayes-ratio coding to implement the exact inversion of a two-layer generative model. By induction, this result can be extended to <italic>n</italic>-layer generative models with Markov structure <bold><italic>z</italic></bold><sup><italic>n</italic></sup> ‚Üí ‚Ä¶ <bold><italic>z</italic></bold><sup>1</sup> ‚Üí <bold><italic>s</italic></bold>, and their corresponding Bayes-ratio encoding recognition models furnished by <italic>n</italic>-layer linear feedforward nets.</p>
</sec>
<sec id="s6j">
<label>B.10</label>
<title>Stimulus specific adaptation for non-ideal-observer models</title>
<p>In this appendix, we show how discrepancies between the internal model and the external environment lead to simultaneous stimulus specific and neuron specific adaptation effects in a homeostatic DDC code. We begin with the special case of a Bayes-ratio code, which is a DDC in which the kernel functions are delta functions (as discussed in <xref ref-type="sec" rid="s2h">Sec. 2.8</xref>). In a Bayes-ratio code, representational curves are given by
<disp-formula id="eqn131">
<graphic xlink:href="564616v4_eqn131.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where Œ† is the posterior distribution under the internal generative model. We consider the case of a non-ideal-observer generative model, in which case there will be a discrepancy between the marginal stimulus distribution predicted by the model, <italic>P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>) and the external environment stimulus marginal <italic>P</italic><sup><italic>E</italic></sup>(<bold><italic>s</italic></bold>). We reason as follows:
<disp-formula id="eqn132">
<graphic xlink:href="564616v4_eqn132.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where
<disp-formula id="eqn133">
<graphic xlink:href="564616v4_eqn133.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
According to homeostatic coding, the tuning curve is given by
<disp-formula id="eqn134">
<graphic xlink:href="564616v4_eqn134.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>œâ</italic><sub><italic>a</italic></sub> is the temporal average of the representational curve, or equivalently, the expectation of Œ©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>) under the <italic>true</italic> environmental stimulus distribution. Thus
<disp-formula id="eqn135">
<graphic xlink:href="564616v4_eqn135.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we defined
<disp-formula id="eqn136">
<graphic xlink:href="564616v4_eqn136.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Notice that when <italic>P</italic><sup><italic>E</italic></sup> = <italic>P</italic><sup><italic>I</italic></sup>, as would be the case for an ideal-observer, we will have <italic>F</italic><sub><italic>a</italic></sub> = 1, due to the normalization of <italic>f</italic> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold>). Substituting <xref ref-type="disp-formula" rid="eqn132">Eq. (132)</xref> and <xref ref-type="disp-formula" rid="eqn135">Eq. (135)</xref> into <xref ref-type="disp-formula" rid="eqn134">Eq. (134)</xref> we obtain
<disp-formula id="eqn137">
<graphic xlink:href="564616v4_eqn137.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
as claimed in <xref ref-type="disp-formula" rid="eqn22">Eq. (22)</xref>.</p>
<p>We now consider the more general case of a DDC code. We define
<disp-formula id="eqn138">
<graphic xlink:href="564616v4_eqn138.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
so that <italic>F</italic><sub><italic>a</italic></sub> in the above is equal to <italic>F</italic> (<bold><italic>z</italic></bold><sub><italic>a</italic></sub>). An analogous derivation yields:
<disp-formula id="eqn139">
<graphic xlink:href="564616v4_eqn139.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We can see that in this case there is no clean separation of neuron specific and stimulus specific factors. In particular, the generalization of <xref ref-type="disp-formula" rid="eqn24">Eq. (24)</xref> takes the form:
<disp-formula id="eqn140">
<graphic xlink:href="564616v4_eqn140.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In the limit as the width of the kernels becomes small, the integrals involving kernels collapse to sampling at a single point, as occurs with the Bayes-ratio code. When this happens, we get full separation to stimulus and neuron specific effects; otherwise these effects are mixed together, with more mixing occurring the larger the width of the kernels. However, when the kernels are unimodal and sufficiently narrow, we would expect an approximate factorization of the effect of adaptation into a stimulus-specific and a neuron-specific suppression factor.</p>
</sec>
</app>
</app-group>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Atick</surname>, <given-names>J. J.</given-names></string-name> and <string-name><surname>Redlich</surname>, <given-names>A. N.</given-names></string-name></person-group> (<year>1990</year>). <article-title>Towards a Theory of Early Visual Processing</article-title>. <source>Neural Computation</source>, <volume>2</volume>(<issue>3</issue>):<fpage>308</fpage>‚Äì<lpage>320</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Attneave</surname>, <given-names>F.</given-names></string-name></person-group> (<year>1954</year>). <article-title>Some informational aspects of visual perception</article-title>. <source>Psychological Review</source>, <volume>61</volume>(<issue>3</issue>):<fpage>183</fpage>‚Äì<lpage>193</lpage>. Place: <publisher-loc>US</publisher-loc> Publisher: <publisher-name>American Psychological Association</publisher-name>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Barlow</surname>, <given-names>H. B.</given-names></string-name></person-group> (<year>2012</year>). <chapter-title>Possible Principles Underlying the Transformations of Sensory Messages</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Rosenblith</surname>, <given-names>W. A.</given-names></string-name></person-group>, editor, <source>Sensory Communication</source>, pages <fpage>216</fpage>‚Äì<lpage>234</lpage>. <publisher-name>The MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Beck</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ma</surname>, <given-names>W. J.</given-names></string-name>, <string-name><surname>Latham</surname>, <given-names>P. E.</given-names></string-name>, and <string-name><surname>Pouget</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2007</year>). <chapter-title>Probabilistic population codes and the exponential family of distributions</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Cisek</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Drew</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Kalaska</surname>, <given-names>J. F.</given-names></string-name></person-group>, editors, <source>Progress in Brain Research, volume 165 of Computational Neuroscience: Theoretical Insights into Brain Function</source>, pages <fpage>509</fpage>‚Äì<lpage>519</lpage>. <publisher-name>Elsevier</publisher-name>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benucci</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Saleem</surname>, <given-names>A. B.</given-names></string-name>, and <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Adaptation maintains population homeostasis in primary visual cortex</article-title>. <source>Nature Neuroscience</source>, <volume>16</volume>(<issue>6</issue>):<fpage>724</fpage>‚Äì<lpage>729</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rotermund</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Pawelzik</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Optimal short-term population coding: when Fisher information fails</article-title>. <source>Neural Computation</source>, <volume>14</volume>(<issue>10</issue>):<fpage>2317</fpage>‚Äì<lpage>2351</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brunel</surname>, <given-names>N.</given-names></string-name> and <string-name><surname>Nadal</surname>, <given-names>J.-P.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Mutual Information, Fisher Information, and Population Coding</article-title>. <source>Neural Computation</source>, <volume>10</volume>(<issue>7</issue>):<fpage>1731</fpage>‚Äì<lpage>1757</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buzs√°ki</surname>, <given-names>G.</given-names></string-name> and <string-name><surname>Mizuseki</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2014</year>). <article-title>The log-dynamic brain: how skewed distributions affect network operations</article-title>. <source>Nature Reviews. Neuroscience</source>, <volume>15</volume>(<issue>4</issue>):<fpage>264</fpage>‚Äì<lpage>278</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Heeger</surname>, <given-names>D. J.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Normalization as a canonical neural computation</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>13</volume>(<issue>1</issue>):<fpage>51</fpage>‚Äì<lpage>62</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clifford</surname>, <given-names>C. W. G.</given-names></string-name>, <string-name><surname>Webster</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Stanley</surname>, <given-names>G. B.</given-names></string-name>, <string-name><surname>Stocker</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Kohn</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sharpee</surname>, <given-names>T. O.</given-names></string-name>, and <string-name><surname>Schwartz</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Visual adaptation: Neural, psychological and computational aspects</article-title>. <source>Vision Research</source>, <volume>47</volume>(<issue>25</issue>):<fpage>3125</fpage>‚Äì<lpage>3131</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desai</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Homeostatic plasticity in the CNS: Synaptic and intrinsic forms</article-title>. <source>Journal of physiology, Paris</source>, <volume>97</volume>:<fpage>391</fpage>‚Äì<lpage>402</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Diamond</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Boyd</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2016</year>). <article-title>CVXPY: A Python-embedded modeling language for convex optimization</article-title>. <source>Journal of Machine Learning Research</source>, <volume>17</volume>(<issue>83</issue>):<fpage>1</fpage>‚Äì<lpage>5</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dragoi</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Sharma</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>E. K.</given-names></string-name>, and <string-name><surname>Sur</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Dynamics of neuronal sensitivity in visual cortex and local feature discrimination</article-title>. <source>Nature Neuroscience</source>, <volume>5</volume>(<issue>9</issue>):<fpage>883</fpage>‚Äì<lpage>891</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dragoi</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Sharma</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Sur</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Adaptation-induced plasticity of orientation tuning in adult visual cortex</article-title>. <source>Neuron</source>, <volume>28</volume>(<issue>1</issue>):<fpage>287</fpage>‚Äì<lpage>298</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ganguli</surname>, <given-names>D.</given-names></string-name> and <string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Efficient Sensory Encoding and Bayesian Inference with Heterogeneous Neural Populations</article-title>. <source>Neural Computation</source>, <volume>26</volume>(<issue>10</issue>):<fpage>2103</fpage>‚Äì<lpage>2134</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gershon</surname>, <given-names>E. D.</given-names></string-name>, <string-name><surname>Wiener</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Latham</surname>, <given-names>P. E.</given-names></string-name>, and <string-name><surname>Richmond</surname>, <given-names>B. J.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Coding Strategies in Monkey V1 and Inferior Temporal Cortices</article-title>. <source>Journal of Neurophysiology</source>, <volume>79</volume>(<issue>3</issue>):<fpage>1135</fpage>‚Äì<lpage>1144</lpage>. Publisher: <publisher-name>American Physiological Society</publisher-name>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Geurts</surname>, <given-names>L. S.</given-names></string-name>, <string-name><surname>Cooke</surname>, <given-names>J. R. H.</given-names></string-name>, <string-name><surname>van Bergen</surname>, <given-names>R. S.</given-names></string-name>, and <string-name><surname>Jehee</surname>, <given-names>J. F. M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Subjective confidence reflects representation of Bayesian probability in cortex</article-title>. <source>Nature Human Behaviour</source>, <volume>6</volume>(<issue>2</issue>):<fpage>294</fpage>‚Äì<lpage>305</lpage>. <publisher-name>Nature Publishing Group</publisher-name>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ghisovan</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Nemri</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Shumikhina</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Molotchnikoff</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Long adaptation reveals mostly attractive shifts of orientation tuning in cat primary visual cortex</article-title>. <source>Neuroscience</source>, <volume>164</volume>(<issue>3</issue>):<fpage>1274</fpage>‚Äì<lpage>1283</lpage>. Publisher: <publisher-name>Elsevier</publisher-name>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goris</surname>, <given-names>R. L. T.</given-names></string-name>, <string-name><surname>Movshon</surname>, <given-names>J. A.</given-names></string-name>, and <string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Partitioning neuronal variability</article-title>. <source>Nature Neuroscience</source>, <volume>17</volume>(<issue>6</issue>):<fpage>858</fpage>‚Äì<lpage>865</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greenberg</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Houweling</surname>, <given-names>A. R.</given-names></string-name>, and <string-name><surname>Kerr</surname>, <given-names>J. N. D.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Population imaging of ongoing neuronal activity in the visual cortex of awake rats</article-title>. <source>Nature Neuroscience</source>, <volume>11</volume>(<issue>7</issue>):<fpage>749</fpage>‚Äì<lpage>751</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hengen</surname>, <given-names>K. B.</given-names></string-name>, <string-name><surname>Lambo</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Van Hooser</surname>, <given-names>S. D.</given-names></string-name>, <string-name><surname>Katz</surname>, <given-names>D. B.</given-names></string-name>, and <string-name><surname>Turrigiano</surname>, <given-names>G. G.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Firing Rate Homeostasis in Visual Cortex of Freely Behaving Rodents</article-title>. <source>Neuron</source>, <volume>80</volume>(<issue>2</issue>):<fpage>335</fpage>‚Äì<lpage>342</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kanitscheider</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Coen-Cagli</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Pouget</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Origin of information-limiting noise correlations</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>112</volume>(<issue>50</issue>):<fpage>E6973</fpage>‚Äì<lpage>6982</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keck</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G. B.</given-names></string-name>, <string-name><surname>Jacobsen</surname>, <given-names>R. I.</given-names></string-name>, <string-name><surname>Eysel</surname>, <given-names>U. T.</given-names></string-name>, <string-name><surname>Bonhoeffer</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>H√ºbener</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Synaptic Scaling and Homeostatic Plasticity in the Mouse Visual Cortex In Vivo</article-title>. <source>Neuron</source>, <volume>80</volume>(<issue>2</issue>):<fpage>327</fpage>‚Äì<lpage>334</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kersten</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Mamassian</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Yuille</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Object perception as Bayesian inference</article-title>. <source>Annual Review of Psychology</source>, <volume>55</volume>:<fpage>271</fpage>‚Äì<lpage>304</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kohn</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Visual adaptation: physiology, mechanisms, and functional benefits</article-title>. <source>Journal of Neurophysiology</source>, <volume>97</volume>(<issue>5</issue>):<fpage>3155</fpage>‚Äì<lpage>3164</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koyama</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2015</year>). <article-title>On the Spike Train Variability Characterized by Variance-to-Mean Power Relationship</article-title>. <source>Neural Computation</source>, <volume>27</volume>(<issue>7</issue>):<fpage>1530</fpage>‚Äì<lpage>1548</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laughlin</surname>, <given-names>S.</given-names></string-name></person-group> (<year>1981</year>). <article-title>A Simple Coding Procedure Enhances a Neuron‚Äôs Information Capacity</article-title>. <source>Zeitschrift f√ºr Naturforschung C</source>, <volume>36</volume>(<issue>9-10</issue>):<fpage>910</fpage>‚Äì<lpage>912</lpage>. Publisher: <publisher-name>De Gruyter</publisher-name>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lennie</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2003</year>). <article-title>The Cost of Cortical Computation</article-title>. <source>Current Biology</source>, <volume>13</volume>(<issue>6</issue>):<fpage>493</fpage>‚Äì<lpage>497</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Levy</surname>, <given-names>W.</given-names></string-name> and <string-name><surname>Baxter</surname>, <given-names>R.</given-names></string-name></person-group> (<year>1996</year>). <article-title>Energy Efficient Neural Codes</article-title>. <source>Neural computation</source>, <volume>8</volume>:<fpage>531</fpage>‚Äì<lpage>43</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Linsker</surname>, <given-names>R.</given-names></string-name></person-group> (<year>1988</year>). <article-title>Self-organization in a perceptual network</article-title>. <source>Computer</source>, <volume>21</volume>(<issue>3</issue>):<fpage>105</fpage>‚Äì<lpage>117</lpage>..</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maffei</surname>, <given-names>A.</given-names></string-name> and <string-name><surname>Turrigiano</surname>, <given-names>G. G.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Multiple Modes of Network Homeostasis in Visual Cortical Layer 2/3</article-title>. <source>The Journal of Neuroscience</source>, <volume>28</volume>(<issue>17</issue>):<fpage>4377</fpage>‚Äì<lpage>4384</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marder</surname>, <given-names>E.</given-names></string-name> and <string-name><surname>Prinz</surname>, <given-names>A. A.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Current compensation in neuronal homeostasis</article-title>. <source>Neuron</source>, <volume>37</volume>(<issue>1</issue>):<fpage>2</fpage>‚Äì<lpage>4</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>M≈Çynarski</surname>, <given-names>W.</given-names></string-name> and <string-name><surname>Tkacik</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Efficient coding theory of dynamic attentional modulation</article-title>. <source>PLOS Biology</source>, <volume>20</volume>(<issue>12</issue>):<fpage>e3001889</fpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moreno-Bote</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Beck</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kanitscheider</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Pitkow</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Latham</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Pouget</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Information-limiting correlations</article-title>. <source>Nature Neuroscience</source>, <volume>17</volume>(<issue>10</issue>):<fpage>1410</fpage>‚Äì<lpage>1417</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moshitch</surname>, <given-names>D.</given-names></string-name> and <string-name><surname>Nelken</surname>, <given-names>I.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Using Tweedie distributions for fitting spike count data</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>225</volume>:<fpage>13</fpage>‚Äì<lpage>28</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Movshon</surname>, <given-names>J. A.</given-names></string-name> and <string-name><surname>Lennie</surname>, <given-names>P.</given-names></string-name></person-group> (<year>1979</year>). <article-title>Pattern-selective adaptation in visual cortical neurones</article-title>. <source>Nature</source>, <volume>278</volume>(<issue>5707</issue>):<fpage>850</fpage>‚Äì<lpage>852</lpage>. <publisher-name>Nature Publishing Group</publisher-name>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>M√ºller</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Metha</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>Krauskopf</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Lennie</surname>, <given-names>P.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Rapid adaptation in visual cortex to the structure of images</article-title>. <source>Science</source>, <volume>285</volume>(<issue>5432</issue>):<fpage>1405</fpage>‚Äì<lpage>1408</lpage>. Place: <publisher-loc>US</publisher-loc> Publisher: <publisher-name>American Assn for the Advancement of Science</publisher-name>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nadal</surname>, <given-names>J.-P.</given-names></string-name> and <string-name><surname>Parga</surname>, <given-names>N.</given-names></string-name></person-group> (<year>1994</year>). <article-title>Nonlinear neurons in the low-noise limit: a factorial code maximizes information transfer</article-title>. <source>Network: Computation in Neural Systems</source>, <volume>5</volume>(<issue>4</issue>):<fpage>565</fpage>‚Äì<lpage>581</lpage>. Publisher: <publisher-name>Taylor &amp; Francis</publisher-name>. <pub-id pub-id-type="doi">10.1088/0954-898X_5_4_008</pub-id>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Nadal</surname>, <given-names>J.-P.</given-names></string-name> and <string-name><surname>Parga</surname>, <given-names>N.</given-names></string-name></person-group> (<year>1999</year>). <chapter-title>Sensory coding: information maximization and redundancy reduction</chapter-title>. In <source>Neuronal Information Processing</source>, volume Volume 7 of Series in Mathematical Biology and Medicine, pages <fpage>164</fpage>‚Äì<lpage>171</lpage>. <publisher-name>World scientific</publisher-name>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nelken</surname>, <given-names>I.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Stimulus-specific adaptation and deviance detection in the auditory system: Experiments and models</article-title>. <source>Biological Cybernetics</source>, <volume>108</volume>(<issue>5</issue>):<fpage>655</fpage>‚Äì<lpage>663</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parker</surname>, <given-names>P. R. L.</given-names></string-name>, <string-name><surname>Abe</surname>, <given-names>E. T. T.</given-names></string-name>, <string-name><surname>Leonard</surname>, <given-names>E. S. P.</given-names></string-name>, <string-name><surname>Martins</surname>, <given-names>D. M.</given-names></string-name>, and <string-name><surname>Niell</surname>, <given-names>C. M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Joint coding of visual input and eye/head position in V1 of freely moving mice</article-title>. <source>Neuron</source>, <volume>110</volume>(<issue>23</issue>):<fpage>3897</fpage>‚Äì<lpage>3906.e5</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rumyantsev</surname>, <given-names>O. I.</given-names></string-name>, <string-name><surname>Lecoq</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Hernandez</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Savall</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chrapkiewicz</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zeng</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ganguli</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Schnitzer</surname>, <given-names>M. J.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Fundamental bounds on the fidelity of sensory cortical coding</article-title>. <source>Nature</source>, <volume>580</volume>(<issue>7801</issue>):<fpage>100</fpage>‚Äì<lpage>105</lpage>. <publisher-name>Nature Publishing Group</publisher-name>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sahani</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Doubly distributional population codes: Simultaneous representation of uncertainty and multiplicity</article-title>. <source>Neural Computation</source>, <volume>15</volume>(<issue>10</issue>):<fpage>2255</fpage>‚Äì<lpage>2279</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sanzeni</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Palmigiano</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>T. H.</given-names></string-name>, <string-name><surname>Luo</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Nassi</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Reynolds</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Histed</surname>, <given-names>M. H.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>K. D.</given-names></string-name>, and <string-name><surname>Brunel</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Mechanisms underlying reshuffling of visual responses by optogenetic stimulation in mice and monkeys</article-title>. <source>Neuron</source>, <volume>111</volume>(<issue>24</issue>):<fpage>4102</fpage>‚Äì<lpage>4115.e9</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwartz</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Hsu</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Space and time in visual context</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>8</volume>(<issue>7</issue>):<fpage>522</fpage>‚Äì<lpage>535</lpage>. <publisher-name>Nature Publishing Group</publisher-name>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shadlen</surname>, <given-names>M. N.</given-names></string-name> and <string-name><surname>Newsome</surname>, <given-names>W. T.</given-names></string-name></person-group> (<year>1998</year>). <article-title>The Variable Discharge of Cortical Neurons: Implications for Connectivity, Computation, and Information Coding</article-title>. <source>The Journal of Neuroscience</source>, <volume>18</volume>(<issue>10</issue>):<fpage>3870</fpage>‚Äì<lpage>3896</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name> and <string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Natural Image Statistics and Neural Representation</article-title>. <source>Annual Review of Neuroscience</source>, <volume>24</volume>(<issue>1</issue>):<fpage>1193</fpage>‚Äì<lpage>1216</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Slomowitz</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Styr</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Vertkin</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Milshtein-Parush</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Nelken</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Slutsky</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Slutsky</surname>, <given-names>I.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Interplay between population firing stability and single neuron dynamics in hippocampal networks</article-title>. <source>eLife</source>, <volume>4</volume>:<elocation-id>e04378</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.04378</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Snow</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Coen-Cagli</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Schwartz</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Specificity and timescales of cortical adaptation as inferences about natural movie statistics</article-title>. <source>Journal of Vision</source>, <volume>16</volume>(<issue>13</issue>):<fpage>1</fpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Solomon</surname>, <given-names>S. G.</given-names></string-name> and <string-name><surname>Kohn</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Moving sensory adaptation beyond suppressive effects in single neurons</article-title>. <source>Current biology: CB</source>, <volume>24</volume>(<issue>20</issue>):<fpage>R1012</fpage>‚Äì<lpage>1022</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stocker</surname>, <given-names>A. A.</given-names></string-name> and <string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Noise characteristics and prior expectations in human visual speed perception</article-title>. <source>Nature Neuroscience</source>, <volume>9</volume>(<issue>4</issue>):<fpage>578</fpage>‚Äì<lpage>585</lpage>. <publisher-name>Nature Publishing Group</publisher-name>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stringer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name></person-group> (<year>2019</year>). <article-title>High-dimensional geometry of population responses in visual cortex</article-title>. <source>Nature</source>, <volume>571</volume>(<issue>7765</issue>):<fpage>361</fpage>‚Äì<lpage>365</lpage>. <publisher-name>Nature Publishing Group</publisher-name>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Szuts</surname>, <given-names>T. A.</given-names></string-name>, <string-name><surname>Fadeyev</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Kachiguine</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sher</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Grivich</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>Agroch√£o</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hottowy</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Dabrowski</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Lubenov</surname>, <given-names>E. V.</given-names></string-name>, <string-name><surname>Siapas</surname>, <given-names>A. G.</given-names></string-name>, <string-name><surname>Uchida</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Litke</surname>, <given-names>A. M.</given-names></string-name>, and <string-name><surname>Meister</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2011</year>). <article-title>A wireless multi-channel neural amplifier for freely moving animals</article-title>. <source>Nature Neuroscience</source>, <volume>14</volume>(<issue>2</issue>):<fpage>263</fpage>‚Äì<lpage>269</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Taaseh</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Yaron</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Nelken</surname>, <given-names>I.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Stimulus-specific adaptation and deviance detection in the rat auditory cortex</article-title>. <source>PLOS One</source>, <volume>6</volume>(<issue>8</issue>):<fpage>e23369</fpage>‚Äì<lpage>None</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Torrado Pacheco</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Tilden</surname>, <given-names>E. I.</given-names></string-name>, <string-name><surname>Grutzner</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Lane</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hengen</surname>, <given-names>K. B.</given-names></string-name>, <string-name><surname>Gjorgjieva</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Turrigiano</surname>, <given-names>G. G.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Rapid and active stabilization of visual cortical firing rates across light‚Äìdark transitions</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>116</volume>(<issue>36</issue>):<fpage>18068</fpage>‚Äì<lpage>18077</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tring</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Dipoppa</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Ringach</surname>, <given-names>D. L.</given-names></string-name></person-group> (<year>2023</year>). <article-title>A power law describes the magnitude of adaptation in neural populations of primary visual cortex</article-title>. <source>Nature Communications</source>, <volume>14</volume>(<issue>1</issue>):<fpage>8366</fpage>. Publisher: <publisher-name>Nature Publishing Group</publisher-name>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turrigiano</surname>, <given-names>G. G.</given-names></string-name></person-group> (<year>2008</year>). <article-title>The self-tuning neuron: synaptic scaling of excitatory synapses</article-title>. <source>Cell</source>, <volume>135</volume>(<issue>3</issue>):<fpage>422</fpage>‚Äì<lpage>435</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turrigiano</surname>, <given-names>G. G.</given-names></string-name>, <string-name><surname>Leslie</surname>, <given-names>K. R.</given-names></string-name>, <string-name><surname>Desai</surname>, <given-names>N. S.</given-names></string-name>, <string-name><surname>Rutherford</surname>, <given-names>L. C.</given-names></string-name>, and <string-name><surname>Nelson</surname>, <given-names>S. B.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Activity-dependent scaling of quantal amplitude in neocortical neurons</article-title>. <source>Nature</source>, <volume>391</volume>(<issue>6670</issue>):<fpage>892</fpage>‚Äì<lpage>896</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turrigiano</surname>, <given-names>G. G.</given-names></string-name> and <string-name><surname>Nelson</surname>, <given-names>S. B.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Homeostatic plasticity in the developing nervous system</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>5</volume>(<issue>2</issue>):<fpage>97</fpage>‚Äì<lpage>107</lpage>. <publisher-name>Nature Publishing Group</publisher-name>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Bergen</surname>, <given-names>R. S.</given-names></string-name>, <string-name><surname>Ji Ma</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Pratte</surname>, <given-names>M. S.</given-names></string-name>, and <string-name><surname>Jehee</surname>, <given-names>J. F. M.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Sensory uncertainty decoded from visual cortex predicts behavior</article-title>. <source>Nature Neuroscience</source>, <volume>18</volume>(<issue>12</issue>):<fpage>1728</fpage>‚Äì<lpage>1730</lpage>. <publisher-name>Nature Publishing Group</publisher-name>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Vertes</surname>, <given-names>E.</given-names></string-name> and <string-name><surname>Sahani</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Flexible and accurate inference and learning for deep generative models</article-title>. <source>arXiv</source> <pub-id pub-id-type="arxiv">1805.11051</pub-id>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name></person-group> (<year>1995</year>). <source>Foundations of vision. Foundations of vision</source>. <publisher-name>Sinauer Associates</publisher-name>, <publisher-loc>Sunderland, MA, US</publisher-loc>. Pages: xvi, 476.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Stocker</surname>, <given-names>A. A.</given-names></string-name>, and <string-name><surname>Lee</surname>, <given-names>D. D.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Efficient neural codes that minimize lp reconstruction error</article-title>. <source>Neural Computation</source>, <volume>28</volume>(<issue>12</issue>):<fpage>2656</fpage>‚Äì<lpage>2686</lpage>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname>, <given-names>X.-X.</given-names></string-name> and <string-name><surname>Stocker</surname>, <given-names>A. A.</given-names></string-name></person-group> (<year>2015</year>). <article-title>A Bayesian observer model constrained by efficient coding can explain ‚Äòanti-Bayesian‚Äô percepts</article-title>. <source>Nature Neuroscience</source>, <volume>18</volume>(<issue>10</issue>):<fpage>1509</fpage>‚Äì<lpage>1517</lpage>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname>, <given-names>X.-X.</given-names></string-name> and <string-name><surname>Stocker</surname>, <given-names>A. A.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Mutual Information, Fisher Information, and Efficient Coding</article-title>. <source>Neural Computation</source>, <volume>28</volume>(<issue>2</issue>):<fpage>305</fpage>‚Äì<lpage>326</lpage>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Westrick</surname>, <given-names>Z. M.</given-names></string-name>, <string-name><surname>Heeger</surname>, <given-names>D. J.</given-names></string-name>, and <string-name><surname>Landy</surname>, <given-names>M. S.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Pattern Adaptation and Normalization Reweighting</article-title>. <source>Journal of Neuroscience</source>, <volume>36</volume>(<issue>38</issue>):<fpage>9805</fpage>‚Äì<lpage>9816</lpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yarrow</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Challis</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Seri√®s</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Fisher and Shannon Information in Finite Neural Populations</article-title>. <source>Neural Computation</source>, <volume>24</volume>(<issue>7</issue>):<fpage>1740</fpage>‚Äì<lpage>1780</lpage>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yerxa</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Kee</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>DeWeese</surname>, <given-names>M. R.</given-names></string-name>, and <string-name><surname>Cooper</surname>, <given-names>E. A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Efficient sensory coding of multidimensional stimuli</article-title>. <source>PLOS Computational Biology</source>, <volume>16</volume>(<issue>9</issue>):<fpage>1</fpage>‚Äì<lpage>25</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104746.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Sharpee</surname>
<given-names>Tatyana O</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Salk Institute for Biological Studies</institution>
</institution-wrap>
<city>La Jolla</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This work derives a <bold>valuable</bold> general theory unifying theories of efficient information transmission in the brain with population homeostasis. The general theory provides an explanation for firing rate homeostasis at the level of neural clusters with firing rate heterogeneity within clusters. Applying this theory to the primary visual cortex, the authors present <bold>solid</bold> evidence that accounts for stimulus-specific and neuron-specific adaptation. Reviewers have provided additional suggestions for improving the readability of the manuscript, as well as discussing previous results on adapting coding as well as those aspects of experimental data that are not fully explained by the present theory.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104746.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This work derives a general theory of optimal gain modulation in neural populations. It demonstrates that population homeostasis is a consequence of optimal modulation for information maximization with noisy neurons. The developed theory is then applied to the distributed distributional code (DDC) model of the primary visual cortex to demonstrate that homeostatic DDCs can account for stimulus specific adaptation.</p>
<p>Strengths:</p>
<p>The theory of gain modulation proposed in the paper is rigorous and the analysis is thorough. It does address the issue in an interesting, general setting. The proposed approach separates the question of which bits of sensory information are transmitted (as defined by a specific computation and tuning curve shapes) and how well are they transmitted (as defined by the tuning curve gain optimized to combat noise). This separation permits the application of the developed theory to different neural systems.</p>
<p>Weaknesses:</p>
<p>The manuscript effectively consits of two parts: a general theory of optimal gain modulation and a DDC model of the visual cortex. From my perspective it is not entirely clear which components of the developed theory and the model it is applied to are essential to explain the experimental phenomena in the visual cortex (Fig. 12). This &quot;separation&quot; into two parts makes this work, in my view, somewhat diffused.</p>
<p>Overall, I think this is an interesting contribution and I assess it positively. It has the potential of deepening our understanding of efficient neural representations beyond sensory periphery.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104746.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Using the theory of efficient coding, the authors study how neural gains may be adjusted to optimize information transmission by noisy neural populations while minimizing metabolic cost, under the assumption that other aspects of neural activity (i.e. tuning) are determined by the computation performed by the network.</p>
<p>The manuscript first presents mathematical results for the general case where the computational goals of the neural population are not specified (the computation is implicit in the assumed tuning curves). It then develops the theory for a specific probabilistic coding scheme. The general theory provides an explanation for firing rate homeostasis at the level of neural clusters with firing rate heterogeneity within clusters. The specific application further explains stimulus-specific adaptation in visual cortex.</p>
<p>The mathematical derivations, simulations and application to visual cortex data are solid as far as I can tell.</p>
<p>This remains a highly technical manuscript although the authors have improved the clarity of presentation of the general theory (which is the bulk of the work presented) and better motivated/explained modeling assumptions and choices. In the second part, the manuscript focuses on a specific code (homeostatic DDC) showing that this can be implemented by divisive normalization and can explain stimulus-specific adaptation.</p>
<p>Strengths:</p>
<p>The problem of efficient coding is a long-standing and important one. This manuscript contributes to that field by proposing a theory of efficient coding through gain adjustments, independent of the computational goals of the system. The main assumption, and insight, is that computational goals and efficiency can be in some sense factorized: tuning curve shapes are determined by the computational goal, whereas gains can be adjusted to optimize transmission of information.</p>
<p>One key result is a normative explanation for firing rate homeostasis at the level of neural clusters (groups of neurons that perform a similar computation) with firing rate heterogeneity within each cluster. Both phenomena are widely observed, and reconciling them under one theory is important.</p>
<p>The mathematical derivations are thorough. Although the model of neural activity is artificial, the authors make sure to include many aspects of cortical physiology, while also keeping the models quite general.</p>
<p>Section 2.5 derives the conditions in which homeostasis would be near-optimal in cortex, which appear to be consistent with many empirical observations in V1. This indicates that homeostasis in V1 might be indeed a close to optimal solution to code efficiently in the face of noise.</p>
<p>The application to the data of Benucci et al 2013 is the first to offer a normative explanation of stimulus-specific adaptation in V1.</p>
<p>The novelty and significance of the work are presented clearly in the newly extended Introduction and Discussion.</p>
<p>Weaknesses:</p>
<p>The manuscript remains hard to read. The general theory occupies most of the manuscript, as needed to convey it fully. But as a result the second part on homeostatic DDC and adaptation is somewhat underdeveloped and risks having less visibility than it might deserve.</p>
<p>The paper Benucci et al 2013 shows that homeostasis holds for some stimulus distributions, but not others i.e. when the 'adapter' is present too often. This manuscript, like the Benucci paper, discards those datasets. But from a theoretical standpoint, it seems important to consider why that would be the case, and if it can be predicted by the theory proposed here. The authors now acknowledge this limitation in the Discussion.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104746.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Young</surname>
<given-names>Edward James</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0884-3195</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Ahmadian</surname>
<given-names>Yashar</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5942-0697</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors‚Äô response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1(Public Review):</bold></p>
<p><bold>Major comments:</bold></p>
<p>(1) Interpretation of key results and relationship between different parts of the manuscript. The manuscript begins with an information-transmission ansatz which is described as ‚Äùindependent of the computational goal‚Äù (e.g. p. 17). While information theory indeed is not concerned with what quantity is being encoded (e.g. whether it is sensory periphery or hippocampus), the goal of the studied system is to *transmit* the largest amount of bits about the input in the presence of noise. In my view, this does not make the proposed framework ‚Äùindependent of the computational goal‚Äù. Furthermore, the derived theory is then applied to a DDC model which proposes a very specific solution to inference problems. The relationship between information transmission and inference is deep and nuanced. Because the writing is very dense, it is quite hard to understand how the information transmission framework developed in the first part applies to the inference problem. How does the neural coding diagram in Figure 3 map onto the inference diagram in Figure 10? How does the problem of information transmission under constraints from the first part of the manuscript become an inference problem with DDCs? I am certain that authors have good answers to these questions - but they should be explained much better.</p>
</disp-quote>
<p>We are very thankful to the reviewer for highlighting the potential confusion surrounding these issues, in particular the relationship between the two halves of the paper ‚Äì which was previously exacerbated by the length of the paper. We have now added further explanations at different points within the manuscript to better disentangle these issues and clarify our key assumptions. We have also significantly cut the length of the paper by moving more technical discussions to the Methods or Appendices. We will summarise these changes here and also clarify the rationale for our approach and point out potential disagreements with the reviewer.</p>
<p>Key to our approach is that we indeed do not assume the entire goal of the studied neural system (whether part of the sensory system or not) is to transmit the largest amount of information about the stimulus input (in the presence of noise). In fact, general computations, including the inference of latent causes of inputs, often require filtering out or ignoring some information in the sensory input. It is thus not plausible that tuning curves in general (i.e. in an arbitrary part of the nervous system) are optimised solely with regards to the criterion of information transmission. Accordingly we do not assume they are entirely optimised for that purpose. However, we do make a key assumption or hypothesis (which like any hypothesis might turn out to be partly or entirely wrong): that (1) a minimal feature of the tuning curve (its scale or gain) is entirely free to be optimised for the aim of information transmission (or more precisely the goal of combating the detrimental effect of neural noise on coding fidelity), (2) other aspects of the population tuning curve structure (i.e. the shape of individual tuning curves and their arrangement across the population) are determined by (other) computational goals beyond efficient coding. (Conceptually, this is akin to the modularization between indispensible error correction and general computations in a digital computer, and the need for the former to be performed in a manner that is agnostic as to the computations performed.) We have added two paragraphs in the manuscript which present the above rationale and our key hypothesis or assumption. The first of these was added to the (second paragraph of the) Introduction section, and the second is a new paragraph following Eq. 1 (which is about the gain-shape decomposition of the tuning curves, and the optimisation of the former based on efficient coding) of Results.</p>
<p>Our paper can be divided into two parts. In the first part, we develop a general, computationally agnostic (in the above sense, just as in the digital computer example), efficient coding theory. In the second part, we apply that theory to a specific form of computation, namely the DDC framework for Bayesian inference. The latter theory now determines the tuning curve shapes. When combined with the results of the first part (which dictate the tuning curve scale or gain according to efficient coding theory), this ‚Äúhomeostatic DDC‚Äù model makes full predictions for the tuning curves (i.e., both scale and shape) and how they should adapt to stimulus statistics.</p>
<p>So to summarise, it is not the case that the problem of information transmission (or rather mitigating the effect noise on coding fidelity under metabolic constraints), dealt with in the first part, has become a problem of Bayesian inference. But rather, the dictates of efficient coding for optimal gains for coding fidelity (under constraints) have been applied to and combined with a computational theory of inference.</p>
<p>We have added new expository text before and after Eq. 17 in Sec. 2.7 (at the beginning of the second part of the paper on homeostatic DDCs) to again make the connection with the first part and the rationale for its combination with the original DDC framework more clear.</p>
<p>With the changes outlined above, we believe and hope the connection between the two parts (which we agree with the reviewer, was indeed rather obscure previously) has been adequately clarified.</p>
<disp-quote content-type="editor-comment">
<p>(2) Clarity of writing for an interdisciplinary audience. I do not believe that in its current form, the manuscript is accessible to a broader, interdisciplinary audience such as eLife readers. The writing is very dense and technical, which I believe unnecessarily obscures the key results of this study.</p>
</disp-quote>
<p>We thank the reviewer for this comment. We have taken several steps to improve the accessibility of this work for an interdisciplinary audience. Firstly, several sections containing dense, mathematical writing have now been moved into appendices or the Methods section, out from the main text; in their place we have made efforts to convey the core of the results, and to providing intuitions, without going into unnecessary technical detail. Secondly, we have added additional figures to help illustrate key concepts or assumptions (see Fig. 1B clarifying the conceptual approach to efficient coding and homeostatic adaptation, and Fig. 8A describing the clustered population). Lastly, we have made sure to refer back to the names of symbols more often, so as to make the analysis easier to follow for a reader with an experimental background.</p>
<disp-quote content-type="editor-comment">
<p>(3) Positioning within the context of the field and relationship to prior work. While the proposed theory is interesting and timely, the manuscript omits multiple closely related results which in my view should be discussed in relationship to the current work. In particular, a number of recent studies propose normative criteria for gain modulation in populations: ‚Ä¢ Duong, L., Simoncelli, E., Chklovskii, D. and Lipshutz, D., 2024. Adaptive whitening with fast gain modulation and slow synaptic plasticity. Advances in Neural Information Processing Systems</p>
<p>Tring, E., Dipoppa, M. and Ringach, D.L., 2023. A power law describes the magnitude of adaptation in neural populations of primary visual cortex. Nature Communications, 14(1), p.8366.</p>
<p>Ml ynarski, W. and TkaÀácik, G., 2022. Efficient coding theory of dynamic attentional modulation. PLoS Biology</p>
<p>Haimerl, C., Ruff, D.A., Cohen, M.R., Savin, C. and Simoncelli, E.P., 2023. Targeted V1 co-modulation supports task-adaptive sensory decisions. Nature Communications ‚Ä¢ The Ganguli and Simoncelli framework has been extended to a multivariate case and analyzed for a generalized class of error measures:</p>
<p>Yerxa, T.E., Kee, E., DeWeese, M.R. and Cooper, E.A., 2020. Efficient sensory coding of multidimensional stimuli. PLoS Computational Biology</p>
<p>Wang, Z., Stocker, A.A. and Lee, D.D., 2016. Efficient neural codes that minimize LP reconstruction error. Neural Computation, 28(12),</p>
</disp-quote>
<p>We thank the reviewer again for bringing these works to our attention. For each, we explain whether we chose to include them in our Discussion section, and why.</p>
<p>(1) Duong et al. (2024): We decided not to discuss this manuscript, as our assessment is that it is very relevant to our work. That study starts with the assumption that the goal of the sensory system under study is to whiten the signal covariance matrix, which is not the assumption we start with. A mechanistic ingredient (but not the only one) in their approach is gain modulation. However, in their case it is the gains of computationally auxiliary inhibitory neurons that is modulated and not (as in our case) the gain the (excitatory) coding neurons (i.e. those which encode information about the stimulus and whose response covariance is whitened). These key distinction make the connection with our work quite loose and we did not discuss this work.</p>
<p>(2) Tring et al. (2023): We have added a discussion of the results of this paper and its relationship to the results of our work and that of Benucci et al. This appears in the 7th paragraph of the Discussion. This study is indeed highly relevant to our paper, as it essentially replicates the Benucci et al. experiment, this time in awake mice (rather than anesthetised cats). However, in contrast to the resul‚Äòts of Benucci et al., Tring et al. do not find firing rate homeostasis in mouse V1. A second, remarkable finding of Tring et al. is that adaptation mainly changes the scale of the population response vector, and only minimally affects its direction. While Tring et al. do not portray it as such, this behaviour amounts to pure stimulus-specific adaptation without the neuron-specific factor found in the Benucci et al. results (see Eq. 24 of our manuscript). As we discuss in our manuscript, when our homeostatic DDC model is based on an ideal-observer generative model, it also displays pure stimulus-specific adaptation with no neuronal factor. Our final model for Benucci‚Äôs data did contain a neural factor, because we used a non-ideal observer DDC (in particular, we assumed a smoother prior distribution over orientations compared to the distribution used in the experiment - which has a very sharp peak ‚Äì as it is more natural given the inductive biases we expect in the brain). The resultant neural factor suppresses the tuning curves tuned to the adaptor stimulus. Interestingly, when gain adaptation is incomplete, and happens to a weaker degree compared to what is necessary for firing rate homeostasis, an additional neural factor emerges that is greater than one for neurons tuned to the adaptor stimulus. These two multiplicative neural factors can approximately cancel each other; such a theory would thus predict both deviation from homeostasis and approximately pure stimulus-specific adaptation. We plan to explore this possibility in future work.</p>
<p>(3) Ml ynarski and TkaÀácik (2022): We are now citing and discussing this work in the Discussion (penultimate paragraph), in the context of a possible future direction, namely extending our framework to cover the dynamics of adaptation (via a dynamic efficient gain modulation and dynamic inference). We have noted there that Mlynarski have used such a framework (which while similar has key technical differences with our approach) based on a task-dependent efficient coding objective to model top-down attentional modulation. By contrast, we have studied bottom-up and task-independent adaptation, and it would be interesting to extend our framework and develop a model to make predictions for the temporal dynamics of such adaptation.</p>
<p>(4) Haimerl et al. (2023): We have elected not to include this work within our discussion either, as we do not believe it is sufficiently relevant to our work to warrant inclusion. Although this paper also considers gain modulation of neural activity, the setting and the aims of the theoretical work and the empirical phenomena it is applied to are very different from our case in various ways. Most importantly, this paper is not offering a normative account of gain modulation; rather, gain modulation is used as a mechanism for enabling fast adaptive readouts of task relevant information.</p>
<p>(5) Yerxa et al. (2020): We have now included a discussion of this paper in our Discussion section. Note that, even though this study generalises the Ganguli and Simoncelli framework to higher diemsnions, just like that paper it still places strict requirements (which are arguably even more stringent in higher dimensions) on the form of the tuning curves in the population, viz. that there exists a differentiable transform of the stimulus space which renders these unimodal curves completely homogeneous (i.e., of the same shape, and placed regularly and with uniform density).</p>
<p>(6) Wang et al. (2016): We have included this paper in our discussion as well. As above, this paper does not consider general tuning curves, and places the same constraint on their shape and arrangement as in Ganguli and Simoncelli paper.</p>
<disp-quote content-type="editor-comment">
<p>More detailed comments and feedback:</p>
<p>(1) I believe that this work offers the possibility to address an important question about novelty responses in the cortex (e.g. Homann et al, 2021 PNAS). Are they encoding novelty per-se, or are they inefficient responses of a not-yet-adapted population? Perhaps it‚Äôs worth speculating about.</p>
</disp-quote>
<p>We are not sure why the relatively large responses to ‚Äúnovel‚Äù or odd-ball stimuli should be considered inefficient or unadapted: in the context in which those stimuli are infrequent odd-balls (and thus novel or surprising when occurring), efficient coding theory would indeed typically predict a large response compared to the (relatively suppressed) responses to frequently occurring stimuli. Of course, if the statistics change and the odd-ball stimulus now becomes frequent, adaptation should occur and would be expected to suppress responses to this stimulus. As to the question of whether (large) responses to infrequent stimuli can or should be characterised as novelty responses: this is partly an interpretational or semantic issue ‚Äì unless it is grounded in knowledge of how downstream populations use this type of coding in V1, which could then provide a basis for solidly linking them to detection of novelty per se. In short, our theory, could be applied to Homann et al.‚Äôs data, but we consider that beyond the scope of the current paper.</p>
<disp-quote content-type="editor-comment">
<p>(2) Clustering in populations - typically in efficient coding studies, tuning curve distributions are a consequence of input statistics, constraints, and optimality criteria. Here the authors introduce randomly perturbed curves for each cluster - how to interpret that in light of the efficient coding theory? This links to a more general aspect of this work - it does not specify how to find optimal tuning curves, just how to modulate them (already addressed in the discussion).</p>
</disp-quote>
<p>We begin by addressing the reviewer‚Äôs more general concern regarding the fact that our theory does not address the problem of finding optimal tuning curves, only that of modulating them optimally. As we expound within the updated version of the paper (see the newly expanded 3rd paragraph in Sec. 2.1 and the expanded 2nd paragraph in Introduction), it is not plausible that the sole function of sensory systems, and neural circuits more generally, is the transmission of information. There are many other computational tasks which must be performed by the system, such as the inference of the latent causes of sensory inputs. For many such tasks, it is not even desirable to have complete transmission of information about the external stimulus, since a substantial portion of that information is not important for the task at hand, and must be discarded. For example, such discarding of information is the basis of invariant representations that occur, e.g., in higher visual areas. So we recognise that tuning curve shapes are in general dictated and shaped by computational goals beyond transmission of information or error correction. As such, we have remained agnostic as to the computational goals of neural systems and therefore the shape of the tuning curve. We have made the assumption and adopted the postulate that those computational goals determine the shape of the tuning curves, leaving the gains to be adjuted freely for the purpose of mitigating the effect noise on coding fidelity (this is similar to how error correction is done in computers independendently of the computations performed). by assuming that those computational goals are captured adequately by the shape of tuning curves, this leaves us free to optimise the gains of those curves for purely information theoretic objectives. Finally, we note that the case where the tuning curve shapes are additionally optimised for information transmission is a special case of our more general approach. For further discussion, see the updated version of our introduction.</p>
<p>We now turn to our choice to model clusters using random perturbations. This is, of course, a toy model for clustering tuning curves within a population. With this toy model we are attempting to capture the important aspects of tuning curve clusters within the population while not over-complicating the simulations. Within any neural population, there will be tuning curves that are similar; however, such curves will inevitably be heterogeneous, as opposed to completely identical. Thus, when we cluster together similar curves there will be an ‚Äúaverage‚Äù cluster tuning curve (found by, e.g., normalising all individual curves and taking the average), which all other tuning curves within the cluster are deviations from. The random perturbations we apply are our attempt to capture these deviations. However, note that the perturbations are not fully random, but instead have an ‚Äúeffective dimensionality‚Äù which we vary over. By giving the perturbations an effective dimensionality, we aim to capture the fact that deviations from the average cluster tuning curve may not be fully random, and may display some structure.</p>
<disp-quote content-type="editor-comment">
<p>(3) Figure 8 - where do Hz come from as physical units? As I understand there are no physical units in simulations.</p>
</disp-quote>
<p>We have clarified this within the figure caption. The within-cluster optimisation problem requires maximising a quadratic program subject to a constraint on the total mean spike count of the cluster. The objective for the quadratic program is however mathematically homogeneous. So we can scale the variables and parameters in a consistent to be in units of Hz ‚Äì i.e., turn them into mean firing rates, instead of mean spike counts, with an assumption on the length of the coding time interval. We fix this cluster firing rate to be k √ó 5 Hz, so that the average single-neuron firing rate is 5 Hz (based on empirical estimates ‚Äì see our Sec. 2.5). This agrees with our choice of ¬µ in our simulations (i.e., ¬µ = 10) if we assume a coding interval of 0.1 seconds.</p>
<disp-quote content-type="editor-comment">
<p>(4) Inference with DDCs in changing environments. To perform efficient inference in a dynamically changing environment (as considered here), an ideal observer needs some form of posterior-prior updating. Where does that enter here?</p>
</disp-quote>
<p>A shortcoming of our theory, in its current form, is that it applies only to the system in ‚Äústeady-state‚Äù, without specifying the dynamics of how adaptation temporlly evolves (we assume the enrivonment has periods of relative stability that are of relatively long duration compared to the dynamical timescales of adaptation, and consider the properties of the well-adapted steady state population). Thus our efficient coding theory (which predicts homeostatic adaptation under the outlined conditions) is silent on the time-course over which homeostasis occurs. Likewise, the DDC theory (in its original formulation in Vertes &amp; Sahani) is silent on dynamic updating of posteriors and considers only static inference with a fixed internal model. We have now discuss a new future directoin in the Discussion (where we cite the work of Mlynarski and Tkacik) to point out that our theory can in principle be extended (based on dynamic inference and efficient coding) to account for the dynamics of attention, but this is beyond the scope of the current work.</p>
<disp-quote content-type="editor-comment">
<p>(5) Page 6 - ‚ÄùWe did this in such a way that, for all , the correlation matrices, (), were derived from covariance matrices with a 1/n power-law eigenspectrum (i.e., the ranked eigenvalues of the covariance matrix fall off inversely with their rank), in line with the findings of Stringer et al. (2019) in the primary visual cortex.‚Äù This is a very specific assumption, taken from a study of a specific brain region - how does it relate to the generality of the approach?</p>
</disp-quote>
<p>Our efficient coding framework has been formulated without relying on any specific assumptions about the form of the (signal or noise) correlation matrices in cortex. The homeostatic solution to this efficient coding problem, however, emerges under certain conditions. But, as we demonstrate in our discussion of the analytic solutions to our efficient coding objective and the conditions necessary for the validity of the homeostatic solution, we expect homeostasis to arise whenever the signal geometry is sufficiently high-dimensional (among other conditions). By this we mean that the fall-off of the eigenvalues of the signal correlation matrix must be sufficiently slow. Thus, a fall-off in the eigenvalue spectrum slower than 1/n would favor homeostasis even more than our results. If the fall off was faster, then whether or not (and to what degree) firing rate homeostasis becomes suboptimal depends on factors such as the fastness of the fall-off and also the size of the population. Thus (1) rate homeostasis does not require the specific 1/n spectrum, but that spectrum is consistent with the conditions for optimality of rate homeostasis, (2) in our simulations we had to make a specific choice, and relying on empirical observations in V1 was of course a well-justified choice (moreover, as far as we are aware, there have been no other studies that have characterised the spectrum of the signal covariance matrix in response to natural stimuli, based on large population recordings).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Strengths:</p>
<p>The problem of efficient coding is a long-standing and important one. This manuscript contributes to that field by proposing a theory of efficient coding through gain adjustments, independent of the computational goals of the system. The main result is a normative explanation for firing rate homeostasis at the level of neural clusters (groups of neurons that perform a similar computation) with firing rate heterogeneity within each cluster. Both phenomena are widely observed, and reconciling them under one theory is important.</p>
<p>The mathematical derivations are thorough as far as I can tell. Although the model of neural activity is artificial, the authors make sure to include many aspects of cortical physiology, while also keeping the models quite general.</p>
<p>Section 2.5 derives the conditions in which homeostasis would be near-optimal in the cortex, which appear to be consistent with many empirical observations in V1. This indicates that homeostasis in V1 might be indeed close to the optimal solution to code efficiently in the face of noise.</p>
<p>The application to the data of Benucci et al 2013 is the first to offer a normative explanation of stimulus-specific and neuron-specific adaptation in V1.</p>
</disp-quote>
<p>We thank the reviewer for these assessments.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>The novelty and significance of the work are not presented clearly. The relation to other theoretical work, particularly Ganguli and Simoncelli and other efficient coding theories, is explained in the Discussion but perhaps would be better placed in the Introduction, to motivate some of the many choices of the mathematical models used here.</p>
</disp-quote>
<p>We thank the reviewer for this comment; we have updated our introduction to make clearer the relationship between this work and previous works within efficient coding theory. Please see the expanded 2nd paragraph of Introduction which gives a short account of previous efficient coding theories and now situates our work and differentiates it more clearly from past work.</p>
<disp-quote content-type="editor-comment">
<p>The manuscript is very hard to read as is, it almost feels like this could be two different papers. The first half seems like a standalone document, detailing the general theory with interesting results on homeostasis and optimal coding. The second half, from Section 2.7 on, presents a series of specific applications that appear somewhat disconnected, are not very clearly motivated nor pursued in-depth, and require ad-hoc assumptions.</p>
</disp-quote>
<p>We thank the reviewer for this suggestion. The reviewer is right to note that our paper contains both the exposition of a general efficient coding theory framework in addition to applications of that framework. Following your advice we have implemented the following changes. (1) significantly shortened or entirely moved some of the less central results in the second half of Results, to the Methods or appendices (this includes the entire former section 2.7 and significant shortening of the section on implementation of Bayes ratio coding by divisive normalisation). (2) We have added a new figure (Fig 1B) and two long pieces of text to the (2nd paragraph of) Introduction, after Eq. (1), and in Sec. 2.7 (introducing homeostatic DDCs) to more clearly explain and clarify the assumptions underlying our efficient coding theory, and its connection with the second half of the Results (i.e. application to DDC theory of Bayesian inference), and better motivate why we consider the homeostatic DDC.</p>
<disp-quote content-type="editor-comment">
<p>For instance, it is unclear if the main significant finding is the role of homeostasis in the general theory or the demonstration that homeostatic DDC with Bayes Ratio coding captures V1 adaptation phenomena. It would be helpful to clarify if this is being proposed as a new/better computational model of V1 compared to other existing models.</p>
</disp-quote>
<p>We see the central contribution of our work as not just that homeostasis arises as a result of an efficient coding objective, but also that this homeostasis is sufficient to explain V1 adaptation phenomena - in particular, stimulus specific adaptation (SSA) - when paired with an existing theory of neural representation, the DDC (itself applied to orientation coding in V1). Homeostatic adaptation alone does not explain SSA; nor do DDCs. However, when the two are combined they provide an explanation for SSA. This finding is significant, as it unifies two forms of adaptation (SSA and homeostatic adaptation) whose relationship was not previously appreciated. Our field does not currently have a standard model of V1, and we do not claim to have provided one either; rather, different models have captured different phenomena in V1, and we have done so for homeostatic SSA in V1.</p>
<disp-quote content-type="editor-comment">
<p>Early on in the manuscript (Section 2.1), the theory is presented as general in terms of the stimulus dimensionality and brain area, but then it is only demonstrated for orientation coding in V1.</p>
</disp-quote>
<p>The efficient coding theory developed in Section 2 is indeed general throughout, we make no assumptions regarding the shape of the tuning curves or the dimensionality of the stimulus. Further, our demonstrations of the efficient coding theory through numerical simulations - make assumptions only about the form of the signal and noise covariance matrices. When we later turn our attention away from the general case, our choice to focus on orientation coding in V1 was motivated by empirical results demonstrating a co-occurrence of neural homeostasis and stimulus specific adaptation in V1.</p>
<disp-quote content-type="editor-comment">
<p>The manuscript relies on a specific response noise model, with arbitrary tuning curves. Using a population model with arbitrary tuning curves and noise covariance matrix, as the basis for a study of coding optimality, is problematic because not all combinations of tuning curves and covariances are achievable by neural circuits (e.g. <ext-link ext-link-type="uri" xlink:href="https://pubmed.ncbi.nlm.nih.gov/27145916/">https://pubmed.ncbi.nlm.nih.gov/27145916/</ext-link> )</p>
</disp-quote>
<p>First, to clarify, our theory allows for complete generality of neural tuning curve shapes, and assumes a broad family of noise models (which, while not completely arbitrary, includes cases of biological relevance and/or models commonly used in the theoretical literature). Within this class of noise covariance models, we have shown numerical results for different values for different parameters of the noise covariance model, but more importantly, have analytically outlined the general properties and requirements on noise strength and structure (and its relationship to tuning curves and signal structure) under which homeostatic adaptation would be optimal. Regarding the point that not all combinations of tuning curves and noise covariances occur in biology or are achievable by neural circuits: (1) If we are guessing correctly the specific point of the reviewer‚Äôs reference to the review paper by Kohn et al. 2016, we have in fact prominently discussed the case of information limiting noise which corresponds to a specific relationship between signal structure (as determined by tuning curves) and noise structure (as specified by the noise covariance matrix). Our family of noise models include that biologically relevant case and we have indeed paid it particular attention in our simulations and discussions (see discussion of Fig. 7 in Sec. 2.3, and that of aligned noise in Sec. 2.5). (2) As for the more general or abstract point that not all combinations of noise covariance and tuning curve structures are achievable by neural circuits, we can make the following comments. First, in lieu of a full theoretical or empirical understanding of the achievable combinations (which does not exist), we have outlined conditions for homeostatic adaptations under a broad class of noise models and arbitrary tuning curves. If some combinations within this class are not realised in biology, that does not invalidate the theoretical results, as the latter have been derived under more general conditions, which nevertheless include combinations that do occur in biology and are achievable by neural circuits (which, as pointed out, include the important case of aligned noise and signal structure ‚Äì as reviewed in Kohn et al.‚Äì to which we have paid particular attention).</p>
<disp-quote content-type="editor-comment">
<p>The paper Benucci et al 2013 shows that homeostasis holds for some stimulus distributions, but not others i.e. when the ‚Äôadapter‚Äô is present too often. This manuscript, like the Benucci paper, discards those datasets. But from a theoretical standpoint, it seems important to consider why that would be the case, and if it can be predicted by the theory proposed here.</p>
</disp-quote>
<p>The theory we provide predicts that, under certain (specified) conditions, we ought to see deviation from exact homeostatic results; indeed, we provide a first order approximation to the optimal gains in this case which quantifies such deviations when they are small. However, unfortunately the form of this deviation depends on a precise choice of stimulus statistics (e.g. the signal correlation matrix, the noise correlation matrix averaged over all stimulus space, and other stimulus statistics), in contrasts to the universality of the homeostatic solution, when it is a valid approximation. In our model of Benucci et al.‚Äôs experiment, we restrict to a simple one-dimensional stimulus space (corresponding to orientated gratings), without specifying neural responses to all stimuli; as such, we are not immediately able to make predictions about whether the homeostatic failure can be predicted using the specific form of deviation from homeostasis. However, we acknowledge that this is a weakness of our analysis, and that a more complete investigation would address this question. For reasons of space, we elected not to pursue this further. We have added a paragraph to our Discussion (8th paragraph) explaining this.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer#1 (Recommendations for the authors):</bold></p>
<p>(1) To make the article more accessible I would suggest the following:</p>
<p>(a) Include a few more illustrations or diagrams that demonstrate key concepts: adaptationof an entire population, clustering within a population, different sources of noise, inference with homeostatic DDCs, etc.</p>
</disp-quote>
<p>We thank the reviewer for this suggestion - we have added an additional figure in (Figure 8, Panel A) to explain the concept of clustering within a population. We also added a new panel to Figure 1 (Figure 1B) which we hope will clarify the conceptual postulate underlying our efficient coding framework and its link to the second half of the paper.</p>
<disp-quote content-type="editor-comment">
<p>(b) Within the text refer to names of quantities much more often, rather than relying onlyon mathematical symbols (e.g. w,r,‚Ñ¶, etc).</p>
</disp-quote>
<p>We thank the reviewer for the suggestion; we have updated the text accordingly and believe this has improved the clarity of the exposition.</p>
<disp-quote content-type="editor-comment">
<p>(2) It is hard to distill which components of the considered theory are crucial to reproducing the experimental observations in Figure 12. Is it the homeostatic modulation, efficient coding, DDCs, or any combination of those or all of them necessary to reproduce the experiment? I believe this could be explained much better, also with an audience of experimentalists in mind.</p>
</disp-quote>
<p>We have updated the text to provide additional clarity on this matter (see the pointers to these changes and additions in the revised manuscript, given above in response to your first comment). In particular, reproducing the experimental results requires combining DDCs with homeostatic modulation ‚Äì with the latter a consequence of our efficient coding theory, and not an independent ingredient or assumption.</p>
<disp-quote content-type="editor-comment">
<p>(3) It would be good to comment on how sensitive the results are to the assumptions made, parameter values, etc. For example: do conclusions depend on statistics of neural responses in simulated environments? Do they generalize for different values of the constraint ¬µ? This could be addressed in the discussion / supplementary material.</p>
</disp-quote>
<p>This issue is already discussed extensively within the text - see Sec. 2.4, Analytical insight on the optimality of homeostasis, and Sec. 2.5, Conditions for the validity of the homeostatic solution to hold in cortex. In these sections, we outline that - provided a certain parameter combination is small - we expect the homeostatic result to hold. Accordingly, we anticipate that our numerical results will generalise to any settings in which that parameter combination remains small.</p>
<disp-quote content-type="editor-comment">
<p>(4) How many neurons/units were used for simulations?</p>
</disp-quote>
<p>We apologies for omitting this detail; we used 10,000 units for our simulations. We have edited both the main text and the methods section to reflect this.</p>
<disp-quote content-type="editor-comment">
<p>(5) Typos etc: a) Figure 5 caption - the order of panels B and C is switched. b) Figure 6A - I suggest adding a colorbar.</p>
</disp-quote>
<p>Thank you. We have relabelled the panels B and C in the appropriate figures so that the ordering in the figure caption is correct. We feel that a colourbar in figure 6A would be unnecessary, since we are only trying to convey the concept of uniform correlations, rather than any particular value for the correlations; as such we have elected not to add a colourbar. We have, however, added a more explicit explanation of this cartoon matrix in the figure caption, by referring to the colors of diagonal vs off-diagonal elements.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer#2 (Recommendations for the authors):</bold></p>
<p>The text on page 10, with the perturbation analysis, could be moved to a supplement, leaving here only the intuition.</p>
</disp-quote>
<p>We thank the reviewer for this suggestion; we have moved much of the argument into the appendix so as to not distract the reader with unnecessary technical details.</p>
<disp-quote content-type="editor-comment">
<p>Text before eq. 12 ‚Äú...in cluster a maximize the objective...‚Äù should be ‚Äòminimize‚Äô?</p>
</disp-quote>
<p>The cluster objective as written is indeed maximised, as stated in the text. Note that, in the revised manuscript, this argument has been moved to an appendix to reduce the density of mathematics in the main text.</p>
<disp-quote content-type="editor-comment">
<p>Top of page 25 ‚ÄúS<sub>0</sub> and S<sub>0</sub>‚Äù should be ‚ÄúS<sub>0</sub> and S<sub>1</sub>‚Äù?</p>
</disp-quote>
<p>Thank you, we have corrected the manuscript accordingly.</p>
</body>
</sub-article>
</article>