<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">106757</article-id>
<article-id pub-id-type="doi">10.7554/eLife.106757</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106757.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Dyadic Interaction Platform: A novel tool to study transparent social interactions</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Isbaner</surname>
<given-names>Sebastian</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A2">2</xref>
<email xlink:href="mailto:sebastian.isbaner@uni-goettingen.de">sebastian.isbaner@uni-goettingen.de</email>
</contrib>
<contrib contrib-type="author">
<name>
    <surname>Báez-Mendoza</surname>
<given-names>Raymundo</given-names>
</name>
<xref ref-type="aff" rid="A2">2</xref>
<xref ref-type="aff" rid="A3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bothe</surname>
<given-names>Ricarda</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Eiteljoerge</surname>
<given-names>Sarah</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Fischer</surname>
<given-names>Anna</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Gail</surname>
<given-names>Alexander</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A2">2</xref>
<xref ref-type="aff" rid="A4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Gläscher</surname>
<given-names>Jan</given-names>
</name>
<xref ref-type="aff" rid="A5">5</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lüschen</surname>
<given-names>Hannah</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Möller</surname>
<given-names>Sebastian</given-names>
</name>
<xref ref-type="aff" rid="A2">2</xref>
<xref ref-type="aff" rid="A6">6</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Penke</surname>
<given-names>Lars</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Priesemann</surname>
<given-names>Viola</given-names>
</name>
<xref ref-type="aff" rid="A7">7</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ruß</surname>
<given-names>Johannes</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Schacht</surname>
<given-names>Anne</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Schneider</surname>
<given-names>Felix</given-names>
</name>
<xref ref-type="aff" rid="A2">2</xref>
<xref ref-type="aff" rid="A8">8</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Shahidi</surname>
<given-names>Neda</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A2">2</xref>
<xref ref-type="aff" rid="A8">8</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Treue</surname>
<given-names>Stefan</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A2">2</xref>
<xref ref-type="aff" rid="A8">8</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wibral</surname>
<given-names>Michael</given-names>
</name>
<xref ref-type="aff" rid="A9">9</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ziereis</surname>
<given-names>Annika</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Fischer</surname>
<given-names>Julia</given-names>
</name>
<xref ref-type="aff" rid="A2">2</xref>
<xref ref-type="aff" rid="A10">10</xref>
<xref ref-type="aff" rid="A11">11</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<name>
<surname>Kagan</surname>
<given-names>Igor</given-names>
</name>
<xref ref-type="aff" rid="A2">2</xref>
<xref ref-type="aff" rid="A6">6</xref>
<email xlink:href="mailto:ikagan@dpz.eu">ikagan@dpz.eu</email>
<xref ref-type="fn" rid="FN1">†</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<name>
<surname>Mani</surname>
<given-names>Nivedita</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A2">2</xref>
<email xlink:href="mailto:nmani@gwdg.de">nmani@gwdg.de</email>
<xref ref-type="fn" rid="FN1">†</xref>
</contrib>
<aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01y9bpm73</institution-id><institution>Georg-Elias-Müller Institute for Psychology, University of Göttingen</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff>
<aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ehdmg18</institution-id><institution>Leibniz ScienceCampus Primate Cognition</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff>
<aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f99v835</institution-id><institution>Social Neurobiology, German Primate Center</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff>
<aff id="A4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f99v835</institution-id><institution>Sensorimotor Group, German Primate Center</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff>
<aff id="A5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01zgy1s35</institution-id><institution>Institute for System Neuroscience, University Medical Center Hamburg-Eppendorf</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff>
<aff id="A6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f99v835</institution-id><institution>Decision and Awareness Group, German Primate Center</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff>
<aff id="A7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0087djs12</institution-id><institution>Max Planck Institute for Dynamics and Self-Organization</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff>
<aff id="A8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f99v835</institution-id><institution>Cognitive Neuroscience Laboratory, German Primate Center</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff>
<aff id="A9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01y9bpm73</institution-id><institution>Göttingen Campus Institute for Dynamics of Biological Networks, University of Göttingen</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff>
<aff id="A10"><label>10</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01y9bpm73</institution-id><institution>Department of Primate Cognition, University of Göttingen</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff>
<aff id="A11"><label>11</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f99v835</institution-id><institution>Cognitive Ethology Laboratory, German Primate Center</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Peelle</surname>
<given-names>Jonathan Erik</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01yc7t268</institution-id><institution>Northeastern University</institution>
</institution-wrap>
<city>Boston</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Frank</surname>
<given-names>Michael J</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8451-0523</contrib-id><role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution>
</institution-wrap>
<city>Providence</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="FN1" fn-type="equal"><label>†</label><p>Shared Senior Authors</p></fn>
<fn fn-type="coi-statement" id="FN2"><p>Competing interests: We declare no conflict of interests.</p></fn>
</author-notes>
<pub-date pub-type="epub">
<day>24</day>
<month>04</month>
<year>2025</year>
</pub-date>
<pub-date date-type="original-publication" iso-8601-date="2025-07-18">
<day>18</day>
<month>07</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP106757</elocation-id>
<history><date date-type="sent-for-review" iso-8601-date="2025-04-21">
<day>21</day>
<month>04</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-04-23">
<day>23</day>
<month>04</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.31234/osf.io/2rckn_v1"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Isbaner et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Isbaner et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-106757-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Studies of social cognition examine how organisms process and act on the presence, intentions, actions, and behavioural outcomes of others in social contexts. Many real-life social interactions unfold during direct face-to-face contact and rely on immediate, time-continuous feedback about mutual behaviour and changes in the shared environment. Yet, essential aspects of these naturalistic conditions are often lacking in experimental laboratory settings for direct dyadic interactions, i.e., interactions between two people. Here, we describe a novel experimental setting, the Dyadic Interaction Platform (DIP), designed to investigate the behavioural and neural mechanisms of real-time social interactions. Based on a transparent, touch-sensitive, bi-directional visual display, this design allows two participants to observe visual stimuli and each other simultaneously, allowing face-to-face interaction in a shared vertical workspace. Different implementations of the DIP facilitate interactions between two human adults, adults and children, two children, nonhuman primates and in mixed nonhuman-human dyads. The platforms allow for diverse manipulations of interactive contexts and synchronized recordings of both participants’ behavioural, physiological, and neural measures. This approach enables us to integrate economic game theory with time-continuous sensorimotor and perceptual decision-making, social signalling and learning, in an intuitive and socially salient setting that affords precise control over stimuli, task timing, and behavioural responses. We demonstrate the applications and advantages of DIPs in several classes of transparent interactions, ranging from value-based strategic coordination games and dyadic foraging to social cue integration, information seeking, and social learning.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="s1" sec-type="intro">
<title>Introduction</title>
<p>Humans and other primates are social beasts. Our identity, beliefs, thoughts, actions, and speech are grounded in the context of our social interactions with others. Decades of research on social cognition in carefully controlled laboratory studies have provided ample evidence that unidirectional, non-interactive social cues, e.g., still faces, gestures, emotional facial expressions, and spoken words, can shape our perception and subsequent behaviour. Natural social interactions are, however, rarely controlled and quintessentially non-static and reciprocal. Even the most basic social exchange between two individuals provides a kaleidoscope of rapidly changing cues regarding the social partners’ facial expressions, gazes, actions, and words. Understanding the extent to which we are capable of attending to and incorporating such dynamic, multidimensional cues in our everyday interactions requires examining behaviour in similarly rich situations. Therefore, experimental settings are needed that strike the desired balance between natural interactions and controlled stimulus presentation while monitoring continuous behavioural and neural data of interacting agents. Here, we present a novel experimental platform that allows two individuals - either two adult humans, a human adult and child, two nonhuman primates, or a nonhuman primate and a human confederate - to interact face-to-face with one another while observing the same stimuli and manipulating a common, shared workspace. During these interactions, a wide range of behavioural and neural indices of their interactions and shared environment can be collected, allowing analysis of how social cues and actions are perceived, processed, and how they influence dynamic social interactions.</p>
<p>In what follows, we first describe the state-of-the-art on how the presentation of social information in static and dynamic contexts influences cognitive processing and decision-making. Next, we describe prior setups that have sought to combine naturalistic social settings with experimental control to examine the integration of social information in social interactions before describing the platform and exemplary use cases in further detail.</p>
<sec id="s1-1">
<title>Social cognition</title>
<p>As in other domains of cognition, social information processing relies on attending to, perceiving, and learning from relevant cues in the individual’s environment. Many earlier studies examining social information processing have employed static unidirectional paradigms where a participant is presented with social stimuli, such as words or still images of faces, as the participant’s behavioural, physiological, or neural responses to these stimuli are recorded. These paradigms have yielded considerable insight into the factors that shape the processing and learning of social information in such static designs. We will not describe this literature in detail here, but direct the reader to excellent reviews on these topics (e.g., <xref ref-type="bibr" rid="c14">Birmingham and Kingstone, 2009</xref>; <xref ref-type="bibr" rid="c42">Deen et al., 2023</xref>).</p>
<p>Moving on from such static designs, a critical step in the social cognition literature was to use more realistic dynamic stimuli, such as movies, animated avatars, or real individuals, albeit in a unidirectional fashion, i.e., focusing on assessing perception and behaviour in a single subject. Such studies suggest that embedding cognition in our lived social experience can impact how participants respond across various situations. For instance, the face recognition literature abounds with studies suggesting that individuals prefer looking at people’s faces, especially their eyes, relative to objects in situations where participants are presented with static images of objects and faces (see <xref ref-type="bibr" rid="c14">Birmingham and Kingstone (2009)</xref> for a review). More recent studies have, however, allowed participants to view or interact with real people as stimuli, e.g., sit across from a stranger (<xref ref-type="bibr" rid="c82">Laidlaw et al., 2011</xref>), walk down a University campus (<xref ref-type="bibr" rid="c51">Foulsham et al., 2011</xref>), follow an individual’s gaze (<xref ref-type="bibr" rid="c59">Gallup et al., 2012</xref>), or make transparent judgments about people, i.e., these judgments would be visible to the person being judged (<xref ref-type="bibr" rid="c60">Gobel et al., 2015</xref>). In stark contrast to the face perception literature, these studies found that participants moderate when they look into people’s faces and eyes in real social interactions and pay more attention to nonsocial information. Similarly, the developmental literature suggests that infants prefer dynamic faces as opposed to static patterns (<xref ref-type="bibr" rid="c37">Courage et al., 2006</xref>) and robustly follow an adult’s gaze when presented with videos of adults directing their gaze toward an object (<xref ref-type="bibr" rid="c124">Senju and Csibra, 2008</xref>; <xref ref-type="bibr" rid="c15">Bohn et al., 2024</xref>). In contrast, recent findings examining infants’ behaviour in naturalistic social interactions with their caregivers with head-mounted eyetrackers suggest that children’s looking toward faces may be impacted by the motor costs associated with fixating on someone’s face (<xref ref-type="bibr" rid="c52">Franchak et al., 2018</xref>) and that infants do not always reliably follow their caregiver’s gaze in such interactions (<xref ref-type="bibr" rid="c87">Madhavan et al., 2025</xref>; <xref ref-type="bibr" rid="c150">Yu and Smith, 2013</xref>). Thus, embedding cognitive science research in transparent social interactions reveals that individual responses in realistic social situations may differ dramatically from findings from static and unidirectional laboratory studies.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1</label>
<caption><title>Timescales of dyadic interactions.</title>
<p>Classical economic game theory mostly focuses on trial-by-trial decisions: each agent learns about the mutual outcomes of dyadic choices at the end of each round, or trial, e.g., both select option “square” in the trial T3 (<italic>row 2</italic>). Across trials, decision strategies, reflecting recent history of interactions and predictions of future choices of the other agent, can emerge and transition gradually or abruptly, e.g. from S1 to S2 (<italic>row 3</italic>). Our approach aims at expanding the classical games, enabling “transparency” in dyadic decision-making paradigms so that each agent can monitor the other agent’s ongoing social cues and actions continuously in real-time (<italic>row 1</italic>). Instantaneously coordinated actions may give rise to new strategies, e.g. leader-follower dynamics that emerge spontaneously, based on time- and space-continuous actions. Of course, decision-making in a social context requires agents to integrate longer-term experiences and predict consequences beyond situational strategies, for example, adapting to partners with different levels of competence or cooperative dispositions (<italic>row 4</italic>). Immediate partner visibility in naturalistic face-to-face interactions allows for efficient partner-specific learning and behavioural adjustments.</p></caption>
<graphic xlink:href="2rcknv1_fig1.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Dynamic social interactions may occur spontaneously without a concrete goal or a joint task. However, understanding the full spectrum of social mechanisms also requires examining goal-directed contexts in which interactions are purposeful for solving joint tasks or attaining rewards. In particular, game-theoretical approaches have been extensively used to study social <italic>value-based decisions</italic> during bidirectional <italic>goal-directed</italic> interactions. Such “games” typically provide information about the discrete choices of both players at the end of each game round or trial, or impose a predetermined order of actions, e.g., turn-taking (sequential games, or actor-observer paradigms). Thus, players can base their decisions on the history of their and their partner’s choices and outcomes and their predictions about the future (<xref ref-type="fig" rid="fig1">Figure 1</xref>). While such paradigms capture an important aspect of social interactions, especially at longer timescales, many real-world interactions typically unfold continuously in real-time within a direct face-to-face sensorimotor context (<xref ref-type="bibr" rid="c43">van Doorn et al., 2014</xref>; <xref ref-type="bibr" rid="c146">Yoo et al., 2021a</xref>). To study the effects of such within-trial short-term dynamics, we have recently introduced probabilistic action visibility (“transparency”) and found that giving participants dynamic and continuous access to a partner’s choices changes evolutionary successful strategies in iterative non-zero-sum games (<xref ref-type="bibr" rid="c142">Unakafov et al., 2019</xref>, <xref ref-type="bibr" rid="c141">2020</xref>). Along the same lines, recent work in humans and nonhuman primates has focused on continuous dynamic strategies in purely competitive dyadic settings (<xref ref-type="bibr" rid="c72">Hosokawa and Watanabe, 2012</xref>; <xref ref-type="bibr" rid="c92">McDonald et al., 2019</xref>) and non-zero-sum games (<xref ref-type="bibr" rid="c23">Brosnan et al., 2012</xref>, <xref ref-type="bibr" rid="c21">2017</xref>; <xref ref-type="bibr" rid="c47">Ferrari-Toniolo et al., 2019</xref>; <xref ref-type="bibr" rid="c68">Hawkins and Goldstone, 2016</xref>; <xref ref-type="bibr" rid="c103">Ong et al., 2021</xref>; <xref ref-type="bibr" rid="c112">Pisauro et al., 2022</xref>). These studies revealed how individuals maximize their rewards by dynamically and continuously integrating the actions of their social partners into strategic choices.</p>
<p>Beyond economic strategic games, the social context also strongly influences basic perceptual processes. Studies examining perceptual judgements (i.e., judgment about ambiguous sensory stimuli) in cooperative contexts typically have individuals complete serial decision tasks, first individually and then jointly with their interaction partners (<xref ref-type="bibr" rid="c6">Bahrami et al., 2010</xref>; <xref ref-type="bibr" rid="c9">Bang et al., 2017</xref>; <xref ref-type="bibr" rid="c9">Bang and Frith, 2017</xref>; <xref ref-type="bibr" rid="c12">Baumgart et al., 2019</xref>). This work suggests that, under certain conditions, joint performance is better than the best individual performance and that partners exhibit mutual alignment of their perceptual judgments or associated confidence. Importantly, the manner and the timing of social exchange has a strong impact on individual responses: for instance, information about others’ choices that is presented serially (e.g., at the end ofa trial or within a structured turn-taking) influences decisions differently compared to when the information exchange is more dynamic or continuous (<xref ref-type="bibr" rid="c109">Pescetelli and Yeung, 2020</xref>, <xref ref-type="bibr" rid="c110">2022</xref>).</p>
<p>This brief overview makes clear that static and dynamic designs - both unidirectional and bidirectional - tap into different levels of cognitive processing that are key to understanding social behaviour. Equally clear, however, is our takeaway that our understanding of how individuals perceive, react, attend to, and learn from social interactions is likely to change based on the extent to which experimental designs consider the continuous dynamics between social partners in an interaction (<xref ref-type="bibr" rid="c63">Hadley et al., 2022</xref>). There is, therefore, a need for experimental platforms that allow us to incorporate and study such dynamics readily. Below, we briefly describe the experimental approaches that have, thus far, enabled studying primate social cognition at different levels of interactivity and transparency (see also <xref ref-type="bibr" rid="c66">Hari et al. (2015)</xref> and <xref ref-type="bibr" rid="c46">Fan et al. (2021)</xref> for reviews on levels of naturalism and interactivity in social neuroscience research).</p>
</sec>
<sec id="s2">
<title>Prior dyadic setups</title>
<p>Experimental dyadic settings range from naturalistic free-flowing interactions “in a room”, which are inherently difficult to control and analyse, to highly specialized configurations with shared or separate visual displays and workspaces for each agent, providing varying access to multifaceted socio-affective signals (e.g., eye gaze, facial expressions, or gestures). Dyadic experiments with human participants are often conducted in separate booths or rooms, using two individual computer monitors to present a task and exchange information between participants (<xref ref-type="bibr" rid="c119">Rollwage et al., 2020</xref>; <xref ref-type="bibr" rid="c130">Steixner-Kumar et al., 2022</xref>; <xref ref-type="bibr" rid="c123">Schneider et al., 2024</xref>). In such tasks, researchers typically manipulate the information presented to individuals or substitute a real human partner with a computer agent or confederate. Furthermore, this arrangement allows recording the brain activity of one or both participants using EEG or in an MEG or MRI scanner (<xref ref-type="bibr" rid="c105">Park et al., 2019</xref>; <xref ref-type="bibr" rid="c4">van Baar et al., 2019</xref>; <xref ref-type="bibr" rid="c84">Levy et al., 2021</xref>; <xref ref-type="bibr" rid="c111">Philippe et al., 2024</xref>). However, under such conditions, the immediacy of the real social context and availability of socio-affective cues from the social partner are limited. Human studies also utilised the side-by-side positioning of the players in front of two separate screens in the same room (<xref ref-type="bibr" rid="c24">Buidze et al., 2024</xref>, <xref ref-type="bibr" rid="c25">2025</xref>). Participants cannot observe each other directly, but see the outcome of the other player’s responses on their respective screen. While the immediacy of the task-related interaction on the screen is preserved, the information about the other player is strictly controlled by the experimenter and does not include facial or gestural responses.</p>
    <p>In more naturalistic settings that facilitated the seminal discovery of mirror neurons, Rizzollatti and colleagues recorded single neurons from the frontal cortex of macaque monkeys as they either observed the experimenters reaching for different objects or reached for the same objects (<xref ref-type="bibr" rid="c107">di Pellegrino et al., 1992</xref>). These findings spurred the development of primate social neurophysiology and advanced our understanding of social interactions at the neuronal level (<xref ref-type="bibr" rid="c31">Chang, 2017</xref>; <xref ref-type="bibr" rid="c75">Isoda et al., 2018</xref>; <xref ref-type="bibr" rid="c100">Nougaret et al., 2019</xref>; <xref ref-type="bibr" rid="c29">Báez-Mendoza et al., 2021</xref>). For instance, in pioneering studies by Fujii and colleagues (<xref ref-type="bibr" rid="c54">Fujii et al., 2007</xref>, <xref ref-type="bibr" rid="c55">2008</xref>), macaque monkeys sat at a table at a 90° angle or opposite to each other and reached for food morsels. Although the timing of the reaching behaviour could not be carefully controlled, such studies allowed researchers, for the first time, to test the neuronal representations of self and other’s reaching space in the parietal cortex.</p>
<p>Since then, many variations of dyadic setups have been implemented. A common variant used both in nonhuman primate conspecific dyads and in mixed human confederate-monkey dyads is based on a side-by-side or at 90° sitting configuration with visual access to separate (touch)screens and workspaces, or a shared screen with individual response devices (<xref ref-type="bibr" rid="c33">Chang et al., 2013</xref>, <xref ref-type="bibr" rid="c34">2015</xref>; <xref ref-type="bibr" rid="c67">Haroush and Williams, 2015</xref>; <xref ref-type="bibr" rid="c45">Falcone et al., 2017</xref>; <xref ref-type="bibr" rid="c21">Brosnan et al., 2017</xref>, <xref ref-type="bibr" rid="c23">2012</xref>; <xref ref-type="bibr" rid="c35">Cirillo et al., 2018</xref>; <xref ref-type="bibr" rid="c47">Ferrari-Toniolo et al., 2019</xref>; <xref ref-type="bibr" rid="c40">Dal Monte et al., 2020</xref>; <xref ref-type="bibr" rid="c50">Formaux et al., 2023</xref>; <xref ref-type="bibr" rid="c93">Meisner et al., 2024</xref>). Such arrangements facilitate social proximity, but do not allow direct face-to-face interactions and social gaze monitoring. Furthermore, they often require perspective switching from one’s own actions and their consequences to the partner’s actions. Thus, many studies employing this approach use sequential turn-taking between one actor who determines the outcome in the current trial and one passive observer/reward recipient at the time. Not only do such tasks enforce a less natural serial interaction, they also require participants to realize when they are the actor and when they are not, which can be a challenging process for nonhuman primates or younger children.</p>
    <p>As an alternative, many studies have presented macaque subjects with a horizontal computer display or a horizontal touchscreen placed between the two partners, allowing them to observe each other’s actions and ensuring controlled presentation of visual stimuli (<xref ref-type="bibr" rid="c149">Yoshida et al., 2011</xref>; <xref ref-type="bibr" rid="c3">Azzi et al., 2012</xref>; <xref ref-type="bibr" rid="c27">Báez-Mendoza et al., 2013</xref>; <xref ref-type="bibr" rid="c28">Báez-Mendoza and Schultz, 2016</xref>; <xref ref-type="bibr" rid="c99">Noritake et al., 2018</xref>; <xref ref-type="bibr" rid="c62">Grabenhorst et al., 2019</xref>; <xref ref-type="bibr" rid="c103">Ong et al., 2021</xref>). In such setups, the gaze and the focus of attention have to be shifted from the screen below to the partner and back, while the action space is separated into own and other’s parts. Along the same lines, several studies had human participants face each other while interacting with objects on a table between partners (<xref ref-type="bibr" rid="c65">Hamilton and Holler, 2023</xref>; <xref ref-type="bibr" rid="c87">Madhavan et al., 2025</xref>; <xref ref-type="bibr" rid="c150">Yu and Smith, 2013</xref>). Such tasks, however, similarly necessitate considerable vertical gaze and attention shifts between the object stimuli and the partner. Furthermore, such interactions allow limited experimental control over the timing and presentation of the stimuli.</p>
<p>As a compromise between side-based and vis-à-vis approaches, in some studies two players faced a shared computer display arranged at an angle, so that both players could observe the stimuli and to a certain extent each other’s actions and faces (<xref ref-type="bibr" rid="c72">Hosokawa and Watanabe, 2012</xref>). Human studies, in turn, have used back-to-back computer screens between opposing partners (<xref ref-type="bibr" rid="c76">Jahng et al., 2017</xref>; <xref ref-type="bibr" rid="c151">Yu et al., 2020</xref>). To enable naturalistic gaze interactions, some recent human and macaque studies also utilized accessible conditions in which the dividing computer screen or a shutter can be lowered (<xref ref-type="bibr" rid="c41">Dal Monte et al., 2022</xref>), arranged laterally (<xref ref-type="bibr" rid="c133">Tang et al., 2016</xref>), or rendered transparent (<xref ref-type="bibr" rid="c115">Pryluk et al., 2020</xref>; <xref ref-type="bibr" rid="c70">Hirsch et al., 2023</xref>), exposing the face of the partner. Likewise, some across-the-table experiments with face-to-face or opaque divider conditions relied on manipulating separate response devices and auditory feedback (<xref ref-type="bibr" rid="c13">Behrens et al., 2020</xref>). But in all these approaches, task-related stimuli are shifted away from the face and the body signals, and workspaces are divided.</p>
<p>Finally, three studies have implemented a transparent face-to-face arrangement. The innovative setup of Ballesta and Duhamel (<xref ref-type="bibr" rid="c7">Ballesta and Duhamel, 2015</xref>) relied upon semitransparent mirrors to project visual stimuli onto a touchscreen plane, and required painstaking alignment and head-fixed subjects (personal communication, J-R Duhamel). Vaziri-Pashkam and colleagues used an ingenious but limited solution consisting of a Plexiglass divider with pieces of foam attached to it as targets, and electromagnetic hand tracking device - with no possibility to display and act on visual stimuli. Recently, Ninomiya and colleagues (<xref ref-type="bibr" rid="c98">Ninomiya et al., 2021</xref>) implemented face-to-face interaction using large illuminated buttons on each side, visible to both agents but only operable by one agent, and no other visual stimuli. These approaches enable monitoring the partner’s face and actions without the need for large gaze shifts, but limit stimulus and response options.</p>
<p>Taken together, the approaches summarized above suggest that most research on cognition in dynamic social interactions faces a trade-off. Participants often have limited access to the continuously changing behaviour of their social partners, requiring them to split their attention between the workspace and their social partners. Paradigms that provide more access, such as studying free-flowing face-to-face interactions between two partners, have limitations in terms of experimental control, stimulus presentation and behavioural recordings. Therefore, there is a need for an approach that allows controlled stimulus presentation and experimental manipulation while still affording direct access to the social partner’s face and actions.</p>
</sec>
<sec id="s3">
<title>Dyadic Interaction Platform</title>
<p>In this paper, we present a dyadic interaction platform (DIP) that unifies many advantages of prior setups and allows studying real-time social interactions between two partners. The key feature of the DIP - contrasting with the commonly used configurations reviewed above - is a transparent bidirectional screen that presents stimuli and allows participants to see each other, while also serving as a shared workspace where both partners can interact. Thus, the experimental setup enables the presentation of tightly controlled visual (in combination with auditory) stimuli, which participants can simultaneously manipulate, trigger, or selectively attend to. The availability of both, the social interaction partner and the stimuli in the same line of sight ensures that participants can readily attend to both sources of information. A range of different recording devices can be flexibly integrated into the platform to capture rich multi-dimensional behavioural, physiological, and neural data, allowing one to evaluate task-driven and spontaneously occurring contingencies in social interaction. Taken together, the platform uniquely embeds tight experimental control in naturalistic face-to-face social interactions, thereby allowing researchers to examine social information processing in bidirectional, dynamic social interactions.</p>
<p>Two technical conference reports have introduced a concept similar to the projection-based dyadic interaction platform variant we are presenting here (<xref ref-type="bibr" rid="c74">Ishii and Kobayashi, 1992</xref>; <xref ref-type="bibr" rid="c69">Heo et al., 2014</xref>). We advance this approach by demonstrating flexible implementations adaptable to diverse experimental designs and target groups, and presenting empirical validation data from four distinct classes of interaction paradigms.</p>
</sec>
</sec>
<sec id="s4" sec-type="materials|methods">
<title>Materials and methods</title>
<p>We begin by summarizing the main platform components and evaluating the advantages and disadvantages of different options. We then describe several DIP variants, and in the Results section showcase compelling use cases where DIPs have been applied to address specific research questions across four example paradigm classes.</p>
<sec id="s4-1">
<title>Platform components</title>
<p>Here, we describe the variety of visual displays, interactive components, and recording devices that can be flexibly integrated and synchronized within the DIP.</p>
<sec id="s4-1-1">
<title>Visual displays</title>
<p>The mutual visibility of the interaction partners is a hallmark of the DIP. Thus, the display, whilst acting as the shared visual workspace, should maintain high transparency, allow stimuli to be displayed precisely in time and space, and offer similar flexibility and ease of use as a computer monitor. The desired optical properties of the display are a low haze, high transmittance, and low reflectivity. A hazy display would blur everything behind the display and low transmittance lowers contrast and brightness. Both, therefore, reduce the salience of the interaction partner’s face and body. Reflections create ghost images of the stimuli and the participants which can be distracting. Furthermore, the optical properties of transparent panels should stay constant over the visible spectrum so that the display does not appear tinted.</p>
<p>We found that using low-iron, anti-reflective glass (sometimes referred to as museum glass) or attaching anti-reflective film to the glass panel can help reduce reflections (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). Importantly, anti-reflective coating or film should be placed on all pane-to-air interfaces, if there are gaps between the layers (e.g., between the visual display and the protective glass). However, participants may still see a faint reflected self-image of themselves on the screen, especially if the two sides of the DIP are not equally illuminated. Thus, maintaining equal illumination on both sides of the DIP is crucial. In our experience, multiple independently controllable diffuse light sources on the two sides - instead of directed spot-lights - allow for suitable ‘titration’ of partner visibility and self-reflection. Thus, by manipulating the degree of illumination on the two sides of the DIP, studies can either ensure that the visual experience is similar across the two participants or create “asymmetric dyads”, where one participant, typically the real subject, can see the other partner and their facial and posture cues well, but not vice versa.</p>
<p>Crucially, a transparent screen cannot display black colour. Thus, black areas of an image are perceived as transparent, i.e., the participant and backdrop on the other side act as the background for the stimuli. If the scene behind such a display is dark, the display will appear similar to a conventional non-transparent monitor. If the scene is illuminated, the presented stimuli will be superimposed on the scene. Thus, both sides of the DIP need to be illuminated appropriately to ensure transparency and access to the partner’s face and actions. While this, in turn, can result in some degree of self-reflection, the distracting effect of self-reflection can be alleviated by offsetting the two partners in depth (distance from the display) and vertical/horizontal directions. We note that, in our experience, the self-reflection typically becomes less distracting once partners engage in interaction. Since the scene behind the display is typically illuminated, the backdrop should be as homogeneous as possible. We found that a dark backdrop (curtains or a painted wall) allows high contrast for the stimuli and displays darker colours with higher fidelity and avoids reflections on the display that bright walls and objects create.</p>
<p>The contrast of presented stimuli is also influenced by the brightness of the display. Bright stimuli will occlude the scene behind. Therefore, the placement of stimuli on the screen should ensure that they minimize occlusions of the partner’s face. For example, stimuli can be placed on the sides or in a ring-like arrangement that leaves enough space in the centre to see the partner’s face. Furthermore, stimuli that work well on standard monitors might not work well on a transparent screen given that, as noted above, black areas are displayed as transparent. Dark parts of an image might still be discernible when they are enclosed by bright outlines while taking care of contrast costs incurred by the background. Given these limitations, we found that bright, simple stimuli such as geometric shapes or cartoons in bold colours were preferable to photo-realistic images.</p>
<p>Finally, studies may need to include transparent and non-transparent settings within a single experiment. A non-transparent setting is one where the two partners independently interact with the stimuli without seeing each other or each other’s responses. A simple solution we employed was to install blinds that block half of the screen on each side, thereby completely occluding the partner, while allowing participants to use the remaining half of the display on their side. An alternative solution would be to use electric switchable glass, which allows opacity to be adjusted continuously and less arduously. Next, we describe the different displays we have used for various instantiations of the DIP and their advantages and disadvantages.</p>
<sec id="s4-1-1-1">
<title>OLED displays</title>
<p>Organic LED (OLED) screens are available in 4 sub-pixel configurations that allow light to pass between the two sides of the screen. This results in a display that is transparent when the stimulus is dark and increasingly opaque as the brightness of the stimulus is increased. Thus, bright objects presented on a dark background are clearly visible from both sides of the screen, while the dark background allows good visibility of the scene on the other side of the screen. One consequence of the 4 sub-pixel structure is that transparency is around 40-45%.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2</label>
<caption><title>Overview and schematic of the composite panels for two variants of DIP visual displays.</title>
<p>The top row presents the configuration and the panels that are combined with the OLED display (implemented in DIP1 and DIP2); note that in the macaque/human DIP1, the eyetracker and the hand/body cameras are illustrated on only one side, and only one side of the panel composition is shown, for clarity. The bottom row presents the configuration and the panels that are combined in the projector-based design (implemented in DIP3 and DIPc).</p></caption>
<graphic xlink:href="2rcknv1_fig2.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Due to OLED design asymmetry, there are noticeable differences in luminosity between the front and the back side of the screen, which can reach magnitudes of 80:1. While this results in considerable difference in brightness across the two sides of the screen, we note that the brightness on the dimmer side is still well above perceptual thresholds (see <xref ref-type="table" rid="tbl1">Table 1</xref>). To avoid giving participants on the brighter side a perceptual advantage, stimuli should be chosen to be clearly above the visibility threshold as judged from the dimmer side. All OLED displays we have tested so far (<xref ref-type="table" rid="tbl1">Table 1</xref>) have provided a sufficiently bright image on the dimmer side. Generally, this asymmetry is undesirable but is negligible in most cases, and it can be alleviated by tuning the background lighting to balance the contrast on both sides.</p>
</sec>
<sec id="s4-1-1-2">
<title>Light projector and projection film</title>
<p>An alternative to the OLED display described above is a projection film that displays an image projected from a light projector. The film can be mounted on any smooth, transparent surface, such as window glass, turning the surface into a projection screen (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). The light transmittance and clarity of the film can be high (both &gt; 90%) and, in combination with anti-reflective glass, allows for excellent transparency (see <xref ref-type="table" rid="tbl1">Table 1</xref>). The projector can be placed relatively freely in space depending on its optics, which decouples the image generation from the screen. This allows flexibility concerning the display size and resolution. A projector with a steep projection angle is favourable because it reduces the possibility of the participant’s body occluding the image from the screen. We, therefore, used ultra-short throw projectors that offer the steepest projection angles and can be placed directly above the screen, thereby reducing the distance from the screen where the occlusion can happen. However, when participants interact with the screen by touch, the hand tapping on the screen will block the light from the projector, thereby obstructing the stimulus on both sides. Usually, the participant performing the tapping is unaware of the occlusion because their hand obstructs the view. For the other participant, however, the stimulus partially or wholly disappears when touched on the other side. To mitigate such occlusions, we used a second projector on the other side, projecting a mirrored image so that occlusion from one side only results in a slight drop in brightness. This has the additional advantage that the brightness of stimuli is equal on both sides, independent of the projection film (see <xref ref-type="table" rid="tbl1">Table 1</xref>).</p>
<p>Including a second projector into the system requires careful alignment of the two projection planes onto one another to ensure that stimuli on both sides precisely overlap. We achieved this as follows: We presented a test image (e.g. a grid of bright lines on a dark background) and performed coarse mechanical alignment of the second projector (height, position, angle, focus). The remaining distortion was then removed with the image shape correction of the projector firmware. The “point correction” allows one to shift individual grid points so that the test images overlap. When the correction is applied, the projector interpolates between the grid points so that the presented stimuli overlap.</p>
</sec>
<sec id="s4-1-1-3">
<title>Luminance and transmittance of DIP displays</title>
<p>We examined the brightness and overall light transmittance of the OLED and projector-based displays used in current instantiations of the DIP. A luminance meter (LS-100 with close-up lens No 135, Minolta) was mounted on a tripod about 50 cm from the screen. The device pointed towards the display at a right angle and received light from a field of view of 1 degree. During the measurements, all room lighting was turned off to only capture the light of the displays. The luminance of a homogeneous white light source placed directly before and behind the display was measured to determine the transmittance. The transmittance was then calculated as the ratio of these luminances. To assess the brightness difference of the two sides of the display, we measured the luminance from each side of the display while the display showed a homogeneous white screen. The average luminances per side and the respective ratio are shown in <xref ref-type="table" rid="tbl1">Table 1</xref>.</p>
<p>We found that the light projector-based DIPs (DIP3, DIPc) have a luminance ratio close to one, as expected from the symmetric projector setup. The OLED displays have one side that is much brighter than the other, while the transmittance is also reduced as compared to the projectionbased systems. On the other hand, OLED-based displays present crisper, non-blurred stimuli, and do not require complex setup and alignment.</p>
<table-wrap id="tbl1" position="float" orientation="portrait">
<label>Table 1</label>
<caption><title>Characterization of transparent displays</title></caption>
<alternatives>
<graphic xlink:href="2rcknv1_tbl1.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="center" valign="top">Macaque/human DIP1</th>
<th align="center" valign="top">DIP2</th>
<th align="center" valign="top">DIP3</th>
<th align="center" valign="top">DIPc</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">Transmittance (%)</td>
<td align="center" valign="top">34</td>
<td align="center" valign="top">36</td>
<td align="center" valign="top">88</td>
<td align="center" valign="top">85</td>
</tr>
<tr>
<td align="left" valign="top">Luminance side A (cd/m<sup>2</sup>)</td>
<td align="center" valign="top">121</td>
<td align="center" valign="top">128</td>
<td align="center" valign="top">7.3</td>
<td align="center" valign="top">58</td>
</tr>
<tr>
<td align="left" valign="top">Luminance side B (cd/m<sup>2</sup>)</td>
<td align="center" valign="top">4.2</td>
<td align="center" valign="top">8.6</td>
<td align="center" valign="top">7.1</td>
<td align="center" valign="top">56</td>
</tr>
<tr>
<td align="left" valign="top">Luminance ratio A/B</td>
<td align="center" valign="top">28.6</td>
<td align="center" valign="top">14.8</td>
<td align="center" valign="top">1.03</td>
<td align="center" valign="top">1.04</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="TFN1"><p>Note. The luminance values could be compared to the reference white of 120cd/m<sup>2</sup> for television or the nominal value for screen luminance of 48cd/m<sup>2</sup> for digital cinema quality as defined by the <xref ref-type="bibr" rid="c127">Society of Motion Picture and Television Engineers (1995</xref>, <xref ref-type="bibr" rid="c128">2006</xref>).</p></fn>
</table-wrap-foot>
</table-wrap>
</sec>
</sec>
<sec id="s4-1-2">
<title>Auditory stimulation</title>
<p>Auditory speakers directed to both sides of the DIP allow researchers to provide instructions and auditory feedback to both participants simultaneously. For example, for non-human primate experiments, information about the amount of reward received by the partner can be encoded in the audio stream (<xref ref-type="bibr" rid="c94">Moeller et al., 2023</xref>), while task instructions for e.g., human child participants, can be pre-recorded and played back as part of the task progression, ensuring reproducibility across dyads (<xref ref-type="bibr" rid="c17">Bothe et al., 2024</xref>). The combination of engaging visual and auditory feedback, for instance during successful performance on a task, can render the task more “game-like” (<xref ref-type="bibr" rid="c2">Allen et al., 2024</xref>; <xref ref-type="bibr" rid="c85">Lewen et al., 2025</xref>), helping evoke and sustain interest and attention.</p>
</sec>
<sec id="s4-1-3">
<title>Interactive components</title>
<p>Human interface devices (HID) - such as computer mice, joysticks, or touch panels - are integral to the DIP concept and functionality. They can be used to steer the progress of the task, e.g., trigger subsequent stimulus presentation by participants’ touching, by moving to or clicking on a specific point on the screen. At the same time, such devices allow continuous read-out of information as participants actively respond to the task, which can be analysed later as a form of behaviour.</p>
<sec id="s4-1-3-1">
<title>Touchscreens</title>
<p>The touchscreen capitalizes on the primary form of visually-guided dexterous manipulations of the environment characteristic of primates. It has been extensively used in sensorimotor neuroscience to study visually-guided reach movements (<xref ref-type="bibr" rid="c11">Battaglia-Mayer et al., 2003</xref>; <xref ref-type="bibr" rid="c56">Gail and Andersen, 2006</xref>; <xref ref-type="bibr" rid="c30">Chang and Snyder, 2012</xref>; <xref ref-type="bibr" rid="c83">Lehmann and Scherberger, 2013</xref>; <xref ref-type="bibr" rid="c131">Suriya-Arunroj and Gail, 2019</xref>). A standard touchscreen-based paradigm, however, typically registers only endpoints of a movement, e.g., when participants tap on their selected target on the screen. Here, the transparency of the DIP yields a significant advantage, since participants can continuously observe the hand and arm movements of their partner and infer where they are going to tap on the screen. This has, for example, allowed us to uncover behavioural dependencies based on visibility of the social partner’s ongoing actions (<xref ref-type="bibr" rid="c94">Moeller et al., 2023</xref>). Researchers interested in continuous dynamics of such behavioural dependencies may consider supplementing the touchscreen recordings with marker- or video-based motion capture (see below) to digitise continuous behaviour in 3D (<xref ref-type="bibr" rid="c57">Gallivan and Chapman, 2014</xref>). Analysing behaviour in 3D could be particularly important given studies showing that reach dynamics can differ depending on the depth of the reaching plane (<xref ref-type="bibr" rid="c48">Ferrea et al., 2022</xref>). Transparent touchscreens also ensure that the visual stimuli, the social partner and the shared workspace are all in the same view, thus allowing participants to devote full attention to the task space and to observing the partner. Touchscreens may also provide participants with a greater sense of agency in interacting with the stimuli and the partner’s hand on the other side, compared to more abstract cursor tasks controlled by mouse or joystick. However, touchscreens incur costs in terms of the participants’ reaching arm obstructing the view on the stimuli or the partner’s actions. Vertical touchscreen tasks may also not be suitable for longer studies or certain types of interactions due to the physical effort required to raise the arm to provide a response.</p>
</sec>
<sec id="s4-1-3-2">
<title>Mouse and joystick</title>
<p>In contrast to touchscreens, computer mice and joysticks (or other types of manipulanda) allow the participant’s continuous behaviour to be represented by a moving cursor on the screen. Mousetracking tasks combined with analyses of the movement trajectories have been successfully employed to study economic decision-making in solo settings (<xref ref-type="bibr" rid="c129">Spivey et al., 2005</xref>; <xref ref-type="bibr" rid="c53">Freeman and Ambady, 2010</xref>; <xref ref-type="bibr" rid="c122">Scherbaum and Dshemuchadse, 2020</xref>; <xref ref-type="bibr" rid="c16">Boschet-Lange et al., 2024</xref>). Recent methodological advances in trajectory quantification, furthermore, allow analysis of space-time continuous data at the level of individual movements in single trials (<xref ref-type="bibr" rid="c58">Gallivan et al., 2018</xref>; <xref ref-type="bibr" rid="c138">Ulbrich and Gail, 2021</xref>, <xref ref-type="bibr" rid="c139">2023</xref>) without having to rely on stereotyped movements averaged across trials.</p>
<p>In dyadic settings, information about each participant’s task-related behaviour (and hence cognitive state) can be made mutually available via mouse- or joystick-controlled cursors on the trans parent screen without significant occlusions by the arm. Overall body motion during mouse or joystick response is reduced, allowing participants to complete longer experiments and respond across the entire screen with minimal effort. Furthermore, minimizing body motions is advantageous for stable recordings of physiological and neural data, e.g., reducing muscle and movement artifacts in EEG and MEG.</p>
<p>Another advantage is that mice and joysticks allow experimenters to create disassociations between a participant’s physical manipulation of the joystick and the observable consequences of this manipulation on screen, for example to introduce increased action costs by limiting the movement speed of the cursor (<xref ref-type="bibr" rid="c85">Lewen et al., 2025</xref>). For certain experimental designs, joysticks or haptic ma- nipulanda may also offer advantages over computer mice. First, joysticks provide two response dimensions such as the direction and the tilt, which can be used to measure distinct aspects of task response (see Perceptual decision-making in dyadic context). Second, the origin of the behavioural response can be normalized to the centre position of the joystick, allowing reproducible positioning of the hand. Third, force-feedback joysticks or robotic manipulanda allow resistive forces important for effort-based decision paradigms (<xref ref-type="bibr" rid="c96">Morel et al., 2017</xref>). Computer mice and joysticks, however, may require additional consideration related to differences in users’ proficiency with such devices, e.g., when working with infants and younger children.</p>
</sec>
</sec>
<sec id="s4-1-4">
<title>Data acquisition components</title>
<p>The DIP can be equipped with recording devices to continuously monitor spontaneous and goal- directed behaviour such as facial and vocal expressions, body movements, gaze, as well as peripheral physiological and neural signals. In what follows, we briefly describe the devices that can be integrated, the behaviour they can capture and how such data can be efficiently analysed, to allow for a comprehensive view of dyadic interactions in social settings.</p>
<sec id="s4-1-4-1">
<title>Facial expressions</title>
<p>One of the hallmarks of the DIP is the availability of the other social partner and their dynamic facial signalling in the same line of the sight of the visual stimuli. Thus, the partner’s face and changes to the partner’s facial expressions can be incorporated in the experimental paradigm, allowing an unprecedented level of precision in examining how changes to one participants’ facial expressions impact behaviour and performance of the other participant. Videos of the faces of both participants can then be analysed with specialized tools such as Affectiva, a face recognition software from iMotions, or other machine learning tools (<xref ref-type="bibr" rid="c8">Ballesteros et al., 2024</xref>), to automate recognition of specific facial expressions. Importantly, we note that for accurate identification of the facial expressions, the cameras must be positioned approximately in front of the face (see <xref ref-type="fig" rid="fig3">Figure 3</xref>, DIP3).</p>
</sec>
<sec id="s4-1-4-2">
<title>Non-linguistic vocalizations</title>
<p>Non-linguistic vocalizations like laughter, sighs, grunts, or grumbling are important socio-emotional cues in social interactions. These non-linguistic vocalisations can occur between spoken content or even in the absence of verbal communication. They are recorded with microphones attached to both sides of the DIP setup. Software, such as Hume AI (Hume AI Inc., 2024), can automatically identify and classify non-verbal utterances that appear in recordings of longer duration into 24 distinct emotional dimensions. Emotional vocalisations can then be analysed with regard to the extent to which participants respond to each other’s performance and whether such vocalisations influence the behaviour of the partners in the task. For instance, non-linguistic vocalisations may be one way for a member of the dyad to express either satisfaction with or a desire to change the interaction strategy currently displayed by the dyad.</p>
</sec>
<sec id="s4-1-4-3">
<title>Linguistic vocalizations</title>
<p>With human participants, language is one of the primary mediums for exchanging social cues. The microphones attached to the DIP setup can be used to capture the linguistic vocalisations exchanged between two participants working together and examine how such exchanges influence the processing of the presented visual and auditory stimuli. This could be expanded to include conversation analysis with varied foci, e.g., understanding the benefit of face-to-face interaction relative to asynchronous interaction in processing linguistic input, examining the linguistic structures that enable participants to coordinate efforts towards the completion of a shared objective, how turn-taking is organised and impacts subsequent decision making, how repairs are perceived or initiated when participants detect a mismatch in the strategies being employed by the social partners. Recent advances in AI, e.g., OpenAI Whisper (<xref ref-type="bibr" rid="c116">Radford et al., 2023</xref>), can be incorporated to automate speech transcription, allowing researchers to time lock speech to the temporal properties of stimulus presentation and response timing.</p>
</sec>
<sec id="s4-1-4-4">
<title>3D hand tracking</title>
<p>As with facial expressions, ongoing visibility ofa partner’s actions is a critical source of information to guide one’s own behaviour in a dyadic interaction. Thus, the timing and direction of the participants’ hand movements needs to be integrated into the analysis of such interactions. One way of tracking the 3D movement of the hand in real-time, isto let subjects use a haptic manipulandum, as introduced above (<xref ref-type="bibr" rid="c96">Morel et al., 2017</xref>), and register the position and even force applied to its handle. Alternative to the manipulandum, one can also track free movements using video-based motion capture, at least for offline analyses. Multiple cameras with different views of the same action can be used to calculate the 3D position of objects during a task. Thanks to advances in machine learning, it is possible to automatically detect the presence and position of body parts, e.g., hands or arm position, in such digital images with remarkable accuracy (<xref ref-type="bibr" rid="c91">Mathis et al., 2018</xref>). Increasing the number of cameras ameliorates the occlusion problem, where some camera perspectives cannot capture all features. Modern systems reconstruct occluded parts very well and can be trained on limb or hand models of different species (<xref ref-type="bibr" rid="c73">Hüser, 2024</xref>). One successfully tested setup included three cameras on each side of the screen, i.e., six cameras in total (macaque/human DIP1, see <xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="table" rid="tbl2">Table 2</xref>). One camera was mounted centrally on top of the setup looking down: this view covered the 2D movements of the hands and also allowed us to monitor the overall experiment. One camera was mounted on the top left (looking to the bottom right), and one was mounted on the top right (looking to the bottom left). Both cameras were mounted on a plane parallel to the screen with the central axis orthogonal to each other. These cameras allowed estimation of the 3D locations of the arm, providing valuable information on when participants begin to interact with the screen and how their hand movements influence their social partner’s behaviour.</p>
</sec>
<sec id="s4-1-4-5">
<title>Gaze behaviour</title>
<p>Eyetracking devices allow continuous monitoring of participants’ gaze behaviour and quantify participants’ attention to the content on the screen and to their social partner during their mutual interaction. The DIP can be combined with eyetrackers mounted either below or above the screen, with head-mounted eyetrackers, or, indeed, with cameras whose output is later coded with regard to participants’ gaze behaviour.</p>
<p>If no physical interaction between the participants and the screen is required and participants remain seated and relatively still throughout the experiment, eyetrackers can be mounted below the screen. In such cases, simple cameras capturing head and eye movements may also provide a cost-efficient alternative to commercial eyetrackers. Cameras, however, come with lower spatial resolution and are only recommended for use when limited content with well-defined areas of interest is presented on screen.</p>
<p>If a touch response or arm movements toward the screen are intrinsic to the task, eyetrackers can be mounted either above the screen, or participants can wear head-mounted eyetrackers (e.g., Tobii glasses, Pupil Labs Core or Neon). Modern binocular head-mounted eyetrackers allow researchers to collect participants’ gaze behaviour in the real world, i.e., beyond the limitations of content displayed on the screen. For example, they enable the tracking of gaze to the social partner’s hand movements towards the screen before the actual touch on-screen happens, or it assesses the extent to which participants track their social partners’ eye or head movements before following their gaze (see Section Attention and social learning). Such eyetrackers can also be used in studies where participants move freely during the task. Head-mounted eyetrackers do, however, change the appearance of the social partner, which might be undesirable in paradigms with young children or where unobstructed facial expressions or gaze cues are critical to task performance.</p>
<p>In the DIP, it can often be necessary to differentiate between gazing at an object on the screen and gazing at the partner located behind the screen. There are straightforward and more complicated solutions to this problem. One simple solution is to ensure that the centre of the screen is left free so that the partner’s face is distinct from task-related objects, e.g., with stimuli arranged in a circle around the partner’s face. For instance, in <xref ref-type="fig" rid="fig7">Figure 7</xref>, we present data captured using Pupil Labs eyetrackers children wore while they were allowed to move freely during the task. The data were fed through an object detection model (YOLOv8, <xref ref-type="bibr" rid="c77">Jocher et al., 2023</xref>). This allowed automated detection of the partner’s face and the different objects presented on screen, which were then mapped onto the gaze data to estimate children’s gaze towards the partner and those objects. However, manipulating stimulus presentation in this manner could result in the object on the screen occluding the partner’s face, depending on where the participant stands. Binocular models that estimate eyetracking coordinates in 3D may provide a more suitable alternative, allowing to calculate the vergence and discriminate between fixations to objects on-screen relative to fixations to the partner’s face beyond.</p>
<p>Automated recognition of different areas on the screen and gaze tracking during free head movements can also be achieved using surface detection, as implemented within the Pupil Labs system. Surface detection works by the position and orientation of predefined markers, e.g., AprilTags, in the visual scene (<xref ref-type="bibr" rid="c102">Olson, 2011</xref>) (see <xref ref-type="fig" rid="fig3">Figure 3</xref>, DIP2). The eyetracking software can be set up to detect where a participant’s gaze falls relative to these AprilTags throughout the study, thereby enabling automatic detection of participants’ fixations to specific parts of the screen.</p>
</sec>
<sec id="s4-1-4-6">
<title>Peripheral physiology</title>
<p>Social decision making and cooperative behaviour in real life interactions are accompanied by strong changes in the physiological and psycho-emotional states of the participant, regulated by autonomous physiological processes (<xref ref-type="bibr" rid="c13">Behrens et al., 2020</xref>). Such changes can influence self-perception (e.g., noticing an increase in heart rate) and can also be perceived by the interaction partner, e.g., blushing (<xref ref-type="bibr" rid="c114">Prochazkova and Kret, 2017</xref>); for a review of physiological synchrony in dyads, see <xref ref-type="bibr" rid="c104">Palumbo et al. (2016)</xref>). There is, therefore, real value in measuring peripheral physiological measures such as electrocardiography (ECG), electromyography (EMG), and electrodermal activity (EDA) in the DIP. For instance, EMG recordings of facial muscle movement serve as indicators of expressions of positive and negative valence. Physiological and emotional state changes, mediated by sympathetic and parasympathetic systems, can be reflected in electrodermal activity (EDA) and the heart rate, measured by ECG or pulse-oximetry. Yet, in the context of behavioural experiments with freely moving subjects, the probe of a pulse oximeter can be distracting or unwieldy. Imagebased photoplethysmography (iPPG) overcomes these challenges, allowing the extraction of pulse information from videos of a subjects face. Indeed, this has been successfully used in human subjects using low cost web cams as sensors (<xref ref-type="bibr" rid="c113">Poh et al., 2011</xref>). Furthermore, we could show that iPPG with low cost video cameras can also be successfully used in head-fixed rhesus macaques in spite of their smaller size and more pigmented skin (<xref ref-type="bibr" rid="c140">Unakafov et al., 2018</xref>).</p>
</sec>
<sec id="s4-1-4-7">
<title>Electrophysiology and hyperscanning</title>
<p>The most direct way to examine the neural correlates of the interaction between individuals is through hyperscanning, a method that describes the simultaneous recording of brain activity from two or more individuals to determine how covariation in their neural activity relates to their behaviour and social interactions (<xref ref-type="bibr" rid="c64">Hakim et al., 2023</xref>). One method that is particularly well suited for investigating the rapidly changing neural processes that occur in dyadic social interactions is EEG. EEG is a non-invasive and cost-effective method that can be used to map neural processes with very high temporal resolution and is easily extendable to a hyperscanning setup (<xref ref-type="bibr" rid="c39">Czeszumski et al., 2020</xref>). The integration of hyperscanning EEG into the DIP, further, allows the monitoring of neural changes based on the shared task environment (visual and auditory cues controlled by the experimenter) and the representation of the partner and their actions (socio-emotional cues, decisions). Hyperscanning can be achieved by different approaches: integration via linked stationary amplifiers, synchronization via trigger signals sent from the same PC, or via Lab Streaming Layer (LSL; <xref ref-type="bibr" rid="c81">Kothe et al. (2024)</xref>, see <ext-link ext-link-type="uri" xlink:href="https://labstreaminglayer.org/">https://labstreaminglayer.org/</ext-link>). Recording in both subjects using a single data acquisition system simplifies the time synchronisation, by only having to synchronize the clock of the electrophysiological recording to the clock of the computer running the paradigm, facilitating inter-brain synchronization analyses. Mobile EEG systems allow for more flexibility in the tasks to be performed and allow participants to move around during the session with comparatively little loss of data quality. The concurrent registration of peripheral physiological measures and neural signals is especially crucial for the investigation of neural correlates of dynamic social behaviours, such as social mimicry (<xref ref-type="bibr" rid="c1">Achaibou et al., 2008</xref>). It also may facilitate artifact correction of noise sources that are common to multiple measurement channels (<xref ref-type="bibr" rid="c86">Li et al., 2021</xref>), which might be more pronounced in dynamic social interactions.</p>
<p>Much like traditional single-subject experimental platforms, the DIP is also well suited for intracranial electrophysiology in one or both subjects, enabling targeted high temporal and spatial resolution recordings of neuronal activity such as single neuron and local field potential recordings. This approach can be used in nonhuman primate experiments (<xref ref-type="bibr" rid="c31">Chang, 2017</xref>; <xref ref-type="bibr" rid="c75">Isoda et al., 2018</xref>), and in human epilepsy patients undergoing intracranial EEG (iEEG) monitoring with subdural grid electrocorticography (ECoG) or stereotactic depth electrodes (<xref ref-type="bibr" rid="c106">Parvizi and Kastner, 2018</xref>).</p>
<p>An important consideration in the DIP-based experimental designs, relevant to all neural data recording modalities, is the vis-à-vis arrangement and transparency, which result in participants representing the same lateralised stimulus/action space in opposite hemispheres. For example, a stimulus appearing on the left for one participant (processed in the right hemisphere) would be on their partner’s “subjective” right side (processed in the left hemisphere). This is particularly crucial for early visual processing in humans and even more so for the highly contralateral cortical representations, including the frontoparietal network, in macaques (<xref ref-type="bibr" rid="c78">Kagan et al., 2010</xref>). These considerations should be taken into account when considering DIP-based dual-brain analysis, relative to common hyper-scanning approaches where participants are in separate rooms or seated side-by-side and observe and act on identical visual displays.</p>
</sec>
<sec id="s4-1-4-8">
<title>MEG and hyperscanning</title>
<p>Magnetoencephalography (MEG) has found applications in neuroscience despite its serious cost compared to EEG, because it offers certain advantages over EEG with respect to source localization of neural activity. This is because (i) MEG measures the magnetic <italic>field</italic>, and is thus reference-free, and (ii) the physics of the magnetic fields induced by neural currents allow for an easier calculation of high-precision forward solutions to the electromagnetic inverse problem. Until very recently, MEG could be considered a less-than-ideal method for neurophysiological studies into transparent interaction due the high cost of superconducting-based MEG devices and their size, which precluded operating two MEG devices in a standard sized single magnetically shielded room. A critical restriction was the fact that subjects had to refrain from any head movement to avoid relative motion between the stationary sensors of the device and the head.</p>
<p>This situation has changed with the advent of usable optically-pumped magnetometer (OPM) - MEG systems of sufficient sensitivity (<xref ref-type="bibr" rid="c20">Brookes et al., 2022</xref>). OPM-MEG sensors need no cooling with liquid helium and are lightweight enough to allow for a sensor montage in a form of helmet that subjects can wear on their heads (<xref ref-type="bibr" rid="c18">Boto et al., 2018</xref>). Moving MEG sensors on a subject’s head through space, however, comes at a cost: As OPM-MEG sensors usually have a low dynamic range any background magnetic field in the room has to be compensated locally at the sensor before the measurement to ensure proper sensor function. Therefore, background fields need to be much more tightly controlled by the magnetically shielded room (MSR) than for stationary, superconducting-based MEGs. Moreover, dynamic compensation of residual fields is possible for experiments with head movements (<xref ref-type="bibr" rid="c71">Holmes et al., 2023</xref>), resulting in a setup better suited to studies of naturalistic dyadic interactions. An important additional advantage brought about by the switch to OPM-MEG is that larger sensor configurations, e.g. for hyper-scanning, can be built-out gradually, whereas sensor number and configuration in a superconducting-based system are fixed. Additional challenges for integrating MEG with the DIP concern magnetic fields produced by DIP components. For instance, projectors for presenting stimuli on the transparent screen have to be mounted outside the MSR (e.g. above its ceiling). Thus, an optic path of considerable length has to be traversed through a relatively narrow opening in the MSR wall, and entirely non-magnetic materials have to be used inside the MSR to realize the necessary short-throw optics.</p>
</sec>
</sec>
</sec>
<sec id="s4-2">
<title>Description of DIP instantiations</title>
<p><xref ref-type="table" rid="tbl2">Table 2</xref> describes four instantiations of the DIP varying in terms of the target subject population, the kind of data being collected, the visual displays and the combination of recording devices that have been successfully incorporated into the platform. The first instantiation of the DIP targeted interactions between two macaques or two humans or a human and a macaque dyad (macaque/human DIP, DIP1). Key features of this setup are flexibility in terms of how responses could be recorded, i.e., a touchscreen, a mouse or a joystick, head- or frame-mounted eyetracking, intracranial electrophysiology, as well as computer-controlled fluid dispenser pump systems to provide reward to the macaques (<xref ref-type="fig" rid="fig3">Figure 3</xref>, top row, left and middle panels). For human participants, the second instantiation of the DIP (human DIP2, (<xref ref-type="fig" rid="fig3">Figure 3</xref>, top row, right panel) included mice or joysticks as the recording devices for task-related responses, and wearable eyetrackers. The next update to the DIP (DIP3), based on the shielded projectors instead of OLED displays, allowed for collection of a wider range of behavioural and neurophysiological data, e.g., recording peripheral physiology and EEG, as well as eyetracking data from a mounted eyetracker on each side of the frame (<xref ref-type="fig" rid="fig3">Figure 3</xref>, bottom row, left panel). Finally, the DIPc extended the set up to human child-adult and child-child interactions, requiring additional changes in terms of dimensions, integrating head- or frame-mounted eyetrackers, object and face detection using machine learning models, as well as vital cosmetic changes, e.g., differently coloured frames on the two sides, to indicate to the child when and how they could respond (<xref ref-type="fig" rid="fig3">Figure 3</xref>, bottom row, middle and right panel).</p>
</sec>
<sec id="s4-3" sec-type="ethics-statement">
<title>Ethics statement</title>
<p>Experiments with human participants were performed in accordance with institutional guidelines and adhered to the principles of the Declaration of Helsinki. Human participants in all experiments presented here provided written informed consent for their (or their child’s) participation in the study. In particular, all adults depicted in <xref ref-type="fig" rid="fig3">Figure 3</xref> gave explicit written consent for themselves and/or their children for photographs to be depicted in the figure and agreed to its publication. All studies were approved by the the ethics committee of the Georg-Elias-Müller-Institute for Psychology, University of Göttingen.</p>
<p>The experimental procedures with macaque monkeys were approved by the responsible regional government office (Niedersächsisches Landesamt für Verbraucherschutz und Lebensmittelsicherheit (LAVES), permits 3392-42502-04-13/1100 and 3319-42502-04-18/2823), and were conducted in accordance with the European Directive 2010/63/EU, the corresponding German law governing animal welfare, and German Primate Center institutional guidelines.</p>
<table-wrap id="tbl2" position="float" orientation="portrait">
<label>Table 2</label>
<caption><title>Description of the four DIPs</title></caption>
<alternatives>
<graphic xlink:href="2rcknv1_tbl2.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="left" valign="top">Macaque/human DIP1</th>
<th align="left" valign="top">Human DIP2</th>
<th align="left" valign="top">DIP3</th>
<th align="left" valign="top">DIPc</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">Visual display</td>
<td align="left" valign="top">- OLED monitor (EYE-TOLED-5500, eyevis/LEYARD) sandwiched between two anti-reflective films (NuShield DayVue)
<break/>- 55” (121 cm × 68 cm) interactive area with a display resolution of 1920x1080 (1.6 pixel/mm)</td>
<td align="left" valign="top">- OLED monitor (EYE-TOLED-5500-TIGF40, eyevis/LEYARD)
<break/>- 55” (121 cm × 68 cm) interactive area with a display resolution of 1920x1080 (1.6 pixel/mm)</td>
<td align="left" valign="top">- projection film (DualView Ultra Clear, Screen Solutions International) sandwiched between two sheets of 3 mm anti-reflective museum glass (claryl-ng UV70, artglass)
<break/>- one projector (EB-735F, Epson) per side
<break/>- 107 cm x75 cm interactive area with a display resolution of 860 × 580 (0.8 pixel/mm)</td>
<td align="left" valign="top">- projection film (ClearBright, Lux Labs) sandwiched between two sheets of 3 mm anti-reflective acrylic glass (Optium Museum acrylic, True Vue)
<break/>- one projector (EB-735F, Epson) per side
<break/>- 42” (93 cmx53 cm) interactive area with a display resolution of 1120 × 630 (1.2 pixel/mm)</td>
</tr>
<tr>
<td align="left" valign="top">Interactive components</td>
<td align="left" valign="top">- one 55” IR touch panels (G5S 55”, PQLabs) with 6 mm glass per side, each sandwiched between two anti-reflective films (NuShield DayVue)</td>
<td align="left" valign="top">- one mouse (Logitech) or joystick (custom design based on multi-functional joystick ‘MoJo’ from Sasse Elektronik) per side</td>
<td align="left" valign="top">- one mouse (Logitech) per side</td>
<td align="left" valign="top">- one 42” IR touch panel (G5 integration kit, PQLabs) per side</td>
</tr>
<tr>
<td align="left" valign="top">Date acquisition devices</td>
<td align="left" valign="top">- one colour camera (CM3-U3-13Y3C-CS, Chameleon3, Flir) to detect facial expressions and estimate iPPG heart rate
<break/>- one colour camera (CM3-U3-13Y3C-CS, Chameleon3, Flir) and two gray-scale cameras (CM3-U3-13Y3M-CS, Chameleon3, Flir) per side for arm/hand tracking
<break/>- head- or chair-mounted eyetracker (Pupil Core, Pupil Labs), or display-mounted eyetracker (EyeLink 1000 Plus, SR Research), per side</td>
<td align="left" valign="top">- one face camera and microphone
<break/>- one head-mounted eyetracker (Pupil Core, Pupil Labs) per side</td>
<td align="left" valign="top">one camera (C920 HD pro webcam, Logitech) per side for recording of facial expressions
<break/>- one mobile EEG system (LiveAmp 64 system, Brain Products GmbH) per participant
<break/>- six facial EMG electrodes and three ECG electrodes per participant recorded with
<break/>Sensor-Trigger-Extension of the EEG amplifier (Live64-STE, Brain Products GmbH)
<break/>- one eyetracker (EyeLink 1000 Plus, SR Research) per side</td>
<td align="left" valign="top">three cameras (FHD06H-BL180, ELP) per side
<break/>- one head-mounted eyetracker (Pupil Core, Pupil Labs) per side
<break/>- one room microphone (AC-44, MXL)</td>
</tr>
<tr>
<td align="left" valign="top">Additional features</td>
<td align="left" valign="top">- optimized for multichannel non-human primate electrophysiology broadband recording system with 160 analogue and 160 digital headstage channels for simultaneous recording of 320 channels (Tucker-Davis Technologies)
<break/>- liquid reward system: one peristaltic pump per side with custom-made control box</td>
<td align="left" valign="top">- optimized for free head-mounted (wearable) eyetracking in adult humans</td>
<td align="left" valign="top">- optimized for human electrophysiology recordings: projectors on top of the experimental cabin (projection through ceiling windows) to allow for maximal noise reduced environment
<break/>- face cameras, eyetrackers, and display height adjustable</td>
<td align="left" valign="top">- optimized for child-child and adult-child interaction
<break/>- blinds can block the view to the other participant and still allow interaction with half of the screen for independent testing in the same session</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3</label>
<caption><title>Dyadic Interaction Platforms in action.</title>
<p>Top row: OLED-based DIP1 and DIP2, bottom row: double projection-based DIP3 and DIPc. See <xref ref-type="table" rid="tbl2">Table 2</xref> for descriptions.</p></caption>
<graphic xlink:href="2rcknv1_fig3.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s5" sec-type="results">
<title>Results</title>
<p>Designing one- and two-way interactive paradigms for a DIP differs from traditional paradigms where a single participant is tested. On the one hand, the inclusion of two agents in trial-based experiments is easily possible by allowing or instructing the participants to take turns. On the other hand, dynamic decision-making, in contrast to discrete, regulated trial-based paradigms, involves navigating through a continuous stream of choices, actions, and outcomes, mirroring the fluid nature of decision-making in real-life scenarios. Such dynamic decision-making tasks allow for a more ecologically valid exploration of behaviour and cognition. Next, we describe four classes of example paradigms that we realized with the DIP - which vary in terms of the dynamic and continuous nature of the interaction between two partners - and the exciting possibilities that such approaches open up for cognitive science research.</p>
<sec id="s5-1">
<title>Transparent economic games</title>
<p>Dyadic economic games are a cornerstone of experimental economics and social science, offering a powerful framework to explore the decision-making processes between two participants in competitive and cooperative contexts (<xref ref-type="bibr" rid="c121">Sanfey, 2007</xref>; <xref ref-type="bibr" rid="c117">Rilling and Sanfey, 2011</xref>; <xref ref-type="bibr" rid="c135">Tremblay et al., 2017</xref>). These games are intended to simulate real-world scenarios where individuals must make choices that affect not only their own outcomes but also those of their partners (<xref ref-type="bibr" rid="c97">von Neumann and Morgenstern, 1944</xref>). Such choices can be presented as one-shot games or as iterated games with repeated interactions that encourage tracking the interaction history and the formulation of predictions regarding the other agent’s decisions. Classical examples include the Prisoner’s Dilemma, the Stag Hunt, the Hawk-Dove / Chicken Game, the Ultimatum Game, and the Trust Game, each offering unique insights into altruism, reciprocity, fairness, and a conflict over a shared resource (<xref ref-type="bibr" rid="c125">Smith, 1997</xref>; <xref ref-type="bibr" rid="c21">Brosnan et al., 2017</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4</label>
<caption><title>Dynamic coordination in transparent Bach-or-Stravinsky decision game.</title>
<p><bold>(A)</bold> Top panel: “fraction choosing own” i.e., choice of the individually preferred target for agent A (human confederate, red) and agent B (monkey, blue) in one session (running average of eight trials). The visual access to other’s actions was occluded in the middle part of the session (opaque). The confederate (red) switched between own and monkey’s preferred targets in blocks of 20 trials. Bottom panel: the average joint reward. Dashed green line - maximal attainable average joint reward, given the used payoff matrix. <bold>(B)</bold> Human vs monkey reaction time difference histograms for the three prevalent outcomes: coordinated selection of human’s preferred target (red), monkey’s preferred target (blue), and selection of own preferred target by each agent (magenta), in the two action visibility conditions. Modified from <xref ref-type="bibr" rid="c94">Moeller et al. (2023)</xref>.</p></caption>
<graphic xlink:href="2rcknv1_fig4.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>While, in classical dyadic games, individual choices are made either “simultaneously” (neither player knows the choice of the other before making their own decision) or sequentially in a predetermined order, real interactions often unfold continuously with the partner’s actions in direct sight (<xref ref-type="bibr" rid="c44">Dugatkin et al., 1992</xref>; <xref ref-type="bibr" rid="c43">van Doorn et al., 2014</xref>). In this “transparent” context, the timing of one’s own and other’s actions becomes part of the strategy (<xref ref-type="bibr" rid="c101">Noe, 2006</xref>; <xref ref-type="bibr" rid="c92">McDonald et al., 2019</xref>; <xref ref-type="bibr" rid="c141">Unakafov et al., 2020</xref>). Moreover, coordinating based on mutual choice history might be more demanding than coordinating based on the immediately observable behaviour of others, especially for children and nonhuman species. For example, visual feedback about the partner’s choices improves coordination in the iterated Stag Hunt in humans, capuchins and rhesus macaques (<xref ref-type="bibr" rid="c23">Brosnan et al., 2012</xref>), and such coordination in chimpanzees is facilitated if one of the agents consistently acts faster than the partner (<xref ref-type="bibr" rid="c26">Bullinger et al., 2011</xref>). Similarly, there are substantial differences in capuchins’ and rhesus’ behaviour in the Chicken game when they had access to the current choice ofa partner (<xref ref-type="bibr" rid="c21">Brosnan et al., 2017</xref>; <xref ref-type="bibr" rid="c103">Ong et al., 2021</xref>). In humans, a real-time anti-coordination game revealed that action visibility and the ability to change an already initiated action increased efficiency and fairness (<xref ref-type="bibr" rid="c68">Hawkins and Goldstone, 2016</xref>).</p>
<p>To study dynamic value-based interactions in humans and rhesus macaques, we implemented a transparent face-to-face version of the iterated Bach-or-Stravinsky (BoS) game (<xref ref-type="bibr" rid="c94">Moeller et al., 2023</xref>). In the BoS paradigm, each player has an individually preferred option, but coordinating on either one of these options increases the reward for both players (<xref ref-type="bibr" rid="c80">Kilgour and Fraser, 1988</xref>). While any coordinated choice results in better rewards than non-coordinated choices, one coordinated choice results in greater rewards for the first player and the other - for the second player. Thus, while the rational choice is to coordinate, BoS includes an inherent conflict about who profits the most. In our DIP implementation, the players used visually-guided manual reaches to a shared vertical workspace on a dual touchscreen to indicate their choice between the two targets representing the two options. We found that both species learned to use mutual action visibility for efficient coordination. Human dyads mainly adopted dynamic cooperative turn-taking to equalize the payoffs. All macaque dyads initially converged to a simpler, more static coordination driven by unilateral reward maximization or effort minimization motives. However, macaques paired with a turn-taking human confederate developed dynamic coordination, if they were able to observe the confederate’s actions (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The incorporation of action timing into strategic behaviour was evident from the analysis of reaction times: the macaque subjects waited for the partner to commit to their non-preferred choice (<italic>human colour</italic>, <xref ref-type="fig" rid="fig4">Figure 4B</xref>, left panel), but this behaviour broke down when they could not observe the partner’s movements (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, right panel). Remarkably, when such confederate-trained macaques were paired together, they exhibited dynamic turn-taking driven by temporal competition, unlike the prosocial turn-taking in humans. Underscoring the importance of sensorimotor dynamics, reaction time differences between the two players strongly predicted the joint choice on a trial-by-trial basis (<xref ref-type="bibr" rid="c94">Moeller et al., 2023</xref>). The currently faster monkey led to its preferred option, and the slower monkey followed. This study demonstrates that dynamic coordination is not limited to humans, but it can be subserved by different social attitudes and cognitive capacities. More generally, these experiments emphasize the importance of action visibility, within-trial dynamics and immediate sensorimotor context for studying the emergence and maintenance of naturalistic coordination and embodied decision-making, grounded in real- world constraints such as effort and movement biomechanics.</p>
</sec>
<sec id="s5-2">
<title>Continuous strategic interactions</title>
<p>The recent emphasis on continuous interactions reflects the paradigm shift from discrete, trialbased choices between limited fixed response options to more naturalistic, dynamic behaviours that characterize most real-world scenarios (<xref ref-type="bibr" rid="c61">Gordon et al., 2021</xref>), such as collective foraging and hunting (<xref ref-type="bibr" rid="c92">McDonald et al., 2019</xref>; <xref ref-type="bibr" rid="c146">Yoo et al., 2021a</xref>,<xref ref-type="bibr" rid="c147">b</xref>; <xref ref-type="bibr" rid="c112">Pisauro et al., 2022</xref>). Conversely, the action component in many classical decision paradigms is trivial (e.g. a button press or a simple eye movement) and bears no consequence on the subsequent perceptual inputs, breaking the recurrent perception-action loop inherent in natural behaviours. Therefore, it is necessary to employ experimental paradigms that realize continuous, embodied dynamic interactions under controlled conditions (<xref ref-type="bibr" rid="c36">Cisek and Green, 2024</xref>).</p>
<p>The transparent DIP is ideally suited to study such interactions. The visually-guided reach coordination BoS game described above already exemplifies sensorimotor interactions where action timing and effort become integral parts of strategy space (<xref ref-type="bibr" rid="c94">Moeller et al., 2023</xref>; <xref ref-type="bibr" rid="c92">McDonald et al., 2019</xref>). But in each trial, the choice was limited to two options equidistant from the central starting position. To implement strategic interactions in a more variable, continuously evolving action space, two interacting “agents” can be represented by virtual avatars (cursors) controlled by a joystick or a computer mouse on a shared 2D playing field. Spatial targets can be placed flexibly, and both players see their own and partner’s cursors in real time, mimicking real-world scenarios where individuals must make rapid decisions based on the positions and actions of others. Crucially, face- to-face visibility ensures a salient social context despite the interaction via virtual agents. The level of embodied realism is achieved by continuous spatiotemporal trajectories of the agents, and access to face, hand, and body movements of the two players. At the same time, the participants do not move excessively, which facilitates physiological and neural recordings. In the following, we present two kinds of real-time dyadic foraging tasks to illustrate the richness of the co-evolving coordination and decision processes that can be studied in a time-resolved manner.</p>
<p>In a purely <italic>competitive foraging task</italic>, we tested adult human subjects to explore how the proximity ofa competitor to a target influences their spatial choices. Participants engaged in a series of trials where they had to quickly navigate to one of several targets to collect points. The placement of targets and the starting location of the agents were varied to emulate various foraging scenarios. Four potential target locations were shown (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). To prevent the players from occupying targets in advance, a no-go zone surrounded targets, and the trial was aborted if either of the players entered this zone during the trial start state; after 1-3 s, two randomly selected targets lit up as active targets and players were allowed to collect the targets by hovering over them. The trial ended when both targets were collected, either by one or by two players.</p>
<p>Subjects were paired with a confederate player in the first experiment and with another naive player in the second experiment. In the first experiment (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, left), we tested the effect of the “competitor’s” proximity on the subjects’ choice by instructing a confederate to choose a randomized position along the horizontal axis at the beginning of each trial. We computed the distance between each cursor and each active target. Qualitatively, our results show that the subject chose the left target when the distance of the confederate’s cursor to the right target was smaller than the left target and vice versa (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, right). In the second experiment, we paired the subjects to play against each other. All players chose the initial location of their cursor in the middle of the screen and as close as possible to the no-go zone, maximizing their chance to win the race to active targets (<italic>cf.</italic> “space dilemma” in <xref ref-type="bibr" rid="c112">Pisauro et al. (2022)</xref>). Analysing the 2D trajectories, we could also identify trials where one of the players did not take a straight path to the chosen target (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). For example, the green player in <xref ref-type="fig" rid="fig5">Figure 5C</xref> left moved to their right but then made a sharp turn to collect the left target, or the purple player in <xref ref-type="fig" rid="fig5">Figure 5C</xref> right first headed to the close left target but then collected the far left target. Such trajectories demonstrate change-of-mind scenarios in which a player tracks the competitor and adjusts accordingly, revealing the complexity of decision-making in a transparent, competitive task.</p>
<p>To expand the study of continuous transparent interactions beyond zero-sum scenarios that focus on competition, we developed a dynamic dyadic foraging paradigm that enables emergence of both, cooperative and competitive strategies, and facilitates continuity of social signals and actions across multiple interaction cycles (<xref ref-type="bibr" rid="c85">Lewen et al., 2025</xref>). In this Cooperation-Competition Foraging (CCF) game, across many continuous interaction cycles, dyads decide between collecting “joint targets” together or “single targets” alone, allowing to elucidate behavioural mechanisms arbitrating between cooperative and competitive strategies. We found that most human dyads converged to their specific ratio of collecting single versus joint targets, exhibiting dyad-specific stable strategies that nevertheless spanned the entire range from pure cooperation to pure competition. These results show the flexibility and the richness of interactions that can emerge in well-balanced foraging games and demonstrates how incorporating sensorimotor variables such as movement speed, curvature, effort minimization and skill differences shapes optimal strategies.</p>
</sec>
<sec id="s5-3">
<title>Perceptual decision-making in dyadic contexts</title>
<p>A perceptual decision, or judgment, is a process of converting sensory inputs to discrete categorical variables. Perceptual decisions are influenced by behavioural relevance (<xref ref-type="bibr" rid="c137">Treue, 2003</xref>), attentional deployment (<xref ref-type="bibr" rid="c136">Treue, 2001</xref>), stimulus/choice history (<xref ref-type="bibr" rid="c145">Witthoft et al., 2018</xref>), reward contingencies (<xref ref-type="bibr" rid="c34">Cicmil et al., 2015</xref>), and perceptual confidence (<xref ref-type="bibr" rid="c79">Kiani and Shadlen, 2009</xref>; <xref ref-type="bibr" rid="c95">Moreira et al., 2018</xref>)). The main emphasis of perceptual decision studies is the judgment of ambiguous, noisy stimuli, under conditions of perceptual uncertainty. Crucially, perceptual decisions are profoundly shaped by social influences (<xref ref-type="bibr" rid="c6">Bahrami et al., 2010</xref>; <xref ref-type="bibr" rid="c9">Bang and Frith, 2017</xref>; <xref ref-type="bibr" rid="c12">Baumgart et al., 2019</xref>; <xref ref-type="bibr" rid="c132">Takagaki and Krug, 2020</xref>; <xref ref-type="bibr" rid="c110">Pescetelli and Yeung, 2022</xref>). Most work on the interaction between individual perceptual choices and social information has focused on cooperative tasks, where participants first make individual judgments and then exchange their opinions, and often associated confidence, before a joint decision is made. Joint performance can, however, exceed the best individual performance under certain conditions, depending on the perceptual similarity between the partners and the mode of social exchange (<xref ref-type="bibr" rid="c9">Bang and Frith, 2017</xref>; <xref ref-type="bibr" rid="c144">Wahn et al., 2018</xref>). However, social influences often adversely affect perceptual accuracy or lead to a disconnect between accuracy and confidence in perceptual choices. For instance, considering a partner’s choices can be motivated by a desire to be correct, especially when one has low confidence in one’s own judgment. On the other hand, social modulation might be driven by reasons unrelated to accuracy - such as social conformity. Therefore, one of the central questions in understanding flexible decision-making in social contexts is to dissociate and quantify the reliability-weighted, adaptive informative influences (such as Bayes-optimal cue integration) from normative, conformity-driving biases (<xref ref-type="bibr" rid="c134">Toelch and Dolan, 2015</xref>; <xref ref-type="bibr" rid="c88">Mahmoodi et al., 2018</xref>, <xref ref-type="bibr" rid="c89">2022</xref>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5</label>
<caption><p><bold>(A)</bold> Flow of the competitive foraging task. Left: two human players in DIP, using a joystick to collect targets, in a starting position. Middle: in each trial, two out of four targets are activated. Right: the active targets may be collected by the same subject or different subjects. <bold>(B)</bold> Effect of initial proximity. Left: a confederate chose pseudorandom locations along the horizontal axis at the beginning of each trial. Right: the dependency of subject’s choice (left or right active target) as a function of self and confederate’s initial distance from the two active targets. To combine the results across trials with various combinations of the active targets, we assigned the two active targets as the left and right targets, then pooled a cursor’s distance to the left and right targets across trials. <bold>(C)</bold> Dynamic decisions in two subjects during example trials, revealed by their cursor’s trajectory. Left: green player’s change of mind. Right: purple player’s change of mind.</p></caption>
<graphic xlink:href="2rcknv1_fig5.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6</label>
<caption><title>Example of a perceptual decision-making paradigm on DIP, with corresponding behavioural data.</title>
<p>Left: Human subjects watched a 100% coherent random dot pattern (RDP) on both sides of the transparent OLED screen (DIP2). Using a joystick, they had to indicate whether the stimulus direction was moving leftward or rightward of the vertical midline. The stimulus direction changed instantly after pseudorandom time intervals. Right: Psychometric curves of two example subjects measured on both sides of the DIP screen (dark/bright). Data points indicate the percentage of reporting rightward direction as a function of stimulus difficulty (deviation from vertical direction) and direction (positive - rightward). The data are fitted with a bounded logistic function.</p></caption>
<graphic xlink:href="2rcknv1_fig6.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>We have recently developed a powerful new approach to studying social information integration using a continuous perceptual report (CPR) paradigm (<xref ref-type="bibr" rid="c123">Schneider et al., 2024</xref>). Its primary advantage is the simultaneous tracking of both perceptual accuracy and confidence in real-time, in individual and social contexts. This paradigm does not separate perceptual decision-making and the social exchange of choices and associated confidence into distinct, imposed stages of the task. Instead, participants continuously track and indicate the perceived direction ofa noisy random dot pattern and their confidence in their perception. In the dyadic condition, both partners’ ongoing report and occasional feedback are added to the visual task display so that each participant can see where their partner thinks the stimulus is moving, how certain, and how successful they are. With this approach, we derived a nuanced view of the relationship between individual expertise and dyadic effects, demonstrating how the bidirectional modulation by social information lawfully depends on solo performance differences between dyadic partners.</p>
<p>The CPR experiments described above have been conducted so far with each dyadic player in a separate booth, using joystick-controlled cursors incorporated into a shared stimulus display to continuously indicate the responses of both participants. A highly promising next step is to utilize the transparent DIP, to exploit the immediacy of the direct face-to-face interaction, as well as the additional social cues such as facial and postural signals. Our pilot data demonstrate that continuous perceptual reports of human participants can be reliably measured on the transparent DIP, on both sides (<xref ref-type="fig" rid="fig6">Figure 6</xref>). The immediate visual access would also allow more embodied response modalities such as hand gestures, capturing the bidirectional sensorimotor link between perception and action. Thus, by tightly controlling the shared perceptual evidence and closely monitoring individual responses, and at the same time embedding the interaction into a naturalistic social context, the behavioural and neural mechanisms of social influences on sensory processing in an individual brain can be elucidated.</p>
</sec>
<sec id="s5-4">
<title>Attention and social learning</title>
<p>The developmental psychology literature has long emphasised the role of the input provided in caregiver-child interactions in driving learning. Indeed, development is contingent on the quality of such social interactions, where caregivers, on the one hand, guide their children in terms of what aspects of the environment to attend to (<xref ref-type="bibr" rid="c38">Csibra and Gergely, 2009</xref>), and children, on the other hand, actively engage in these learning situations by choosing what, when, and from whom they want to learn (<xref ref-type="bibr" rid="c90">Mani and Ackermann (2018)</xref>; <xref ref-type="bibr" rid="c120">Ruggeri et al. (2019)</xref>; <xref ref-type="bibr" rid="c126">Smith et al. (2018)</xref>; <xref ref-type="bibr" rid="c108">Pelz and Kidd (2020)</xref>. Consequently, the information children engage with, attend to, and learn from depends on their ability to observe and navigate not just their own actions but also those of others. There is, therefore, a critical need to investigate learning in the context of social interactions from which children learn.</p>
<p>The DIP provides a unique opportunity to study how children explore the world around them and how such exploration drives selective attention and learning in social interactions (<xref ref-type="bibr" rid="c17">Bothe et al., 2024</xref>). While previous studies highlight the factors (e.g., uncertainty, novelty) that drive exploration and learning in isolated contexts, more recent work suggests that exploration and attention to, e.g., faces or objects in more social contexts differs dramatically from isolated contexts. For instance, adults spend only around a fifth of the time looking at people’s faces in natural settings, e.g., walking around a University campus, relative to tasks where participants are presented with faces on a screen (<xref ref-type="bibr" rid="c51">Foulsham et al. (2011)</xref>; see <xref ref-type="bibr" rid="c118">Risko et al. (2016)</xref>, fora review). Similarly, while the developmental literature has touted the importance of gaze following in early infancy, recent tasks examining natural caregiver-child interactions found that children rarely follow the gaze of their caregivers and tend to be more egocentric in their interactions with others (<xref ref-type="bibr" rid="c87">Madhavan et al., 2025</xref>; <xref ref-type="bibr" rid="c150">Yu and Smith, 2013</xref>).</p>
<p>While such studies offer valuable insights into the dynamics of social interactions, they offer little possibility of controlling the environment presented to participants, especially regarding the timing and presentation of visual and auditory stimuli related to participants’ attention to the world around them. In contrast, the DIP allows researchers to continuously monitor (i) children’s attention to and exploration of varying visual and auditory stimuli presented on the screen during interaction with others, (ii) the extent to which one partner’s attention to an object on the screen influences the others’ exploration and sustained attention towards the same object and (iii) children’s learning of information provided in such social contexts. Moreover, the transparent and dynamic nature of the DIP facilitates the study of individual gaze patterns within social contexts and across development. For example, we integrated mobile eyetracking with the DIP (here, DIPc) in children between 4- to 5-years of age, enabling the assessment of the timing and accuracy of children’s visual attention to stimuli on screen or their interaction partner behind the screen, as well as the modulation of their visual attention in response to visual or auditory stimuli presented via the loudspeaker in real-time. Preliminary data from such tasks finds that children do fixate on their social partners’ faces during the task but spend less than a fifth of the time attending to their partner’s face relative to the objects on the screen (see <xref ref-type="fig" rid="fig7">Figure 7</xref>). This mirrors findings with adults walking around a University campus (<xref ref-type="bibr" rid="c51">Foulsham et al., 2011</xref>). The parallel between findings with adults in natural settings and children in the DIP speaks to similar availability and access to the social partner’s face across the two settings.</p>
<p>Furthermore, the DIPc also records when participants interact with particular objects on the screen by tapping them. In other work, we allowed children and their social partners to tap on their chosen objects on the screen. We followed children’s fixations and sustained attention to these objects based on whether they had tapped on them or whether their partner had tapped on them. We found that children fixated objects prior to their tapping on this object, highlighting the timeline of their decision to tap on this object. They also fixated objects that their partner tapped on prior to their partner actually tapping on this object, likely due to their social partner’s manual hand and arm movements and gaze towards the to-be-tapped object (see <xref ref-type="fig" rid="fig7">Figure 7</xref>, where children fixate the tapped object more than other objects, even before an object is tapped upon - as indicated by the vertical line - regardless of whether the child or their social partner tapped on this object). This observation speaks to the accessibility of the manual actions, and consequently, intentions of social partners in such paradigms.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7</label>
<caption><title>Eyetracking data collected using the DIPc.</title>
<p><bold>(A)</bold> The proportion of time children spent looking at their social partner’s face relative to the images on screen across the trial. The lines depict the mean and shaded area the SE across the trial. The vertical line indicates the point at which children or their partner tapped on one of the images onscreen. <bold>(B)</bold> Children’s proportion of looking at the image that was chosen, i.e., tapped on, across trials based on whether they or their social partner in the task chose the image. The vertical line indicates the point at which children or their partner tapped on one of the images onscreen. Across trials where they or their social partner tapped on an image, children fixated this image prior to it being chosen showcasing the extent to which children were able to pre-empt their partner’s choice given the transparency of the DIPc setup.</p></caption>
<graphic xlink:href="2rcknv1_fig7.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>In summary, the DIP allows precise experimental control in more naturalistic social settings, where children have continuous access to a range of cues from the social partner’s face and manual actions, while researchers similarly continuously monitor whom and what children are attending to and learning from. Embedding future studies in platforms like the DIP, we believe, has real promise of transforming our understanding of early sociocognitive development.</p>
</sec>
</sec>
<sec id="s6" sec-type="discussion">
<title>Discussion</title>
<p>We are social beings, and our interactions with the world present a constantly changing, dynamic exchange of social cues, facial expressions, actions and language. Understanding how behaviour and cognition play out in such rich settings requires capturing the complexity of real interactions in our experimental paradigms. At the same time, to examine complex social interactions with the degree of detail currently available in the research to-date, there is a need to integrate experimental control into social settings and enable researchers to collect a range of behavioural and neurophysiological indices of cognitive processing in natural social interactions.</p>
<sec id="s6-1">
<title>Advantages of Dyadic Interaction Platform</title>
<p>To meet this need, we developed the Dyadic Interaction Platform (DIP) - an innovative experimental environment that allows researchers to study interactions between two human or non-human primate participants. The platform features a shared transparent workspace that both participants can manipulate, enabling real-time, interactive engagement. Participants face each other, separated by a transparent touchscreen where visual stimuli can be presented with high temporal precision within the view of a social partner so that participants can easily attend to both sources of information. The DIP offers several key advantages over previous dyadic setups: (i) precise experimental control over stimuli and interactions, (ii) salient face-to-face engagement and social gaze monitoring, (iii) continuous access to the partner’s actions, decisions and behaviour, and (iv) joint manipulation of a shared workspace for cooperative or competitive tasks. The platform, therefore, presents a significant step forward in studying social behaviour in more naturalistic yet controlled settings (<xref ref-type="bibr" rid="c46">Fan et al., 2021</xref>), and enables engaging game-like paradigms that are increasingly utilized to study cognition (<xref ref-type="bibr" rid="c2">Allen et al., 2024</xref>).</p>
<p>We presented four different instantiations of the platform which integrate a variety of recording devices to capture rich multi-dimensional behavioural, physiological, and neural data, simultaneously from two participants. The different instantiations vary in terms of the visual displays and the kinds of behavioural responses that can be recorded (touchscreens, computer mice, joysticks, head-mounted or wall-mounted eyetrackers, video and audio recordings), and allow for a diverse range of participant groups to be examined (human adults and children, nonhuman primates, and mixed-species dyads). Some instantiations also record intracranial neural signals, EEG and EMG data synced to the behavioural devices listed above. The four instantiations showcase the remarkable flexibility in design choices for future studies, which can be tailored to the particular requirements of the study population, research question, and available resources. For instance, while studies with younger, less dexterous participants may rely more on touchscreens than mice or joysticks, such studies may be constrained in duration given the motor costs involved in touchscreen responses. For older study participants, joysticks or mice may provide a more suitable and easy-to- use interface while allowing participants to continuously follow each other’s behaviour. Along the same lines, the research question that depends on realistic movement costs might require real arm reaching to a touchscreen (or a spring-loaded joystick), while studying effects of agency or sensorimotor adaptation necessitates a dissociation between the real movement and its sensory consequences, achievable via a mouse or a joystick interface. Similarly, researchers can choose between wearable or mounted eyetrackers, depending on the degree of precision, flexibility, analysis effort and participant comfort desired while monitoring participants’ eye movements during the task. Finally, we highlight recent advancements in machine learning that we have integrated into the preprocessing pipeline to make analysis of such complex datasets more efficient, especially regarding face and object detection and posture estimation during different tasks.</p>
<p>To illustrate the capabilities of DIP, we also presented example data collected from different DIP instantiations. The breadth of paradigms and research questions outlined above highlights the range of possibilities open to researchers interested in social cognition - from social learning in young human children to dynamic cooperation and competitive turn-taking in macaque dyads. In what follows, we briefly list the key insights of dynamic dyadic behaviour from each paradigm that would not have been possible without the features of the DIP highlighted above.</p>
<p>First, we examined how dyads coordinate their decisions in the transparent version of the classical Bach-or-Stravinsky economic game. In this game, both players receive larger rewards for converging to one choice, with the partner whose option they converged to benefiting more. Since players could observe each other in this task, they quickly learned to coordinate their choices based on their partner’s action cues. Thus, players could dynamically integrate their partners’ actions in their own choices, with macaques converging on a choice based on which of the two monkeys was the first to reach to its preferred option. In a second paradigm, we examined how participants collected points in a competitive foraging task based on the proximity of their competitor to one of several possible targets. We showed that participants could use their knowledge of their competitor’s position in the task to navigate to points further away from their competitor, once again demonstrating how participants can integrate a variety of cues online in their social decision-making. Indeed, this study showed how participants changed their decision online in the task based on information about their competitor’s path across the screen. Taken together, the results showcase the advantages of including transparency and dynamicity in the platform, where continuous access to the actions and decisions of their partners helped players to optimize their strategies (<italic>cf</italic>. <xref ref-type="bibr" rid="c94">Moeller et al., 2023</xref>; <xref ref-type="bibr" rid="c85">Lewen et al., 2025</xref>). Fine-grained analysis of reaction times and movement trajectories recorded continuously through the task helps elucidating the underlying dynamics of cognitive processes and mutual dependencies rather than only discrete decision endpoints (<xref ref-type="bibr" rid="c47">Ferrari-Toniolo et al., 2019</xref>; <xref ref-type="bibr" rid="c68">Hawkins and Goldstone, 2016</xref>; <xref ref-type="bibr" rid="c92">McDonald et al., 2019</xref>; <xref ref-type="bibr" rid="c146">Yoo et al., 2021a</xref>,<xref ref-type="bibr" rid="c147">b</xref>, <xref ref-type="bibr" rid="c148">2020</xref>).</p>
<p>We also proposed extending the platform to incorporate a truly transparent, face-to-face version of the novel continuous perceptual report paradigm (<xref ref-type="bibr" rid="c123">Schneider et al., 2024</xref>), where participants track and indicate how confident they are of the perceived direction of a noisy sensory pattern. We demonstrated that the DIP makes it possible to present and measure continuous perceptual reports of each participant on both sides of the transparent display. This will be an important step forward in terms of experimental design, especially given previous findings that continuous access to information about other’s choices in decision tasks influences individual decisions differently from when such information is presented serially (<xref ref-type="bibr" rid="c109">Pescetelli and Yeung, 2020</xref>, <xref ref-type="bibr" rid="c110">2022</xref>). We, therefore, see enormous potential in such an extension to examine how social information can influence even the very early stages of sensory processing. Indeed, our study of young children’s allocation of attention to their social partners and objects in the shared visual environment targets precisely this question. We found - first - that children fixate on their social partner’s face during the task, albeit less than previously assumed in studies presenting children with static, unidirectional face stimuli (<xref ref-type="bibr" rid="c87">Madhavan et al., 2025</xref>). At the same time, we found that continuous access to the actions and behaviour of their social partner meant that children could preempt their partner’s focus of attention - by following their gaze and hand movements - and fixate on the object that their partner would subsequently tap on even before the partner tapped on this object. Taken together, findings such as these highlight (a) the advantages to be gained from examining behaviour in transparent settings and (b) the potential pitfalls when drawing conclusions about, e.g., children’s prioritised attention to faces from studies using unidirectional stimuli (see also <xref ref-type="bibr" rid="c51">Foulsham et al. (2011)</xref> for similar findings with adults).</p>
<p>One key advantage of the DIP mutual face-to-face and action visibility is its suitability for comparative research between humans and non-human primates. Like many natural interactions (<xref ref-type="bibr" rid="c44">Dugatkin etal., 1992</xref>; <xref ref-type="bibr" rid="c101">Noe, 2006</xref>; <xref ref-type="bibr" rid="c43">van Doorn etal., 2014</xref>), most DIP-based paradigms rely, at least in part, on real-time observation of a partner’s actions and outcomes, allowing for direct social interaction without requiring abstract representations of the partner’s presence, intentions, or complex inferences based on past experiences. This is particularly important for studying nonhuman primates, who may be underperforming in tasks that require predicting actions and outcomes based solely on interaction history or abstract cues (<xref ref-type="bibr" rid="c22">Brosnan et al., 2010</xref>, <xref ref-type="bibr" rid="c23">2012</xref>, <xref ref-type="bibr" rid="c24">2017</xref>; <xref ref-type="bibr" rid="c103">Ong et al., 2021</xref>; <xref ref-type="bibr" rid="c49">Formaux et al., 2022</xref>). By enabling immediate, visually grounded social exchanges, the DIP provides a more ecologically valid and accessible framework for investigating shared and divergent mechanisms of social cognition and coordination across species.</p>
</sec>
<sec id="s6-2">
<title>Limitations and outlook</title>
<p>While the different versions of the DIP demonstrate the flexibility of the platform in terms of experimental designs and the complexity of data that can be acquired, the results document how behaviour is impacted by the social context in which tasks are presented. Our findings stress the need to examine behaviour and cognition in controlled social settings. At the same time, we acknowledge the limitations of the platform as it is currently conceptualised and implemented, which we discuss in detail next.</p>
<p>Perhaps the first criticism that can be levelled at the platform concerns the ecological validity of the experimental designs and social settings outlined above and possible within the platform. Indeed, unless we are on a command deck of a futuristic spaceship, real social settings do not necessarily include floating transparent screens where information is presented to both participants simultaneously. To what extent can we assume that the DIP captures processing in natural social settings? Real-world situations often present individuals with objects in their environment that their social partner can choose to attend to or not. Consider two individuals going to grasp a door handle at the same time. They need to consider the intentions and actions of their social partner. The DIP emulates such situations, presenting participants a very similar context to that described above, but with the bonus that the researchers can manipulate the timing and contingencies of the setting. Indeed, some of our past studies have observed young infants and their caregivers interacting with one another in a play setting (<xref ref-type="bibr" rid="c87">Madhavan et al., 2025</xref>). While studies in real play settings may portend higher ecological validity than the paradigms described above, they are limited in terms of the timing and presentation of stimuli, e.g., the number of objects that can be presented or when during the task such objects are presented. We see such experimental control in social settings as the primary advantage of the DIP, while acknowledging the compromise between the limited ecological validity of the platform relative to completely natural, free-flowing social interactions.</p>
<p>The DIP and modern augmented reality approaches are similar in that they both allow overlaid digital interaction while maintaining visual contact with the environment and the interaction partner(s). In comparison to augmented reality environments, the biggest limitation of the DIP is its non-expandability of the digital space. Unlike augmented reality devices, which can display digital content in three dimensions overlaid on the physical environment, the DIP is limited to two dimensions on a vertical plane of the screen. At the same time, using augmented reality requires attaching additional devices to the participant’s head, while the DIP eliminates the need for bulky goggles that affect head movement or face visibility. This advantage, combined with the fact that the digital space provided by the DIP is inherently synchronised between participants, makes the DIP much more accessible.</p>
<p>Close face-to-face real-time interactions, such as reaching to and manipulating the same targets on the shared touchscreen (and almost touching a partner’s hand), create a highly salient social context. If the experimental paradigm is configured such that the facial signals and other subtle postural cues are relevant, the advantage of seeing both task stimuli and an interaction partner in the same line of sight is undeniable. However, in tasks that use more abstract, mouse or joystick- driven cursors or avatars, where the actual interaction takes place in a “virtual plane” on the screen, the relevance of face visibility becomes less clear. Indeed, our experience suggests that in such tasks, virtual avatars are very quickly imbued with agency, and when the task load is high, socio- emotional signals of the actual partner receive less attention. Nevertheless, we argue that the undeniable physical presence of the actual interactive partner on the other side provides a highly salient social context, even when most of actual task-related interaction takes place on a virtual plane. Future studies leveraging advanced video or EMG analysis will further clarify the role of facial socio-emotional cues in such settings.</p>
<p>We also note that the transparent display offers the advantage of immediate face-to-face and action visibility but comes with some inherent limitations. Independent of the specific implementation or technology used, the visibility (contrast) of the stimuli depends on the background. Although this is true for non-transparent modes of presentation, the control over the background of the transparent display is more limited because typically at least a part of the background will be the other participant’s face, arms, and clothes. Thus, the backdrop, as well as the illumination of the room, may become an important experimental parameter that might influence replicability. Furthermore, given the high transmissivity of the projection film, a second image can be formed behind the screen either on the opposing participant oron the floor. These images are usually dim, distorted, and blurry, but can be distracting to participants. We note above several workarounds to these issues, e.g., a patterned carpet can help to reduce contrast on the floor and make the reflected images less distracting to the participants. We also see high potential in terms of the future in the use of glass that can switch between transparent and opaque displays, allowing researchers to flexibly manipulate transmissivity as required in their paradigms. Such switchable glass could then also be used to flexibly move between independent and joint stimulus presentation or create asymmetry in the available information across participants, which allows for exciting possibilities for future research.</p>
<p>The DIP has the potential to elucidate the impact of individual differences in social interaction behaviours. Psychometric personality measures and additional cognitive tests can be used to predict indices of accumulated interaction behaviours, which in our initial experience show substantial reliability. Due to the continuous stream of behaviour and the perceptibility of the interaction partner, the DIP might have an advantage over other lab paradigms to study social behaviour, like turn-based economic games, which show only weak relationships with personality traits (<xref ref-type="bibr" rid="c152">Zhao and Smillie, 2015</xref>). To decompose dyadic behaviours in the DIP into actor, partner, and relationship effects, groups of participants can be tested in a round robin design, which can then be analysed via Social Relations Modelling (<xref ref-type="bibr" rid="c5">Back et al., 2023</xref>). Hormonal measures taken in the context of a DIP task can also be used to explain individual differences in interaction behaviours. For example, depending on the strategy of the interaction partner, the <italic>competitive foraging task</italic> can be expected to trigger reactive increases in hormones like testosterone, which responds to social contests, or cortisol, which responds to stressful challenges. These endocrine responses are accessible in a non-invasive manner from saliva sampling (<xref ref-type="bibr" rid="c19">Botzet et al., 2024</xref>).</p>
<p>The continuous, dynamic interactions afforded by DIP result in rich and heterogeneous data. This is a huge advantage, but also a challenge compared to classical paradigms, where only one or few bits of information (e.g. a simple button press, corresponding to a discrete choice) are acquired per trial. Instead of the simple and precise timing of stimuli or events, to which, e.g., neural data such as ERPs or neuronal firing can be time-locked, the researcher now needs to extract the time points or periods of interest from continuous, often non-stationary, data, and to classify spontaneous emerging interactions into meaningful classes. Typically, there is no unique solution and one needs to find a suitable compromise between generality, specificity, and other non-optimal assessment factors of the time points or periods. Here, Bayesian inference can be especially informative because it can define a precise model of the feature one is searching for (e.g., change of movement direction or slowing down to indicate uncertainty of the decision). Bayesian inference can then, with relatively sparse data, provide not only a single maximum-likelihood estimate but the full posterior probability with its uncertainty. Depending on the analysis, one can then concentrate on the “clear” time points or periods or accept larger uncertainty in the classification. Complementary, machine learning approaches could also be adapted to parse the rich behaviour into classes. Overall, these novel approaches allow full use of the rich data provided from the continuous recording of dyadic, mutually coupled behaviour in a shared environment.</p>
<p>One promising approach to analysing hyperscanning data from a DIP setup is to measure informational alignment rather than direct brain-to-brain synchrony. The informational alignment estimates the similarity of perceptual and cognitive representations, and can be detected using “inter-brain representational similarity analysis” (IRSA; <xref ref-type="bibr" rid="c143">Varlet and Grootswagers (2024)</xref>). In this analysis approach time-frequency profiles of same and different objects are submitted to a RSA analysis using data from different participants. Compared with inter-brain synchrony (IBS), such an inter-brain RSA detected significantly more same objects vs. different objects effects in both amplitude and phase. This provides first evidence for higher sensitivity to informational alignment in hyperscanning participants using IRSA.</p>
<p>Finally, while the DIP has been designed and used primarily for studying dyadic interactions, it is not inherently limited to two-way interactions. The setup allows for the inclusion of multiple participants on each side of the screen or the integration of observers, making it a viable platform for investigating group interactions in future research.</p>
</sec>
<sec id="s6-3">
<title>Conclusion</title>
<p>We present a novel dyadic interaction platform that allows researchers to study naturalistic, dynamic social interactions in different subject populations in diverse tasks while collecting a rich multi-dimensional array of behavioural, physiological and neural data. The DIP’s exceptional versatility comes from integrating different stimulus presentation and recording devices, thereby allowing researchers to flexibly tailor the platform to their research question and measures of cognitive processing they are particularly interested in. We include several example tasks that document both the transparency and flexibility of the DIP as well as the more fine-grained understanding of the dynamics of social behaviour gained by providing individuals continuous access to the decisions, social signals, and actions of their partners. Indeed, the examples outlined here document the real need to study rich, dynamic and complex settings, showcasing how performance is impacted at the millisecond level by embedding behaviour into such settings. We look eagerly forward to the advances in our understanding of primate social behaviour that such platforms can provide.</p>
</sec>
</sec>
</body>
<back>
<sec id="s8" sec-type="data-availability">
<title>Availability of data, materials, and code</title>
<p>The BoS dataset described in the section on Transparent economic games and links to the GitHub code repositories are available at public OSF repository <ext-link ext-link-type="uri" xlink:href="https://osf.io/f5u8z/">https://osf.io/f5u8z/</ext-link>.</p>
<p>The data and code related to the Competitive Foraging task described in the section on Continuous strategic interactions will be available at public OSF repository <ext-link ext-link-type="uri" xlink:href="https://osf.io/8r6e2/">https://osf.io/8r6e2/</ext-link>.</p>
<p>The data and code related to the Cooperation-Competition Foraging study described in the section on Continuous strategic interactions will be available at public OSF repository <ext-link ext-link-type="uri" xlink:href="https://osf.io/56hw7">https://osf.io/56hw7</ext-link>.</p>
<p>The data and code related to the CPR DIP dataset described in the section on Perceptual decisionmaking in dyadic context will be available at public OSF repository <ext-link ext-link-type="uri" xlink:href="https://osf.io/8r6e2/">https://osf.io/8r6e2/</ext-link>.</p>
<p>The dataset and code described in the section on Attention and social learning are available at public OSF repository <ext-link ext-link-type="uri" xlink:href="https://osf.io/6dven/">https://osf.io/6dven/</ext-link>.</p>
</sec>
<ack>
<title>Acknowledgments</title>
<p>We thank Dr. Chris Schloegl for efficient scientific coordination of the Leibniz ScienceCampus Primate Cognition and the Collaborative Research Center SFB 1528 “Cognition of Interaction”. We also thank Dr. Aleksandra Bovt, scientific coordinator of the RTG 2906 “Curiosity”, and the members of the “Cognition of Interaction” and “Curiosity” consortia for stimulating discussions.</p>
<p>This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project ID 454648639: SFB 1528 (Collaborative Research Center) “Cognition of Interaction” and Project ID 50280717: RTG 2906 “Curiosity”, the Leibniz ScienceCampus Primate Cognition, and Leibniz Collaborative Excellence grant K265/2019 “Neurophysiological mechanisms of primate interactions in dynamic sensorimotor settings”.</p>
</ack>
<sec id="d1e2350" sec-type="additional-information">
<title>Additional information</title>
<sec id="s7">
<title>Author contributions</title>
    <p><bold>Sebastian Isbaner</bold>: Conceptualization (DIP), Conceptualization, Methodology, Software, Formal analysis, Investigation, Data Curation, Writing—original draft, Writing—review &amp; editing, Visualisation. <bold>Raymundo Báez-Mendoza</bold>: Writing—original draft, Writing—review &amp; editing. <bold>Ricarda Bothe</bold>: Methodology, Formal analysis, Investigation, Data Curation, Writing—original draft, Writing—review &amp; editing, Visualisation. <bold>Sarah Eiteljörge</bold>: Writing—original draft, Writing—review &amp; editing. <bold>Anna Fischer</bold>: Methodology, Writing—original draft, Writing—review &amp; editing. <bold>Alexander Gail</bold>: Conceptualization (DIP), Methodology, Writing—original draft, Writing—review &amp; editing, Supervision of individual projects, Funding acquisition. <bold>Jan Gläscher</bold>: Writing—original draft, Writing—review &amp; editing, Funding acquisition. <bold>Hannah Lüschen</bold>: Methodology, Investigation. <bold>Sebastian Moeller</bold>: Conceptualization (DIP), Methodology, Software, Formal analysis, Investigation, Writing—original draft, Writing—review &amp; editing, Visualisation. <bold>Lars Penke</bold>: Writing—original draft, Writing—review &amp; editing, Supervision of individual projects, Funding acquisition. <bold>Viola Priesemann</bold>: Methodology, Writing—original draft, Writing—review &amp; editing, Supervision of individual projects, Funding acquisition. <bold>Johannes Ruß</bold>: Writing—original draft, Writing—review &amp; editing. <bold>Anne Schacht</bold>: Methodology, Supervision of individual projects, Funding acquisition. <bold>Felix Schneider</bold>: Methodology, Software, Formal analysis, Investigation, Writing—original draft, Writing—review &amp; editing, Visualisation. <bold>Neda Shahidi</bold>: Methodology, Software, Formal analysis, Investigation, Writing—original draft, Writing—review &amp; editing, Visualisation, Supervision of individual projects. <bold>Stefan Treue</bold>: Conceptualization (DIP), Methodology, Writing—review &amp; editing, Supervision of individual projects, Funding acquisition. <bold>Michael Wibral</bold>: Writing—original draft, Writing—review &amp; editing, Funding acquisition. <bold>Annika Ziereis</bold>: Methodology, Writing—original draft, Writing—review &amp; editing. <bold>Julia Fischer</bold>: Writing—original draft, Writing—review &amp; editing, Funding acquisition. <bold>Igor Kagan</bold>: Conceptualization (DIP), Conceptualization, Methodology, Formal analysis, Data Curation, Writing—original draft, Writing—review &amp; editing, Visualisation, Supervision of individual projects, Supervision, Project administration, Funding acquisition. <bold>Nivedita Mani</bold>: Conceptualization (DIP), Conceptualization, Methodology, Formal analysis, Writing—original draft, Writing—review &amp; editing, Visualisation, Supervision of individual projects, Supervision, Project administration, Funding acquisition.</p>
</sec>
</sec>
    <sec id="nt1">
        <title>Note</title>
        <p>This reviewed preprint has been updated to correct a spelling error in an author name.</p>
    </sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Achaibou</surname> <given-names>A</given-names></string-name>, <string-name><surname>Pourtois</surname> <given-names>G</given-names></string-name>, <string-name><surname>Schwartz</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vuilleumier</surname> <given-names>P</given-names></string-name></person-group>. <article-title>Simultaneous recording of EEG and facial muscle reactions during spontaneous emotional mimicry</article-title>. <source>Neuropsychologia</source>. <year>2008</year>; <volume>46</volume>(<issue>4</issue>):<fpage>1104</fpage>–<lpage>1113</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.10.019</pub-id>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Allen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Brändle</surname> <given-names>F</given-names></string-name>, <string-name><surname>Botvinick</surname> <given-names>M</given-names></string-name>, <string-name><surname>Fan</surname> <given-names>JE</given-names></string-name>, <string-name><surname>Gershman</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Gopnik</surname> <given-names>A</given-names></string-name>, <string-name><surname>Griffiths</surname> <given-names>TL</given-names></string-name>, <string-name><surname>Hartshorne</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Hauser</surname> <given-names>TU</given-names></string-name>, <string-name><surname>Ho</surname> <given-names>MK</given-names></string-name>, <string-name><surname>de Leeuw</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>WJ</given-names></string-name>, <string-name><surname>Murayama</surname> <given-names>K</given-names></string-name>, <string-name><surname>Nelson</surname> <given-names>JD</given-names></string-name>, <string-name><surname>van Opheusden</surname> <given-names>B</given-names></string-name>, <string-name><surname>Pouncy</surname> <given-names>T</given-names></string-name>, <string-name><surname>Rafner</surname> <given-names>J</given-names></string-name>, <string-name><surname>Rahwan</surname> <given-names>I</given-names></string-name>, <string-name><surname>Rutledge</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Sherson</surname> <given-names>J</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Using games to understand the mind</article-title>. <source>Nature Human Behaviour</source>. <year>2024</year> <month>Jun</month>; <volume>8</volume>(<issue>6</issue>):<fpage>1035</fpage>–<lpage>1043</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41562-024-01878-9">https://www.nature.com/articles/s41562-024-01878-9</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41562-024-01878-9</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Azzi</surname> <given-names>JCB</given-names></string-name>, <string-name><surname>Sirigu</surname> <given-names>A</given-names></string-name>, <string-name><surname>Duhamel</surname> <given-names>JR</given-names></string-name></person-group>. <article-title>Modulation of value representation by social context in the primate orbitofrontal cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2012</year> <month>Feb</month>; <volume>109</volume>(<issue>6</issue>):<fpage>2126</fpage>–<lpage>2131</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/content/109/6/2126">http://www.pnas.org/content/109/6/2126</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.1111715109</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Baar</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>LJ</given-names></string-name>, <string-name><surname>Sanfey</surname> <given-names>AG</given-names></string-name></person-group>. <article-title>The computational and neural substrates of moral strategies in social decision-making</article-title>. <source>Nature Communications</source>. <year>2019</year> <month>Apr</month>; <volume>10</volume>(<issue>1</issue>):<fpage>1483</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-019-09161-6">https://www.nature.com/articles/s41467-019-09161-6</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41467-019-09161-6</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Back</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Branje</surname> <given-names>S</given-names></string-name>, <string-name><surname>Eastwick</surname> <given-names>PW</given-names></string-name>, <string-name><surname>Human</surname> <given-names>LJ</given-names></string-name>, <string-name><surname>Penke</surname> <given-names>L</given-names></string-name>, <string-name><surname>Sadikaj</surname> <given-names>G</given-names></string-name>, <string-name><surname>Slatcher</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Thielmann</surname> <given-names>I</given-names></string-name>, <string-name><surname>van Zalk</surname> <given-names>MHW</given-names></string-name>, <string-name><surname>Wrzus</surname> <given-names>C</given-names></string-name></person-group>. <article-title>Personality and social relationships: What do we know and where do we go?</article-title> <source>Personality Science</source>. <year>2023</year>; <volume>4</volume>:<elocation-id>e7505</elocation-id>. doi: <pub-id pub-id-type="doi">10.5964/ps.7505</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bahrami</surname> <given-names>B</given-names></string-name>, <string-name><surname>Olsen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Latham</surname> <given-names>PE</given-names></string-name>, <string-name><surname>Roepstorff</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rees</surname> <given-names>G</given-names></string-name>, <string-name><surname>Frith</surname> <given-names>CD</given-names></string-name></person-group>. <article-title>Optimally Interacting Minds</article-title>. <source>Science</source>. <year>2010</year> <month>Aug</month>; <volume>329</volume>(<issue>5995</issue>):<fpage>1081</fpage>–<lpage>1085</lpage>. <ext-link ext-link-type="uri" xlink:href="http://science.sciencemag.org/content/329/5995/1081">http://science.sciencemag.org/content/329/5995/1081</ext-link>, doi: <pub-id pub-id-type="doi">10.1126/science.1185718</pub-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Ballesta</surname> <given-names>S</given-names></string-name>, <string-name><surname>Duhamel</surname> <given-names>JR</given-names></string-name></person-group>. <article-title>Rudimentary empathy in macaques’ social decision-making</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2015</year> <month>Nov</month>; p. <fpage>201504454</fpage>. <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/content/early/2015/11/24/1504454112">http://www.pnas.org/content/early/2015/11/24/1504454112</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.1504454112</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ballesteros</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Ramirez</surname> <given-names>V GM</given-names></string-name>, <string-name><surname>Moreira</surname> <given-names>F</given-names></string-name>, <string-name><surname>Solano</surname> <given-names>A</given-names></string-name>, <string-name><surname>Pelaez</surname> <given-names>CA</given-names></string-name></person-group>. <article-title>Facial emotion recognition through artificial intelligence</article-title>. <source>Frontiers in Computer Science</source>. <year>2024</year> <month>Jan</month>; <fpage>6</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2024.1359471/full">https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2024.1359471/full</ext-link>, doi: <pub-id pub-id-type="doi">10.3389/fcomp.2024.1359471</pub-id>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Aitchison</surname> <given-names>L</given-names></string-name>, <string-name><surname>Moran</surname> <given-names>R</given-names></string-name>, <string-name><surname>Herce Castanon</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rafiee</surname> <given-names>B</given-names></string-name>, <string-name><surname>Mahmoodi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lau</surname> <given-names>JYF</given-names></string-name>, <string-name><surname>Latham</surname> <given-names>PE</given-names></string-name>, <string-name><surname>Bahrami</surname> <given-names>B</given-names></string-name>, <string-name><surname>Summerfield</surname> <given-names>C</given-names></string-name></person-group>. <article-title>Confidence matching in group decision-making</article-title>. <source>Nature Human Behaviour</source>. <year>2017</year> <month>May</month>; <volume>1</volume>(<issue>6</issue>):<fpage>0117</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41562-017-0117">https://www.nature.com/articles/s41562-017-0117</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41562-017-0117</pub-id>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Frith</surname> <given-names>CD</given-names></string-name></person-group>. <article-title>Making better decisions in groups</article-title>. <source>Royal Society Open Science</source>. <year>2017</year> <month>Aug</month>; <volume>4</volume>(<issue>8</issue>):<fpage>170193</fpage>. <ext-link ext-link-type="uri" xlink:href="https://royalsocietypublishing.org/doi/10.1098/rsos.170193">https://royalsocietypublishing.org/doi/10.1098/rsos.170193</ext-link>, doi: <pub-id pub-id-type="doi">10.1098/rsos.170193</pub-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Battaglia-Mayer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Caminiti</surname> <given-names>R</given-names></string-name>, <string-name><surname>Lacquaniti</surname> <given-names>F</given-names></string-name>, <string-name><surname>Zago</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Multiple Levels of Representation of Reaching in the Parieto-frontal Network</article-title>. <source>Cerebral Cortex</source>. <year>2003</year> <month>Oct</month>; <volume>13</volume>(<issue>10</issue>):<fpage>1009</fpage>–<lpage>1022</lpage>. <ext-link ext-link-type="uri" xlink:href="https://academic.oup.com/cercor/article/13/10/1009/372555">https://academic.oup.com/cercor/article/13/10/1009/372555</ext-link>, doi: <pub-id pub-id-type="doi">10.1093/cercor/13.10.1009</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baumgart</surname> <given-names>KG</given-names></string-name>, <string-name><surname>Byvshev</surname> <given-names>P</given-names></string-name>, <string-name><surname>Sliby</surname> <given-names>AN</given-names></string-name>, <string-name><surname>Strube</surname> <given-names>A</given-names></string-name>, <string-name><surname>König</surname> <given-names>P</given-names></string-name>, <string-name><surname>Wahn</surname> <given-names>B</given-names></string-name></person-group>. <article-title>Neurophysiological correlates of collective perceptual decision-making</article-title>. <source>European Journal of Neuroscience</source>. <year>2019</year>; <volume>n/a</volume>(<issue>n/a</issue>). <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/abs/10.1111/ejn.14545">https://onlinelibrary.wiley.com/doi/abs/10.1111/ejn.14545</ext-link>, doi: <pub-id pub-id-type="doi">10.1111/ejn.14545</pub-id>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Behrens</surname> <given-names>F</given-names></string-name>, <string-name><surname>Snijdewint</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Moulder</surname> <given-names>RG</given-names></string-name>, <string-name><surname>Prochazkova</surname> <given-names>E</given-names></string-name>, <string-name><surname>Sjak-Shie</surname> <given-names>EE</given-names></string-name>, <string-name><surname>Boker</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Kret</surname> <given-names>ME</given-names></string-name></person-group>. <article-title>Physiological synchrony is associated with cooperative success in real-life interactions</article-title>. <source>Scientific Reports</source>. <year>2020</year> <month>Nov</month>; <volume>10</volume>(<issue>1</issue>):<fpage>19609</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41598-020-76539-8">https://www.nature.com/articles/s41598-020-76539-8</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41598-020-76539-8</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Birmingham</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kingstone</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Human Social Attention</article-title>. <source>Annals of the New York Academy of Scienees</source>. <year>2009</year>; <volume>1156</volume>(<issue>1</issue>):<fpage>118</fpage>–<lpage>140</lpage>. <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.eom/doi/abs/10.1111/j.1749-6632.2009.04468.x">https://onlinelibrary.wiley.eom/doi/abs/10.1111/j.1749-6632.2009.04468.x</ext-link>, doi: <pub-id pub-id-type="doi">10.1111/j.1749-6632.2009.04468.x</pub-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Bohn</surname> <given-names>M</given-names></string-name>, <string-name><surname>Prein</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Ayikoru</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bednarski</surname> <given-names>FM</given-names></string-name>, <string-name><surname>Dzabatou</surname> <given-names>A</given-names></string-name>, <string-name><surname>Frank</surname> <given-names>MC</given-names></string-name>, <string-name><surname>Henderson</surname> <given-names>AME</given-names></string-name>, <string-name><surname>Isabella</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kalbitz</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kanngiesser</surname> <given-names>P</given-names></string-name>, <string-name><surname>Kejjafoglu</surname> <given-names>D</given-names></string-name>, <string-name><surname>Koymen</surname> <given-names>B</given-names></string-name>, <string-name><surname>Manrique-Hernandez</surname> <given-names>M</given-names></string-name>, <string-name><surname>Magazi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Mújica-Manrique</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ohlendorf</surname> <given-names>J</given-names></string-name>, <string-name><surname>Olaoba</surname> <given-names>D</given-names></string-name>, <string-name><surname>Pieters</surname> <given-names>W</given-names></string-name>, <string-name><surname>Pope-Caldwell</surname> <given-names>S</given-names></string-name>, <string-name><surname>Slocombe</surname> <given-names>K</given-names></string-name>, <etal>et al.</etal></person-group>, <conf-name>A universal of human social cognition</conf-name>: <article-title>Children from 17 communities process gaze in similar ways</article-title>. <publisher-name>OSF</publisher-name>; <year>2024</year>. <ext-link ext-link-type="uri" xlink:href="https://osf.io/z3ahv_v1">https://osf.io/z3ahv_v1</ext-link>, doi: <pub-id pub-id-type="doi">10.31234/osf.io/z3ahv</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boschet-Lange</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Scherbaum</surname> <given-names>S</given-names></string-name>, <string-name><surname>Pittig</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Temporal dynamics of costly avoidance in naturalistic fears: Evidence for sequential-sampling of fear and reward information</article-title>. <source>Journal of Anxiety Disorders</source>. <year>2024</year> <month>Apr</month>; <volume>103</volume>:<fpage>102844</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0887618524000203">https://www.sciencedirect.com/science/article/pii/S0887618524000203</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.janxdis.2024.102844</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Bothe</surname> <given-names>R</given-names></string-name>, <string-name><surname>Isbaner</surname> <given-names>S</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>X</given-names></string-name>, <string-name><surname>Kagan</surname> <given-names>I</given-names></string-name>, <string-name><surname>Gail</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mani</surname> <given-names>N</given-names></string-name></person-group>. <article-title>Little scientists &amp; social apprentices</article-title>: <source>Active word learning in dynamic social contexts using a transparent dyadic interaction platform</source>; <year>2024</year>, <ext-link ext-link-type="uri" xlink:href="https://osf.io/preprints/psyarxiv/fx2bg_v1">osf.io/preprints/psyarxiv/fx2bg_v1</ext-link>, doi: <pub-id pub-id-type="doi">10.31234/osf.io/fx2bg</pub-id>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boto</surname> <given-names>E</given-names></string-name>, <string-name><surname>Holmes</surname> <given-names>N</given-names></string-name>, <string-name><surname>Leggett</surname> <given-names>J</given-names></string-name>, <string-name><surname>Roberts</surname> <given-names>G</given-names></string-name>, <string-name><surname>Shah</surname> <given-names>V</given-names></string-name>, <string-name><surname>Meyer</surname> <given-names>SS</given-names></string-name>, <string-name><surname>Muñoz</surname> <given-names>LD</given-names></string-name>, <string-name><surname>Mullinger</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Tierney</surname> <given-names>TM</given-names></string-name>, <string-name><surname>Bestmann</surname> <given-names>S</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Moving magnetoencephalography towards real-world applications with a wearable system</article-title>. <source>Nature</source>. <year>2018</year>; <volume>555</volume>(<issue>7698</issue>):<fpage>657</fpage>–<lpage>661</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Botzet</surname> <given-names>LJ</given-names></string-name>, <string-name><surname>Kordsmeyer</surname> <given-names>TL</given-names></string-name>, <string-name><surname>Ostermann</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ruß</surname> <given-names>J</given-names></string-name>, <string-name><surname>Penke</surname> <given-names>L</given-names></string-name></person-group>. <article-title>Behavioural endocrinology in the social sciences</article-title>. <source>Kölner Zeitschrift für Soziologie und Sozialpsychologie</source>. <year>2024</year>; <volume>76</volume>:<fpage>649</fpage>–<lpage>680</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s11577-024-00945-3</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brookes</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Leggett</surname> <given-names>J</given-names></string-name>, <string-name><surname>Rea</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hill</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Holmes</surname> <given-names>N</given-names></string-name>, <string-name><surname>Boto</surname> <given-names>E</given-names></string-name>, <string-name><surname>Bowtell</surname> <given-names>R</given-names></string-name></person-group>. <article-title>Magnetoencephalography with optically pumped magnetometers (OPM-MEG): the next generation of functional neuroimaging</article-title>. <source>Trends in Neurosciences</source>. <year>2022</year> <month>Aug</month>; <volume>45</volume>(<issue>8</issue>):<fpage>621</fpage>–<lpage>634</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0166223622001023">https://www.sciencedirect.com/science/article/pii/S0166223622001023</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.tins.2022.05.008</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brosnan</surname> <given-names>SF</given-names></string-name>, <string-name><surname>Price</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Leverett</surname> <given-names>K</given-names></string-name>, <string-name><surname>Prétot</surname> <given-names>L</given-names></string-name>, <string-name><surname>Beran</surname> <given-names>M</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>BJ</given-names></string-name></person-group>. <article-title>Human and monkey responses in a symmetric game of conflict with asymmetric equilibria</article-title>. <source>Journal of Economic Behavior &amp; Organization</source>. <year>2017</year> <month>Oct</month>; <volume>142</volume>:<fpage>293306</fpage>. <ext-link ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0167268117302159">http://linkinghub.elsevier.com/retrieve/pii/S0167268117302159</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.jebo.2017.07.037</pub-id>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brosnan</surname> <given-names>SF</given-names></string-name>, <string-name><surname>Salwiczek</surname> <given-names>L</given-names></string-name>, <string-name><surname>Bshary</surname> <given-names>R</given-names></string-name></person-group>. <article-title>The interplay of cognition and cooperation</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>. <year>2010</year> <month>Sep</month>; <volume>365</volume>(<issue>1553</issue>):<fpage>2699</fpage>–<lpage>2710</lpage>. <ext-link ext-link-type="uri" xlink:href="https://royalsocietypublishing.org/doi/10.1098/rstb.2010.0154">https://royalsocietypublishing.org/doi/10.1098/rstb.2010.0154</ext-link>, doi: <pub-id pub-id-type="doi">10.1098/rstb.2010.0154</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brosnan</surname> <given-names>SF</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Beran</surname> <given-names>MJ</given-names></string-name></person-group>. <article-title>Old World monkeys are more similar to humans than New World monkeys when playing a coordination game</article-title>. <source>Proceedings Biological Sciences</source>. <year>2012</year> <month>Apr</month>; <volume>279</volume>(<issue>1733</issue>):<fpage>1522</fpage>–<lpage>1530</lpage>. doi: <pub-id pub-id-type="doi">10.1098/rspb.2011.1781</pub-id>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Buidze</surname> <given-names>T</given-names></string-name>, <string-name><surname>Sommer</surname> <given-names>T</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>K</given-names></string-name>, <string-name><surname>Fu</surname> <given-names>X</given-names></string-name>, <string-name><surname>Gläscher</surname> <given-names>J</given-names></string-name></person-group>, <article-title>Communication with Surprise - Computational and Neural Mechanisms for Non-Verbal Human Interactions</article-title>. <source>bioRxiv</source>; <year>2024</year>. <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2024.02.20.581193v1">https://www.biorxiv.org/content/10.1101/2024.02.20.581193v1</ext-link>, doi: <pub-id pub-id-type="doi">10.1101/2024.02.20.581193</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buidze</surname> <given-names>T</given-names></string-name>, <string-name><surname>Sommer</surname> <given-names>T</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>K</given-names></string-name>, <string-name><surname>Fu</surname> <given-names>X</given-names></string-name>, <string-name><surname>Gläscher</surname> <given-names>J</given-names></string-name></person-group>. <article-title>Expectation violations signal goals in novel human communication</article-title>. <source>Nature Communications</source>. <year>2025</year> <month>Feb</month>; <volume>16</volume>(<issue>1</issue>):<fpage>1989</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-025-57025-z">https://www.nature.com/articles/s41467-025-57025-z</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41467-025-57025-z</pub-id>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bullinger</surname> <given-names>AF</given-names></string-name>, <string-name><surname>Wyman</surname> <given-names>E</given-names></string-name>, <string-name><surname>Melis</surname> <given-names>AP</given-names></string-name>, <string-name><surname>Tomasello</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Coordination of chimpanzees (Pan troglodytes) in a stag hunt game</article-title>. <source>International Journal of Primatology</source>. <year>2011</year>; <volume>32</volume>:<fpage>1296</fpage>–<lpage>1310</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Báez-Mendoza</surname> <given-names>R</given-names></string-name>, <string-name><surname>Harris</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Schultz</surname> <given-names>W</given-names></string-name></person-group>. <article-title>Activity of striatal neurons reflects social action and own reward</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2013</year> <month>Oct</month>; <volume>110</volume>(<issue>41</issue>):<fpage>16634</fpage>–<lpage>16639</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/content/110/41/16634">http://www.pnas.org/content/110/41/16634</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.1211342110</pub-id>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Báez-Mendoza</surname> <given-names>R</given-names></string-name>, <string-name><surname>Schultz</surname> <given-names>W</given-names></string-name></person-group>. <article-title>Performance error-related activity in monkey striatum during social interactions</article-title>. <source>Scientific Reports</source>. <year>2016</year> <month>Nov</month>; <volume>6</volume>:<fpage>srep37199</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/srep37199">https://www.nature.com/articles/srep37199</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/srep37199</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Báez-Mendoza</surname> <given-names>R</given-names></string-name>, <string-name><surname>Vázquez</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Mastrobattista</surname> <given-names>EP</given-names></string-name>, <string-name><surname>Williams</surname> <given-names>ZM</given-names></string-name></person-group>. <article-title>Neuronal Circuits for Social Decision-Making and Their Clinical Implications</article-title>. <source>Frontiers in Neuroscience</source>. <year>2021</year>; <fpage>15</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2021.720294</pub-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname> <given-names>SWC</given-names></string-name>, <string-name><surname>Snyder</surname> <given-names>LH</given-names></string-name></person-group>. <article-title>The representations of reach endpoints in posterior parietal cortex depend on which hand does the reaching</article-title>. <source>Journal of Neurophysiology</source>. <year>2012</year> <month>May</month>; <volume>107</volume>(<issue>9</issue>):<fpage>2352</fpage>–<lpage>2365</lpage>. <ext-link ext-link-type="uri" xlink:href="http://jn.physiology.org/cgi/doi/10.1152/jn.00852.2011">http://jn.physiology.org/cgi/doi/10.1152/jn.00852.2011</ext-link>, doi: <pub-id pub-id-type="doi">10.1152/jn.00852.2011</pub-id>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname> <given-names>SWC</given-names></string-name></person-group>. <article-title>An Emerging Field of Primate Social Neurophysiology: Current Developments</article-title>. <source>eNeuro</source>. <year>2017</year> <month>Sep</month>; <volume>4</volume>(<issue>5</issue>):<elocation-id>Eneuro.0295-17.2017</elocation-id>. <ext-link ext-link-type="uri" xlink:href="http://www.eneuro.Org/content/4/5/ENEURO.0295-17.2017">http://www.eneuro.Org/content/4/5/ENEURO.0295-17.2017</ext-link>, doi: <pub-id pub-id-type="doi">10.1523/ENEURO.0295-17.2017</pub-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname> <given-names>SWC</given-names></string-name>, <string-name><surname>Fagan</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Toda</surname> <given-names>K</given-names></string-name>, <string-name><surname>Utevsky</surname> <given-names>AV</given-names></string-name>, <string-name><surname>Pearson</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Platt</surname> <given-names>ML</given-names></string-name></person-group>. <article-title>Neural mechanisms of social decision-making in the primate amygdala</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2015</year> <month>Dec</month>; <volume>112</volume>(<issue>52</issue>):<fpage>16012</fpage>–<lpage>16017</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/content/112/52/16012">http://www.pnas.org/content/112/52/16012</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.1514761112</pub-id>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname> <given-names>SWC</given-names></string-name>, <string-name><surname>Gariepy</surname> <given-names>JF</given-names></string-name>, <string-name><surname>Platt</surname> <given-names>ML</given-names></string-name></person-group>. <article-title>Neuronal reference frames for social decisions in primate frontal cortex</article-title>. <source>Nature Neuroscience</source>. <year>2013</year> <month>Feb</month>; <volume>16</volume>(<issue>2</issue>):<fpage>243</fpage>–<lpage>250</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.nature.com.3126-0.han.sub.uni-goettingen.de/neuro/journal/v16/n2/full/nn.3287.html">http://www.nature.com.3126-0.han.sub.uni-goettingen.de/neuro/journal/v16/n2/full/nn.3287.html</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/nn.3287</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cicmil</surname> <given-names>N</given-names></string-name>, <string-name><surname>Cumming</surname> <given-names>BG</given-names></string-name>, <string-name><surname>Parker</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Krug</surname> <given-names>K</given-names></string-name></person-group>. <article-title>Reward modulates the effect of visual cortical microstimulation on perceptual decisions</article-title>. <source>eLife</source>. <year>2015</year> <month>Sep</month>; <volume>4</volume>:<elocation-id>e07832</elocation-id>. doi: <pub-id pub-id-type="doi">10.7554/eLife.07832</pub-id>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cirillo</surname> <given-names>R</given-names></string-name>, <string-name><surname>Ferrucci</surname> <given-names>L</given-names></string-name>, <string-name><surname>Marcos</surname> <given-names>E</given-names></string-name>, <string-name><surname>Ferraina</surname> <given-names>S</given-names></string-name>, <string-name><surname>Genovesio</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Coding of Self and Other’s Future Choices in Dorsal Premotor Cortex during Social Interaction</article-title>. <source>Cell Reports</source>. <year>2018</year> <month>Aug</month>; <volume>24</volume>(<issue>7</issue>):<fpage>1679</fpage>–<lpage>1686</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S2211124718311203">https://linkinghub.elsevier.com/retrieve/pii/S2211124718311203</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.celrep.2018.07.030</pub-id>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cisek</surname> <given-names>P</given-names></string-name>, <string-name><surname>Green</surname> <given-names>AM</given-names></string-name></person-group>. <article-title>Toward a neuroscience of natural behavior</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2024</year> <month>Jun</month>; <volume>86</volume>:<fpage>102859</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0959438824000217">https://www.sciencedirect.com/science/article/pii/S0959438824000217</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.conb.2024.102859</pub-id>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Courage</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Reynolds</surname> <given-names>GD</given-names></string-name>, <string-name><surname>Richards</surname> <given-names>JE</given-names></string-name></person-group>. <article-title>Infants’ Attention to Patterned Stimuli: Developmental Change From 3 to 12 Months of Age</article-title>. <source>Child Development</source>. <year>2006</year>; <volume>77</volume>(<issue>3</issue>):<fpage>680</fpage>–<lpage>695</lpage>. <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8624.2006.00897.x">https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8624.2006.00897.x</ext-link>, doi: <pub-id pub-id-type="doi">10.1111/j.1467-8624.2006.00897.x</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Csibra</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gergely</surname> <given-names>G</given-names></string-name></person-group>. <article-title>Natural pedagogy</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2009</year> <month>Apr</month>; <volume>13</volume>(<issue>4</issue>):<fpage>148</fpage>–<lpage>153</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(09)00047-3">https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(09)00047-3</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.tics.2009.01.005</pub-id>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Czeszumski</surname> <given-names>A</given-names></string-name>, <string-name><surname>Eustergerling</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lang</surname> <given-names>A</given-names></string-name>, <string-name><surname>Menrath</surname> <given-names>D</given-names></string-name>, <string-name><surname>Gerstenberger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Schuberth</surname> <given-names>S</given-names></string-name>, <string-name><surname>Schreiber</surname> <given-names>F</given-names></string-name>, <string-name><surname>Rendon</surname> <given-names>ZZ</given-names></string-name>, <string-name><surname>König</surname> <given-names>P</given-names></string-name></person-group>. <article-title>Hyperscanning: A Valid Method to Study Neural Inter-brain Underpinnings of Social Interaction</article-title>. <source>Frontiers in Human Neuroscience</source>. <year>2020</year> <month>Feb</month>; <fpage>14</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2020.00039/full">https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2020.00039/full</ext-link>, doi: <pub-id pub-id-type="doi">10.3389/fnhum.2020.00039</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dal Monte</surname> <given-names>O</given-names></string-name>, <string-name><surname>Chu</surname> <given-names>CCJ</given-names></string-name>, <string-name><surname>Fagan</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>SWC</given-names></string-name></person-group>. <article-title>Specialized medial prefrontal-amygdala coordination in other- regarding decision preference</article-title>. <source>Nature Neuroscience</source>. <year>2020</year> <month>Apr</month>; <volume>23</volume>(<issue>4</issue>):<fpage>565</fpage>–<lpage>574</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41593-020-0593-y">https://www.nature.com/articles/s41593-020-0593-y</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41593-020-0593-y</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dal Monte</surname> <given-names>O</given-names></string-name>, <string-name><surname>Fan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Fagan</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Chu</surname> <given-names>CCJ</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Putnam</surname> <given-names>PT</given-names></string-name>, <string-name><surname>Nair</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>SWC</given-names></string-name></person-group>. <article-title>Widespread implementations of interactive social gaze neurons in the primate prefrontal-amygdala networks</article-title>. <source>Neuron</source>. <year>2022</year> <month>Jul</month>; <volume>110</volume>(<issue>13</issue>):<fpage>2183</fpage>–<lpage>2197.e7</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0896627322003580">https://linkinghub.elsevier.com/retrieve/pii/S0896627322003580</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2022.04.013</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deen</surname> <given-names>B</given-names></string-name>, <string-name><surname>Schwiedrzik</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Sliwa</surname> <given-names>J</given-names></string-name>, <string-name><surname>Freiwald</surname> <given-names>WA</given-names></string-name></person-group>. <article-title>Specialized Networks for Social Cognition in the Primate Brain</article-title>. <source>Annual Review of Neuroscience</source>. <year>2023</year>; <volume>46</volume>(<issue>1</issue>):<fpage>381</fpage>–<lpage>401</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-neuro-102522-121410</pub-id>, doi: <pub-id pub-id-type="doi">10.1146/annurev-neuro-102522-121410</pub-id>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Doorn</surname> <given-names>GS</given-names></string-name>, <string-name><surname>Riebli</surname> <given-names>T</given-names></string-name>, <string-name><surname>Taborsky</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Coaction versus reciprocity in continuous-time models of cooperation</article-title>. <source>Journal of Theoretical Biology</source>. <year>2014</year> <month>Sep</month>; <volume>356</volume>:<fpage>1</fpage>–<lpage>10</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S0022519314001544">http://www.sciencedirect.com/science/article/pii/S0022519314001544</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.jtbi.2014.03.019</pub-id>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dugatkin</surname> <given-names>LA</given-names></string-name>, <string-name><surname>Mesterton-Gibbonsand</surname> <given-names>M</given-names></string-name>, <string-name><surname>Houston</surname> <given-names>AI</given-names></string-name></person-group>. <article-title>Beyond the prisoner’s dilemma: Toward models to discriminate among mechanisms of cooperation in nature</article-title>. <source>Trends in Ecology &amp; Evolution</source>. <year>1992</year> <month>Jun</month>; <volume>7</volume>(<issue>6</issue>):<fpage>202</fpage>–<lpage>205</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/016953479290074L">https://www.sciencedirect.com/science/article/pii/016953479290074L</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/0169-5347(92)90074-L</pub-id>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Falcone</surname> <given-names>R</given-names></string-name>, <string-name><surname>Cirillo</surname> <given-names>R</given-names></string-name>, <string-name><surname>Ferraina</surname> <given-names>S</given-names></string-name>, <string-name><surname>Genovesio</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Neural activity in macaque medial frontal cortex represents others’ choices</article-title>. <source>Scientific Reports</source>. <year>2017</year> <month>Dec</month>; <volume>7</volume>(<issue>1</issue>). <ext-link ext-link-type="uri" xlink:href="http://www.nature.com/articles/s41598-017-12822-5">http://www.nature.com/articles/s41598-017-12822-5</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41598-017-12822-5</pub-id>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Fan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Monte</surname> <given-names>OD</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>SWC</given-names></string-name></person-group>. <article-title>Levels of Naturalism in Social Neuroscience Research</article-title>. <source>iScience</source>. <year>2021</year> <month>Jun</month>; p. <fpage>102702</fpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S2589004221006702">https://linkinghub.elsevier.com/retrieve/pii/S2589004221006702</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.isci.2021.102702</pub-id>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ferrari-Toniolo</surname> <given-names>S</given-names></string-name>, <string-name><surname>Visco-Comandini</surname> <given-names>F</given-names></string-name>, <string-name><surname>Battaglia-Mayer</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Two Brains in Action: Joint-Action Coding in the Primate Frontal Cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2019</year> <month>May</month>; <volume>39</volume>(<issue>18</issue>):<fpage>3514</fpage>–<lpage>3528</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/content/39/18/3514">https://www.jneurosci.org/content/39/18/3514</ext-link>, doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1512-18.2019</pub-id>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ferrea</surname> <given-names>E</given-names></string-name>, <string-name><surname>Franke</surname> <given-names>J</given-names></string-name>, <string-name><surname>Morel</surname> <given-names>P</given-names></string-name>, <string-name><surname>Gail</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Statistical determinants of visuomotor adaptation along different dimensions during naturalistic 3D reaches</article-title>. <source>Scientific Reports</source>. <year>2022</year> <month>Jun</month>; <volume>12</volume>(<issue>1</issue>):<fpage>10198</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41598-022-13866-y">https://www.nature.com/articles/s41598-022-13866-y</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41598-022-13866-y</pub-id>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Formaux</surname> <given-names>A</given-names></string-name>, <string-name><surname>Paleressompoulle</surname> <given-names>D</given-names></string-name>, <string-name><surname>Fagot</surname> <given-names>J</given-names></string-name>, <string-name><surname>Claidière</surname> <given-names>N</given-names></string-name></person-group>. <article-title>The experimental emergence of convention in a non-human primate</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>. <year>2022</year> <month>Jan</month>; <volume>377</volume>(<issue>1843</issue>):<fpage>20200310</fpage>. <ext-link ext-link-type="uri" xlink:href="https://royalsocietypublishing.org/doi/10.1098/rstb.2020.0310">https://royalsocietypublishing.org/doi/10.1098/rstb.2020.0310</ext-link>, doi: <pub-id pub-id-type="doi">10.1098/rstb.2020.0310</pub-id>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Formaux</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sperber</surname> <given-names>D</given-names></string-name>, <string-name><surname>Fagot</surname> <given-names>J</given-names></string-name>, <string-name><surname>Claidière</surname> <given-names>N</given-names></string-name></person-group>. <article-title>Guinea baboons are strategic cooperators</article-title>. <source>Science Advances</source>. <year>2023</year> <month>Oct</month>; <volume>9</volume>(<issue>43</issue>):<elocation-id>eadi5282</elocation-id>. <ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/10.1126/sciadv.adi5282">https://www.science.org/doi/10.1126/sciadv.adi5282</ext-link>, doi: <pub-id pub-id-type="doi">10.1126/sciadv.adi5282</pub-id>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Foulsham</surname> <given-names>T</given-names></string-name>, <string-name><surname>Walker</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kingstone</surname> <given-names>A</given-names></string-name></person-group>. <article-title>The where, what and when of gaze allocation in the lab and the natural environment</article-title>. <source>Vision Research</source>. <year>2011</year> <month>Sep</month>; <volume>51</volume>(<issue>17</issue>):<fpage>1920</fpage>–<lpage>1931</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0042698911002392">https://www.sciencedirect.com/science/article/pii/S0042698911002392</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.visres.2011.07.002</pub-id>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Franchak</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Kretch</surname> <given-names>KS</given-names></string-name>, <string-name><surname>Adolph</surname> <given-names>KE</given-names></string-name></person-group>. <article-title>See and be seen: Infant-caregiver social looking during locomotor free play</article-title>. <source>Developmental Science</source>. <year>2018</year>; <volume>21</volume>(<issue>4</issue>):<elocation-id>e12626</elocation-id>. doi: <pub-id pub-id-type="doi">10.1111/desc.12626</pub-id>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Freeman</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Ambady</surname> <given-names>N</given-names></string-name></person-group>. <article-title>MouseTracker: Software for studying real-time mental processing using a computer mouse-tracking method</article-title>. <source>Behavior Research Methods</source>. <year>2010</year> <month>Feb</month>; <volume>42</volume>(<issue>1</issue>):<fpage>226</fpage>–<lpage>241</lpage>. <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.3758/BRM.42.1.226">http://link.springer.com/10.3758/BRM.42.1.226</ext-link>, doi: <pub-id pub-id-type="doi">10.3758/BRM.42.1.226</pub-id>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fujii</surname> <given-names>N</given-names></string-name>, <string-name><surname>Hihara</surname> <given-names>S</given-names></string-name>, <string-name><surname>Iriki</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Dynamic Social Adaptation of Motion-Related Neurons in Primate Parietal Cortex</article-title>. <source>PLoS ONE</source>. <year>2007</year> <month>Apr</month>; <volume>2</volume>(<issue>4</issue>):<elocation-id>e397</elocation-id>. <ext-link ext-link-type="uri" xlink:href="http://dx.plos.org/10.1371/journal.pone.0000397">http://dx.plos.org/10.1371/journal.pone.0000397</ext-link>, doi: <pub-id pub-id-type="doi">10.1371/jour-nal.pone.0000397</pub-id>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fujii</surname> <given-names>N</given-names></string-name>, <string-name><surname>Hihara</surname> <given-names>S</given-names></string-name>, <string-name><surname>Iriki</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Social cognition in premotor and parietal cortex</article-title>. <source>Social Neuroscience</source>. <year>2008</year> <month>Sep</month>; <volume>3</volume>(<issue>3-4</issue>):<fpage>250</fpage>–<lpage>260</lpage>. <pub-id pub-id-type="doi">10.1080/17470910701434610</pub-id>, doi: <pub-id pub-id-type="doi">10.1080/17470910701434610</pub-id>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gail</surname> <given-names>A</given-names></string-name>, <string-name><surname>Andersen</surname> <given-names>RA</given-names></string-name></person-group>. <article-title>Neural Dynamics in Monkey Parietal Reach Region Reflect Context-Specific Sensorimotor Transformations</article-title>. <source>Journal of Neuroscience</source>. <year>2006</year> <month>Sep</month>; <volume>26</volume>(<issue>37</issue>):<fpage>9376</fpage>–<lpage>9384</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/content/26/37/9376">https://www.jneurosci.org/content/26/37/9376</ext-link>, doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1570-06.2006</pub-id>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gallivan</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Chapman</surname> <given-names>CS</given-names></string-name></person-group>. <article-title>Three-dimensional reach trajectories as a probe of real-time decision-making between multiple competing targets</article-title>. <source>Frontiers in Neuroscience</source>. <year>2014</year>; <fpage>8</fpage>. <ext-link ext-link-type="uri" xlink:href="http://journal.frontiersin.org/article/10.3389/fnins.2014.00215/full">http://journal.frontiersin.org/article/10.3389/fnins.2014.00215/full</ext-link>, doi: <pub-id pub-id-type="doi">10.3389/fnins.2014.00215</pub-id>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gallivan</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Chapman</surname> <given-names>CS</given-names></string-name>, <string-name><surname>Wolpert</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Flanagan</surname> <given-names>JR</given-names></string-name></person-group>. <article-title>Decision-making in sensorimotor control</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2018</year> <month>Sep</month>; <volume>19</volume>(<issue>9</issue>):<fpage>519</fpage>–<lpage>534</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41583-018-0045-9">https://www.nature.com/articles/s41583-018-0045-9</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41583-018-0045-9</pub-id>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gallup</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Hale</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Sumpter</surname> <given-names>DJT</given-names></string-name>, <string-name><surname>Garnier</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kacelnik</surname> <given-names>A</given-names></string-name>, <string-name><surname>Krebs</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Couzin</surname> <given-names>ID</given-names></string-name></person-group>. <article-title>Visual attention and the acquisition of information in human crowds</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2012</year> <month>May</month>; <volume>109</volume>(<issue>19</issue>):<fpage>7245</fpage>–<lpage>7250</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/abs/10.1073/pnas.1116141109">https://www.pnas.org/doi/abs/10.1073/pnas.1116141109</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.1116141109</pub-id>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gobel</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>HS</given-names></string-name>, <string-name><surname>Richardson</surname> <given-names>DC</given-names></string-name></person-group>. <article-title>The dual function of social gaze</article-title>. <source>Cognition</source>. <year>2015</year> <month>Mar</month>; <volume>136</volume>:<fpage>359</fpage>–<lpage>364</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0010027714002662">https://www.sciencedirect.com/science/article/pii/S0010027714002662</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cognition.2014.11.040</pub-id>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gordon</surname> <given-names>J</given-names></string-name>, <string-name><surname>Maselli</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lancia</surname> <given-names>GL</given-names></string-name>, <string-name><surname>Thiery</surname> <given-names>T</given-names></string-name>, <string-name><surname>Cisek</surname> <given-names>P</given-names></string-name>, <string-name><surname>Pezzulo</surname> <given-names>G</given-names></string-name></person-group>. <article-title>The road towards understanding embodied decisions</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>. <year>2021</year> <month>Dec</month>; <volume>131</volume>:<fpage>722</fpage>–<lpage>736</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0149763421004164">https://www.sciencedirect.com/science/article/pii/S0149763421004164</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neubiorev.2021.09.034</pub-id>.</mixed-citation></ref>
    <ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grabenhorst</surname> <given-names>F</given-names></string-name>, <string-name><surname>Báez-Mendoza</surname> <given-names>R</given-names></string-name>, <string-name><surname>Genest</surname> <given-names>W</given-names></string-name>, <string-name><surname>Deco</surname> <given-names>G</given-names></string-name>, <string-name><surname>Schultz</surname> <given-names>W</given-names></string-name></person-group>. <article-title>Primate Amygdala Neurons Simulate Decision Processes of Social Partners</article-title>. <source>Cell</source>. <year>2019</year> <month>May</month>; <volume>177</volume>(<issue>4</issue>):<fpage>986</fpage>–<lpage>998.e15</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.cell.com/cell/abstract/S0092-8674(19)30225-9">https://www.cell.com/cell/abstract/S0092-8674(19)30225-9</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cell.2019.02.042</pub-id>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hadley</surname> <given-names>LV</given-names></string-name>, <string-name><surname>Naylor</surname> <given-names>G</given-names></string-name>, <string-name><surname>Hamilton</surname> <given-names>AFdC</given-names></string-name></person-group>. <article-title>A review of theories and methods in the science of face-to-face social interaction</article-title>. <source>Nature Reviews Psychology</source>. <year>2022</year> <month>Jan</month>; <volume>1</volume>(<issue>1</issue>):<fpage>42</fpage>–<lpage>54</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s44159-021-00008-w">https://www.nature.com/articles/s44159-021-00008-w</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s44159-021-00008-w</pub-id>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hakim</surname> <given-names>U</given-names></string-name>, <string-name><surname>De Felice</surname> <given-names>S</given-names></string-name>, <string-name><surname>Pinti</surname> <given-names>P</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Noah</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ono</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Burgess</surname> <given-names>P</given-names></string-name>, <string-name><surname>Hamilton</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hirsch</surname> <given-names>J</given-names></string-name>, <string-name><surname>Tachtsidis</surname> <given-names>I</given-names></string-name></person-group>. <article-title>Quantification of inter-brain coupling: A review of current methods used in haemodynamic and electrophysiological hyperscanning studies</article-title>. <source>NeuroImage</source>. <year>2023</year>; p. <fpage>120354</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2023.120354</pub-id>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hamilton</surname> <given-names>AFdC</given-names></string-name>, <string-name><surname>Holler</surname> <given-names>J</given-names></string-name></person-group>. <article-title>Face2face: advancing the science of social interaction</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>. <year>2023</year> <month>Mar</month>; <volume>378</volume>(<issue>1875</issue>):<fpage>20210470</fpage>. <ext-link ext-link-type="uri" xlink:href="https://royalsocietypublishing.org/doi/10.1098/rstb.2021.0470">https://royalsocietypublishing.org/doi/10.1098/rstb.2021.0470</ext-link>, doi: <pub-id pub-id-type="doi">10.1098/rstb.2021.0470</pub-id>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hari</surname> <given-names>R</given-names></string-name>, <string-name><surname>Henriksson</surname> <given-names>L</given-names></string-name>, <string-name><surname>Malinen</surname> <given-names>S</given-names></string-name>, <string-name><surname>Parkkonen</surname> <given-names>L</given-names></string-name></person-group>. <article-title>Centrality of Social Interaction in Human Brain Function</article-title>. <source>Neuron</source>. <year>2015</year> <month>Oct</month>; <volume>88</volume>(<issue>1</issue>):<fpage>181</fpage>–<lpage>193</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0896627315007795">https://linkinghub.elsevier.com/retrieve/pii/S0896627315007795</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.022</pub-id>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haroush</surname> <given-names>K</given-names></string-name>, <string-name><surname>Williams</surname> <given-names>Z</given-names></string-name></person-group>. <article-title>Neuronal Prediction of Opponent’s Behavior during Cooperative Social Interchange in Primates</article-title>. <source>Cell</source>. <year>2015</year> <month>Mar</month>; <volume>160</volume>(<issue>6</issue>):<fpage>1233</fpage>–<lpage>1245</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S0092867415001233">http://www.sciencedirect.com/science/article/pii/S0092867415001233</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cell.2015.01.045</pub-id>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hawkins</surname> <given-names>RXD</given-names></string-name>, <string-name><surname>Goldstone</surname> <given-names>RL</given-names></string-name></person-group>. <article-title>The Formation of Social Conventions in Real-Time Environments</article-title>. <source>PLOS One</source>. <year>2016</year> <month>Mar</month>; <volume>11</volume>(<issue>3</issue>):<elocation-id>e0151670</elocation-id>. <ext-link ext-link-type="uri" xlink:href="https://dx.plos.org/10.1371/journal.pone.0151670">https://dx.plos.org/10.1371/journal.pone.0151670</ext-link>, doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0151670</pub-id>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Heo</surname> <given-names>H</given-names></string-name>, <string-name><surname>Park</surname> <given-names>HK</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>S</given-names></string-name>, <string-name><surname>Chung</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>G</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>W</given-names></string-name></person-group>. <article-title>Transwall: a transparent double-sided touch display facilitating co-located face-to-face interactions</article-title>. <conf-name>CHI ‘14 Extended Abstracts on Human Factors in Computing Systems CHI EA’14</conf-name>: <year>2014</year>. p. <fpage>435</fpage>–<lpage>438</lpage> doi: <pub-id pub-id-type="doi">10.1145/2559206.2574828</pub-id>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hirsch</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Noah</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Bhattacharya</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Neural mechanisms for emotional contagion and spontaneous mimicry of live facial expressions</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>. <year>2023</year> <month>Mar</month>; <volume>378</volume>(<issue>1875</issue>):<fpage>20210472</fpage>. <ext-link ext-link-type="uri" xlink:href="https://royalsocietypublishing.org/doi/10.1098/rstb.2021.0472">https://royalsocietypublishing.org/doi/10.1098/rstb.2021.0472</ext-link>, doi: <pub-id pub-id-type="doi">10.1098/rstb.2021.0472</pub-id>.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holmes</surname> <given-names>N</given-names></string-name>, <string-name><surname>Rea</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hill</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Leggett</surname> <given-names>J</given-names></string-name>, <string-name><surname>Edwards</surname> <given-names>LJ</given-names></string-name>, <string-name><surname>Hobson</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Boto</surname> <given-names>E</given-names></string-name>, <string-name><surname>Tierney</surname> <given-names>TM</given-names></string-name>, <string-name><surname>Rier</surname> <given-names>L</given-names></string-name>, <string-name><surname>Rivero</surname> <given-names>GR</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Enabling ambulatory movement in wearable magnetoencephalography with matrix coil active magnetic shielding</article-title>. <source>NeuroImage</source>. <year>2023</year>; <volume>274</volume>:<fpage>120157</fpage>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hosokawa</surname> <given-names>T</given-names></string-name>, <string-name><surname>Watanabe</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Prefrontal Neurons Represent Winning and Losing during Competitive Video Shooting Games between Monkeys</article-title>. <source>Journal of Neuroscience</source>. <year>2012</year> <month>May</month>; <volume>32</volume>(<issue>22</issue>):<fpage>7662</fpage>–<lpage>7671</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.6479-11.2012">http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.6479-11.2012</ext-link>, doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.6479-11.2012</pub-id>.</mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Huser</surname> <given-names>T</given-names></string-name></person-group>, <source>JARVIS-MoCap/JARVlS-AcquisitionTool</source>. <publisher-name>GitHub</publisher-name>; <year>2024</year>. <ext-link ext-link-type="uri" xlink:href="https://github.com/JARVIS-MoCap/JARVIS-AcquisitionTool">https://github.com/JARVIS-MoCap/JARVIS-AcquisitionTool</ext-link>.</mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Ishii</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kobayashi</surname> <given-names>M</given-names></string-name></person-group>. <article-title>ClearBoard: a seamless medium for shared drawing and conversation with eye contact</article-title>. In: <conf-name>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems CHI ‘92</conf-name>, <publisher-loc>New York, NY, USA</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>; <year>1992</year>. p. <fpage>525</fpage>–<lpage>532</lpage>. <ext-link ext-link-type="uri" xlink:href="https://dl.acm.org/doi/10.1145/142750.142977">https://dl.acm.org/doi/10.1145/142750.142977</ext-link>, doi: <pub-id pub-id-type="doi">10.1145/142750.142977</pub-id>.</mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Isoda</surname> <given-names>M</given-names></string-name>, <string-name><surname>Noritake</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ninomiya</surname> <given-names>T</given-names></string-name></person-group>. <article-title>Development of social systems neuroscience using macaques</article-title>. <source>Proceedings of the Japan Academy, Series B</source>. <year>2018</year> <month>Aug</month>; <volume>94</volume>(<issue>7</issue>):<fpage>305</fpage>–<lpage>323</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.jstage.jst.go.jp/article/pjab/94/7/94_PJA9407B-03/_article">https://www.jstage.jst.go.jp/article/pjab/94/7/94_PJA9407B-03/_article</ext-link>, doi: <pub-id pub-id-type="doi">10.2183/pjab.94.020</pub-id>.</mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jahng</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kralik</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Hwang</surname> <given-names>DU</given-names></string-name>, <string-name><surname>Jeong</surname> <given-names>J</given-names></string-name></person-group>. <article-title>Neural dynamics of two players when using nonverbal cues to gauge intentions to cooperate during the Prisoner’s Dilemma Game</article-title>. <source>NeuroImage</source>. <year>2017</year> <month>Aug</month>; <volume>157</volume>:<fpage>263</fpage>–<lpage>274</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1053811917304937">https://linkinghub.elsevier.com/retrieve/pii/S1053811917304937</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.024</pub-id>.</mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Jocher</surname> <given-names>G</given-names></string-name>, <string-name><surname>Qiu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chaurasia</surname> <given-names>A</given-names></string-name></person-group>, <source>Ultralytics YOLO</source>. <publisher-name>GitHub</publisher-name>; <year>2023</year>. <ext-link ext-link-type="uri" xlink:href="https://github.com/ultralytics/ultralytics">https://github.com/ultralytics/ultralytics</ext-link>.</mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kagan</surname> <given-names>I</given-names></string-name>, <string-name><surname>Iyer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lindner</surname> <given-names>A</given-names></string-name>, <string-name><surname>Andersen</surname> <given-names>RA</given-names></string-name></person-group>. <article-title>Space representation for eye movements is more contralateral in monkeys than in humans</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2010</year> <month>Apr</month>; <volume>107</volume>(<issue>17</issue>):<fpage>7933</fpage>–<lpage>7938</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/doi/10.1073/pnas.1002825107">http://www.pnas.org/lookup/doi/10.1073/pnas.1002825107</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.1002825107</pub-id>.</mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kiani</surname> <given-names>R</given-names></string-name>, <string-name><surname>Shadlen</surname> <given-names>MN</given-names></string-name></person-group>. <article-title>Representation of Confidence Associated with a Decision by Neurons in the Parietal Cortex</article-title>. <source>Science</source>. <year>2009</year> <month>May</month>; <volume>324</volume>(<issue>5928</issue>):<fpage>759</fpage>–<lpage>764</lpage>. doi: <pub-id pub-id-type="doi">10.1126/science.1169405</pub-id>.</mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kilgour</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Fraser</surname> <given-names>NM</given-names></string-name></person-group>. <article-title>A taxonomy of all ordinal 2x 2 games</article-title>. <source>Theory and decision</source>. <year>1988</year>; <volume>24</volume>(<issue>2</issue>):<fpage>99</fpage>–<lpage>117</lpage>.</mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Kothe</surname> <given-names>C</given-names></string-name>, <string-name><surname>Shirazi</surname> <given-names>SY</given-names></string-name>, <string-name><surname>Stenner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Medine</surname> <given-names>D</given-names></string-name>, <string-name><surname>Boulay</surname> <given-names>C</given-names></string-name>, <string-name><surname>Grivich</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Mullen</surname> <given-names>T</given-names></string-name>, <string-name><surname>Delorme</surname> <given-names>A</given-names></string-name>, <string-name><surname>Makeig</surname> <given-names>S</given-names></string-name></person-group>. <article-title>The Lab Streaming Layer for Synchronized Multimodal Recording</article-title>. <source>bioRxiv</source>. <year>2024</year>; <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2024/02/14/2024.02.13.580071">https://www.biorxiv.org/content/early/2024/02/14/2024.02.13.580071</ext-link>, doi: <pub-id pub-id-type="doi">10.1101/2024.02.13.580071</pub-id>.</mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laidlaw</surname> <given-names>KEW</given-names></string-name>, <string-name><surname>Foulsham</surname> <given-names>T</given-names></string-name>, <string-name><surname>Kuhn</surname> <given-names>G</given-names></string-name>, <string-name><surname>Kingstone</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Potential social interactions are important to social attention</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2011</year> <month>Apr</month>; <volume>108</volume>(<issue>14</issue>):<fpage>5548</fpage>–<lpage>5553</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/abs/10.1073/pnas.1017022108">https://www.pnas.org/doi/abs/10.1073/pnas.1017022108</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.1017022108</pub-id>.</mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lehmann</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Scherberger</surname> <given-names>H</given-names></string-name></person-group>. <article-title>Reach and Gaze Representations in Macaque Parietal and Premotor Grasp Areas</article-title>. <source>Journal of Neuroscience</source>. <year>2013</year> <month>Apr</month>; <volume>33</volume>(<issue>16</issue>):<fpage>7038</fpage>–<lpage>7049</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.5568-12.2013">http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.5568-12.2013</ext-link>, doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5568-12.2013</pub-id>.</mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Levy</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lankinen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Hakonen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Feldman</surname> <given-names>R</given-names></string-name></person-group>. <article-title>The integration of social and neural synchrony: a case for ecologically valid research using MEG neuroimaging</article-title>. <source>Social Cognitive and Affective Neuroscience</source>. <year>2021</year> <month>Jan</month>; <volume>16</volume>(<issue>1-2</issue>):<fpage>143</fpage>–<lpage>152</lpage>. <pub-id pub-id-type="doi">10.1093/scan/nsaa061</pub-id>, doi: <pub-id pub-id-type="doi">10.1093/scan/nsaa061</pub-id>.</mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Lewen</surname> <given-names>D</given-names></string-name>, <string-name><surname>Ivanov</surname> <given-names>F</given-names></string-name>, <string-name><surname>Dehning</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ruß</surname> <given-names>J</given-names></string-name>, <string-name><surname>Fischer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Penke</surname> <given-names>L</given-names></string-name>, <string-name><surname>Schacht</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gail</surname> <given-names>A</given-names></string-name>, <string-name><surname>Priesemann</surname> <given-names>V</given-names></string-name>, <string-name><surname>Kagan</surname> <given-names>I</given-names></string-name></person-group>. <source>Continuous dynamics of cooperation and competition in social foraging</source>. Submitted. <year>2025</year>; .</mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>PT</given-names></string-name>, <string-name><surname>Vaidya</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Flint</surname> <given-names>RD</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>CY</given-names></string-name>, <string-name><surname>Slutzky</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Do</surname> <given-names>AH</given-names></string-name></person-group>. <article-title>Electromyogram (EMG) Removal by Adding Sources of EMG (ERASE)—A Novel ICA-Based Algorithm for Removing Myoelectric Artifacts From EEG</article-title>. <source>Frontiers in Neuroscience</source>. <year>2021</year> <month>Jan</month>; <fpage>14</fpage>. doi: <pub-id pub-id-type="doi">10.3389/fnins.2020.597941</pub-id>.</mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Madhavan</surname> <given-names>R</given-names></string-name>, <string-name><surname>Sia</surname> <given-names>MY</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>X</given-names></string-name>, <string-name><surname>Mundry</surname> <given-names>R</given-names></string-name>, <string-name><surname>Mani</surname> <given-names>N</given-names></string-name></person-group>. <article-title>Egocentricity in infants’ play with familiar objects in caregiverchild interactions</article-title>. <source>PsyArXiv</source>. <year>2025</year> <month>Feb</month>; <ext-link ext-link-type="uri" xlink:href="https://osf.io/preprints/psyarxiv/2ewqc_v2">osf.io/preprints/psyarxiv/2ewqc_v2</ext-link>, doi: <pub-id pub-id-type="doi">10.31234/osf.io/2ewqc_v2</pub-id>.</mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mahmoodi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bahrami</surname> <given-names>B</given-names></string-name>, <string-name><surname>Mehring</surname> <given-names>C</given-names></string-name></person-group>. <article-title>Reciprocity of social influence</article-title>. <source>Nature Communications</source>. <year>2018</year> <month>Jun</month>; <volume>9</volume>(<issue>1</issue>):<fpage>2474</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-018-04925-y">https://www.nature.com/articles/s41467-018-04925-y</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41467-018-04925-y</pub-id>.</mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mahmoodi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Nili</surname> <given-names>H</given-names></string-name>, <string-name><surname>Bang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Mehring</surname> <given-names>C</given-names></string-name>, <string-name><surname>Bahrami</surname> <given-names>B</given-names></string-name></person-group>. <article-title>Distinct neurocomputational mechanisms support informational and socially normative conformity</article-title>. <source>PLOS Biology</source>. <year>2022</year> <month>Mar</month>; <volume>20</volume>(<issue>3</issue>):<elocation-id>e3001565</elocation-id>. <ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001565">https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001565</ext-link>, doi: <pub-id pub-id-type="doi">10.1371/journal.pbio.3001565</pub-id>.</mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mani</surname> <given-names>N</given-names></string-name>, <string-name><surname>Ackermann</surname> <given-names>L</given-names></string-name></person-group>. <article-title>Why do children learn the words they do?</article-title> <source>Child Development Perspectives</source>. <year>2018</year>; <volume>12</volume>(<issue>4</issue>):<fpage>253</fpage>–<lpage>257</lpage>. doi: <pub-id pub-id-type="doi">10.1111/cdep.12295</pub-id>.</mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Mathis</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mamidanna</surname> <given-names>P</given-names></string-name>, <string-name><surname>Cury</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Abe</surname> <given-names>T</given-names></string-name>, <string-name><surname>Murthy</surname> <given-names>VN</given-names></string-name>, <string-name><surname>Mathis</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Bethge</surname> <given-names>M</given-names></string-name></person-group>. <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nature Neuroscience</source>. <year>2018</year>; <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41593-018-0209-y">https://www.nature.com/articles/s41593-018-0209-y</ext-link>.</mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDonald</surname> <given-names>KR</given-names></string-name>, <string-name><surname>Broderick</surname> <given-names>WF</given-names></string-name>, <string-name><surname>Huettel</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Pearson</surname> <given-names>JM</given-names></string-name></person-group>. <article-title>Bayesian nonparametric models characterize instantaneous strategies in a competitive dynamic game</article-title>. <source>Nature Communications</source>. <year>2019</year> <month>Apr</month>; <volume>10</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>12</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-019-09789-4">https://www.nature.com/articles/s41467-019-09789-4</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41467-019-09789-4</pub-id>.</mixed-citation></ref>
<ref id="c93"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meisner</surname> <given-names>OC</given-names></string-name>, <string-name><surname>Shi</surname> <given-names>W</given-names></string-name>, <string-name><surname>Fagan</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Greenwood</surname> <given-names>J</given-names></string-name>, <string-name><surname>Jadi</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Nandy</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>SW</given-names></string-name></person-group>. <article-title>Development of a Marmoset Apparatus for Automated Pulling to study cooperative behaviors</article-title>. <source>eLife</source>. <year>2024</year> <month>Oct</month>; <volume>13</volume>:<elocation-id>RP97088</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.97088</pub-id>, doi: <pub-id pub-id-type="doi">10.7554/eLife.97088</pub-id>.</mixed-citation></ref>
<ref id="c94"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moeller</surname> <given-names>S</given-names></string-name>, <string-name><surname>Unakafov</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Fischer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Gail</surname> <given-names>A</given-names></string-name>, <string-name><surname>Treue</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kagan</surname> <given-names>I</given-names></string-name></person-group>. <article-title>Human and macaque pairs employ different coordination strategies in a transparent decision game</article-title>. <source>eLife</source>. <year>2023</year> <month>Jan</month>; <volume>12</volume>:<elocation-id>e81641</elocation-id>. doi: <pub-id pub-id-type="doi">10.7554/eLife.81641</pub-id>.</mixed-citation></ref>
<ref id="c95"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moreira</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Rollwage</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kaduk</surname> <given-names>K</given-names></string-name>, <string-name><surname>Wilke</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kagan</surname> <given-names>I</given-names></string-name></person-group>. <article-title>Post-decision wagering after perceptual judgments reveals bi-directional certainty readouts</article-title>. <source>Cognition</source>. <year>2018</year> <month>Jul</month>; <volume>176</volume>:<fpage>40</fpage>–<lpage>52</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0010027718300623">https://www.sciencedirect.com/science/article/pii/S0010027718300623</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cognition.2018.02.026</pub-id>.</mixed-citation></ref>
<ref id="c96"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morel</surname> <given-names>P</given-names></string-name>, <string-name><surname>Ulbrich</surname> <given-names>P</given-names></string-name>, <string-name><surname>Gail</surname> <given-names>A</given-names></string-name></person-group>. <article-title>What makes a reach movement effortful? Physical effort discounting supports common minimization principles in decision making and motor control</article-title>. <source>PLOS Biology</source>. <year>2017</year> <month>Jun</month>; <volume>15</volume>(<issue>6</issue>):<elocation-id>e2001323</elocation-id>. <ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2001323">https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2001323</ext-link>, doi: <pub-id pub-id-type="doi">10.1371/journal.pbio.2001323</pub-id>.</mixed-citation></ref>
<ref id="c97"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>von Neumann</surname> <given-names>J</given-names></string-name>, <string-name><surname>Morgenstern</surname> <given-names>O</given-names></string-name></person-group>. <source>Theory of Games and Economic Behavior</source>. <publisher-name>Princeton University Press</publisher-name>; <year>1944</year>.</mixed-citation></ref>
<ref id="c98"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ninomiya</surname> <given-names>T</given-names></string-name>, <string-name><surname>Noritake</surname> <given-names>A</given-names></string-name>, <string-name><surname>Isoda</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Live agent preference and social action monitoring in the macaque mid-superior temporal sulcus region</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2021</year> <month>Nov</month>; <volume>118</volume>(<issue>44</issue>):<elocation-id>e2109653118</elocation-id>. <ext-link ext-link-type="uri" xlink:href="https://pnas.org/doi/full/10.1073/pnas.2109653118">https://pnas.org/doi/full/10.1073/pnas.2109653118</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.2109653118</pub-id>.</mixed-citation></ref>
<ref id="c99"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Noritake</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ninomiya</surname> <given-names>T</given-names></string-name>, <string-name><surname>Isoda</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Social reward monitoring and valuation in the macaque brain</article-title>. <source>Nature Neuroscience</source>. <year>2018</year> <month>Oct</month>; <volume>21</volume>(<issue>10</issue>):<fpage>1452</fpage>–<lpage>1462</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.nature.com/articles/s41593-018-0229-7">http://www.nature.com/articles/s41593-018-0229-7</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41593-018-0229-7</pub-id>.</mixed-citation></ref>
<ref id="c100"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nougaret</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ferrucci</surname> <given-names>L</given-names></string-name>, <string-name><surname>Genovesio</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Role of the social actor during social interaction and learning in humanmonkey paradigms</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>. <year>2019</year> <month>Jul</month>; <volume>102</volume>:<fpage>242</fpage>–<lpage>250</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0149763419302131">https://www.sciencedirect.com/science/article/pii/S0149763419302131</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neubiorev.2019.05.004</pub-id>.</mixed-citation></ref>
<ref id="c101"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Noë</surname> <given-names>R</given-names></string-name></person-group>. <article-title>Cooperation experiments: coordination through communication versus acting apart together</article-title>. <source>Animal Behaviour</source>. <year>2006</year> <month>Jan</month>; <volume>71</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>18</lpage>. <ext-link ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0003347205003611">http://linkinghub.elsevier.com/retrieve/pii/S0003347205003611</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.anbehav.2005.03.037</pub-id>.</mixed-citation></ref>
<ref id="c102"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Olson</surname> <given-names>E</given-names></string-name></person-group>. <article-title>AprilTag: A robust and flexible visual fiducial system</article-title>. In: <conf-name>2011 IEEE International Conference on Robotics and Automation</conf-name>; <year>2011</year>. p. <fpage>3400</fpage>–<lpage>3407</lpage>. <ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/abstract/document/5979561">https://ieeexplore.ieee.org/abstract/document/5979561</ext-link>, doi: <pub-id pub-id-type="doi">10.1109/ICRA.2011.5979561</pub-id>.</mixed-citation></ref>
<ref id="c103"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ong</surname> <given-names>WS</given-names></string-name>, <string-name><surname>Madlon-Kay</surname> <given-names>S</given-names></string-name>, <string-name><surname>Platt</surname> <given-names>ML</given-names></string-name></person-group>. <article-title>Neuronal correlates of strategic cooperation in monkeys</article-title>. <source>Nature Neuroscience</source>. <year>2021</year> <month>Jan</month>; <volume>24</volume>(<issue>1</issue>):<fpage>116</fpage>–<lpage>128</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41593-020-00746-9">https://www.nature.com/articles/s41593-020-00746-9</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41593-020-00746-9</pub-id>.</mixed-citation></ref>
<ref id="c104"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Palumbo</surname> <given-names>RV</given-names></string-name>, <string-name><surname>Marraccini</surname> <given-names>ME</given-names></string-name>, <string-name><surname>Weyandt</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Wilder-Smith</surname> <given-names>O</given-names></string-name>, <string-name><surname>McGee</surname> <given-names>HA</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>S</given-names></string-name>, <string-name><surname>Goodwin</surname> <given-names>MS</given-names></string-name></person-group>. <article-title>Interpersonal Autonomic Physiology: A Systematic Review of the Literature</article-title>. <source>Personality and Social Psychology Review</source>. <year>2016</year> <month>Feb</month>; <volume>21</volume>(<issue>2</issue>):<fpage>99</fpage>–<lpage>141</lpage>. doi: <pub-id pub-id-type="doi">10.1177/1088868316628405</pub-id>.</mixed-citation></ref>
<ref id="c105"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Park</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Sestito</surname> <given-names>M</given-names></string-name>, <string-name><surname>Boorman</surname> <given-names>ED</given-names></string-name>, <string-name><surname>Dreher</surname> <given-names>JC</given-names></string-name></person-group>. <article-title>Neural computations underlying strategic social decision-making in groups</article-title>. <source>Nature Communications</source>. <year>2019</year> <month>Nov</month>; <volume>10</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>12</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-019-12937-5">https://www.nature.com/articles/s41467-019-12937-5</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41467-019-12937-5</pub-id>.</mixed-citation></ref>
<ref id="c106"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parvizi</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kastner</surname> <given-names>S</given-names></string-name></person-group>. <article-title>Promises and limitations of human intracranial electroencephalography</article-title>. <source>Nature Neuroscience</source>. <year>2018</year> <month>Apr</month>; <volume>21</volume>(<issue>4</issue>):<fpage>474</fpage>–<lpage>483</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41593-018-0108-2">https://www.nature.com/articles/s41593-018-0108-2</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41593-018-0108-2</pub-id>.</mixed-citation></ref>
<ref id="c107"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>di Pellegrino</surname> <given-names>G</given-names></string-name>, <string-name><surname>Fadiga</surname> <given-names>L</given-names></string-name>, <string-name><surname>Fogassi</surname> <given-names>L</given-names></string-name>, <string-name><surname>Gallese</surname> <given-names>V</given-names></string-name>, <string-name><surname>Rizzolatti</surname> <given-names>G</given-names></string-name></person-group>. <article-title>Understanding motor events: a neurophysiological study</article-title>. <source>Experimental Brain Research</source>. <year>1992</year>; <volume>91</volume>(<issue>1</issue>):<fpage>176</fpage>–<lpage>180</lpage>. doi: <pub-id pub-id-type="doi">10.1007/BF00230027</pub-id>.</mixed-citation></ref>
<ref id="c108"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pelz</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kidd</surname> <given-names>C</given-names></string-name></person-group>. <article-title>The elaboration of exploratory play</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>. <year>2020</year> <month>Jul</month>; <volume>375</volume>(<issue>1803</issue>):<fpage>20190503</fpage>. doi: <pub-id pub-id-type="doi">10.1098/rstb.2019.0503</pub-id>.</mixed-citation></ref>
<ref id="c109"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pescetelli</surname> <given-names>N</given-names></string-name>, <string-name><surname>Yeung</surname> <given-names>N</given-names></string-name></person-group>. <article-title>The effects of recursive communication dynamics on belief updating</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source>. <year>2020</year> <month>Jul</month>; <volume>287</volume>(<issue>1931</issue>):<fpage>20200025</fpage>. <ext-link ext-link-type="uri" xlink:href="https://royalsocietypublishing.org/doi/10.1098/rspb.2020.0025">https://royalsocietypublishing.org/doi/10.1098/rspb.2020.0025</ext-link>, doi: <pub-id pub-id-type="doi">10.1098/rspb.2020.0025</pub-id>.</mixed-citation></ref>
<ref id="c110"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pescetelli</surname> <given-names>N</given-names></string-name>, <string-name><surname>Yeung</surname> <given-names>N</given-names></string-name></person-group>. <article-title>Benefits of spontaneous confidence alignment between dyad members</article-title>. <source>Collective Intelligence</source>. <year>2022</year> <month>Dec</month>; <volume>1</volume>(<issue>2</issue>). <ext-link ext-link-type="uri" xlink:href="https://dl.acm.org/doi/10.1177/26339137221126915">https://dl.acm.org/doi/10.1177/26339137221126915</ext-link>, doi: <pub-id pub-id-type="doi">10.1177/26339137221126915</pub-id>.</mixed-citation></ref>
<ref id="c111"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Philippe</surname> <given-names>R</given-names></string-name>, <string-name><surname>Janet</surname> <given-names>R</given-names></string-name>, <string-name><surname>Khalvati</surname> <given-names>K</given-names></string-name>, <string-name><surname>Rao</surname> <given-names>RPN</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>D</given-names></string-name>, <string-name><surname>Dreher</surname> <given-names>JC</given-names></string-name></person-group>. <article-title>Neurocomputational mechanisms involved in adaptation to fluctuating intentions of others</article-title>. <source>Nature Communications</source>. <year>2024</year> <month>Apr</month>; <volume>15</volume>(<issue>1</issue>):<fpage>3189</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-024-47491-2">https://www.nature.com/articles/s41467-024-47491-2</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41467-024-47491-2</pub-id>.</mixed-citation></ref>
<ref id="c112"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pisauro</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Fouragnan</surname> <given-names>EF</given-names></string-name>, <string-name><surname>Arabadzhiyska</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Apps</surname> <given-names>MaJ</given-names></string-name>, <string-name><surname>Philiastides</surname> <given-names>MG</given-names></string-name></person-group>. <article-title>Neural implementation of computational mechanisms underlying the continuous trade-off between cooperation and competition</article-title>. <source>Nature Communications</source>. <year>2022</year> <month>Nov</month>; <volume>13</volume>(<issue>1</issue>):<fpage>6873</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-022-34509-w">https://www.nature.com/articles/s41467-022-34509-w</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41467-022-34509-w</pub-id>.</mixed-citation></ref>
<ref id="c113"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poh</surname> <given-names>MZ</given-names></string-name>, <string-name><surname>McDuff</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Picard</surname> <given-names>RW</given-names></string-name></person-group>. <article-title>Advancements in Noncontact, Multiparameter Physiological Measurements Using a Webcam</article-title>. <source>IEEE Transactions on Biomedical Engineering</source>. <year>2011</year> <month>Jan</month>; <volume>58</volume>(<issue>1</issue>):<fpage>7</fpage>–<lpage>11</lpage>. <ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/document/5599853">https://ieeexplore.ieee.org/document/5599853</ext-link>, doi: <pub-id pub-id-type="doi">10.1109/TBME.2010.2086456</pub-id>.</mixed-citation></ref>
<ref id="c114"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Prochazkova</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kret</surname> <given-names>ME</given-names></string-name></person-group>. <article-title>Connecting minds and sharing emotions through mimicry: A neurocognitive model of emotional contagion</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>. <year>2017</year>; <volume>80</volume>:<fpage>99</fpage>–<lpage>114</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0149763416306704">https://www.sciencedirect.com/science/article/pii/S0149763416306704</ext-link>, <pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.05.013</pub-id>.</mixed-citation></ref>
<ref id="c115"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pryluk</surname> <given-names>R</given-names></string-name>, <string-name><surname>Shohat</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Morozov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Friedman</surname> <given-names>D</given-names></string-name>, <string-name><surname>Taub</surname> <given-names>AH</given-names></string-name>, <string-name><surname>Paz</surname> <given-names>R</given-names></string-name></person-group>. <article-title>Shared yet dissociable neural codes across eye gaze, valence and expectation</article-title>. <source>Nature</source>. <year>2020</year> <month>Oct</month>; <volume>586</volume>(<issue>7827</issue>):<fpage>95</fpage>–<lpage>100</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41586-020-2740-8">https://www.nature.com/articles/s41586-020-2740-8</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41586-020-2740-8</pub-id>.</mixed-citation></ref>
<ref id="c116"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Radford</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>T</given-names></string-name>, <string-name><surname>Brockman</surname> <given-names>G</given-names></string-name>, <string-name><surname>Mcleavey</surname> <given-names>C</given-names></string-name>, <string-name><surname>Sutskever</surname> <given-names>I</given-names></string-name></person-group>. <article-title>Robust Speech Recognition via Large-Scale Weak Supervision</article-title>. In: <conf-name>Proceedings of the 40th International Conference on Machine Learning PMLR</conf-name>; <year>2023</year>. p. <fpage>28492</fpage>–<lpage>28518</lpage>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v202/radford23a.html">https://proceedings.mlr.press/v202/radford23a.html</ext-link>.</mixed-citation></ref>
<ref id="c117"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rilling</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Sanfey</surname> <given-names>AG</given-names></string-name></person-group>. <article-title>The Neuroscience of Social Decision-Making</article-title>. <source>Annual Review of Psychology</source>. <year>2011</year> <month>Jan</month>; <volume>62</volume>(Volume <volume>62</volume>, <year>2011</year>):<fpage>23</fpage>–<lpage>48</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.annualreviews.org/content/journals/10.1146/annurev.psych.121208.131647">https://www.annualreviews.org/content/journals/10.1146/annurev.psych.121208.131647</ext-link>, doi: <pub-id pub-id-type="doi">10.1146/annurev.psych.121208.131647</pub-id>.</mixed-citation></ref>
<ref id="c118"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Risko</surname> <given-names>EF</given-names></string-name>, <string-name><surname>Richardson</surname> <given-names>DC</given-names></string-name>, <string-name><surname>Kingstone</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Breaking the Fourth Wall of Cognitive Science: Real-World Social Attention and the Dual Function of Gaze</article-title>. <source>Current Directions in Psychological Science</source>. <year>2016</year> <month>Feb</month>; <volume>25</volume>(<issue>1</issue>):<fpage>70</fpage>–<lpage>74</lpage>. <pub-id pub-id-type="doi">10.1177/0963721415617806</pub-id>, doi: <pub-id pub-id-type="doi">10.1177/0963721415617806</pub-id>.</mixed-citation></ref>
<ref id="c119"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rollwage</surname> <given-names>M</given-names></string-name>, <string-name><surname>Pannach</surname> <given-names>F</given-names></string-name>, <string-name><surname>Stinson</surname> <given-names>C</given-names></string-name>, <string-name><surname>Toelch</surname> <given-names>U</given-names></string-name>, <string-name><surname>Kagan</surname> <given-names>I</given-names></string-name>, <string-name><surname>Pooresmaeili</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Judgments of effort exerted by others are influenced by received rewards</article-title>. <source>Scientific Reports</source>. <year>2020</year> <month>Feb</month>; <volume>10</volume>(<issue>1</issue>):<fpage>1868</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-020-58686-0</pub-id>.</mixed-citation></ref>
<ref id="c120"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruggeri</surname> <given-names>A</given-names></string-name>, <string-name><surname>Markant</surname> <given-names>DB</given-names></string-name>, <string-name><surname>Gureckis</surname> <given-names>TM</given-names></string-name>, <string-name><surname>Bretzke</surname> <given-names>M</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>F</given-names></string-name></person-group>. <article-title>Memory enhancements from active control of learning emerge across development</article-title>. <source>Cognition</source>. <year>2019</year>; <volume>186</volume>:<fpage>82</fpage>–<lpage>94</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cognition.2019.01.010</pub-id>.</mixed-citation></ref>
<ref id="c121"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sanfey</surname> <given-names>AG</given-names></string-name></person-group>. <article-title>Social Decision-Making: Insights from Game Theory and Neuroscience</article-title>. <source>Science</source>. <year>2007</year> <month>Oct</month>; <volume>318</volume>(<issue>5850</issue>):<fpage>598</fpage>–<lpage>602</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/abs/10.1126/science.1142996">https://www.science.org/doi/abs/10.1126/science.1142996</ext-link>, doi: <pub-id pub-id-type="doi">10.1126/science.1142996</pub-id>.</mixed-citation></ref>
<ref id="c122"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scherbaum</surname> <given-names>S</given-names></string-name>, <string-name><surname>Dshemuchadse</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Psychometrics of the continuous mind: Measuring cognitive sub-processes via mouse tracking</article-title>. <source>Memory &amp; Cognition</source>. <year>2020</year> <month>Apr</month>; <volume>48</volume>(<issue>3</issue>):<fpage>436</fpage>–<lpage>454</lpage>. <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.3758/s13421-019-00981-x">http://link.springer.com/10.3758/s13421-019-00981-x</ext-link>, doi: <pub-id pub-id-type="doi">10.3758/s13421-019-00981-x</pub-id>.</mixed-citation></ref>
    <ref id="c123"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Schneider</surname> <given-names>F</given-names></string-name>, <string-name><surname>Calapai</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mundry</surname> <given-names>R</given-names></string-name>, <string-name><surname>Báez-Mendoza</surname> <given-names>R</given-names></string-name>, <string-name><surname>Gail</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kagan</surname> <given-names>I</given-names></string-name>, <string-name><surname>Treue</surname> <given-names>S</given-names></string-name></person-group>. <article-title>Confidence over competence: Real-time integration of social information in human continuous perceptual decision-making</article-title>. <source>bioRxiv</source>. <year>2024</year> <month>Aug</month>; p. <fpage>2024.08.19.608609</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2024.08.19.608609v1">https://www.biorxiv.org/content/10.1101/2024.08.19.608609v1</ext-link>, doi: <pub-id pub-id-type="doi">10.1101/2024.08.19.608609</pub-id>.</mixed-citation></ref>
<ref id="c124"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Senju</surname> <given-names>A</given-names></string-name>, <string-name><surname>Csibra</surname> <given-names>G</given-names></string-name></person-group>. <article-title>Gaze Following in Human Infants Depends on Communicative Signals</article-title>. <source>Current Biology</source>. <year>2008</year> <month>May</month>; <volume>18</volume>(<issue>9</issue>):<fpage>668</fpage>–<lpage>671</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.cell.com/current-biology/abstract/S0960-9822(08)00455-7">https://www.cell.com/current-biology/abstract/S0960-9822(08)00455-7</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cub.2008.03.059</pub-id>.</mixed-citation></ref>
<ref id="c125"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname> <given-names>JM</given-names></string-name></person-group>. <article-title>Game theory and the evolution of behaviour</article-title>. <source>Proceedings of the Royal Society of London Series B Biological Sciences</source>. <year>1997</year> <month>Jan</month>; <volume>205</volume>(<issue>1161</issue>):<fpage>475</fpage>–<lpage>488</lpage>. <ext-link ext-link-type="uri" xlink:href="https://royalsocietypublishing.org/doi/abs/10.1098/rspb.1979.0080">https://royalsocietypublishing.org/doi/abs/10.1098/rspb.1979.0080</ext-link>, doi: <pub-id pub-id-type="doi">10.1098/rspb.1979.0080</pub-id>.</mixed-citation></ref>
<ref id="c126"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Grundmann</surname> <given-names>O</given-names></string-name>, <string-name><surname>Li</surname> <given-names>RM</given-names></string-name></person-group>. <article-title>The development and impact of active learning strategies on self-confidence in a newly designed first-year self-care pharmacy course-outcomes and experiences</article-title>. <source>Currents in Pharmacy Teaching and Learning</source>. <year>2018</year>; <volume>10</volume>(<issue>4</issue>):<fpage>499</fpage>–<lpage>504</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cptl.2017.12.008</pub-id>.</mixed-citation></ref>
<ref id="c127"><mixed-citation publication-type="book"><person-group person-group-type="author"><collab>Society of Motion Picture and Television Engineers</collab></person-group>. <source>Critical Viewing Conditions for Evaluation of Color Television Pictures</source>; <year>1995</year>, doi: <pub-id pub-id-type="doi">10.5594/SMPTE.RP166.1995</pub-id>.</mixed-citation></ref>
<ref id="c128"><mixed-citation publication-type="book"><person-group person-group-type="author"><collab>Society of Motion Picture and Television Engineers</collab></person-group>. <source>Screen Luminance Level, Chromaticity and Uniformity</source>; <year>2006</year>, doi: <pub-id pub-id-type="doi">10.5594/SMPTE.ST431-1.2006</pub-id>.</mixed-citation></ref>
<ref id="c129"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spivey</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Grosjean</surname> <given-names>M</given-names></string-name>, <string-name><surname>Knoblich</surname> <given-names>G</given-names></string-name></person-group>. <article-title>Continuous attraction toward phonological competitors</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2005</year> <month>Jul</month>; <volume>102</volume>(<issue>29</issue>):<fpage>10393</fpage>–<lpage>10398</lpage>. <ext-link ext-link-type="uri" xlink:href="https://pnas.org/doi/full/10.1073/pnas.0503903102">https://pnas.org/doi/full/10.1073/pnas.0503903102</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.0503903102</pub-id>.</mixed-citation></ref>
<ref id="c130"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steixner-Kumar</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rusch</surname> <given-names>T</given-names></string-name>, <string-name><surname>Doshi</surname> <given-names>P</given-names></string-name>, <string-name><surname>Spezio</surname> <given-names>M</given-names></string-name>, <string-name><surname>Glascher</surname> <given-names>J</given-names></string-name></person-group>. <article-title>Humans depart from optimal computational models of interactive decision-making during competition under partial information</article-title>. <source>Scientific Reports</source>. <year>2022</year> <month>Jan</month>; <volume>12</volume>(<issue>1</issue>):<fpage>289</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41598-021-04272-x">https://www.nature.com/articles/s41598-021-04272-x</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41598-021-04272-x</pub-id>.</mixed-citation></ref>
<ref id="c131"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Suriya-Arunroj</surname> <given-names>L</given-names></string-name>, <string-name><surname>Gail</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Complementary encoding of priors in monkey frontoparietal network supports a dual process of decision-making</article-title>. <source>eLife</source>. <year>2019</year> <month>Oct</month>; <volume>8</volume>:<elocation-id>e47581</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.47581</pub-id>, doi: <pub-id pub-id-type="doi">10.7554/eLife.47581</pub-id>.</mixed-citation></ref>
<ref id="c132"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Takagaki</surname> <given-names>K</given-names></string-name>, <string-name><surname>Krug</surname> <given-names>K</given-names></string-name></person-group>. <article-title>The effects of reward and social context on visual processing for perceptual decisionmaking</article-title>. <source>Current Opinion in Physiology</source>. <year>2020</year> <month>Aug</month>; <volume>16</volume>:<fpage>109</fpage>–<lpage>117</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S2468867320300894">https://www.sciencedirect.com/science/article/pii/S2468867320300894</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cophys.2020.08.006</pub-id>.</mixed-citation></ref>
<ref id="c133"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname> <given-names>H</given-names></string-name>, <string-name><surname>Mai</surname> <given-names>X</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>S</given-names></string-name>, <string-name><surname>Zhu</surname> <given-names>C</given-names></string-name>, <string-name><surname>Krueger</surname> <given-names>F</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>C</given-names></string-name></person-group>. <article-title>Interpersonal brain synchronization in the right temporoparietal junction during face-to-face economic exchange</article-title>. <source>Social Cognitive and Affective Neuroscience</source>. <year>2016</year> <month>Jan</month>; <volume>11</volume>(<issue>1</issue>):<fpage>23</fpage>–<lpage>32</lpage>. <ext-link ext-link-type="uri" xlink:href="https://academic.oup.com/scan/article/11/1/23/2375145">https://academic.oup.com/scan/article/11/1/23/2375145</ext-link>, doi: <pub-id pub-id-type="doi">10.1093/scan/nsv092</pub-id>.</mixed-citation></ref>
<ref id="c134"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Toelch</surname> <given-names>U</given-names></string-name>, <string-name><surname>Dolan</surname> <given-names>RJ</given-names></string-name></person-group>. <article-title>Informational and Normative Influences in Conformity from a Neurocomputational Perspective</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2015</year> <month>Oct</month>; <volume>19</volume>(<issue>10</issue>):<fpage>579</fpage>–<lpage>589</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(15)00171-0">https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(15)00171-0</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.tics.2015.07.007</pub-id>.</mixed-citation></ref>
<ref id="c135"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tremblay</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sharika</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Platt</surname> <given-names>ML</given-names></string-name></person-group>. <article-title>Social Decision-Making and the Brain: A Comparative Perspective</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2017</year> <month>Apr</month>; <volume>21</volume>(<issue>4</issue>):<fpage>265</fpage>–<lpage>276</lpage>. <ext-link ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S136466131730013X">http://linkinghub.elsevier.com/retrieve/pii/S136466131730013X</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.tics.2017.01.007</pub-id>.</mixed-citation></ref>
<ref id="c136"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Treue</surname> <given-names>S</given-names></string-name></person-group>. <article-title>Neural correlates of attention in primate visual cortex</article-title>. <source>Trends in Neurosciences</source>. <year>2001</year> <month>May</month>; <volume>24</volume>(<issue>5</issue>):<fpage>295</fpage>–<lpage>300</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S0166-2236(00)01814-2</pub-id>.</mixed-citation></ref>
<ref id="c137"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Treue</surname> <given-names>S</given-names></string-name></person-group>. <article-title>Visual attention: the where, what, how and why of saliency</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2003</year> <month>Aug</month>; <volume>13</volume>(<issue>4</issue>):<fpage>428</fpage>–<lpage>432</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S0959-4388(03)00105-3</pub-id>.</mixed-citation></ref>
<ref id="c138"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ulbrich</surname> <given-names>P</given-names></string-name>, <string-name><surname>Gail</surname> <given-names>A</given-names></string-name></person-group>. <article-title>The cone method: Inferring decision times from single-trial 3D movement trajectories in choice behavior</article-title>. <source>Behavior Research Methods</source>. <year>2021</year> <month>Apr</month>; <volume>53</volume>(<issue>6</issue>):<fpage>2456</fpage>–<lpage>2472</lpage>. <ext-link ext-link-type="uri" xlink:href="https://link.springer.com/10.3758/s13428-021-01579-5">https://link.springer.com/10.3758/s13428-021-01579-5</ext-link>, doi: <pub-id pub-id-type="doi">10.3758/s13428-021-01579-5</pub-id>.</mixed-citation></ref>
<ref id="c139"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ulbrich</surname> <given-names>P</given-names></string-name>, <string-name><surname>Gail</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Deciding While Acting—Mid-Movement Decisions Are More Strongly Affected by Action Probability than Reward Amount</article-title>. <source>eNeuro</source>. <year>2023</year> <month>Apr</month>; <volume>10</volume>(<issue>4</issue>). <ext-link ext-link-type="uri" xlink:href="https://www.eneuro.org/content/10/4/ENEURO.0240-22.2023">https://www.eneuro.org/content/10/4/ENEURO.0240-22.2023</ext-link>, doi: <pub-id pub-id-type="doi">10.1523/ENEURO.0240-22.2023</pub-id>.</mixed-citation></ref>
<ref id="c140"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Unakafov</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Möller</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kagan</surname> <given-names>I</given-names></string-name>, <string-name><surname>Gail</surname> <given-names>A</given-names></string-name>, <string-name><surname>Treue</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wolf</surname> <given-names>F</given-names></string-name></person-group>. <article-title>Using imaging photoplethysmography for heart rate estimation in non-human primates</article-title>. <source>PLOS One</source>. <year>2018</year> <month>08</month>; <volume>13</volume>(<issue>8</issue>):<fpage>1</fpage>–<lpage>22</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0202581</pub-id>, doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0202581</pub-id>.</mixed-citation></ref>
<ref id="c141"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Unakafov</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Schultze</surname> <given-names>T</given-names></string-name>, <string-name><surname>Gail</surname> <given-names>A</given-names></string-name>, <string-name><surname>Moeller</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kagan</surname> <given-names>I</given-names></string-name>, <string-name><surname>Eule</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wolf</surname> <given-names>F</given-names></string-name></person-group>. <article-title>Emergence and suppression of cooperation by action visibility in transparent games</article-title>. <source>PLoS Computational Biology</source>. <year>2020</year> <month>Jan</month>; <volume>16</volume>(<issue>1</issue>). <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6975562/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6975562/</ext-link>, doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1007588</pub-id>.</mixed-citation></ref>
<ref id="c142"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Unakafov</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Schultze</surname> <given-names>T</given-names></string-name>, <string-name><surname>Kagan</surname> <given-names>I</given-names></string-name>, <string-name><surname>Moeller</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gail</surname> <given-names>A</given-names></string-name>, <string-name><surname>Treue</surname> <given-names>S</given-names></string-name>, <string-name><surname>Eule</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wolf</surname> <given-names>F</given-names></string-name></person-group>. <chapter-title>Evolutionary Successful Strategies in a Transparent iterated Prisoner’s Dilemma</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Kaufmann</surname> <given-names>P</given-names></string-name>, <string-name><surname>Castillo</surname> <given-names>PA</given-names></string-name></person-group>, editors. <source>Applications of Evolutionary Computation</source> <publisher-name>Springer International Publishing</publisher-name>; <year>2019</year>. p. <fpage>204</fpage>–<lpage>219</lpage>. doi: <pub-id pub-id-type="doi">10.1007/978-3-030-16692-2_14</pub-id>.</mixed-citation></ref>
<ref id="c143"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Varlet</surname> <given-names>M</given-names></string-name>, <string-name><surname>Grootswagers</surname> <given-names>T</given-names></string-name></person-group>. <article-title>Measuring information alignment in hyperscanning research with representational analyses: moving beyond interbrain synchrony</article-title>. <source>Frontiers in Human Neuroscience</source>. <year>2024</year> <month>Jul</month>; <fpage>18</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2024.1385624/full">https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2024.1385624/full</ext-link>, doi: <pub-id pub-id-type="doi">10.3389/fn-hum.2024.1385624</pub-id>.</mixed-citation></ref>
<ref id="c144"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wahn</surname> <given-names>B</given-names></string-name>, <string-name><surname>Kingstone</surname> <given-names>A</given-names></string-name>, <string-name><surname>König</surname> <given-names>P</given-names></string-name></person-group>. <article-title>Group benefits in joint perceptual tasks—a review</article-title>. <source>Annals of the New York Academy of Sciences</source>. <year>2018</year>; <volume>1426</volume>(<issue>1</issue>):<fpage>166</fpage>–<lpage>178</lpage>. <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/abs/10.1111/nyas.13843">https://onlinelibrary.wiley.com/doi/abs/10.1111/nyas.13843</ext-link>, doi: <pub-id pub-id-type="doi">10.1111/nyas.13843</pub-id>.</mixed-citation></ref>
<ref id="c145"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Witthoft</surname> <given-names>N</given-names></string-name>, <string-name><surname>Sha</surname> <given-names>L</given-names></string-name>, <string-name><surname>Winawer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kiani</surname> <given-names>R</given-names></string-name></person-group>. <article-title>Sensory and decision-making processes underlying perceptual adaptation</article-title>. <source>Journal of Vision</source>. <year>2018</year> <month>Aug</month>; <volume>18</volume>(<issue>8</issue>):<fpage>10</fpage>. doi: <pub-id pub-id-type="doi">10.1167/18.8.10</pub-id>.</mixed-citation></ref>
<ref id="c146"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yoo</surname> <given-names>SBM</given-names></string-name>, <string-name><surname>Hayden</surname> <given-names>BY</given-names></string-name>, <string-name><surname>Pearson</surname> <given-names>JM</given-names></string-name></person-group>. <article-title>Continuous decisions</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>. <year>2021</year> <month>Jan</month>; <volume>376</volume>(<issue>1819</issue>):<fpage>20190664</fpage>. <ext-link ext-link-type="uri" xlink:href="https://royalsocietypublishing.org/doi/10.1098/rstb.2019.0664">https://royalsocietypublishing.org/doi/10.1098/rstb.2019.0664</ext-link>, doi: <pub-id pub-id-type="doi">10.1098/rstb.2019.0664</pub-id>.</mixed-citation></ref>
<ref id="c147"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yoo</surname> <given-names>SBM</given-names></string-name>, <string-name><surname>Tu</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Hayden</surname> <given-names>BY</given-names></string-name></person-group>. <article-title>Multicentric tracking of multiple agents by anterior cingulate cortex during pursuit and evasion</article-title>. <source>Nature Communications</source>. <year>2021</year> <month>Mar</month>; <volume>12</volume>(<issue>1</issue>):<fpage>1985</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-021-22195-z">https://www.nature.com/articles/s41467-021-22195-z</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41467-021-22195-z</pub-id>.</mixed-citation></ref>
<ref id="c148"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yoo</surname> <given-names>SBM</given-names></string-name>, <string-name><surname>Tu</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Piantadosi</surname> <given-names>ST</given-names></string-name>, <string-name><surname>Hayden</surname> <given-names>BY</given-names></string-name></person-group>. <article-title>The neural basis of predictive pursuit</article-title>. <source>Nature Neuroscience</source>. <year>2020</year> <month>Feb</month>; <volume>23</volume>(<issue>2</issue>):<fpage>252</fpage>–<lpage>259</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.nature.com/articles/s41593-019-0561-6">http://www.nature.com/articles/s41593-019-0561-6</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41593-019-0561-6</pub-id>.</mixed-citation></ref>
<ref id="c149"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yoshida</surname> <given-names>K</given-names></string-name>, <string-name><surname>Saito</surname> <given-names>N</given-names></string-name>, <string-name><surname>Iriki</surname> <given-names>A</given-names></string-name>, <string-name><surname>Isoda</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Representation of Others’ Action by Neurons in Monkey Medial Frontal Cortex</article-title>. <source>Current Biology</source>. <year>2011</year> <month>Aug</month>; <volume>21</volume>(<issue>3</issue>):<fpage>249</fpage>–<lpage>253</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.cell.com/article/S0960982211000273/abstract">http://www.cell.com/article/S0960982211000273/abstract</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cub.2011.01.004</pub-id>.</mixed-citation></ref>
<ref id="c150"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname> <given-names>C</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>LB</given-names></string-name></person-group>. <article-title>Joint Attention without Gaze Following: Human Infants and Their Parents Coordinate Visual Attention to Objects through Eye-Hand Coordination</article-title>. <source>PLOS One</source>. <year>2013</year> <month>Nov</month>; <volume>8</volume>(<issue>11</issue>):<elocation-id>e79659</elocation-id>. <ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079659">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079659</ext-link>, doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0079659</pub-id>.</mixed-citation></ref>
<ref id="c151"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname> <given-names>L</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>C</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>DH</given-names></string-name></person-group>. <article-title>Communication speeds up but impairs the consensus decision in a dyadic colour estimation task</article-title>. <source>Royal Society Open Science</source>. <year>2020</year> <month>Jul</month>; <volume>7</volume>(<issue>7</issue>):<fpage>191974</fpage>. <ext-link ext-link-type="uri" xlink:href="https://royalsocietypublishing.org/doi/10.1098/rsos.191974">https://royalsocietypublishing.org/doi/10.1098/rsos.191974</ext-link>, doi: <pub-id pub-id-type="doi">10.1098/rsos.191974</pub-id>.</mixed-citation></ref>
<ref id="c152"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhao</surname> <given-names>K</given-names></string-name>, <string-name><surname>Smillie</surname> <given-names>LD</given-names></string-name></person-group>. <article-title>The Role of Interpersonal Traits in Social Decision Making: Exploring Sources of Behavioral Heterogeneity in Economic Games</article-title>. <source>Personality and Social Psychology Review</source>. <year>2015</year> <month>Aug</month>; <volume>19</volume>(<issue>3</issue>):<fpage>277</fpage>–<lpage>302</lpage>. doi: <pub-id pub-id-type="doi">10.1177/1088868314553709</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106757.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Peelle</surname>
<given-names>Jonathan Erik</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01yc7t268</institution-id><institution>Northeastern University</institution>
</institution-wrap>
<city>Boston</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> paper introduces the Dyadic Interaction Platform, an experimental setup that enables researchers to study real-time social interactions between two participants in a controlled environment while maintaining direct face-to-face visibility. The evidence supporting the platform's effectiveness is <bold>convincing</bold>, with demonstrations of distinct experimental paradigms showing how transparency and continuous access to partners' actions can influence strategic coordination, decision-making, and learning. The work will be of broad interest to researchers studying social cognition across humans and non-human primates, providing a versatile tool that bridges the gap between naturalistic social interactions and controlled laboratory experiments.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106757.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this manuscript, the authors aim to address significant limitations of existing experimental paradigms used to study dyadic social interactions by introducing a novel experimental setup - the Dyadic Interaction Platform (DIP). The DIP uniquely allows participants to interact dynamically, face-to-face, with simultaneous access to both social cues and task-related stimuli. The authors demonstrate the versatility and utility of this platform across several exemplary scenarios, notably highlighting cases of significant behavioral differences in conditions involving direct visibility of a partner.</p>
<p>Major strengths include comprehensive descriptions of previous paradigms, detailed explanations of the DIP's technical features, and clear illustrations of multimodal data integration. These elements greatly enhance the reproducibility of the methods and clarify the potential applications across various research domains and species. Particularly compelling is the authors' demonstration of behavioral impacts related to transparency in interactions, as evidenced by the macaque-human experiments using the Bach-or-Stravinsky game scenario.</p>
<p>Strengths:</p>
<p>The DIP represents a methodological advance in the study of social cognition. Its transparent, touch-sensitive display elegantly solves the problem of enabling participants to attend to both their social partner and task stimuli simultaneously without requiring attention switching. This paper marks a notable step forward toward more options for naturalistic yet still lab-based studies of social decision-making, an area where the field is actively moving, especially given recent research highlighting significant differences in neural activity depending upon the context in which an action is performed. The DIP offers researchers a valuable tool to bridge the gap between tightly controlled laboratory paradigms and the dynamic, bidirectional nature of real-world social interactions.</p>
<p>The authors do well to provide comprehensive documentation of the technical specifications for the four different implementations of the platform, allowing other researchers to adapt and build upon their work. The detailed information about hardware configurations demonstrates careful attention to practical implementation details. They also highlight numerous options for integration with other tools and software, further demonstrating the versatility of this apparatus and the variety of research questions to which it could be applied.</p>
<p>The historical review of dyadic experimental paradigms is thorough and effectively positions the DIP as addressing a critical gap in existing methodologies. The authors convincingly argue that studying continuous, dynamic social interactions is essential for understanding real-world social cognition, and that existing paradigms often force unnatural attention-splitting or turn-taking behaviors that don't reflect naturalistic interaction patterns.</p>
<p>The four example applications showcase the DIP's versatility across diverse research questions. The Bach-or-Stravinsky economic game example is particularly compelling, demonstrating how continuous access to partners' actions substantially changes coordination strategies in non-human primates. This highlights a key strength of the DIP, which is that it removes a level of abstraction that can make tasks more difficult for non-human primates to learn. By being able to see their partner and actions directly, rather than having to understand that a cursor on a screen represents a partner, the platform makes the task more accessible to non-human primates and possibly children as well. This opens up important avenues for enhanced cross-species investigations of cognition, allowing researchers to study social dynamics in a setting that remains naturalistic yet controlled across different populations.</p>
<p>Weaknesses:</p>
<p>Some of the experimental applications would benefit from stronger evidence demonstrating the unique advantages of the transparent setup. For instance, in the dyadic foraging example, it's not entirely clear how participants' behavior differs from what might be observed when simply tracking each other's cursor movements in a non-transparent setup. More evidence showing how direct visibility of the partner, beyond simply being able to track the position of the partner's cursor, influences behavior would strengthen this example. Similarly, in the continuous perceptual report (CPR) task, the subjects could perform this task and see feedback from their partners' actions without having to see their partner through the transparent screen. Evidence showing that 1) subjects do indeed look at their partner during the task and 2) viewing their partner influences their performance on the task would significantly strengthen the claim that the ability to view the partner brings in a new dimension to this task. These additions would better demonstrate the specific value added by the transparent nature of the DIP beyond what could be achieved with standard cursor-tracking paradigms.</p>
<p>A significant limitation that is inadequately addressed relates to neural investigations. While the authors position the platform's ability to merge attention to social stimuli and task stimuli as a key advantage, they don't sufficiently acknowledge the challenges this creates for dissociating neural signals attributed to social cues versus task-based stimuli. More traditional lab-based experiments intentionally separate components like task-stimulus perception, social perception, and decision-making periods so that researchers can isolate the neural signals associated with each process. This deliberate separation, which the authors frame as a weakness, actually serves an important functional purpose in neural investigations. The paper would be strengthened by explicitly discussing this limitation and offering potential approaches to address it in experimental design or data analysis. For instance, the authors could suggest methodological innovations or analytical techniques that might help disentangle the overlapping neural signals that would inevitably arise from the integrated presentation of social and task stimuli in the DIP setup.</p>
<p>Furthermore, the authors' suggestion to arrange task stimuli around the periphery of the screen to maintain a clear middle area for viewing the partner appears to contradict their own critique of traditional paradigms. This recommended arrangement would seemingly reintroduce the very problem of attentional switching between task stimuli and social partners that the authors identified as a limitation of previous approaches. The paper would be strengthened by discussing the potential trade-offs associated with their suggested stimulus arrangement. Additionally, offering potential approaches to address these limitations in experimental design or data analysis would enhance the paper's contribution to the field.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106757.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This work proposes a new platform to study social cognition in a more naturalistic setting. The authors give an overview of previous work that extends from static unidirectional paradigms (i.e., subject is presented with social stimuli such as still images or faces), to more dynamic unidirectional paradigms (i.e., the subject is presented with movies, or another individual's behavior) to dyadic interactions in a laboratory setting or in real life (i.e., interacting with a real person). Overall, this literature demonstrates that findings from realistic social situations can differ dramatically from unidirectional laboratory settings. Moreover, current and previous work are put in the perspective of an experimental framework that has tightly controlled experimental set-ups and low ecological validity on one end, and high ecological validity, naturalistic, without any experimental constraints on the other end, and all that is in between. The authors frame previous work along a spectrum, ranging from highly controlled, low-ecological-validity experiments to naturalistic, unconstrained approaches with high ecological validity, situating their current work within this continuum. They focus on a specific sub-domain of social interactions, i.e., goal-directed contexts in which interactions are purposeful for solving joint tasks or obtaining rewards. This new dyadic interaction platform claims to embed tight experimental control in a naturalistic face-to-face social interaction with the goal of investigating social information processing in bidirectional, dynamic social interactions.</p>
<p>Strengths:</p>
<p>The proposed dyadic interaction platform (DIP) is highly flexible, accommodating diverse visual displays, interactive components, and recording devices, making it suitable for various experiments.</p>
<p>The manuscript does a good job of highlighting the strengths and weaknesses of the various display options. This clarity allows readers to easily assess which display best suits their specific experimental setup and objectives.</p>
<p>One of the platform's key strengths is its versatility, allowing the same experimental setup to be used across multiple species and developmental stages, and enabling NHPs and humans to be studied as subjects within the same paradigm. Highlighting this capability could further underscore the platform's broad applicability.</p>
<p>Weaknesses:</p>
<p>The manuscript emphasizes the importance of ecological validity alongside tight experimental control, a significant challenge in naturalistic neuroscience. While the platform achieves tight control, the ecological validity of such a set-up remains questionable and warrants further testing and validation. For example, while the platform is designed to be more naturalistic in principle, its application to NHPs is still complex and may be comparably constrained as traditional NHP research. To realize its full potential for animal studies, the platform should be combined with complementary methodologies - such as wireless electrophysiology and freely moving paradigms - to truly achieve a balance between ecological validity and experimental control. Further validation in this direction could significantly enhance its utility.</p>
<p>The manuscript is somewhat lengthy and occasionally reads more like a review paper, which slightly shifts the focus away from the primary emphasis on the innovative technological advancement and the considerable effort invested in optimizing this new platform. Streamlining the presentation to more directly highlight these key contributions could enhance clarity and impact.</p>
<p>Overall, there is compelling evidence supporting the feasibility and value of DIP for investigating specific types of social interactions, particularly in contexts where individuals share a workspace and have full transparency regarding their opponent's actions. While I believe that DIP has the potential to significantly impact the field, which is supported by preliminary data, its broader applicability remains an open question. This platform aligns well with recent initiatives aimed at enhancing ecological validity in neuroscience research across both human and animal models. To maximize its impact, it would be beneficial to more explicitly situate this work within that broader movement, emphasizing its relevance and potential to advance ecologically valid approaches in the field.</p>
</body>
</sub-article>
</article>