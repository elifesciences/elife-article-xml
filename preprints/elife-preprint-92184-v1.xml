<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">92184</article-id>
<article-id pub-id-type="doi">10.7554/eLife.92184</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.92184.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Protein language model embedded geometric graphs power inter-protein contact prediction</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Si</surname>
<given-names>Yunda</given-names>
</name>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2010-6668</contrib-id>
<name>
<surname>Yan</surname>
<given-names>Chengfei</given-names>
</name>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff><institution>School of Physics, Huazhong University of Science and Technology</institution>, <country>China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Bitbol</surname>
<given-names>Anne-Florence</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Ecole Polytechnique Federale de Lausanne (EPFL)</institution>
</institution-wrap>
<city>Lausanne</city>
<country>Switzerland</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Walczak</surname>
<given-names>Aleksandra M</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>CNRS</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Correspondence: <email>chengfeiyan@hust.edu.cn</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-12-12">
<day>12</day>
<month>12</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP92184</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-09-04">
<day>04</day>
<month>09</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-08-24">
<day>24</day>
<month>08</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.01.07.523121"/>
</event>
</pub-history>
<permissions>
<copyright-statement>Â© 2023, Si &amp; Yan</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Si &amp; Yan</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-92184-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Accurate prediction of contacting residue pairs between interacting proteins is very useful for structural characterization of protein-protein interactions (PPIs). Although significant improvement has been made in inter-protein contact prediction recently, there is still large room for improving the prediction accuracy. Here we present a new deep learning method referred to as PLMGraph-Inter for inter-protein contact prediction. Specifically, we employ rotationally and translationally invariant geometric graphs obtained from structures of interacting proteins to integrate multiple protein language models, which are successively transformed by graph encoders formed by geometric vector perceptrons and residual networks formed by dimensional hybrid residual blocks to predict inter-protein contacts. Extensive evaluation on multiple test sets illustrates that PLMGraph-Inter outperforms five top inter-protein contact prediction methods, including DeepHomo, GLINTER, CDPred, DeepHomo2 and DRN-1D2D_Inter by large margins. In addition, we also show that the prediction of PLMGraph-Inter can complement the result of AlphaFold-Multimer. Finally, we show leveraging the contacts predicted by PLMGraph-Inter as constraints for protein-protein docking can dramatically improve its performance for protein complex structure prediction.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Additional benchmarking studies were included in the revision.</p></fn>
</fn-group>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/ChengfeiYan/PLMGraph-Inter">https://github.com/ChengfeiYan/PLMGraph-Inter</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Protein-protein interactions(PPIs) are essential activities of most cellular processes(<xref ref-type="bibr" rid="c1">Alberts, 1998</xref>; <xref ref-type="bibr" rid="c38">Spirin &amp; Mirny, 2003</xref>). Structure characterization of PPIs is important for mechanistic investigation of these cellular processes and therapeutic development(<xref ref-type="bibr" rid="c9">Goodsell &amp; Olson, 2000</xref>). However, currently experimental structures of many important PPIs are still missing as experimental methods to resolve complex structures such as X-ray crystallography, nuclear magnetic resonance, cryo-electron microscopy are costly and time-consuming(<xref ref-type="bibr" rid="c3">Berman et al., 2000</xref>). Therefore, it is necessary to develop computational methods to predict protein complex structures(<xref ref-type="bibr" rid="c4">Bonvin, 2006</xref>). Predicting contacting residue pairs between interacting proteins can be considered as an intermediate step for protein complex structure prediction(<xref ref-type="bibr" rid="c14">Hopf et al., 2014</xref>; <xref ref-type="bibr" rid="c26">Ovchinnikov et al., 2014</xref>), as the predicted contacts can be integrated into protein-protein docking algorithms to assist protein complex structure prediction(<xref ref-type="bibr" rid="c7">Dominguez et al., 2003</xref>; <xref ref-type="bibr" rid="c21">H. Li &amp; Huang, 2021</xref>; <xref ref-type="bibr" rid="c40">Sun et al., 2020</xref>). Besides, the predicted contacts can also be very useful to guide protein interfacial design(<xref ref-type="bibr" rid="c25">Martino et al., 2021</xref>) to and the inter-protein contact prediction methods can be further extended to predict novel PPIs (<xref ref-type="bibr" rid="c5">Cong et al., 2019</xref>; <xref ref-type="bibr" rid="c10">Green et al., 2021</xref>).</p>
<p>Based on the fact that contacting residue pairs often vary co-operatively during evolution, coevolutionary analysis methods(<xref ref-type="bibr" rid="c46">Weigt et al., 2009</xref>) have been used in previous studies to predict inter-protein contacts(<xref ref-type="bibr" rid="c14">Hopf et al., 2014</xref>; <xref ref-type="bibr" rid="c26">Ovchinnikov et al., 2014</xref>). However, coevolutionary analysis methods do have certain limitations. For examples, effective coevolutionary analysis requires a large number of interolog sequences, which are often difficult to obtain, especially for heteromeric PPIs(<xref ref-type="bibr" rid="c30">R. M. Rao et al., 2021a</xref>); and it is difficult to distinguish inter-protein and intra-protein coevolutionary signals for homomeric PPIs(<xref ref-type="bibr" rid="c42">Uguzzoni et al., 2017</xref>). Inspired by its great success in intra-protein contact prediction(<xref ref-type="bibr" rid="c12">Hanson et al., 2018</xref>; <xref ref-type="bibr" rid="c19">Ju et al., 2021</xref>; <xref ref-type="bibr" rid="c22">Y. Li et al., 2019</xref>; <xref ref-type="bibr" rid="c35">Si &amp; Yan, 2021</xref>; <xref ref-type="bibr" rid="c45">Wang et al., 2017</xref>), deep learning has also been applied to predict inter-protein contacts(<xref ref-type="bibr" rid="c11">Guo et al., 2022</xref>; <xref ref-type="bibr" rid="c33">Roy et al., 2022</xref>; <xref ref-type="bibr" rid="c48">Xie &amp; Xu, 2022</xref>; <xref ref-type="bibr" rid="c49">Yan &amp; Huang, 2021</xref>; <xref ref-type="bibr" rid="c50">Zeng et al., 2018</xref>). ComplexContact(<xref ref-type="bibr" rid="c50">Zeng et al., 2018</xref>), to the best of our knowledge, the first deep learning method for inter-protein contact prediction, has significantly improved the prediction accuracy over coevolutionary analysis methods. However, its performance on eukaryotic PPIs is still quite limited, partly due to the difficulty to accurately infer interologs for eukaryotic PPIs. In a later study, coming from the same group as ComplexContact, Xie et al. developed GLINTER(<xref ref-type="bibr" rid="c48">Xie &amp; Xu, 2022</xref>), another deep learning method for inter-protein contact prediction. Comparing with ComplexContact, GLINTER leverages structures of interacting monomers, from which their rotational invariant graph representations are used as additional input features. GLINTER outperforms ComplexContact in the prediction accuracy, although there is still large room for improvement, especially for heteromeric PPIs. It is worth mentioning that CDPred(<xref ref-type="bibr" rid="c11">Guo et al., 2022</xref>), a recently developed method, further surpasses GINTER in prediction accuracy with 2D attention-based neural networks. Apart from these methods developed to predict inter-protein contacts for both homomeric and heteromeric PPIs, inter-protein contact prediction methods specifically for homomeric PPIs were also developed(<xref ref-type="bibr" rid="c33">Roy et al., 2022</xref>; <xref ref-type="bibr" rid="c47">Wu et al., 2022</xref>; <xref ref-type="bibr" rid="c49">Yan &amp; Huang, 2021</xref>), as predicting the inter-protein contacts for homomeric PPIs is generally much easier due to the symmetric restriction, relatively larger interfaces and the trivialness of interologs identification. For example, Yan et al. developed DeepHomo(<xref ref-type="bibr" rid="c49">Yan &amp; Huang, 2021</xref>), a deep learning method specifically to predict inter-protein contacts of homomeric PPIs, which also significantly outperforms coevolutionary analysis-based methods. However, DeepHomo requires docking maps calculated from structures of interacting monomers, which is computationally expensive and is also sensitive to the quality of monomeric structures. Besides, coming from the same group, Lin et al. further developed DeepHomo2(<xref ref-type="bibr" rid="c23">P. Lin et al., 2023</xref>) for inter-protein contact prediction for homomeric PPIs by including the MSA (multiple sequence alignment) embeddings and attentions from an MSA-based protein language model (MSA transformer)(<xref ref-type="bibr" rid="c31">R. M. Rao et al., 2021b</xref>) in their prediction model, which further improved the prediction performance. In almost the same time with DeepHomo2, we proved that embeddings from protein language models(<xref ref-type="bibr" rid="c29">R. Rao et al., 2021</xref>; <xref ref-type="bibr" rid="c32">Rives et al., 2021</xref>) (PLMs) are very effective features to predict inter-protein contacts for both homomeric and heteromeric PPIs, and we further show the sequence embeddings, MSA embeddings and the inter-protein coevolutionary information complement each other in the prediction, with which we developed DRN-1D2D_Inter(<xref ref-type="bibr" rid="c37">Si &amp; Yan, 2023</xref>). Extensive benchmark results show that DRN-1D2D_Inter significantly outperforms DeepHomo and GLINTER in inter-protein contact prediction, although DRN-1D2D_Inter makes the prediction purely from sequences.</p>
<p>In this study, we developed a structure-informed method to predict inter-protein contacts. Given structures of two interacting proteins, we first build rotationally and translationally (SE(3)) invariant geometric graphs from the two monomeric structures, with which encoded both the inter-residue distance and orientation information in the geometric graphs. We further embedded the single sequence embeddings, multiple sequence alignment (MSA) embeddings and structure embeddings from PLMs in the graph nodes of the corresponding residues to build the PLM embedded geometric graphs, which are then transformed by graph encoders formed by geometric vector perceptrons to generate graph embeddings for interacting monomers. The graph embeddings are further combined with inter-protein pairwise features and transformed by residual networks formed by dimensional hybrid residual blocks to predict inter-protein contacts. The developed method referred to as PLMGraph-Inter was extensive benchmarked on multiple tests with application of either experimental or predicted structures of interacting monomers as the input. The result shows that in both cases, PLMGraph-Inter outperforms other top prediction methods including DeepHomo, GLINTER, CDPred, DeepHomo2 and DRN-1D2D_Inter by large margins. In addition, we also compared the prediction results of PLMGraph-Inter with the protein complex structures generated by AlphaFold-Multimer(<xref ref-type="bibr" rid="c8">Evans et al., 2022</xref>). The result shows that for many targets which AlphaFold-Multimer made poor predictions, PLMGraph-Inter yielded better results. Finally, we show leveraging the contacts predicted by PLMGraph-Inter as constraints for protein-protein docking can dramatically improve its performance for protein complex structure prediction.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Overview of PLMGraph-Inter</title>
<p>The method of PLMGraph-Inter is summarized in <xref rid="fig1" ref-type="fig">Figure 1</xref>. PLMGraph-Inter consists of three modules: the graph representation module, the graph encoder module and the residual network module. Each interacting monomer is first transformed into a PLM-embedded graph by the graph representation module, then the graph is passed through the graph encoder module to obtain a 1D representation of each protein. The two protein representations are transformed into 2D pairwise features through outer concatenation and further concatenated with other 2D pairwise features, which are then transformed by the residual network module to obtain the predicted inter-protein contact map.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Overview of PLMGraph-Inter. (a) The network architecture of PLMGraph-Inter. (b) The graph representation module. (c) The graph encoder module, s denotes scalar features, v denotes vector features. (d) The dimensional hybrid residual block (âINâ denotes Instance Normalization).</p></caption>
<graphic xlink:href="523121v3_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<title>The graph representation module</title>
<p>The first step of the graph representation module is to represent the protein 3D structure as a geometric graph, where each residue is represented as a node, and an edge is defined if the <italic>C<sub>Î±</sub></italic> atom distance between two residues is less than 18Ã. For each node and edge, we use scalars and vectors extracted from the 3D structures as their geometric features. To make the geometric graph SE(3) invariant, we use a set of local coordinate systems to extract the geometric vectors. The SE(3) invariance of representation of each interacting monomer is important, as in principal, the inter-protein contact prediction result should not depend on the initial positions and orientations of protein structures. A detailed description can be found in the Methods section. The second step is to integrate the single sequence embedding from ESM-1b(<xref ref-type="bibr" rid="c32">Rives et al., 2021</xref>), the MSA embedding from ESM-MSA-1b(<xref ref-type="bibr" rid="c29">R. Rao et al., 2021</xref>), the Position-Specific Scoring Matrix (PSSM) calculated from the MSA and structure embedding from ESM-IF(<xref ref-type="bibr" rid="c15">Hsu et al., 2022</xref>) for each interacting monomer using its corresponding geometric graph. Where ESM-1b and ESM-MSA-1b are pretrained PLMs learned from large datasets of sequences and MSAs respectively without label supervision, and ESM-IF is a supervised PLM trained from 12 million protein structures predicted by AlphaFold2(<xref ref-type="bibr" rid="c20">Jumper et al., 2021</xref>) for fixed backbone design. The embeddings from these models contain high dimensional representations of each residue in the protein, which are concatenated and further combined with the PSSM to form additional features of each node in the geometric graph. Since the sequence embeddings, the MSA embeddings, the PSSM and the structure embeddings are all SE(3) invariant, the PLM-embedded geometric graph of each protein is also SE(3) invariant.</p>
</sec>
<sec id="s2c">
<title>The graph encoder module</title>
<p>The graph encoder module is formed by geometric vector perceptron (GVP) and GVP convolutional layer (GVPConv)(<xref ref-type="bibr" rid="c16">Jing, Eismann, Soni, et al., 2021</xref>; <xref ref-type="bibr" rid="c17">Jing, Eismann, Suriana, et al., 2021</xref>). Where GVP is a graph neural network module consisting of a scalar track and a vector track, which can perform SE(3) invariant transformations on scalar features and SE(3) equivariant on vector features of nodes and edges; GVPConv follows the message passing paradigm of graph neural network and mainly consists of GVP, which updates the embedding of each node by passing information from its neighboring nodes and edges. A detailed description of GVP and GVPConv can be found in the Methods section and also in the work of GVP(<xref ref-type="bibr" rid="c16">Jing, Eismann, Soni, et al., 2021</xref>; <xref ref-type="bibr" rid="c17">Jing, Eismann, Suriana, et al., 2021</xref>). For each protein graph, we first use a GVP module to reduce the dimension of the scalar features of each node from 2586 to 256, which is then transformed successively by three GVPConv layers. Finally, we stitch the scalar features and the vector features of each node to form the 1D representation of the protein. Since the input protein graph is SE(3) invariant and the GVP and GVPConv transformations are SE(3) equivariant, the 1D representation of each interacting monomer is also SE(3) invariant.</p>
</sec>
<sec id="s2d">
<title>The residual network module</title>
<p>The residual network module block is mainly formed by nine dimensional hybrid residual blocks. Our previous study illustrated the effective receptive field can be enlarged with the application of the dimensional hybrid residual block, thus helps improve the model performance(<xref ref-type="bibr" rid="c35">Si &amp; Yan, 2021</xref>). Specifically, we first use a convolution layer with kernel size of 1*1 to reduce the number of channels of the input 2D maps from 1044 to 96, which are then transformed successively by nine dimensional hybrid residual blocks and another convolution layer with kernel size of 1*1 for the channel reduction (from 96 to 1). Finally, we use the sigmoid function to transform the feature map to obtain the predicted inter-protein contact map.</p>
</sec>
<sec id="s2e">
<title>Evaluation of PLMGraph-Inter on HomoPDB and HeteroPDB test sets</title>
<p>We first evaluated PLMGraph-Inter on two self-built test sets which are non-redundant to the training dataset of PLMGraph-Inter: HomoPDB and HeteroPDB. Where HomoPDB is the test set for homomeric PPIs containing 400 homodimers and HeteroPDB is the test set for heteromeric PPIs containing 200 heterodimers. For comparison, we also evaluated DeepHomo, GLINTER, DeepHomo2, CDPred and DRN-1D2D_Inter on the same datasets. Since DeepHomo and DeepHomo2 was developed to predict inter-protein contacts only for homomeric PPIs, its evaluation was only performed on HomoPDB. In all the evaluations, the structural related features were drawn from experimental bound structures of interacting monomers separated from complex structures of PPIs (i.e. native structures) after randomizing their initial positions and orientations (DRN-1D2D_Inter does not use structural information). It should be noted that since HomoPDB and HeteroPDB are not de-redundant with the training sets of DeepHomo, DeepHomo2, CDPred and GLINTER, the performances of the four methods may be overestimated.</p>
<p><xref rid="tbl1" ref-type="table">Table 1</xref> shows the mean precision of each method on the HomoPDB and HeteroPDB when top (5, 10, 50, L/10, L/5) predicted inter-protein contacts are considered, where L denotes the sequence length of the shorter protein in the PPI and (Note: GLINTER encountered errors in 81 targets in HomoPDB and 15 targets in HeteroPDB at run time and did not produce predictions, thus we removed these targets in the evaluation of the performance of GLINTER. The performances of other methods on HomoPDB and HeteroPDB after the removal of these targets are shown in <xref ref-type="table" rid="tbls1">Table S1</xref>). As can be seen from the table, the mean precision of PLMGraph-Inter far exceeds those of other algorithms in each metric for both datasets. In particular, the mean precision of PLMGraph-Inter is substantially improved in each metric on each dataset compared to our previous method DRN-1D2D_Inter which used most features of PLMGraph-Inter except these drawn from structures of the interacting monomers, illustrating the importance of the inclusion of structural information. Besides, GLINTER, CDPred and DeepHomo2 also use structural information and PLMs, has much lower performance than PLMGraph-Inter, illustrating the efficacy of our deep learning framework.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>The performances of DeepHomo, GLINTER, DRN-1D2D_Inter and PLMGraph-Inter on the HomoPDB and HeteroPDB test sets using native structures.</title></caption>
<graphic xlink:href="523121v3_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>It can also be seen from the result that all the methods tend to have better performances on HomoPDB than those on HeteroPDB. This is reasonable as complex structures of homodimers are generally C2 symmetric, which largely restricts the configurational spaces of PPIs, making the inter-protein contact prediction a much easier task (e.g., the inter-protein contact maps for homodimers are also symmetric). Besides, comparing with heteromeric PPIs, we may more likely to successfully infer the inter-protein coevolutionary information for homomeric PPIs to assist the contact prediction for two reasons: First, it is straightforward to pair sequences in the MSAs for homomeric PPIs, thus the paired MSA for homomeric PPIs may have higher qualities; second, homomeric PPIs may undergo stronger evolutionary constraints, as homomeric PPIs are generally permanent interactions, but many heteromeric PPIs are transient interactions.</p>
<p>In addition to using the mean precision on each test set to evaluate the performance of each method, the performance comparisons between PLMGraph-Inter and other models on the top 50 predicted contacts for each individual target in HomoPDB and HeteroPDB are shown in <xref rid="fig2" ref-type="fig">Figure 2a-b</xref>. Specifically, PLMGraph-Inter achieved the best performance for 60% of the targets in HomoPDB and 58% of the targets in HeteroPDB. We further group targets in each dataset according to their inter-protein contact densities defined as <inline-formula><alternatives><inline-graphic xlink:href="523121v3_inline1.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where <italic>L<sub>A</sub></italic> and <italic>L<sub>B</sub></italic> are the protein lengths. In <xref rid="fig2" ref-type="fig">Figure 2c-d</xref>, we show the mean precisions of the top 50 predicted contacts on targets within different ranges of contact densities. As can be seen from the figure that the predicted contacts tend to have lower precision when contact densities of the targets are lower, however, PLMGraph-Inter consistently achieved the best performance.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p>The performances of PLMGraph-Inter and other methods on the HomoPDB and HeteroPDB test sets. (a)â¼(b): The head-to-head comparison of the precisions (%) of the top 50 contacts predicted by PLMGraph-Inter and other methods for each target in (a) HomoPDB and (b) HeteroPDB. (c)â¼(d): The mean precisions of the top 50 contacts predicted by each method on PPIs within different intervals of contact densities in (c) HomoPDB and (d) HeteroPDB.</p></caption>
<graphic xlink:href="523121v3_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2f">
<title>Impact of the monomeric structure quality on contact prediction</title>
<p>In real practice, native structures of interacting monomers generally do not exist, and only unbound or predicted structures can be employed in the prediction, which often contain certain levels of structural variances comparing with native structures. An effective inter-protein contact prediction method should be robust to structural variances of interacting monomers. Recently, AlphaFold2 achieved unprecedented progress in protein structure prediction, which can predict protein structures with the accuracy close to experimental results for many proteins. Therefore, it is necessary to evaluate the performance of PLMGraph-Inter using the predicted structures generated by AlphaFold2.</p>
<p>We used AlphaFold2 to predict structures of interacting monomers for all PPIs in HomoPDB and HeteroPDB. Considering that interacting monomers in HomoPDB and HeteroPDB are not de-redundant with the training set of AlphaFold2, using default settings of AlphaFold2 may overestimate its performance. To mimic the performance of AlphaFold2 in real practice and produce predicted monomeric structures with more diverse qualities, we only used the MSA searched from Uniref100(<xref ref-type="bibr" rid="c41">Suzek et al., 2015</xref>) protein sequence database as the input to the model_5_ptm of AlphaFold2, and did not use other sequence databases and prediction models included in AlphaFold2. Besides, we set to not use templates in the structure prediction. The predicted structures yielded a mean TM-score 0.88, which is close to the performance of AlphaFold2 for CASP14 targets (mean TM-score 0.85)(<xref ref-type="bibr" rid="c23">Z. Lin et al., 2023</xref>). The predicted structures of interacting monomers were then severed as the input to PLMGraph-Inter and other prediction methods (except for DRN-1D2D_Innter, which does not use structural information) for inter-protein contact prediction.</p>
<p>As shown in <xref rid="fig3" ref-type="fig">Figures 3a-b</xref>, when the predicted structures were used by PLMGraph-Inter for inter-protein contact prediction, mean precisions of the predicted inter-protein contacts in each metric on both HomoPDB and HeteroPDB test sets decrease by about 5%, indicating qualities of the input structures do have certain impact on the prediction performance. However, the performance of PLMGraph-Inter using the predicted monomer structures still dramatically outperforms DRN-1D2D_Inter and other prediction methods (see <xref ref-type="table" rid="tbls2">Table S2</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><p>The performances of PLMGraph-Inter when using native and AlphaFold2 predicted structures as the input. (a)â¼(b): The performance comparison of PLMGraph-Inter when using native structures and AlphaFold2 predicted structures as the input on (a) HomoPDB and (b) HeteroPDB. (c) The performance gaps (measured as the difference of the mean precision of the top 50 predicted contacts) of PLMGraph-Inter with the application of AlphaFold2 predicted structures and native structures as the input when the PPIs are within different intervals of DTM-score. (d) The comparison of the precision of top 50 contacts predicted by PLMGraph-Inter for each target when using native structures and AlphaFold2 predicted structures as the input.</p></caption>
<graphic xlink:href="523121v3_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We further explored the impact of the monomeric structure quality on the inter-protein contact prediction performance of PLMGraph-Inter. Specifically, for a given PPI, the TM-score(<xref ref-type="bibr" rid="c51">Zhang &amp; Skolnick, 2004</xref>) was used to evaluate the quality of the predicted structure for each interacting monomer, and the TM-score of the predicted structure with lower quality was used to evaluate the overall monomeric structure prediction quality for the PPI, denoted as âDTM-scoreâ. In <xref rid="fig3" ref-type="fig">Figure 3c</xref>, we show the performance gaps (using the mean precisions of the top 50 predicted contacts as the metric) between applying the predicted structures and applying the experimental structures in the inter-protein contact prediction, in which we grouped targets according to DTM-scores of their monomeric structure prediction, and in <xref rid="fig3" ref-type="fig">Figure 3d</xref>, we show the performance comparison for each specific target. From <xref rid="fig3" ref-type="fig">Figure 3c-d</xref>, we can clearly see that when the DTM-score is lower, the prediction using the prediction structure tends to have lower accuracy. However, when the DTM-score is greater than or equal to 0.8, there is almost no difference between the applying the predicted structures and applying the experimental structure, which shows the robustness of PLMGraph-Inter to the structure quality.</p>
</sec>
<sec id="s2g">
<title>Ablation study</title>
<p>To explore the contribution of each input component to the performance of PLMGraph-Inter, we conducted ablation study on PLMGraph-Inter. The graph representation from the structure of each interacting proteins is the base feature of PLMGraph-Inter, so we first trained the baseline model using only the geometric graphs as the input feature, denoted as model a. Our previous study in DRN-1D2D_Inter has shown that the single sequence embeddings, the MSA 1D features (including the MSA embeddings and PSSMs) and the 2D pairwise features from the paired MSA play important roles in the model performance. To further explore the importance of these features when integrated with the geometric graphs, we trained model b-d separately (model b: geometric graphs + sequence embeddings, model c: geometric graphs + sequence embeddings + MSA 1D features, model d: geometric graphs + sequence embeddings + MSA 1D features + 2D features). Finally, we included the structure embeddings as additional features to train the model e (model e uses all the input features of PLMGraph-Inter). All the five models were trained using the same protocol as PLMGraph-Inter on the same training and validation partition without cross validation. We further evaluated performances of five models together with PLMGraph-Inter (i.e., model f: model e + cross validation) on HomoPDB and HeteroPDB using native structures of interacting monomers respective.</p>
<p>In <xref rid="fig4" ref-type="fig">Figure 4a</xref>, we show the mean precisions of the top 50 predicted contacts by model a-f on HomoPDB and HeteroPDB respectively. It can be seen from <xref rid="fig4" ref-type="fig">Figure 4a</xref> that including the sequence embeddings in the geometric graphs has a very good boost to the model performance (model b versus model a), while the additional introduction of MSA 1D features and 2D pairwise features can further improve the model performance (model d versus model c versus model b). DRN-1D2D_Inter also uses the same set of sequence embeddings, MSA 1D features and 2D pairwise features as the input, and our model d shows a significant performance improvement over DRN-1D2D_Inter (single model) (the model trained on the same training and validation partition without cross validation) on both HomoPDB and HeteroPDB (the mean precision improvement: HomoPDB:14%, HeteroPDB:5.6%), indicating that the introduced graph representation is important for the model performance. The head-to-head comparison of model d and DRN-1D2D_Inter (single model) on each specific target in <xref rid="fig4" ref-type="fig">Figure 4b</xref> further demonstrates the value of the graph representation. Besides, the additional introduced structure embeddings from ESM-IF can further improve the mean precisions of the predicted contacts by 3â¼4% on both HomoPDB and HeteroPDB (model e versus model d) and the application of the cross validation can also improve the precisions by 1.0 % on HomoPDB and 2.7% on HeteroPDB (model f versus model d) (see <xref ref-type="table" rid="tbls3">Table S3</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><p>The ablation study of PLMGraph-Inter on the HomoPDB and HeteroPDB test sets. (a) The mean precisions of the top 50 contacts predicted by different ablation models on the HomoPDB and HeteroPDB test sets. (b) The head-to-head comparisons of mean precisions of the top 50 contacts predicted by model d and DRN-1D2D_Inter (single model) for each target in HomoPDB and HeteroPDB. (c) The head-to-head comparison of mean precisions of the top 50 contacts predicted by the model using our geometric graphs and the GVP geometric graphs.</p></caption>
<graphic xlink:href="523121v3_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To demonstrate the efficacy of our proposed graph representation of protein structures, we also trained a model using the structural representation proposed in the work of GVP(<xref ref-type="bibr" rid="c16">Jing, Eismann, Soni, et al., 2021</xref>; <xref ref-type="bibr" rid="c17">Jing, Eismann, Suriana, et al., 2021</xref>) (denoted as âGVP Graphâ), as a control. Our structural representation differs significantly from GVP Graph. For example, we extracted inter-residue distances and orientations between five atoms (C,O,CÎ±,N and a virtual CÎ²) from the structure as the geometric scalar and vector features, in which the vector features are calculated in a local coordinate system. However, GVP Graph only uses the distances and orientations between CÎ± atoms as the geometric scalar and vector features and the vector features are calculated in a global coordinate system. In addition, after the geometric graph is transformed by the graph encoder module, GVP Graph only uses the scalar features of each node as the node representation, while we concatenate the scalar and vector features of the node as the node representation. In <xref rid="fig4" ref-type="fig">Figure 4c</xref>, we show the performance comparison between this model and our base model (model a). From <xref rid="fig4" ref-type="fig">Figure 4c</xref>, we can clear see that our base model significantly outperforms the GVP Graph-based model on both HomoPDB and HeteroPDB, illustrating the high efficacy of our proposed graph representation.</p>
</sec>
<sec id="s2h">
<title>Evaluation of PLMGraph-Inter on DHTest and DB5.5 test sets</title>
<p>We further evaluated PLMGraph-Inter on DHTest and DB5.5. The DHTest test set was formed by removing PPIs redundant to our training set from the original test set of DeepHomo, which contains 130 homomeric PPIs. The DB5.5 test set was formed by removing PPIs redundant to our training dataset from the heterodimers in Protein-protein Docking Benchmark 5.5, which contains 59 heteromeric PPIs. Still, both the native structures and the predicted structures (generated using the same protocol as in HomoPDB and HeteroPDB) of the interacting monomers were used respectively in the inter-protein contact prediction. It should be noted that since DHTest and DB5.5 are not de-redundant with the training sets of CDPred and GLINTER, particularly, all PPIs in the DHTest test set are included in the training set of CDPred, thus the performances of the two methods may be overestimated.</p>
<p>As shown in <xref rid="fig5" ref-type="fig">Figure 5a-b</xref>, when using the native structures in the prediction, the mean precisions of the top 50 contacts predicted by PLMGraph-Inter are 71.9% on DHTest and 29.5% on DB5.5 (also see <xref ref-type="table" rid="tbls4">Table S4</xref>), which are dramatically higher than DeepHomo, GLINTER, DeepHomo2 and DRN-1D2D_Inter (Note: GLINTER encountered errors for 47 targets in DHTest and 3 targets in DB5.5 at run time and did not produce predictions, thus we removed these targets in the evaluation of the performance of GLINTER. The performances of other methods on DHTest and DB5.5 after the removal of these targets are shown in <xref ref-type="table" rid="tbls5">Table S5</xref>). We can also see that although PLMGraph-Inter achieved significantly better performance than CDPred on DB5.5, its performance on DHTest is quite close to CDPred. However, it should be noted that the performance of CDPred on DHTest might be grossly overestimated since PPIs in DHTest are fully included in the training set of CDPred. The distributions of the precisions of top 50 predicted contacts by different methods on DHTest and DB5.5 are shown in <xref rid="fig5" ref-type="fig">Figure 5d-e</xref>, from which we can clearly see that PLMGraph-Inter can make high-quality predictions for more targets on both DHTest and DB5.5.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><p>The performances of PLMGraph-Inter and other methods on the DHTest and DB5.5 test sets. (a)â¼(b): The mean precisions of top 50 contacts predicted by PLMGraph-Inter, GLINTER, DeepHomo2, CDPred and DeepHomo on (a) DHTest and (b) DB5.5 when using native structures and AlphaFold2 predicted structures as the input, where the green lines indicate the performance of DRN-1D2D_Inter. (c) The performance gaps (measured as the difference of the mean precision of the top 50 predicted contacts) of PLMGraph-Inter with the application of AlphaFold2 predicted structures and native structures as the input when the PPIs are within different intervals of DTM-score. (d)â¼(e): The distributions of precisions of the top 50 contacts predicted by PLMGraph-Inter and other methods for PPIs in (d) DHTest and (e) DB5.5. (f) The mean precisions of the top 50 contacts predicted by PLMGraph-Inter on PPIs within different intervals of contact densities in DB5.5.</p></caption>
<graphic xlink:href="523121v3_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>When using the predicted structures in the prediction, the mean precisions of top 50 contacts predicted by PLMGraph-Inter show reasonable decrease to 61.1% on DHTest and 23.8% on DB5.5 respectively (see <xref rid="fig5" ref-type="fig">Figure 5a-b</xref> and <xref ref-type="table" rid="tbls6">Table S6</xref>). We also analyzed the impact of the monomeric structure quality on the inter-protein contact prediction performance of PLMGraph-Inter. As shown in <xref rid="fig5" ref-type="fig">Figure 5c</xref>, when the DTM-score is greater than or equal to 0.8, there is almost no difference between applying the predicted structures and the native structures, which is consistent with our analysis in HomoPDB and HeteroPDB.</p>
<p>We noticed that the performance of PLMGraph-Inter on the DB5.5 is significantly lower than that on HeteroPDB, and so are the performances of other methods. That the targets in DB5.5 have relatively lower mean contact densities (1.01% versus 1.29%) may partly explain this phenomenon. In <xref rid="fig5" ref-type="fig">Figure 5f</xref>, we show the variations of the precisions of predicted contacts with the variation of contact density. As can be seen from <xref rid="fig5" ref-type="fig">Figure 5f</xref>, as the contact density increases, precisions of predicted contacts tend to increase regardless of whether the native structures or predicted structures are used in the prediction. 37.29% targets in DB5.5 are with inter-protein contact densities lower than 0.5%, for which precisions of predicted contacts are generally very low, making the overall inter-protein contact prediction performance on DB5.5 relatively low.</p>
</sec>
<sec id="s2i">
<title>Comparison of PLMGraph-Inter with AlphaFold-Multimer</title>
<p>After the development of AlphaFold2, DeepMind also released AlphaFold-Multimer, as an extension of AlphaFold2 for protein complex structure prediction. The inter-protein contacts can also be extracted from the complex structures generated by AlphaFold-Multimer. It is worth to making a comparison between the performances of AlphaFold-Multimer and PLMGraph-Inter on inter-protein contact prediction. Therefore, we also employed AlphaFold-Multimer to generate complex structures for all the PPIs in the four datasets which we used to evaluate PLMGraph-Inter. We then selected the 50 inter-protein residue pairs with the shortest heavy atom distances in each generated protein complex structures as the predicted inter-protein contacts. It should be noted that AlphaFold-Multimer used all protein complex structures in Protein Data Bank deposited before 2018-04-30 for the model development, thus these PPIs may have a large overlap with the training set of AlphaFold-Multimer. Therefore, there is no doubt that the performance of AlphaFold-Multimer would be overestimated here. Besides, comparing with AlphaFold-Multimer, PLMGraph-inter is a much lighter model which was trained on a single GPU for less than two days, but the development of AlphaFold-Multimer utilized huge computational resources.</p>
<p>In <xref rid="fig6" ref-type="fig">Figure 6a</xref>, we show the relationship between the quality of the generated protein complex structure (evaluated with DockQ) and the precision of the top 50 inter-protein contacts extracted from the protein complex structure for each PPI in the homomeric PPI (DHTest + HomoPDB) and heteromeric PPI (DB5.5 + HeteroPDB) datasets. As it can be seen from the figure that the precision of the predicted contacts is highly correlated with the quality of the generated structure. Especially when the precision of the contacts is higher than 50%, most of the generated complex structures have at least acceptable qualities (DockQâ¥0.23), in contrast, almost all the generated complex structures are incorrect (DockQ&lt;0.23) when the precision of the contacts is below 50%. Therefore, 50% can be considered as a critical precision threshold for inter-protein contact prediction (the top 50 contacts).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>The comparison of PLMGraph-Inter with AlphaFold-Multimer.</title>
<p>(a) The head-to-head comparison between the qualities of the protein complex structures generated by AlphaFold-Multimer (evaluated with DockQ) and the precision of the top 50 inter-protein contacts extracted from the generated protein complex structures. The red horizontal lines represent the threshold (DockQ=0.23) to determine whether the complex structure prediction is successful or not. (b) The head-to-head comparisons of precisions of the top 50 inter-protein contacts predicted by PLMGraph-Inter and AlphaFold-Multimer for each target in the homomeric PPI and heteromeric PPI datasets. (c)â¼(d): The mean precisions of top 50 inter-protein contacts predicted by PLMGraph-Inter and AlphaFold-Multimer on the PPI subsets from (c) â DHTest+HomoPDBâ and (d) âDB5.5+HeteroPDBâ in which the precision of the top 50 inter-protein contacts predicted by AlphaFold-Multimer is lower than 50% or the DockQ of the complex structure predicted by AlphaFold-Multimer is lower than 0.23.</p></caption>
<graphic xlink:href="523121v3_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In <xref rid="fig6" ref-type="fig">Figure 6b</xref>, we show the comparison of the precisions of top 50 contacts predicted by AlphaFold-Multimer and PLMGraph-Inter for each target when using the native monomeric structures as the input for PLMGraph-Inter respectively (The comparison when using the AlphaFold2 predict structures is shown in <xref ref-type="fig" rid="figs2">Figure S2</xref>). It can be seen from the figure that although for most of the targets, AlphaFold-Multimer yielded better results, but for a significant number of the targets that AlphaFold-Multimer made poor predictions (precision&lt;50%), the results of PLMGraph-Inter can have certain improvement over the AlphaFold-Multimer predictions.</p>
<p>We further explored the performance of PLMGraph-Inter on the PPIs which AlphaFold-Multimer failed to make correct predictions. Specifically, we denoted a PPI for which the precision of top 50 inter-protein contacts predicted by AlphaFold-Multimer is lower than 50% or the DockQ of protein complex structure predicted by AlphaFold-Multimer is less than 0.23 as âAFM-precision-Failedâ and âAFM-DockQ-Failedâ, then the mean precisions of top 50 contacts predicted by PLMGraph-Inter and AlphaFold-Multimer on the âAFM-precision-Failedâ and âAFM-DockQ-Failedâ sub-test sets from âDHTest+HomoPDBâ and âDB5.5+HeteroPDBâ are shown in <xref rid="fig6" ref-type="fig">Figure 6c-d</xref>. From <xref rid="fig6" ref-type="fig">Figure 6c-d</xref> we can see that the mean precisions of contacts predicted by PLMGraph-Inter are much higher than the mean precisions of contacts predicted by AlphaFold-Multimer, demonstrating that PLMGraph-Inter can complement AlphaFold-Multimer well.</p>
</sec>
<sec id="s2j">
<title>PLMGraph-Inter can significantly improve protein-protein docking performance</title>
<p>Prior to AlphaFold-Multimer, protein-protein docking is generally used for protein complex structure prediction. HADDOCK(<xref ref-type="bibr" rid="c13">Honorato et al., 2021</xref>; <xref ref-type="bibr" rid="c43">van Zundert et al., 2016</xref>) is a widely used information-driven protein-protein docking approach to model complex structures of PPIs, which allows us to encode predicted inter-protein contacts as constraints to drive the docking. In this study, we used HADDOCK (version 2.4) to explore the contribution of PLMGraph-Inter to protein complex structure prediction.</p>
<p>We prepared the test set of homomeric PPIs by merging HomoPDB and DHTest and the test set of heteromeric PPIs by merging HeteroPDB and DB5.5, where the monomeric structures generated previously by AlphaFold2 were used as the input to HADDOCK for protein-protein docking, in which the top 50 contacts predicted by PLMGraph-Inter with the application of the predicted monomeric structures were used as the constraints. Since HADDOCK generally cannot model large conformational changes in protein-protein docking, we filtered PPIs in which either of the AlphaFold2 generated interacting monomeric structure has a TM-score lower than 0.8. Finally, the homomeric PPI test set contains 462 targets, denoted as Homodimer, and the heteromeric PPI test set contains 174 targets, denoted as Heterodimer.</p>
<p>For each PPI, we used the top 50 contacts predicted by PLMGraph-Inter and other methods as ambiguous distance restraints between the alpha carbons (CAs) of residues (distance=8Ã, lower bound correction=8Ã, upper-bound correction=4Ã) to drive the protein-protein docking. All other parameters of HADDOCK were set as the default parameters. In each protein-protein docking, HADDOCK output 200 predicted complex structures ranked by the HADDOCK scores. As a control, we also performed protein-protein docking with HADDOCK in ab initio docking mode (center of mass restraints and random ambiguous interaction restraints definition). Besides, for homomeric PPIs, we additionally added the C2 symmetry constraint in both cases.</p>
<p>As shown in <xref rid="fig7" ref-type="fig">Figure 7a-c</xref>, the success rate of docking on Homodimer and Heterodimer test sets can be significantly improved when using the PLMGraph-Inter predicted inter-protein contacts as restraints. Where on the Homodimer test set, the success rate (DockQ â¥ 0.23) of the top 1 (top 10) prediction of HADDOCK in ab initio docking mode are 15.37% (39.18%), and when predicted by HADDOCK with PLMGraph-Inter predicted contacts, the success rate of the top 1 (top 10) prediction 57.58% (61.04%). On the Heterodimer test set, the success rate of top 1 (top 10) predictions of HADDOCK in ab initio docking mode is only 1.72% (6.32%), and when predicted by HADDOCK with PLMGraph-Inter predicted contacts, the success rate of top 1 (top 10) prediction is 29.89% (37.93%). From <xref rid="fig7" ref-type="fig">Figure 7a</xref>-<xref rid="fig7" ref-type="fig">7b</xref> we can also see that integrating PLMGraph-Inter predicted contacts with HADDOCK not only allows for a higher success rate, but also more high-quality models in the docking results.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><p>Protein-protein docking performances on the Homodimer and Heterodimer test sets. (a)â¼(b) The protein-protein docking performance comparison between HADDOCK with and without (ab-initio) using PLMGraph-Inter predicted contacts as restraints on (a) Homodimer and (b) Heterodimer. The left side of each column shows the performance when the top 1 predicted model for each PPI is considered, and the right side shows the performance when the top 10 predicted models for each PPI are considered. (c) The head-to-head comparison of qualities of the top 1 model predicted by HADDOCK with and without using PLMGraph-Inter predicted contacts as restraints for each target PPI. The red lines represent the threshold (DockQ=0.23) to determine whether the complex structure prediction is successful or not. (d) The success rates (the top 1 model) for protein complex structure prediction when only including targets for which precisions of the predicted contacts are higher than certain thresholds.</p></caption>
<graphic xlink:href="523121v3_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We further explored the relationship between the precision of top 50 contacts predicted by PLMGraph-Inter and the success rate of the top prediction of HADDOCK with PLMGraph-Inter predicted contacts. It can be clearly seen from <xref rid="fig7" ref-type="fig">Figure 7d</xref> that the success rate of protein-protein docking increases with the precision of contact prediction. Especially, when the precision of the predicted contacts reaches 50%, the docking success rate of both homologous and heterologous complexes can reach 80%, which is consistent with our finding in AlphaFold-Multimer. Therefore, we think this threshold can be used as a critical criterion for inter-protein contact prediction. It is important to emphasize that for some targets, although precisions of predicted contacts are very high, HADDOCK still failed to produce acceptable models. We manually checked these targets and found many of these targets have at least one chain totally entangled by another chain (e.g., PDB 3DFU in <xref ref-type="fig" rid="figs1">Figure S1</xref>). We doubt large structural rearrangements may exist in forming the complex structures, which is difficult to model by traditional protein-protein docking approach.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this study, we proposed a new method to predict inter-protein contacts, denoted as PLMGraph-Inter. PLMGraph-Inter is based on the SE(3) invariant geometric graphs obtained from structures of interacting proteins which are embedded with multiple PLMs. The predicted inter-protein contacts are obtained by successively transforming the PLM embedded geometric graphs with graph encoders and residual networks. Benchmarking results on four test datasets show that PLMGraph-Inter outperforms five state-of-the-art inter-protein contact prediction methods including GLINTER, DeepHomo, CDPred, DeepHomo2 and DRN-1D2D_Inter by large margins, regardless of whether the native or predicted monomeric structures are used in building the geometric graphs. The ablation study further shows that the integration of the PLMs with the protein geometric graphs can dramatically improve the model performance, illustrating the efficacy of the PLM embedded geometric graphs in protein representations. The protein representation framework proposed in this work can also be used to develop models for other tasks like protein function prediction, PPI prediction, etc. We further show PLMGraph-Inter can complement the result of AlphaFold-Multimer and leveraging the inter-protein contacts predicted by PLMGraph-Inter as constraints in protein-protein docking implemented with HADDOCK can dramatically improve its performance for protein complex structure prediction.</p>
<p>We noticed that although PLMGraph-Inter has achieved remarkable progress in inter-protein contact prediction, there is still room for further improvement, especially for heteromeric PPIs. Using more advanced PLMs, larger training datasets and explicitly integrating physicochemical features of interacting proteins are directions worthy of exploration. Besides, since protein-protein docking approach generally have difficulties in modelling large conformational changes in PPIs, developing new approaches to integrate the predicted inter-protein contacts in the more advanced folding- and-docking framework like AlphaFold-Multimer can also be a future research direction.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Training and test datasets</title>
<p>We used the training set and test sets prepared in our previous work DRN-1D2D_Inter(<xref ref-type="bibr" rid="c37">Si &amp; Yan, 2023</xref>) to train and evaluate PLMGraph-Inter. More details for the dataset generation can be found in the previous work. Specifically, we first prepared a non-redundant PPI dataset containing 4828 homomeric PPIs and 3134 heteromeric PPIs (with sequence identity 40% as the threshold), and after randomly selecting 400 homomeric PPIs (denoted as HomoPDB) and 200 heteromeric PPIs (denoted as HeteroPDB) as independent test sets, the remaining 7362 homomeric and heteromeric PPIs were used for training and validation.</p>
<p>DHTest and DB5.5 were also prepared in the work of DRN-1D2D_Inter by removing PPIs which are redundant (with sequence identity 40% as the threshold) to our training and validation set from the test set of DeepHomo and Docking Benchmark 5.5. DHTest contains 130 homomeric PPIs, and DB5.5 contains 59 heteromeric PPIs. Therefore, all the test sets used in this study are non-redundant (with sequence identity 40% as the threshold) to the dataset for the model development.</p>
</sec>
<sec id="s4b">
<title>Inter-protein contact definition</title>
<p>For a given PPI, two residues from the two interacting proteins are defined to be in contact if the distance of any two heavy atoms belonging the two residues is smaller than 8 A.</p>
</sec>
<sec id="s4c">
<title>Preparing the Input features</title>
<sec id="s4c1">
<title>Geometric graphs from structures of interacting monomers</title>
<p>We first represent the protein as a graph, where each residue is represented as a node, and an edge is defined if the heavy atom distance between two residues is less than 18Ã (In our small-scale tests, increasing the cutoff used for defining edges can slightly increase the performance of the model. However, due to GPU memory limitations, we set the cutoff as 18Ã). For each node and edge, we use scalars and vectors extracted from the 3D structures as their geometric features.</p>
<p>For each residue, we use its C,O,CÎ±,N and a virtual CÎ² atom coordinates to extract information, the virtual CÎ² coordinates are calculated using the following formula(<xref ref-type="bibr" rid="c6">Dauparas et al., 2022</xref>): b = CÎ± - N, c = C -CÎ±, a = cross(b, c), CÎ² = â0.58273431*a + 0.56802827*b - 0.54067466*c + CÎ±.</p>
<p>To achieve a SE(3) invariant graph representation, as shown in <xref ref-type="fig" rid="figs3">Figure S3b</xref>, we define a local coordinate system on each residue(<xref ref-type="bibr" rid="c20">Jumper et al., 2021</xref>; <xref ref-type="bibr" rid="c27">PagÃ¨s et al., 2019</xref>). Specifically, for each residue, the unit vector in the CÎ± - C direction is set as the <inline-formula><alternatives><inline-graphic xlink:href="523121v3_inline2.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> axis, the unit vector in the CÎ±-C-N plane and perpendicular <inline-formula><alternatives><inline-graphic xlink:href="523121v3_inline3.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> to is used as <inline-formula><alternatives><inline-graphic xlink:href="523121v3_inline4.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and the z-direction is obtained through the cross product of <inline-formula><alternatives><inline-graphic xlink:href="523121v3_inline5.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="523121v3_inline6.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>For the ith node, we use the three dihedral angles (â¡, Ï, Ï) of the corresponding residue as the scalar features of the node (<xref ref-type="fig" rid="figs3">Figure S3a</xref>), and the unit vectors between the <italic>C<sub>i</sub>, N<sub>i</sub>,O<sub>i</sub>, CÎ±<sub>i</sub></italic>, and CÎ²<italic><sub>i</sub></italic> atoms of the corresponding residue and the <italic>C<sub>i-</sub></italic><sub>1</sub>, <italic>N<sub>i</sub></italic><sub>-1</sub>, <italic>O<sub>i</sub></italic><sub>-1</sub>, <italic>CÎ±<sub>i</sub></italic><sub>-1</sub>, and CÎ²<italic><sub>i</sub></italic><sub>-1</sub> atoms of the forward residue and the <italic>C<sub>i</sub></italic><sub>+1</sub>, <italic>N<sub>i</sub></italic><sub>+1</sub>, <italic>O<sub>i</sub></italic><sub>+1</sub>, <italic>CÎ±<sub>i</sub></italic><sub>+1</sub>, and CÎ²<sub><italic>i</italic>+1</sub> atoms of backward residue as the vector features of the node. In total, for each node, the dimension of the scalar features is 6 (each dihedral angle is encoded with its sine and cosine) and the dimension of the vector features is 50*3.</p>
<p>For the edge between ith node and jth node, we use the distances and directions between the atoms of the two residues as the scalar features and vector features(See <xref ref-type="fig" rid="figs3">Figure S3b</xref>). The distances between the <italic>C<sub>i</sub></italic>, <italic>N<sub>i</sub></italic>,<italic>O<sub>i</sub></italic>, <italic>CÎ±<sub>i</sub></italic>, and CÎ²<sub><italic>i</italic></sub> atoms of ith residue and the <italic>C<sub>j</sub>, N<sub>j</sub>,O<sub>j</sub>, CÎ±<sub>j</sub></italic>, and CÎ²<italic><sub>j</sub></italic> atoms of the jth residue are used as scalar features after encoded with the 16 Gaussian radial basis functions(<xref ref-type="bibr" rid="c16">Jing, Eismann, Suriana, et al., 2021</xref>). The position difference between i and j (j-i) is also used as a scalar feature after sinusoidal encoding(<xref ref-type="bibr" rid="c44">Vaswani et al., 2017</xref>). The unit vectors between the <italic>C<sub>i</sub>, N<sub>i</sub>,O<sub>i</sub>, CÎ±<sub>i</sub></italic>, and CÎ²<italic><sub>i</sub></italic> atoms of ith residue and the <italic>C<sub>j</sub>, N<sub>j</sub>,O<sub>j</sub>, CÎ±<sub>j</sub></italic>, and CÎ²<sub><italic>j</italic></sub> atoms of the jth residue are used as vector features. In total, for each edge, the dimension of the scalar features is 432 and the dimension of the vector features is 25*3.</p>
</sec>
<sec id="s4d">
<title>Embeddings of single sequence, MSA and structure</title>
<p>The single sequence embedding is obtained by feeding the sequence into ESM-1b, and the structure embedding is obtained by feeding the structure into ESM-IF. To obtain the MSA embedding, we first search the Uniref100 protein sequence database for the sequence using JACKHMMER(<xref ref-type="bibr" rid="c28">Potter et al., 2018</xref>) with the parameter (--incT L/2) to obtain the MSA, which is then inputted to hhmake(<xref ref-type="bibr" rid="c39">Steinegger et al., 2019</xref>) to get the HMM file, and to the LoadHMM.py script from RaptorX_Contact(<xref ref-type="bibr" rid="c45">Wang et al., 2017</xref>) to obtain the PSSM. The number of sequences of MSA is limited to 256 by hhfilter(<xref ref-type="bibr" rid="c39">Steinegger et al., 2019</xref>) and then input to ESM-MSA-1b to get the MSA embedding. The dimensions of the sequence embedding, PSSM, MSA embedding and structural embeddings are 1280, 20, 768 and 512 respectively. After adding embeddings to the scalar features of the nodes, the dimension of the scalar features of each node is 2586.</p>
</sec>
<sec id="s5">
<title>2D feature from paired MSA</title>
<p>For homomeric PPIs, the paired MSA is formed by concatenating two copies of the MSA. For heteromeric PPIs, the paired MSA is formed by pairing the MSAs through the phylogeny-based approach described in (<ext-link ext-link-type="uri" xlink:href="https://github.com/ChengfeiYan/PPI_MSA-taxonomy_rank">https://github.com/ChengfeiYan/PPI_MSA-taxonomy_rank</ext-link>)(Si &amp; Yan, 2022). We input the paired MSA into CCMpred(<xref ref-type="bibr" rid="c34">Seemayer et al., 2014</xref>) to get the evolutionary coupling matrix, and into alnstats(<xref ref-type="bibr" rid="c18">Jones et al., 2015</xref>) to get mutual information matrix, APC-corrected mutual information matrix and contact potential matrix. The number of sequences of paired MSA is limited to 256 by hhfilter(<xref ref-type="bibr" rid="c39">Steinegger et al., 2019</xref>) and then input to ESM-MSA-1b to get the attention maps. In total, the channel of 2D features is 148.</p>
</sec>
</sec>
<sec id="s5a">
<title>GVP and GVPConv</title>
<p>GVP is a two-track neural network module consisting of a scalar track and a vector track, which can perform SE(3) invariant transformations on scalar features and SE(3) equivariant transformations on vector features. A detailed description can be found in the work of GVP(<xref ref-type="bibr" rid="c16">Jing, Eismann, Soni, et al., 2021</xref>; <xref ref-type="bibr" rid="c17">Jing, Eismann, Suriana, et al., 2021</xref>).</p>
<p>GVPConv is a message passing based graph neural network, which mainly consists of a message function and a feedforward function. Where the message function contains a sequence of three GVP modules and the feedforward function contains a sequence of two GVP modules. GVPConv is used to transform the node features. Specifically, the input node features are first processed by the message function. We denote the features of node i by <bold><italic>h</italic></bold><italic><sup>i</sup></italic>, the feature of edge (jâi) by <bold><italic>h</italic></bold><italic><sup>j</sup></italic><sup>â<italic>i</italic></sup>, the set of nodes connected to node i by <italic><bold>Îµ</bold><sub>i</sub></italic>, and the three GVP modules of the message function by <italic>gm</italic>, then the node features processed by the message function can be represented as:
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="523121v3_eqn1.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Where <italic>len(<bold>Îµ</bold><sub>i</sub>)</italic> denotes the number of nodes connected to node i. After sequential normalization (<xref rid="eqn2" ref-type="disp-formula">Equation 2</xref>) and feedforward function (<xref rid="eqn3" ref-type="disp-formula">Equation 3</xref>), the features of node i updated by GVPConv Layer are obtained:
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="523121v3_eqn2.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
</p>
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="523121v3_eqn3.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<p>Where <italic>gs</italic> denotes the two GVP modules of the feedforward function, <inline-formula><alternatives><inline-graphic xlink:href="523121v3_inline7.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> denotes the outputs of GVPConv Layer.</p>
</sec>
<sec id="s5b">
<title>Training protocol</title>
<p>Our training set contains 7362 PPIs, and we used seven-fold cross-validation to train PLMGraph-Inter. Specifically, we randomly divided the training set into seven subsets, and each time, we selected six subsets as the training set and the remaining subset as the validation set. Seven models were trained in total, and the final prediction was the average of the predictions from the seven models. Each model was trained using AdamW optimizer with 0.001 as the initial learning rate, in which the singularity enhanced loss function proposed by in our previous study(<xref ref-type="bibr" rid="c35">Si &amp; Yan, 2021</xref>) was used calculate the training and validation loss. During training, if the validation loss did not decrease within 2 epochs, we would decay the learning rate by 0.1. The training stopped after the learning rate decayed twice and the model with the highest top-50 mean precision on the validation dataset was saved as the prediction model.</p>
<p>PLMGraph-Inter was implemented with pytorch (v.1.11) and trained on one NVIDIA TESLA A100 GPU with batch size equaling to 1. Due to memory limitation of GPU, the length of each protein sequence was limited to 400. When a sequence was longer than 400, a fragment with sequence length equaling to 400 was randomly selected in the model training.</p>
</sec>
<sec id="s5c">
<title>Quality assessment of the predicted protein complex structures</title>
<p>We evaluated the models generated by AlphaFold-Multimer and HADDOCK using DockQ(<xref ref-type="bibr" rid="c2">Basu &amp; Wallner, 2016</xref>), a score ranging between 0 and 1. Specifically, a model with DockQ&lt;0.23 means that the prediction is incorrect; 0.23â¤DockQ&lt;0.49 means the model is an acceptable prediction; 0.49â¤DockQ&lt;0.8 corresponds to a medium quality prediction; and 0.8â¤ DockQ corresponds to a high quality prediction.</p>
</sec>
</sec>
</body>
<back>
<sec id="s6">
<title>Data Availability</title>
<p>The PDB accession codes for the all the training and test sets are provided in <ext-link ext-link-type="uri" xlink:href="https://github.com/ChengfeiYan/PLMGraph-Inter/tree/main/data">https://github.com/ChengfeiYan/PLMGraph-Inter/tree/main/data</ext-link>. Other data for supporting the finds of this study are available from the corresponding author upon request.</p>
</sec>
<sec id="s7">
<title>Code Availability</title>
<p>The code for implementing PLMGraph-Inter is provided in <ext-link ext-link-type="uri" xlink:href="https://github.com/ChengfeiYan/PLMGraph-Inter">https://github.com/ChengfeiYan/PLMGraph-Inter</ext-link>.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>The work was supported by the National Natural Science Foundation of China (32101001) and new faculty startup grant (3004012167) of Huazhong University of Science and Technology. The computation is completed in the HPC Platform of Huazhong University of Science and Technology.</p>
</ack>
<sec id="s8">
<title>Author Contributions</title>
<p>Y.S. and C.Y. designed and performed the experiments. Y.S. and C.Y. wrote the manuscript. C.Y. supervised the work.</p>
</sec>
<sec id="s9">
<title>Ethics declarations</title>
<sec id="s9a">
<title>Competing interests</title>
<p>The authors declare no competing interests.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Alberts</surname>, <given-names>B</given-names></string-name>. (<year>1998</year>). <article-title>The Cell as a Collection of Protein Machines: Preparing the Next Generation of Molecular Biologists</article-title>. <source>Cell</source>, <volume>92</volume>(<issue>3</issue>), <fpage>291</fpage>â<lpage>294</lpage>. <pub-id pub-id-type="doi">10.1016/S0092-8674(00)80922-8</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Basu</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Wallner</surname>, <given-names>B</given-names></string-name>. (<year>2016</year>). <article-title>DockQ: A Quality Measure for Protein-Protein Docking Models</article-title>. <source>PLOS ONE</source>, <volume>11</volume>(<issue>8</issue>), <fpage>e0161879</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0161879</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Berman</surname>, <given-names>H. M.</given-names></string-name>, <string-name><surname>Westbrook</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Feng</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Gilliland</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Bhat</surname>, <given-names>T. N.</given-names></string-name>, <string-name><surname>Weissig</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Shindyalov</surname>, <given-names>I. N.</given-names></string-name>, &amp; <string-name><surname>Bourne</surname>, <given-names>P. E</given-names></string-name>. (<year>2000</year>). <article-title>The Protein Data Bank</article-title>. <source>Nucleic Acids Research</source>, <volume>28</volume>(<issue>1</issue>), <fpage>235</fpage>â<lpage>242</lpage>. <pub-id pub-id-type="doi">10.1093/nar/28.1.235</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Bonvin</surname>, <given-names>A. M</given-names></string-name>. (<year>2006</year>). <article-title>Flexible proteinâprotein docking</article-title>. <source>Current Opinion in Structural Biology</source>, <volume>16</volume>(<issue>2</issue>), <fpage>194</fpage>â<lpage>200</lpage>. <pub-id pub-id-type="doi">10.1016/j.sbi.2006.02.002</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Cong</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Anishchenko</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Ovchinnikov</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Baker</surname>, <given-names>D</given-names></string-name>. (<year>2019</year>). <article-title>Protein interaction networks revealed by proteome coevolution</article-title>. <source>Science</source>, <volume>365</volume>(<issue>6449</issue>), <fpage>185</fpage>â<lpage>189</lpage>. <pub-id pub-id-type="doi">10.1126/science.aaw6718</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Dauparas</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Anishchenko</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Bennett</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Bai</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ragotte</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Milles</surname>, <given-names>L. F.</given-names></string-name>, <string-name><surname>Wicky</surname>, <given-names>B. I. M.</given-names></string-name>, <string-name><surname>Courbet</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>de Haas</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Bethel</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Leung</surname>, <given-names>P. J. Y.</given-names></string-name>, <string-name><surname>Huddy</surname>, <given-names>T. F.</given-names></string-name>, <string-name><surname>Pellock</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tischer</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Chan</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Koepnick</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Kang</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sankaran</surname>, <given-names>B.</given-names></string-name>, â¦ <string-name><surname>Baker</surname>, <given-names>D.</given-names></string-name> (<year>2022</year>). <article-title>Robust deep learningâbased protein sequence design using ProteinMPNN</article-title>. <source>Science</source>, <volume>0</volume>(<issue>0</issue>), <fpage>eadd2187</fpage>. <pub-id pub-id-type="doi">10.1126/science.add2187</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Dominguez</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Boelens</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Bonvin</surname>, <given-names>A. M. J. J</given-names></string-name>. (<year>2003</year>). <article-title>HADDOCK: A ProteinâProtein Docking Approach Based on Biochemical or Biophysical Information</article-title>. <source>Journal of the American Chemical Society</source>, <volume>125</volume>(<issue>7</issue>), <fpage>1731</fpage>â<lpage>1737</lpage>. <pub-id pub-id-type="doi">10.1021/ja026939x</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Evans</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>OâNeill</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pritzel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Antropova</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Senior</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Green</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Å½Ã­dek</surname>, <given-names>A</given-names></string-name>., <string-name><surname>Bates</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Blackwell</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Yim</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ronneberger</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Bodenstein</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Zielinski</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bridgland</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Potapenko</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cowie</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Tunyasuvunakool</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Jain</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Clancy</surname>, <given-names>E.</given-names></string-name>, â¦ <string-name><surname>Hassabis</surname>, <given-names>D.</given-names></string-name> (<year>2022</year>). <source>Protein complex prediction with AlphaFold-Multimer</source> (p. <volume>2021</volume>.<fpage>10</fpage>.<lpage>04</lpage>.463034). bioRxiv. <pub-id pub-id-type="doi">10.1101/2021.10.04.463034</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Goodsell</surname>, <given-names>D. S.</given-names></string-name>, &amp; <string-name><surname>Olson</surname>, <given-names>A. J</given-names></string-name>. (<year>2000</year>). <article-title>Structural Symmetry and Protein Function</article-title>. <source>Annual Review of Biophysics and Biomolecular Structure</source>, <volume>29</volume>(<issue>1</issue>), <fpage>105</fpage>â<lpage>153</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.biophys.29.1.105</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Green</surname>, <given-names>A. G.</given-names></string-name>, <string-name><surname>Elhabashy</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Brock</surname>, <given-names>K. P.</given-names></string-name>, <string-name><surname>Maddamsetti</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Kohlbacher</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Marks</surname>, <given-names>D. S</given-names></string-name>. (<year>2021</year>). <article-title>Large-scale discovery of protein interactions at residue resolution using co-evolution calculated from genomic sequences</article-title>. <source>Nature Communications</source>, <volume>12</volume>(<issue>1</issue>), <fpage>Article 1</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-021-21636-z</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Guo</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Skolnick</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Cheng</surname>, <given-names>J</given-names></string-name>. (<year>2022</year>). <article-title>Prediction of inter-chain distance maps of protein complexes with 2D attention-based deep neural networks</article-title>. <source>Nature Communications</source>, <volume>13</volume>(<issue>1</issue>), <fpage>Article 1</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-022-34600-2</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Hanson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Paliwal</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Litfin</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Zhou</surname>, <given-names>Y</given-names></string-name>. (<year>2018</year>). <article-title>Accurate prediction of protein contact maps by coupling residual two-dimensional bidirectional long short-term memory with convolutional neural networks</article-title>. <source>Bioinformatics</source>, <volume>34</volume>(<issue>23</issue>), <fpage>4039</fpage>â<lpage>4045</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bty481</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Honorato</surname>, <given-names>R. V.</given-names></string-name>, <string-name><surname>Koukos</surname>, <given-names>P. I.</given-names></string-name>, <string-name><surname>JimÃ©nez-GarcÃ­a</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Tsaregorodtsev</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Verlato</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Giachetti</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rosato</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Bonvin</surname>, <given-names>A. M. J. J</given-names></string-name>. (<year>2021</year>). <article-title>Structural Biology in the Clouds: The WeNMR-EOSC Ecosystem</article-title>. <source>Frontiers in Molecular Biosciences</source>, <volume>8</volume>. <pub-id pub-id-type="doi">10.3389/fmolb.2021.729513</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Hopf</surname>, <given-names>T. A.</given-names></string-name>, <string-name><surname>SchÃ¤rfe</surname>, <given-names>C. P. I.</given-names></string-name>, <string-name><surname>Rodrigues</surname>, <given-names>J. P. G. L. M.</given-names></string-name>, <string-name><surname>Green</surname>, <given-names>A. G.</given-names></string-name>, <string-name><surname>Kohlbacher</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Sander</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bonvin</surname>, <given-names>A. M. J. J.</given-names></string-name>, &amp; <string-name><surname>Marks</surname>, <given-names>D. S</given-names></string-name>. (<year>2014</year>). <article-title>Sequence co-evolution gives 3D contacts and structures of protein complexes</article-title>. <source>ELife</source>, <volume>3</volume>. <pub-id pub-id-type="doi">10.7554/eLife.03430</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Hsu</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Verkuil</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Hie</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Sercu</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Lerer</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Rives</surname>, <given-names>A</given-names></string-name>. (<year>2022</year>). <source>Learning inverse folding from millions of predicted structures</source> (p. <volume>2022</volume>.<fpage>04</fpage>.<lpage>10</lpage>.487779). bioRxiv. <pub-id pub-id-type="doi">10.1101/2022.04.10.487779</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="web"><string-name><surname>Jing</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Eismann</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Soni</surname>, <given-names>P. N.</given-names></string-name>, &amp; <string-name><surname>Dror</surname>, <given-names>R. O.</given-names></string-name> (<year>2021</year>). <article-title>Equivariant Graph Neural Networks for 3D Macromolecular Structure (arXiv:2106.03843)</article-title>. <source>arXiv</source>. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2106.03843">http://arxiv.org/abs/2106.03843</ext-link></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="other"><string-name><surname>Jing</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Eismann</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Suriana</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Townshend</surname>, <given-names>R. J. L.</given-names></string-name>, &amp; <string-name><surname>Dror</surname>, <given-names>R. O.</given-names></string-name> (<year>2021</year>). <source>LEARNING FROM PROTEIN STRUCTURE WITH GEOMETRIC VECTOR PERCEPTRONS</source>. <fpage>18</fpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Jones</surname>, <given-names>D. T.</given-names></string-name>, <string-name><surname>Singh</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kosciolek</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Tetchner</surname>, <given-names>S</given-names></string-name>. (<year>2015</year>). <article-title>MetaPSICOV: Combining coevolution methods for accurate prediction of contacts and long range hydrogen bonding in proteins. Bioinformatics (Oxford</article-title>, <source>England</source>), <volume>31</volume>(<issue>7</issue>), <fpage>999</fpage>â<lpage>1006</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btu791</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Ju</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Zhu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Shao</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Kong</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>T. Y.</given-names></string-name>, <string-name><surname>Zheng</surname>, <given-names>W. M.</given-names></string-name>, &amp; <string-name><surname>Bu</surname>, <given-names>D</given-names></string-name>. (<year>2021</year>). <article-title>CopulaNet: Learning residue co-evolution directly from multiple sequence alignment for protein structure prediction</article-title>. <source>Nature Communications</source>, <volume>12</volume>(<fpage>1</fpage>). <pub-id pub-id-type="doi">10.1038/S41467-021-22869-8</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Jumper</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Evans</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Pritzel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Green</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Figurnov</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ronneberger</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Tunyasuvunakool</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Bates</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Å½Ã­dek</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Potapenko</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bridgland</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Meyer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kohl</surname>, <given-names>S. A. A.</given-names></string-name>, <string-name><surname>Ballard</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Cowie</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Romera-Paredes</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Nikolov</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Jain</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Adler</surname>, <given-names>J.</given-names></string-name>, â¦ <string-name><surname>Hassabis</surname>, <given-names>D.</given-names></string-name> (<year>2021</year>). <article-title>Highly accurate protein structure prediction with AlphaFold</article-title>. <source>Nature</source>, <volume>596</volume>(<issue>7873</issue>), <fpage>Article 7873</fpage>. <pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Li</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Huang</surname>, <given-names>S.-Y</given-names></string-name>. (<year>2021</year>). <article-title>Proteinâprotein docking with interface residue restraints\ast</article-title>. <source>Chinese Physics B</source>, <volume>30</volume>(<issue>1</issue>), <fpage>018703</fpage>. <pub-id pub-id-type="doi">10.1088/1674-1056/abc14e</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Yu</surname>, <given-names>D. J</given-names></string-name>. (<year>2019</year>). <article-title>ResPRE: High-accuracy protein contact prediction by coupling precision matrix with deep residual neural networks</article-title>. <source>Bioinformatics</source>, <volume>35</volume>(<issue>22</issue>), <fpage>4647</fpage>â<lpage>4655</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btz291</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Lin</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Yan</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Huang</surname>, <given-names>S.-Y</given-names></string-name>. (<year>2023</year>). <article-title>DeepHomo2.0: Improved proteinâprotein contact prediction of homodimers by transformer-enhanced deep learning</article-title>. <source>Briefings in Bioinformatics</source>, <volume>24</volume>(<issue>1</issue>), <fpage>bbac499</fpage>. <pub-id pub-id-type="doi">10.1093/bib/bbac499</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Lin</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Akin</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Rao</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Hie</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Zhu</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Smetanin</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Verkuil</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Kabeli</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Shmueli</surname>, <given-names>Y</given-names></string-name>., <string-name><surname>dos Santos Costa</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Fazel-Zarandi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sercu</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Candido</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Rives</surname>, <given-names>A.</given-names></string-name> (<year>2023</year>). <article-title>Evolutionary-scale prediction of atomic-level protein structure with a language model</article-title>. <source>Science</source>, <volume>379</volume>(<issue>6637</issue>), <fpage>1123</fpage>â<lpage>1130</lpage>. <pub-id pub-id-type="doi">10.1126/science.ade2574</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Martino</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Chiarugi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Margheriti</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Garau</surname>, <given-names>G</given-names></string-name>. (<year>2021</year>). <article-title>Mapping, Structure and Modulation of PPI</article-title>. <source>Frontiers in Chemistry</source>, <volume>9</volume>, <fpage>718405</fpage>. <pub-id pub-id-type="doi">10.3389/fchem.2021.718405</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Ovchinnikov</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kamisetty</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Baker</surname>, <given-names>D</given-names></string-name>. (<year>2014</year>). <article-title>Robust and accurate prediction of residueâ residue interactions across protein interfaces using evolutionary information</article-title>. <source>ELife</source>, <volume>3</volume>, <fpage>e02030</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.02030</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>PagÃ¨s</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Charmettant</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Grudinin</surname>, <given-names>S</given-names></string-name>. (<year>2019</year>). <article-title>Protein model quality assessment using 3D oriented convolutional neural networks</article-title>. <source>Bioinformatics</source>, <volume>35</volume>(<issue>18</issue>), <fpage>3313</fpage>â<lpage>3319</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btz122</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Potter</surname>, <given-names>S. C.</given-names></string-name>, <string-name><surname>Luciani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Eddy</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Park</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Lopez</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Finn</surname>, <given-names>R. D</given-names></string-name>. (<year>2018</year>). <article-title>HMMER web server: 2018 update</article-title>. <source>Nucleic Acids Research</source>, <volume>46</volume>(<issue>W1</issue>), <fpage>W200</fpage>â<lpage>W204</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gky448</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="other"><string-name><surname>Rao</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Verkuil</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Meier</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Canny</surname>, <given-names>J. F.</given-names></string-name>, <string-name><surname>Abbeel</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Sercu</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Rives</surname>, <given-names>A.</given-names></string-name> (<year>2021</year>). <article-title>MSA Transformer</article-title>. <source>BioRxiv</source>, 2021.02.12.430858.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Rao</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Verkuil</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Meier</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Canny</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Abbeel</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Sercu</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Rives</surname>, <given-names>A.</given-names></string-name> (<year>2021a</year>). <source>MSA Transformer. Proceedings of the 38th International Conference on Machine Learning, 8844â8856</source>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v139/rao21a.html">https://proceedings.mlr.press/v139/rao21a.html</ext-link></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Rao</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Verkuil</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Meier</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Canny</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Abbeel</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Sercu</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Rives</surname>, <given-names>A.</given-names></string-name> (<year>2021b</year>). <source>MSA Transformer. Proceedings of the 38th International Conference on Machine Learning, 8844â8856</source>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v139/rao21a.html">https://proceedings.mlr.press/v139/rao21a.html</ext-link></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Rives</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Meier</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sercu</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Goyal</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Guo</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ott</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Zitnick</surname>, <given-names>C. L.</given-names></string-name>, <string-name><surname>Ma</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Fergus</surname>, <given-names>R</given-names></string-name>. (<year>2021</year>). <article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>118</volume>(<issue>15</issue>), <fpage>1</fpage>â<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Roy</surname>, <given-names>R. S.</given-names></string-name>, <string-name><surname>Quadir</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Soltanikazemi</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Cheng</surname>, <given-names>J</given-names></string-name>. (<year>2022</year>). <article-title>A deep dilated convolutional residual network for predicting interchain contacts of protein homodimers</article-title>. <source>Bioinformatics</source>, <volume>38</volume>(<issue>7</issue>), <fpage>1904</fpage>â<lpage>1910</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btac063</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Seemayer</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gruber</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>SÃ¶ding</surname>, <given-names>J</given-names></string-name>. (<year>2014</year>). <article-title>CCMpredâFast and precise prediction of protein residue-residue contacts from correlated mutations. Bioinformatics (Oxford</article-title>, <source>England</source>), <volume>30</volume>(<issue>21</issue>), <fpage>3128</fpage>â<lpage>3130</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btu500</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Si</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Yan</surname>, <given-names>C</given-names></string-name>. (<year>2021</year>). <article-title>Improved protein contact prediction using dimensional hybrid residual networks and singularity enhanced loss function</article-title>. <source>Briefings in Bioinformatics</source>, <volume>22</volume>(<issue>6</issue>), <fpage>bbab341</fpage>. <pub-id pub-id-type="doi">10.1093/bib/bbab341</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Si</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Yan</surname>, <given-names>C</given-names></string-name>. (<year>2022</year>). <article-title>Protein complex structure prediction powered by multiple sequence alignments of interologs from multiple taxonomic ranks and AlphaFold2</article-title>. <source>Briefings in Bioinformatics</source>, <volume>23</volume>(<issue>4</issue>), <fpage>bbac208</fpage>. <pub-id pub-id-type="doi">10.1093/bib/bbac208</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Si</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Yan</surname>, <given-names>C</given-names></string-name>. (<year>2023</year>). <article-title>Improved inter-protein contact prediction using dimensional hybrid residual networks and protein language models. Briefings in Bioinformatics</article-title>, <source>bbad</source><volume>039</volume>. <pub-id pub-id-type="doi">10.1093/bib/bbad039</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Spirin</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Mirny</surname>, <given-names>L. A</given-names></string-name>. (<year>2003</year>). <article-title>Protein complexes and functional modules in molecular networks</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>100</volume>(<issue>21</issue>), <fpage>12123</fpage>â<lpage>12128</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.2032324100</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Steinegger</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Meier</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Mirdita</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>VÃ¶hringer</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Haunsberger</surname>, <given-names>S. J.</given-names></string-name>, &amp; <string-name><surname>SÃ¶ding</surname>, <given-names>J</given-names></string-name>. (<year>2019</year>). <article-title>HH-suite3 for fast remote homology detection and deep protein annotation</article-title>. <source>BMC Bioinformatics</source>, <volume>20</volume>(<issue>1</issue>), <fpage>Article 1</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-019-3019-7</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Sun</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Gong</surname>, <given-names>X</given-names></string-name>. (<year>2020</year>). <article-title>Review of multimer proteinâprotein interaction complex topology and structure prediction\ast</article-title>. <source>Chinese Physics B</source>, <volume>29</volume>(<issue>10</issue>), <fpage>108707</fpage>. <pub-id pub-id-type="doi">10.1088/1674-1056/abb659</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Suzek</surname>, <given-names>B. E.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>McGarvey</surname>, <given-names>P. B.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>C. H.</given-names></string-name>, &amp; <collab>the UniProt Consortium</collab>. (<year>2015</year>). <article-title>UniRef clusters: A comprehensive and scalable alternative for improving sequence similarity searches</article-title>. <source>Bioinformatics</source>, <volume>31</volume>(<issue>6</issue>), <fpage>926</fpage>â<lpage>932</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btu739</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Uguzzoni</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>John Lovis</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Oteri</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Schug</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Szurmant</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Weigt</surname>, <given-names>M</given-names></string-name>. (<year>2017</year>). <article-title>Large-scale identification of coevolution signals across homo-oligomeric protein interfaces by direct coupling analysis</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>114</volume>(<issue>13</issue>), <fpage>E2662</fpage>â<lpage>E2671</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1615068114</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>van Zundert</surname>, <given-names>G. C. P.</given-names></string-name>, <string-name><surname>Rodrigues</surname>, <given-names>J. P. G. L. M.</given-names></string-name>, <string-name><surname>Trellet</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Schmitz</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kastritis</surname>, <given-names>P. L.</given-names></string-name>, <string-name><surname>Karaca</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Melquiond</surname>, <given-names>A. S. J.</given-names></string-name>, <string-name><surname>van Dijk</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>de Vries</surname>, <given-names>S. J.</given-names></string-name>, &amp; <string-name><surname>Bonvin</surname>, <given-names>A. M. J. J.</given-names></string-name> (<year>2016</year>). <article-title>The HADDOCK2.2 Web Server: User-Friendly Integrative Modeling of Biomolecular Complexes</article-title>. <source>Journal of Molecular Biology</source>, <volume>428</volume>(<issue>4</issue>), <fpage>720</fpage>â<lpage>725</lpage>. <pub-id pub-id-type="doi">10.1016/j.jmb.2015.09.014</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Vaswani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Shazeer</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Parmar</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Uszkoreit</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Gomez</surname>, <given-names>A. N.</given-names></string-name>, <string-name><surname>Kaiser</surname>, <given-names>Å.</given-names></string-name>, &amp; <string-name><surname>Polosukhin</surname>, <given-names>I.</given-names></string-name> (<year>2017</year>). <source>Attention is All you Need. Advances in Neural Information Processing Systems</source>, <volume>30</volume>. <ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</ext-link></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Xu</surname>, <given-names>J</given-names></string-name>. (<year>2017</year>). <article-title>Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model</article-title>. <source>PLOS Computational Biology</source>, <volume>13</volume>(<fpage>1</fpage>). <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005324</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Weigt</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>White</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Szurmant</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Hoch</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>Hwa</surname>, <given-names>T</given-names></string-name>. (<year>2009</year>). <article-title>Identification of direct residue contacts in proteinâprotein interaction by message passing</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>106</volume>(<issue>1</issue>), <fpage>67</fpage>â<lpage>72</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0805923106</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Wu</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Gong</surname>, <given-names>X</given-names></string-name>. (<year>2022</year>). <article-title>Inter-chain contact map prediction for protein complex based on graph attention network and triangular multiplication update</article-title>. <source>2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</source>, <fpage>2143</fpage>â <lpage>2148</lpage>. <pub-id pub-id-type="doi">10.1109/BIBM55620.2022.9995360</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Xie</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Xu</surname>, <given-names>J</given-names></string-name>. (<year>2022</year>). <article-title>Deep graph learning of inter-protein contacts</article-title>. <source>Bioinformatics</source>, <volume>38</volume>(<issue>4</issue>), <fpage>947</fpage>â <lpage>953</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btab761</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Yan</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Huang</surname>, <given-names>S. Y</given-names></string-name>. (<year>2021</year>). <article-title>Accurate prediction of inter-protein residue-residue contacts for homo-oligomeric protein complexes</article-title>. <source>Briefings in Bioinformatics</source>, <volume>22</volume>(<issue>5</issue>), <fpage>1</fpage>â<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1093/bib/bbab038</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Zeng</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>Q.</given-names></string-name>, &amp; <string-name><surname>Xu</surname>, <given-names>J</given-names></string-name>. (<year>2018</year>). <article-title>ComplexContact: A web server for inter-protein contact prediction using deep learning</article-title>. <source>Nucleic Acids Research</source>, <volume>46</volume>(<issue>W1</issue>), <fpage>W432</fpage>â<lpage>W437</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gky420</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Skolnick</surname>, <given-names>J</given-names></string-name>. (<year>2004</year>). <article-title>Scoring function for automated assessment of protein structure template quality. Proteins: Structure</article-title>, <source>Function, and Bioinformatics</source>, <volume>57</volume>(<issue>4</issue>), <fpage>702</fpage>â<lpage>710</lpage>. <pub-id pub-id-type="doi">10.1002/prot.20264</pub-id></mixed-citation></ref>
</ref-list>
<sec id="s10">
<title>Supplementary</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1.</label>
<caption><p>3D structure of the homodimer (PDB: 3DFU).</p></caption>
<graphic xlink:href="523121v3_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2.</label>
<caption><title>The comparison of PLMGraph-Inter with AlphaFold-Multimer.</title>
<p>(a) The head-to-head comparisons of precisions of the top 50 inter-protein contacts predicted by PLMGraph-Inter(using AlphaFold2 predicted structures) and AlphaFold-Multimer for each target in the homomeric PPI and heteromeric PPI datasets. (b)â¼(c): The mean precisions of top 50 inter-protein contacts predicted by PLMGraph-Inter(using AlphaFold2 predicted structures as input) and AlphaFold-Multimer on the PPI subsets from (b)âDHTest+HomoPDBâ and (c) âDB5.5+HeteroPDBâ in which the precision of the top 50 inter-protein contacts predicted by AlphaFold-Multimer is lower than 50% or the DockQ of the complex structure predicted by AlphaFold-Multimer is lower than 0.23.</p></caption>
<graphic xlink:href="523121v3_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Figure S3.</label>
<caption><p>The graph representation of protein structures. (a) Dihedral angles of the protein backbone. (b) The local coordinate system of each amino acid. (c) The scalar (distances) and vector (directions) of the edge i-&gt;j.</p></caption>
<graphic xlink:href="523121v3_figs3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Table S1.</label><caption><title>The performances of DeepHomo, GLINTER, DRN-1D2D_Inter, DeepHomo2, CDPred and PLMGraph-Inter on HomoPDB and HeteroPDB after the removal of targets which GLINTER failed to make the prediction</title></caption>
<graphic xlink:href="523121v3_tbls1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls2" orientation="portrait" position="float">
<label>Table S2.</label><caption><title>The performances of DeepHomo, GLINTER, DRN-1D2D_Inter, DeepHomo2, CDPred and PLMGraph-Inter on HomoPDB and HeteroPDB using AlphaFold2 predicted structures</title></caption>
<graphic xlink:href="523121v3_tbls2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls3" orientation="portrait" position="float">
<label>Table S3.</label><caption><title>The performances of different ablation study models on the HomoPDB and HeteroPDB test sets</title></caption>
<graphic xlink:href="523121v3_tbls3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls4" orientation="portrait" position="float">
<label>Table S4.</label><caption><title>The performances of DeepHomo, GLINTER, DRN-1D2D_Inter, DeepHomo2, CDPred and PLMGraph-Inter on the DHTest and DB5.5 test sets using native structures</title></caption>
<graphic xlink:href="523121v3_tbls4.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls5" orientation="portrait" position="float">
<label>Table S5.</label><caption><title>The performances of DeepHomo, GLINTER, DRN-1D2D_Inter, DeepHomo2, CDPred and PLMGraph-Inter on DHTest and DB5.5 after the removal of targets which GLINTER failed to make the prediction</title></caption>
<graphic xlink:href="523121v3_tbls5.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls6" orientation="portrait" position="float">
<label>Table S6.</label><caption><title>The performances of DeepHomo, GLINTER, DRN-1D2D_Inter, DeepHomo2, CDPred and PLMGraph-Inter on the DHTest and DB5.5 test sets using AlphaFold2 predicted structures</title></caption>
<graphic xlink:href="523121v3_tbls6.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92184.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bitbol</surname>
<given-names>Anne-Florence</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Ecole Polytechnique Federale de Lausanne (EPFL)</institution>
</institution-wrap>
<city>Lausanne</city>
<country>Switzerland</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents a <bold>useful</bold> deep learning-based inter-protein contact prediction method named PLMGraph-Inter which combines protein language models and geometric graphs. The evidence supporting the claims of the authors is <bold>solid</bold>, although it could have information leakage between training and test sets, and although more emphasis should be given to predictions starting from unbound monomer structures. The authors show that their approach may be <bold>useful</bold> in some cases where AlphaFold-Multimer performs poorly. This work will be of interest to researchers working on protein complex structure prediction, particularly when accurate experimental structures are available for one or both of the monomers in isolation.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92184.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Given knowledge of the amino acid sequence and of some version of the 3D structure of two monomers that are expected to form a complex, the authors investigate whether it is possible to accurately predict which residues will be in contact in the 3D structure of the expected complex. To this effect, they train a deep learning model that takes as inputs the geometric structures of the individual monomers, per-residue features (PSSMs) extracted from MSAs for each monomer, and rich representations of the amino acid sequences computed with the pre-trained protein language models ESM-1b, MSA Transformer, and ESM-IF. Predicting inter-protein contacts in complexes is an important problem. Multimer variants of AlphaFold, such as AlphaFold-Multimer, are the current state of the art for full protein complex structure prediction, and if the three-dimensional structure of a complex can be accurately predicted then the inter-protein contacts can also be accurately determined. By contrast, the method presented here seeks state-of-the-art performance among models that have been trained end-to-end for inter-protein contact prediction.</p>
<p>Strengths:</p>
<p>The paper is carefully written and the method is very well detailed. The model works both for homodimers and heterodimers. The ablation studies convincingly demonstrate that the chosen model architecture is appropriate for the task. Various comparisons suggest that PLMGraph-Inter performs substantially better, given the same input than DeepHomo, GLINTER, CDPred, DeepHomo2, and DRN-1D2D_Inter. As a byproduct of the analysis, a potentially useful heuristic criterion for acceptable contact prediction quality is found by the authors: namely, to have at least 50% precision in the prediction of the top 50 contacts.</p>
<p>Weaknesses:</p>
<p>My biggest issue with this work is the evaluations made using *bound* monomer structures as inputs, coming from the very complexes to be predicted. Conformational changes in protein-protein association are the key element of the binding mechanism and are challenging to predict. While the GLINTER paper (Xie &amp; Xu, 2022) is guilty of the same sin, the authors of CDPred (Guo et al., 2022) correctly only report test results obtained using predicted unbound tertiary structures as inputs to their model. Test results using experimental monomer structures in bound states can hide important limitations in the model, and thus say very little about the realistic use cases in which only the unbound structures (experimental or predicted) are available. I therefore strongly suggest reducing the importance given to the results obtained using bound structures and emphasizing instead those obtained using predicted monomer structures as inputs.</p>
<p>In particular, the most relevant comparison with AlphaFold-Multimer (AFM) is given in Figure S2, *not* Figure 6. Unfortunately, it substantially shrinks the proportion of structures for which AFM fails while PLMGraph-Inter performs decently. Still, it would be interesting to investigate why this occurs. One possibility would be that the predicted monomer structures are of bad quality there, and PLMGraph-Inter may be able to rely on a signal from its language model features instead. Finally, AFM multimer confidence values (&quot;iptm + ptm&quot;) should be provided, especially in the cases in which AFM struggles.</p>
<p>Besides, in cases where *any* experimental structures - bound or unbound - are available and given to PLMGraph-Inter as inputs, they should also be provided to AlphaFold-Multimer (AFM) as templates. Withholding these from AFM only makes the comparison artificially unfair. Hence, a new test should be run using AFM templates, and a new version of Figure 6 should be produced. Additionally, AFM's mean precision, at least for top-50 contact prediction, should be reported so it can be compared with PLMGraph-Inter's.</p>
<p>It's a shame that many of the structures used in the comparison with AFM are actually in the AFM v2 training set. If there are any outside the AFM v2 training set and, ideally, not sequence- or structure-homologous to anything in the AFM v2 training set, they should be discussed and reported on separately. In addition, why not test on structures from the &quot;Benchmark 2&quot; or &quot;Recent-PDB-Multimers&quot; datasets used in the AFM paper?</p>
<p>It is also worth noting that the AFM v2 weights have now been outdated for a while, and better v3 weights now exist, with a training cutoff of 2021-09-30.</p>
<p>Another weakness in the evaluation framework: because PLMGraph-Inter uses structural inputs, it is not sufficient to make its test set non-redundant in sequence to its training set. It must also be non-redundant in structure. The Benchmark 2 dataset mentioned above is an example of a test set constructed by removing structures with homologous templates in the AF2 training set. Something similar should be done here.</p>
<p>Finally, the performance of DRN-1D2D for top-50 precision reported in Table 1 suggests to me that, in an ablation study, language model features alone would yield better performance than geometric features alone. So, I am puzzled why model &quot;a&quot; in the ablation is a &quot;geometry-only&quot; model and not a &quot;LM-only&quot; one.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92184.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This work introduces PLMGraph-Inter, a new deep-learning approach for predicting inter-protein contacts, which is crucial for understanding protein-protein interactions. Despite advancements in this field, especially driven by AlphaFold, prediction accuracy and efficiency in terms of computational cost) still remains an area for improvement. PLMGraph-Inter utilizes invariant geometric graphs to integrate the features from multiple protein language models into the structural information of each subunit. When compared against other inter-protein contact prediction methods, PLMGraph-Inter shows better performance which indicates that utilizing both sequence embeddings and structural embeddings is important to achieve high-accuracy predictions with relatively smaller computational costs for the model training.</p>
<p>The conclusions of this paper are mostly well supported by data, but test examples should be revisited with a more strict sequence identity cutoff to avoid any potential information leakage from the training data. The main figures should be improved to make them easier to understand.</p>
<p>1. The sequence identity cutoff to remove redundancies between training and test set was set to 40%, which is a bit high to remove test examples having homology to training examples. For example, CDPred uses a sequence identity cutoff of 30% to strictly remove redundancies between training and test set examples. To make their results more solid, the authors should have curated test examples with lower sequence identity cutoffs, or have provided the performance changes against sequence identities to the closest training examples.</p>
<p>2. Figures with head-to-head comparison scatter plots are hard to understand as scatter plots because too many different methods are abstracted into a single plot with multiple colors. It would be better to provide individual head-to-head scatter plots as supplementary figures, not in the main figure.</p>
<p>3. The authors claim that PLMGraph-Inter is complementary to AlphaFold-multimer as it shows better precision for the cases where AlphaFold-multimer fails. To strengthen the point, the qualities of predicted complex structures via protein-protein docking with predicted contacts as restraints should have been compared to those of AlphaFold-multimer structures.</p>
<p>4. It would be interesting to further analyze whether there is a difference in prediction performance depending on the depth of multiple sequence alignment or the type of complex (antigen-antibody, enzyme-substrates, single species PPI, multiple species PPI, etc).</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92184.1.sa3</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Si</surname>
<given-names>Yunda</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Yan</surname>
<given-names>Chengfei</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2010-6668</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>Given knowledge of the amino acid sequence and of some version of the 3D structure of two monomers that are expected to form a complex, the authors investigate whether it is possible to accurately predict which residues will be in contact in the 3D structure of the expected complex. To this effect, they train a deep learning model that takes as inputs the geometric structures of the individual monomers, per-residue features (PSSMs) extracted from MSAs for each monomer, and rich representations of the amino acid sequences computed with the pre-trained protein language models ESM-1b, MSA Transformer, and ESM-IF. Predicting inter-protein contacts in complexes is an important problem. Multimer variants of AlphaFold, such as AlphaFold-Multimer, are the current state of the art for full protein complex structure prediction, and if the three-dimensional structure of a complex can be accurately predicted then the inter-protein contacts can also be accurately determined. By contrast, the method presented here seeks state-of-the-art performance among models that have been trained end-to-end for inter-protein contact prediction.</p>
<p>Strengths:</p>
<p>The paper is carefully written and the method is very well detailed. The model works both for homodimers and heterodimers. The ablation studies convincingly demonstrate that the chosen model architecture is appropriate for the task. Various comparisons suggest that PLMGraph-Inter performs substantially better, given the same input than DeepHomo, GLINTER, CDPred, DeepHomo2, and DRN-1D2D_Inter. As a byproduct of the analysis, a potentially useful heuristic criterion for acceptable contact prediction quality is found by the authors: namely, to have at least 50% precision in the prediction of the top 50 contacts.</p>
</disp-quote>
<p>We thank the reviewer for recognizing the strengths of our work!</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>My biggest issue with this work is the evaluations made using <italic>bound</italic> monomer structures as inputs, coming from the very complexes to be predicted. Conformational changes in protein-protein association are the key element of the binding mechanism and are challenging to predict. While the GLINTER paper (Xie &amp; Xu, 2022) is guilty of the same sin, the authors of CDPred (Guo et al., 2022) correctly only report test results obtained using predicted unbound tertiary structures as inputs to their model. Test results using experimental monomer structures in bound states can hide important limitations in the model, and thus say very little about the realistic use cases in which only the unbound structures (experimental or predicted) are available. I therefore strongly suggest reducing the importance given to the results obtained using bound structures and emphasizing instead those obtained using predicted monomer structures as inputs.</p>
</disp-quote>
<p>We thank the reviewer for the suggestion! We evaluated PLMGraph-Inter with the predicted monomers and analyzed the result in details (see the âImpact of the monomeric structure quality on contact predictionâ section and Figure 3). To mimic the real cases, we even deliberately reduced the performance of AF2 by using reduced MSAs (see the 2nd paragraph in the ââImpact of the monomeric structure quality on contact predictionâ section). We leave some of the results in the supplementary of the current manuscript (Table S2). We will move these results to the main text to emphasize the performance of PLMGraph-Inter with the predicted monomers in the revision.</p>
<disp-quote content-type="editor-comment">
<p>In particular, the most relevant comparison with AlphaFold-Multimer (AFM) is given in Figure S2, <italic>not</italic> Figure 6. Unfortunately, it substantially shrinks the proportion of structures for which AFM fails while PLMGraph-Inter performs decently. Still, it would be interesting to investigate why this occurs. One possibility would be that the predicted monomer structures are of bad quality there, and PLMGraph-Inter may be able to rely on a signal from its language model features instead. Finally, AFM multimer confidence values (&quot;iptm + ptm&quot;) should be provided, especially in the cases in which AFM struggles.</p>
</disp-quote>
<p>We thank the reviewer for the suggestion! Yes! The performance of PLMGraph-Inter drops when the predicted monomers are used in the prediction. However, it is difficult to say which is a fairer comparison, Figure 6 or Figure S2, since AFM also searched monomer templates (see the third paragraph in 7. Supplementary Information : 7.1 Data in the AlphaFold-Multimer preprint: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2.full">https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2.full</ext-link>) in the prediction. When we checked our AFM runs, we found that 99% of the targets in our study (including all the targets in the four datasets: HomoPDB, HeteroPDB, DHTest and DB5.5) employed at least 20 templates in their predictions, and 87.8% of the targets employed the native templates.
We will provide the AFM confidence values of the AFM predictions in the revision.</p>
<disp-quote content-type="editor-comment">
<p>Besides, in cases where <italic>any</italic> experimental structures - bound or unbound - are available and given to PLMGraph-Inter as inputs, they should also be provided to AlphaFold-Multimer (AFM) as templates. Withholding these from AFM only makes the comparison artificially unfair. Hence, a new test should be run using AFM templates, and a new version of Figure 6 should be produced. Additionally, AFM's mean precision, at least for top-50 contact prediction, should be reported so it can be compared with PLMGraph-Inter's.</p>
</disp-quote>
<p>We thank the reviewers for the suggestion! We would like to notify that AFM also searched monomer templates (see the third paragraph in 7. Supplementary Information : 7.1 Data in the AlphaFold-Multimer preprint: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2.full">https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2.full</ext-link>) in the prediction. When we checked our AFM runs, we found that 99% of the targets in our study (including all the targets in the four datasets: HomoPDB, HeteroPDB, DHTest and DB5.5) employed at least 20 templates in their predictions, and 87.8% of the targets employed the native template.</p>
<disp-quote content-type="editor-comment">
<p>It's a shame that many of the structures used in the comparison with AFM are actually in the AFM v2 training set. If there are any outside the AFM v2 training set and, ideally, not sequence- or structure-homologous to anything in the AFM v2 training set, they should be discussed and reported on separately. In addition, why not test on structures from the &quot;Benchmark 2&quot; or &quot;Recent-PDB-Multimers&quot; datasets used in the AFM paper?</p>
</disp-quote>
<p>We thank the reviewer for the suggestion! The biggest challenge to objectively evaluate AFM is that as far as we known, AFM does not release the PDB ids of its training set and the âRecent-PDB-Multimersâ dataset. âBenchmark 2â only includes 17 heterodimer proteins, and the number can be further decreased after removing targets redundant to our training set. We think it is difficult to draw conclusions from such a small number of targets. In the revision, we will analyze the performance of AFM on targets released after the date cutoff of the AFM training set, but with which we cannot totally remove the redundancy between the training and the test sets of AFM.</p>
<disp-quote content-type="editor-comment">
<p>It is also worth noting that the AFM v2 weights have now been outdated for a while, and better v3 weights now exist, with a training cutoff of 2021-09-30.</p>
</disp-quote>
<p>We thank the reviewer for reminding the new version of AFM. The only difference between AFM V3 and V2 is the cutoff date of the training set. Our test set would have more overlaps with the training set of AFM V3, which is one reason that we think AFM V2 is more appropriate to be used in the comparison.</p>
<disp-quote content-type="editor-comment">
<p>Another weakness in the evaluation framework: because PLMGraph-Inter uses structural inputs, it is not sufficient to make its test set non-redundant in sequence to its training set. It must also be non-redundant in structure. The Benchmark 2 dataset mentioned above is an example of a test set constructed by removing structures with homologous templates in the AF2 training set. Something similar should be done here.</p>
</disp-quote>
<p>We agree with the reviewer that testing whether the model can keep its performance on targets with no templates (i.e. non-redundant in structure) is important. We will perform the analysis in the revision.</p>
<disp-quote content-type="editor-comment">
<p>Finally, the performance of DRN-1D2D for top-50 precision reported in Table 1 suggests to me that, in an ablation study, language model features alone would yield better performance than geometric features alone. So, I am puzzled why model &quot;a&quot; in the ablation is a &quot;geometry-only&quot; model and not a &quot;LM-only&quot; one.</p>
</disp-quote>
<p>Using the protein geometric graph to integrate multiple protein language models is the main idea of PLMGraph-Inter. Comparing with our previous work (DRN-1D2D_Inter), we consider the building of the geometric graph as one major contribution of this work. To emphasize the efficacy of this geometric graph, we chose to use the âgeometry-onlyâ model as the base model. We will further clarity this in the revision.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>This work introduces PLMGraph-Inter, a new deep-learning approach for predicting inter-protein contacts, which is crucial for understanding protein-protein interactions. Despite advancements in this field, especially driven by AlphaFold, prediction accuracy and efficiency in terms of computational cost) still remains an area for improvement. PLMGraph-Inter utilizes invariant geometric graphs to integrate the features from multiple protein language models into the structural information of each subunit. When compared against other inter-protein contact prediction methods, PLMGraph-Inter shows better performance which indicates that utilizing both sequence embeddings and structural embeddings is important to achieve high-accuracy predictions with relatively smaller computational costs for the model training.</p>
<p>The conclusions of this paper are mostly well supported by data, but test examples should be revisited with a more strict sequence identity cutoff to avoid any potential information leakage from the training data. The main figures should be improved to make them easier to understand.</p>
</disp-quote>
<p>We thank the reviewer for recognizing the significance of our work! We will revise the manuscript carefully to address the reviewerâs concerns.</p>
<disp-quote content-type="editor-comment">
<p>1. The sequence identity cutoff to remove redundancies between training and test set was set to 40%, which is a bit high to remove test examples having homology to training examples. For example, CDPred uses a sequence identity cutoff of 30% to strictly remove redundancies between training and test set examples. To make their results more solid, the authors should have curated test examples with lower sequence identity cutoffs, or have provided the performance changes against sequence identities to the closest training examples.</p>
</disp-quote>
<p>We thank the reviewer for the valuable suggestion! Using different thresholds to reduce the redundancy between the test set and the training set is a very good suggestion, and we will perform the analysis in the revision. In the current version of the manuscript, the 40% sequence identity is used as the cutoff for many previous studies used this cutoff (e.g. the Recent-PDB-Multimers used in AlphaFold-Multimer (see: 7.8 Datasets in the AlphaFold-Multimer paper); the work of DSCRIPT: <ext-link ext-link-type="uri" xlink:href="https://www.cell.com/action/showPdf?pii=S2405-4712%2821%2900333-1">https://www.cell.com/action/showPdf?pii=S2405-4712%2821%2900333-1</ext-link> (see: the PPI dataset paragraph in the METHODS DETAILS section of the STAR METHODS)). One reason for using the relatively higher threshold for PPI studies is that PPIs are generally not as conserved as protein monomers.</p>
<p>We performed a preliminary analysis using different thresholds to remove redundancy when preparing this provisional response letter:</p>
<table-wrap id="sa3table1">
<label>Author response table 1.</label>
<caption>
<title>Table1.</title>
<p>The performance of PLMGraph-Inter on the HomoPDB and HeteroPDB test sets using native structures(AlphaFold2 predicted structures).</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-92184-sa3-table1.jpg" mimetype="image"/>
</table-wrap>
<p>Method:</p>
<p>To remove redundancy, we clustered 11096 sequences from the training set and test sets (HomoPDB, HeteroPDB) using MMSeq2 with different sequence identity threshold (40%, 30%, 20%, 10%) (the lowest cutoff for CD-HIT is 40%, so we switched to MMSeq2).  Each sequence is then uniquely labeled by the cluster (e.g. cluster 0, cluster 1, â¦) to which it belongs, from which each PPI can be marked with a pair of clusters (e.g. cluster 0-cluster 1). The PPIs belonging to the same cluster pair (note: cluster n - cluster m and cluster n-cluster m were considered as the same pair) were considered as redundant. For each PPI in the test set, if the pair cluster it belongs to contains the PPI belonging to the training set, we remove that PPI from the test set.</p>
<p>We will perform more detailed analyses in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>1. Figures with head-to-head comparison scatter plots are hard to understand as scatter plots because too many different methods are abstracted into a single plot with multiple colors. It would be better to provide individual head-to-head scatter plots as supplementary figures, not in the main figure.</p>
</disp-quote>
<p>We thank the reviewer for the suggestion! We will include the individual head-to-head scatter plots as supplementary figures in the revision.</p>
<p>1. The authors claim that PLMGraph-Inter is complementary to AlphaFold-multimer as it shows better precision for the cases where AlphaFold-multimer fails. To strengthen the point, the qualities of predicted complex structures via protein-protein docking with predicted contacts as restraints should have been compared to those of AlphaFold-multimer structures.</p>
<p>We thank the reviewer for the suggestion! We will add this comparison in the revision.</p>
<p>1. It would be interesting to further analyze whether there is a difference in prediction performance depending on the depth of multiple sequence alignment or the type of complex (antigen-antibody, enzyme-substrates, single species PPI, multiple species PPI, etc).</p>
<p>We thank the reviewer for the suggestion! We will perform such analysis in the revision.</p>
</body>
</sub-article>
</article>