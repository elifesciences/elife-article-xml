<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">101802</article-id>
<article-id pub-id-type="doi">10.7554/eLife.101802</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.101802.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>

<title-group>
<article-title>Statistical learning beyond words in human neonates</article-title>
</title-group>

<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3260-0559</contrib-id>
<name>
<surname>Fló</surname>
<given-names>Ana</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>ana.flo@unipd.it</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9578-6039</contrib-id>
<name>
<surname>Benjamin</surname>
<given-names>Lucas</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Palu</surname>
<given-names>Marie</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2221-9081</contrib-id>
<name>
<surname>Dehaene-Lambertz</surname>
<given-names>Ghislaine</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Cognitive Neuroimaging Unit</institution>, CNRS ERL 9003, INSERM U992, CEA, <institution>Université Paris-Saclay, NeuroSpin center</institution>, Gif/Yvette, <country>France</country></aff>
<aff id="a2"><label>2</label><institution>Department of Developmental Psychology and Socialisation and Department of Neuroscience, University of Padova</institution>, Padova, <country>Italy</country></aff>
<aff id="a3"><label>3</label><institution>Departement d’étude Cognitives, École Normale Supérieure</institution>, Paris, <country>France</country></aff>
<aff id="a4"><label>4</label><institution>Aix Marseille Univ, INSERM, INS, Inst Neurosci syst</institution>, Marseille, <country>France</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Herrmann</surname>
<given-names>Björn</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Baycrest Hospital</institution>
</institution-wrap>
<city>Toronto</city>
<country>Canada</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="original-publication" iso-8601-date="2024-10-29">
<day>29</day>
<month>10</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP101802</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-08-19">
<day>19</day>
<month>08</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-07-26">
<day>26</day>
<month>07</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.07.26.605295"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Fló et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Fló et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-101802-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Interest in statistical learning in developmental studies stems from the observation that 8-month-olds were able to extract words from a monotone speech stream solely using the transition probabilities (TP) between syllables (<xref ref-type="bibr" rid="c52">Saffran et al., 1996</xref>). A simple mechanism was thus part of the human infant’s toolbox for discovering regularities in language. Since this seminal study, observations on statistical learning capabilities have multiplied across domains and species, challenging the hypothesis of a dedicated mechanism for language acquisition. Here, we leverage the two dimensions conveyed by speech –speaker identity and phonemes– to examine (1) whether neonates can compute TPs on one dimension despite irrelevant variation on the other and (2) whether the linguistic dimension enjoys an advantage over the voice dimension. In two experiments, we exposed neonates to artificial speech streams constructed by concatenating syllables while recording EEG. The sequence had a statistical structure based either on the phonetic content, while the voices varied randomly (Experiment 1) or on voices with random phonetic content (Experiment 2). After familiarisation, neonates heard isolated duplets adhering, or not, to the structure they were familiarised with. In both experiments, we observed neural entrainment at the frequency of the regularity and distinct Event-Related Potentials (ERP) to correct and incorrect duplets, highlighting the universality of statistical learning mechanisms and suggesting it operates on virtually any dimension the input is factorised. However, only linguistic duplets elicited a specific ERP component consistent with an N400, suggesting a lexical stage triggered by phonetic regularities already at birth. These results show that, from birth, multiple input regularities can be processed in parallel and feed different higher-order networks.</p>
</abstract>
<abstract>
<title>Highlights</title>
<list list-type="bullet">
<list-item><p>Human neonates are sensitive to regularities in speech, encompassing both phonetic content and voice dimension.</p></list-item>
<list-item><p>There is no observed advantage for linguistic content over voice content</p></list-item>
<list-item><p>Both speech dimensions are processed in parallel by distinct networks.</p></list-item>
<list-item><p>Only phonetic regularities evoked an N400, suggesting activations along the pathway to the lexicon.</p></list-item>
</list>
</abstract>
<kwd-group kwd-group-type="author">
<title>KEYWORDS:</title>
<kwd>Neonates</kwd>
<kwd>statistical learning</kwd>
<kwd>neural entrainment</kwd>
<kwd>speech</kwd>
<kwd>phonemes</kwd>
<kwd>voices</kwd>
<kwd>ERP</kwd>
<kwd>language</kwd>
<kwd>steady-state</kwd>
<kwd>frequency tagging</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Since speech is a continuous signal, one of the infants’ first challenges during language acquisition is to break it down into smaller units, notably to be able to extract words. Parsing has been shown to rely on prosodic cues (e.g., pitch and duration changes) but also on identifying regular patterns across perceptual units. Almost 20 years ago, Saffran, Newport, and Aslin (1996) demonstrated that infants are sensitive to local regularities between syllables. After hearing a stream of continuous and monotonous syllables constructed by concatenating four tri-syllabic pseudo-words, 8-month-old infants distinguished a list of these triplets from a list of triplets formed by the first part of one pseudo-word and the last part of another (called part-words). Indeed, for the correct triplets (called words), the TP between syllables was 1, whereas it drops to 1/3 for the transition encompassing two words present in the part-words. Since this seminal study, statistical learning has been regarded as an essential mechanism for language acquisition because it allows for the extraction of regular patterns without prior knowledge.</p>
<p>During the last two decades, many studies have extended this finding by demonstrating sensitivity to statistical regularities in sequences across domains and species. For example, segmentation capacities analogous to those observed for a syllable stream are observed throughout life in the auditory modality for tones (<xref ref-type="bibr" rid="c40">Kudo et al., 2011</xref>; <xref ref-type="bibr" rid="c53">Saffran et al., 1999</xref>) and in the visual domain for shapes (<xref ref-type="bibr" rid="c12">Bulf et al., 2011</xref>; <xref ref-type="bibr" rid="c23">Fiser and Aslin, 2002</xref>; <xref ref-type="bibr" rid="c39">Kirkham et al., 2002</xref>) and actions (<xref ref-type="bibr" rid="c3">Baldwin et al., 2008</xref>; <xref ref-type="bibr" rid="c45">Monroy et al., 2017</xref>). Non-human animals, such as cotton-top tamarins (<xref ref-type="bibr" rid="c33">Hauser et al., 2001</xref>), rats (<xref ref-type="bibr" rid="c60">Toro and Trobalón, 2005</xref>), dogs (<xref ref-type="bibr" rid="c10">Boros et al., 2021</xref>), and chicks (<xref ref-type="bibr" rid="c55">Santolin et al., 2016</xref>) are also sensitive to TPs. While the level of complexity that each species can track might differ, statistical learning between events appears as a general learning mechanism for auditory and visual sequence processing (for a review of statistical learning capacities across species, see (<xref ref-type="bibr" rid="c56">Santolin and Saffran, 2018</xref>)).</p>
<p>Using near-infra-red spectroscopy (NIRS) and electroencephalography (EEG), we have shown that statistical learning is observed in sleeping neonates (<xref ref-type="bibr" rid="c24">Flo et al., 2022</xref>; <xref ref-type="bibr" rid="c25">Fló et al., 2019</xref>), highlighting the automaticity of this mechanism. We also discovered that tracking statistical probabilities might not lead to stream segmentation in the case of quadrisyllabic words in both neonates and adults, revealing an unsuspected limitation of this mechanism (<xref ref-type="bibr" rid="c6">Benjamin et al., 2022</xref>). Here, we aimed to further characterise the characteristics of this mechanism in order to shed light on its role in the early stages of language acquisition. In particular, we wanted to clarify whether, in human neonates, statistical learning is a general learning mechanism applicable to any speech feature or whether there is a bias in favour of calculations on linguistic content to extract words; and secondly, at what level infant newborns compute transitions between syllables, at a low auditory level, i.e. between the presented events or later in the processing chain, at the phonetic level after normalisation through an irrelevant dimension such as voices.</p>
<p>We have, therefore, taken advantage of the fact that syllables convey two important pieces of information for humans: what is being said and who is speaking, i.e. linguistic information and information about the speaker’s identity. While statistical learning can be helpful to word extraction, a statistical relationship between successive voices is not of obvious use and could even hinder word extraction if instances of a word uttered by a different speaker are considered independently. However, as auditory processing is organised along several hierarchical and parallel pathways integrating different spectro-temporal dimensions (<xref ref-type="bibr" rid="c4">Belin et al., 2000</xref>; <xref ref-type="bibr" rid="c19">DeWitt and Rauschecker, 2012</xref>; <xref ref-type="bibr" rid="c46">Norman-Haignere et al., 2015</xref>; <xref ref-type="bibr" rid="c61">Zatorre and Belin, 2001</xref>), statistical learning might be computed on one dimension independently of the variation of the other along the linguistic and the voice pathways in parallel. Given the numerous behavioural and brain imaging studies showing phonetic normalisation across speakers in infants (<xref ref-type="bibr" rid="c17">Dehaene-Lambertz and Pena, 2001</xref>; <xref ref-type="bibr" rid="c30">Gennari et al., 2021</xref>; <xref ref-type="bibr" rid="c41">Kuhl and Miller, 1982</xref>), as also the statistical learning studies in the second half of the first year using different voices (<xref ref-type="bibr" rid="c22">Estes and Lew-Williams, 2015</xref>) and natural speech in which there is already variations of production from one instance to another (<xref ref-type="bibr" rid="c34">Hay et al., 2011</xref>; <xref ref-type="bibr" rid="c49">Pelucchi et al., 2015</xref>), we were expecting that even if each syllable is produced by a different speaker, TPs between syllables would be computed after a normalisation process at the syllable or phonetic level, even in neonates. The predictions for learning the TPs between different voices were more open. Either statistical learning is universal and can be similarly computed over any dimension comprising voices, or listening to a speech stream favours the processing of phonetic regularities over other non-linguistic dimensions of speech and thus hinders the possibility of computing regularities over voices.</p>
<p>To study these possibilities, we constructed artificial streams based on the random concatenation of three bi-syllabic pseudo-words (duplets). To build the duplets, we used six consonant-vowel (CV) syllables produced by six voices (Table S1 and S2), resulting in 36 possible tokens. To form the streams, tokens were combined either by considering their phonetic content (Experiment 1: Structure over Phonemes) or their voice content (Experiment 2: Structure over Voices), while the second dimension varied randomly (<xref rid="fig1" ref-type="fig">Figure 1</xref>). For example, in Experiment 1, one duplet could be <italic>petu</italic> with <italic>pe</italic> and <italic>tu</italic> uttered by a random voice each time. In contrast, in Experiment 2, one duplet could be the combination [<italic>yellow</italic> voice-<italic>red</italic> voice], each uttering randomly any of the syllables.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Experimental protocol.</title>
<p>The experiments started with a Random stream (120 s) in which both syllables and voices changed randomly, followed by a long-Structured stream (120 s). Then, 10 short familiarisation streams (30 s), each followed by test blocks comprising 18 isolated duplets (SOA 2-2.3 s) were presented. Example streams are presented to illustrate the construction of the streams, with different colours representing different voices. In Experiment 1, the Structured stream had a statistical structure based on phonemes (TPs alternated between 1 and 0.5), while the voices were randomly changing (uniform TPs of 0.2). For example, the two syllables of the word “petu” were produced by different voices, which randomly changed at each presentation of the word. In Experiment 2, the statistical structure was based on voices (TPs alternated between 1 and 0.5), while the syllables changed randomly (uniform TPs of 0.2). For example, the “green” voice was always followed by the “red” voice, but they were randomly saying different syllables “boda” then “tupe” in our example. The test duplets were either Words (TP=1) or Partwords (TP=0.5). Words and Partwords were defined in terms of phonetic content for Experiment 1 and voice content for Experiment 2.</p></caption>
<graphic xlink:href="605295v1_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>If infants at birth compute regularities on the pure auditory signal, this implies computing the TPs over the 36 tokens. Thus, they should compute a 36 × 36 TPs matrix relating each acoustic signal, with TPs alternating between 1/6 within words and 1/12 between words. With this type of computation, we predict infants should fail the task in both experiments since previous studies showing successful segmentation in infants use high TP within words (usually 1) and much fewer elements (most studies 4 to 12) (<xref ref-type="bibr" rid="c54">Saffran and Kirkham, 2018</xref>). If speech input is processed along the two studied dimensions in distinct pathways, it enables the calculation of two independent TP matrices of 6×6 between the six voices and six syllables. These computations would result in TPs alternating between 1 and 1/2 for the informative feature and uniform at 1/5 for the uninformative feature, leading to stream segmentation based on the informative dimension.</p>
<p>As in our previous experiments (<xref ref-type="bibr" rid="c6">Benjamin et al., 2022</xref>; <xref ref-type="bibr" rid="c24">Flo et al., 2022</xref>), we used high-density EEG (128 electrodes) to study speech segmentation abilities. Using artificial language with syllables with a fixed duration elicits Steady State Evoked Potentials (SSEP) at the syllable rate. Crucially, if the artificial language presents a regular structure between the syllables (i.e., regular drops in TPs marking word boundaries), if the structure is perceived, then the neural responses reflect the slower frequency of the word as well (<xref ref-type="bibr" rid="c11">Buiatti et al., 2009</xref>). In other words, the brain activity becomes phase-locked to the regular input, increasing the Inter Trial Coherence (ITC) and power at the input regularity frequencies. Under these circumstances, the analysis in the frequency domain is advantageous since fast and periodic responses can be easily investigated by looking at the target frequencies without considering their specific timing (<xref ref-type="bibr" rid="c38">Kabdebon et al., 2022</xref>). The phenomenon is also named frequency tagging or neural entrainment in the literature. Here, we will refer to it indistinctively as SSEP or neural entrainment since we do not aim to make any hypothesis on the origin of the response (i.e., pure evoked or phase reset of endogenous oscillations (<xref ref-type="bibr" rid="c31">Giraud and Poeppel, 2012</xref>)). Our study used an orthogonal design across two groups of 1-4 day-old neonates. In Experiment 1 (34 infants), the regularities in the speech stream were based on the phonetic content, while the voices varied randomly (Phoneme group). Conversely, in Experiment 2 (33 infants), regularities were based on voices, while the phonemes changed randomly (Voice group). Both experiments started with a control stream in which both features varied randomly (i.e. Random stream, 120 s). Next, neonates were exposed to the Structured stream (120 s) with statistical structure over one or the other feature. The experiments ended with ten sets of 18 test duplets presented in isolation, preceded by short Structured streams (30 s) to maintain learning (<xref rid="fig1" ref-type="fig">Figure 1</xref>). Half of the test duplets corresponded to familiar regularities (Words, TP =1), and the other half were duplets present in the language but which straddled a drop in TPs (Partwords, TP = 0.5).</p>
<p>To investigate online learning, we quantified the ITC as a measure of neural entrainment at the syllable (4 Hz) and word rate (2 Hz) during the presentation of the continuous streams. For the recall process, we compared ERPs to Word and Part-Word duplets. We also tested 57 adult participants in a comparable behavioural experiment to investigate adults’ segmentation capacities under the same conditions.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Neural entrainment during the familiarisation phase</title>
<p>To measure neural entrainment, we quantified the ITC in non-overlapping epochs of 7.5 s. We compared the studied frequency (syllabic rate 4 Hz or duplet rate 2 Hz) with the 12 adjacent frequency bins following the same methodology as in our previous studies.</p>
<p>For the Random streams, we observed significant entrainment at syllable rate (4Hz) over a broad set of electrodes in both experiments (p &lt; 0.05, FDR corrected) and no enhanced activity at the duplet rate for any electrode (p &gt; 0.05, FDR corrected). Concerning the Structured streams, ITC increased at both the syllable and duplet rate (p &lt; 0.05, FDR corrected) in both experiments (<xref rid="fig2" ref-type="fig">Figure 2A</xref>, 2B). The duplet effect was localised over occipital and central-left electrodes in the Phoneme group and over occipital and temporal-right electrodes in the Voice group.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Neural entrainment during the random and structured streams.</title>
<p>(A) SNR for the ITC during the Random and Structured streams of Experiment 1 (structure on phonetic content). The topographies represent the entrainment in the electrode space at the syllabic (4 Hz) and duplet rates (2 Hz). Crosses indicate the electrodes showing enhanced neural entrainment (cross: p &lt; 0.05, one-sided paired permutation test, FDR corrected by the number of electrodes; dot: p &lt; 0.05, without FDR correction). Colour scale limits [-1.8, 1.8]. The entrainment for each electrode is shown in light grey. The thick orange line shows the mean over the electrodes with significant entrainment relative to the adjacent frequency bins at the syllabic rate (4 Hz) (p &lt; 0.05 FDR corrected). The thick green line shows the mean over the electrodes showing significant entrainment relative to the adjacent frequency bins at the duplet rate (2 Hz) (p &lt; 0.05 FDR corrected). The asterisks indicate frequency bins with entrainment significantly higher than on adjacent frequency bins for the average across electrodes (p &lt; 0.05, one-sided permutation test, FDR corrected for the number of frequency bins). (B) Analog to A for Experiment 2 (structure on voice content). (C) The first two rows show the topographies for the difference in entrainment during the Structured and Random streams at 4 Hz and 2 Hz for both experiments. Crosses indicate the electrodes showing stronger entrainment during the Structured stream (cross: p &lt; 0.05, one-sided paired permutation test, FDR corrected by the number of electrodes; dot: p &lt; 0.05, without FDR correction). The bottom row shows the interaction effect by comparing the difference in entrainment during the Structured and Random streams between Experiments 1 and 2. Crosses indicate significant differences (cross: p &lt; 0.05, two-sided unpaired permutation test, FDR corrected by the number of electrodes; dot: p &lt; 0.05, without FDR correction). (D) Time course of the neural entrainment at 4 Hz for the average over electrodes showing significant entrainment during the Random stream and at 2 Hz for the average over electrodes showing significant entrainment during the Structured stream (Phoneme: green line, Voice blue line). The shaded area represents standard errors. The horizontal lines on the bottom indicate when the entrainment was larger than 0 (p &lt; 0.05, one-sided t-test, corrected by FDR by the number of time points).</p></caption>
<graphic xlink:href="605295v1_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We also directly compared the ITC at both frequencies of interest between the Random and Structured conditions (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). We found electrodes with significantly higher ITC at the duplet rate during Structured streams than Random streams in both experiments (p &lt; 0.05, FDR corrected). We also found electrodes with higher entrainment at syllable rate during the Structured than Random streams in both experiments (p &lt; 0.05, FDR corrected). This effect might result from stronger or more phase-locked responses to syllables over those electrodes when the input is structured. While the first harmonic of the duplet rate that coincides with the syllable rate could also contribute to this effect, this contribution is unlikely since the electrodes differ from the electrodes, showing enhanced word-rate activity at 2 Hz.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Cluster-based permutation analysis of ERPs to isolated duplets during recall</title>
<p>The topographies show the difference between the two conditions corresponding to each main effect. Results obtained from the cluster-based permutation analyses are shown at the bottom of each panel. Thick lines correspond to the grand averages for the two main tested conditions. Shaded areas correspond to the standard error across participants. Thin lines show the ERPs separated by duplet type and familiarisation type. The shaded areas between the thick lines show the time extension of the cluster. The topographies correspond to the difference between conditions during the time extension of the cluster. The electrodes belonging to the cluster are marked with a cross. Significant clusters are indicated with an asterisk. Color scale limits [-0.07, 0.07] a.u. (A) Main effect of Test-duplets (Words - Part-words) over a frontal-right positive cluster (p = 0.019) and a left temporal negative cluster (p = 0.0056). (B) Main effect of familiarisation (Phonemes - Voices) over a posterior negative cluster (p = 0.018). The frontal positive cluster did not reach significance (p = 0.12). Results are highly comparable to the ROIs-based analysis presented in SI (Fig S3).</p></caption>
<graphic xlink:href="605295v1_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Adults’ behavioural experiment.</title>
<p>Each subject’s average score attributed to the Words (blue) and Partwords (orange) is represented. On the right, for the group familiarised with the Phoneme structure and on the left, for the group familiarised with the Voice structure. The difference between test duplets was significant for the Phoneme group (<italic>p = 0.007</italic>) and only marginally significant for the Voice group (<italic>p = 0.050</italic>).</p></caption>
<graphic xlink:href="605295v1_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Finally, we looked for an interaction effect between groups and conditions (Structured vs. Random streams) (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). A few electrodes show differential responses between groups, reflecting the topographical differences observed in the previous analysis, notably the trend for stronger ITC at 2 Hz over the left central electrodes for the Phoneme group compared to the Voice group, but none survive multiple comparison corrections.</p>
</sec>
<sec id="s2b">
<title>Learning time-course</title>
<p>To investigate the time course of the learning, we computed neural entrainment at the duplet rate in sliding time windows of 2 minutes with a 1 s step across both random and structured streams (<xref rid="fig2" ref-type="fig">Figure 2D</xref>). Notice that because the integration window was two minutes long, the entrainment during the first minute of the structure stream included data from the random stream. To test whether ITC at 2 Hz increased during long Structured familiarisation (120 s), we fitted a Linear Mixed Model (LMM) with a fixed effect of time and random slopes and interceptions for individual subjects: <italic>ITC</italic> ∼ − 1 + <italic>time</italic> + (1 + <italic>time</italic>|<italic>subject</italic>). In the Phoneme group, we found a significant time effect (β=4.16×10<sup>-3</sup>, 95% CI=[2.06×10<sup>-3</sup>, 6.29×10<sup>-</sup> <sup>3</sup>], SE=1.05×10<sup>-3</sup>, p=4×10<sup>-4</sup>), as well as in the Voice group (β=2.46×10<sup>-3</sup>, 95% CI=[2.6×10<sup>-4</sup>, 4.66×10<sup>-3</sup>], SE=1.09×10<sup>-3</sup>, p=0.03). To test for differences in the time effect between groups, we included all data in a single LMM: <italic>ITC</italic> ∼ − 1 + <italic>time</italic> ∗ <italic>group</italic> + (1 + <italic>time</italic>|<italic>subject</italic>). The model showed a significant fixed effect of time for the Phoneme group consistent with the previous results (β=4.22×10<sup>-3</sup>, 95% CI=[1.07×10<sup>-3</sup>, 7.37×10<sup>-3</sup>], SE=1.58×10<sup>-3</sup>, p=0.0096), while the fixed effect estimating the difference between the Phoneme and Voice groups was not significant (β=-1.84×10<sup>-3</sup>, 95% CI=[-6.29×10<sup>-3</sup>, 2.61×10<sup>-3</sup>], SE=2.24×10<sup>-3</sup>, p=0.4).</p>
</sec>
<sec id="s2c">
<title>ERPs during the test phase</title>
<p>To test the recall process, we also measured ERP to isolated duplets afterwards. The average ERP to all conditions merged is shown in Figure S1. We investigated (1) the main effect of test duplets (Word vs. Part-word) across both experiments, (2) the main effect of familiarisation structure (Phoneme group vs. Voice group), and finally (3) the interaction between these two factors. We used non-parametric cluster-based permutation analyses (i.e. without a priori ROIs) (<xref ref-type="bibr" rid="c47">Oostenveld et al., 2011</xref>).</p>
<p>The difference between Word and Part-word consisted of a dipole with a median positivity and a left temporal negativity ranging from 400 to 1500 ms, with a maximum around 800-900 ms (<xref rid="fig3" ref-type="fig">Fig 3</xref>). Cluster-based permutations recovered two significant clusters around 500-1500 ms: a frontal-right positive cluster (p = 0.019) and a left temporal negative cluster (p = 0.0056). A difference between groups was observed consisting of a dipole that started with a right temporal positivity left temporo-occipital negativity around 300 ms and rotated anti-clockwise to bring the positivity over the frontal electrodes and the negativity at the back of the head (500-800 ms) (Fig S2). Cluster-based permutations on the Phoneme group vs. Voice group recovered a posterior cluster (p = 0.018) around 500 ms; with no positive cluster reaching significance (p &gt;0.10). A cluster-based permutation analysis on the interaction effect, i.e., comparing Words - Part-Words between both experiments, showed no significant clusters (p &gt; 0.1).</p>
<p>As cluster-based statistics are not very sensitive, we also analysed the ERPs over seven ROIS defined on the grand average ERP of all merged conditions (see Methods). Results replicated what we observed with the cluster-based permutation analysis with similar differences between Words and Part-words for the effect of familiarisation and no significant interactions. Results are presented in SI. The temporal progression of voltage topographies for all ERPs is presented in Figure S2. To verify that the effects were not driven by one group per duplet type condition, we ran a mixed two-way ANOVA for the average activity in each ROI and significant time window, with duplet type (Word/Part-word) as within-subjects factor and familiarisation as between-subjects factor. We did not observe significant interactions in any case. Future studies should consider a within-subject design to gain sensitivity to possible interaction effects.</p>
</sec>
<sec id="s2d">
<title>Adult’s behavioural performance in the same task</title>
<p>Adult participants heard a Structure Learning stream lasting 120 s and then ten sets of 18 test duplets preceded by Short Structure streams (30 s). For each test bigram, they had to rate their familiarity with it on a scale from 1 to 6. For the group familiarised with the Phoneme structure, there was a significant difference between the score attributed to Words and Part-words (<italic>t(26)=2.92, p = 0.007, Cohen’s d =0.562</italic>). The difference was marginally significant for the group familiarised with the Voice structure (<italic>t(29)=2.0443, p = 0.050, Cohen’s d=0.373</italic>). A 2-way ANOVA with test-duplets and familiarisation as factors revealed a main effect of Word (<italic>F(1,55)=12.52, p=0.0008, η <sup>2</sup>=0.039</italic>), no effect of familiarisation (<italic>F(1,55) &lt;1</italic>), and a significant interaction Word × Familiarisation (<italic>F(1,55) = 5.28, p= 0.025, η<sub>g</sub><sup>2</sup>=0.017</italic>).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<sec id="s3a">
<title>Statistical learning is a general learning mechanism</title>
<p>In two experiments, we compared STATISTICAL LEARNING over a linguistic and a non-linguistic dimension in sleeping neonates. We took advantage of the possibility of constructing streams based on the same tokens, the only difference between the experiments being the arrangement of the tokens in the streams. We showed that neonates were sensitive to regularities based either on the phonetic or the voice dimensions of speech, even in the presence of a non-informative feature that must be disregarded.</p>
<p>Parsing based on statistical information was revealed by steady-state evoked potentials at the duplet rate observed around 2 min after the onset of the familiarisation stream and by different ERPs to Words and Part-words presented during the test in both experiments. Despite variations in the other dimension, statistical learning was possible, showing that this mechanism operates at a stage when these dimensions have already been separated along different processing pathways. Our results, thus, revealed that linguistic content and voice identity are calculated independently and in parallel. This result confirms that, even in newborns, the syllable is not a holistic unit (<xref ref-type="bibr" rid="c30">Gennari et al., 2021</xref>) but that the rich temporo-frequential spectrum of speech is processed in parallel along different networks, probably using different integration factors (<xref ref-type="bibr" rid="c9">Boemio et al., 2005</xref>; <xref ref-type="bibr" rid="c44">Moerel et al., 2012</xref>; <xref ref-type="bibr" rid="c61">Zatorre and Belin, 2001</xref>).</p>
<p>Second, we observed no obvious advantage for the linguistic dimension in neonates. These results reveal the universality of statistical learning. While statistical learning has already been described in many domains and species (<xref ref-type="bibr" rid="c29">Frost et al., 2015</xref>; <xref ref-type="bibr" rid="c51">Ren and Wang, 2023</xref>; <xref ref-type="bibr" rid="c56">Santolin and Saffran, 2018</xref>), we add here that even sleeping neonates distinctly applied it to possibly all the dimensions in which speech is factorised in the auditory cortex (<xref ref-type="bibr" rid="c30">Gennari et al., 2021</xref>; <xref ref-type="bibr" rid="c32">Gwilliams et al., 2022</xref>). This mechanism gives them a powerful tool to create associations between recurrent events.</p>
</sec>
<sec id="s3b">
<title>Differences between statistical learning over voices and over phonemes</title>
<p>While the main pattern of results between experiments was comparable, we did observe some differences. The word-rate steady-state response (2 Hz) for the group of infants exposed to structure over phonemes was left lateralised over central electrodes, while the group of infants hearing structure over voices showed mostly entrainment over right temporal electrodes. These results are compatible with statistical learning in different lateralised neural networks for processing speech’s phonetic and voice content. Recent brain imaging studies on infants do indeed show precursors of later networks with some hemispheric biases (<xref ref-type="bibr" rid="c8">Blasi et al., 2011</xref>; <xref ref-type="bibr" rid="c16">Dehaene-Lambertz et al., 2010</xref>), even if specialisation increases during development (<xref ref-type="bibr" rid="c58">Shultz et al., 2014</xref>; <xref ref-type="bibr" rid="c59">Sylvester et al., 2023</xref>). The hemispheric differences reported here should be considered cautiously since the group comparison did not survive multiple comparison corrections. Future work investigating the neural networks involved should implement a within-subject design to gain statistical power.</p>
<p>The time course of the entrainment at the duplet rate revealed that entrainment emerged at a similar time for both statistical structures. While this duplet rate response seemed more stable in the Phoneme group (i.e., the ITC at the word rate was higher than zero in a sustained way only in the Phoneme group, and the slope of the increase was steeper), no significant difference was observed between groups. Since we did not observe group differences in the ERPs to Words and Part-words during the test, it is unlikely that these differences during learning were due to a worse computation of the statistical transitions for the voice stream relative to the phoneme stream. An alternative explanation might be related to the nature of the duplet rate entrainment. Entrainment might result either from a different response to low and high TPs or (and) from a response to chunks in the stream (i.e., “Words”). In a previous study (<xref ref-type="bibr" rid="c6">Benjamin et al., 2022</xref>), we showed that in some circumstances, neonates compute TPs, but entrainment does not emerge, likely due to the absence of chunking. It is thus possible that chunking was less stable when the regularity was over voices, consistent with the results of previous studies reporting challenges with voice identification in infants as in adults (<xref ref-type="bibr" rid="c35">Johnson et al., 2011</xref>; <xref ref-type="bibr" rid="c43">Mahmoudzadeh et al., 2016</xref>).</p>
</sec>
<sec id="s3c">
<title>Phoneme regularities might trigger a lexical search</title>
<p>In the test part on isolated duplets, we also observed a significant difference between groups: A dipole, consisting of a posterior negative pole and a frontal positivity, was observed around 500 ms following linguistic duplets but not voice duplets. Since the acoustic properties of the duplets were the same in the two experiments, the difference can only be due to an endogenous process that modulates stimuli processing. Given its topography, latency and the context in which it appears, we hypothesise that this component is congruent with an N400, a component elicited by lexico-semantic manipulations in adults (<xref ref-type="bibr" rid="c42">Kutas and Federmeier, 2011</xref>). As often in infants, the component latency is delayed, and its topography is more posterior relative to later ages (<xref ref-type="bibr" rid="c28">Friedrich and Friederici, 2005</xref>; <xref ref-type="bibr" rid="c36">Junge et al., 2021</xref>). A larger posterior negativity has been reported in infants as young as five months when they hear their name compared to a stranger’s name (<xref ref-type="bibr" rid="c48">Parise et al., 2010</xref>), when a pseudo-word is consistent vs inconsistently associated with an object (<xref ref-type="bibr" rid="c27">Friedrich and Friederici, 2011</xref>), and when they see unexpected vs expected actions (<xref ref-type="bibr" rid="c50">Reid et al., 2009</xref>), suggesting that such negativity might be related to semantic.</p>
<p>There is also evidence that infants extract and store possible word-forms (<xref ref-type="bibr" rid="c37">Jusczyk and Hohne, 1997</xref>). A stronger fMRI activation for forward speech than backward speech in the left angular gyrus in 3-mo-olds was assumed to be related to activations of possible word forms in a proto-lexicon for native language sentences (<xref ref-type="bibr" rid="c15">Dehaene-Lambertz et al., 2002</xref>). As elegantly shown by (<xref ref-type="bibr" rid="c57">Shukla et al., 2011</xref>), these chunks extracted from the speech stream are candidate words to which meanings can be attached. These authors show that 6-month-olds spontaneously associate a non-word extracted from natural sentences with a visual object. Although in their experiment, infants used prosodic cues to extract the word, and here statistical cues, a spontaneous bias to use possible word-forms as referring to a meaning (see also (<xref ref-type="bibr" rid="c7">Bergelson and Aslin, 2017</xref>) might trigger activation along a lexicon pathway and explain the difference seen here between the two groups. Only speech chunks based on phonetic regularities, and not voice regularities, would be considered as possible candidate.</p>
<p>A similar interpretation of an N400 induced by possible words, even without a clear semantic, explains the observation of an N400 in adult participants listening to artificial languages. Sanders et al. (2002) observed an N400 in adults listening to an artificial language only when they were previously exposed to the isolated pseudo-words. Other studies reported larger N400 amplitudes when adult participants listened to a structured stream compared to a random sequence of syllables (<xref ref-type="bibr" rid="c13">Cunillera et al., 2009</xref>, <xref ref-type="bibr" rid="c14">2006</xref>), tones (<xref ref-type="bibr" rid="c1">Abla et al., 2008</xref>), and shapes (<xref ref-type="bibr" rid="c2">Abla and Okanoya, 2009</xref>). Our results show an N400 for both Words and Part-words in the post-learning phase, possibly related to a top-down effect induced by the familiarisation stream. Since computing ERPs during the streams has inherent baseline issues that become critical with young infants’ EEG recordings due to the large amplitude of the slow waves at this age (<xref ref-type="bibr" rid="c21">Eisermann et al., 2013</xref>), we cannot perform the ERP analysis during the stream as in previous adult studies preventing a direct comparison. However, the component we observed for duplets presented after the familiarisation streams might result from a related phenomenon. Learning the phonetic structure, but not the voice sequence, might induce a lexical search during the presentation of the isolated duplets, as it happens in the Structured stream vs. Random streams in the previously mentioned adult studies (<xref ref-type="bibr" rid="c1">Abla et al., 2008</xref>; <xref ref-type="bibr" rid="c2">Abla and Okanoya, 2009</xref>; <xref ref-type="bibr" rid="c13">Cunillera et al., 2009</xref>, <xref ref-type="bibr" rid="c14">2006</xref>). A lexical entry might also explain the more sustained activity during the familiarisation stream in the phonemes group, as the chunk might be encoded as a putative word in this admittedly rudimentary but present lexical store, and the neural entrainment reflects not only the TP but also the recovery of the “lexical” item.</p>
<p>Finally, we would like to point out that it is not natural for a word not to be produced by the same speaker, nor for speakers to have statistical relationships of the kind we used here. Neonates, who have little experience and therefore no (or few) expectations or constraints, are probably better revealers of the possibilities opened by statistical learning than older participants. In fact, adults obtained better results for phoneme structure than for voice structure, perhaps because of an effective auditory normalisation process or the use of a writing code for phonemes but not for voices. It is also possible that the difference between neonates and adults is related to the behavioural test being a more explicit measure of word recognition than the implicit task allowed by EEG recordings. In any case, results show that even adults displayed some learning on the voice duplets.</p>
<p>Altogether, our results show that statistical learning works similarly on different speech features with no clear advantage for computing linguistically relevant regularities in speech. This supports the idea that statistical learning is a general and evolutionarily ancient learning mechanism, probably operating on common computational principles but within different neural networks with a different chain of operations: phonetic regularities induce a supplementary component - not seen in the case of voice regularities-that we related to a lexical N400. Understanding how statistical learning computations over linguistically relevant dimensions, such as the phonetic content of speech, are extracted and passed to other networks might be fundamental to understanding what enables the infant brain to acquire language. Further work is needed to understand how extracting regularities over different features articulates the language network.</p>
</sec>
</sec>
<sec id="d1e842" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e856">
<label>Supplemental Information</label>
<media xlink:href="supplements/605295_file02.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We want to thank all the families who participated in the study. This research has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (grant agreement No. 695710).</p>
</ack>
<sec id="s4">
<title>Author contributions</title>
<p>A.F. and G.D.L. conceptualised the research; A.F., L.B. and M.P. performed the research; A.F. analysed the data; and A.F., L.B., and G.D.L. wrote the paper</p>
</sec>
<sec id="s5">
<title>Declaration of interests</title>
<p>The authors declare no competing interests.</p>
</sec>
<sec id="s6">
<title>Methods</title>
<sec id="s6a">
<title>Participants</title>
<p>Participants were healthy-full-term neonates with normal pregnancy and birth (GA &gt; 38 weeks, Apgar scores ≥ 7/8 at 1/5 minute, birthweight &gt; 2.5 Kg, cranial perimeter ≥ 33.0 cm), tested at the Port Royal Maternity (AP-HP), in Paris, France. Parents provided informed consent. The regional ethical committee for biomedical research (Comité de Protection des Personnes Region Centre Ouest 1, EudraCT/ID RCB: 2017-A00513-50) approved the protocol, and the study was carried out according to relevant guidelines and regulations. 67 participants (34 in Experiment 1 and 33 in Experiment 2) who provided enough data without motion artefacts were included (Experiment 1: 19 females; 1 to 4 days old; mean GA: 39.3 weeks; mean weight: 3387 g; Experiment 2: 15 females; 1 to 4 days old; mean GA: 39.0 weeks; mean weight: 3363 g). 12 other infants were excluded from the analyses (11 due to fussiness; 1 due to bad data quality).</p>
</sec>
<sec id="s6b">
<title>Stimuli</title>
<p>The stimuli were synthesised using the MBROLA diphone database (<xref ref-type="bibr" rid="c20">Dutoit et al., 1996</xref>). Syllables had a consonant-vowel structure and lasted 250 ms (consonants 90 ms, vowels 160 ms). Six different syllables (<italic>ki</italic>, <italic>da</italic>, <italic>pe</italic>, <italic>tu</italic>, <italic>bo</italic>, <italic>gɛ</italic>) and six different voices were used (<italic>fr3</italic>, <italic>fr1</italic>, <italic>fr7</italic>, <italic>fr2</italic>, <italic>it4</italic>, <italic>fr4</italic>), resulting in a total of 36 syllable-voice combinations, from now on, tokens. The voices could be female or male and have three different pitch levels (low, middle, and high) (Table S1). The 36 tokens were synthesised independently in MBROLA, their intensity was normalised, and the first and last 5 ms were ramped to zero to avoid “clicks.” The streams were synthesised by concatenating the tokens’ audio files, and they were ramped up and down during the first and last 5 s to avoid the start and end of the stream serving as perceptual anchors.</p>
<p>The Structured streams were created by concatenating the tokens in such a way that they resulted in a semi-random concatenation of the duplets (i.e., pseudo-words) formed by one of the features (syllable/voice) while the other feature (voice/syllable) vary semi-randomly. In other words, in Experiment 1, the order of the tokens was such that Transitional Probabilities (TPs) between syllables alternated between 1 (within duplets) and 0.5 (between duplets), while between voices, TPs were uniformly 0.2. The design was orthogonal for the Structured streams of Experiment 2 (i.e., TPs between voices alternated between 1 and 0.5, while between syllables were evenly 0.2). The random streams were created by semi-randomly concatenating the 36 tokens to achieve uniform TPs equal to 0.2 over both features. The semi-random concatenation implied that the same element could not appear twice in a row, and the same two elements could not repeatedly alternate more than two times (i.e., the sequence <italic>X<sub>k</sub>X<sub>j</sub>X<sub>k</sub>X<sub>j</sub></italic>, where <italic>X<sub>k</sub></italic> and <italic>X<sub>j</sub></italic> are two elements, was forbidden). Notice that with an element, we refer to a duplet when it concerns the choice of the structured feature and to the identity of the second feature when it involves the other feature. The same statistical structures were used for both Experiments, only changing over which dimension the structure was applied. The learning stream lasted 120 seconds, with each duplet appearing 80 times. The 10 short structured streams lasted 30 seconds each, each duplet appearing a total of 200 times (10 × 20). The same random stream was used for both Experiments, and it lasted 120 seconds.</p>
<p>In Experiment 1, the duplets were created to prevent specific phonetic features from facilitating stream segmentation. In each experiment, two different structured streams (lists A and B) were used by modifying how the syllables/voices were combined to form the duplets (Table S2). Crucially, the Words/duplets of list A are the Part-words of list B and vice versa any difference between those two conditions can thus not be caused by acoustical differences. Participants were randomly assigned and balanced between lists and Experiments.</p>
<p>The test words were duplets formed by the concatenation of two tokens, such that they formed a Word or a Part-word according to the structured feature.</p>
</sec>
<sec id="s6c">
<title>Procedure and data acquisition</title>
<p>Scalp electrophysiological activity was recorded using a 128-electrode net (Electrical Geodesics, Inc.) referred to the vertex with a sampling frequency of 250 Hz. Neonates were tested in a soundproof booth while sleeping or during quiet rest. The study involved: (1) 120 s of a random stream, (2) 120 s of a structured stream, (3) 10 series of 30 s of structured streams followed by 18 test sequences (SOA 2-2.3s).</p>
</sec>
<sec id="s6d">
<title>Data pre-processing</title>
<p>Data were band-pass filtered 0.1-40 Hz and pre-processed using custom MATLAB scripts based on the EEGLAB toolbox 2021.0 according to the APICE pre-processing pipeline to recover as much free-artifacts data as possible (<xref ref-type="bibr" rid="c26">Fló et al., 2022</xref>).</p>
</sec>
<sec id="s6e">
<title>Neural entrainment</title>
<p>The pre-processed data were further high-pass filtered at 0.2 Hz. Then, data was segmented from the beginning of each phase into 0.5 s long segments (240 duplets for the Random, 240 duplets for the long Structured, and 600 duplets for the short Structured). Segments containing samples with artefacts defined as bad data in more than 30% of the channels were rejected, and the remaining channels with artefacts were spatially interpolated.</p>
<sec id="s6e1">
<title>Neural entrainment per condition</title>
<p>The 0.5 s epochs belonging to the same condition were reshaped into non-overlapping epochs (<xref ref-type="bibr" rid="c5">Benjamin et al., 2021</xref>) of 7.5 s (15 duplets, 30 syllables), retaining the chronological order; thus, the timing of the steady-state response. Subjects who did not provide at least 50% artifact-free epochs for each condition (at least 8 long epochs during Random and 28 during Structured) were excluded from the entrainment analysis (32 included subjects in Experiment 1, and 32 included subjects in Experiment 2). The retained subjects for Experiment 1, on average provided 13.59 epochs for the Random condition (SD 2.07, range [8, 16]) and 48.16 for the Structured conditions (SD 5.89, range [33, 55]). The retained subjects for Experiment 2, on average provided 13.78 epochs for the Random condition (SD 1.93, range [8, 16]) and 46.88 for the Structured conditions (SD 5.62, range [36, 55]). After data rejection, data were referenced to the average and normalized by dividing by the standard deviation within an epoch across electrodes and time. Next, data were converted to the frequency domain using the Fast Fourier Transform (FFT) algorithm, and the ITC was estimated for each electrode during each condition (Random, Structured) as <inline-formula><inline-graphic xlink:href="605295v1_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where N is the number of trials and φ(f,i) is the phase at frequency f and trial i. The ITC ranges from 0 to 1 (i.e., completely desynchronized activity to perfectly phased locked activity). Since we aim to detect an increase in signal synchronization at specific frequencies, the SNR was computed relative to the twelve adjacent frequency bins (six of each side corresponding to 0.8 Hz) (<xref ref-type="bibr" rid="c38">Kabdebon et al., 2022</xref>). This procedure also enables correcting differences in the ITC due to a different number of trials. Specifically, the SNR was <italic>SNR(f) = ( ITC(f)-mean(ITC<sub>noise</sub>(f)) )/std(ITC<sub>noise</sub>(f))</italic>, where <italic>ITC<sub>noise</sub>(f)</italic> is the ITC over the adjacent frequency bins. For statistical analysis, we compared the SNR at syllable rate (4 Hz) and duplet rate (2 Hz) against the average SNR over the 12 adjacent frequency bins using a one-tail paired permutation test (5000 permutations). We also directly compared the entrainment during the two conditions to individuate the electrodes showing a greater entrainment during the Structured than Random streams. We evaluated the interaction between Stream type (Random and Structured) and Familiarization type (Structured over Phonemes or Voices) by comparing the difference in entrainment between Structured and Random during the two experiments using a two sides unpaired permutation test (5000 permutations). All P-values were corrected across electrodes by FDR.</p>
</sec>
<sec id="s6e2">
<title>Neural entrainment time course</title>
<p>The 0.5 s epochs were concatenated chronologically (2 minutes of Random, 2 minutes of long Structured stream, and 5 minutes of short Structured blocks). The same analysis as above was performed in sliding time windows of 2 minutes with a 1 s step. A time window was considered valid if at least 8 out of the 16 epochs were free of motion artefacts. Missing values due to the presence of motion artifacts where linearly interpolated. Then, the entrainment time course at the syllable rate was computed as the average over the electrodes showing significant entrainment at 4 Hz during the Random condition, and at the duplet rate, as the average over the electrodes showing significant entrainment at 2 Hz during the Structured condition. Finally, data was smooth over a time window of 30 s.</p>
<p>To investigate the increase in the neural activity locked to the regularity during the long familiarisation, we fitted a LMM for each group of subjects. We included time as a fixed effect and random slopes and interceptions for individual subjects: <italic>ITC</italic> ∼ − 1 + <italic>time</italic> + (1 + <italic>time</italic>|<italic>subject</italic>). We then compare the time effect between groups by including all data in a single LMM with time and group as fixed effects: <italic>ITC</italic> ∼ − 1 + <italic>time</italic> ∗ <italic>group</italic> + (1 + <italic>time</italic>|<italic>subject</italic>).</p>
</sec>
</sec>
<sec id="s6f">
<title>ERPs to test words</title>
<p>The pre-processed data were filtered between 0.2 and 20 Hz, and epoched between [-0.2, 2.0] s from the onset of the duplets. Epochs containing samples identified as artifacts by APICE procedure were rejected. Subjects who did not provide at least half of the trials (45 trials) per condition were excluded (34 subjects kept for Experiment 1, and 33 for Experiment 2). None subjects were excluded based on this criteria in the Phonemes groups, and one subject was excluded in the Voice groups. For Experiment 1, we retained on average 77.47 trials (SD 9.98, range [52, 89]) for the Word condition and 77.12 trials (SD 10.04, range [56, 89]) for the Partword condition. For Experiment 2, we retained on average 73.73 trials (SD 10.57, range [47, 90]) for the Word condition and 74.18 trials (SD 11.15, range [46, 90]) for the Partword condition. Data were reference averaged and normalised within each epoch by dividing by the standard deviation across electrodes and time.</p>
<p>Since the gran average response across both groups and conditions returned to the pre-stimulus level at around 1500 ms, we defined [0, 1500] ms as time windows of analysis. We first analysed the data using non-parametric cluster-based permutation analysis (<xref ref-type="bibr" rid="c47">Oostenveld et al., 2011</xref>) in the time window [0, 1500] ms (alpha threshold for clustering 0.10, neighbour distance ≤ 2.5 cm, clusters minimum size 3 and 5,000 permutations).</p>
<p>We also analysed the data in seven ROIs to ensure that no other effects were present that were not caught by the cluster-based permutation analysis. By inspecting the grand average ERP across both Experiments and conditions, we identified three characteristic topographies: (a) positivity over central electrodes, (b) positivity over frontal electrodes and negativity over occipital electrodes, and (c) positivity over prefrontal electrodes and negativity over temporal electrodes (Figure S1). Then, we defined 7 symmetric ROIs: Central, Frontal Left, Frontal Right, Occipital, Prefrontal, Temporal Left, Temporal Right. We evaluated the main effect of Test-word type by comparing the EPRs between Words and Partwords (paired t-test) and the main effect of Familiarization type by comparing ERPs between Experiment 1 (structured over Phonemes) and Experiment 2 (structure over Voices) (unpaired t-test). All p-values were FDR corrected by the number of time points (n = 376) and ROIs (n = 7). To test for possible interaction effects, we compared the difference between Words and Partwords between the two groups. To verify that the main effects were not driven by one condition or group, we computed the average on each of the time windows where a main effect was identified considering both the Test-word type and Familiarization type factors, and we ran a two ways-ANOVA (Test-word type x Familiarization type). Results are presented in SI (Figure S3).</p>
</sec>
<sec id="s6g">
<title>Adults’ behavioural experiment</title>
<p>57 French-speaking adults were tested in an online experiment analogous to the infant study through the Prolific platform. All participants provided informed consent and received monetary compensation for their participation. The study was approved by the Ethical Research Committee of Paris Saclay University under the reference CER-Paris-Saclay-2019-063. The same stimuli as in the infants’ experiment were used. Participants first heard 2 min of familiarisation with the Structured stream. Then, they completed ten sessions of re-familiarisation and testing. Each re-familiarization lasted 30 s, and in each test session, all 18 test words were presented. The structure could be either over the phonetic or the voice content, and two lists were used (see Table S2). Participants were randomly assigned to one of the groups and to one list. The Phonemes group included 27 participants, and the Voices group 30 participants. Before starting the experiment, subjects were instructed to pay attention to an invented language because later, they would have to answer if different sequences adhered to the structure of the language. During the test phase, subjects were asked to scale their familiarity with each test word by clicking with a cursor on a scale from 1 to 6.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abla</surname> <given-names>D</given-names></string-name>, <string-name><surname>Katahira</surname> <given-names>K</given-names></string-name>, <string-name><surname>Okanoya</surname> <given-names>K</given-names></string-name></person-group>. <year>2008</year>. <article-title>On-line assessment of statistical learning by event-related potentials</article-title>. <source>Journal of Cognitive Neuroscience</source> <volume>20</volume>:<fpage>952</fpage>–<lpage>64</lpage>. doi:<pub-id pub-id-type="doi">10.1162/jocn.2008.20058</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abla</surname> <given-names>D</given-names></string-name>, <string-name><surname>Okanoya</surname> <given-names>K</given-names></string-name></person-group>. <year>2009</year>. <article-title>Visual statistical learning of shape sequences: An ERP study</article-title>. <source>Neuroscience Research</source> <volume>64</volume>:<fpage>185</fpage>–<lpage>190</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neures.2009.02.013</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baldwin</surname> <given-names>D</given-names></string-name>, <string-name><surname>Andersson</surname> <given-names>A</given-names></string-name>, <string-name><surname>Saffran</surname> <given-names>J</given-names></string-name>, <string-name><surname>Meyer</surname> <given-names>M</given-names></string-name></person-group>. <year>2008</year>. <article-title>Segmenting dynamic human action via statistical structure</article-title>. <source>Cognition</source> <volume>106</volume>:<fpage>1382</fpage>–<lpage>1407</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cognition.2007.07.005</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Belin</surname> <given-names>P</given-names></string-name>, <string-name><surname>Zatorre</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Lafaille</surname> <given-names>P</given-names></string-name>, <string-name><surname>Ahad</surname> <given-names>P</given-names></string-name>, <string-name><surname>Pike</surname> <given-names>B.</given-names></string-name></person-group> <year>2000</year>. <article-title>Voice-selective areas in human auditory cortex 403</article-title>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benjamin</surname> <given-names>L</given-names></string-name>, <string-name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></string-name>, <string-name><surname>Fló</surname> <given-names>A</given-names></string-name></person-group>. <year>2021</year>. <article-title>Remarks on the analysis of steady-state responses: spurious artifacts introduced by overlapping epochs</article-title>. <source>Cortex</source>. doi:<pub-id pub-id-type="doi">10.1016/j.cortex.2021.05.023</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benjamin</surname> <given-names>L</given-names></string-name>, <string-name><surname>Fló</surname> <given-names>A</given-names></string-name>, <string-name><surname>Palu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Naik</surname> <given-names>S</given-names></string-name>, <string-name><surname>Melloni</surname> <given-names>L</given-names></string-name>, <string-name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></string-name></person-group>. <year>2022</year>. <article-title>Tracking transitional probabilities and segmenting auditory sequences are dissociable processes in adults and neonates</article-title>. <source>Developmental Science</source> <volume>n/a</volume>:<fpage>e13300</fpage>. doi:<pub-id pub-id-type="doi">10.1111/desc.13300</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bergelson</surname> <given-names>E</given-names></string-name>, <string-name><surname>Aslin</surname> <given-names>RN</given-names></string-name></person-group>. <year>2017</year>. <article-title>Nature and origins of the lexicon in 6-mo-olds</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>114</volume>:<fpage>12916</fpage>–<lpage>12921</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1712966114</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blasi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mercure</surname> <given-names>E</given-names></string-name>, <string-name><surname>Lloyd-Fox</surname> <given-names>S</given-names></string-name>, <string-name><surname>Thomson</surname> <given-names>A</given-names></string-name>, <string-name><surname>Brammer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sauter</surname> <given-names>D</given-names></string-name>, <string-name><surname>Deeley</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Barker</surname> <given-names>GJ</given-names></string-name>, <string-name><surname>Renvall</surname> <given-names>V</given-names></string-name>, <string-name><surname>Deoni</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gasston</surname> <given-names>D</given-names></string-name>, <string-name><surname>Williams</surname> <given-names>SCR</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>MH</given-names></string-name>, <string-name><surname>Simmons</surname> <given-names>A</given-names></string-name>, <string-name><surname>Murphy</surname> <given-names>DGM</given-names></string-name></person-group>. <year>2011</year>. <article-title>Early Specialization for Voice and Emotion Processing in the Infant Brain</article-title>. <source>Current Biology</source> <volume>21</volume>:<fpage>1220</fpage>–<lpage>1224</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2011.06.009</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boemio</surname> <given-names>A</given-names></string-name>, <string-name><surname>Fromm</surname> <given-names>S</given-names></string-name>, <string-name><surname>Braun</surname> <given-names>A</given-names></string-name>, <string-name><surname>Poeppel</surname> <given-names>D</given-names></string-name></person-group>. <year>2005</year>. <article-title>Hierarchical and asymmetric temporal sensitivity in human auditory cortices</article-title>. <source>Nat Neurosci</source> <volume>8</volume>:<fpage>389</fpage>–<lpage>395</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn1409</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boros</surname> <given-names>M</given-names></string-name>, <string-name><surname>Magyari</surname> <given-names>L</given-names></string-name>, <string-name><surname>Török</surname> <given-names>D</given-names></string-name>, <string-name><surname>Bozsik</surname> <given-names>A</given-names></string-name>, <string-name><surname>Deme</surname> <given-names>A</given-names></string-name>, <string-name><surname>Andics</surname> <given-names>A</given-names></string-name></person-group>. <year>2021</year>. <article-title>Neural processes underlying statistical learning for speech segmentation in dogs</article-title>. <source>Current Biology</source>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2021.10.017</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buiatti</surname> <given-names>M</given-names></string-name>, <string-name><surname>Peña</surname> <given-names>M</given-names></string-name>, <string-name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></string-name></person-group>. <year>2009</year>. <article-title>Investigating the neural correlates of continuous speech computation with frequency-tagged neuroelectric responses</article-title>. <source>NeuroImage</source> <volume>44</volume>:<fpage>509</fpage>–<lpage>519</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.09.015</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bulf</surname> <given-names>H</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Valenza</surname> <given-names>E</given-names></string-name></person-group>. <year>2011</year>. <article-title>Visual statistical learning in the newborn infant</article-title>. <source>Cognition</source> <volume>121</volume>:<fpage>127</fpage>–<lpage>132</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cognition.2011.06.010</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cunillera</surname> <given-names>T</given-names></string-name>, <string-name><surname>Càmara</surname> <given-names>E</given-names></string-name>, <string-name><surname>Toro</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Marco-Pallares</surname> <given-names>J</given-names></string-name>, <string-name><surname>Sebastián-Galles</surname> <given-names>N</given-names></string-name>, <string-name><surname>Ortiz</surname> <given-names>H</given-names></string-name>, <string-name><surname>Pujol</surname> <given-names>J</given-names></string-name>, <string-name><surname>Rodríguez-Fornells</surname> <given-names>A</given-names></string-name></person-group>. <year>2009</year>. <article-title>Time course and functional neuroanatomy of speech segmentation in adults</article-title>. <source>NeuroImage</source> <volume>48</volume>:<fpage>541</fpage>–<lpage>553</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.069</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cunillera</surname> <given-names>T</given-names></string-name>, <string-name><surname>Toro</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Sebastián-Galles</surname> <given-names>N</given-names></string-name>, <string-name><surname>Rodríguez-Fornells</surname> <given-names>A.</given-names></string-name></person-group> <year>2006</year>. <article-title>The effects of stress and statistical cues on continuous speech segmentation: An event-related brain potential study</article-title>. <source>Brain Research</source> <volume>1123</volume>:<fpage>168</fpage>–<lpage>178</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.brainres.2006.09.046</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></string-name>, <string-name><surname>Dehaene</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hertz-Pannier</surname> <given-names>L</given-names></string-name></person-group>. <year>2002</year>. <article-title>Functional Neuroimaging of Speech Perception in Infants</article-title>. <source>Science</source> <volume>298</volume>:<fpage>2013</fpage>–<lpage>2015</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1077066</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></string-name>, <string-name><surname>Montavont</surname> <given-names>A</given-names></string-name>, <string-name><surname>Jobert</surname> <given-names>A</given-names></string-name>, <string-name><surname>Allirol</surname> <given-names>L</given-names></string-name>, <string-name><surname>Dubois</surname> <given-names>J</given-names></string-name>, <string-name><surname>Hertz-Pannier</surname> <given-names>L</given-names></string-name>, <string-name><surname>Dehaene</surname> <given-names>S</given-names></string-name></person-group>. <year>2010</year>. <article-title>Language or music, mother or Mozart? Structural and environmental influences on infants’ language networks</article-title>. <source>Brain and Language</source> <volume>114</volume>:<fpage>53</fpage>–<lpage>65</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.bandl.2009.09.003</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></string-name>, <string-name><surname>Pena</surname> <given-names>M</given-names></string-name></person-group>. <year>2001</year>. <article-title>Electrophysiological evidence for automatic phonetic processing in neonates</article-title>: <source>Neuroreport</source> <volume>12</volume>:<fpage>3155</fpage>–<lpage>3158</lpage>. doi:<pub-id pub-id-type="doi">10.1097/00001756-200110080-00034</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Delorme</surname> <given-names>A</given-names></string-name>, <string-name><surname>Makeig</surname> <given-names>S</given-names></string-name></person-group>. <year>2004</year>. <article-title>EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title>. <source>Journal of Neuroscience Methods</source> <volume>134</volume>:<fpage>9</fpage>–<lpage>21</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DeWitt</surname> <given-names>I</given-names></string-name>, <string-name><surname>Rauschecker</surname> <given-names>JP</given-names></string-name></person-group>. <year>2012</year>. <article-title>Phoneme and word recognition in the auditory ventral stream</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>109</volume>:<fpage>E505</fpage>–<lpage>514</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1113427109</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Dutoit</surname> <given-names>T</given-names></string-name>, <string-name><surname>Pagel</surname> <given-names>V</given-names></string-name>, <string-name><surname>Pierret</surname> <given-names>N</given-names></string-name>, <string-name><surname>Bataille</surname> <given-names>F</given-names></string-name>, <string-name><surname>van der Vrecken</surname> <given-names>O.</given-names></string-name></person-group> <year>1996</year>. <chapter-title>The MBROLA project: towards a set of high quality speech synthesizers free of use for non commercial purposesProceeding of Fourth International Conference on Spoken Language Processing</chapter-title>. <source>ICSLP ’96</source>. <publisher-name>IEEE</publisher-name>. pp. <fpage>1393</fpage>–<lpage>1396</lpage>. doi:<pub-id pub-id-type="doi">10.1109/ICSLP.1996.607874</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eisermann</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kaminska</surname> <given-names>A</given-names></string-name>, <string-name><surname>Moutard</surname> <given-names>M-L</given-names></string-name>, <string-name><surname>Soufflet</surname> <given-names>C</given-names></string-name>, <string-name><surname>Plouin</surname> <given-names>P</given-names></string-name></person-group>. <year>2013</year>. <article-title>Normal EEG in childhood: From neonates to adolescents</article-title>. <source>Neurophysiologie Clinique/Clinical Neurophysiology</source> <volume>43</volume>:<fpage>35</fpage>–<lpage>65</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neucli.2012.09.091</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Estes</surname> <given-names>KG</given-names></string-name>, <string-name><surname>Lew-Williams</surname> <given-names>C</given-names></string-name></person-group>. <year>2015</year>. <article-title>Listening through voices: Infant statistical word segmentation across multiple speakers</article-title>. <source>Dev Psychol</source> <volume>51</volume>:<fpage>1517</fpage>–<lpage>1528</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0039725</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fiser</surname> <given-names>J</given-names></string-name>, <string-name><surname>Aslin</surname> <given-names>RN</given-names></string-name></person-group>. <year>2002</year>. <article-title>Statistical learning of new visual feature combinations by infants</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>99</volume>:<fpage>15822</fpage>–<lpage>6</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.232472899</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Flo</surname> <given-names>A</given-names></string-name>, <string-name><surname>Benjamin</surname> <given-names>L</given-names></string-name>, <string-name><surname>Palu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></string-name></person-group>. <year>2022</year>. <article-title>Sleeping neonates track transitional probabilities in speech but only retain the first syllable of words</article-title>. <source>Sci Rep</source> <volume>12</volume>:<fpage>4391</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41598-022-08411-w</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fló</surname> <given-names>A</given-names></string-name>, <string-name><surname>Brusini</surname> <given-names>P</given-names></string-name>, <string-name><surname>Macagno</surname> <given-names>F</given-names></string-name>, <string-name><surname>Nespor</surname> <given-names>M</given-names></string-name>, <string-name><surname>Mehler</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ferry</surname> <given-names>AL</given-names></string-name></person-group>. <year>2019</year>. <article-title>Newborns are sensitive to multiple cues for word segmentation in continuous speech</article-title>. <source>Dev Sci</source> <volume>22</volume>:<fpage>e12802</fpage>. doi:<pub-id pub-id-type="doi">10.1111/desc.12802</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Fló</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gennari</surname> <given-names>G</given-names></string-name>, <string-name><surname>Benjamin</surname> <given-names>L</given-names></string-name>, <string-name><surname>Dehane-Lambertz</surname> <given-names>G</given-names></string-name></person-group>. <year>2022</year>. <chapter-title>Automated Pipeline for Infants Continuous EEG (APICE): A flexible pipeline for developmental cognitive studies</chapter-title> | <publisher-name>Elsevier Enhanced Reader</publisher-name>. <source>Developmental Cognitive Neuroscience</source>. doi:<pub-id pub-id-type="doi">10.1016/j.dcn.2022.101077</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedrich</surname> <given-names>M</given-names></string-name>, <string-name><surname>Friederici</surname> <given-names>AD</given-names></string-name></person-group>. <year>2011</year>. <article-title>Word Learning in 6-Month-Olds: Fast Encoding–Weak Retention</article-title>. <source>Journal of Cognitive Neuroscience</source> <volume>23</volume>:<fpage>3228</fpage>–<lpage>3240</lpage>. doi:<pub-id pub-id-type="doi">10.1162/jocn_a_00002</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedrich</surname> <given-names>M</given-names></string-name>, <string-name><surname>Friederici</surname> <given-names>AD</given-names></string-name></person-group>. <year>2005</year>. <article-title>Semantic sentence processing reflected in the event-related potentials of one- and two-year-old children</article-title>. <source>NeuroReport</source> <volume>16</volume>:<fpage>1801</fpage>. doi:<pub-id pub-id-type="doi">10.1097/01.wnr.0000185013.98821.62</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frost</surname> <given-names>R</given-names></string-name>, <string-name><surname>Armstrong</surname> <given-names>BC</given-names></string-name>, <string-name><surname>Siegelman</surname> <given-names>N</given-names></string-name>, <string-name><surname>Christiansen</surname> <given-names>MH</given-names></string-name></person-group>. <year>2015</year>. <article-title>Domain generality versus modality specificity: The paradox of statistical learning</article-title>. <source>Trends in Cognitive Sciences</source> <volume>19</volume>:<fpage>117</fpage>–<lpage>125</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.tics.2014.12.010</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gennari</surname> <given-names>G</given-names></string-name>, <string-name><surname>Marti</surname> <given-names>S</given-names></string-name>, <string-name><surname>Palu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Flo</surname> <given-names>A</given-names></string-name>, <string-name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></string-name></person-group>. <year>2021</year>. <article-title>Orthogonal neural codes for speech in the infant brain</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>118</volume>:<fpage>e2020410118</fpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.2020410118</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giraud</surname> <given-names>A-L</given-names></string-name>, <string-name><surname>Poeppel</surname> <given-names>D</given-names></string-name></person-group>. <year>2012</year>. <article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title>. <source>Nature neuroscience</source> <volume>15</volume>:<fpage>511</fpage>–<lpage>7</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.3063</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gwilliams</surname> <given-names>L</given-names></string-name>, <string-name><surname>King</surname> <given-names>J-R</given-names></string-name>, <string-name><surname>Marantz</surname> <given-names>A</given-names></string-name>, <string-name><surname>Poeppel</surname> <given-names>D</given-names></string-name></person-group>. <year>2022</year>. <article-title>Neural dynamics of phoneme sequences reveal position-invariant code for content and order</article-title>. <source>Nat Commun</source> <volume>13</volume>:<fpage>6606</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41467-022-34326-1</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hauser</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Newport</surname> <given-names>EL</given-names></string-name>, <string-name><surname>Aslin</surname> <given-names>RN</given-names></string-name></person-group>. <year>2001</year>. <article-title>Segmentation of the speech stream in a non-human primate: Statistical learning in cotton-top tamarins</article-title>. <source>Cognition</source> <volume>78</volume>:<fpage>53</fpage>–<lpage>64</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0010-0277(00)00132-3</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hay</surname> <given-names>JF</given-names></string-name>, <string-name><surname>Pelucchi</surname> <given-names>B</given-names></string-name>, <string-name><surname>Estes</surname> <given-names>KG</given-names></string-name>, <string-name><surname>Saffran</surname> <given-names>JR</given-names></string-name></person-group>. <year>2011</year>. <article-title>Linking sounds to meanings: Infant statistical learning in a natural language</article-title>. <source>Cognitive Psychology</source> <volume>63</volume>:<fpage>93</fpage>–<lpage>106</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cogpsych.2011.06.002</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Westrek</surname> <given-names>E</given-names></string-name>, <string-name><surname>Nazzi</surname> <given-names>T</given-names></string-name>, <string-name><surname>Cutler</surname> <given-names>A</given-names></string-name></person-group>. <year>2011</year>. <article-title>Infant ability to tell voices apart rests on language experience: Infant voice discernment</article-title>. <source>Developmental Science</source> <volume>14</volume>:<fpage>1002</fpage>–<lpage>1011</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1467-7687.2011.01052.x</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Junge</surname> <given-names>C</given-names></string-name>, <string-name><surname>Boumeester</surname> <given-names>M</given-names></string-name>, <string-name><surname>Mills</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Paul</surname> <given-names>M</given-names></string-name>, <string-name><surname>Cosper</surname> <given-names>SH</given-names></string-name></person-group>. <year>2021</year>. <article-title>Development of the N400 for Word Learning in the First 2 Years of Life: A Systematic Review</article-title>. <source>Frontiers in Psychology</source> <volume>12</volume>:<fpage>2420</fpage>. doi:<pub-id pub-id-type="doi">10.3389/fpsyg.2021.689534</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jusczyk</surname> <given-names>PW</given-names></string-name>, <string-name><surname>Hohne</surname> <given-names>EA</given-names></string-name></person-group>. <year>1997</year>. <article-title>Infants’ Memory for Spoken Words</article-title>. <source>Science</source> <volume>277</volume>:<fpage>1984</fpage>–<lpage>1986</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.277.5334.1984</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kabdebon</surname> <given-names>C</given-names></string-name>, <string-name><surname>Flo</surname> <given-names>A</given-names></string-name>, <string-name><surname>de Heering</surname> <given-names>A</given-names></string-name>, <string-name><surname>Aslin</surname> <given-names>R.</given-names></string-name></person-group> <year>2022</year>. <article-title>The power of rhythms: how steady-state evoked responses reveal early neurocognitive development</article-title>. <source>NeuroImage</source> <fpage>119150</fpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119150</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kirkham</surname> <given-names>NZ</given-names></string-name>, <string-name><surname>Slemmer</surname> <given-names>J a</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>SP.</given-names></string-name></person-group> <year>2002</year>. <article-title>Visual statistical learning in infancy: Evidence for a domain-general learning mechanism</article-title>. <source>Cognition</source> <volume>83</volume>:<fpage>4</fpage>–<lpage>5</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kudo</surname> <given-names>N</given-names></string-name>, <string-name><surname>Nonaka</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Mizuno</surname> <given-names>N</given-names></string-name>, <string-name><surname>Mizuno</surname> <given-names>K</given-names></string-name>, <string-name><surname>Okanoya</surname> <given-names>K</given-names></string-name></person-group>. <year>2011</year>. <article-title>On-line statistical segmentation of a non-speech auditory stream in neonates as demonstrated by event-related brain potentials</article-title>. <source>Developmental Science</source> <volume>14</volume>:<fpage>1100</fpage>–<lpage>1106</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1467-7687.2011.01056.x</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kuhl</surname> <given-names>PK</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>JD</given-names></string-name></person-group>. <year>1982</year>. <article-title>Discrimination of auditory target dimensions in the presence or absence of variation in a second dimension by infants</article-title>. <source>Perception &amp; Psychophysics</source> <volume>31</volume>:<fpage>279</fpage>–<lpage>292</lpage>. doi:<pub-id pub-id-type="doi">10.3758/BF03202536</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kutas</surname> <given-names>M</given-names></string-name>, <string-name><surname>Federmeier</surname> <given-names>KD</given-names></string-name></person-group>. <year>2011</year>. <article-title>Thirty years and counting: Finding meaning in the N400 component of the event related brain potential (ERP)</article-title>. <source>Annu Rev Psychol</source> <volume>62</volume>:<fpage>621</fpage>–<lpage>647</lpage>. doi:<pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mahmoudzadeh</surname> <given-names>M</given-names></string-name>, <string-name><surname>Wallois</surname> <given-names>F</given-names></string-name>, <string-name><surname>Kongolo</surname> <given-names>G</given-names></string-name>, <string-name><surname>Goudjil</surname> <given-names>S</given-names></string-name>, <string-name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></string-name></person-group>. <year>2016</year>. <article-title>Functional Maps at the Onset of Auditory Inputs in Very Early Preterm Human Neonates</article-title>. <source>Cereb Cortex</source> <volume>27</volume>:<fpage>2500</fpage>–<lpage>2512</lpage>. doi:<pub-id pub-id-type="doi">10.1093/cercor/bhw103</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moerel</surname> <given-names>M</given-names></string-name>, <string-name><surname>De Martino</surname> <given-names>F</given-names></string-name>, <string-name><surname>Formisano</surname> <given-names>E.</given-names></string-name></person-group> <year>2012</year>. <article-title>Processing of Natural Sounds in Human Auditory Cortex: Tonotopy, Spectral Tuning, and Relation to Voice Sensitivity</article-title>. <source>Journal of Neuroscience</source> <volume>32</volume>:<fpage>14205</fpage>–<lpage>14216</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.1388-12.2012</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Monroy</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Gerson</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Hunnius</surname> <given-names>S</given-names></string-name></person-group>. <year>2017</year>. <article-title>Toddlers’ action prediction: Statistical learning of continuous action sequences</article-title>. <source>Journal of Experimental Child Psychology</source> <volume>157</volume>:<fpage>14</fpage>–<lpage>28</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jecp.2016.12.004</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Norman-Haignere</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>NG</given-names></string-name>, <string-name><surname>McDermott</surname> <given-names>JH</given-names></string-name></person-group>. <year>2015</year>. <article-title>Distinct Cortical Pathways for Music and Speech Revealed by Hypothesis-Free Voxel Decomposition</article-title>. <source>Neuron</source> <volume>88</volume>:<fpage>1281</fpage>– <lpage>1296</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.035</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oostenveld</surname> <given-names>R</given-names></string-name>, <string-name><surname>Fries</surname> <given-names>P</given-names></string-name>, <string-name><surname>Maris</surname> <given-names>E</given-names></string-name>, <string-name><surname>Schoffelen</surname> <given-names>J-M</given-names></string-name></person-group>. <year>2011</year>. <article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title>. <source>Computational Intelligence and Neuroscience</source> <volume>2011</volume>:<fpage>1</fpage>–<lpage>9</lpage>. doi:<pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parise</surname> <given-names>E</given-names></string-name>, <string-name><surname>Friederici</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Striano</surname> <given-names>T</given-names></string-name></person-group>. <year>2010</year>. “<article-title>Did You Call Me?” 5-Month-Old Infants Own Name Guides Their Attention</article-title>. <source>PLoS ONE</source> <volume>5</volume>:<fpage>e14208</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0014208</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pelucchi</surname> <given-names>B</given-names></string-name>, <string-name><surname>Hay</surname> <given-names>JF</given-names></string-name>, <string-name><surname>Saffran</surname> <given-names>JR</given-names></string-name></person-group>. <year>2015</year>. <article-title>Statistical learning in a natural language by 8-month-old infants</article-title>. <source>Child development</source> <volume>80</volume>:<fpage>674</fpage>–<lpage>685</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1467-8624.2009.01290.x</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reid</surname> <given-names>VM</given-names></string-name>, <string-name><surname>Hoehl</surname> <given-names>S</given-names></string-name>, <string-name><surname>Grigutsch</surname> <given-names>M</given-names></string-name>, <string-name><surname>Groendahl</surname> <given-names>A</given-names></string-name>, <string-name><surname>Parise</surname> <given-names>E</given-names></string-name>, <string-name><surname>Striano</surname> <given-names>T</given-names></string-name></person-group>. <year>2009</year>. <article-title>The neural correlates of infant and adult goal prediction: Evidence for semantic processing systems</article-title>. <source>Developmental Psychology</source> <volume>45</volume>:<fpage>620</fpage>–<lpage>629</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0015209</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ren</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>M</given-names></string-name></person-group>. <year>2023</year>. <article-title>Development of statistical learning ability across modalities, domains, and languages</article-title>. <source>Journal of Experimental Child Psychology</source> <volume>226</volume>:<fpage>105570</fpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jecp.2022.105570</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Saffran</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Aslin</surname> <given-names>RN</given-names></string-name>, <string-name><surname>Newport</surname> <given-names>EL</given-names></string-name></person-group>. <year>1996</year>. <article-title>Statistical learning by 8-month-old infants</article-title>. doi:<pub-id pub-id-type="doi">10.1126/science.274.5294.1926</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saffran</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Aslin</surname> <given-names>RN</given-names></string-name>, <string-name><surname>Newport</surname> <given-names>EL</given-names></string-name></person-group>. <year>1999</year>. <article-title>Statistical learning of tone sequences by human infants and adults</article-title>. <source>Cognition</source> <volume>70</volume>:<fpage>27</fpage>–<lpage>52</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0010-0277(98)00075-4</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saffran</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Kirkham</surname> <given-names>NZ</given-names></string-name></person-group>. <year>2018</year>. <article-title>Infant Statistical Learning</article-title>. <source>Annual Review of Psychology</source> <volume>69</volume>:<fpage>181</fpage>–<lpage>203</lpage>. doi:<pub-id pub-id-type="doi">10.1146/annurev-psych-122216-011805</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Santolin</surname> <given-names>C</given-names></string-name>, <string-name><surname>Rosa-Salva</surname> <given-names>O</given-names></string-name>, <string-name><surname>Vallortigara</surname> <given-names>G</given-names></string-name>, <string-name><surname>Regolin</surname> <given-names>L</given-names></string-name></person-group>. <year>2016</year>. <article-title>Unsupervised statistical learning in newly hatched chicks</article-title>. <source>Current Biology</source> <volume>26</volume>:<fpage>R1218</fpage>–<lpage>R1220</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2016.10.011</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Santolin</surname> <given-names>C</given-names></string-name>, <string-name><surname>Saffran</surname> <given-names>JR</given-names></string-name></person-group>. <year>2018</year>. <article-title>Constraints on Statistical Learning Across Species</article-title>. <source>Trends in Cognitive Sciences</source> <volume>22</volume>:<fpage>52</fpage>–<lpage>63</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.tics.2017.10.003</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shukla</surname> <given-names>M</given-names></string-name>, <string-name><surname>White</surname> <given-names>KS</given-names></string-name>, <string-name><surname>Aslin</surname> <given-names>RN</given-names></string-name></person-group>. <year>2011</year>. <article-title>Prosody guides the rapid mapping of auditory word forms onto visual objects in 6-mo-old infants</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>108</volume>:<fpage>6038</fpage>–<lpage>6043</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1017617108</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shultz</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vouloumanos</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bennett</surname> <given-names>RH</given-names></string-name>, <string-name><surname>Pelphrey</surname> <given-names>K</given-names></string-name></person-group>. <year>2014</year>. <article-title>Neural specialization for speech in the first months of life</article-title>. <source>Developmental Science</source> <volume>17</volume>:<fpage>766</fpage>–<lpage>774</lpage>. doi:<pub-id pub-id-type="doi">10.1111/desc.12151</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sylvester</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Kaplan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Myers</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Gordon</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Schwarzlose</surname> <given-names>RF</given-names></string-name>, <string-name><surname>Alexopoulos</surname> <given-names>D</given-names></string-name>, <string-name><surname>Nielsen</surname> <given-names>AN</given-names></string-name>, <string-name><surname>Kenley</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Meyer</surname> <given-names>D</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Graham</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Fair</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Warner</surname> <given-names>BB</given-names></string-name>, <string-name><surname>Barch</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Rogers</surname> <given-names>CE</given-names></string-name>, <string-name><surname>Luby</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Petersen</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Smyser</surname> <given-names>CD</given-names></string-name></person-group>. <year>2023</year>. <article-title>Network-specific selectivity of functional connections in the neonatal brain</article-title>. <source>Cerebral Cortex</source> <volume>33</volume>:<fpage>2200</fpage>–<lpage>2214</lpage>. doi:<pub-id pub-id-type="doi">10.1093/cercor/bhac202</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Toro</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Trobalón</surname> <given-names>JB</given-names></string-name></person-group>. <year>2005</year>. <article-title>Statistical computations over a speech stream in a rodent</article-title>. <source>Perception &amp; psychophysics</source> <volume>67</volume>:<fpage>867</fpage>–<lpage>875</lpage>. doi:<pub-id pub-id-type="doi">10.3758/BF03193539</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zatorre</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Belin</surname> <given-names>P</given-names></string-name></person-group>. <year>2001</year>. <article-title>Spectral and Temporal Processing in Human Auditory Cortex</article-title>. <source>Cerebral Cortex</source>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101802.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Herrmann</surname>
<given-names>Björn</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Baycrest Hospital</institution>
</institution-wrap>
<city>Toronto</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>The manuscript provides <bold>important</bold> new insights into the mechanisms of statistical learning in early human development, showing that statistical learning in neonates occurs robustly and is not limited to linguistic features but occurs across different domains. The evidence is <bold>convincing</bold>, although an additional experimental manipulation with conflicting linguistic and non-linguistic information as well as further discussion about the linguistic vs non-linguistic nature of the stimulus materials would have strengthened the manuscript. The findings are highly relevant for researchers working in several domains, including developmental cognitive neuroscience, developmental psychology, linguistics, and speech pathology.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101802.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Parsing speech into meaningful linguistic units is a fundamental yet challenging task that infants face while acquiring the native language. Computing transitional probabilities (TPs) between syllables is a segmentation cue well-attested since birth. In this research, the authors examine whether newborns compute TPs over any available speech feature (linguistic and non-linguistic), or whether by contrast newborns' favor the computation of TPs over linguistic content over non-linguistic speech features such as speaker's voice. Using EEG and the artificial language learning paradigm, they record the neural responses of two groups of newborns presented with speech streams in which either phonetic content or speaker's voice are structured to provide TPs informative of word boundaries, while the other dimension provides uninformative information. They compare newborns' neural responses to these structured streams to their processing of a stream in which both dimensions vary randomly. After the random and structured familiarization streams, the newborns are presented with (pseudo)words as defined by their informative TPs, as well as partwords (that is, sequences that straddle a word boundary), extracted from the same streams. Analysis of the neural responses shows that while newborns neural activity entrained to the syllabic rate (2 Hz) when listening to the random and structured streams, it additionally entrained at the word rate (4 Hz) only when listening to the structured streams, finding no differential response between the streams structured around voice or phonetic information. Newborns showed also different neural activity in response to the words and part words. In sum, the study reveals that newborns compute TPs over linguistic and non-linguistic features of speech, these are calculated independently, and linguistic features do not lead to a processing advantage.</p>
<p>Strengths:</p>
<p>This interesting research furthers our knowledge of the scope of the statistical learning mechanism, which is confirmed to be a general-purpose powerful tool that allows humans to extract patterns of co-occurring events while revealing no apparent preferential processing for linguistic features. To answer its question, the study combines a highly replicated and well-established paradigm, i.e. the use of an artificial language in which pseudowords are concatenated to yield informative TPs to word boundaries, with a state-of-the-art EEG analysis, i.e. neural entrainment. The sample size of the groups is sufficient to ensure power, and the design and analysis are solid and have been successfully employed before.</p>
<p>Weaknesses:</p>
<p>There are no significant weaknesses to signal in the manuscript. However, in order to fully conclude that there is no obvious advantage for the linguistic dimension in neonates, it would have been most useful to test a third condition in which the two dimensions were pitted against each other, that is, in which they provide conflicting information as to the boundaries of the words comprised in the artificial language. This last condition would have allowed us to determine whether statistical learning weighs linguistic and non-linguistic features equally, or whether phonetic content is preferentially processed.</p>
<p>To sum up, the authors achieved their central aim of determining whether TPs are computed over both linguistic and non-linguistic features, and their conclusions are supported by the results. This research is important for researchers working on language and cognitive development, and language processing, as well as for those working on cross-species comparative approaches.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101802.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The manuscript investigates to what degree neonates show evidence for statistical learning from regularities in streams of syllables, either with respect to phonemes or with respect to speaker identity. Using EEG, the authors found evidence for both, stronger entrainment to regularities as well as ERP differences in response to violations of previously introduced regularities. In addition, violations of phoneme regularities elicited an ERP pattern which the authors argue might index a precursor of the N400 response in older children and adults.</p>
<p>Strengths:</p>
<p>All in all, this is a very convincing paper, which uses a clever manipulation of syllable streams to target the processing of different features. The combination of neural entrainment and ERP analysis allows for the assessment of different processing stages, and implementing this paradigm in a comparably large sample of neonates is impressive. I only have some smaller comments.</p>
<p>Weaknesses:</p>
<p>I am skeptical regarding the interpretation of the phoneme-specific ERP effect as a precursor of the N400 and would suggest toning it down. While the authors are correct in that infant ERP components are typically slower and more posterior compared to adult components, and the observed pattern is hence consistent with an adult N400, at the same time, it could also be a lot of other things. On a functional level, I can't follow the author's argument as to why a violation in phoneme regularity should elicit an N400, since there is no evidence for any semantic processing involved. In sum, I think there is just not enough evidence from the present paradigm to confidently call it an N400.</p>
<p>Why did the authors choose to include male and female voices? While using both female and male stimuli of course leads to a higher generalizability, it also introduces a second dimension for one feature that is not present for this other (i.e., phoneme for Experiment 1 and voice identity plus gender for Experiment 2). Hence, couldn't it also be that the infants extracted the regularity with which one gender voice followed the other? For instance, in List B, in the words, one gender is always followed by the other (M-F or F-M), while in 2/3 of the part-words, the gender is repeated (F-F and M-M). Wouldn't you expect the same pattern of results if infants learned regularities based on gender rather than identity?</p>
<p>Do you have any idea why the duplet entrainment effect occurs over the electrodes it does, in particular over the occipital electrodes (which seems a bit unintuitive given that this is a purely auditory experiment with sleeping neonates).</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101802.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study is focused on testing whether statistical learning (a mechanism for parsing the speech signal into smaller chunks) preferentially operates over certain features of the speech at birth in humans. The features under investigation are phonetic content and speaker identity. Newborns are tested in an EEG paradigm in which they are exposed to a long stream of syllables. In Experiment 1, newborns are familiarized with a sound stream that comprises regularities (transitional probabilities) over syllables (e.g., &quot;pe&quot; followed by &quot;tu&quot; in &quot;petu&quot; with 1.0 probability) while the voices uttering the syllables remain random. In Experiment 2, newborns are familiarized with the same sound stream but, this time, the regularities are built over voices (e.g., &quot;green voice&quot; followed by &quot;red voice&quot; with 1.0 probability) while the concatenation of syllables stays random. At the test, all newborns listened to duplets (individual chunks) that either matched or violated the structure of the familiarization. In both experiments, newborns showed neural entrainment to the regularities implemented in the stream, but only the duplets defined by transitional probabilities over syllables (aka word forms) elicited a N400 ERP component. These results suggest that statistical learning operates in parallel and independently on different dimensions of the speech already at birth and that there seems to be an advantage for processing statistics defining word forms rather than voice patterns.</p>
<p>Strengths:</p>
<p>This paper presents an original experimental design that combines two types of statistical regularities in a speech input. The design is robust and appropriate for EEG with newborns. I appreciated the clarity of the Methods section. There is also a behavioral experiment with adults that acts like a control study for newborns. The research question is interesting, and the results add new information about how statistical learning works at the beginning of postnatal life, and on which features of the speech. The figures are clear and helpful in understanding the methods, especially the stimuli and how the regularities were implemented.</p>
<p>Weaknesses:</p>
<p>(1) I'm having a hard time understanding the link between the results of the study and the universality of statistical learning. The main goal of the study was testing whether statistical learning is a general mechanism for newborns that operates on any speech dimension, or whether it operates over linguistic features only. To test that, statistical regularities (TPs) were built over syllables (e.g., pe followed by tu in petu with 1.0 probability) or voices (e.g., green voice followed by red voice with 1.0 probability). Voices were considered as the non-linguistic dimension.</p>
<p>While it's true that voice is not essential for language (i.e., sign languages are implemented over gestures; the use of voices to produce non-linguistic sounds, like laughter), it is a feature of spoken languages. Thus I'm not sure if we can really consider this study as a comparison between linguistic and non-linguistic dimensions. In turn, I'm not sure that these results show that statistical learning at birth operates on non-linguistic features, being voices a linguistic dimension at least in spoken languages. I'd like to hear the authors' opinions on this.</p>
<p>Along the same line, in the Discussion section, the present results are interpreted within a theoretical framework showing statistical learning in auditory non-linguistic (string of tones, music) and visual domains as well as visual and other animal species. I'm not sure if that theoretical framework is the right fit for the present results.</p>
<p>(2) I'm not sure whether the fact that we see parallel and independent tracking of statistics in the two dimensions of speech at birth indicates that newborns would be able to do so in all the other dimensions of the speech. If so, what other dimensions are the authors referring to?</p>
<p>(3) Lines 341-345: Statistical learning is an evolutionary ancient learning mechanism but I do not think that the present results are showing it. This is a study on human neonates and adults, there are no other animal species involved therefore I do not see a connection with the evolutionary history of statistical learning. It would be much more interesting to make claims on the ontogeny (rather than philogeny) of statistical learning, and what regularities newborns are able to detect right after birth. I believe that this is one of the strengths of this work.</p>
<p>(4) The description of the stimuli in Lines 110-113 is a bit confusing. In Experiment 1, e.g., &quot;pe&quot; and &quot;tu&quot; are both uttered by the same voice, correct? (&quot;random voice each time&quot; is confusing). Whereas in Experiment 2, e.g., &quot;pe&quot; and &quot;tu&quot; are uttered by different voices, for example, &quot;pe&quot; by yellow voice and &quot;tu&quot; by red voice. If this is correct, then I recommend the authors to rephrase this section to make it more clear.</p>
<p>(5) Line 114: the sentence &quot;they should compute a 36 x 36 TPs matrix relating each acoustic signal, with TPs alternating between 1/6 within words and 1/12 between words&quot; is confusing as it seems like there are different acoustic signals. Can the authors clarify this point?</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101802.1.sa4</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Fló</surname>
<given-names>Ana</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3260-0559</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Benjamin</surname>
<given-names>Lucas</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9578-6039</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Palu</surname>
<given-names>Marie</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dehaene-Lambertz</surname>
<given-names>Ghislaine</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2221-9081</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 1:</bold></p>
<p>There are no significant weaknesses to signal in the manuscript. However, in order to fully conclude that there is no obvious advantage for the linguistic dimension in neonates, it would have been most useful to test a third condition in which the two dimensions were pitted against each other, that is, in which they provide conflicting information as to the boundaries of the words comprised in the artificial language. This last condition would have allowed us to determine whether statistical learning weighs linguistic and non-linguistic features equally, or whether phonetic content is preferentially processed.</p>
</disp-quote>
<p>We appreciate the reviewers' suggestion that a stream with conflicting information would provide valuable insights. In the present study, we started with a simpler case involving two orthogonal features (i.e., phonemes and voices), with one feature being informative and the other uninformative, and we found similar learning capacities for both. Future work should explore whether infants—and humans more broadly—can simultaneously track regularities in multiple speech features. However, creating a stream with two conflicting statistical structures is challenging. To use neural entrainment, the two features must lead to segmentation at different chunk sizes so that their effects lead to changes in power/PLV at different frequencies—for instance, using duplets for the voice dimension and triplets for the linguistic dimension  (or vice versa). Consequently, the two dimensions would not be directly comparable within the same participant in terms of the number of distinguishable syllables/voices, memory demand, or SNR given the 1/F decrease in amplitude of background EEG activity. This would involve comparisons between two distinct groups counter-balancing chunk size and linguistic non-linguistic dimension. Considering the test phase, words for one dimension would have been part-words for the other dimension. As we are measuring differences and not preferences, interpreting the results would also have been difficult. Additionally, it may be difficult to find a sufficient number of clearly discriminable voices for such a design (triplets imply 12 voices). Therefore, an entirely different experimental paradigm would need to be developed.</p>
<p>If such a design were tested, one possibility is that the regularities for the two dimensions are calculated in parallel, in line with the idea that the calculation of statistical regularities is a ubiquitous implicit mechanism (see Benjamin et al., 2024, for a proposed neural mechanism). Yet, similar to our present study, possibly only phonetic features would be used as word candidates. Another possibility is that only one informative feature would be explicitly processed at a time due to the serial nature of perceptual awareness, which may prioritise one feature over the other.</p>
<p>Note: The reviewer’s summary contains a typo: syllabic rate (4 Hz) –not 2 Hz, and word rate (2 Hz) –not 4 Hz.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 2:</bold></p>
<p>N400: I am skeptical regarding the interpretation of the phoneme-specific ERP effect as a precursor of the N400 and would suggest toning it down. While the authors are correct in that infant ERP components are typically slower and more posterior compared to adult components, and the observed pattern is hence consistent with an adult N400, at the same time, it could also be a lot of other things. On a functional level, I can't follow the author's argument as to why a violation in phoneme regularity should elicit an N400, since there is no evidence for any semantic processing involved. In sum, I think there is just not enough evidence from the present paradigm to confidently call it an N400.</p>
</disp-quote>
<p>The reviewer is correct that we cannot definitively determine the type of processing reflected by the ERP component that appears when neonates hear a triplet after exposure to a stream with phonetic regularities. We interpreted this component as a precursor to the N400, based on prior findings in speech segmentation tasks without semantic content, where a ~400 ms component emerged when adult participants recognised pseudowords (Sander et al., 2002) or during structured streams of syllables (Cunillera et al., 2006, 2009). Additionally, the component we observed had a similar topography and timing to those labelled as N400 in infant studies, where semantic processing was involved (Parise et al., 2010; Friedrich &amp; Friederici, 2011).</p>
<p>Given our experimental design, the difference we observed must be related to the type of regularity during familiarisation (either phonemes or voices). Thus, we interpreted this component as reflecting lexical search— a process which could be triggered by a linguistic structure but which would not be relevant to a non-linguistic regularity such as voices. However, we are open to alternative interpretations. In any case, this difference between the two streams reveals that computing regularities based on phonemes versus voices does not lead to the same processes. We will revise and tone down the corresponding part of the discussion to clarify that it is just a possible interpretation of the results.</p>
<disp-quote content-type="editor-comment">
<p>Female and male voices: Why did the authors choose to include male and female voices? While using both female and male stimuli of course leads to a higher generalizability, it also introduces a second dimension for one feature that is not present for this other (i.e., phoneme for Experiment 1 and voice identity plus gender for Experiment 2). Hence, couldn't it also be that the infants extracted the regularity with which one gender voice followed the other? For instance, in List B, in the words, one gender is always followed by the other (M-F or F-M), while in 2/3 of the part-words, the gender is repeated (F-F and M-M). Wouldn't you expect the same pattern of results if infants learned regularities based on gender rather than identity?</p>
</disp-quote>
<p>We used three female and three male voices to maximise acoustic variability. The streams were synthesised using MBROLA, which provides a limited set of artificial voices. Indeed, there were not enough French voices of acceptable quality, so we also used two Italian voices (the phonemes used existed in both Italian and French).</p>
<p>Voices differ in timbre, and female voices tend to be higher pitched. However, it is sometimes difficult to categorise low-pitched female voices and high-pitched male voices. Given that gender may be an important factor in infants' speech perception (newborns, for instance, prefer female voices at birth), we conducted tests to assess whether this dimension could have influenced our results.</p>
<p>We first quantified the transitional probabilities matrices during the structured stream of Experiment 2, considering that there are only two types of voices: Female and Male.</p>
<p>For List A, all transition probabilities are equal to 0.5 (P(M|F), P(F|M), P(M|M), P(F|F)), resulting in flat TPs throughout the stream (see Author response image 1, top). Therefore, we would not expect neural entrainment at the word rate (2 Hz), nor would we anticipate ERP differences between the presented duplets in the test phase.</p>
<p>For List B, P(M|F)=P(F|M)=0.66 while P(M|M)=P(F|F)=0.33. However, this does not produce a regular pattern of TP drops throughout the stream (see Author response image 1, bottom). As a result, strong neural entrainment at 2 Hz was unlikely, although some degree of entrainment might have occasionally occurred due to some drops occurring at a 2 Hz frequency. Regarding the test phase, all three Words and only one Part-word presented alternating patterns (TP=0.6). Therefore, the difference in the ERPs between Words and Partwords in List B might be attributed to gender alternation.</p>
<p>However, it seems unlikely that gender alternation alone explains the entire pattern of results, as the effect is inconsistent and appears in only one of the lists. To rule out this possibility, we analysed the effects in each list separately.</p>
<fig id="sa4fig1">
<label>Author response image 1.</label>
<caption>
<title>Transition probabilities (TPs) across the structured stream in Experiment 2, considering voices processed by gender (Female or Male).</title>
<p>Top: List A. Bottom: List B.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-101802-sa4-fig1.jpg" mimetype="image"/>
</fig>
<p>We computed the mean activation within the time windows and electrodes of interest and compared the effects of word type and list using a two-way ANOVA. For the difference between Words and Part-words over the positive cluster, we observed a main effect of word type (F(1,31) = 5.902, p = 0.021), with no effects of list or interactions (p &gt; 0.1). Over the negative cluster, we again observed a main effect of word type (F(1,31) = 10.916, p = 0.0016), with no effects of list or interactions (p &gt; 0.1). See Author response image 2.</p>
<fig id="sa4fig2">
<label>Author response image 2.</label>
<caption>
<title>Difference in ERP voltage (Words – Part-words) for the two lists (A and B); W=Words; P=Part-Words,</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-101802-sa4-fig2.jpg" mimetype="image"/>
</fig>
<p>We conducted a similar analysis for neural entrainment during the structured stream on voices. A comparison of entrainment at 2 Hz between participants who completed List A and List B showed no significant differences (t(30) = -0.27, p = 0.79). A test against zero for each list indicated significant entrainment in both cases (List A: t(17) = 4.44, p = 0.00036; List B: t(13) = 3.16, p = 0.0075). See Author response image 3.</p>
<fig id="sa4fig3">
<label>Author response image 3.</label>
<caption>
<title>Neural entrainment at 2Hz during the structured stream of Experiment 2 for Lists A and B.</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-101802-sa4-fig3.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>Words entrainment over occipital electrodes: Do you have any idea why the duplet entrainment effect occurs over the electrodes it does, in particular over the occipital electrodes (which seems a bit unintuitive given that this is a purely auditory experiment with sleeping neonates).</p>
</disp-quote>
<p>Neural entrainment might be considered as a succession of evoked response induced by the stream. After applying an average reference in high-density EEG recordings, the auditory ERP in neonates typically consists of a central positivity and a posterior negativity with a source located at the electrical zero in a single-dipole model (i.e. approximately in the superior temporal region (Dehaene-Lambertz &amp; Dehaene, 1994). In adults, because of the average reference (i.e. the sum of voltages is equal to zero at each time point) and because the electrodes cannot capture the negative pole of the auditory response, the negativity is distributed around the head. In infants, however, the brain is higher within the skull, allowing for a more accurate recording of the negative pole of the auditory ERP (see Author response image 4 for the location of electrodes in an infant head model).</p>
<p>Besides the posterior electrodes, we can see some entrainment on more anterior electrodes that probably corresponds to the positive pole of the auditory ERP.</p>
<fig id="sa4fig4">
<label>Author response image 4.</label>
<caption>
<title>International 10–20 sensors' location on the skull of an infant template, with the underlying 3-D reconstruction of the grey-white matter interface and projection of each electrode to the cortex.</title>
<p>Computed across 16 infants (from Kabdebon et al, Neuroimage, 2014). The O1, O2, T5, and T6 electrodes project lower than in adults.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-101802-sa4-fig4.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 3:</bold></p>
<p>(1) While it's true that voice is not essential for language (i.e., sign languages are implemented over gestures; the use of voices to produce non-linguistic sounds, like laughter), it is a feature of spoken languages. Thus I'm not sure if we can really consider this study as a comparison between linguistic and non-linguistic dimensions. In turn, I'm not sure that these results show that statistical learning at birth operates on non-linguistic features, being voices a linguistic dimension at least in spoken languages. I'd like to hear the authors' opinions on this.</p>
</disp-quote>
<p>On one hand, it has been shown that statistical learning (SL) operates across multiple modalities and domains in human adults and animals. On the other hand, SL is considered essential for infants to begin parsing speech. Therefore, we aimed to investigate whether SL capacities at birth are more effective on linguistic dimensions of speech, potentially as a way to promote language learning.</p>
<p>We agree with the reviewer that voices play an important role in communication (e.g., for identifying who is speaking); however, they do not contribute to language structure or meaning, and listeners are expected to normalize across voices to accurately perceive phonemes and words. Thus, voices are speech features but not linguistic features. Additionally, in natural speech, there are no abrupt voice changes within a word as in our experiment; instead, voice changes typically occur on a longer timescale and involve only a limited number of voices, such as in a dialogue. Therefore, computing regularities based on voice changes would not be useful in real-life language learning. We considered that contrasting syllables and voices was an elegant way to test SL beyond its linguistic dimension, as the experimental paradigm is identical in both experiments.</p>
<disp-quote content-type="editor-comment">
<p>Along the same line, in the Discussion section, the present results are interpreted within a theoretical framework showing statistical learning in auditory non-linguistic (string of tones, music) and visual domains as well as visual and other animal species. I'm not sure if that theoretical framework is the right fit for the present results.</p>
<p>(2) I'm not sure whether the fact that we see parallel and independent tracking of statistics in the two dimensions of speech at birth indicates that newborns would be able to do so in all the other dimensions of the speech. If so, what other dimensions are the authors referring to?</p>
</disp-quote>
<p>The reviewer is correct that demonstrating the universality of SL requires testing additional modalities and acoustic dimensions. However, we postulate that SL is grounded in a basic mechanism of long-term associative learning, as proposed in Benjamin et al. (2024), which relies on a slow decay in the representation of a given event. This simple mechanism, capable of operating on any representational output, accounts for many types of sequence learning reported in the literature (Benjamin et al., in preparation). We will revise the discussion section to clarify this theoretical framework.</p>
<disp-quote content-type="editor-comment">
<p>(3) Lines 341-345: Statistical learning is an evolutionary ancient learning mechanism but I do not think that the present results are showing it. This is a study on human neonates and adults, there are no other animal species involved therefore I do not see a connection with the evolutionary history of statistical learning. It would be much more interesting to make claims on the ontogeny (rather than philogeny) of statistical learning, and what regularities newborns are able to detect right after birth. I believe that this is one of the strengths of this work.</p>
</disp-quote>
<p>We did not intend to make claims about the phylogeny of SL. Since SL appears to be a learning mechanism shared across species, we use it as a framework to suggest that SL may arise from general operational principles applicable to diverse neural networks. Thus, while it is highly useful for language acquisition, it is not specific to it. We will revise this section to tone down our claims.</p>
<disp-quote content-type="editor-comment">
<p>(4) The description of the stimuli in Lines 110-113 is a bit confusing. In Experiment 1, e.g., &quot;pe&quot; and &quot;tu&quot; are both uttered by the same voice, correct? (&quot;random voice each time&quot; is confusing). Whereas in Experiment 2, e.g., &quot;pe&quot; and &quot;tu&quot; are uttered by different voices, for example, &quot;pe&quot; by yellow voice and &quot;tu&quot; by red voice. If this is correct, then I recommend the authors to rephrase this section to make it more clear.</p>
</disp-quote>
<p>To clarify, in Experiment 1, the voices were randomly assigned to each syllable, with the constraint that no voice was repeated consecutively. This means that syllables within the same word were spoken by different voices, and each syllable was heard with various voices throughout the stream. As a result, neonates had to retrieve the words based solely on syllabic patterns, without relying on consistent voice associations or specific voice relationships.</p>
<p>In Experiment 2, the design was orthogonal: while the syllables were presented in a random order, the voices followed a structured pattern. Similar to Experiment 1, each syllable (e.g., “pe” and “tu”) was spoken by different voices. The key difference is that in Experiment 2, the structured regularities were applied to the voices rather than the syllables. In other words, the “green” voice was always followed by the “red” voice for example but uttered different syllables.</p>
<p>We will revise the methods section to clarify these important points.</p>
<disp-quote content-type="editor-comment">
<p>(5) Line 114: the sentence &quot;they should compute a 36 x 36 TPs matrix relating each acoustic signal, with TPs alternating between 1/6 within words and 1/12 between words&quot; is confusing as it seems like there are different acoustic signals. Can the authors clarify this point?</p>
</disp-quote>
<p>Thank you for highlighting this point. To clarify, our suggestion is that neonates might not track regularities between phonemes and voices as separate features. Instead, they may treat each syllable-voice combination as a distinct item—for example, &quot;pe&quot; spoken by the &quot;yellow&quot; voice is one item, while &quot;pe&quot; spoken by the &quot;red&quot; voice is another. Under this scenario, there would be a total of 36 unique items (6 syllables × 6 voices), and infants would need to track regularities between these 36 combinations.</p>
<p>We will rephrase this sentence in the manuscript to make it clearer.</p>
</body>
</sub-article>
</article>