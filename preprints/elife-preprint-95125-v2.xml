<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">95125</article-id>
<article-id pub-id-type="doi">10.7554/eLife.95125</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95125.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Predicting individual traits from models of brain dynamics accurately and reliably using the Fisher kernel</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9287-1254</contrib-id>
<name>
<surname>Ahrends</surname>
<given-names>Christine</given-names>
</name>
<xref ref-type="author-notes" rid="n1">*</xref>
<xref ref-type="aff" rid="a1">1</xref>
<email>christine.ahrends@cfin.au.dk</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Woolrich</surname>
<given-names>Mark W</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Vidaurre</surname>
<given-names>Diego</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>dvidaurre@cfin.au.dk</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01aj84f44</institution-id><institution>Center of Functionally Integrative Neuroscience, Department of Clinical Medicine, Aarhus University</institution></institution-wrap>, <city>Aarhus</city>, <country>Denmark</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Oxford Centre for Human Brain Activity, Department of Psychiatry, University of Oxford</institution></institution-wrap>, <city>Oxford</city>, <country>United Kingdom</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Marquand</surname>
<given-names>Andre F</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Radboud University Nijmegen</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Marquand</surname>
<given-names>Andre F</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Radboud University Nijmegen</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="present-address"><label>*</label><p>Current affiliation: Wellcome Centre for Integrative Neuroimaging, John Radcliffe Hospital, University of Oxford, United Kingdom</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-03-07">
<day>07</day>
<month>03</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-01-15">
<day>15</day>
<month>01</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP95125</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-12-22">
<day>22</day>
<month>12</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-07-21">
<day>21</day>
<month>07</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.02.530638"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-03-07">
<day>07</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95125.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.95125.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95125.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95125.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95125.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Ahrends et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Ahrends et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-95125-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Predicting an individual’s cognitive traits or clinical condition using brain signals is a central goal in modern neuroscience. This is commonly done using either structural aspects, such as structural connectivity or cortical thickness, or aggregated measures of brain activity that average over time. But these approaches are missing a central aspect of brain function: the unique ways in which an individual’s brain activity unfolds over time. One reason why these dynamic patterns are not usually considered is that they have to be described by complex, high-dimensional models; and it is unclear how best to use these models for prediction. We here propose an approach that describes dynamic functional connectivity and amplitude patterns using a Hidden Markov model (HMM) and combines it with the Fisher kernel, which can be used to predict individual traits. The Fisher kernel is constructed from the HMM in a mathematically principled manner, thereby preserving the structure of the underlying model. We show here, in fMRI data, that the HMM-Fisher kernel approach is accurate and reliable. We compare the Fisher kernel to other prediction methods, both time-varying and time-averaged functional connectivity-based models. Our approach leverages information about an individual’s time-varying amplitude and functional connectivity for prediction and has broad applications in cognitive neuroscience and personalised medicine.</p>
</abstract>

<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Added time-averaged methods; improved statistical comparisons; added section on separation between training and test set</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Observing a person’s behaviour over time is how we understand the individual’s personality, cognitive traits, or psychiatric condition. The same should apply at the brain level, where we may be able to gain crucial insights by observing the patterns in which brain activity unfolds over time, i.e., brain dynamics. One way of describing brain dynamics are state-space models, which allow capturing recurring patterns of activity and functional connectivity (FC) across the whole brain. However, although research into brain dynamics has recently gained traction (<xref ref-type="bibr" rid="c9">Breakspear, 2017</xref>; <xref ref-type="bibr" rid="c11">Calhoun et al., 2014</xref>; <xref ref-type="bibr" rid="c16">Fox et al., 2005</xref>; <xref ref-type="bibr" rid="c23">Hutchison et al., 2013</xref>; <xref ref-type="bibr" rid="c28">Liégeois et al., 2017</xref>), it is still unclear how best to use this spatiotemporal level of description to characterise subject differences or predict individual traits from brain signals. One reason why brain dynamics are not usually considered in this context pertains to their representation: They are represented using models of varying complexity that are estimated from modalities such as functional MRI or MEG. Although there exists a variety of methods for estimating time-varying or dynamic FC (<xref ref-type="bibr" rid="c30">Lurie et al., 2019</xref>), like the commonly used sliding-window approach, there is currently no widely accepted way of using them for prediction problems. This is because these models are usually parametrised by a high number of parameters with complex mathematical relationships between the parameters that reflect the model assumptions. How to leverage these parameters for prediction is currently an open question.</p>
<p>We here propose the Fisher kernel for predicting individual traits from brain dynamics, using information from generative models that do not assume any knowledge of task timings. We focus on models of brain dynamics that capture within-session changes in functional connectivity and amplitude from fMRI scans, in this case acquired during wakeful rest, and how the parameters from these models can be used to predict behavioural variables or traits. In particular, we use the Hidden Markov Model (HMM), which is a probabilistic generative model of time-varying amplitude and functional connectivity (FC) dynamics (<xref ref-type="bibr" rid="c55">Vidaurre et al., 2017</xref>). HMMs have previously been shown to be able to predict certain complex subject traits, such as fluid intelligence, more accurately than structural or static (time-averaged) FC representations (<xref ref-type="bibr" rid="c53">Vidaurre et al., 2021</xref>). We combine the HMM with the Fisher kernel, which allows for the efficient use of the entire set of parameters from the generative model. The Fisher kernel takes the complex relationships between the model parameters into account by preserving the structure of the underlying model (here, the HMM) (<xref ref-type="bibr" rid="c24">Jaakkola et al., 1999</xref>; <xref ref-type="bibr" rid="c25">Jaakkola &amp; Haussler, 1998</xref>). Mathematically, the HMM parameters lie on a Riemannian manifold (the structure). This defines, for instance, the relation between parameters, such as: how changing one parameter, like the probabilities of transitioning from one state to another, would affect the fitting of other parameters, like the states’ FC. It also defines the relative importance of each parameter; for example, how a change of 0.1 in the transition probabilities would not be the same as a change of 0.1 in one edge of the states’ FC matrices.</p>
<p>For empirical evaluation, we consider two criteria that are important in both scientific and practical applications. First, predictions should be as accurate as possible, i.e., the correlation between predicted and actual values should be high. Second, predictions should be reliable, in the sense that a predictive model should never produce excessively large errors, and the outcome should be robust to reasonable variations in the data set, e.g., the choice of which subjects from the same population are included in the training set. The latter criterion is especially important if we want to be able to meaningfully interpret prediction errors, e.g., in assessing brain age (<xref ref-type="bibr" rid="c12">Cole &amp; Franke, 2017</xref>; <xref ref-type="bibr" rid="c13">Denissen et al., 2022</xref>; <xref ref-type="bibr" rid="c44">Smith et al., 2019</xref>). Despite this crucial role in interpreting model errors, reliability is not often considered in models predicting individual traits from neuroimaging features.</p>
<p>In summary, we show that using the Fisher kernel approach, which preserves the mathematical structure of the underlying HMM, we can predict individual traits from patterns of brain dynamics accurately and reliably. We show that our approach significantly outperforms methods that do not take the mathematical structure of the model into account, as well as methods based on time-averaged FC that do not consider brain dynamics. For interpretation, we also investigate which aspects of the model drive the prediction accuracy, both in real data and in simulations. Bringing accuracy, reliability and interpretation together, this work opens possibilities for practical applications such as the development of biomarkers and the investigation of individual differences in cognitive traits.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<p>We here aimed to predict behavioural and demographic variables from a model of brain dynamics using different kernel functions. The general workflow is illustrated in <xref rid="fig1" ref-type="fig">Figure 1</xref>. We started with the concatenated fMRI time-series of a group of subjects (<xref rid="fig1" ref-type="fig">Figure 1</xref>, step 1), here the resting-state fMRI timeseries of 1,001 subjects from the Human Connectome Project (HCP, described in detail under 4.1). We estimated a model of brain dynamics, here the Hidden Markov Model (HMM), which is a state-space model of time-varying amplitude and FC. The HMM and its parameters are explained in detail in <xref ref-type="sec" rid="s4b">section 4.2</xref>. We estimated the model at the group level, where the state descriptions, initial state probabilities, and the state transition probability matrix are shared across subjects (<xref rid="fig1" ref-type="fig">Figure 1</xref>, step 2). Next, we estimated subject-specific versions of this group-level model by dual estimation, where the group-level HMM parameters are re-estimated to fit the individual-level timeseries (<xref ref-type="bibr" rid="c53">Vidaurre et al., 2021</xref>) (<xref rid="fig1" ref-type="fig">Figure 1</xref>, step 3).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Workflow of the Fisher kernel prediction approach.</title>
<p>To generate a description of brain dynamics, we (1) concatenate all subjects’ individual timeseries; then (2) estimate a Hidden Markov Model (HMM) on these timeseries to generate a group-level model; then (3) dual-estimate into subject-level HMM models. Steps 1-3 are the same for all kernels. In order to then use this description of all subjects’ individual patterns of brain dynamics, we map each subject into a feature space (4). This mapping can be done in different ways: In the naïve kernels (4a), the manifold (i.e., the curved structure) on which the parameters lie is ignored and examples are treated as if they were in Euclidean space. The Fisher kernel (4b), on the other hand, respects the structure of the parameters in their original Riemannian manifold by working in the gradient space. We then construct kernel matrices (κ), where each pair of subjects has a similarity value given their parameters in the respective embedding space. Finally, we feed <italic>κ</italic> to kernel ridge regression to predict a variety of demographic and behavioural traits in a cross-validated fashion (5).</p></caption>
<graphic xlink:href="530638v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Next, we used this (HMM-mediated) description of the individuals’ brain dynamics to predict their individual traits. The parameters of the model lie on a Riemannian manifold, which is a space that has some degree of curvature, illustrated by the curved structure in <xref rid="fig1" ref-type="fig">Figure 1</xref>, step 4. We mapped these parameters into a feature space. This step works in different ways for the different kernels: In the naïve kernel (step 4a), the features are simply the parameters in the Euclidean space (i.e., ignoring the curvature of the space in <xref rid="fig1" ref-type="fig">Figure 1</xref>, step 4a); while in the Fisher kernel (step 4b), the features are mapped into the gradient space, which is a tangent space to the Riemannian manifold. We then estimated the similarity between each pair of subjects in this feature space using kernel functions. In this way, we can compare kernels that do not take the structure of the underlying model into account (the naïve kernels) with a kernel that preserves this structure (the Fisher kernel). We also compared these kernels to a previously established method based on Kullback-Leibler divergence, which estimates the similarity between the probability distributions of each pair of individual HMMs. The different kernels are described in more detail in <xref ref-type="sec" rid="s4c">section 4.3</xref>.</p>
<p>Finally, we used these kernels to predict the behavioural variables using kernel ridge regression (<xref rid="fig1" ref-type="fig">Figure 1</xref>, step 5, described in detail in <xref ref-type="sec" rid="s4d">section 4.4</xref>). The first three steps are identical for all kernels and therefore carried out only once. The fourth step (mapping the examples and constructing the kernels) is carried out once for each of the different kernels. The last step is repeated 3,500 times for each kernel to predict a set of 35 different behavioural variables using 100 randomised iterations of 10-fold nested cross validation (CV). We evaluated 24,500 predictive models using different kernels constructed from the same model of brain dynamics in terms of their ability to predict phenotypes, as well as another 24,500 predictive models based on time-averaged features, described in detail in <xref ref-type="sec" rid="s4e">section 4.5</xref>.</p>
<sec id="s2a">
<label>2.1</label>
<title>The Fisher kernel predicts more accurately than Euclidean methods</title>
<p>Using the resting-state fMRI timeseries from the HCP dataset, we found that among the kernels constructed from HMMs, the linear Fisher kernel had the highest prediction accuracy on average across the range of behavioural variables and CV folds and iterations, as shown in <xref rid="fig2" ref-type="fig">Figure 2a</xref>. Compared to the other linear kernels (which do not respect the geometry of the HMM parameters), the linear Fisher kernel (mean <italic>r κ</italic><sub><italic>Fl</italic></sub> : 0.192) was significantly more accurate than the linear naïve kernel (mean <italic>r κ</italic><sub><italic>Nl</italic></sub> : 0.05, <italic>t</italic><sub><italic>rkCV</italic></sub>=2.631, <italic>p</italic><sub><italic>BH</italic></sub>=0.031). The comparison with the linear naïve normalised kernel was not significant (mean <italic>r κ</italic><sub><italic>NNl</italic></sub>: 0.153, <italic>t</italic><sub><italic>rkCV</italic></sub>=1.022, <italic>p</italic><sub><italic>BH</italic></sub>=0.192). This indicates a positive effect of using a tangent space embedding rather than incorrectly treating the HMM parameters as Euclidean, but that this effect can be mitigated by normalising the parameters before constructing the kernel.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Distributions of performance across subject traits and CV iterations when using different methods for prediction of subject traits on HCP data.</title>
<p>The best-performing methods are highlighted by black arrows in each plot. <bold>a)</bold> Pearson’s correlation coefficients (r) between predicted and actual variable values in deconfounded space as a measure of prediction accuracy (x-axis) of each method (y-axis). Larger values indicate that the model predicts more accurately. The linear Fisher kernel has the highest average accuracy among the time-varying methods, while the Ridge regression model in Riemannian space had the highest average accuracy among the time-averaged methods. Note that we here show the distribution across target variables and CV iterations but averaged over folds for visualisation purposes, while the fold-wise accuracies were used for significance testing. Asterisks indicate significant Benjamini-Hochberg corrected p-values of repeated k-fold cross-validation corrected t-tests below 0.05 (*). <bold>b)</bold> Coefficient of determination (R<sup>2</sup>) in deconfounded space (x-axis) for each of the methods (y-axis). The x-axis is cropped at -0.1 for visualisation purposes since individual runs can produce large negative outliers, see panel c. <bold>c)</bold> Normalised maximum errors (NMAXAE) in original (non-deconfounded) space as a measure of excessive errors (x-axis) by method (y-axis). Large maximum errors indicate that the model predicts very poorly in single cases. Differences between the methods mainly lie in the tails of the distributions, where the naïve normalised Gaussian kernel produces extreme maximum errors in some runs (NMAXAE &gt; 10,000), while the linear naïve normalised kernel and the linear Fisher kernel, along with several time-averaged methods have the smallest risk of excessive errors (NMAXAE below 1). The x-axis is plotted on the log-scale. <bold>d)</bold> Robustness of prediction accuracies. The plot shows the distribution across variables of the standard deviation of correlation coefficients over folds and CV iterations on the x-axis for each method (on the y-axis). Smaller values indicate greater robustness. The linear Fisher kernel and the time-averaged Ridge regression model in Riemannian space are the most robust. Asterisks indicate significant Benjamini-Hochberg corrected p-values for repeated measures t-tests below 0.01 (**) and 0.001 (***). a), b), c) Each violin plot shows the distribution over 3,500 runs (100 iterations of 10-fold CV for all 35 variables) that were predicted from each method. d) Each violin plot shows the distribution over 35 variables that were predicted from each method.</p></caption>
<graphic xlink:href="530638v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Among the Gaussian kernels, the Gaussian Fisher kernel also had the highest average prediction accuracy <inline-formula><inline-graphic xlink:href="530638v3_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, though the comparisons with the other kernels were not significant (mean <inline-formula><inline-graphic xlink:href="530638v3_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, <italic>t</italic><sub><italic>rkCV</italic></sub>=1.277, <italic>p</italic><sub><italic>BH</italic></sub>=0.173; mean <italic>r κ</italic><sub><italic>NNg</italic></sub> : 0.094, <italic>t</italic><sub><italic>rkCV</italic></sub>=1.466, <italic>p</italic><sub><italic>BH</italic></sub> =0.145; mean <italic>r κ</italic><sub><italic>KL</italic></sub> : 0.163, <italic>t</italic><sub><italic>rkCV</italic></sub> =0.069, <italic>p</italic><sub><italic>BH</italic></sub>=0.482). Comparing prediction accuracies of the linear with the Gaussian Fisher kernel was not significant (<italic>t</italic><sub><italic>rkCV</italic></sub>=0.993, <italic>p</italic><sub><italic>BH</italic></sub>=0.192).</p>
<p>Compared to the methods using time-averaged FC for prediction, the linear Fisher kernel significantly outperformed the Selected Edges method (mean <italic>r</italic> Selected Edges: 0.081, <italic>t</italic><sub><italic>rkCV</italic></sub>=2.417, <italic>p</italic><sub><italic>BH</italic></sub>=0.031). The average prediction accuracy of the linear Fisher kernel is also higher than the log-Euclidean kernel (mean <italic>r</italic>: 0.140, <italic>t</italic><sub><italic>rkCV</italic></sub>=1.456, <italic>p</italic><sub><italic>BH</italic></sub>=0.145) and the Ridge regression model (mean <italic>r</italic>: 0.111, <italic>t</italic><sub><italic>rkCV</italic></sub>=1.906, <italic>p</italic><sub><italic>BH</italic></sub>=0.085), but outperformed by time-averaged KL divergence (mean <italic>r</italic>: 0.194, <italic>t</italic><sub><italic>rkCV</italic></sub>=-0.044, <italic>p</italic><sub><italic>BH</italic></sub>=0.482) and Ridge regression in Riemannian space (mean <italic>r</italic> Ridge Riem.: 0.223, <italic>t</italic><sub><italic>rkCV</italic></sub>=-0.999, <italic>p</italic><sub><italic>BH</italic></sub>=0.192), though these comparisons were all not significant. Analogous to the effect of tangent space embedding for the HMM parameters (linear Fisher kernel compared to linear naïve kernel), using a tangent space embedding on the time-averaged covariance matrices (Ridge regression in Riemannian space compared to Ridge regression) also significantly improved the prediction accuracy (<italic>t</italic><sub><italic>rkCV</italic></sub>=-2.537, <italic>p</italic><sub><italic>BH</italic></sub>=0.0313). The Elastic Net and Elastic Net in Riemannian space were not used for statistical comparisons since they failed to converge in a substantial portion of runs (&gt;20%). The Elastic Nets showed similar performance to the Ridge regression models. We also observed that the non-kernel-based Euclidean time-averaged FC models, i.e., the Ridge regression and the Selected Edges model, make predictions at a smaller range than the actual variables, close to the variables’ means. This leads to weak relationships between predicted and actual variables but smaller errors, as shown in Supplementary Figure 3. The distributions of correlation coefficients for the different methods are shown in <xref rid="fig2" ref-type="fig">Figure 2a</xref> and the explained variance in <xref rid="fig2" ref-type="fig">Figure 2b</xref>. The performance of all methods is also summarised in Supplementary Table 2.</p>
<p>We found that the linear Fisher kernel also predicted more accurately than other HMM-based kernels when fitting the HMM to only the first resting-state session of each subject, as shown in Supplementary Figure 1. However, the overall accuracies of all kernels were lower in this case, indicating that predicting traits benefits from a large amount of available data per subject. Similarly, the Fisher kernel outperformed the other kernels when HMM states were defined only in terms of covariance (not mean) at comparable accuracy, as shown in Supplementary Figure 2.</p>
<p>As shown in <xref rid="fig3" ref-type="fig">Figure 3</xref>, there are differences in how well these demographic or behavioural variables can be predicted from a model of brain dynamics. Certain variables may be more related to static or structural measures (<xref ref-type="bibr" rid="c29">Liegeois et al., 2019</xref>; <xref ref-type="bibr" rid="c53">Vidaurre et al., 2021</xref>), or just be difficult to predict in general. Overall, age could be best predicted, followed by language-related cognitive items (PicVocab, ReadEng both age adjusted and unadjusted).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Model performance estimates over cross-validation (CV) iterations by behavioural variable and method, ordered by accuracy on HCP data.</title>
<p>Boxplots show the distribution over 100 iterations of 10-fold CV of correlation coefficient values (x-axis) of each method, separately for each of the 35 predicted variables (y-axes). Among the time-varying methods, the linear Fisher kernel (green) predicts at higher accuracy for many variables, and also shows the narrowest range, indicating high robustness. However, for many target variables, it is outperformed by the time-averaged tangent space models (Ridge reg. Riem. and Elastic Net Riem.). Black lines within each boxplot represent the median.</p></caption>
<graphic xlink:href="530638v3_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In summary, the linear Fisher kernel has the highest prediction accuracy of the time-varying methods, significantly outperforming the linear naïve kernel which does not take the geometry of the HMM parameters into account. The linear Fisher kernel also has a higher prediction accuracy than several methods using time-averaged FC features, but it is outperformed by time-averaged methods that work in tangent space.</p>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>The linear Fisher kernel has a lower risk of excessive errors and is more robust than other methods</title>
<p>We now show empirically that the linear Fisher kernel is more reliable than other kernels, both in terms of risk of large errors and in terms of robustness over CV iterations.</p>
<p>The linear versions of the Fisher kernel and the naïve normalised kernel had the overall lowest risk of large errors among the time-varying methods, as shown in <xref rid="fig2" ref-type="fig">Figure 2c</xref>. We assessed the risk of large errors (NMAXAE &gt; 10), very large errors (NMAXAE &gt; 100), and extreme errors (NMAXAE &gt; 1,000), corresponding to one, two, and three orders of magnitude of the range of the actual variable. For the Fisher kernel, the risk of large errors is low: 0% in the linear version <italic>κ</italic><sub><italic>Fl</italic></sub> and 0.029% in the Gaussian version <inline-formula><inline-graphic xlink:href="530638v3_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> That means that the linear Fisher kernel never makes large errors exceeding the range of the actual variable by orders of magnitude. In the naïve kernel, both the linear <italic>κ</italic><sub><italic>Nl</italic></sub> and the Gaussian version <inline-formula><inline-graphic xlink:href="530638v3_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula> have a low risk of large errors at 0.057% for the linear version and 0.029% for the Gaussian version. While the linear naïve normalised kernel <italic>κ</italic><sub><italic>NNl</italic></sub> has a 0% risk of large errors, its Gaussian version <italic>κ</italic><sub><italic>NNg</italic></sub> has the overall highest risk of large errors at 1.229%, a risk of very large errors at 0.143%, and even a risk of extreme errors at 0.029%. The KL divergence model <italic>κ</italic><sub><italic>Kl</italic></sub> has a 0.686% risk of large errors and a 0.029% risk of very large errors. The time-averaged KL divergence model performs slightly better than the time-varying KL divergence, but also has a risk of large errors at 0.600%. The other time-averaged models had no risk of excessive errors. The maximum error distributions are shown in <xref rid="fig2" ref-type="fig">Figure 2b</xref>.</p>
<p>A reason for the higher risk of large errors in the Gaussian kernels is likely that the radius <italic>τ</italic> of the radial basis function needs to be selected (using cross-validation), introducing an additional factor of variability and leaving more room for error. Supplementary Figure 6 shows the relation between the estimated hyperparameters (the regularisation parameter <italic>λ</italic> and the radius <italic>τ</italic> of the radial basis function) and how large errors in the predictions may be related to poor estimation of these parameters.</p>
<p>With respect to robustness, we found that the linear Fisher kernel <italic>κ</italic><sub><italic>Fl</italic></sub> had the most robust performance among the time-varying methods, and the Ridge regression model in Riemannian space among the time-averaged methods, on average across the range of variables tested, as shown in <xref rid="fig2" ref-type="fig">Figure 2c</xref>. Robustness was quantified as the standard deviation of the correlation between model-predicted and actual values over 100 iterations and 10 folds of CV. A low standard deviation indicates high robustness since the method’s performance does not differ greatly depending on the specific subjects it was trained and tested on.</p>
<p>Among the time-varying methods, the linear Fisher kernel was the most robust method (<italic>κ</italic><sub><italic>Fl</italic></sub> mean S.D. <italic>r</italic>: 0.088), though the comparison with the other linear kernels was not significant (linear naïve kernel <italic>κ</italic><sub><italic>Nl</italic></sub> mean S.D. <italic>r</italic>: 0.096, <italic>t</italic>=-2.351, <italic>p</italic><sub><italic>BH</italic></sub> =0.099; linear naïve normalised kernel <italic>κ</italic><sub><italic>NNl</italic></sub> mean S.D. <italic>r</italic>: 0.090, <italic>t</italic>= -1.745, <italic>p</italic><sub><italic>BH</italic></sub>=0.180). Similarly, the comparison between the Gaussian Fisher kernel (<inline-formula><inline-graphic xlink:href="530638v3_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula> mean S.D. <italic>r</italic>: 0.089) and the other Gaussian kernels was not significant (Gaussian naïve kernel <inline-formula><inline-graphic xlink:href="530638v3_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula> mean S.D. <italic>r</italic>: 0.089, <italic>t</italic>=0.311, <italic>p</italic><sub><italic>BH</italic></sub>=0.826; Gaussian naïve normalised kernel <italic>κ</italic><sub><italic>NNg</italic></sub> mean S.D. <italic>r</italic>: 0.093, <italic>t</italic>=-1.802, <italic>p</italic><sub><italic>BH</italic></sub> =0.180; KL divergence <italic>κ</italic><sub><italic>KL</italic></sub> mean S.D. <italic>r</italic>: 0.093, <italic>t</italic>=-1.758, <italic>p</italic><sub><italic>BH</italic></sub>=0.180). Compared to the time-averaged methods, the linear Fisher kernel was significantly more robust than the time-averaged KL divergence model (mean S.D. <italic>r</italic>: 0.100, <italic>t</italic>=-3.798, <italic>p</italic><sub><italic>BH</italic></sub>=0.003) and the Selected Edges model (mean S.D. <italic>r</italic>: 0.100, <italic>t</italic>=-5.446, <italic>p</italic><sub><italic>BH</italic></sub>&lt;0.0001), while the other comparisons were not significant (log-Euclidean mean S.D. <italic>r</italic>: 0.091, <italic>t</italic>=-1.317, <italic>p</italic><sub><italic>BH</italic></sub>=0.277; Ridge reg. mean S.D. <italic>r</italic>: 0.091, <italic>t</italic>=-1.451, <italic>p</italic><sub><italic>BH</italic></sub>=0.268; Ridge reg. Riem. mean S.D. <italic>r</italic>: 0.088, <italic>t</italic>=0.059, <italic>p</italic><sub><italic>BH</italic></sub>=0.953). This large variation in model performance depending on the CV fold structure in the time-averaged KL divergence model and the Selected Edges method is problematic. Among the time-averaged methods, the Ridge Regression model in Riemannian space was the most robust method. The ranges in model performance across CV iterations for each variable of the different kernels are shown in <xref rid="fig3" ref-type="fig">Figure 3</xref>.</p>
<p>Overall, the linear Fisher kernel was the most reliable method among the time-varying methods, and the Ridge regression model in Riemannian space was the most reliable among the time-averaged methods. For both methods, there was no risk of large errors and the variability over CV iterations was the smallest. The Gaussian kernels had higher risks of large errors, and both the time-varying and the time-averaged KL divergence model risked producing large errors, indicating that their performance was less reliable. The Gaussian naïve normalised kernel was the most problematic in terms of reliability with a risk of extreme errors ranging up to four orders of magnitude of the actual variable’s range. Two of the time-averaged methods, the time-averaged KL divergence model and the Selected Edges method, showed problems with robustness, indicating considerable susceptibility to changes in CV folds.</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>State features drive predictions of individual differences for Fisher kernel</title>
<p>To understand which features drive the prediction, we next simulated timeseries of two groups of subjects that were different either in the mean amplitude of one state or in the transition probabilities. As shown in <xref rid="fig4" ref-type="fig">Figure 4a</xref>, when we simulated two groups of subjects that are different in terms of the mean amplitude, the Fisher kernel was able to recover this difference in all runs with 0% error, meaning that it identified all subjects correctly in all runs. The Fisher kernel significantly outperformed the other two kernels (Fisher kernel <italic>κ</italic><sub><italic>Fl</italic></sub> vs. naïve kernel: <italic>κ</italic><sub><italic>Nl</italic></sub> <italic>p=</italic>0.0003, vs. naïve normalised kernel <italic>κ</italic><sub><italic>NNl</italic></sub>: <italic>p=</italic>0.0001). There was no significant difference between the naïve and the naïve normalised kernel (<italic>p</italic>=1). However, when we simulated differences in transition probabilities between two groups, neither of the kernels were able to reliably recover this difference. In this simulation, the Fisher kernel performed significantly worse than the other two kernels on average (compared to naïve kernel: <italic>p=</italic>0.006, compared to naïve normalised kernel: <italic>p</italic>=0.004), as shown in <xref rid="fig4" ref-type="fig">Figure 4b</xref>. As in the previous simulation, the naïve kernel and the naïve normalised kernel did not significantly differ from each other (<italic>p</italic>=1).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Simulations.</title>
<p><bold>a)</bold> Simulating two groups of subjects that are different in their state means. The error distributions of all 10 iterations show that the Fisher kernel recovers the simulated group difference in all runs with 0% error. <bold>b)</bold> Simulating two groups of subjects that are different in their transition probabilities. Neither kernel is able to reliably recover the group difference in all 10 iterations. <bold>c)</bold> Simulating two groups of subjects that are different in their transition probabilities but excluding state parameters when constructing the kernels. The Fisher kernel performs best in recovering the group difference.</p></caption>
<graphic xlink:href="530638v3_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The above results suggest that all kernels, but particularly the Fisher kernel, are most sensitive to differences in state parameters rather than differences in transition probabilities. To understand whether the difference in transition probabilities can be recovered when it is not overshadowed by the more dominant state parameters, we ran the second case of simulations again, where we introduced a group difference in terms of transition probabilities, but this time we exclude the state parameters when we constructed the kernels. As shown in <xref rid="fig4" ref-type="fig">Figure 4c</xref>, the Fisher kernel was now able to recover the group difference with minimal errors, while the naïve normalised kernel improved but did not perform as well as the Fisher kernel. The naïve kernel performed below chance. The Fisher kernel significantly outperformed both the naïve kernel (<italic>p</italic>=0.0001) and the naïve normalised kernel (<italic>p</italic>=0.02), and the naïve normalised kernel was significantly more accurate than the naïve kernel (<italic>p</italic>=0.0004). This shows that the Fisher kernel can recover the group difference in transition probabilities when this difference is not overshadowed by the state parameters. The features and kernel matrices of example runs for all three simulations are shown in <xref rid="fig4" ref-type="fig">Figure 4a-c</xref> middle and right panels.</p>
<p>When using real data, the features driving the prediction may differ from trait to trait: For some traits, state parameters may be more relevant, while for other traits, transitions may be more relevant. Given our above findings on simulated data, we therefore compared the effects of systematically removing state features or removing transition features in all traits in the real data.</p>
<p>We found that state parameters were the most relevant features for the Fisher kernel predictions in all traits: As shown in <xref rid="fig5" ref-type="fig">Figure 5a</xref>, the prediction accuracy of the Fisher kernel was significantly diminished when state features were removed (<italic>t</italic><sub><italic>rkCV</italic></sub>=2.922, <italic>p</italic><sub><italic>BH</italic></sub>=0.016), while removing transition features had no significant effect (<italic>t</italic><sub><italic>rkCV</italic></sub>= -0.173, <italic>p</italic><sub><italic>BH</italic></sub>=0.480). We observed the same effect in the naïve normalised kernel (no state features: <italic>t</italic><sub><italic>rkCV</italic></sub>=2.460, <italic>p</italic><sub><italic>BH</italic></sub>=0.031; no transition features: <italic>t</italic><sub><italic>rkCV</italic></sub>=0.264, <italic>p</italic><sub><italic>BH</italic></sub>=0.480). For the naïve kernel, removing features did not have any significant effects (no state features: <italic>t</italic><sub><italic>rkCV</italic></sub>=0.501, <italic>p</italic><sub><italic>BH</italic></sub>=0.462; no transition features: <italic>t</italic><sub><italic>rkCV</italic></sub>=0.050, <italic>p</italic><sub><italic>BH</italic></sub>=0.480). One reason for the dominance of state parameters may simply be that the state parameters outnumber the other parameters: In the full kernels, we have 15,300 state features (300 features associated with the state means and 15,000 features associated with the state covariances), but only 42 transition features (6 features associated with the state probabilities and 36 features associated with the transition probabilities). To compensate for this imbalance, we also constructed a version of the kernels where state parameters were reduced to the same amount as transition features using Principal Component Analysis (PCA), so that we have 84 features in total (42 transition features and the first 42 PCs of the 15,300 state features). These PCA-kernels performed better than the ones where state features were removed, but worse than the kernels including all features at the original dimensionality, though not significantly (Fisher kernel: <italic>t</italic><sub><italic>rkCV</italic></sub>=1.434, <italic>p</italic><sub><italic>BH</italic></sub>=0.170; naïve kernel: <italic>t</italic><sub><italic>rkCV</italic></sub>=0.860, <italic>p</italic><sub><italic>BH</italic></sub>=0.351; naïve normalised kernel: <italic>t</italic><sub><italic>rkCV</italic></sub>=1.555, <italic>p</italic><sub><italic>BH</italic></sub>=0.170). This indicates that the fact that state parameters are more numerous than transition parameters does not explain why kernels including state features performed better. Instead, the content of the state features is more relevant for the prediction than the content of the transition features.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Effects of removing sets of features from the kernels on prediction accuracies.</title>
<p><bold>a)</bold> In the overall prediction accuracies, removing state features significantly decreased performance in the Fisher kernel and the naïve normalised kernel, while removing transition features had no significant effect. <bold>b)</bold> Removing features has similar effects on all variables, both better predicted (left panel) and worse predicted ones (right panel).</p></caption>
<graphic xlink:href="530638v3_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>When looking at the performance separately for each variable (<xref rid="fig5" ref-type="fig">Figure 5b</xref>), we found that all variables were better predicted by the version of the kernel which included state features than the ones where state features were removed, while it did not seem to matter whether transition features were included. This indicates that the simulation case described above, where the relevant changes are in the transition probabilities, did not occur in the real data. In certain variables, reducing state features using PCA improved the accuracy compared to the full kernels. This is not unexpected since feature dimensionality reduction is known to be able to improve prediction accuracy by removing redundant features (<xref ref-type="bibr" rid="c33">Mwangi et al., 2014</xref>).</p>
<p>Empirically, we thus found that the Fisher kernel is most sensitive to individual differences in state parameters, both in simulated timeseries and in the real data. This means that predictions are driven more by what an individual’s states look like, rather than by how they transition between states. However, the Fisher kernel can be modified to recover differences in transition probabilities if these are relevant for specific traits.</p>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Separation between training and test set in HMM training</title>
<p>In the results above, we have constructed kernels from HMMs fit to all subjects in the dataset and only separated them into training and test set at the regression (and deconfounding) step. In machine learning, the gold standard is considered to be a full separation between training and test set at all stages of (pre-)processing to avoid data leakage from the test set into the training set (<xref ref-type="bibr" rid="c27">Kapoor &amp; Narayanan, 2023</xref>; <xref ref-type="bibr" rid="c34">Poldrack et al., 2020</xref>). However, a recent study in neuroimaging has found minimal effects of data leakage for breaches of separation between training and test set not involving the target variable or feature selection (<xref ref-type="bibr" rid="c36">Rosenblatt et al., 2024</xref>). To test whether training the HMM on all subjects may have inflated prediction accuracies, we repeated the HMM-based predictions for kernels that were constructed from HMMs trained only on training subjects. Consistent with the results in (<xref ref-type="bibr" rid="c53">Vidaurre et al., 2021</xref>), for all kernels, both linear and Gaussian versions, the separation of training and test subjects before vs. after fitting the HMM had no significant effects on prediction accuracies (training together vs. separate: linear Fisher kernel: <italic>t</italic><sub><italic>kCV</italic></sub>=0, <italic>p</italic><sub><italic>BH</italic></sub>=1; Gaussian Fisher kernel: <italic>t</italic><sub><italic>kCV</italic></sub>=0, <italic>p</italic><sub><italic>BH</italic></sub>=1; linear naïve kernel: <italic>t</italic><sub><italic>kCV</italic></sub>=-0.139, <italic>p</italic><sub><italic>BH</italic></sub>=1; Gaussian naïve kernel: <italic>t</italic><sub><italic>kCV</italic></sub>=-0.018, <italic>p</italic><sub><italic>BH</italic></sub>=1; linear naïve normalised kernel: <italic>t</italic><sub><italic>kCV</italic></sub>=-0.030, <italic>p</italic><sub><italic>BH</italic></sub>=1; Gaussian naïve normalised kernel: <italic>t</italic><sub><italic>kCV</italic></sub>=0.040, <italic>p</italic><sub><italic>BH</italic></sub>=1). <xref rid="fig6" ref-type="fig">Figure 6a</xref> shows the distribution of accuracies across target variables and CV folds for all kernels and training schemes, confirming that at this sample size (<italic>N</italic>=1,001), the effect is negligible.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Effects of HMM training scheme.</title>
<p><bold>a)</bold> Prediction accuracies for HMM-based kernels depending on HMM training scheme (training on all subjects: together; training only on training set: separate). In the real data (N=1,001), fitting the HMM to all subjects before constructing the kernels compared to fitting it only to the training set to preserve train-test separation has no effect. Note that we are here plotting the fold-wise accuracies (as opposed to averaged over folds, as in the figures above), and we only ran one iteration of CV (rather than 100 repetitions, as in the figures above). <bold>b)</bold> Prediction accuracies in simulated heterogeneous subject groups depending on training scheme, between-group difference, and target variable (Y) noise. In simulated data, the Fisher kernel’s performance decreases when the test subjects are increasingly different from the training subjects. <bold>c)</bold> Example kernels for high between-group difference. While the naive kernel underperforms in both cases, the strong difference between training and test subjects is visible in the naive normalised kernel, while it completely dominates the Fisher kernel.</p></caption>
<graphic xlink:href="530638v3_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Importantly, in the Fisher kernel, individual subjects’ features are defined in reference to a group “average”. While this has no effect in situations where training and test subjects are taken from the same distribution, it may introduce a bias where test subjects are taken from a different distribution (e.g., the case where one might want to train a model on healthy controls and test on patients). To illustrate this effect, we simulated timeseries for two groups of subjects with varying degrees of between-group difference and noise on the target variable. As shown in <xref rid="fig6" ref-type="fig">Fig. 6b</xref>, this does not affect the kernels when training the HMM on all subjects (training scheme: together, middle panel), but the performance of the Fisher kernel worsens as group difference increases when training the HMM only on the training set (training scheme: separate, right panel). This is because in the Fisher kernel, subjects’ similarity is determined by their difference from the group-level HMM. When the test subjects are included in this group-level HMM, their scores will be similar to the training subjects scores, but when they are excluded from the group-level HMM, their scores may be overestimated since they are all different from the group-level model. This between-group difference may then overshadow the more subtle differences related to the target variable, as shown in the example kernels in <xref rid="fig6" ref-type="fig">Fig. 6c</xref>. Whether or not this behaviour is desired will depend on the use case.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<p>In this work, we aimed to establish an approach that allows leveraging a rich description of the patterns in which brain activity unfolds over time to predict individual traits. We showed that the HMM-Fisher kernel approach accurately and reliably predicts traits from brain dynamics models trained on neuroimaging data. It preserves the structure of the underlying brain dynamics model, making it ideal for combining generative and predictive models. We compared the Fisher kernel to kernels which ignore the structure of the brain dynamics model (“naïve” kernels), to a previously used method based on Kullback-Leibler divergence (<xref ref-type="bibr" rid="c53">Vidaurre et al., 2021</xref>), and to methods based on time-averaged functional connectivity. The linear Fisher kernel had an overall higher prediction accuracy than all other time-varying methods and several time-averaged methods, though most comparisons were not statistically significant given the narrow margin for improvements. The linear Fisher kernel was also among the most reliable: It never produced excessive errors and was robust to changes in training sets. Like in the time-varying methods, working in Riemannian space also improved prediction for the time-averaged methods, indicating that respecting the geometry of the space that the predictors lie on is an important factor for predictive modelling in neuroscience. While we here focussed on fMRI, the method can also be applied to other modalities like MEG or EEG. It can also be straightforwardly implemented in any kernel-based prediction model or classifier, including kernel ridge regression, support vector machines (SVM), kernel fisher discriminant analysis (k-FDA), kernel logistic regression (KLR), or nonlinear PCA. Indeed, it can also be applied to other probabilistic generative models aside from the HMM, e.g., Dynamic network modes (<xref ref-type="bibr" rid="c18">Gohil et al., 2022</xref>).</p>
<p>Our findings were consistent in two alternative settings: when using less data per subject and when modelling only time-varying FC (rather than amplitude and FC states). This supports the generalisability of the results. However, we observed overall lower accuracies when using only one scanning session consisting of 1,200 timepoints per subject. This indicates that more available timepoints per subject allow better characterisation of an individual. We also found that the Gaussian versions of the kernels are generally more error-prone and susceptible to changes in the training set, although they may predict more accurately in certain runs. Implementing Gaussian kernels in a predictive model is also computationally more expensive, making them less practical.</p>
<p>While we here tested robustness in terms of susceptibility to changes in CV folds, it remains to be shown to what extent model performance is sensitive to the random initialisation of the HMM, which affects the parameter estimation (<xref ref-type="bibr" rid="c2">Alonso &amp; Vidaurre, 2023</xref>; <xref ref-type="bibr" rid="c21">Griffin et al., 2024</xref>). We also showed that the Fisher kernel is most sensitive to changes in state descriptions, i.e., individual differences in the amplitude or functional connectivity of certain brain states. While this could be a disadvantage if a trait was more closely related to how an individual transitions between brain states, we found that this was not the case in any of the traits we tested here. Other traits than the ones we tested here may of course be more related to individual transition patterns. For this case, we showed in simulations that the Fisher kernel can be modified to recognise changes in transitions if they are of interest for the specific research question.</p>
<p>Finally, we showed that the results we presented here are unaffected by separation between training and test set at the step of training the group-level HMM. However, since the Fisher kernel defines individuals in reference to the group-level model, we showed in simulations that separating training and test subjects prior to fitting the HMM may result in biased kernels that overestimate dissimilarity of the test subjects. This is an important consideration as it may affect studies with small sample sizes and with heterogeneous training and test sets (e.g., where a researcher may want to fit a model to healthy controls and subsequently test it on patients).</p>
<p>We here aimed to show the potential of the HMM-Fisher kernel approach to leverage information from patterns of brain dynamics to predict individual traits in an example fMRI dataset as well as simulated data. The fMRI dataset we used (HCP 1200 Young Adult) is a large sample taken from a healthy, young population, and it remains to be shown how the exhibited performance generalises to other datasets, e.g. other modalities such as EEG/MEG, clinical data, older populations, different data quality, or smaller sample sizes both in terms of the number of participants and the scanning duration. Additionally, we only tested our approach for the prediction of a specific set of demographic items and cognitive scores; it may be interesting to test the framework also on clinical variables, such as the presence of a disease or the response to pharmacological treatment.</p>
<p>There is growing interest in combining different data types or modalities, such as structural, static, and dynamic measures, to predict phenotypes (<xref ref-type="bibr" rid="c15">Engemann et al., 2020</xref>; <xref ref-type="bibr" rid="c40">Schouten et al., 2016</xref>). While directly combining the features from each modality can be problematic, modality-specific kernels, such as the Fisher kernel for time-varying amplitude and/or FC, can be easily combined using approaches such as stacking (<xref ref-type="bibr" rid="c10">Breiman, 1996</xref>) or Multi Kernel Learning (MKL) (Gönen &amp; Alpaydin, 2011). MKL can improve prediction accuracy of multimodal studies (<xref ref-type="bibr" rid="c48">Vaghari et al., 2022</xref>), and stacking has recently been shown to be a useful framework for combining static and time-varying FC predictions (<xref ref-type="bibr" rid="c21">Griffin et al., 2024</xref>). A detailed comparison of different multimodal prediction strategies including kernels for time-varying amplitude/FC may be the focus of future work.</p>
<p>In a clinical context, while there are nowadays highly accurate biomarkers and prognostics for many diseases, others, such as psychiatric diseases, remain poorly understood, diagnosed, and treated. Here, improving the description of individual variability in brain measures may have potential benefits for a variety of clinical goals, e.g., to diagnose or predict individual patients’ outcomes, find biomarkers, or to deepen our understanding of changes in the brain related to treatment responses like drugs or non-pharmacological therapies (<xref ref-type="bibr" rid="c32">Marquand et al., 2016</xref>; <xref ref-type="bibr" rid="c47">Stephan et al., 2017</xref>; <xref ref-type="bibr" rid="c56">Wen et al., 2022</xref>; <xref ref-type="bibr" rid="c58">Wolfers et al., 2015</xref>). However, the focus so far has mostly been on static or structural information, leaving the potentially crucial information from brain dynamics untapped. Our proposed approach provides one avenue of addressing this by leveraging individual patterns of time-varying amplitude and FC, as one of many possible descriptions of brain dynamics, and it can be flexibly modified or extended to include, e.g., information about temporally recurring frequency patterns (<xref ref-type="bibr" rid="c54">Vidaurre et al., 2016</xref>). In order to be able to use predictive models from brain dynamics in a clinical context, predictions must be reliable, particularly if we want to interpret model errors, as in models of “brain age” (<xref ref-type="bibr" rid="c12">Cole &amp; Franke, 2017</xref>; <xref ref-type="bibr" rid="c13">Denissen et al., 2022</xref>; <xref ref-type="bibr" rid="c44">Smith et al., 2019</xref>). As we demonstrated in this work, there can be extreme errors and large variation in some predictive models, and these issues are not resolved by estimating model performance in a standard cross-validated fashion. We here showed that taking the structure of the underlying model or predictors into account, and thoroughly assessing not only accuracy but also errors and robustness, we can reliably use information from brain dynamics to predict individual traits. This will allow gaining crucial insights into cognition and behaviour from how brain function changes over time, beyond structural and static information.</p>
</sec>
<sec id="s4">
<label>4</label>
<title>Methods</title>
<sec id="s4a">
<label>4.1</label>
<title>HCP imaging and behavioural data</title>
<p>We used data from the open-access Human Connectome Project (HCP) S1200 release (Smith, Beckmann, et al., 2013; <xref ref-type="bibr" rid="c50">Van Essen et al., 2013</xref>), which contains MR imaging data and various demographic and behavioural data from 1,200 healthy, young adults (age 22-35). All data described below, i.e., timecourses of the resting-state fMRI data and demographic and behavioural variables, are publicly available at <ext-link ext-link-type="uri" xlink:href="https://db.humanconnectome.org">https://db.humanconnectome.org</ext-link>.</p>
<p>Specifically, we used resting state fMRI data of 1,001 subjects, for whom any of the behavioural variables of interest were available. Each participant completed four resting state scanning sessions of 14 mins and 33 seconds duration each. This resulted in 4,800 timepoints per subject. For the main results, we used all four resting state scanning sessions of each participant to fit the model of brain dynamics (but see Supplementary Figure 1 for results with just one session). The acquisition parameters are described in the HCP acquisition protocols and in <xref ref-type="bibr" rid="c50">Van Essen et al. (2013)</xref>; <xref ref-type="bibr" rid="c51">Van Essen et al. (2012)</xref>. Briefly, structural and functional MRI data were acquired on a 3T MRI scanner. The resting state fMRI data was acquired using multiband echo planar imaging sequences with an acceleration factor of 8 at 2 mm isotropic spatial resolution and a repetition time (TR) of 0.72 seconds. The preprocessing and timecourse extraction pipeline is described in detail in Smith, Beckmann, et al. (2013). For the resting state fMRI scans, preprocessing consisted of minimal spatial preprocessing and surface projection (<xref ref-type="bibr" rid="c17">Glasser et al., 2013</xref>), followed by temporal preprocessing. Temporal preprocessing consisted of single-session Independent Component Analysis (ICA) (<xref ref-type="bibr" rid="c6">Beckmann, 2012</xref>) and removal of noise components (<xref ref-type="bibr" rid="c20">Griffanti et al., 2014</xref>; <xref ref-type="bibr" rid="c37">Salimi-Khorshidi et al., 2014</xref>). Data were high-pass-filtered with a cut-off at 2,000 seconds to remove linear trends.</p>
<p>The parcellation was estimated from the data using multi-session spatial ICA on the temporally concatenated data from all subjects. Note that this means that there is no strict divide between the subjects used for training and the subjects for testing the later predictive models, so that there is potential for leakage of information between training and test set. However, since this step does not concern the target variable, but only the preprocessing of the predictors, the effect can be expected to be minimal (<xref ref-type="bibr" rid="c36">Rosenblatt et al., 2024</xref>). Using this approach, a data-driven functional parcellation with 50 parcels was estimated, where all voxels are weighted according to their activity in each parcel, resulting in a weighted, overlapping parcellation. While other parcellations are available for the resting-state fMRI HCP dataset, we chose this parcellation because dynamic changes in FC have been shown to be better detected in this parcellation compared to other functional or anatomical parcellations or more fine-grained parcellations (<xref ref-type="bibr" rid="c1">Ahrends et al., 2022</xref>). Timecourses were extracted using dual regression (<xref ref-type="bibr" rid="c7">Beckmann et al., 2009</xref>), where group-level components are regressed onto each subject’s fMRI data to obtain subject-specific versions of the parcels and their timecourses. We normalised the timecourses of each subject to ensure that the model of brain dynamics and, crucially, the kernels were not driven by (averaged) amplitude and variance differences between subjects.</p>
<p>Subjects in the HCP study completed a range of demographic and behavioural questionnaires. Following <xref ref-type="bibr" rid="c53">Vidaurre et al. (2021)</xref>, we here focus on a subset of those items, including age and various cognitive variables. The cognitive variables span items assessing memory, executive function, fluid intelligence, language, processing speed, spatial orientation, and attention. The full list of the 35 behavioural variables used here, as well as their categorisation within the HCP dataset can be found in Supplementary Table 1.</p>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>The Hidden Markov Model</title>
<p>To estimate patterns of time-varying amplitude and FC, we here use the Hidden Markov Model (<xref ref-type="bibr" rid="c54">Vidaurre et al., 2016</xref>; <xref ref-type="bibr" rid="c55">Vidaurre et al., 2017</xref>). However, the kernels, which are explained in detail in the following section, can be constructed from any generative probabilistic model.</p>
<p>The Hidden Markov model (HMM) is a generative probabilistic model, which assumes that an observed time-series, such as BOLD signal in a given parcellation, was generated by a sequence of “hidden states” (<xref ref-type="bibr" rid="c4">Baum &amp; Eagon, 1967</xref>; <xref ref-type="bibr" rid="c5">Baum &amp; Petrie, 1966</xref>). We here model the states as multivariate Gaussian distributions, defined both in terms of mean and covariance —which can be interpreted as distinct patterns of amplitude and FC (<xref ref-type="bibr" rid="c55">Vidaurre et al., 2017</xref>). For comparison, we also considered a second variety of the HMM where state means were pinned to zero (given that the data was demeaned and the global average is zero) and only the covariance was allowed to vary across states (<xref ref-type="bibr" rid="c55">Vidaurre et al., 2017</xref>), which is shown in Supplementary Figure 2. This is equivalent to using a Wishart state model, and therefore focuses more specifically on time-varying FC.</p>
<p>The HMM is described by a set of parameters <italic>θ</italic>, containing the state probabilities <italic>π</italic>, the transition probabilities <italic>A</italic>, the mean vectors <italic>μ</italic> of all states (if modelled), and the covariance matrices <italic>Σ</italic> of all states:
<disp-formula id="ueqn1">
<graphic xlink:href="530638v3_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>K</italic> is the number of states and <italic>M</italic> is the number of parcels in the parcellation. The entire set of parameters <italic>θ</italic> is estimated from the data. The number of states can be understood as the level of detail or granularity with which we describe the spatiotemporal patterns in the data, akin to a dimensionality reduction, where a small number of states will lead to a very general, coarse description and a large number of states will lead to a very detailed, fine-grained description. Here, we chose a small number of states, <italic>K</italic> = 6, to ensure that the group-level HMM states are general enough to be found in all subjects, since a larger number of states increases the chances of certain states being present only in a subset of subjects. The exact number of states is less relevant in this context, since the same HMM estimation is used for all kernels.</p>
<p>The HMM is a probabilistic generative model, as the generative process works by sampling probabilistically in this case from a Gaussian distribution with mean <italic>μ</italic><sub><italic>k</italic></sub> and covariance <italic>Σ</italic><sub><italic>k</italic></sub> when state <italic>k</italic> is active:
<disp-formula id="ueqn2">
<graphic xlink:href="530638v3_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>X</italic><sub><italic>t</italic></sub> is the timeseries at timepoint <italic>t</italic> and <italic>q</italic><sub><italic>t</italic></sub> is the currently active state. Which state is active depends on the previous state <italic>q</italic><sub><italic>t</italic>−1</sub> and is determined by the transition probabilities <italic>A</italic>, so that the generated state sequence is sampled from a categorical distribution with parameters:
<disp-formula id="ueqn3">
<graphic xlink:href="530638v3_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>A</italic><sub><italic>k</italic></sub> indicates the k-th row of the transition probability matrix.</p>
<p>The space of parameters <italic>θ</italic> forms a Riemannian manifold <italic>R</italic><sub><italic>θ</italic></sub>, where the relationships between the different parameters of the HMM are acknowledged by construction. The Fisher kernel, as described below, is built upon a projection on this manifold, so predictions based on this kernel account for the mathematical structure of the HMM.</p>
<p>Here, we fit the HMM to the concatenated timeseries of all <italic>N</italic> subjects (see <xref rid="fig1" ref-type="fig">Figure 1</xref>, step 1). We refer to the group-level estimate as HMM<sup>0</sup>, which is defined by the parameters <italic>θ</italic><sup>0</sup> (see <xref rid="fig1" ref-type="fig">Figure 1</xref>, step 2):
<disp-formula id="ueqn4">
<graphic xlink:href="530638v3_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>To use the information from the HMM to predict subjects’ phenotypes, we estimate subject-specific versions of the group-level HMM (see <xref rid="fig1" ref-type="fig">Figure 1</xref>, step 3) through dual estimation (<xref ref-type="bibr" rid="c55">Vidaurre et al., 2017</xref>). Dual estimation refers to the process of fitting the previously estimated group-level model again to a single subject’s timeseries, so that the parameters from the group-level model HMM<sup>0</sup> are adapted to fit the individual. We will refer to the subject-specific estimate for subject <italic>n</italic> as HMM<sup><italic>n</italic></sup>, with parameters <italic>θ</italic><sup><italic>n</italic></sup>.</p>
<p>These subject-specific HMM parameters are the features from which we construct the kernels. To understand which features are most important for the predictions, we also construct versions of the kernels that include only subsets of the features. Specifically, we can group the features into two subsets: 1. the state features, describing <italic>what</italic> states look like, containing the mean vectors <italic>μ</italic>, and the covariance matrices <italic>Σ</italic> of all states, and 2. the transition features, describing <italic>how</italic> individuals transition between these states, containing the initial state probabilities <italic>π</italic> and the transition probabilities <italic>A</italic>. By removing one or the other set of features and evaluating how model performance changes compared to the full kernels, we can draw conclusions about the importance of these two different types of changes for the predictions. Since the state features are considerably more numerous than the transition features (15,300 state features compared to 42 transition features in this case), we also construct a version of the kernels where state features have been reduced to the same number as the transition features using PCA, i.e., we use all 42 transition features and the first 42 PCs of the state features. This allowed us to perform a fairer comparison of what elements in the model are more predictive of the subject traits.</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Kernels from Hidden Markov Models</title>
<p>Kernels (<xref ref-type="bibr" rid="c41">Shawe-Taylor &amp; Cristianini, 2004</xref>) are a convenient approach to accommodate nonlinearity and to work with high-dimensional, complex features, such as parameters from a model of brain dynamics. In general, kernels are similarity functions between subjects, and they can be used straightforwardly in a prediction algorithm. While feature matrices can be very high dimensional, a kernel is represented by a (no. of subjects by no. of subjects) matrix. Kernel methods can readily be adapted to deal with nonlinear decision boundaries in prediction, by projecting the data into a high-dimensional (possibly infinite-dimensional) space through an embedding <italic>x</italic> → <italic>ϕ</italic><sub><italic>x</italic></sub>; then, by estimating a linear separating hyperplane on this space, we can effectively have a nonlinear estimator on the original space (<xref ref-type="bibr" rid="c41">Shawe-Taylor &amp; Cristianini, 2004</xref>). In practice, instead of working explicitly in a higher-dimensional embedding space, the so-called kernel trick uses a kernel function <italic>κ</italic>(<italic>n,m</italic>) containing the similarity between data points <italic>n</italic> and <italic>m</italic> (here, subjects) in the higher-dimensional embedding space (<xref ref-type="bibr" rid="c39">Schölkopf et al., 2002</xref>; <xref ref-type="bibr" rid="c41">Shawe-Taylor &amp; Cristianini, 2004</xref>), which can be simpler to calculate. Once <italic>κ</italic>(·,·) is computed for each pair of subjects, this is all that is needed for the prediction. This makes kernels computationally very efficient, since in most cases the number of subjects will be smaller than the number of features —which, in the case of HMMs, can be very large (potentially, in the order of millions). However, finding the right kernel can be a challenge because there are many available alternatives for the embedding.</p>
<p>Here, in combination with a linear predictive model, we apply a kernel that is specifically conceived to be used to compare instances of generative models such as the HMM. We expected this to result in better predictions than existing methods. Using the same HMM estimate, we compare three different kernels, which map the HMM parameters into three distinct spaces, corresponding to different embeddings (see <xref rid="fig1" ref-type="fig">Figure 1</xref>, step 4): the naïve kernel, the naïve normalised kernel, and the Fisher kernel.</p>
<p>While the first two kernels (naïve and naïve normalised kernel) do not take into account constraints imposed by the HMM on how the model parameters can change with respect to each other, the Fisher kernel does. The Fisher kernel achieves this by calculating how, and by how much, one should change the group level HMM parameters to make the model generate data that is more like the data from a particular subject.</p>
<p>We then construct linear and Gaussian versions of the different kernels (see <xref rid="fig1" ref-type="fig">Figure 1</xref>, step 4), which take the general form <inline-formula><inline-graphic xlink:href="530638v3_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for the linear kernel and <inline-formula><inline-graphic xlink:href="530638v3_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for the Gaussian kernel. We compare these to a kernel constructed using Kullback-Leibler divergence, previously used for predicting behavioural phenotypes (<xref ref-type="bibr" rid="c53">Vidaurre et al., 2021</xref>).</p>
<sec id="s4c1">
<label>4.3.1</label>
<title>Naïve kernel</title>
<p>The naïve kernel is based on a simple vectorisation of the subject-specific version of the HMM’s parameters, each on their own scale. This means that the kernel does not take relationships between the parameters into account and the parameters are here on different scales. This procedure can be thought of as computing Euclidean distances between two sets of HMM parameters, ignoring the actual geometry of the space of parameters. For each subject <italic>n</italic>, we vectorise parameters <italic>θ</italic><sup><italic>n</italic></sup> obtained through dual estimation of the group-level parameters <italic>θ</italic><sup>0</sup> to map the example <inline-formula><inline-graphic xlink:href="530638v3_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to:
<disp-formula id="ueqn5">
<graphic xlink:href="530638v3_ueqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>We will refer to this vectorised version of the subject-specific HMM parameters as “naïve <italic>θ</italic>”. from The naïve <italic>θ</italic> are the features used in the naïve kernel <italic>κ</italic><sub><italic>N</italic></sub>. We first construct a linear kernelfrom the naïve <italic>θ</italic> features using the inner product of the feature vectors. The linear naïve kernel <italic>κ</italic><sub><italic>Nl</italic></sub> between subjects <italic>n</italic> and <italic>m</italic> is thus defined as:
<disp-formula id="ueqn6">
<graphic xlink:href="530638v3_ueqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where ⟨<italic>θ</italic><sup><italic>n</italic></sup>,<italic>θ</italic><sup><italic>m</italic></sup>⟩ denotes the inner product between <italic>θ</italic><sup><italic>n</italic></sup> and <italic>θ</italic><sup><italic>m</italic></sup>. Using the same feature vectors, we can also construct a Gaussian kernel from the naïve <italic>θ</italic>. The Gaussian naïve kernel <italic>κ</italic><sub><italic>Ng</italic></sub> for subjects <italic>n</italic> and <italic>m</italic> is thus defined as:
<disp-formula id="ueqn7">
<graphic xlink:href="530638v3_ueqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>τ</italic> is the radius of the radial basis function, and ‖ <italic>θ</italic><sup><italic>n</italic> −</sup> <italic>θ</italic><sup><italic>m</italic></sup>‖ is the L<sub>2</sub>-norm of the difference of the feature vectors (naïve <italic>θ</italic> for subjects <italic>n</italic> and <italic>m</italic>). Compared to a linear kernel, a Gaussian kernel embeds the features into a more complex space, which can potentially improve the accuracy. However, this kernel has an additional parameter <italic>τ</italic> that needs to be chosen, typically through cross-validation. This makes a Gaussian kernel computationally more expensive and, if the additional parameter <italic>τ</italic> is poorly estimated, more error-prone. The effect of the hyperparameters on errors is shown in Supplementary Figure 3.</p>
<p>While the naïve kernel takes all the information from the HMM into account by using all parameters from a subject-specific version of the model, it uses these parameters in a way that ignores the structure of the model that these parameters come from. In this way, the different parameters in the feature vector are difficult to compare, since e.g., a change of 0.1 in the transition probabilities between two states is not of the same magnitude as a change of 0.1 in one entry of the covariance matrix of a specific state. In the naïve kernel, these two very different types of changes would be treated indistinctly.</p>
</sec>
<sec id="s4c2">
<label>4.3.2</label>
<title>Naïve normalised kernel</title>
<p>To address the problem of parameters being on different scales, the naïve normalised kernel makes the scale of the subject-specific vectorised parameters (i.e., the naïve <italic>θ</italic>) comparable across parameters. Here, the mapping <italic>x</italic> → <italic>ϕ</italic><sub><italic>x</italic></sub> consists of a vectorisation and normalisation across subjects of the subject-specific HMM parameters, by subtracting the mean over subjects from each parameter and dividing by the standard deviation. This kernel does not respect the geometry of the space of parameters either.</p>
<p>As for the naïve kernel, we can then construct a linear kernel from these vectorised, normalised parameters <inline-formula><inline-graphic xlink:href="530638v3_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula> by computing the inner product for all pairs of subjects <italic>n</italic> and <italic>m</italic> to obtain the linear naïve normalised kernel <italic>κ</italic><sub><italic>NNl</italic></sub>:
<disp-formula id="ueqn8">
<graphic xlink:href="530638v3_ueqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>We can also compute a Gaussian kernel from the naïve normalised feature vectors to obtain the Gaussian version of the naïve normalised kernel <italic>κ</italic><sub><italic>NNg</italic></sub>:
<disp-formula id="ueqn9">
<graphic xlink:href="530638v3_ueqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>In this way, we have constructed a kernel in which parameters are all on the same scale, but which still ignores the complex relationships between parameters originally encoded by the underlying model of brain dynamics.</p>
</sec>
<sec id="s4c3">
<label>4.3.3</label>
<title>Fisher kernel</title>
<p>The Fisher kernel (<xref ref-type="bibr" rid="c24">Jaakkola et al., 1999</xref>; <xref ref-type="bibr" rid="c25">Jaakkola &amp; Haussler, 1998</xref>) is specifically designed to preserve the structure of a generative probabilistic model (here, the HMM). This can be thought of as a “proper” projection on the manifold, as illustrated in <xref rid="fig1" ref-type="fig">Figure 1</xref>, step 4b. Similarity between subjects is here defined in reference to a group-level model of brain dynamics. The mapping <italic>x</italic> → <italic>ϕ</italic><sub><italic>x</italic></sub> is given by the “Fisher score”, which indicates how (i.e., in which direction in the Riemannian parameter space) we would have to change the group-level model to better explain a particular subject’s timeseries. The similarity between subjects can then be described based on this score, so that two subjects are defined as similar if the group-level model would have to be changed in a similar direction for both, and dissimilar otherwise.</p>
<p>More precisely, the Fisher score is given by the gradient of the log-likelihood with respect to each model parameter:
<disp-formula id="ueqn10">
<graphic xlink:href="530638v3_ueqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>x</italic><sup><italic>n</italic></sup> is the timeseries of subject <italic>n</italic>, and <italic>ℒ</italic><sub><italic>θ</italic></sub>(<italic>x</italic>) =<italic>P</italic>(<italic>x</italic>|<italic>θ</italic>)represents the likelihood of the timeseries <italic>x</italic> given the model parameters <italic>θ</italic>. This way, the Fisher score maps an example (i.e., a subject’s timeseries) <italic>x</italic><sup><italic>n</italic></sup> into a point in the gradient space of the Riemannian manifold <italic>R</italic><sub><italic>θ</italic></sub> defined by the HMM parameters.</p>
<p>The invariant Fisher kernel <italic>κ</italic><sub><italic>F</italic>−</sub> is the inner product of the Fisher score <italic>g</italic>, scaled by the Fisher information matrix <italic>F</italic>, which gives a local metric on the Riemannian manifold <italic>R</italic><sub><italic>θ</italic>:</sub>
<disp-formula id="ueqn11">
<graphic xlink:href="530638v3_ueqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
for subjects <italic>n</italic> and <italic>m</italic>. <inline-formula><inline-graphic xlink:href="530638v3_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the Fisher information matrix, defined as
<disp-formula id="ueqn12">
<graphic xlink:href="530638v3_ueqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the expectation is with respect to <italic>x</italic> under the distribution <italic>P</italic>(<italic>x</italic>| <italic>θ</italic>). The Fisher information matrix <inline-formula><inline-graphic xlink:href="530638v3_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula> can be approximated empirically:
<disp-formula id="ueqn13">
<graphic xlink:href="530638v3_ueqn13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
which is simply the covariance matrix of the gradients <italic>g</italic>. Using <inline-formula><inline-graphic xlink:href="530638v3_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula> essentially serves to whiten the gradients; therefore, given the large computational cost associated with <inline-formula><inline-graphic xlink:href="530638v3_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,we here disregard the Fisher information matrix and reduce the invariant Fisher kernel to the so-called practical Fisher kernel (<xref ref-type="bibr" rid="c24">Jaakkola et al., 1999</xref>; <xref ref-type="bibr" rid="c25">Jaakkola &amp; Haussler, 1998</xref>; <xref ref-type="bibr" rid="c49">van der Maaten, 2011</xref>), for which the linear version <italic>κ</italic><sub><italic>Fl</italic></sub> takes the form:
<disp-formula id="ueqn14">
<graphic xlink:href="530638v3_ueqn14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>In this study, we will use the practical Fisher kernel for all computations.</p>
<p>One issue when working with the linear Fisher kernel is that the gradients of typical examples (i.e., subjects whose timeseries can be described by similar parameters as the group-level model) are close to zero, while gradients of atypical examples (i.e., subjects who are very different from the group-level model) can be very large. This may lead to an underestimation of the similarity between two typical examples because their inner product is very small even though they are very similar. To mitigate this, we can plug the gradient features (i.e., the Fisher scores <italic>g</italic>) into a Gaussian kernel, which essentially normalises the kernel. For subjects <italic>n</italic> and <italic>m</italic>, where <italic>x</italic><sup><italic>n</italic></sup> is the timeseries of subject <italic>n</italic> and <italic>x</italic><sup><italic>m</italic></sup> is the timeseries of subject <italic>m</italic>, the Gaussian Fisher kernel <italic>κ</italic><sub><italic>Fg</italic></sub> is defined as
<disp-formula id="ueqn15">
<graphic xlink:href="530638v3_ueqn15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="530638v3_inline15.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the distance between examples <italic>n</italic> and <italic>m</italic> in the gradient space, and <italic>τ</italic> is the width of the Gaussian kernel.</p>
</sec>
<sec id="s4c4">
<label>4.3.4</label>
<title>Kullback-Leibler divergence</title>
<p>The Kullback-Leibler (KL) divergence is an information-theoretic distance measure which estimates divergence between probability distributions —in this case between subject-specific versions of the HMM. Here, KL divergence of subject <italic>n</italic> from subject <italic>m</italic>, KL(HMM<sup><italic>n</italic></sup>|| HMM<sup><italic>m</italic></sup>) can be interpreted as how much new information the HMM of subject <italic>n</italic> contains if the true distribution was the HMM of subject <italic>m</italic>. KL divergence is not symmetric, i.e., KL(HMM<sup><italic>n</italic></sup>|| HMM<sup><italic>m</italic></sup>) is different than KL(HMM<sup><italic>n</italic></sup>|| HMM<sup><italic>m</italic></sup>). We here use an approximation of KL divergence as in <xref ref-type="bibr" rid="c14">Do (2003)</xref> and <xref ref-type="bibr" rid="c53">Vidaurre et al. (2021)</xref>. That is, given two models HMM<sup><italic>n</italic></sup> from HMM<sup><italic>m</italic></sup> for subject <italic>n</italic> and subject <italic>m</italic>, we have
<disp-formula id="ueqn16">
<graphic xlink:href="530638v3_ueqn16.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="530638v3_inline16.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are the transition probabilities from state <italic>k</italic> into any other state according to HMM<sup><italic>n</italic></sup> and <inline-formula><inline-graphic xlink:href="530638v3_inline17.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are the state Gaussian distributions for state <italic>k</italic> and HMM<sup><italic>n</italic></sup> (respectively <inline-formula><inline-graphic xlink:href="530638v3_inline18.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="530638v3_inline19.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for HMM<sup><italic>m</italic></sup>); see <xref ref-type="bibr" rid="c31">MacKay et al. (2003)</xref>. Since the transition probabilities are Dirichlet-distributed and the state distributions are Gaussian distributed, KL divergence for those has a closed-form solution. Variables <italic>v</italic><sub><italic>k</italic></sub> can be computed numerically such that
<disp-formula id="ueqn17">
<graphic xlink:href="530638v3_ueqn17.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>To be able to use KL divergence as a kernel, we symmetrise the KL divergence matrix as
<disp-formula id="ueqn18">
<graphic xlink:href="530638v3_ueqn18.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>This symmetrised KL divergence can be plugged into a radial basis function, analogous to the Gaussian kernels to obtain a similarity matrix <italic>κ</italic><sub><italic>KL</italic></sub>
<disp-formula id="ueqn19">
<graphic xlink:href="530638v3_ueqn19.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>The resulting KL similarity matrix can be used in the predictive model in a similar way as the kernels described above.</p>
</sec>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>Predictive model: Kernel ridge regression</title>
<p>Similarly to <xref ref-type="bibr" rid="c53">Vidaurre et al. (2021)</xref>, we use kernel ridge regression (KRR) to predict demographic and behavioural variables from the different kernels (other kernel-based prediction model or classifier such as a support vector machine are also possible). KRR is the kernelised version of ridge regression (<xref ref-type="bibr" rid="c38">Saunders et al., 1998</xref>):
<disp-formula id="ueqn20">
<graphic xlink:href="530638v3_ueqn20.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>α</italic> are the regression weights; <italic>h</italic> is the (number of subjects in test set by number of subjects in training set) kernel matrix between the subjects in the training set and the subjects in the test set; <italic>ŷ</italic> are the predictions in the (out-of-sample) test set; and <italic>S</italic><sub><italic>train</italic></sub> are the number of subjects in the training set; and <italic>S</italic><sub><italic>test</italic></sub> are the number of subjects in the test set. The regression weights <italic>α</italic> can be estimated using the kernels specified above as
<disp-formula id="ueqn21">
<graphic xlink:href="530638v3_ueqn21.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>λ</italic> is a regularisation parameter that we can choose through cross-validation; <italic>I</italic> is the identity matrix; <italic>κ</italic> is the (<italic>S</italic><sub><italic>train</italic></sub> by <italic>S</italic><sub><italic>train</italic></sub>) kernel matrix of the subjects in the training set; and <italic>y</italic> are the training examples.</p>
<p>We use KRR to separately predict each of the 35 demographic and behavioural variables from each of the different methods, removing subjects with missing entries from the prediction. We used k-fold nested cross-validation (CV) to select and evaluate the models. We used 10 folds for both the outer loop (used to train and test the model) and the inner loop (used to select the optimal hyperparameters) such that 90% were used for training and 10% for testing. The optimal hyperparameters <italic>λ</italic> (and <italic>τ</italic> in the case of the Gaussian kernels) were selected using grid-search from the vectors <italic>λ</italic> = [0.0001,0.001,0.01,0.1,0.3,0.5,0.7,0.9,1] and <italic>τ</italic> = [<sup>1</sup>/<sub>5</sub>,<sup>1</sup>/<sub>3</sub>, <sup>1</sup>/<sub>2</sub>,1,2,3,5]. In both the outer and the inner loop, we accounted for family structure in the HCP dataset so that subjects from the same family were never split across folds (<xref ref-type="bibr" rid="c57">Winkler et al., 2015</xref>). Within the CV, we regressed out sex and head motion confounds, i.e., we estimated the regression coefficients for the confounds on the training set and applied them to the test set (<xref ref-type="bibr" rid="c46">Snoek et al., 2019</xref>). We repeated the nested 10-fold CV 100 times, so that different combinations of subjects were randomly assigned to the folds at each new CV iteration to obtain a distribution of model performance values for each variable. This is to explicitly show how susceptible each model was to changes in the training folds, which we can take as a measure of the robustness of the estimators, as described below. We generated the 100 random repetitions of the 10 outer CV folds once, and then used them for training and prediction of all methods, so that all methods were fit to the same partitions.</p>
<sec id="s4d1">
<label>4.4.1</label>
<title>Evaluation criteria</title>
<p>We evaluate the models in terms of two outcome criteria: prediction accuracy and reliability. For prediction accuracy, we used Pearson’s correlation coefficient <italic>r</italic> (<italic>ŷ, y</italic>) between the model-predicted values <italic>ŷ</italic> and the actual values <italic>y</italic> of each variable and the coefficient of determination <italic>R</italic><sup>2</sup>. The second criterion, reliability, concerns two aspects: i) that the model will never show excessively large errors for single subjects that could harm interpretation; and ii) that the model’s accuracy will be consistent across random variations of the training set —in this case by using different (random) iterations for the CV folds. This is important if we want to interpret prediction errors for example in clinical contexts, which assumes that the error size of a model in a specific subject reflects something biologically meaningful, e.g., whether a certain disease causes the brain to “look” older to a model than the actual age of the subject (<xref ref-type="bibr" rid="c13">Denissen et al., 2022</xref>). Maximum errors inform us about single cases where a model (that may typically perform well) fails. The maximum absolute error (MAXAE) is the single largest error made by the regression model in each iteration, i.e.,
<disp-formula id="ueqn22">
<graphic xlink:href="530638v3_ueqn22.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Since the traits we predict are on different scales, the MAXAE is difficult to interpret, e.g., a MAXAE of 10 would be considered small if the true range of the variable we are predicting was 1,000, while it would be considered large if the true range of the variable was 1. To make the results comparable across the different traits, we therefore normalise the MAXAE by dividing it by the range of the respective variable. In this way, we obtain the NMAXAE:
<disp-formula id="ueqn23">
<graphic xlink:href="530638v3_ueqn23.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Since the NMAXAEs follow extreme value distributions, it is more meaningful to consider the proportion of the values exceeding relevant thresholds than testing for differences in the means of these distributions (<xref ref-type="bibr" rid="c22">Gumbel, 1958</xref>). We here consider the risk of large errors (NMAXAE &gt; 10), very large errors (NMAXAE &gt; 100), and extreme errors (NMAXAE &gt; 1,000) as the percentage of runs (across variables and CV iterations) where the model’s NMAXAE exceeds the given threshold. Since NMAXAE is normalised by the range of the actual variable, these thresholds correspond to one, two, and three orders of magnitude of the actual variable’s range. If we are predicting age, for instance, and the true ages of the subjects range from 25 years to 35 years, an NMAXAE of 1 would mean that the model’s least accurate prediction is off by 10 years, an NMAXAE of 10 would mean that the least accurate prediction is off by 100 years, an NMAXAE of 100 would be off by 1,000 years, and an NMAXAE of 1,000 would be off by 10,000 years. A model that makes such large errors, even in single cases, would be unusable for interpretation. Our reliability criterion in terms of maximum errors is therefore that the risk of large errors (NMAXAE &gt; 10) should be 0%.</p>
<p>For a model to be reliable, it should also be robust in the sense of susceptibility to changes in the training examples. Robustness is an important consideration in prediction studies, as it determines how reproducible a study or method is —which is often a shortcoming in neuroimaging-based prediction studies (<xref ref-type="bibr" rid="c52">Varoquaux et al., 2017</xref>). We evaluated the robustness by iterating nested 10-fold CV 100 times for each variable, randomising the subjects in the folds at each iteration, so that the models would encounter a different combination of subjects in the training and test sets each time they are run. Looking at this range of 1,000 accuracies (10 folds * 100 repetitions) for each variable, we can assess whether the model’s performance changes drastically depending on which combinations of subjects it encountered in the training phase, or whether the performance is the same regardless of the subjects encountered in the training phase. The former would be an example of a model that is susceptible to changes in training examples and the latter an example of a model that is robust. We here quantify robustness as the standard deviation (S.D.) of the prediction accuracy <italic>r</italic> across the 10 folds and 100 CV iterations of each variable, where a small mean S.D. over variables indicates higher robustness.</p>
<p>We test for significant differences in mean prediction accuracy between the methods using repeated k-fold cross-validation corrected t-tests (<xref ref-type="bibr" rid="c8">Bouckaert &amp; Frank, 2004</xref>). For results obtained through cross-validation with <italic>k</italic> folds and <italic>r</italic> repetitions, the corrected t-statistic is calculated as:
<disp-formula id="ueqn24">
<graphic xlink:href="530638v3_ueqn24.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>x</italic> is the difference in prediction accuracy, <italic>n</italic><sub>1</sub> is the number of samples in the training data, <italic>n</italic><sub>2</sub> the number of samples in the test data, and <italic>σ</italic><sup>2</sup> the variance estimate. To test for significant differences in mean robustness (S.D. across CV repetitions), we use paired t-tests. All p-values are corrected across multiple comparisons using Benjamini-Hochberg correction to control the false discovery rate (FDR). For the main results, we separately compare the linear Fisher kernel to the other linear kernels, and the Gaussian Fisher kernel to the other Gaussian kernels, as well as to each other. We also compare the linear Fisher kernel to all time-averaged methods. Finally, to test for the effect of tangent space projection for the time-averaged FC prediction, we also compare the Ridge regression model to the Ridge Regression in Riemannian space. To test for effects of removing sets of features, we use the approach described above to compare the kernels constructed from the full feature sets to their versions where features were removed or reduced. Finally, to test for effects of training the HMM either on all subjects or only on the subjects that were later used as training set, we compare each kernel to the corresponding kernel constructed from HMM parameters, where training and test set were kept separate.</p>
</sec>
</sec>
<sec id="s4e">
<label>4.5</label>
<title>Models based on time-averaged FC features</title>
<p>To compare our approach’s performance to simpler methods that do not take dynamics into account, we compared them to seven different regression models based on time-averaged FC features. For each subject, time-averaged FC was computed as covariance between each pair of regions. The first time-averaged model is, analogous to the HMM-derived KL divergence model, a time-averaged KL divergence model. The model is described in detail in <xref ref-type="bibr" rid="c53">Vidaurre et al. (2021)</xref>. Briefly, we construct symmetrised KL divergence matrices of each subject’s time-averaged FC and predict from these matrices using the KRR pipeline described above. We refer to this model as time-averaged KL divergence. We next used a kernel constructed from a geodesic distance (i.e., a metric defined on the Riemannian manifold of time-averaged covariance matrices). Specifically, we used a Gaussian kernel on the log-Euclidean distance, which is the Frobenius norm of the logarithm map of the time-averaged covariance matrices (<xref ref-type="bibr" rid="c26">Jayasumana et al., 2013</xref>). We refer to this model as log-Euclidean. The other five time-averaged FC benchmark models do not involve kernels but predict directly from the features instead. Namely, we use two variants of a Ridge Regression and an Elastic Net model (<xref ref-type="bibr" rid="c59">Zou &amp; Hastie, 2005</xref>), one using the unwrapped time-averaged FC matrices as input (i.e., in Euclidean space), and one using the time-averaged FC matrices in Riemannian space, where covariance matrices are projected into tangent space (<xref ref-type="bibr" rid="c3">Barachant et al., 2013</xref>; Smith, Vidaurre, et al., 2013), which we refer to as Ridge Reg. and Ridge Reg. (Riem), and Elastic Net and Elastic Net (Riem.), respectively. Finally, we compare our models to the approach taken in <xref ref-type="bibr" rid="c35">Rosenberg et al. (2016)</xref>, where relevant edges of the time-averaged FC matrices are first selected, and then used as predictors in a regression model. We refer to this model as Selected Edges. All time-averaged FC models are fitted using the same (nested) cross-validation strategy as described above (10-fold CV using the outer loop for model evaluation and the inner loop for model selection using grid-search for hyperparameter tuning, accounting for family structure in the dataset, and repeated 100 times with randomised folds).</p>
</sec>
<sec id="s4f">
<label>4.6</label>
<title>Simulations</title>
<sec id="s4f1">
<label>4.6.1</label>
<title>Feature importance</title>
<p>To further understand the behaviour of the different kernels, we simulate data and compare the kernels’ ability to recover the ground truth. Specifically, we aim to understand which type of parameter change the kernels are most sensitive to. We generate timeseries for two groups of subjects, timeseries <italic>X</italic><sup>1</sup>for group 1 and timeseries <italic>X</italic><sup>2</sup>for group 2, from two separate HMMs with respective sets of parameters <italic>θ</italic><sup>1</sup> and <italic>θ</italic><sup>2</sup>:
<disp-formula id="ueqn25">
<graphic xlink:href="530638v3_ueqn25.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>We simulate timeseries from HMMs with these parameters through the generative process described in 4.2.</p>
<p>For the simulations, we use the group-level HMM of the real dataset with <italic>K</italic> = 6 states used in the main text as basis for group 1, i.e., <italic>HMM</italic>(<italic>θ</italic><sup>1</sup>) = <italic>HMM</italic>(<italic>θ</italic><sup>0</sup>) We then manipulate two different types of parameters, the state means <italic>μ</italic> and the transition probabilities <italic>A</italic>, while keeping all remaining parameters the same between the groups. In the first case, we manipulate one state’s mean between the groups, i.e.:
<disp-formula id="ueqn26">
<graphic xlink:href="530638v3_ueqn26.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>μ</italic><sup>2</sup> is obtained by simply adding a Gaussian noise vector to the state mean vector of one state:
<disp-formula id="ueqn27">
<graphic xlink:href="530638v3_ueqn27.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here, <italic>φ</italic> is the Gaussian noise vector of size 1 × <italic>M, M</italic> is the number of parcels, here 50, and <inline-formula><inline-graphic xlink:href="530638v3_inline20.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="530638v3_inline21.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are the first rows (corresponding to the first state) of the state mean matrices for groups 1 and 2, respectively. We control the amplitude of <italic>φ</italic> so that the difference between <inline-formula><inline-graphic xlink:href="530638v3_inline22.gif" mime-subtype="gif" mimetype="image"/></inline-formula>and <inline-formula><inline-graphic xlink:href="530638v3_inline23.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is smaller than the minimum distance between any pair of states within one HMM. This is to ensure that the HMM recovers the difference between groups as difference in one state’s mean vector, rather than detecting a new state for group 2 that does not occur in group 1 and consequently collapsing two other states. Since the state means <italic>μ</italic> and the state covariances <italic>Σ</italic> are the first- and second-order parameters of the same respective distributions, it is to be expected that, although we only directly manipulate <italic>μ, Σ</italic> changes as well.</p>
<p>In the second case, we manipulate the transition probabilities for one state between the groups, while keeping all other parameters the same, i.e.:
<disp-formula id="ueqn28">
<graphic xlink:href="530638v3_ueqn28.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>A</italic><sup>2</sup> is obtained by randomly permuting the probabilities of one state to transition into any of the other states, excluding self-transition probability:
<disp-formula id="ueqn29">
<graphic xlink:href="530638v3_ueqn29.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here <italic>p</italic>, is a random permutation vector of size (<italic>K</italic>-1) × 1, <inline-formula><inline-graphic xlink:href="530638v3_inline24.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="530638v3_inline25.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are the self-transition probabilities of state 1, and <inline-formula><inline-graphic xlink:href="530638v3_inline26.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="530638v3_inline27.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are the probabilities of state 1 to transition into each of the other states.</p>
<p>We then concatenate the generated timeseries
<disp-formula id="ueqn30">
<graphic xlink:href="530638v3_ueqn30.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
containing 100 subjects for group 1 and 100 subjects for group 2, with 1,200 timepoints and 50 parcels per subject. Note that we do not introduce any differences between subjects within a group, so that the between-group difference in HMM parameters should be the most dominant distinction and easily recoverable by the classifiers. We then apply the pipelines described above, running a new group-level HMM and constructing the linear versions of the naïve kernel, the naïve normalised kernel, and the Fisher kernel on these synthetic timeseries. The second case of simulations, manipulating the transition probabilities, only introduces a difference in few (<italic>K</italic>-1) features and keeps the majority of the features the same between the groups, while the first case introduces a difference in a large number of features (<italic>M</italic> features directly, by changing one state’s mean vector, and an additional <italic>M</italic> × <italic>M</italic> features indirectly, as this state’s covariance matrix will also be affected). To account for this difference, we additionally construct a version of the kernels for the second case of simulations that includes <italic>π</italic> only and <italic>A</italic>, removing the state parameters <italic>μ</italic> and <italic>Σ</italic>.</p>
<p>Finally, we use a support vector machine (SVM) in combination with the different kernels to recover the group labels and measure the error produced by each kernel. We repeat the whole process (generating timeseries, constructing kernels, and running the SVM) 10 times, in each iteration randomising on 3 levels: generating new random noise/permutation vectors to simulate the timeseries, randomly initialising the HMM parameters when fitting the group-level HMM to the simulated timeseries, and randomly assigning subjects to 10 CV folds in the SVM.</p>
</sec>
<sec id="s4f2">
<label>4.6.2</label>
<title>Separating training and test set</title>
<p>We also simulate data to understand the sensitivity of the different kernels to separating training and test set before or after running the HMM. While separating training and test set is generally considered to be the gold standard to avoid leakage of information, this strategy may cause issues when the training and test set are very different from each other. In the simplest case, this would cause models to poorly generalise, which we aim to assess with the train-test split. However, in the case of the Fisher kernel, features are not independent but defined in reference to a group set of parameters. That means that, if we train the HMM only on the training set and then construct the Fisher kernel of a group of test subjects that are very different from the training set, this difference will be overrepresented in the kernel and likely overshadow other, more subtle differences between individuals.</p>
<p>Similar to the feature importance simulations, we generate time courses for two groups of subjects from two HMMs: timeseries <italic>X</italic><sup><italic>train</italic></sup> for the training set and timeseries <italic>X</italic><sup><italic>test</italic></sup> for the test set, from HMMs with parameters <italic>θ</italic><sup><italic>train</italic></sup> and <italic>θ</italic><sup><italic>test</italic></sup> :
<disp-formula id="ueqn31">
<graphic xlink:href="530638v3_ueqn31.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>We use the group-level HMM fit to real data as the basis for the simulations, so that <italic>HMM (θ</italic><sup><italic>train</italic></sup><italic>)</italic> = <italic>HMM (θ</italic><sup>0</sup><italic>)</italic>. For the test set, we then add different levels of noise to one of the state’s mean vector to simulate varying degrees of between-group difference:
<disp-formula id="ueqn32">
<graphic xlink:href="530638v3_ueqn32.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Where <italic>ϵ</italic> is the 1 × <italic>M</italic> Gaussian noise vector representing the heterogeneity between the training and the test set and <italic>b</italic> is a scalar controlling the noise level.</p>
<p>We randomly generate a continuous target variable, <italic>Y</italic>, which the models will aim to predict. We will simulate a single state’s mean to be correlated with this target variable, but we will add varying degrees of noise, which will make it gradually more difficult to predict the target.</p>
<p>We do this by adding a fraction of the target variable and the noise to a second state mean vector for each subject in the training and the test set:
<disp-formula id="ueqn33">
<graphic xlink:href="530638v3_ueqn33.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>μ</italic> <sub><italic>k</italic></sub> is the 1 × <italic>M</italic> state mean vector for state <italic>k, Y</italic><sup><italic>s</italic></sup> is the simulated target variable and <italic>φ</italic><sup><italic>s</italic></sup> is the 1 × <italic>M</italic> Gaussian noise vector, controlled by the scaler <italic>c</italic>. That means that the two effects (the target variable and the difference between training and test set) are represented in two separate states’ parameters. The models should be able to retrieve the variable of interest in one state while ignoring the between-group noise affecting another state.</p>
<p>We then simulate individual subjects’ timeseries for 50 training subjects and 50 test subjects, varying the values for <italic>b</italic> (controlling the level of between-group difference) in the range <italic>b</italic>= [0.1,0.3, 0.5, 0.7, 0.9] and <italic>c</italic> (controlling the level of noise on the target variable <italic>Y</italic>) in the range <italic>c</italic> = [0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9,]. We then use two different training schemes: the first one where we train the HMM on all subjects (training and test set) before constructing the features (training: together) and the second one where we train the HMM only on the training subjects (training: separate). For the Fisher kernel, that means that the features for the subjects in the test set will be computed in reference to a group-level HMM which they are either part of (training: together) or not a part of (training: separate). We then use the same pipeline described above to predict the simulated target variables from the linear naïve kernel, the linear naïve normalised kernel, and the linear Fisher kernel.</p>
</sec>
</sec>
<sec id="s4g">
<label>4.7</label>
<title>Implementation</title>
<p>The code used in this study was implemented in Matlab and is publicly available within the repository of the HMM-MAR toolbox at <ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/HMM-MAR">https://github.com/OHBA-analysis/HMM-MAR</ext-link>. A Python-version of the Fisher kernel is also available at <ext-link ext-link-type="uri" xlink:href="https://github.com/vidaurre/glhmm">https://github.com/vidaurre/glhmm</ext-link>. Code for Ridge Regression and Elastic Net prediction from time-averaged FC features is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/vidaurre/NetsPredict/blob/master/nets_predict5.m">https://github.com/vidaurre/NetsPredict/blob/master/nets_predict5.m</ext-link>. The procedure for Selected Edges time-averaged FC prediction is described in detail in <xref ref-type="bibr" rid="c42">Shen et al. (2017)</xref> and code is provided at <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/bioimagesuite/behavioralprediction.m">https://www.nitrc.org/projects/bioimagesuite/behavioralprediction.m</ext-link>. All code used in this paper, including scripts to reproduce the figures and additional application examples of the Fisher Kernel can be found in the repository <ext-link ext-link-type="uri" xlink:href="https://github.com/ahrends/FisherKernel">https://github.com/ahrends/FisherKernel</ext-link>.</p>
</sec>
</sec>

</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>Data were provided by the Human Connectome Project, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University. DV is supported by a Novo Nordisk Foundation Emerging Investigator Fellowship (NNF19OC-0054895) and an ERC Starting Grant (ERC-StG-2019-850404). This research was funded in part by the Wellcome Trust (215573/Z/19/Z). For the purpose of Open Access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission. We thank Ben Griffin and Steve Smith for useful discussions and technical collaboration.</p>
</ack>
<sec id="d1e2542" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>Author contributions</title>
<p>C Ahrends: Conceptualization, Methodology, Software, Formal analysis, Investigation, Writing – original draft, Visualization. MW Woolrich: Methodology, Writing – review &amp; editing. D Vidaurre: Conceptualization, Methodology, Software, Investigation, Writing – review &amp; editing, Supervision, Funding acquisition.</p>
</sec>
</sec>
<sec id="suppd1e2542" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e2533">
<label>Supplementary Material</label>
<media xlink:href="supplements/530638_file02.docx"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ahrends</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Stevner</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pervaiz</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Kringelbach</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Vuust</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Data and model considerations for estimating time-varying functional connectivity in fMRI</article-title>. <source>Neuroimage</source>, <volume>252</volume>, <fpage>119026</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119026</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Alonso</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Towards stability of dynamic FC estimates in neuroimaging and electrophysiology: solutions and limits</article-title>. <source>bioRxiv</source>, 2023.2001.2018.524539. <pub-id pub-id-type="doi">10.1101/2023.01.18.524539</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barachant</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bonnet</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Congedo</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Jutten</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Classification of covariance matrices using a Riemannian-based kernel for BCI applications</article-title>. <source>Neurocomputing</source>, <volume>112</volume>, <fpage>172</fpage>–<lpage>178</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2012.12.039</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baum</surname>, <given-names>L. E.</given-names></string-name>, &amp; <string-name><surname>Eagon</surname>, <given-names>J. A.</given-names></string-name></person-group> (<year>1967</year>). <article-title>An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology</article-title>. <source>Bulletin of the American Mathematical Society</source>, <volume>73</volume>(<issue>3</issue>), <fpage>360</fpage>-<lpage>363</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baum</surname>, <given-names>L. E.</given-names></string-name>, &amp; <string-name><surname>Petrie</surname>, <given-names>T.</given-names></string-name></person-group> (<year>1966</year>). <article-title>Statistical Inference for Probabilistic Functions of Finite State Markov Chains</article-title>. <source>The Annals of Mathematical Statistics</source>, <volume>37</volume>(<issue>6</issue>), <fpage>1554</fpage>-<lpage>1563</lpage>, 1510. <pub-id pub-id-type="doi">10.1214/aoms/1177699147</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Modelling with independent components</article-title>. <source>Neuroimage</source>, <volume>62</volume>(<issue>2</issue>), <fpage>891</fpage>–<lpage>901</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.020</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Mackay</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Filippini</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Group comparison of resting-state FMRI data using multi-subject ICA and dual regression</article-title>. <source>Neuroimage</source>, <volume>47</volume>, <fpage>S148</fpage>. <pub-id pub-id-type="doi">10.1016/S1053-8119(09)71511-3</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Bouckaert</surname>, <given-names>R. R.</given-names></string-name>, &amp; <string-name><surname>Frank</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Evaluating the replicability of significance tests for comparing learning algorithms</article-title>. <conf-name>Pacific-Asia conference on knowledge discovery and data mining</conf-name>,</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Breakspear</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Dynamic models of large-scale brain activity</article-title>. <source>Nat Neurosci</source>, <volume>20</volume>(<issue>3</issue>), <fpage>340</fpage>–<lpage>352</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4497</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Breiman</surname>, <given-names>L.</given-names></string-name></person-group> (<year>1996</year>). <article-title>Bagging predictors</article-title>. <source>Machine Learning</source>, <volume>24</volume>(<issue>2</issue>), <fpage>123</fpage>–<lpage>140</lpage>. <pub-id pub-id-type="doi">10.1007/BF00058655</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calhoun</surname>, <given-names>Vince D.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Pearlson</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Adalı</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2014</year>). <article-title>The Chronnectome: Time-Varying Connectivity Networks as the Next Frontier in fMRI Data Discovery</article-title>. <source>Neuron</source>, <volume>84</volume>(<issue>2</issue>), <fpage>262</fpage>–<lpage>274</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2014.10.015</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cole</surname>, <given-names>J. H.</given-names></string-name>, &amp; <string-name><surname>Franke</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Predicting Age Using Neuroimaging: Innovative Brain Ageing Biomarkers</article-title>. <source>Trends Neurosci</source>, <volume>40</volume>(<issue>12</issue>), <fpage>681</fpage>–<lpage>690</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2017.10.001</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Denissen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Engemann</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>De Cock</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Costers</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Baijot</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Laton</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Penner</surname>, <given-names>I. K.</given-names></string-name>, <string-name><surname>Grothe</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kirsch</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>D’Hooghe</surname> <given-names>M B.</given-names></string-name>,, <string-name><surname>D’Haeseleer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dive</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>De Mey</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Van Schependom</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sima</surname>, <given-names>D. M.</given-names></string-name>, &amp; <string-name><surname>Nagels</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Brain age as a surrogate marker for cognitive performance in multiple sclerosis</article-title>. <source>Eur J Neurol</source>, <volume>29</volume>(<issue>10</issue>), <fpage>3039</fpage>–<lpage>3049</lpage>. <pub-id pub-id-type="doi">10.1111/ene.15473</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Do</surname>, <given-names>M. N.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Fast approximation of Kullback-Leibler distance for dependence trees and hidden Markov models [Article]</article-title>. <source>IEEE Signal Processing Letters</source>, <volume>10</volume>(<issue>4</issue>), <fpage>115</fpage>–<lpage>118</lpage>. <pub-id pub-id-type="doi">10.1109/LSP.2003.809034</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engemann</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Kozynets</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Sabbagh</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lemaître</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Liem</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Combining magnetoencephalography with magnetic resonance imaging enhances learning of surrogate-biomarkers</article-title>. <source>eLife</source>, <volume>9</volume>, <elocation-id>e54055</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.54055</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fox</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>A. Z.</given-names></string-name>, <string-name><surname>Vincent</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Corbetta</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name>, &amp; <string-name><surname>Raichle</surname>, <given-names>M. E.</given-names></string-name></person-group> (<year>2005</year>). <article-title>The human brain is intrinsically organized into dynamic, anticorrelated functional networks [Article]</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>102</volume>(<issue>27</issue>), <fpage>9673</fpage>–<lpage>9678</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0504136102</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Sotiropoulos</surname>, <given-names>S. N.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Coalson</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Fischl</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Andersson</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Jbabdi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Webster</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Polimeni</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, &amp; <collab>Consortium, W. U.-M. H</collab></person-group>. (<year>2013</year>). <article-title>The minimal preprocessing pipelines for the Human Connectome Project</article-title>. <source>Neuroimage</source>, <volume>80</volume>, <fpage>105</fpage>–<lpage>124</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.127</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gohil</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Roberts</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Timms</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Skates</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Higgins</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Quinn</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pervaiz</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>van Amersfoort</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Notin</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Gal</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Adaszewski</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Woolrich</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Mixtures of large-scale dynamic functional brain network modes</article-title>. <source>Neuroimage</source>, <volume>263</volume>, <fpage>119595</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119595</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gönen</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Alpaydın</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Multiple kernel learning algorithms</article-title>. <source>J Mach Learn Res</source>, <volume>12</volume>, <fpage>2211</fpage>–<lpage>2268</lpage>. <pub-id pub-id-type="doi">10.5555/1953048.2021071</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Griffanti</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Salimi-Khorshidi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Auerbach</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Douaud</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Sexton</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Zsoldos</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ebmeier</surname>, <given-names>K. P.</given-names></string-name>, <string-name><surname>Filippini</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mackay</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Moeller</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Yacoub</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Baselli</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Ugurbil</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>K. L.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name></person-group> (<year>2014</year>). <article-title>ICA-based artefact removal and accelerated fMRI acquisition for improved resting state network imaging</article-title>. <source>Neuroimage</source>, <volume>95</volume>, <fpage>232</fpage>–<lpage>247</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.03.034</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Griffin</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ahrends</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gohil</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Alfaro-Almagro</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Stacking models of brain dynamics to improve prediction of subject traits in fMRI</article-title>. <source>Imaging Neuroscience</source>, <volume>2</volume>, <fpage>1</fpage>–<lpage>22</lpage>. <pub-id pub-id-type="doi">10.1162/imag_a_00267</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Gumbel</surname>, <given-names>E. J.</given-names></string-name></person-group> (<year>1958</year>). <source>Statistics of Extremes</source>. <publisher-name>Columbia University Press</publisher-name>. <pub-id pub-id-type="doi">10.7312/gumb92958</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hutchison</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Womelsdorf</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Allen</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Bandettini</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Calhoun</surname>, <given-names>V. D.</given-names></string-name>, <string-name><surname>Corbetta</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Della Penna</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Duyn</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Glover</surname>, <given-names>G. H.</given-names></string-name>, <string-name><surname>Gonzalez-Castillo</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Handwerker</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Keilholz</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kiviniemi</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Leopold</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>de Pasquale</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Walter</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Chang</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Dynamic functional connectivity: Promise, issues, and interpretations [Article]</article-title>. <source>Neuroimage</source>, <volume>80</volume>, <fpage>360</fpage>–<lpage>378</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.079</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Jaakkola</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Diekhans</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Haussler</surname>, <given-names>D.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Using the Fisher kernel method to detect remote protein homologies</article-title>. <conf-name>Proc Int Conf Intell Syst Mol Biol</conf-name>, <fpage>149</fpage>–<lpage>158</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Jaakkola</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Haussler</surname>, <given-names>D.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Exploiting Generative Models in Discriminative Classifiers</article-title>. <conf-name>NIPS</conf-name>,</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Jayasumana</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hartley</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Salzmann</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Harandi</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Kernel methods on the Riemannian manifold of symmetric positive definite matrices</article-title>. <conf-name>proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>,</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kapoor</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Narayanan</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Leakage and the reproducibility crisis in machine-learning-based science</article-title>. <source>Patterns</source>, <volume>4</volume>(<issue>9</issue>). <pub-id pub-id-type="doi">10.1016/j.patter.2023.100804</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liégeois</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Laumann</surname>, <given-names>T. O.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>A. Z.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Yeo</surname>, <given-names>B. T. T.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Interpreting temporal fluctuations in resting-state functional connectivity MRI</article-title>. <source>Neuroimage</source>, <volume>163</volume>, <fpage>437</fpage>–<lpage>455</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.09.012</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liegeois</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kong</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Orban</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Van De Ville</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ge</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Sabuncu</surname>, <given-names>M. R.</given-names></string-name>, &amp; <string-name><surname>Yeo</surname>, <given-names>B. T. T.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Resting brain dynamics at different timescales capture distinct aspects of human behavior</article-title>. <source>Nat Commun</source>, <volume>10</volume>(<issue>1</issue>), <fpage>2317</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-10317-7</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lurie</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Kessler</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bassett</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Betzel</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Breakspear</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kheilholz</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kucyi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Liégeois</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Lindquist</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>McIntosh</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Poldrack</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Shine</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Thompson</surname>, <given-names>W. H.</given-names></string-name>, <string-name><surname>Bielczyk</surname>, <given-names>N. Z.</given-names></string-name>, <string-name><surname>Douw</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kraft</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>R. L.</given-names></string-name>, <string-name><surname>Muthuraman</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pasquini</surname>, <given-names>L.</given-names></string-name>, <etal>…</etal> <string-name><surname>Calhoun</surname>, <given-names>V. D.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Questions and controversies in the study of time-varying functional connectivity in resting fMRI</article-title>. <source>Network Neuroscience</source>, <volume>4</volume>(<issue>1</issue>), <fpage>30</fpage>–<lpage>69</lpage>. <pub-id pub-id-type="doi">10.1162/netn_a_00116</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>MacKay</surname>, <given-names>D. J. C.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>D. J. C. M.</given-names></string-name>, &amp; <string-name><surname>Press</surname>, <given-names>C. U.</given-names></string-name></person-group> (<year>2003</year>). <source>Information Theory, Inference and Learning Algorithms</source>. <publisher-name>Cambridge University Press</publisher-name>. <ext-link ext-link-type="uri" xlink:href="https://books.google.dk/books?id=AKuMj4PN_EMC">https://books.google.dk/books?id=AKuMj4PN_EMC</ext-link></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marquand</surname>, <given-names>A. F.</given-names></string-name>, <string-name><surname>Rezek</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Buitelaar</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Understanding Heterogeneity in Clinical Cohorts Using Normative Models: Beyond Case-Control Studies</article-title>. <source>Biological psychiatry</source>, <volume>80</volume>(<issue>7</issue>), <fpage>552</fpage>–<lpage>561</lpage>. <pub-id pub-id-type="doi">10.1016/j.biopsych.2015.12.023</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mwangi</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Tian</surname>, <given-names>T. S.</given-names></string-name>, &amp; <string-name><surname>Soares</surname>, <given-names>J. C.</given-names></string-name></person-group> (<year>2014</year>). <article-title>A Review of Feature Reduction Techniques in Neuroimaging</article-title>. <source>Neuroinformatics</source>, <volume>12</volume>(<issue>2</issue>), <fpage>229</fpage>–<lpage>244</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-013-9204-3</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poldrack</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Huckins</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Establishment of Best Practices for Evidence for Prediction: A Review</article-title>. <source>JAMA Psychiatry</source>, <volume>77</volume>(<issue>5</issue>), <fpage>534</fpage>–<lpage>540</lpage>. <pub-id pub-id-type="doi">10.1001/jamapsychiatry.2019.3671</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rosenberg</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Finn</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Scheinost</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Papademetris</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Shen</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Constable</surname>, <given-names>R. T.</given-names></string-name>, &amp; <string-name><surname>Chun</surname>, <given-names>M. M.</given-names></string-name></person-group> (<year>2016</year>). <article-title>A neuromarker of sustained attention from whole-brain functional connectivity</article-title>. <source>Nat Neurosci</source>, <volume>19</volume>(<issue>1</issue>), <fpage>165</fpage>–<lpage>171</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4179</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rosenblatt</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Tejavibulya</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Jiang</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Noble</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Scheinost</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Data leakage inflates prediction performance in connectome-based machine learning models</article-title>. <source>Nat Commun</source>, <volume>15</volume>(<issue>1</issue>), <fpage>1829</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-024-46150-w</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Salimi-Khorshidi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Douaud</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Griffanti</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Automatic denoising of functional MRI data: combining independent component analysis and hierarchical fusion of classifiers</article-title>. <source>Neuroimage</source>, <volume>90</volume>, <fpage>449</fpage>–<lpage>468</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.11.046</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Saunders</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gammerman</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Vovk</surname>, <given-names>V.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Ridge regression learning algorithm in dual variables</article-title>. <conf-name>Proceedings of the 15th International Conference on Machine Learning</conf-name>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Schölkopf</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Smola</surname>, <given-names>A. J.</given-names></string-name>, &amp; <string-name><surname>Bach</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2002</year>). <source>Learning with kernels: support vector machines, regularization, optimization, and beyond</source>. <publisher-name>MIT press</publisher-name>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schouten</surname>, <given-names>T. M.</given-names></string-name>, <string-name><surname>Koini</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>de Vos</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Seiler</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>van der Grond</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lechner</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hafkemeijer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Möller</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>de Rooij</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Rombouts</surname>, <given-names>S. A. R. B.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Combining anatomical, diffusion, and resting state functional magnetic resonance imaging for individual classification of mild and moderate Alzheimer’s disease</article-title>. <source>NeuroImage: Clinical</source>, <volume>11</volume>, <fpage>46</fpage>–<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1016/j.nicl.2016.01.002</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Shawe-Taylor</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Cristianini</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2004</year>). <source>Kernel Methods for Pattern Analysis</source>. <publisher-name>Cambridge University Press</publisher-name>. <pub-id pub-id-type="doi">10.1017/CBO9780511809682</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Finn</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Scheinost</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Rosenberg</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Chun</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Papademetris</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Constable</surname>, <given-names>R. T.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Using connectome-based predictive modeling to predict individual behavior from brain connectivity</article-title>. <source>Nat Protoc</source>, <volume>12</volume>(<issue>3</issue>), <fpage>506</fpage>–<lpage>518</lpage>. <pub-id pub-id-type="doi">10.1038/nprot.2016.178</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Andersson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Auerbach</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Bijsterbosch</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Douaud</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Duff</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Feinberg</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Griffanti</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Harms</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Kelly</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Laumann</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Moeller</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Petersen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Power</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Salimi-Khorshidi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>A. Z.</given-names></string-name>, <string-name><surname>Vu</surname>, <given-names>A. T.</given-names></string-name>, <etal>…</etal> <collab>Consortium, W. U.-M. H</collab></person-group>. (<year>2013</year>). <article-title>Resting-state fMRI in the Human Connectome Project</article-title>. <source>Neuroimage</source>, <volume>80</volume>, <fpage>144</fpage>–<lpage>168</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.039</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Alfaro-Almagro</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name>, &amp; <string-name><surname>Miller</surname>, <given-names>K. L.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Estimation of brain age delta from brain imaging</article-title>. <source>Neuroimage</source>, <volume>200</volume>, <fpage>528</fpage>–<lpage>539</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.06.017</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>E. C.</given-names></string-name>, <string-name><surname>Salimi-Khorshidi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Barch</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Uğurbil</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Functional connectomics from resting-state fMRI</article-title>. <source>Trends Cogn Sci</source>, <volume>17</volume>(<issue>12</issue>), <fpage>666</fpage>–<lpage>682</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2013.09.016</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Snoek</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Miletić</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Scholte</surname>, <given-names>H. S.</given-names></string-name></person-group> (<year>2019</year>). <article-title>How to control for confounds in decoding analyses of neuroimaging data</article-title>. <source>Neuroimage</source>, <volume>184</volume>, <fpage>741</fpage>–<lpage>760</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.09.074</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stephan</surname>, <given-names>K. E.</given-names></string-name>, <string-name><surname>Schlagenhauf</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Huys</surname>, <given-names>Q. J. M.</given-names></string-name>, <string-name><surname>Raman</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Aponte</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Brodersen</surname>, <given-names>K. H.</given-names></string-name>, <string-name><surname>Rigoux</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Moran</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Daunizeau</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Dolan</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, &amp; <string-name><surname>Heinz</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Computational neuroimaging strategies for single patient predictions</article-title>. <source>Neuroimage</source>, <volume>145</volume>, <fpage>180</fpage>–<lpage>199</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.06.038</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vaghari</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kabir</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Henson</surname>, <given-names>R. N.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Late combination shows that MEG adds to MRI in classifying MCI versus controls</article-title>. <source>Neuroimage</source>, <volume>252</volume>, <fpage>119054</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119054</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>van der Maaten</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Learning Discriminative Fisher Kernels</article-title>. <conf-name>Proceedings of the 28th International Conference on Machine Learning</conf-name>, <publisher-loc>Bellevue, WA, USA</publisher-loc>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Barch</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Yacoub</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Ugurbil</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2013</year>). <article-title>The WU-Minn Human Connectome Project: an overview</article-title>. <source>Neuroimage</source>, <volume>80</volume>, <fpage>62</fpage>–<lpage>79</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.041</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Ugurbil</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Auerbach</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Barch</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>T. E. J.</given-names></string-name>, <string-name><surname>Bucholz</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Corbetta</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Curtiss</surname>, <given-names>S. W.</given-names></string-name>, <string-name><surname>Della Penna</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Feinberg</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Harel</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Heath</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>Larson-Prior</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Marcus</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Michalareas</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Moeller</surname>, <given-names>S.</given-names></string-name>, <collab>…Consortium, W. U.-M. H</collab></person-group>. (<year>2012</year>). <article-title>The Human Connectome Project: a data acquisition perspective</article-title>. <source>Neuroimage</source>, <volume>62</volume>(<issue>4</issue>), <fpage>2222</fpage>–<lpage>2231</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.018</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Raamana</surname>, <given-names>P. R.</given-names></string-name>, <string-name><surname>Engemann</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Hoyos-Idrobo</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schwartz</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Thirion</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Assessing and tuning brain decoders: Cross-validation, caveats, and guidelines</article-title>. <source>Neuroimage</source>, <volume>145</volume>, <fpage>166</fpage>–<lpage>179</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.10.038</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Llera</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Behavioural relevance of spontaneous, transient brain network interactions in fMRI</article-title>. <source>Neuroimage</source>, <volume>229</volume>, <fpage>117713</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117713</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Quinn</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Baker</surname>, <given-names>A. P.</given-names></string-name>, <string-name><surname>Dupret</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Tejero-Cantero</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Spectrally resolved fast transient brain states in electrophysiological data</article-title>. <source>Neuroimage</source>, <volume>126</volume>, <fpage>81</fpage>–<lpage>95</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.11.047</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Brain network dynamics are hierarchically organized in time</article-title>. <source>Proc Natl Acad Sci U S A</source>, <volume>114</volume>(<issue>48</issue>), <fpage>12827</fpage>–<lpage>12832</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1705120114</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Varol</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Sotiras</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Chand</surname>, <given-names>G. B.</given-names></string-name>, <string-name><surname>Erus</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Shou</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Abdulkadir</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hwang</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Dwyer</surname>, <given-names>D. B.</given-names></string-name>, <string-name><surname>Pigoni</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dazzan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kahn</surname>, <given-names>R. S.</given-names></string-name>, <string-name><surname>Schnack</surname>, <given-names>H. G.</given-names></string-name>, <string-name><surname>Zanetti</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>Meisenzahl</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Busatto</surname>, <given-names>G. F.</given-names></string-name>, <string-name><surname>Crespo-Facorro</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Rafael</surname>, <given-names>R. G.</given-names></string-name>,<collab>…Alzheimer’s Disease Neuroimaging, I</collab></person-group>. (<year>2022</year>). <article-title>Multi-scale semi-supervised clustering of brain images: Deriving disease subtypes</article-title>. <source>Med Image Anal</source>, <volume>75</volume>, <fpage>102304</fpage>. <pub-id pub-id-type="doi">10.1016/j.media.2021.102304</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Winkler</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Webster</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Multi-level block permutation</article-title>. <source>Neuroimage</source>, <volume>123</volume>, <fpage>253</fpage>–<lpage>268</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.05.092</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolfers</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Buitelaar</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Franke</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Marquand</surname>, <given-names>A. F.</given-names></string-name></person-group> (<year>2015</year>). <article-title>From estimating activation locality to predicting disorder: A review of pattern recognition for neuroimaging-based psychiatric diagnostics</article-title>. <source>Neurosci Biobehav Rev</source>, <volume>57</volume>, <fpage>328</fpage>–<lpage>349</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2015.08.001</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zou</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Hastie</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Regularization and Variable Selection Via the Elastic Net</article-title>. <source>Journal of the Royal Statistical Society Series B: Statistical Methodology</source>, <volume>67</volume>(<issue>2</issue>), <fpage>301</fpage>–<lpage>320</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9868.2005.00503.x</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95125.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Marquand</surname>
<given-names>Andre F</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Radboud University Nijmegen</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study combines the use of Fisher Kernels with Hidden Markov models aiming to improve brain-behaviour prediction. The evidence supporting the authors' conclusions is <bold>compelling</bold>, comparing brain-behaviour prediction accuracies across a range of different traits, including out of sample assessment. This work is timely and will be of interest to neuroscientists working on functional connectivity for brain-behaviour association.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95125.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors attempt to validate Fisher Kernels on the top of HMM as a way to better describe human brain dynamics at resting-state. The objective criterion was the better prediction of the proposed pipeline of the individual traits.</p>
<p>Comments on revisions:</p>
<p>The authors addressed adequately all my comments.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95125.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this work, the authors use a Hidden Markov Model (HMM) to describe dynamic connectivity and amplitude patterns in fMRI data, and propose to integrate these features with the Fisher kernel to improve the prediction of individual traits. The approach is tested using a large sample of healthy young adults from the Human Connectome Project. The HMM-Fisher Kernel approach was shown to achieve higher prediction accuracy with lower variance on many individual traits compared to alternate kernels and measures of static connectivity. As an additional finding, the authors demonstrate that parameters of the HMM state matrix may be more informative in predicting behavioral/cognitive variables in this data compared to state-transition probabilities.</p>
<p>Comments on revisions:</p>
<p>The authors have now addressed my comments, and I believe this work will be an interesting contribution to the literature.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95125.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ahrends</surname>
<given-names>Christine</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9287-1254</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Woolrich</surname>
<given-names>Mark W</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Vidaurre</surname>
<given-names>Diego</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>The authors attempt to validate Fisher Kernels on the top of HMM as a way to better describe human brain dynamics at resting state. The objective criterion was the better prediction of the proposed pipeline of the individual traits.</p>
<p>Strengths:</p>
<p>The authors analyzed rs-fMRI dataset from the HCP providing results also from other kernels.</p>
<p>The authors also provided findings from simulation data.</p>
<p>Weaknesses:</p>
<p>(1) The authors should explain in detail how they applied cross-validation across the dataset for both optimization of parameters, and also for cross-validation of the models to predict individual traits.</p>
</disp-quote>
<p>Indeed, there were details about the cross-validation for hyperparameter tuning and prediction missing. This problem was also raised by Reviewer #2. We have now rephrased this section in 4.4 and added details: ll. 804-813:</p>
<p>“We used k-fold nested cross-validation (CV) to select and evaluate the models. We used 10 folds for both the outer loop (used to train and test the model) and the inner loop (used to select the optimal hyperparameters) such that 90% were used for training and 10% for testing. The optimal hyperparameters λ (and τ in the case of the Gaussian kernels) were selected using grid-search from the vectors λ=[0.0001,0.001,0.01,0.1,0.3,0.5,0.7,0.9,1] and <inline-formula id="sa3equ1"><inline-graphic xlink:href="elife-95125-sa3-equ1.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula>. In both the outer and the inner loop, we accounted for family structure in the HCP dataset so that subjects from the same family were never split across folds (Winkler et al., 2015). Within the CV, we regressed out sex and head motion confounds, i.e., we estimated the regression coefficients for the confounds on the training set and applied them to the test set (Snoek et al., 2019).“ and ll. 818-820: “We generated the 100 random repetitions of the 10 outer CV folds once, and then used them for training and prediction of all methods, so that all methods were fit to the same partitions.”</p>
<disp-quote content-type="editor-comment">
<p>(2) They discussed throughout the paper that their proposed (HMM+Fisher) kernel approach outperformed dynamic functional connectivity (dFC). However, they compared the proposed methodology with just static FC.</p>
</disp-quote>
<p>We would like to clarify that the HMM is itself a method for estimating dynamic (or time-varying) FC, just like the sliding window approach, see also Vidaurre, 2024 (<ext-link ext-link-type="uri" xlink:href="https://direct.mit.edu/imag/article/doi/10.1162/imag_a_00363/124983">https://direct.mit.edu/imag/article/doi/10.1162/imag_a_00363/124983</ext-link>) for an overview of terminology.</p>
<p>See also our response to Q3.</p>
<disp-quote content-type="editor-comment">
<p>(3) If the authors wanted to claim that their methodology is better than dFC, then they have to demonstrate results based on dFC with the trivial sliding window approach.</p>
</disp-quote>
<p>We would like to be clear that we do not claim in the manuscript that our method outperforms other dynamic functional connectivity (dFC) approaches, such as sliding window FC. We have now made changes to the manuscript to make this clearer.</p>
<p>First, we have clarified our use of the term “brain dynamics” to signify “time-varying amplitude and functional connectivity patterns” in this context, as Reviewer #2 raised the point that the former term is ambiguous (ll.33-35: “One way of describing brain dynamics are state-space models, which allow capturing recurring patterns of activity and functional connectivity (FC) across the whole brain.”).</p>
<p>Second, our focus is on our method being a way of using dFC for predictive modelling, since there currently is no widely accepted way of doing this. One reason why dFC is not usually considered in prediction studies is that it is mathematically not trivial how to use the parameters from estimators of dynamic FC for a prediction. This includes the sliding window approach. We do not aim at comparing across different dFC estimators in this paper. To make these points clearer, we have revised the introduction to now say:</p>
<p>Ll. 39-50:</p>
<p>“One reason why brain dynamics are not usually considered in this context pertains to their representation: They are represented using models of varying complexity that are estimated from modalities such as functional MRI or MEG. Although there exists a variety of methods for estimating time-varying or dynamic FC (Lurie et al., 2019), like the commonly used sliding-window approach, there is currently no widely accepted way of using them for prediction problems. This is because these models are usually parametrised by a high number of parameters with complex mathematical relationships between the parameters that reflect the model assumptions. How to leverage these parameters for prediction is currently an open question.</p>
<p>We here propose the Fisher kernel for predicting individual traits from brain dynamics, using information from generative models that do not assume any knowledge of task timings. We focus on models of brain dynamics that capture within-session changes in functional connectivity and amplitude from fMRI scans, in this case acquired during wakeful rest, and how the parameters from these models can be used to predict behavioural variables or traits. In particular, we use the Hidden Markov Model (HMM), which is a probabilistic generative model of time-varying amplitude and functional connectivity (FC) dynamics (Vidaurre et al., 2017).”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary:</p>
<p>The manuscript presents a valuable investigation into the use of Fisher Kernels for extracting representations from temporal models of brain activity, with the aim of improving regression and classification applications. The authors provide solid evidence through extensive benchmarks and simulations that demonstrate the potential of Fisher Kernels to enhance the accuracy and robustness of regression and classification performance in the context of functional magnetic resonance imaging (fMRI) data. This is an important achievement for the neuroimaging community interested in predictive modeling from brain dynamics and, in particular, state-space models.</p>
<p>Strengths:</p>
<p>(1) The study's main contribution is the innovative application of Fisher Kernels to temporal brain activity models, which represents a valuable advancement in the field of human cognitive neuroimaging.</p>
<p>(2) The evidence presented is solid, supported by extensive benchmarks that showcase the method's effectiveness in various scenarios.</p>
<p>(3) Model inspection and simulations provide important insights into the nature of the signal picked up by the method, highlighting the importance of state rather than transition probabilities.</p>
<p>(4) The documentation and description of the methods are solid including sufficient mathematical details and availability of source code, ensuring that the study can be replicated and extended by other researchers.</p>
<p>Weaknesses:</p>
<p>(1) The generalizability of the findings is currently limited to the young and healthy population represented in the Human Connectome Project (HCP) dataset. The potential of the method for other populations and modalities remains to be investigated.</p>
</disp-quote>
<p>As suggested by the reviewer, we have added a limitations paragraph and included a statement about the dataset: Ll. 477-481: “The fMRI dataset we used (HCP 1200 Young Adult) is a large sample taken from a healthy, young population, and it remains to be shown how our findings generalise to other datasets, e.g. other modalities such as EEG/MEG, clinical data, older populations, different data quality, or smaller sample sizes both in terms of the number of participants and the scanning duration”.</p>
<p>We would like to emphasise that this is a methodological contribution, rather than a basic science investigation about cognition and brain-behaviour associations. Therefore, the method would be equally usable on different populations, even if the results vary.</p>
<disp-quote content-type="editor-comment">
<p>(2) The possibility of positivity bias in the HMM, due to the use of a population model before cross-validation, needs to be addressed to confirm the robustness of the results.</p>
</disp-quote>
<p>As pointed out by both Reviewers #2 and #3, we did not separate subjects into training and test set before fitting the HMM. To address this issue, we have now repeated the predictions for HMMs fit only to the training subjects. We show that this has no effect on the results. Since this question has consequences for the Fisher kernel, we have also added simulations showing how the different kernels react to increasing heterogeneity between training and test set. These new results are added as results section 2.4 (ll. 376-423).</p>
<disp-quote content-type="editor-comment">
<p>(3) The statistical significance testing might be compromised by incorrect assumptions about the independence between cross-validation distributions, which warrants further examination or clearer documentation.</p>
</disp-quote>
<p>We have now replaced the significance testing with repeated k-fold cross-validated corrected tests. Note that this required re-running the models to be able to test differences in accuracies on the level of individual folds, resulting in different plots throughout the manuscript and different statistical results. This does not, however, change the main conclusions of our manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(4) The inclusion of the R^2 score, sensitive to scale, would provide a more comprehensive understanding of the method's performance, as the Pearson correlation coefficient alone is not standard in machine learning and may not be sufficient (even if it is common practice in applied machine learning studies in human neuroimaging).</p>
</disp-quote>
<p>We have now added the coefficient of determination to the results figures.</p>
<disp-quote content-type="editor-comment">
<p>(5) The process for hyperparameter tuning is not clearly documented in the methods section, both for kernel methods and the elastic net.</p>
</disp-quote>
<p>As mentioned above in the response to Reviewer #1, we have now added details about hyperparameter tuning for the kernel methods and the non-kernelised static FC regression models (see also Reviewer #1 comment 1): Ll.804-813: “We used k-fold nested cross-validation (CV) to select and evaluate the models. We used 10 folds for both the outer loop (used to train and test the model) and the inner loop (used to select the optimal hyperparameters) such that 90% were used for training and 10% for testing. The optimal hyperparameters  (and  in the case of the Gaussian kernels) were selected using grid-search from the vectors λ=[0.0001,0.001,0.01,0.1,0.3,0.5,0.7,0.9,1] and <inline-formula id="sa3equ2"><inline-graphic xlink:href="elife-95125-sa3-equ2.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula>. In both the outer and the inner loop, we accounted for family structure in the HCP dataset so that subjects from the same family were never split across folds (Winkler et al., 2015). Within the CV, we regressed out sex and head motion confounds, i.e., we estimated the regression coefficients for the confounds on the training set and applied them to the test set (Snoek et al., 2019).” and ll. 818-820: “We generated the 100 random repetitions of the 10 outer CV folds once, and then used them for training and prediction of all methods, so that all methods were fit to the same partitions.”, as well as ll.913-917: “All time-averaged FC models are fitted using the same (nested) cross-validation strategy as described above (10-fold CV using the outer loop for model evaluation and the inner loop for model selection using grid-search for hyperparameter tuning, accounting for family structure in the dataset, and repeated 100 times with randomised folds).”</p>
<disp-quote content-type="editor-comment">
<p>(6) For the time-averaged benchmarks, a comparison with kernel methods using metrics defined on the Riemannian SPD manifold, such as employing the Frobenius norm of the logarithm map within a Gaussian kernel, would strengthen the analysis, cf. Jayasumana (<ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.4172">https://arxiv.org/abs/1412.4172</ext-link>) Table 1, log-euclidean metric.</p>
</disp-quote>
<p>We have now added the log-Euclidean Gaussian kernel proposed by the reviewer to the model comparisons. The additional model does not change our conclusions.</p>
<disp-quote content-type="editor-comment">
<p>(7) A more nuanced and explicit discussion of the limitations, including the reliance on HCP data, lack of clinical focus, and the context of tasks for which performance is expected to be on the low end (e.g. cognitive scores), is crucial for framing the findings within the appropriate context.</p>
</disp-quote>
<p>We have now revised the discussion section and added an explicit limitations paragraph: Ll. 475-484:</p>
<p>“We here aimed to show the potential of the HMM-Fisher kernel approach to leverage information from patterns of brain dynamics to predict individual traits in an example fMRI dataset as well as simulated data. The fMRI dataset we used (HCP 1200 Young Adult) is a large sample taken from a healthy, young population, and it remains to be shown how the exhibited performance generalises to other datasets, e.g. other modalities such as EEG/MEG, clinical data, older populations, different data quality, or smaller sample sizes both in terms of the number of participants and the scanning duration. Additionally, we only tested our approach for the prediction of a specific set of demographic items and cognitive scores; it may be interesting to test the framework in also on clinical variables, such as the presence of a disease or the response to pharmacological treatment.”</p>
<disp-quote content-type="editor-comment">
<p>(8) While further benchmarks could enhance the study, the authors should provide a critical appraisal of the current findings and outline directions for future research, considering the scope and budget constraints of the work.</p>
</disp-quote>
<p>In addition to the new limitations paragraph (see previous comment), we have now rephrased our interpretation of the results and extended the outlook paragraph: Ll. 485-507:</p>
<p>“There is growing interest in combining different data types or modalities, such as structural, static, and dynamic measures, to predict phenotypes (Engemann et al., 2020; Schouten et al., 2016). While directly combining the features from each modality can be problematic, modality-specific kernels, such as the Fisher kernel for time-varying amplitude and/or FC, can be easily combined using approaches such as stacking (Breiman, 1996) or Multi Kernel Learning (MKL) (Gönen &amp; Alpaydın, 2011). MKL can improve prediction accuracy of multimodal studies (Vaghari et al., 2022), and stacking has recently been shown to be a useful framework for combining static and time-varying FC predictions (Griffin et al., 2024). A detailed comparison of different multimodal prediction strategies including kernels for time-varying amplitude/FC may may be the focus of future work.</p>
<p>In a clinical context, while there are nowadays highly accurate biomarkers and prognostics for many diseases, others, such as psychiatric diseases, remain poorly understood, diagnosed, and treated. Here, improving the description of individual variability in brain measures may have potential benefits for a variety of clinical goals, e.g., to diagnose or predict individual patients’ outcomes, find biomarkers, or to deepen our understanding of changes in the brain related to treatment responses like drugs or non-pharmacological therapies (Marquand et al., 2016; Stephan et al., 2017; Wen et al., 2022; Wolfers et al., 2015). However, the focus so far has mostly been on static or structural information, leaving the potentially crucial information from brain dynamics untapped. Our proposed approach provides one avenue of addressing this by leveraging individual patterns of time-varying amplitude and FC, and it can be flexibly modified or extended to include, e.g., information about temporally recurring frequency patterns (Vidaurre et al., 2016).”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Summary:</p>
<p>In this work, the authors use a Hidden Markov Model (HMM) to describe dynamic connectivity and amplitude patterns in fMRI data, and propose to integrate these features with the Fisher Kernel to improve the prediction of individual traits. The approach is tested using a large sample of healthy young adults from the Human Connectome Project. The HMM-Fisher Kernel approach was shown to achieve higher prediction accuracy with lower variance on many individual traits compared to alternate kernels and measures of static connectivity. As an additional finding, the authors demonstrate that parameters of the HMM state matrix may be more informative in predicting behavioral/cognitive variables in this data compared to state-transition probabilities.</p>
<p>Strengths:</p>
<p>- Overall, this work helps to address the timely challenge of how to leverage high-dimensional dynamic features to describe brain activity in individuals.</p>
<p>- The idea to use a Fisher Kernel seems novel and suitable in this context.</p>
<p>- Detailed comparisons are carried out across the set of individual traits, as well as across models with alternate kernels and features.</p>
<p>- The paper is well-written and clear, and the analysis is thorough.</p>
<p>Potential weaknesses:</p>
<p>- One conclusion of the paper is that the Fisher Kernel &quot;predicts more accurately than other methods&quot; (Section 2.1 heading). I was not certain this conclusion is fully justified by the data presented, as it appears that certain individual traits may be better predicted by other approaches (e.g., as shown in Figure 3) and I found it hard to tell if certain pairwise comparisons were performed -- was the linear Fisher Kernel significantly better than the linear Naive normalized kernel, for example?</p>
</disp-quote>
<p>We have revised the abstract and the discussion to state the results more appropriately. For instance, we changed the relevant section in the abstract to (ll. 24-26):</p>
<p>“We show here, in fMRI data, that the HMM-Fisher kernel approach is accurate and reliable. We compare the Fisher kernel to other prediction methods, both time-varying and time-averaged functional connectivity-based models.”,</p>
<p>and in the discussion, removing the sentence</p>
<p>“resulting in better generalisability and interpretability compared to other methods”,</p>
<p>and adding (given the revised statistical results) ll. 435-436:</p>
<p>“though most comparisons were not statistically significant given the narrow margin for improvements.”</p>
<p>In conjunction with the new statistical approach (see Reviewer #2, comment 3), we have now streamlined the comparisons. We explained which comparisons were performed in the methods ll.880-890:</p>
<p>“For the main results, we separately compare the linear Fisher kernel to the other linear kernels, and the Gaussian Fisher kernel to the other Gaussian kernels, as well as to each other. We also compare the linear Fisher kernel to all time-averaged methods. Finally, to test for the effect of tangent space projection for the time-averaged FC prediction, we also compare the Ridge regression model to the Ridge Regression in Riemannian space. To test for effects of removing sets of features, we use the approach described above to compare the kernels constructed from the full feature sets to their versions where features were removed or reduced. Finally, to test for effects of training the HMM either on all subjects or only on the subjects that were later used as training set, we compare each kernel to the corresponding kernel constructed from HMM parameters, where training and test set were kept separate.“</p>
<p>Model performance evaluation is done on the level of all predictions (i.e., across target variables, CV folds, and CV iterations) rather than for each of the target variables separately. That means different best-performing methods depending on the target variables are to be expected.</p>
<disp-quote content-type="editor-comment">
<p>- While 10-fold cross-validation is used for behavioral prediction, it appears that data from the entire set of subjects is concatenated to produce the initial group-level HMM estimates (which are then customized to individuals). I wonder if this procedure could introduce some shared information between CV training and test sets. This may be a minor issue when comparing the HMM-based models to one another, but it may be more important when comparing with other models such as those based on time-averaged connectivity, which are calculated separately for train/test partitions (if I understood correctly).</p>
</disp-quote>
<p>The lack of separation between training and test set before fitting the HMM was also pointed out by Reviewer #2. We are addressing this issue in the new Results section 2.4 (see also our response to Reviewer #2, comment 2).</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p>The individual public reviews all indicate the merits of the study, however, they also highlight relatively consistent questions or issues that ought to be addressed. Most significantly, the authors ought to provide greater clarity surrounding the use of the cross-validation procedures they employ, and the use of a common atlas derived outside the cross-validation loop. Also, the authors should ensure that the statistical testing procedures they employ accommodate the dependencies induced between folds by the cross-validation procedure and give care to ensuring that the conclusions they make are fully supported by the data and statistical tests they present.</p>
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>Overall, the study is interesting but demands further improvements. Below, I summarize my comments:</p>
<p>(1) The authors should explain in detail how they applied cross-validation across the dataset for both optimization of parameters, and also for cross-validation of the models to predict individual traits.</p>
<p>How did you split the dataset for both parameters optimization, and for the CV of the prediction of behavioral traits?</p>
<p>A review and a summary of various CVs that have been applied on the same dataset should be applied.</p>
</disp-quote>
<p>We apologise for the oversight and have now added more details to the CV section of the methods, see our response to Reviewer #1 comment 1:</p>
<p>In ll. 804-813:</p>
<p>“We used k-fold nested cross-validation (CV) to select and evaluate the models. We used 10 folds for both the outer loop (used to train and test the model) and the inner loop (used to select the optimal hyperparameters) such that 90% were used for training and 10% for testing. The optimal hyperparameters  (and  in the case of the Gaussian kernels) were selected using grid-search from the vectors λ=[0.0001,0.001,0.01,0.1,0.3,0.5,0.7,0.9,1] and <inline-formula id="sa3equ3"><inline-graphic xlink:href="elife-95125-sa3-equ3.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula>. In both the outer and the inner loop, we accounted for family structure in the HCP dataset so that subjects from the same family were never split across folds (Winkler et al., 2015). Within the CV, we regressed out sex and head motion confounds, i.e., we estimated the regression coefficients for the confounds on the training set and applied them to the test set (Snoek et al., 2019).“ and ll. 818-820: “We generated the 100 random repetitions of the 10 outer CV folds once, and then used them for training and prediction of all methods, so that all methods were fit to the same partitions.”</p>
<disp-quote content-type="editor-comment">
<p>(2) The authors should explain in more detail how they applied ICA-based parcellation at the group-level.</p>
<p>A. Did you apply it across the whole group? If yes, then this is problematic since it rejects the CV approach. It should be applied within the folds.</p>
<p>B. How did you define the representative time-source per ROI?</p>
</disp-quote>
<p>A: How group ICA was applied was stated in the Methods section (4.1 HCP imaging and behavioural data), ll. 543-548:</p>
<p>“The parcellation was estimated from the data using multi-session spatial ICA on the temporally concatenated data from all subjects.”</p>
<p>We have now added a disclaimer about the divide between training and test set:</p>
<p>“Note that this means that there is no strict divide between the subjects used for training and the subjects for testing the later predictive models, so that there is potential for leakage of information between training and test set. However, since this step does not concern the target variable, but only the preprocessing of the predictors, the effect can be expected to be minimal (Rosenblatt et al., 2024).”</p>
<p>We understand that in order to make sure we avoid data leakage, it would be desirable to estimate and apply group ICA separately for the folds, but the computational load of this would be well beyond the constraints of this particular work, where we have instead used the parcellation provided by the HCP consortium.</p>
<p>B: This was also stated in 4.1, ll. 554-559: “Timecourses were extracted using dual regression (Beckmann et al., 2009), where group-level components are regressed onto each subject’s fMRI data to obtain subject-specific versions of the parcels and their timecourses. We normalised the timecourses of each subject to ensure that the model of brain dynamics and, crucially, the kernels were not driven by (averaged) amplitude and variance differences between subjects.”</p>
<disp-quote content-type="editor-comment">
<p>(3) The authors discussed throughout the paper that their proposed (HMM+Fisher) kernel approach outperformed dynamic functional connectivity (dFC). However, they compared the proposed methodology with just static FC.</p>
<p>A. The authors didn't explain how static and dFC have been applied.</p>
<p>B. If the authors wanted to claim that their methodology is better than dFC, then they have to demonstrate results based on dFC with the trivial sliding window approach.</p>
<p>C. Moreover, the static FC networks have been constructed by concatenating time samples that belong to the same state across the time course of resting-state activity.</p>
<p>So, it's HMM-informed static FC analysis, which is problematic since it's derived from HMM applied over the brain dynamics.</p>
<p>I don't agree that connectivity is derived exclusively from the clustering of human brain dynamics!</p>
<p>D. A static approach of using the whole time course, and a dFC following the trivial sliding-window approach should be adopted and presented for comparison with (HMM+Fisher) kernel.</p>
</disp-quote>
<p>We do not intend to claim our manuscript that our method outperforms other methods for doing dynamic FC. Indeed, we would like to be clear that the HMM itself is a method for capturing dynamic FC. Please see our responses to public review comments 2 and 3 by reviewer #1, copied below, which is intended to clear up this misunderstanding:</p>
<p>We would like to clarify that the HMM is itself a method for estimating dynamic (or time-varying) FC, just like the sliding window approach, see also Vidaurre, 2024 (<ext-link ext-link-type="uri" xlink:href="https://direct.mit.edu/imag/article/doi/10.1162/imag_a_00363/124983">https://direct.mit.edu/imag/article/doi/10.1162/imag_a_00363/124983</ext-link>) for an overview of terminology.</p>
<p>We would like to be clear that we do not claim in the manuscript that our method outperforms other dynamic functional connectivity (dFC) approaches, such as sliding window FC. We have now made changes to the manuscript to make this clearer.</p>
<p>First, we have clarified our use of the term “brain dynamics” to signify “time-varying amplitude and functional connectivity patterns” in this context, as Reviewer #2 raised the point that the former term is ambiguous.</p>
<p>Second, our focus is on our method being a way of using dFC for predictive modelling, since there currently is no widely accepted way of doing this. One reason why dFC is not usually considered in prediction studies is that it is mathematically not trivial how to use the parameters from estimators of dynamic FC for a prediction. This includes the sliding window approach. We do not aim at comparing across different dFC estimators in this paper. To make these points clearer, we have revised the introduction to now say:</p>
<p>Ll. 39-50:</p>
<p>“One reason why brain dynamics are not usually considered in this context pertains to their representation: They are represented using models of varying complexity that are estimated from modalities such as functional MRI or MEG. Although there exists a variety of methods for estimating time-varying or dynamic FC (Lurie et al., 2019), like the commonly used sliding-window approach, there is currently no widely accepted way of using them for prediction problems. This is because these models are usually parametrised by a high number of parameters with complex mathematical relationships between the parameters that reflect the model assumptions. How to leverage these parameters for prediction is currently an open question.</p>
<p>We here propose the Fisher kernel for predicting individual traits from brain dynamics, using information from generative models that do not assume any knowledge of task timings. We focus on models of brain dynamics that capture within-session changes in functional connectivity and amplitude from fMRI scans, in this case acquired during wakeful rest, and how the parameters from these models can be used to predict behavioural variables or traits. In particular, we use the Hidden Markov Model (HMM), which is a probabilistic generative model of time-varying amplitude and functional connectivity (FC) dynamics (Vidaurre et al., 2017).”</p>
<p>To the additional points raised here:</p>
<p>A: How static and dynamic FC have been estimated is explicitly stated in the relevant Methods sections 4.2 (The Hidden Markov Model), which explains the details of using the HMM to estimate dynamic functional connectivity; and 4.5 (Regression models based on time-averaged FC features), which explains how static FC was computed.</p>
<p>B: We are not making this claim. We have now modified the Introduction to avoid further misunderstandings, as per ll. 33-36: “One way of describing brain dynamics are state-space models, which allow capturing recurring patterns of activity and functional connectivity (FC) across the whole brain.”</p>
<p>C: This is not how static FC networks were constructed; we apologise for the confusion. We also do not perform any kind of clustering. The only “HMM-informed static FC analysis” is the static FC KL divergence model to allow for a more direct comparison with the time-varying FC KL divergence model, but we have included several other static FC models (log-Euclidean, Ridge regression, Ridge regression Riem., Elastic Net, Elastic Net Riem., and Selected Edges), which do not use HMMs. This is explained in Methods section 4.5.</p>
<p>D: As explained above, we have included four (five in the revised manuscript) static approaches using the whole time course, and we do not claim that our method outperforms other dynamic FC models. We also disagree that using the sliding window approach for predictive modelling is trivial, as explained in the introduction of the manuscript and under public review comment 3.</p>
<disp-quote content-type="editor-comment">
<p>(4) Did you correct for multiple comparisons across the various statistical tests?</p>
</disp-quote>
<p>All statistical comparisons have been corrected for multiple comparisons. Please find the relevant text in Methods section 4.4.1.</p>
<disp-quote content-type="editor-comment">
<p>(5) Do we expect that behavioral traits are encapsulated in resting-state human brain dynamics, and on which brain areas mostly? Please, elaborate on this.</p>
</disp-quote>
<p>While this is certainly an interesting question, our paper is a methodological contribution about how to predict from models of brain dynamics, rather than a basic science study about the relation between resting-state brain dynamics and behaviour. The biological aspects and interpretation of the specific brain-behaviour associations are a secondary point and out of scope for this paper. Our approach uses whole-brain dynamics, which does not require selecting brain areas of interest.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>Beyond the general principles included in the public review, here are a few additional pointers to minor issues that I would wish to see addressed.</p>
<p>Introduction:</p>
<p>- The term &quot;brain dynamics&quot; encompasses a broad spectrum of phenomena, not limited to those captured by state-space models. It includes various measures such as time-averaged connectivity and mean EEG power within specific frequency bands. To ensure clarity and relevance for a diverse readership, it would be beneficial to adopt a more inclusive and balanced approach to the terminology used.</p>
</disp-quote>
<p>The reviewer rightly points out the ambiguity of the term “brain dynamics”, which we use in the interest of readability. The HMM is one of several possible descriptions of brain dynamics. We have now included a statement early in the introduction to narrow this down:</p>
<p>Ll. 32-35:</p>
<p>“… the patterns in which brain activity unfolds over time, i.e., brain dynamics. One way of describing brain dynamics are state-space models, which allow capturing recurring patterns of activity and functional connectivity (FC) across the whole brain.”</p>
<p>And ll. 503-507:</p>
<p>“Our proposed approach provides one avenue of addressing this by leveraging individual patterns of time-varying amplitude and FC, as one of many possible descriptions of brain dynamics, and it can be flexibly modified or extended to include, e.g., information about temporally recurring frequency patterns (Vidaurre et al., 2016).”</p>
<disp-quote content-type="editor-comment">
<p>Figures:</p>
<p>- The font sizes across the figures, particularly in subpanels 2B and 2C, are quite small and may challenge readability. It is advisable to standardize the font sizes throughout all figures to enhance legibility.</p>
</disp-quote>
<p>We have slightly increased the overall font sizes, while we are generally following figure recommendations set out by Nature. The font sizes are the same throughout the figures.</p>
<disp-quote content-type="editor-comment">
<p>- When presenting performance comparisons, a horizontal layout is often more intuitive for readers, as it aligns with the natural left-to-right reading direction. This is not just a personal preference; it is supported by visualization best practices as outlined in resources like the NVS Cheat Sheet (<ext-link ext-link-type="uri" xlink:href="https://github.com/GraphicsPrinciples/CheatSheet/blob/master/NVSCheatSheet.pdf">https://github.com/GraphicsPrinciples/CheatSheet/blob/master/NVSCheatSheet.pdf</ext-link>) and Kieran Healy's book (<ext-link ext-link-type="uri" xlink:href="https://socviz.co/lookatdata.html">https://socviz.co/lookatdata.html</ext-link>).</p>
</disp-quote>
<p>We have changed all figures to use horizontal layout, hoping that this will ease visual comparison between the different models.</p>
<disp-quote content-type="editor-comment">
<p>- In the kernel density estimation (KDE) and violin plot representations, it appears that the data displays may be truncated. It is crucial to indicate where the data distribution ends. Overplotting individual data points could provide additional clarity.</p>
</disp-quote>
<p>To avoid confusion about the data distribution in the violin plots, we have now overlaid scatter plots, as suggested by the reviewer. Overlaying the fold-level accuracies was not feasible (since this would result in ~1.5 million transparent points for a single figure), so we instead show the accuracies averaged over folds but separate for target variables and CV iterations. Only the newly added coefficient of determination plots had to be truncated, which we have noted in the figure legend.</p>
<disp-quote content-type="editor-comment">
<p>- Figure 3 could inadvertently suggest that time-varying features correspond to panel A and time-averaged features to panel B. To avoid confusion, consider reorganizing the labels at the bottom into two rows for clearer attribution.</p>
</disp-quote>
<p>We have changed the layout of the time-varying and time-averaged labels in the new version of the plots to avoid this issue.</p>
<disp-quote content-type="editor-comment">
<p>Discussion:</p>
<p>- The discussion on multimodal modeling might give the impression that it is more effective with multiple kernel learning (MKL) than with other methods. To present a more balanced view, it would be appropriate to rephrase this section. For instance, stacking, examples of which are cited in the same paragraph, has been successfully applied in practice. The text could be adjusted to reflect that Fisher Kernels via MKL adds to the array of viable options for multimodal modeling. As a side thought: additionally, a well-designed comparison between MKL and stacking methods, conducted by experts in each domain, could greatly benefit the field. In certain scenarios, it might even be demonstrated that the two approaches converge, such as when using linear kernels.</p>
</disp-quote>
<p>We would like to thank the reviewer for the suggestion about the discussion concerning multimodal modelling. We agree that there are other relevant methods that may lead to interesting future work and have now included stacking and refined the section: ll. 487-494:</p>
<p>“While directly combining the features from each modality can be problematic, modality-specific kernels, such as the Fisher kernel for time-varying amplitude and/or FC, can be easily combined using approaches such as stacking (Breiman, 1996) or Multi Kernel Learning (MKL) (Gönen &amp; Alpaydın, 2011). MKL can improve prediction accuracy of multimodal studies (Vaghari et al., 2022), and stacking has recently been shown to be a useful framework for combining static and time-varying FC predictions (Griffin et al., 2024). A detailed comparison of different multimodal prediction strategies including kernels for time-varying amplitude/FC may be the focus of future work.”</p>
<disp-quote content-type="editor-comment">
<p>- The potential clinical applications of brain dynamics extend beyond diagnosis and individual outcome prediction. They play a significant role in the context of biomarkers, including pharmacodynamics, prognostic assessments, responder analysis, and other uses. The current discussion might be misinterpreted as being specific to hidden Markov model (HMM) approaches. For diagnostic purposes, where clinical assessment or established biomarkers are already available, the need for new models may be less pressing. It would be advantageous to reframe the discussion to emphasize the potential for gaining deeper insights into changes in brain activity that could indicate therapeutic effects or improvements not captured by structural brain measures. However, this forward-looking perspective is not the focus of the current work. A nuanced revision of this section is recommended to better reflect the breadth of applications.</p>
</disp-quote>
<p>We appreciate the reviewer’s thoughtful suggestions regarding the discussion of potential clinical applications. We have included the suggestions and refined this section of the discussion: Ll. 495-507:</p>
<p>“In a clinical context, while there are nowadays highly accurate biomarkers and prognostics for many diseases, others, such as psychiatric diseases, remain poorly understood, diagnosed, and treated. Here, improving the description of individual variability in brain measures may have potential benefits for a variety of clinical goals, e.g., to diagnose or predict individual patients’ outcomes, find biomarkers, or to deepen our understanding of changes in the brain related to treatment responses like drugs or non-pharmacological therapies (Marquand et al., 2016; Stephan et al., 2017; Wen et al., 2022; Wolfers et al., 2015). However, the focus so far has mostly been on static or structural information, leaving the potentially crucial information from brain dynamics untapped. Our proposed approach provides one avenue of addressing this by leveraging individual patterns of time-varying amplitude and FC, and it can be flexibly modified or extended to include, e.g., information about temporally recurring frequency patterns (Vidaurre et al., 2016).”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<p>- I wondered if the authors could provide, within the Introduction, an intuitive description for how the Fisher Kernel &quot;preserves the structure of the underlying model of brain dynamics&quot; / &quot;preserves the mathematical structure of the underlying HMM&quot;? Providing more background may help to motivate this study to a general audience.</p>
</disp-quote>
<p>We agree that this would be helpful and have now added this to the introduction: Ll.61-67:</p>
<p>“Mathematically, the HMM parameters lie on a Riemannian manifold (the structure). This defines, for instance, the relation between parameters, such as: how changing one parameter, like the probabilities of transitioning from one state to another, would affect the fitting of other parameters, like the states’ FC. It also defines the relative importance of each parameter; for example, how a change of 0.1 in the transition probabilities would not be the same as a change of 0.1 in one edge of the states’ FC matrices.”</p>
<p>To communicate the intuition behind the concept, the idea was also illustrated in Figure 1, panel 4 by showing Euclidean distances as straight lines through a curved surface (4a, Naïve kernel), as opposed to the tangent space projection onto the curved manifold (4b, Fisher kernel).</p>
<disp-quote content-type="editor-comment">
<p>- Some clarifications regarding Figure 2a would be helpful. Was the linear Fisher Kernel significantly better than the linear Naive normalized kernel? I couldn't find whether this comparison was carried out. Apologies if I have missed it in the text. For some of the brackets indicating pairwise tests and their significance values, the start/endpoints of the bracket fall between two violins; in this case, were the results of the linear and Gaussian Fisher Kernels pooled together for this comparison?</p>
</disp-quote>
<p>We have now streamlined the statistical comparisons and avoided plotting brackets falling between two violin plots. The comparisons that were carried out are stated in the methods section 4.4.1. Please see also our response to above to Reviewer #3 public review, potential weaknesses, point 1, relevant point copied below:</p>
<p>In conjunction with the new statistical approach (see Reviewer #2, comment 3), we have now streamlined the comparisons. We explained which comparisons were performed in the methods ll.880-890:</p>
<p>“For the main results, we separately compare the linear Fisher kernel to the other linear kernels, and the Gaussian Fisher kernel to the other Gaussian kernels, as well as to each other. We also compare the linear Fisher kernel to all time-averaged methods. Finally, to test for the effect of tangent space projection for the time-averaged FC prediction, we also compare the Ridge regression model to the Ridge Regression in Riemannian space. To test for effects of removing sets of features, we use the approach described above to compare the kernels constructed from the full feature sets to their versions where features were removed or reduced. Finally, to test for effects of training the HMM either on all subjects or only on the subjects that were later used as training set, we compare each kernel to the corresponding kernel constructed from HMM parameters, where training and test set were kept separate”.</p>
<disp-quote content-type="editor-comment">
<p>- The authors may wish to include, in the Discussion, some remarks on the use of all subjects in fitting the group-level HMM and the implications for the cross-validation performance, and/or try some analysis to ensure that the effect is minor.</p>
</disp-quote>
<p>As suggested by reviewers #2 and #3, we have now performed the suggested analysis and show that fitting the group-level HMM to all subjects compared to only to the training subjects has no effect on the results. Please see our response to Reviewer #2, public review, comment 2.</p>
<disp-quote content-type="editor-comment">
<p>- The decision to use k=6 states was made here, and I wondered if the authors may include some support for this choice (e.g., based on findings from prior studies)?</p>
</disp-quote>
<p>We have now refined and extended our explanation and rationale behind the number of states: Ll. 586-594: “The number of states can be understood as the level of detail or granularity with which we describe the spatiotemporal patterns in the data, akin to a dimensionality reduction, where a small number of states will lead to a very general, coarse description and a large number of states will lead to a very detailed, fine-grained description. Here, we chose a small number of states, K=6, to ensure that the group-level HMM states are general enough to be found in all subjects, since a larger number of states increases the chances of certain states being present only in a subset of subjects. The exact number of states is less relevant in this context, since the same HMM estimation is used for all kernels.”</p>
<disp-quote content-type="editor-comment">
<p>- (minor) Abstract: &quot;structural aspects&quot; - do you mean structural connectivity?</p>
</disp-quote>
<p>With “structural aspects”, we refer to the various measures of brain structure that are used in predictive modelling. We have now specified: Ll. 14-15: “structural aspects, such as structural connectivity or cortical thickness”.</p>
</body>
</sub-article>
</article>