<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">106593</article-id>
<article-id pub-id-type="doi">10.7554/eLife.106593</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106593.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Unsupervised Representation Learning of C. elegans Poses and Behavior Sequences From Microscope Video Recordings</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6583-7360</contrib-id>
<name>
<surname>Deserno</surname>
<given-names>Maurice</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
<email>maurice.deserno@uni-koeln.de</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0917-6876</contrib-id>
<name>
<surname>Bozek</surname>
<given-names>Katarzyna</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00rcxh774</institution-id><institution>Center for Molecular Medicine Cologne (CMMC), Faculty of Medicine and University Hospital Cologne, University of Cologne</institution></institution-wrap>, <city>Cologne</city>, <country country="DE">Germany</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05mxhda18</institution-id><institution>Institute for Biomedical Informatics, Faculty of Medicine and University Hospital Cologne, University of Cologne</institution></institution-wrap>, <city>Cologne</city>, <country country="DE">Germany</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00rcxh774</institution-id><institution>Cologne Excellence Cluster on Cellular Stress Responses in Aging- Associated Diseases (CECAD), University of Cologne</institution></institution-wrap>, <city>Cologne</city>, <country country="DE">Germany</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00rcxh774</institution-id><institution>Faculty of Mathematics and Natural Sciences, University of Cologne</institution></institution-wrap>, <city>Cologne</city>, <country country="DE">Germany</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Walczak</surname>
<given-names>Aleksandra M</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>CNRS</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-04-24">
<day>24</day>
<month>04</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP106593</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-03-03">
<day>03</day>
<month>03</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-02-19">
<day>19</day>
<month>02</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.02.14.638285"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Deserno &amp; Bozek</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Deserno &amp; Bozek</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-106593-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Caenorhabditis elegans (<italic>C. elegans</italic>) is an important model system for studying molecular mechanisms in disease and aging. The nematode can be imaged in highly parallel phenotypic screens resulting in large volumes of video data of the moving worm. However converting the rich, pixel-encoded phenotypical information into meaningful, quantitative description of behavior is a challenging task. There is a range of methods for quantification of the simple body shape of <italic>C. elegans</italic> and the features of its motion. These methods however are often multi-step and fail in the case of highly coiled and self-overlapping worms. Motivated by the recent development of self-supervised deep learning methods in computer vision and natural language processing, we propose an unbiased, label-free approach to quantify worm pose and motion from video data directly. We represent worm posture and behavior as embedding vectors and visualize them in a unified embeddings space. We observe that the vector embeddings capture meaningful features describing worm shape and motion, such as the degree of body bend or the speed of movement. Importantly, using pixel values directly as input, our method captures coiled worm behaviors which are inaccessible to methods based on keypoint tracking or skeletonization. While our work focuses on <italic>C. elegans</italic>, the ability to quantify behavior directly from video data opens possibilities to study organisms without rigid skeletons whose behavior is difficult to quantify using keypoint-based approaches.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Behavior is a window to an animal’s nervous system. Precise quantification of behavior allows to determine fine phenotypic effects of genetic mutations or pharmacological interventions and, eventually, their underlying neural mechanisms. Keypoint tracking methods and motion tracking imaging systems have enabled acquiring precise information on animal posture and its change in time in natural settings<sup><xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c3">3</xref></sup>. It is however unclear how to quantitatively measure behavior of invertebrate species with flexible bodies and appendages. Organisms such as worms lack natural skeletons and hence distinct keypoints on their bodies. The shape of <italic>C. elegans</italic> is typically represented as its central body line and reduced to eigenworms<sup><xref ref-type="bibr" rid="c4">4</xref></sup> that enable quantification e.g. of the motion features and dynamics. However, this approach fails in the case of coiled or self-intersecting poses of <italic>C. elegans</italic> and current solutions apply multi-step approaches<sup><xref ref-type="bibr" rid="c5">5</xref></sup> to resolve these shapes.</p>
<p>Here we present a method for quantification of <italic>C. elegans</italic> motion based on video recordings directly. Unlike keypoint- or central body line-based approaches our method does not estimate the body structure but quantifies the behavior from the raw pixel values. Our method does not require any annotations but relies on self-supervised learning approach to learn sequence representations. This combination of self-supervision and keypoint-free pose estimation enables to forgo skeletonization and feature engineering which allows studying the full repertoire of <italic>C. elegans</italic> poses and behavior in a comprehensive manner.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Related Work</title>
<p>Previous tracking and pose estimation methods for <italic>C. elegans</italic> enabled a quantitative, automated analysis and a better understanding of its poses and behavior<sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c6">6</xref>–<xref ref-type="bibr" rid="c11">11</xref></sup>. These methods allowed to comprehensively analyze worm behavior and to better understand phenotypic effects of genetic mutations, disease, or aging.</p>
<p>Stephens et al.<sup><xref ref-type="bibr" rid="c4">4</xref></sup> tracked <italic>C. elegans</italic> in microscopy videos and approximated their pose with a curve. They found that approximately 95% of the total variance in angles along the curve is represented by four eigenvalues. Based on these findings they introduced the term <italic>eigenworms</italic> as “templates” to describe the <italic>C. elegans</italic> poses. Javer et al.<sup><xref ref-type="bibr" rid="c9">9</xref></sup> developed a widely used single- and multiworm tracking software called Tierpsy. The software segments <italic>C. elegans</italic> and estimates their outlines and the skeleton. Additionally, it computes several hand-engineered features characterizing pose and motion of an individual e.g. the 6 eigenworms<sup><xref ref-type="bibr" rid="c4">4</xref></sup>, the maximum amplitude of the skeleton’s major axis, the degree of bend of different body segments, different body size measurements (such as length and width) or the motion mode (backward or forward). Both the eigenworms quantification and Tierpsy are based on classical computer vision approaches and do not allow to quantify coiled or overlapping poses of the worm. These poses are inaccessible to these methods.</p>
<p>Several methods address the challenge of accurate estimation of coiled and (self-)intersecting poses. WormPose<sup><xref ref-type="bibr" rid="c5">5</xref></sup> is a Residual Network<sup><xref ref-type="bibr" rid="c12">12</xref></sup>(ResNet)-based method applying a multi-step approach that allows for estimating poses of coiling worms. To estimate the center line using equidistant keypoints, the method relies on video data with detected/annotated center lines (e.g. by Tierpsy) for frames prior to the occurrence of coiling behavior. The authors train their network with synthetically generated images of <italic>C. elegans</italic> to avoid time-consuming human labeling. The network learns to predict the two different centerlines resulting from different head/tail orientations. During evaluation a synthetic image is generated for each predicted centerline. By comparing the generated images to the input the best prediction is determined. Recent methods like DeepTangle by Alonso and Kirkegaard<sup><xref ref-type="bibr" rid="c13">13</xref></sup> and its extension DeepTangleCrawl<sup><xref ref-type="bibr" rid="c14">14</xref></sup> by Weheliye et al. enable robust skeletonization and tracking of <italic>C. elegans</italic> with overlaps and on a noisy background and allow better phenotypic screening. Still these methods fail when <italic>C. elegans</italic> are very tightly coiled or individuals lie parallel to each other over an extended time.</p>
</sec>
<sec id="s3">
<label>3</label>
<title>Methods</title>
<sec id="s3a">
<label>3.1</label>
<title>Datasets</title>
<p>All data we used for our experiments are publicly available on Zenodo<sup><xref ref-type="bibr" rid="c15">15</xref></sup> in the <italic>Open Worm Movement Database</italic><sup><xref ref-type="bibr" rid="c9">9</xref></sup>. The data was downloaded using a python script filtering for specific parameters such as strain. For accessing the repository, we used the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH). We created two datasets grouping genetic strains and long-term recordings of single individuals, respectively. The first dataset consists of seven genetic strains with one of them being the wild type. In the following we refer to this set as <italic>strain dataset</italic>.</p>
<p>This dataset includes a total of 165 videos of the following strains:</p>
<list list-type="bullet">
<list-item><p>N2</p></list-item>
<list-item><p>AQ2932 (nca-2(gk5)III; unc-77(gk9)IV; nzIs29[punc-17::rho-1(G14V); punc-122::GFP])</p></list-item>
<list-item><p>AQ2934 (nca-2(gk5);nzIs29[punc-17::rho-1(G14V); punc-122::GFP])</p></list-item>
<list-item><p>TQ225 (trp-1(sy690)III)</p></list-item>
<list-item><p>DG1856 (goa-1(sa734)I)</p></list-item>
<list-item><p>DA609 (npr-1(ad609)X)</p></list-item>
<list-item><p>VC731 (unc-63(ok1075)I)</p></list-item>
<list-item><p>CB1141 (cat-4(e1141)V)</p></list-item>
</list>
<p>The second set consists of 71 videos of three strains with the same individuals recorded every day for multiple days (between 15 to 24 days per individual) during their adulthood. We call this last set the <italic>aging dataset</italic>. This dataset includes following genetic strains:</p>
<list list-type="bullet">
<list-item><p>AQ2947 (CGC N2 (Bristol, UK))</p></list-item>
<list-item><p>OW940 (zgIs128[P(dat-1)::alpha-Synuclein::YFP])</p></list-item>
<list-item><p>OW956 (zgIs144[P(dat-1)::YFP])</p></list-item>
</list>
<p>The data consists of video frames with masked background. Video data was recorded with frame rates varying between 25 frames per second (fps) and 32 fps. The videos of both sets have a length of almost 15 minutes each. Using Tierpsy<sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup> we calculated features of poses and motion in all our recordings. These features together with the worm genetic strain and age represent the metadata we use for interpretation of the image and sequence representations we developed in this study.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
    <caption><title>Processing pipeline and behavior representation.</title>
    <p>(a) Processing pipeline overview. We use a large set of video data of worm genetic strains and employ a contrastive learning approach to encode individual poses of the worm directly from the video frames. We next inspect these pose embeddings using their visualization in a 3D scatter plot. The trained pose embedding network is used to embed each video frame which is next an input to the sequence embedding network. Similarly to pose embeddings, we inspect the embedding space of worm behaviors using visualization techniques and motion features quantified with Tierpsy. (b) Visualization of the strain dataset behavior embedding space colored by the underlying genetic strain. (c) Visualization of the aging dataset behavior embedding space, illustrating the behavioral change with age in the direction of the arrow moving from young (blue) to old (red).</p></caption>
<graphic xlink:href="638285v1_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s3b">
<label>3.2</label>
<title>Data pre-processing</title>
<p>The background of the <italic>C. elegans</italic> images downloaded from Zenodo<sup><xref ref-type="bibr" rid="c15">15</xref></sup> is masked with black pixels. In some images the masking contains errors with background objects not masked out. Here we apply a combination of different methods including connected components and morphological operations to filter out smaller foreground blobs and to better match the background mask to the worm shape (similar to<sup><xref ref-type="bibr" rid="c5">5</xref></sup>). As a result we remove the errors and limit the foreground to one object only - the worm (see <xref rid="fig2" ref-type="fig">Fig. 2</xref> “Data Preprocessing”). Further, we change the background mask pixel value from black (0) to gray (127). Next, we crop the foreground in the image, pad and resize it to a common image size of 128 × 128 pixels. This way, we remove excessive background pixels and center the object in the middle of the image while preserving its relative size. In the final step, we apply Principal Component Analysis (PCA) to rotate the object to a vertical orientation (<xref rid="fig2" ref-type="fig">Fig. 2</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
    <caption><title>Data preprocessing and network architecture.</title>
    <p>1) Data preprocessing pipeline: Artifacts are removed keeping only the worm as foreground object. We change the background to gray, crop the image to keep the worm centered and resize it to 128 × 128<italic>px</italic>. Finally, we rotate the worm to a vertical orientation. 2) A contrastive learning network is trained with images in random order to learn pose embeddings. 3) Using the ResNet-18 trained in (2) we embed sequences of 12 frames of moving <italic>C. elegans</italic>. Rotation information is concatenated with the encoded sequences and the last 5 frame embeddings are masked out. A Transformer-encoder learns behavior embeddings by imputing the masked sequence elements.</p></caption>
<graphic xlink:href="638285v1_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We store the degree of rotation in addition to the strain and the day of adulthood in the aging dataset as the metadata. The metadata is not used in model training but in the model interpretation and visualization. We split the dataset into train, validation and test subsets with the proportions 0.76, 0.10, 0.14 and save the video frames as PyTorch<sup><xref ref-type="bibr" rid="c17">17</xref></sup> tensors in a .pt file per subset.</p>
<p>Following the data pre-processing, we train our deep learning approach, which includes two parts. The first part consists of a contrastive learning method to represent spatial poses of <italic>C. elegans</italic> based on their images. The second part is a Transformer encoder architecture that uses the learned pose representations to predict masked parts of a spatiotemporal sequence. In the following, we describe the two parts in detail.</p>
</sec>
<sec id="s3c">
<label>3.3</label>
<title>Contrastive Learning for pose representations</title>
<p>We apply contrastive learning to learn representations of poses from <italic>C. elegans</italic> image data. It is a self-supervised approach that does not require labels. Specifically, we use a version of VICReg<sup><xref ref-type="bibr" rid="c18">18</xref></sup> adapted to our task. As backbone we chose ResNet18 over ResNet50 originally used in VICReg because of its smaller size. Our experiments suggested that the results do not improve using a larger feature extractor. We use a modified set of augmentations to ensure the network focuses on the important pose differences and learns to embed them rather than embedding the differences in e.g. lightning conditions or size of individuals. The output dimensionality was set to 64 with a hidden network dimensionality of 128.</p>
<p>To avoid having many similar poses in the training set, we subsample video frames by a factor of 10. We train the network using a batch size of 512 for 80 epochs on a NVIDIA Tesla V100-SXM2 with 32 GB of memory. As optimizer we chose AdamW<sup><xref ref-type="bibr" rid="c19">19</xref></sup> with a learning rate of 0.001. Additionally we use Cosine Annealing<sup><xref ref-type="bibr" rid="c20">20</xref></sup> as learning rate scheduler. The loss is calculated the same way as proposed by the authors of VICReg<sup><xref ref-type="bibr" rid="c18">18</xref></sup>: a weighted combination (compare with 1) of variance <italic>v</italic>, invariance <italic>s</italic> and covariance <italic>c</italic> loss with weights set to <italic>µ</italic> = 25.0, <italic>λ</italic> = 25.0 and <italic>ν</italic> = 1.0.
<disp-formula id="eqn1">
<graphic xlink:href="638285v1_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s3d">
<label>3.4</label>
<title>Transformer encoder for sequence data imputation</title>
<p>To integrate the temporal component of behavior into the learned embeddings we employ a Transformer encoder neural network architecture<sup><xref ref-type="bibr" rid="c21">21</xref>–<xref ref-type="bibr" rid="c23">23</xref></sup>. The Transformer encoder consists of a multi-head attention block and a feed-forward network. This type of architecture has been used primarily in natural language processing (NLP) (e.g. by BERT<sup><xref ref-type="bibr" rid="c22">22</xref></sup>) and was later adapted to images (e.g. in Vision Transformer<sup><xref ref-type="bibr" rid="c23">23</xref></sup>). We attach the pre-trained pose representation network (see 3.3) as backbone to the Transformer network and freeze this backbone. We add a linear projection network to the last layer of the Transformer encoder network that infers embeddings of individual poses in the sequence.</p>
<p>During training, we input 12 ordered video frames as a sequence into the pre-trained pose representation network to generate pose embeddings. Here, we downsample the videos by a factor of 5 which is sufficient to capture the worm’s motion in a smooth manner. With frame rates between ∼25 − 32 fps (see section 3.1) this results in a sequence covering between ∼2 - 1.6 seconds in real time. We store the ordered pose embeddings generated by the pose backbone as ground truth information for later evaluation.</p>
<p>Next, we construct sequences of 12 consecutive frame embeddings and attach frame rotation information generated during pre-processing. We mask the last 5 sequence elements by replacing them with zeroes (similar to<sup><xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c24">24</xref></sup>) before passing the sequence to the transformer network. We add sine-cosine positional encoding<sup><xref ref-type="bibr" rid="c21">21</xref></sup> and masked position encoding to the pose embeddings. The masked position encoding is a vector, indicating if a sequence element (frame) is masked (value 1 in the vector) or is not masked (value 0 in the vector)<sup><xref ref-type="bibr" rid="c24">24</xref></sup>. This vector is embedded and then added the same way as the positional encoding<sup><xref ref-type="bibr" rid="c25">25</xref></sup> (see <xref rid="fig2" ref-type="fig">Fig. 2</xref>). The pose embeddings together with positional and mask embeddings are the input to the transformer encoder. The Transformer network is trained to impute the missing values in the sequence. Using the linear projection network, pose embeddings and their rotations are predicted for each of the masked positions. We calculate the Mean Squared Error (MSE) loss between the embeddings generated by the pose representation network and the predictions of the linear projection network.</p>
<p>Pose representations have a dimensionality of 64 (see 3.3). The transformer uses a hidden dimensionality of 128 and consists of one encoding block and two heads. For training we use AdamW<sup><xref ref-type="bibr" rid="c19">19</xref></sup> as optimizer with a learning rate of 0.0005 for 250 epochs with a batch size of 64. The network was trained and tested on a NVIDIA Tesla V100-SXM2 with 32 GB of memory.</p>
</sec>
<sec id="s3e">
<label>3.5</label>
<title>Visualization of pose and motion embeddings</title>
<p>To inspect the <italic>C. elegans</italic> pose and sequence embeddings we use the dimensionality reduction technique Uniform Manifold Approximation and Projection (UMAP)<sup><xref ref-type="bibr" rid="c26">26</xref></sup>. By applying UMAP, we reduce the embeddings to three dimensions to visualize them as scatter plots. We used the python implementation<sup><xref ref-type="bibr" rid="c1">1</xref></sup> of UMAP with the parameters <italic>n_neighbors=30, min_dist=0</italic>.<italic>25, n_components=3</italic> and <italic>random_state=42</italic> for the pose embedding space and for the behavior sequence embedding space.</p>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Results</title>
<sec id="s4a">
<label>4.1</label>
<title>Pose representations</title>
<p>We first inspected the embeddings of individual worm poses. We project the embeddings in 3D using UMAP and inspect whether the embedding space reflects Tierpsy-based<sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup> pose features, as well as worm genetic strain. <xref rid="fig3" ref-type="fig">Figure 3a</xref> illustrates the pose embedding space of the strain datasets (see 3.1). This space shows a clear spatial ordering of poses according to their degree of bending (see <xref rid="fig3" ref-type="fig">Fig. 3a</xref>). While one end of the point cloud consists of strongly coiled worms, the opposite end clusters worms with poses close to a straight line. The points are colored according to the maximum amplitude of the bend along the worm body line. The straight poses have a low amplitude value, the more bent ones a higher one. There is a clear gradient of this value along the point cloud. However, the coiled worm shapes are missing this feature value (marked in gray color in <xref rid="fig3" ref-type="fig">Fig. 3a</xref>) as Tierpsy cannot resolve these poses<sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup>. This reveals an advantage of our approach: it allows to capture all worm poses, from straight to strongly coiled ones, in a uniform and smooth embedding space. Our approach groups coiling and bending poses together with a clear transition between them, whereas an important fraction of worm poses is not possible to quantify with skeletonization-based methods. Since our approach does not require any skeleton or keypoint estimations, it is robust against coiling and self-intersecting postures. A large number of embeddings in the gray area in <xref rid="fig3" ref-type="fig">Fig. 3a</xref> belongs to <italic>C. elegans</italic> of the AQ2934 strain.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
    <caption><title>Visualization of the pose embedding space.</title>
    <p>(a) We reduced the embedding space to 3D using UMAP and colored it with the Tierpsy <italic>max_amplitude</italic> feature. Dark gray dots indicate poses for which this feature could not be quantified using Tierpsy. There is a gradient in coloring suggesting that similar poses occupy neighboring parts of the embedding space. Example images of poses are shown with an indication of their position in the embedding space. Strongly coiled and almost straight worms occupy opposite ends of the point cloud. (b-e) Pose embedding space colored according to their eigenworm 1 to 4 values.</p></caption>
<graphic xlink:href="638285v1_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Sequence representations</title>
<p>We next trained a Transformer-based approach to embed sequences of worm postures captured in a video recording. The Transformer network takes as input sequences of pose embeddings where the second half of each sequence is masked. The network is trained to infer the masked part of the sequence as well as the rotation angle of the worm in the video. The MSE of the masked pose estimation in the strain dataset is 0.106 while of the rotation angle 0.0129, which represents an error of ∼ 20.47°. Via this self-supervised approach the network learns representations of the sequences that encode worm posture, its change in time, and the dynamics of this motion in a comprehensive manner. Similar to the pose embeddings, we visualize the embedding space of <italic>C. elegans</italic> short-term behaviors in 3D (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
    <caption><title>Behavior embedding space of the strain dataset.</title>
    <p>(a) Embedding space colored by strain. Worm images above correspond to 1<sup>st</sup>, 6<sup>th</sup> and 12<sup>th</sup> frame of three example sequences. (b) Embeddings space colored by tail tip speed and (c) head speed. Gray dots in (b) and (c) indicate sequences for which these Tierpsy features are missing.</p></caption>
<graphic xlink:href="638285v1_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>This visualization shows a clear separation of sequences of the strain AQ2934 (labeled in orange) from sequences of the other strains. This separation was also present in the pose embedding space, and reflects the frequent and heavy coiling behavior of the AQ2934 strain. Behavior sequences of strain DA609 (marked in brown) are also grouped together in the embedding space. This strain is known for aggregating and burrowing behavior<sup><xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup>. Next to the DA609 cluster is a larger area where behavior sequences of different strains mix. This likely occurs since most strains share common behaviors such as simple forward locomotion.</p>
<p>To further interpret the behavior embedding space, we colored it according to motion speed features quantified with Tierpsy (<xref rid="fig4" ref-type="fig">Fig. 4ab-c</xref>). We observed that sequences with faster movement are more frequent in the center of mass of the embedding space. This confirms our observation that crawling behavior, common to most of the strains, is located in this part of the embedding space.</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Worm behavior changes with age</title>
<p>We next inspected the behavior embedding space of the aging dataset. This dataset contains 71 individuals that were recorded over their adulthood, for time span of up to 24 days. We employed our approach to inspect which parts of the embedding space those individuals occupy as they age (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>). Young individuals appear to display a wide range of behaviors, while as they age their behavior repertoire reduces. Markedly, the patterns of aging in behavior are consistent among individuals. This can be seen in the embedding space labeled by day of adulthood (see <xref rid="fig5" ref-type="fig">Fig. 5a</xref>). Behaviors of individuals from day 1 to 10 span a wide area in the space, while embeddings for day 10 to 15 cover much more limited areas at the bottom part of the point cloud. From day 15 onward the embeddings almost only form outlying groupings. The reason for these behaviors to localize on the outside of the embedding space can be two-fold. On the one hand these individuals move slower and assume fewer different poses which differ from those of more agile younger individuals with their typical crawling/swimming locomotion and coiling behaviors. On the other hand, old individuals are not included in the strain dataset on which the Transformer was trained. Although the strain dataset and the aging dataset were recorded in similar ways, the behaviors of older individuals were never seen by the network.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
    <caption><title>Behavior embedding space of the strain and aging datasets combined.</title>
    <p>(a) Embedding space colored by age. With age we refer to the day of adulthood of an individual <italic>C. elegans</italic>. Gray color indicates missing age data of worms from the strain dataset. (b) Behaviors of one individual linked over the course of its aging. Starting with blue at the last day of the L4 stage, progressing to red until the last recorded day (23) of adulthood.</p></caption>
<graphic xlink:href="638285v1_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Additionally to the age color-coded embedding space, we plotted the trajectory in the embedding space of one individual of strain AQ2947 (<xref rid="fig5" ref-type="fig">Fig. 5b</xref>). This trajectory links behaviors of this worm as it ages. It illustrates the broad variety of the behavior of this individual up to day 15 after which the its behaviors are limited to the bottom part of the embedding space.</p>
</sec>
</sec>
<sec id="s5">
<label>5</label>
<title>Discussion</title>
<p>In this work, we presented a deep learning-based approach for representation learning of <italic>C. elegans</italic> poses and behavior sequences from bright-field microscopy videos without human annotations. Our method uses a combination of Contrastive Learning and a Transformer architecture originally developed for self-supervised learning in computer vision and NLP<sup><xref ref-type="bibr" rid="c22">22</xref></sup>. We draw inspiration from these methods to demonstrate that the pose and motion of <italic>C. elegans</italic> can be quantified in a meaningful manner without the use of labels. Contrary to previous approaches, our method does not require worm skeletonization, keypoints definition, or any pose or behavior categories. Our approach allows to embed all worm poses and pose sequences, from straight ones to the challenging poses of tightly coiling and strongly bending <italic>C. elegans</italic>. We demonstrate that, even though our methods are based exclusively on image pixel values, the resulting image and video embeddings reflect quantitative features describing the worm shape and its motion, such as degree of bend, eigenworms and speed of motion. We apply our method to the video data of different genetic strains as well as aging worms and illustrate the differences in behavior of worms of various strains and ages.</p>
<p>To summarize, the advantages of our approach are:</p>
<list list-type="order">
<list-item><p>Embedding challenging poses without relying on annotations.</p></list-item>
<list-item><p>Quantifying previously inaccessible behaviors.</p></list-item>
<list-item><p>Capturing hand-engineered features without explicitly calculating them.</p></list-item>
<list-item><p>Ability to capture in a comprehensive manner properties of poses and behavior.</p></list-item>
</list>
<p>One limitation of our approach is the inability to distinguish between the head and tail of the worm. Head and tail movements are important elements of the worm behavior. Since <italic>C. elegans</italic> typically move head-first, the head/tail orientation can be estimated based on their direction of movement. However, for strains that frequently move backward, this rule would not apply and the head/tail orientation would need to be estimated based on their visual features. While this remains a challenging task, future work should incorporate predicted head/tail orientation as input to the network in our approach. Alternatively, video frames could be adjusted so that <italic>C. elegans</italic> always face head-up, rather than simply aligning all worms to a vertical orientation without considering head/tail direction.</p>
<p>While our approach offers many advantages over methods based on hand-engineered features, one drawback is its lower direct interpretability. For example, a feature such as head speed provides straightforward, low-level behavioral insights, whereas our embedding space visualizations combine all characteristics of the worm motion and are therefore more difficult to interpret, similar to eigenworm features<sup><xref ref-type="bibr" rid="c4">4</xref></sup>. On the other hand, the comprehensive motion embeddings derived from our method are a powerful representation for downstream tasks such as behavior or strain classification, reaching beyond analyses based individual motion features.</p>
<p>In this work we focused on behaviors spanning two seconds. Future experiments could explore embedding sequences with different time spans. Extending the length of the input video to four or eight seconds may allow to capture additional behaviors, from brief actions to prolonged activities such as mating. Longer videos can be incorporated in various ways, such as adjusting the step size between frames or increasing the sequence length.</p>
<p>Since pixel-based approaches like ours do not rely on skeleton or keypoints definition, they can be applied to any body form. This ability to quantify behavior directly from pixels opens possibilities to study a wide range of organisms, including cephalopods<sup><xref ref-type="bibr" rid="c29">29</xref></sup> or single-celled organisms with flagella or cilia<sup><xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup> in a comprehensive manner.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We would like to thank Greg J. Stephens and André E. X. Brown for their valuable comments and discussions. Maurice Deserno and Katarzyna Bozek were supported by the North Rhine-Westphalia return program (311-8.03.03.02-147635), BMBF program Junior Group Consortia in Systems Medicine (01ZX1917B) and hosted by the Center for Molecular Medicine Cologne.</p>
</ack>
<sec id="d1e956" sec-type="additional-information">
<title>Additional information</title>
<sec id="s6">
<title>Author contributions statement</title>
<p>M.D.: methodology, software, experiments and analysis. K.B.: supervision, data and funding acquisition. M.D. and K.B. writing the article. All authors reviewed the manuscript.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hu</surname>, <given-names>B.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>3d mouse pose from single-view video and a new dataset</article-title>. <source>Sci. Reports</source> <volume>13</volume>, <fpage>13554</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Karashchuk</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Anipose: A toolkit for robust markerless 3d pose estimation</article-title>. <source>Cell Reports</source> <volume>36</volume>, <fpage>109730</fpage>, DOI: <pub-id pub-id-type="doi">10.1016/j.celrep.2021.109730</pub-id> (<year>2021</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Deeplabcut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nat. Neurosci</source>. (<year>2018</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stephens</surname>, <given-names>G. J.</given-names></string-name>, <string-name><surname>Johnson-Kerner</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Ryu</surname>, <given-names>W. S.</given-names></string-name></person-group> <article-title>Dimensionality and dynamics in the behavior of c. elegans</article-title>. <source>PLOS Comput. Biol</source>. <volume>4</volume>, <fpage>1</fpage>–<lpage>10</lpage>, DOI: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000028</pub-id> (<year>2008</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hebert</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Ahamed</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Costa</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>O’Shaughnessy</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Stephens</surname>, <given-names>G. J.</given-names></string-name></person-group> <article-title>Wormpose: Image synthesis and convolutional networks for pose estimation in c. elegans</article-title>. <source>PLOS Comput. Biol</source>. <volume>17</volume>, <fpage>1</fpage>–<lpage>20</lpage>, DOI: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1008914</pub-id> (<year>2021</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yemini</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Jucikas</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Grundy</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Brown</surname>, <given-names>A. E.</given-names></string-name> &amp; <string-name><surname>Schafer</surname>, <given-names>W. R.</given-names></string-name></person-group> <article-title>A database of caenorhabditis elegans behavioral phenotypes</article-title>. <source>Nat. methods</source> <volume>10</volume>, <fpage>877</fpage>–<lpage>879</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nagy</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Goessling</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Amit</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Biron</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>A generative statistical algorithm for automatic detection of complex postures</article-title>. <source>PLOS Comput. Biol</source>. <volume>11</volume>, <fpage>e1004517</fpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baek</surname>, <given-names>J.-H.</given-names></string-name>, <string-name><surname>Cosman</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Feng</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Silver</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Schafer</surname>, <given-names>W. R.</given-names></string-name></person-group> <article-title>Using machine vision to analyze and classify caenorhabditis elegans behavioral phenotypes quantitatively</article-title>. <source>J. neuroscience methods</source> <volume>118</volume>, <fpage>9</fpage>–<lpage>21</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Javer</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>An open-source platform for analyzing and sharing worm-behavior data</article-title>. <source>Nat. methods</source> <volume>15</volume>, <fpage>645</fpage>–<lpage>646</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Restif</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Celest: computer vision software for quantitative analysis of c. elegans swim behavior reveals novel features of locomotion</article-title>. <source>PLoS computational biology</source> <volume>10</volume>, <fpage>e1003702</fpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nagy</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Goessling</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Amit</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Biron</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>A generative statistical algorithm for automatic detection of complex postures</article-title>. <source>PLOS Comput. Biol</source>. <volume>11</volume>, <fpage>1</fpage>–<lpage>23</lpage>, DOI: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004517</pub-id> (<year>2015</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</article-title>. In <conf-name>2015 IEEE International Conference on Computer Vision (ICCV)</conf-name>, <fpage>1026</fpage>–<lpage>1034</lpage>, DOI: <pub-id pub-id-type="doi">10.1109/ICCV.2015.123</pub-id> (<year>2015</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alonso</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Kirkegaard</surname>, <given-names>J. B.</given-names></string-name></person-group> <article-title>Fast detection of slender bodies in high density microscopy data</article-title>. <source>Commun. Biol</source>. <volume>6</volume>, <fpage>754</fpage>, DOI: <pub-id pub-id-type="doi">10.1038/s42003-023-05098-1</pub-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Weheliye</surname>, <given-names>W. H.</given-names></string-name>, <string-name><surname>Rodriguez</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Feriani</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Javer</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Brown</surname>, <given-names>A. E.</given-names></string-name></person-group> <article-title>An improved neural network model enables worm tracking in challenging conditions and increases signal-to-noise ratio in phenotypic screens</article-title>. <source>bioRxiv</source> DOI: <pub-id pub-id-type="doi">10.1101/2024.12.20.629717</pub-id> (<year>2024</year>). <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2024/12/21/2024.12.20.629717.full.pdf">https://www.biorxiv.org/content/early/2024/12/21/2024.12.20.629717.full.pdf</ext-link>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="other"><source>European Organization For Nuclear Research &amp; OpenAIRE</source>. <publisher-loc>Zenodo</publisher-loc>, DOI: <pub-id pub-id-type="doi">10.25495/7GXK-RD71</pub-id> (<year>2013</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Javer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Ripoll-Sánchez</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Brown</surname>, <given-names>A. E.</given-names></string-name></person-group> <article-title>Powerful and interpretable behavioural features for quantitative phenotyping of caenorhabditis elegans</article-title>. <source>Philos. Transactions Royal Soc. B: Biol. Sci</source>. <volume>373</volume>, <fpage>20170375</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Ansel</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation</article-title>. In <conf-name>29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS ‘24)</conf-name>, DOI: <pub-id pub-id-type="doi">10.1145/3620665.3640366</pub-id> (<publisher-name>ACM</publisher-name>, <year>2024</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Bardes</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Ponce</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name></person-group> <article-title>VICReg: Variance-invariance-covariance regularization for self-supervised learning</article-title>. In <conf-name>International Conference on Learning Representations</conf-name> (<year>2022</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Loshchilov</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Hutter</surname>, <given-names>F.</given-names></string-name></person-group> <article-title>Decoupled weight decay regularization</article-title>. In <conf-name>International Conference on Learning Representations</conf-name> (<year>2017</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Loshchilov</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Hutter</surname>, <given-names>F.</given-names></string-name></person-group> <article-title>SGDR: Stochastic gradient descent with warm restarts</article-title>. In <conf-name>International Conference on Learning Representations</conf-name> (<year>2017</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Vaswani</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Attention is all you need</article-title>. In <conf-name>Neural Information Processing Systems</conf-name> (<year>2017</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Devlin</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>M.-W.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Toutanova</surname>, <given-names>K.</given-names></string-name></person-group> <article-title>Bert: Pre-training of deep bidirectional transformers for language understanding</article-title>. In <conf-name>North American Chapter of the Association for Computational Linguistics</conf-name> (<year>2019</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Dosovitskiy</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>An image is worth 16×16 words: Transformers for image recognition at scale</article-title>. In <conf-name>International Conference on Learning Representations</conf-name> (<year>2021</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Du</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Côté</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name></person-group> <article-title>Saits: Self-attention-based imputation for time series</article-title>. <source>Expert. Syst. with Appl</source>. <volume>219</volume>, <fpage>119619</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Rose</surname>, <given-names>F.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Deep imputation for skeleton data (disk) for behavioral science</article-title>. <source>bioRxiv</source> <elocation-id>2024.05.03.592173</elocation-id> (<year>2024</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McInnes</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Healy</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Saul</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Großberger</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>Umap: Uniform manifold approximation and projection</article-title>. <source>J. Open Source Softw</source>. <volume>3</volume>, <fpage>861</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ding</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Romenskyy</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sarkisyan</surname>, <given-names>K. S.</given-names></string-name> &amp; <string-name><surname>Brown</surname>, <given-names>A. E. X.</given-names></string-name></person-group> <article-title>Measuring caenorhabditis elegans spatial foraging and food intake using bioluminescent bacteria</article-title>. <source>Genetics</source> <volume>214</volume>, <fpage>577</fpage>–<lpage>587</lpage>, DOI: <pub-id pub-id-type="doi">10.1534/genetics.119.302804</pub-id> (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://academic.oup.com/genetics/article-pdf/214/3/577/42105699/genetics0577.pdf">https://academic.oup.com/genetics/article-pdf/214/3/577/42105699/genetics0577.pdf</ext-link>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>López-Puebla</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mayoral-Peña</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Gómez-Cepeda</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Arellano-Carbajal</surname>, <given-names>F.</given-names></string-name></person-group> <article-title>Caenorhabditis elegans daf-7 mutants exhibit burrowing behavior</article-title>, <source>microPublication Biology</source> DOI: <pub-id pub-id-type="doi">10.17912/MICROPUB.BIOLOGY.000172</pub-id> (<year>2019</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Woo</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The dynamics of pattern matching in camouflaging cuttlefish</article-title>. <source>Nature</source> <volume>619</volume>, <fpage>122</fpage>–<lpage>128</lpage>, DOI: <pub-id pub-id-type="doi">10.1038/s41586-023-06259-2</pub-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wan</surname>, <given-names>K. Y.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Reorganization of complex ciliary flows around regenerating &lt;i&gt;stentor coeruleus&lt;/i&gt;</article-title>. <source>Philos. Transactions Royal Soc. B: Biol. Sci</source>. <volume>375</volume>, <fpage>20190167</fpage>, DOI: <pub-id pub-id-type="doi">10.1098/rstb.2019.0167</pub-id> (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://royalsocietypublishing.org/doi/pdf/10.1098/rstb.2019.0167">https://royalsocietypublishing.org/doi/pdf/10.1098/rstb.2019.0167</ext-link>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wan</surname>, <given-names>K. Y.</given-names></string-name></person-group> <article-title>Synchrony and symmetry-breaking in active flagellar coordination</article-title>. <source>Philos. Transactions Royal Soc. B: Biol. Sci</source>. <volume>375</volume>, <fpage>20190393</fpage>, DOI: <pub-id pub-id-type="doi">10.1098/rstb.2019.0393</pub-id> (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://royalsocietypublishing.org/doi/pdf/10.1098/rstb.2019.0393">https://royalsocietypublishing.org/doi/pdf/10.1098/rstb.2019.0393</ext-link>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106593.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study introduces a self-supervised machine learning method to classify C. elegans postures and behaviors directly from video data, offering an alternative to the skeleton-based approaches that rely on often error-prone tracking. This novel approach holds promise for advancing ethology research. That said, the strength of evidence is currently <bold>incomplete</bold>, as key aspects - including measuring head-tail orientation, increased behavioral interpretability, and quantitative comparisons to established methods - are underdeveloped and would benefit from further validation.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106593.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The submitted article reports the development of an unsupervised learning method that enables quantification of behaviour and poses of C. elegans from 15 minute long videos and presents a spatial map of both. The entire pipeline is a two part process, with the first part based on contrastive learning that represents spatial poses onto an embedded space, while the second part uses a transformer encoder to enable estimation of masked parts in a spatiotemporal sequence.</p>
<p>Strengths:</p>
<p>This analysis approach will prove to be useful for the C. elegans community. The application of the method on various age-related videos on various strains presents a good use-case for the approach. The manuscript is well written and presented.</p>
<p>Specific comments:</p>
<p>(1) One of the main motivations as mentioned in the introduction as well as emphasized in the discussion section is that this approach does not require key-point estimation for skeletonization and is also not dependent on the eigenworm approach for pose estimation. However, the eigenworm data has been estimated using the Tierpsy tracker in videos used in this work and stored as metadata. This is subsequently used for interpretation. It is not clear at this point, how else the spatial embedded map may be interpreted without using this kind of pose estimates obtained from other approaches. Please elaborate and comment.</p>
<p>(2) As per the manuscript, the second part of the pipeline is used to estimate the masked sequences of the spatiotemporal behavioral feature. However, it is not clear what the numbers listed in Fig. 2.3 represent?</p>
<p>(3) It is not clear how motion speed is linked to individual poses as mentioned in Figs. 4 (b) and (c).</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106593.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The manuscript by Maurice and Katarzyna describes a self-supervised, annotation-free deep-learning approach capable of quantitatively representing complex poses and behaviors of C. elegans directly from video pixel values. Their method overcomes limitations inherent to traditional methods relying on skeletonization or keypoint tracking, which often fail with highly coiled or self-intersecting worms. By applying self-supervised contrastive learning and a Transformer-based network architecture, the authors successfully capture diverse behavioral patterns and depict the aging trajectory of behavioral repertoire. This provides a useful new tool for behavioral research in C. elegans and other flexible-bodied organisms.</p>
<p>Strengths:</p>
<p>Reliable tracking and segmentation of complex poses remain significant bottlenecks in C. elegans behavioral research, and the authors made valuable attempts to address these challenges. The presented method offers several advantages over existing tools, including freedom from manual labeling, independence from explicit skeletonization or keypoint tracking, and the capability to capture highly coiled or overlapping poses. Thus, the proposed method would be useful to the C. elegans research community.</p>
<p>The research question is clearly defined. Methods and results are engagingly presented, and the manuscript is concise and well-organized.</p>
<p>Weaknesses:</p>
<p>(1) In the abstract, the claim of an 'unbiased' approach is not well-supported. The method is still affected by dataset biases, as mentioned in the aging results (section 4.3).</p>
<p>
(2) In section 3.2, the rationale behind rotating worm images to a vertical orientation is unclear.</p>
<p>
(3) The methods section is clearly written but uses overly technical language, making it less accessible to the audience of eLife, the majority of whom are biologists. Clearer explanations of key methods and the rationale behind their selection are needed. For example, in section 3.3, the authors should briefly explain in simple language what contrastive learning is, why they chose it, and why this method potentially achieves their goal.</p>
<p>
(4) The reason why the gray data points could not be resolved by Tierpsy is not quantitatively described. Are they all due to heavily coiled or overlapping poses?</p>
<p>
(5) In section 4.1, generating pose representations grouped by genetic strains would provide insights into strain-specific differences resolved by the proposed method.</p>
<p>
(6) Fig. 3a requires clarification. Highly bent poses (red points) intuitively should be close to highly coiled poses (gray points). The authors should explain the observed greenish/blueish points interfacing with the gray points.</p>
<p>
(7) In Fig. 3a, some colored points overlap with the gray point cloud. Why can Tierpsy resolve these overlapping points representing highly coiled poses? A more systematic quantitative comparison between Tierpsy and the proposed method is required.</p>
<p>
(8) The claim in section 4.2 regarding strain separation in pose embedding spaces is unsupported by Fig. 3a, which lacks strain-based distinctions. As mentioned in point #5, showing pose representations grouped by different strains is required.</p>
<p>
(9) In section 4.2, how the authors could verify the statement, &quot;This likely occurs since most strains share common behaviors such as simple forward locomotion&quot;?</p>
<p>
(10) An important weakness of the proposed method is its low direct interpretability, as it is not based on handcrafted features. To better interpret the pose/behavior embedding space, it would be helpful to compare it against more basic Tierpsy features in Fig. 3 and 4. This comparison could reveal what understandable features were learned by the neural network, thereby increasing human interpretability.</p>
<p>
(11) The main conclusion of section 4.3 is not sufficiently tested. Is Fig. 5a generated only from data of N2 animals? To quantitatively verify the statement, &quot;Young individuals appear to display a wide range of behaviors, while as they age their behavior repertoire reduces,&quot; the authors should perform a formal analysis of behavioral variability throughout aging.</p>
<p>
(12) In Fig. 5a, better visualization of aging trajectories could include plotting the center of mass along with variance of the point cloud over time.</p>
<p>
(13) To better reveal aging trajectories of behavioral changes for different genetic backgrounds, it would be meaningful to generate behavior representations for different strains as they age.</p>
<p>
(14) As a methods paper, the ease of use for other researchers should be explicitly addressed, and source code and datasets should be provided.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106593.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this paper, the authors present an unsupervised learning approach to represent C. elegans poses and temporal sequences of poses in low-dimensional spaces by directly using pixel values from video frames. The method does not rely on the exact identification of the worm's contour/midline, nor on the identification of the head and tail prior to analyzing behavioral parameters. In particular, using contrastive learning, the model represents worm poses in low-dimensional spaces, while a transformer encoder neural network embeds sequences of worm postures over short time scales. The study evaluates this newly developed method using a dataset of different C. elegans genetic strains and aging individuals. The authors compared the representations inferred by the unsupervised learning with features extracted by an established approach, which relies on direct identification of the worm's posture and its head-tail direction.</p>
<p>Strengths:</p>
<p>The newly developed method provides a coarse classification of C. elegans posture types in a low-dimensional space using a relatively simple approach that directly analyzes video frames. The authors demonstrate that representations of postures or movements of different genotypes, based on pixel values, can be distinguishable to some extent.</p>
<p>Weaknesses:</p>
<p>- A significant disadvantage of the presented method is that it does not include the direction of the worm's body (e.g., head/tail identification). This highly limits the detailed and comprehensive identification of the worm's behavioral repertoire (on- and off-food), which requires body directionality in order to infer behaviors (for example, classifying forward vs. reverse movements). In addition, including a mix of opposite postures as input to the new method may create significant classification artifacts in the low-dimensional representation-such that, for example, curvature at opposite parts of the body could cluster together. This concern applies both to the representation of individual postures and to the representation of sequences of postures.</p>
<p>
- The authors state that head-tail direction can be inferred during forward movement. This is true when individuals are measured off-food, where they are highly likely to move forward. However, when animals are grown on food, head-tail identification can also be based on quantifying the speed of the two ends of the worm (the head shows side-to-side movements). This does not require identifying morphological features. See, for example, Harel et al. (2024) or Yemini et al. (2013).</p>
<p>
- Another confounding parameter that cannot be distinguished using the presented method is the size of individuals. Size can differ between genotypes, as well as with aging. This can potentially lead to clustering of individuals based on their size rather than behavior.</p>
<p>
- There is no quantitative comparison between classification based on the presented method and methods that rely on identifying the skeleton.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106593.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Deserno</surname>
<given-names>Maurice</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6583-7360</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Bozek</surname>
<given-names>Katarzyna</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0917-6876</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We thank the editors and the reviewers for their valuable comments and for taking the time to evaluate our manuscript.</p>
<p><bold>Answers to Reviewer 1:</bold></p>
<p>(1) The core contribution of our method is that it learns meaningful spatiotemporal embeddings directly from image data without requiring pose estimation or eigenworm-based features as input. The learned embedding space can serve as a foundation for downstream tasks such as behavioral classification, clustering, or anomaly detection, further supporting its utility beyond visualization through eigenworm-derived features. Here we use the Tierpsy-derived features for latent space interpretation and for validation that our approach does indeed encode meaningful postural information. Additionally, without any Tierpsy-calculated features users can still color embeddings by known metadata like mutation or age and compare different strains to each other.</p>
<p>(2) The numbers shown in Fig. 2.3 are illustrative placeholders intended to conceptually represent a vector of behavioral features. They do not correspond to any specific measurements or carry intrinsic meaning. We agree that this may lead to confusion, and we will clarify this in the revised manuscript.</p>
<p>(3) The visualizations in Figs. 4 (b) and (c) show the embeddings of sequences of behavior, rather than individual poses. Therefore, motion-related features such as speed are related to temporal patterns in those sequences rather than static postures. The color overlays reflect average motion characteristics (e.g., speed) of short behavior clips projected into the embedding space, rather than being directly linked to any single frame or pose.</p>
<p><bold>Answers to Reviewer 2:</bold></p>
<p>(1) In the abstract, our use of the term &quot;unbiased&quot; refers specifically to the avoidance of human-generated bias through feature engineering—i.e., the model does not rely on handcrafted features or predefined pose representations – the representations are based on data only. However, we agree that the model is still subject to dataset biases and will rectify this in the revised manuscript.</p>
<p>(2) The worm images are rotated to a common vertical orientation to remove orientation as a source of variability in the input. This ensures that the model focuses on learning pose and behavioral dynamics rather than arbitrary head-tail or angular positioning. While data augmentation could in theory account for this variability, we found in our preliminary experiments that applying this preprocessing step led to more stable and interpretable embeddings.</p>
<p>(3) We agree that simplifying the technical explanations would enhance the manuscript’s accessibility. In the revised version, we will briefly introduce contrastive learning in a less technical language.</p>
<p>(4) The gray points in Fig. 3a represent frames that Tierpsy could not resolve, primarily due to coiled, self-intersecting, or overlapping worm postures as Tierpsy uses skeletonization to estimate the centerline. This approach can fail if kind of challenging elements are part of the image.</p>
<p>(5) We appreciate this suggestion and consider it for a revised version of the manuscript.</p>
<p>(6) Although it may seem intuitive for highly bent (red) poses to lie near coiled (gray) ones in the embedding space, the clustering pattern observed reflects how the network organizes pose information. The red/orange cluster consists of distinguishable bent poses that are visually distinct and consistently separable from other postures. In contrast, the greenish and blueish poses are less strongly bent and may share more visual overlap with the unresolved (gray) images.</p>
<p>(7) The overlap occurs because some highly bent or coiled worms can still be (partially) resolved by Tierpsy, depending on specific pose conditions (e.g., head and tail not touching, not self-overlapping). However, Tierpsy fails to consistently resolve such frames. We will describe these cases in more detail in the revised manuscript.</p>
<p>(8) Thank you, we agree this claim needs to be better supported and will develop it in the revision.</p>
<p>(9) To support this statement we mainly visualized the respective sequences embedded in this area of the embedding space and found that it mostly consists of common behaviors such as forward locomotion.</p>
<p>(10) We agree that interpretability is important and plan to include additional figures quantifications of the embedding space using more basic Tierpsy features.</p>
<p>(11) Fig. 5a is indeed based solely on N2 animals. In the revised manuscript we will include quantitative measures of behavioral variability and its change with age.</p>
<p>(12) We appreciate this suggestion and consider it for a revised version</p>
<p>(13) We agree this would be a valuable analysis. However, our current dataset primarily includes aging data for N2 animals. We acknowledge this limitation and consider adding more strains for future work.</p>
<p>(14) We will include links to our source code in the revised manuscript</p>
<p><bold>Answers to Reviewer 3:</bold></p>
<p>(1-2) Our current method is agnostic to head-tail orientation, which indeed restricts the ability to distinguish behaviors that rely on directional cues. We made this design choice as we believe that correctly identifying head/tail orientation can be a challenging task that may introduce additional biases or fail in difficult imaging conditions. However, we fully agree that integrating directional information would improve behavioral resolution, and this is a natural extension of our current framework. In future work, we aim to incorporate head-tail disambiguation.</p>
<p>(3) We explicitly designed our preprocessing and training pipeline to encourage size invariance, for example by resizing individuals to a consistent scale, as the focus of our work is to encode posture and movement only. However, we acknowledge that absolute size information is lost in this process, which can be informative for distinguishing genotypes or age-related changes.</p>
<p>(4) We agree that a direct quantitative comparison between our embedding-based representations and skeleton-based feature sets would strengthen the paper. Our current focus was to assess whether meaningful behavioral features could be learned from a skeleton-free representation.</p>
</body>
</sub-article>
</article>