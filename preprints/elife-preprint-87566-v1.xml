<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">87566</article-id>
<article-id pub-id-type="doi">10.7554/eLife.87566</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.87566.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Objective assessment of visual attention in toddlerhood</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Braithwaite</surname>
<given-names>E.</given-names>
</name>
<xref ref-type="author-notes" rid="n1">$</xref>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kyriakopoulou</surname>
<given-names>V.</given-names>
</name>
<xref ref-type="author-notes" rid="n1">$</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Mason</surname>
<given-names>L.</given-names>
</name>
<xref ref-type="author-notes" rid="n1">$</xref>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Davidson</surname>
<given-names>A.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Tusor</surname>
<given-names>N.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Harper</surname>
<given-names>N.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Earl</surname>
<given-names>M.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Datoo-Partridge</surname>
<given-names>S.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Young</surname>
<given-names>A.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chew</surname>
<given-names>A.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Falconer</surname>
<given-names>S.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hajnal</surname>
<given-names>Joseph V</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Johnson</surname>
<given-names>M.H.</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6305-9710</contrib-id>
<name>
<surname>Nosarti</surname>
<given-names>C.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4801-7066</contrib-id>
<name>
<surname>Edwards</surname>
<given-names>A.D.</given-names>
</name>
<xref ref-type="author-notes" rid="n2">*</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5747-9540</contrib-id>
<name>
<surname>Jones</surname>
<given-names>E.J.H.</given-names>
</name>
<xref ref-type="author-notes" rid="n2">*</xref>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">#</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Birkbeck, University of London</institution></aff>
<aff id="a2"><label>2</label><institution>Centre for the Developing Brain, School of Biomedical Engineering and Imaging Sciences, King’s College London</institution>, London, <country>UK</country></aff>
<aff id="a3"><label>3</label><institution>Department of Child and Adolescent Psychiatry, Institute of Psychiatry, Psychology and Neuroscience, King’s College London</institution>, London, <country>UK</country></aff>
<aff id="a4"><label>4</label><institution>University of</institution> Cambridge</aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Spering</surname>
<given-names>Miriam</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>The University of British Columbia</institution>
</institution-wrap>
<city>Vancouver</city>
<country>Canada</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>#</label>Corresponding author; email: <email>e.jones@bbk.ac.uk</email></corresp>
<fn fn-type="other" id="n1"><label>$</label><p>Joint first authors</p></fn>
<fn fn-type="other" id="n2"><label>*</label><p>Joint last authors</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-07-27">
<day>27</day>
<month>07</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP87566</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-03-28">
<day>28</day>
<month>03</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-04-04">
<day>04</day>
<month>04</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.04.04.534573"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Braithwaite et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Braithwaite et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-87566-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Visual attention is an important mechanism through which children learn about their environment, and individual differences could substantially shape later development. Eyetracking provides a sensitive and scalable tool for assessing visual attention that has potential for objective assessment of child development, but to date the majority of studies are small and replication attempts are rare. This study investigates the feasibility of a comprehensive eye-tracking assessment of visual attention and introduces a shared data resource for the scientific community. Data from eight eyetracking tasks were collected from 350 term-born (166 females) 18-month-olds recruited as neonates <ext-link ext-link-type="uri" xlink:href="http://www.developingconnectome.org/">http://www.developingconnectome.org/</ext-link>). Analyses showed expected condition effects for seven of eight tasks (<italic>p</italic>-values from &lt;.001 to .04), an important indication of replicability. Consistent with some theoretical models of visual attention, structural equation modelling indicated participants’ performance could be explained by two factors representing social and non-social attention. Comprehensive eye-tracking batteries can objectively measure individual differences in core components of visual attention in large-scale toddlerhood studies. This is the first large-scale comprehensive study to present high-quality normative eye-tracking data from a large task battery in toddlers and make them freely available to the scientific community.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s2">
<title>Introduction</title>
<p>The first three years of life are critical for brain development (<xref ref-type="bibr" rid="c19">Fox et al., 2010</xref>) and have a significant impact on later quality of life and economic contributions (<xref ref-type="bibr" rid="c35">Knudsen et al., 2006</xref>). Most current approaches to large-scale measurement of early brain and cognitive development either use indirect measures (e.g. head circumference), or behavioural assessments. This severely limits our capacity to rapidly and objectively track early cognitive development as behavioural measures require skilled assessors, rely on children’s verbal comprehension and motor skills, and may not be translatable to different contexts and cultures. To provide more direct assessment of cognitive development, one highly promising method is eye-tracking. Eye-tracking is a non-invasive technology that can provide exquisite temporal and spatial resolution on a child’s direction of gaze, and can be largely automated to produce scalable measures of individual differences in visual attention in early development. Humans are a highly visual species, and control information input through eye movements and visual attention is a particularly crucial modality for learning in early development. Eye-tracking tasks can be conducted without the need for complex verbal instruction and do not rely upon children’s comprehension ability or motor skills (Karatekin, 2007; Richmond &amp; Nelson, 2009). Eye-tracking limits potential researcher bias as it minimises the role of the researcher and provides a more direct and objective measure of children’s processing (Karatekin, 2007; Sasson &amp; Elison, 2012). Additionally, compliance rates are usually high since there are no requirements for the child to wear any equipment, nor the need to interact with anyone unfamiliar to them, enabling a broader community of children to successful participate (Karatekin, 2007). This is particularly important in toddlerhood, where the challenges involved in working with children with limited verbal and executive functioning skills but strong motor skills have limited progress in studying neurocognitive development in the second and third years of life.</p>
<p>Many developmental studies have utilised eye-tracking to investigate attention, but these have typically focussed on comparing group differences in performance on a single eye-tracking task or a battery of tasks in small size cohorts (B. M. <xref ref-type="bibr" rid="c29">Hood &amp; Atkinson, 1993</xref>; <xref ref-type="bibr" rid="c31">Hutchinson &amp; Turk-Browne, 2012</xref>; <xref ref-type="bibr" rid="c42">van Baar et al., 2020</xref>). Single task studies have yielded important findings about how bias for faces changes over development (<xref ref-type="bibr" rid="c36">Leppänen, 2016</xref>), how orienting to stimuli differs under conditions of competition (B. M. <xref ref-type="bibr" rid="c29">Hood &amp; Atkinson, 1993</xref>) and how memory guides attention (<xref ref-type="bibr" rid="c31">Hutchinson &amp; Turk-Browne, 2012</xref>). However, much less is known about whether eyetracking can provide appropriate measures of individual differences, and whether tasks can be combined effectively into larger batteries.</p>
<p>Recently, a multiple-task battery showed reasonable reliability and validity for use with toddlers, though sample size was limited (n = 12 in the reliability sample) (<xref ref-type="bibr" rid="c42">van Baar et al., 2020</xref>). To use eye-tracking as a large-scale, comprehensive assessment of development, acquisition and robustness of task performance when multiple tasks are combined must be examined.</p>
<p>Using a comprehensive battery of visual attention tasks also allows investigators to move beyond metrics extracted from single tasks and to test whether profiles of attention across the battery fit are consistent with theoretical models of visual attention. In designing eye-tracking tests of attention, many investigators have distinguished between endogenous and exogenous control of attention (<xref ref-type="bibr" rid="c10">Connor et al., 2004</xref>; <xref ref-type="bibr" rid="c40">Sarter et al., 2001</xref>). Endogenous control involves executive attention systems that select goal-relevant actions by resolving conflict between competing inputs or impulses (<xref ref-type="bibr" rid="c3">Amso &amp; Scerif, 2015</xref>; <xref ref-type="bibr" rid="c37">Miller &amp; Cohen, 2001</xref>). Conversely, exogenous control is stimulus-driven, and relies primarily on ‘bottom-up’ mechanisms.</p>
<p>Although most work on the development of attention has employed non-social stimuli, there has been increasing recent interest in the construct of ‘social attention’, the motivation to attend to social stimuli such as people and faces. This is of particular interest given its perturbation in the emergence of neurodevelopmental conditions such as autism (<xref ref-type="bibr" rid="c34">Klin et al., 2015</xref>). A rich historical literature indicates that infants orient to faces from birth (<xref ref-type="bibr" rid="c18">Farroni et al., 2005</xref>) and show preference for face-like stimuli (versus non-face configurations), direct gaze (versus averted gaze or eyes-closed gaze) and biological motion (versus inverted or scrambled) throughout development (<xref ref-type="bibr" rid="c41">Shultz et al., 2018</xref>). However, the intersection between behaviours labelled as ‘social attention’ and the exogenous and endogenous attention systems described above remains unclear (see <xref ref-type="bibr" rid="c8">Braithwaite et al., 2020</xref> for review). Possibly, attention to social stimuli represents a specific case of attention to object features or ’what’ forms of attention. Alternatively, social stimuli may gain attention due to the interplay between social motivation and other attentional systems (<xref ref-type="bibr" rid="c9">Chevallier et al., 2012</xref>; <xref ref-type="bibr" rid="c12">Dawson et al., 2005</xref>). Whilst such substructures have been examined longitudinally and through examination of condition differences, it remains unclear whether there are meaningful individual differences in separable subdomains of visual attention.</p>
<sec id="s2a">
<title>The current study</title>
<p>In this paper we present data collected from a large cohort of 18-month-old children tested on a large battery of eye-tracking tasks designed to measure both exogenous and endogenous attentional control in toddlerhood. Tasks were selected for inclusion based on previous literature, and relevance to assessing exogenous, endogenous and social attention (<xref ref-type="bibr" rid="c15">Elsabbagh et al., 2009</xref>, <xref ref-type="bibr" rid="c14">2013</xref>), <xref ref-type="bibr" rid="c33">Kidd et al. (2012)</xref> <xref ref-type="bibr" rid="c43">Wass et al. (2011)</xref> Kaldy et al. (2011); (Saez de Urabain et al., 2017); (Võ et al., 2012). Several tasks are also present in other large-scale data collection efforts in younger infants (<xref ref-type="bibr" rid="c17">Falck-Ytter et al., 2021</xref>; <xref ref-type="bibr" rid="c26">Hessels &amp; Hooge, 2019</xref>; <xref ref-type="bibr" rid="c32">Jones et al., 2019</xref>, <xref ref-type="bibr" rid="c32">2019</xref>), providing important opportunities for synergies. We examine the robustness of these tasks to combined delivery and examine whether derived metrics can provide independent estimates of core domains of visual attention. For each task, we examined data quality, attrition rates, and the presence or absence of expected condition effects. We then tested the fit of a theoretical model of visual attention to the pattern of data across tasks using structural equation modelling, to determine whether a smaller set of underlying constructs explained individual differences in visual attention within this cohort. All data and extracted metrics presented in this study are available to the scientific community as part of the developing Human Connectome Project scheduled data release (<ext-link ext-link-type="uri" xlink:href="http://www.developingconnectome.org/">http://www.developingconnectome.org/</ext-link>).</p>
</sec>
</sec>
<sec id="s3">
<title>Materials and Methods</title>
<sec id="s3a">
<label>2.1</label><title>Ethics</title>
<p>The study was approved by the West London &amp; GTAC Research ethics committee (REC: 14/LO/1169). Written informed consent was obtained from the parents or legal guardians of all participating children. The data are available from the National Institute of Mental Health Data Archive (<ext-link ext-link-type="uri" xlink:href="https://nda.nih.gov/">https://nda.nih.gov/</ext-link>), under Developing Human Connectome Project (dHCP) Collection 3955 (<ext-link ext-link-type="uri" xlink:href="https://nda.nih.gov/edit_collection.html?id=3955">https://nda.nih.gov/edit_collection.html?id=3955</ext-link>). All access requests are reviewed and handled by the NDA.</p>
</sec>
<sec id="s3b">
<label>2.2.</label><title>Cohort</title>
<p>The cohort included 350 term-born children (184 males, 166 females) recruited from low-risk pregnancies as part of the Developing Human Connectome Project (DHCP, <ext-link ext-link-type="uri" xlink:href="http://www.developingconnectome.org/">http://www.developingconnectome.org/</ext-link>). Children aged 18 months who had been recruited to the study as neonates were invited to attend an eye-tracking assessment and a neurodevelopmental assessment at the Centre for the Developing brain in St Thomas’ hospital. Exclusion criteria were preterm birth (&lt;37 weeks) and the presence of a sibling with a diagnosis of ASD. Cohort demographics are presented in Table S1.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Mean, standard deviation and range for demographic information</p></caption>
<graphic xlink:href="534573v1_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s3c">
<label>2.3</label><title>Eye tracking data acquisition</title>
<p>Eye tracking data were acquired from a Tobii TX-300 (Tobii AB, Sweden) at a sampling rate of 120Hz, with children sitting approximately 60cm from the 23” screen (58.42cm x 28.6cm, 52.0° x 26.8° @ 60cm, native resolution of 1920 x 1080 pixels and an aspect ratio of 16:9). Stimuli were presented on Apple (Apple Inc., USA) Macbook Pro computers, using our custom-written stimulus presentation framework (Task Engine, sites.google.com/site/taskenginedoc/), running in Matlab using Psychtoolbox 3 (Brainard, 1997; Kleiner et al., 2007) and the GStreamer library (<ext-link ext-link-type="uri" xlink:href="http://gstreamer.freedesktop.org">gstreamer.freedesktop.org</ext-link>) for video decoding (SM1.1 for more details).</p>
</sec>
<sec id="s3d">
<label>2.4.</label><title>Tasks</title>
<p>At the start of the eye tracking assessment the experimenter positioned each participant in front of the eye tracker. Online feedback was given to allow a position to be chosen as close as possible to the centre of the eye tracker head box, to maximise data quality. An automatic five-point calibration was then performed. At the beginning of each trial, a gaze-contingent fixation stimulus was presented in the centre of the screen; when gaze fell upon this stimulus the trial started. All tasks except for the Gap-Overlap began when the participant fixated a gaze-contingent central fixation stimulus, at a size of 3cm x 3cm (2.86° x 2.86° at 60cm viewing distance). Tasks were administered in blocks that were intermixed with each other and distributed throughout the battery. SM1.2 provides additional task details.</p>
<sec id="s3d1">
<label>2.4.1</label><title>Gap-Overlap</title>
<sec id="s3d1a">
<label>2.4.1.1.</label><title>Stimulus presentation</title>
<p>Trials were presented in blocks of 12. Each trial started with the onset of a central stimulus (CS), a cartoon image of an analogue clock accompanied by an alerting sound. When the infant fixated the CS, after a 600-700ms wait period the peripheral stimulus (PS) was presented at 21.7° of visual angle on the left or right of the screen (random). In the baseline condition the CS disappeared concurrent with the appearance of the PS. In the overlap condition the CS continued to be presented for the duration of the rest of the trial. In the gap condition the CS was removed from the screen 200ms before the PS onset. The PS was a cartoon cloud that appeared on either the left or the right side of the screen and was accompanied by a sound, 3cm (2.86°) from the edge, rotating at 500° per second until fixated by the participant. A reward stimulus was then presented at the location of the PS for 1000ms.</p>
</sec>
<sec id="s3d1b">
<label>2.4.1.2.</label><title>Data extraction</title>
<p>Mean SRTs (time of first sample to enter the PS – time of PS onset) were calculated for gap, overlap and baseline conditions (<underline>GO-Gap-SRT</underline>; <underline>GO-Baseline-SRT</underline>; <underline>GO-Overlap-SRT</underline>) separately, using only valid trials (see SM1.2.1.2). For examination of condition differences, disengagement scores (<underline>GO-Disengagement)</underline> were computed as Overlap-SRT minus Baseline-SRT and facilitation scores (<underline>GO-Facilitation)</underline> were computed as Baseline-SRT minus Gap-SRT. Reaction times are expected to be fastest in the gap then baseline then overlap condition (<xref ref-type="bibr" rid="c15">Elsabbagh et al., 2009</xref>).</p>
</sec>
</sec>
<sec id="s3d2">
<label>2.4.2</label><title>Non-social contingency</title>
<sec id="s3d2a">
<label>2.4.2.1.</label><title>Stimulus presentation</title>
<p>This task consisted of three blocks of 19 trials. Blocks were distributed across the battery in a randomised order but constrained such that the 60% condition was always presented within the first two blocks. At the start and end of each block, a static picture of four balls was presented for a fixed period of 5 seconds. Between these was a set of trials. In each trial, participants were first presented with a fixation stimulus (hummingbird, 3cm x 3cm, 2.86° x 2.86°) that remained on screen until it was fixated by the infant. The fixation stimulus was replaced by a display of four balls (4.5cm, 2.3°), one in each corner of the screen, 3cm (2.9°) from each edge. Participants then saccaded to one of the four balls (within an AOI 50% bigger than the ball, subtending 4.3° degrees of visual angle); the ball they selected became their ‘chosen’ ball for that trial. The ‘reward’ for choosing a ball was that one of the balls would become animated; in the 100% reward block it was always the ball they chose that was animated; in the 0% reward block it was never the ball they chose (one of the other balls was randomly selected) and in the 60% reward block on 60% of the trials it was the ball they chose that was animated (and on the remaining 40%, a randomly chosen ball was animated). Rewards lasted 1000ms.</p>
</sec>
<sec id="s3d2b">
<title>• 2.4.2.1. Data extraction</title>
<p>Offline: Saccadic reaction time to select a ball was computed as the difference between the frame on which the four balls appeared and the first sample to enter one of the four AOIs and averaged within a block. Fixation times to return to the central stimulus at the beginning of each trial were an index of engagement and computed as the difference between the frame on which the fixation stimulus appeared and the first sample to enter the fixation stimulus and averaged within a block. Trials were excluded if the reward was not played (to ensure the child had selected a ball) or if reaction time was less than 150ms (to avoid trials where children were looking at a ball when the trial started); skipped trials were also excluded.</p>
<p>Key dependent variables were saccadic reaction time to select a ball with removing reaction times &gt; 2000ms (“zone outs”; <underline>NSC-100-SRT-NoZone</underline>, <underline>NSC-60-SRT-NoZone; NSC-0-SRT-NoZone</underline>); and fixation times to return to the central stimulus at the beginning of each trial (<underline>NSC-100-FixRT</underline>, <underline>NSC-60-FixRT; NSC-0-FixRT</underline>). Reaction times are expected to be fastest in the 60% condition because it is moderately predictable (<xref ref-type="bibr" rid="c33">Kidd et al., 2012</xref>).</p>
</sec>
</sec>
<sec id="s3d3">
<label>2.4.3</label><title>Reversal learning (‘Cognitive Control’)</title>
<sec id="s3d3a">
<label>2.4.3.1.</label><title>Stimulus presentation</title>
<p>This task was based on <xref ref-type="bibr" rid="c43">Wass et al. (2011)</xref>. Two purple 17cm by 13cm (16.1° x 12.5° @ 60cm) rectangles were presented on the left and right of the screen (1.5cm, or 1.43° from the outermost edge). These remained on screen until either 1) either one of the rectangles was fixated by the participant, or 2) 2000ms had elapsed. At this point, one of the rectangles was replaced by video of the same dimensions, showing a 2s clip of the animated children’s TV programme Thomas the Tank Engine. After the 2s clip had played, the screen became blank and the next trial began with another gaze-contingent fixation stimulus.</p>
<p>In the first (non-scored) trial, the side that the child chose to look at first was recorded; if no side was chosen after 2000ms it was determined randomly. On the following set of 8 trials (termed the “learning phase”), the video was always presented on the opposite side to that chosen on the first trial. This learning phase ended after either a) the child made three anticipatory saccades to the correct side of the screen, or b) eight trials (excluding the first, non-scored, trial) had been presented. The task then entered the “reversal phase” where the correct side was reversed and an additional nine trials were presented. The first trial of the reversal phase was not scored, but instead served to indicate (implicitly) to the child that the correct side had been reversed.</p>
</sec>
<sec id="s3d3b">
<label>2.4.3.2.</label><title>Data extraction</title>
<p>AOIs were placed around the location of each of the rectangles (within one of which the video played) and dilated by 2° to account for poor calibration. Accuracy was calculated as <italic>number of correct trials/ total number of trials where an anticipation occurred</italic> and saccadic reaction times were logged, then averaged across valid trials. Trials were considered invalid if SRT was less than 300ms or if no antisaccade was made. Participants were excluded from analysis if they made fewer than 2 antisaccades (regardless of correctness) with valid SRTs per phase (learning/reversal), per block.</p>
<p>Key dependent variables were pre-switch accuracy (<underline>CC-Pre-Acc)</underline> and reaction time <underline>(CC-Pre-SRT);</underline> we also assessed task performance using post-switch accuracy (<underline>CC-Post-Acc)</underline>, and reaction time (and <underline>CC-Post-SRT</underline>) but because these were not completed by all participants and were dependent on performance in the pre-switch phase (missing not at random) they were not included in later modelling. Accuracy is expected to be lower and reaction times are expected to be slower during the pre-switch phase (in line with <xref ref-type="bibr" rid="c43">Wass et al., 2011</xref>).</p>
</sec>
</sec>
<sec id="s3d4">
<label>2.4.4</label><title>Working memory</title>
<sec id="s3d4a">
<label>2.4.4.1.</label><title>Stimulus presentation</title>
<p>Images of two theatre stages with a lowered curtain were drawn on either side of the screen (16.0cm x 23.4cm, 15.2° x 22.1° @ 60cm). The stages remained presented through the trial. Over 500ms a toy appeared and dropped from the top to the vertical centre of the screen. Once the participant fixated the toy, the curtains on both the left and right stages lifted over 400ms. Over the next 750ms the toy moved to one of the (randomly chosen) stages. The toy remained motionless for 200ms before spinning for 400ms to engage attention at its stopping point on the stage. Over 400ms both stage curtains lowered, hiding the toy.</p>
<p>A central fixation was then presented until it was fixated by the participant; at this point it paused for 200ms before spinning for 200ms to engage attention and then disappeared. The task then waited for the participant to fixate one of the two curtains (defined as gaze being over the curtain for minimum 100ms); the choice was coded to be either correct or incorrect, depending upon whether or not the chosen curtain was hiding the toy. Over the next 400ms the chosen curtain was raised, revealing either the toy or an empty stage, depending upon whether the participant chose the correct side. If correct, the toy spun for 400ms as a reward before dropping off the bottom of the screen. The curtain then lowered. The reaction time (relative to the offset of the spinning central fixation stimulus) to choose a curtain, and the correctness of the choice, were recorded.</p>
</sec>
<sec id="s3d4b">
<label>2.4.4.2.</label><title>Data extraction</title>
<p>Accuracy was calculated as number of correct trials/ total number of valid trials. Saccadic reaction times were computed as the difference between the timestamps of the presentation of the first two rectangles and the first sample to enter a rectangle AOI, averaged across valid trials. Trials were considered invalid and were discarded if 2000ms elapsed without either curtain being fixated. The key dependent variables were accuracy of selecting the correct curtain (<underline>WM-Acc)</underline> and saccadic reaction times overall (<underline>WM-SRT-All);</underline> we additionally compared reaction times for each of correct and incorrect responses (<underline>WM-SRT-Acc</underline> and <underline>WM-SRT-Inacc</underline>). Infants were expected to achieve above chance-level accuracy (<xref ref-type="bibr" rid="c11">Daehler et al., 1976</xref>; <xref ref-type="bibr" rid="c27">Hofstadter &amp; Reznick, 1996</xref>).</p>
</sec>
</sec>
<sec id="s3d5">
<label>2.4.5</label><title>Visual search</title>
<sec id="s3d5a">
<label>2.4.5.1.</label><title>Stimulus presentation</title>
<p>Search displays consisted of three different items; red apple (target, 4.57cm x 4.57cm, 4.3° x 4.3°), blue apple (colour distractor, 4.57cm x 4.57cm, 4.3° x 4.3°) and a red slice of an apple (an elongated rectangle, cropped from the full apple image, shape distractor, 1.12cm x 6.67cm, 1.1° x 6.4°). These stimuli were used to produce two trail types, feature search and conjunction search trials.</p>
<p>Stimuli in feature search trials varied on only one dimension, either colour or shape (e.g. a red apple surrounded by blue apples, or a red full apple surrounded by red slices). The set size in these trials was always 9, with one target stimulus and 8 distractors. Conjunction search trials consisted of an equal number of both colour and shape distractors, and the set size was either 9 or 13. In total there were six possible configurations of trial, two feature search trials and four conjunction search trials.</p>
<p>Stimuli were arranged on screen algorithmically. All stimuli were required to be within a circular region located at the centre of screen with a diameter of 27.2cm (25.5° @ 60cm). The target stimulus was then positioned at a random point within this circle, ensuring that it was not within 6cm (5.7° @ 60cm) of the centre of the screen, where it would overlap with the central fixation stimulus presented at the start of each trial. Next, each distractor stimulus was placed at a random location within the circle, ensuring that no stimulus (target or distractor) overlapped the spatial location of any other stimulus.</p>
<p>Each trial began with an animated central stimulus unique to this task. The target stimulus (a red apple) “flew” into the screen from one edge (left, top, right, bottom – chosen randomly) over 800ms, ending at the centre of the screen. When the participant fixated the apple, it faded into the background colour of the screen over 750ms, and then the full array was drawn. Note that the location of the target amongst the distractors was not the central location of this attention-getter, indeed the target was never presented within 6cm of this location. The array was presented for 4000ms or until the participant fixated the target stimulus, after which the target span as a visual reward for 1300s until the trial ended.</p>
</sec>
<sec id="s3d5b">
<label>2.4.5.2.</label><title>Data extraction</title>
<p>Offline: AOIs were drawn around the target and distractors. Accuracy was calculated as the number of trials on which the infant fixated the target (apple) before the animation was automatically triggered/ the number of trials administered. Saccadic reaction time to find the target on each trial was the difference between the time at which the search slide was presented and the time of the first gaze sample in the target AOI for each condition. Trials were excluded if reaction time was less than 150ms or if the trial had been skipped.</p>
<p>The key dependent variables are accuracy (<underline>VS-S9-Acc</underline>, <underline>VS-C9-Acc</underline>, <underline>VS-C13-Acc)</underline> and the saccadic reaction time to find the target on each trial (<underline>VS-S9-SRT</underline>, <underline>VS-C9-SRT</underline>, <underline>VS-C13-SRT</underline>). Reaction times are expected to be fastest in the single search, then conjunctive 9-item search, then the conjunctive 13-item search (in line with <xref ref-type="bibr" rid="c21">Gerhardstein &amp; Rovee-Collier, 2002</xref>).</p>
</sec>
</sec>
<sec id="s3d6">
<label>2.4.6</label><title>Face Pop-out</title>
<sec id="s3d6a">
<label>2.4.6.1.</label><title>Stimulus presentation</title>
<p>Infants were presented with a series of six annular visual arrays each composed of five objects in different locations on the screen (<xref ref-type="bibr" rid="c22">Gliga, Elsabbagh, Andravizou &amp; Johnson, 2009</xref>; Hendry et al, 2018). Each array contained: a face with direct gaze, a visual ‘noise’ image generated from the same face presented within the array by randomising the phase spectra of the face whilst keeping the amplitude and colour spectra constant to act as a control for the low-level visual properties of the face stimuli (Halit, Csibra, Volein &amp; Johnson, 2004), a bird, a car and a mobile phone.</p>
</sec>
<sec id="s3d6b">
<label>2.4.6.2.</label><title>Data extraction</title>
<p>Areas-of-interest (AOIs) masks were placed around each stimulus. For each AOI, the proportion of samples within it was calculated by <italic>number of samples in AOI/ number of valid (non-missing) samples.</italic> Contiguous runs of samples within an AOI were identified and the mean proportion looking time and peak look duration to each AOI were calculated across valid trials only (see SM1.2.5.2.).</p>
<p>Peak look to each AOI is the duration of the longest look to that AOI during each trial averaged across the number of valid trials. Key dependent variables are the proportion of trials on which the infants first look to the face divided by the number of trials with a valid initial look (reflective of social orienting, <underline>Pop-Face-First</underline>), percent looking to faces (<underline>Pop-Face-Pct</underline>, reflective of general interest (<xref ref-type="bibr" rid="c14">Elsabbagh et al., 2013</xref>)), and two key comparative AOIs for examination of the success of the manipulation, selected because they are the best visual control for faces (scrambled face or ‘Noise’) or are an object of interest (Car); (<underline>Pop-Car-Pct</underline>, <underline>Pop-Noise-Pct</underline>); peak look to each AOI (reflective of sustained attention(<xref ref-type="bibr" rid="c23">Gui et al., 2020</xref>) <underline>Pop-Face-Peak</underline>, <underline>Pop-Car-Peak</underline>, <underline>Pop-Noise-Peak</underline>). Percentage looking and looking times are expected to be greater to faces than comparison stimuli (<xref ref-type="bibr" rid="c22">Gliga et al., 2009</xref>).</p>
</sec>
</sec>
<sec id="s3d7">
<label>2.4.7</label><title>Dancing ladies</title>
<sec id="s3d7a">
<label>2.4.7.1.</label><title>Stimulus presentation</title>
<p>Each trial consisted of the presentation of one of six videos (20-25s duration each, 25 fps, 1920×1080 resolution) of three women dancing with objects. Videos were designed to be semi-naturalistic. Three videos were played in their native format and three matched videos were visually-scrambled such that the social information was degraded.</p>
</sec>
<sec id="s3d7b">
<label>2.4.7.2.</label><title>Data extraction</title>
<p>Offline: AOIs were hand traced around the faces and objects on a frame-by-frame basis using Motion (Apple Inc, USA) software, and were approximately 14.5° x 33.5° @ 60cm. Samples were assigned to AOIs and interpolated across gaps of &lt; 200ms preceded and succeeded by the same AOI. Percent attention to each AOI was computed as <italic>number of samples in each AOI/ total number of valid samples</italic> for that video and averaged across videos within each condition; peak look to each AOI was defined as the longest run of samples within one AOI during each video, averaged across videos within each condition. Data was excluded if there were fewer than 25% valid samples for each video. Key dependent variables were percent attention to faces, <underline>Dance-Soc-Face-Pct</underline>, <underline>Dance-Scr-Face-Pct</underline>) and peak look (<underline>Dance-Soc-Face-Peak</underline>, <underline>Dance-Scr-Face-Peak</underline>). <underline>I</underline>n addition, as a comparator we examined percent attention to the object (<underline>Dance-Soc-Object-Pct, Dance-Scr-Object-Pct</underline>); peak look to the object (<underline>Dance-Soc-Object-Peak</underline>, <underline>Dance-Scr-Object-Peak</underline>). Looking times are expected to be greater to faces than comparison stimuli (<xref ref-type="bibr" rid="c20">Frank et al., 2014</xref>; <xref ref-type="bibr" rid="c22">Gliga et al., 2009</xref>).</p>
</sec>
</sec>
<sec id="s3d8">
<label>2.4.8</label><title>50 Fifty faces</title>
<sec id="s3d8a">
<label>2.4.8.1</label><title>Stimulus presentation</title>
<p>Originating from the “50 People, One Question” project (Krolak, 2011), infants watched a video comprised of street interviews in English with a number of people (41s, 1280px x 720px, 25fps): We removed the soundtrack of the original video and replaced it with classical music in order not to introduce linguistic confounds.</p>
</sec>
<sec id="s3d8b">
<label>2.4.8.2</label><title>Data extraction</title>
<p>Offline: AOIs were hand traced around the faces, bodies and background people on a frame-by-frame basis using Motion (Apple Inc, USA) software. Samples were assigned to AOIs and interpolated across gaps of &lt;200ms preceded and succeeded by the same AOI. Percent attention to each AOI was computed <italic>as number of samples in each AOI/total number of valid samples for the video</italic>; peak look to each AOI was defined as the longest run of samples within one AOI during the video. Key dependent variables were percent attention to faces (<underline>50Face-Face-Pct</underline>); peak look to faces (<underline>50Face-Face-Peak</underline>). Data was excluded if there were fewer than 25% valid samples for the video. To examine whether social attention was greater than to other elements of the scene, we also examined looking to the background (<underline>50Face-Background-Pct, 50Face-Background-Peak).</underline> Looking times are expected to be greater to faces than comparison stimuli (<xref ref-type="bibr" rid="c20">Frank et al., 2014</xref>; <xref ref-type="bibr" rid="c22">Gliga et al., 2009</xref>).</p>
</sec>
</sec>
</sec>
<sec id="s3e">
<label>2.5.</label><title>Data quality assessment</title>
<p>In addition to task-specific data quality metrics (duration of valid data extracted from free viewing tasks and number of trials available from trial-based tasks), we computed two measures of the general quality of the eye-tracking data across the session. Accuracy (the spatial displacement of recorded gaze from the point fixated) and precision (variability in consecutive samples on the same fixation point) were extracted as proxies of general eye-tracking quality across the session (see SM1.3 for details).</p>
</sec>
<sec id="s3f">
<label>2.6.</label><title>Statistics</title>
<p>We first examined a range of variables pertinent to the valid acquisition of data from each task. These included the percentage of children who provided valid data; whether key metrics significantly varied with data quality (operationalised as having a correlation coefficient &gt;= .2 or &lt;= -.2); and whether the expected pattern of condition differences was elicited by the tasks administered in combination (SM1.4 for details). Expected condition differences were assessed using Analysis of Variance tests (ANOVAs) or one-tailed t-tests where appropriate and where we had a strong directional hypothesis. Analyses were repeated after children with too few valid trials were removed (SM2.3) and additional analyses were conducted to assess the impact of task-specific data quality metrics on condition effects (see SM2.2.2 &amp; SM2.2.3 for details). Finally, we used a hypothesis-driven SEM modelling approach to examine the underlying structure of visual attention (using lavaan (<xref ref-type="bibr" rid="c6">Best, 2020</xref>)). Selected scores from individual tasks were z-scored (see <xref rid="fig3" ref-type="fig">Figure 3</xref>); we initially selected items to fit a theoretically motivated three-factor model (representing social attention, and exogenous and endogenous shifting) with equal numbers of items (three items from three tasks each). We used a factor analysis to decompose key data quality metrics (Accuracy, Precision, Gap valid trial numbers, reversal learning trial numbers, proportion lost samples in the popout, dancing ladies and fifty faces) into two factors representing accuracy and precision, and data quantity; these factors were then entered into the confirmatory factor analysis as covariates.</p>
</sec>
</sec>
<sec id="s4">
<title>Results</title>
<sec id="s4a">
<label>3.1</label><title>Feasibility of the battery</title>
<p>Overall, both data retention and data quality were good, indicating that the current eye-tracking battery was successfully implemented.</p>
<sec id="s4a1">
<label>3.1.1</label><title>Data retention</title>
<p>Retention of data was good (<xref rid="tbl2" ref-type="table">Table 2</xref>). A high proportion of children (at least 70%) reached and provided at least some valid data for each task in the battery. For all but the non-social contingency task, over 90% of those who reached the task provided enough data to be included in condition effect analyses and for all tasks fewer than 10% of these participants were excluded after cut-off criteria were applied. Tasks towards the end of the battery were typically reached by fewer participants, though a high percentage (over 90%) of the children who reached these tasks did provide usable data.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><p>Data retention number and rates for each task. All percentages are based on children who had enough data for inclusion on the highest yield variable</p></caption>
<graphic xlink:href="534573v1_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s4a2">
<label>3.1.2</label><title>Data quality</title>
<p>Data quality was generally good (<xref rid="fig2" ref-type="fig">Fig. 1</xref>). Accuracy and precision were 1.7 (<italic>SD</italic> = 0.8) and 1.5 (<italic>SD</italic> = 0.5) degrees respectively, with AOIs typically ranging from 4.3-39 degrees, indicating that the quality of the eye-tracking data was generally sufficient for the task design. Despite cleaning and validation procedures, accuracy and precision did associate significantly with individual differences in key variables across many tasks (SM2.2.1, tables S4 &amp; S5).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Illustration of the tasks. a) Gap overlap task, showing baseline (A), overlap (B) and gap (C) conditions; b) non-social contingency task; c) reversal learning task, showing the attention-getter (A) and learning (B) and reversal (C) phases; d) working memory task, showing the object appearing (A), moving under a curtain (B), central attention-getter (C), then choice of curtain (D) and revealing of reward (E); e) visual search task pop out task, showing the target (A),single (B) and conjunctive (C) search conditions; f) pop-out task; g) dancing ladies task in both unscrambled (A) and scrambled (B) conditions; h) fifty faces video task.</p></caption>
<graphic xlink:href="534573v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Raincloud plots for (a) precision and (b) accuracy of eye-tracking in the whole sample. <italic>All raincloud plots in this paper were based on <xref ref-type="bibr" rid="c2">Allen et al. (2021</xref>)</italic>.</p></caption>
<graphic xlink:href="534573v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Structual Equation Model of the structure of visual attention within the current eyetracking battery. Although our original model with three factors (a) provided a good fit to the data, it was not significantly better than a model with two latent variables representing social and non-social attention (b).</p></caption>
<graphic xlink:href="534573v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="534573v1_fig3a.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s4b">
<label>3.3</label><title>Condition differences</title>
<p>The expected pattern of condition differences was observed in each task, apart from the working memory task. Briefly, a) in the gap-overlap (SM3.2.2) there were significantly longer reaction times in the overlap than baseline condition, and in the baseline than gap condition (F(1.68, 556.52) = 1015.76, <italic>n</italic> = 332, <italic>p</italic> &lt; 0.001, η<sub>p</sub><sup>2</sup> = 0.75’ SM2.2.1); b) in the non-social contingency task (SM3.2.2), times to return to the fixation stimulus in the sixty condition (<italic>M</italic> = 488.06ms, <italic>SD</italic> = 170.78) were faster than in both the zero (<italic>M</italic> = 553.21ms, <italic>SD</italic> = 285.92) and hundred conditions (<italic>M</italic> = 661.82ms, <italic>SD</italic> = 352.26); (Overall condition, F(1.89, 475.86) = 37.88, <italic>p</italic> &lt; 0.001, η<sub>p</sub><sup>2</sup> = 0.13; sixty versus zero, F(1, 252) = 11.90, p = 0.001, η<sub>p</sub><sup>2</sup> = 0.05; sixty versus hundred, F(1, 252) = 59.69, <italic>p</italic> &lt; 0.001, η<sub>p</sub><sup>2</sup> = 0.19 ; SM2.2.2); c) in the reversal learning task (SM3.2.3), accuracy was significantly higher in the pre-switch phase (<underline>CC-Pre-Acc)</underline> (<italic>M</italic> = 0.83, <italic>SD</italic> = 0.17) compared to the post-switch phase (<underline>CC-Post-Acc)</underline> (<italic>M</italic> = 0.78, <italic>SD</italic> = 0.25, <italic>n</italic> = 203), <italic>t</italic>(202) = 2.10, <italic>p</italic> = 0.04, CIs 95%[0.003, 0.10], <italic>d</italic> = 0.23); d) in the working memory task (SM2.2.4), a one-sample two-tailed t-test revealed that the proportion of trials in which participants chose the correct location (<underline>WM-Acc)</underline> was not significantly different to chance, <italic>t</italic>(330) = -1.91, <italic>p</italic> = 0.06, [-0.04, 0.001], <italic>d</italic> = - 0.11, <italic>M</italic> = 0.48, <italic>SD</italic> = 0.19; e) in the visual search task (SM2.2.5), reaction times were slower in the conjunctive (<italic>M</italic> = 1316.68ms, <italic>SD</italic> = 36.12) than single feature displays (<italic>M</italic> = 938.79ms, <italic>SD</italic> = 241.21) with nine elements; (Overall effect of condition in ANOVA, <italic>F</italic>(1.76, 450.90) = 159.90, <italic>p</italic> &lt; 0.001, <italic>n</italic> = 257, η<sub>p</sub><sup>2</sup> = 0.38; <underline>VS-S9-SRT</underline> versus <underline>VS-C9-SRT</underline>, <italic>F</italic>(1, 256) = 255.46, <italic>p</italic> &lt; 0.001, η<sub>p</sub><sup>2</sup> = 0.50) and slower in the conjunctive thirteen (<italic>M</italic> = 1391.81ms, <italic>SD</italic> = 372.28) than conjunctive nine conditions; (F(1, 256) = 5.62, <italic>p</italic> = 0.02, η<sub>p</sub><sup>2</sup> = 0.02; Figure 6); f) in the Face pop-out (SM2.2.6), there was more looking to faces than cars and scrambled faces (Overall, <italic>F</italic>(1.39, 354.20) = 165.96, <italic>p</italic> &lt; 0.001, η<sub>p</sub><sup>2</sup> = 0.39; face versus car, <italic>F</italic>(1, 255) = 60.31, <italic>p</italic> &lt; 0.001, η<sub>p</sub><sup>2</sup> = 0.19; face versus noise, <italic>F</italic>(1, 255) = 607.29, <italic>p</italic> &lt; 0.001, η<sub>p</sub><sup>2</sup> = 0.70; means in table 3); g) in the Dancing Ladies videos (SM2.2.7), proportion looking to faces (<italic>M</italic> = 0.14, se = 0.01) was also significantly higher than to objects (<italic>M</italic> = 0.10, se = 0.002), (<italic>F</italic>(1, 248) = 29.66, <italic>p</italic> &lt; 0.001, η<sub>p</sub><sup>2</sup> = 0.11), and proportion looking to faces was significantly higher in social (<italic>M</italic> = 0.20, sd = 0.14) than scrambled conditions (<italic>M</italic> = 0.07, <italic>SD</italic> = 0.06), (<italic>t</italic>(248)) = 16.67, <italic>p</italic> &lt; 0.001, <italic>d</italic> = 1.21); h) in the fifty faces video (SM2.2.8) proportion looking to faces (<italic>M</italic> = 0.57, <italic>SD</italic> = 0.15) was also significantly higher than to background people (<italic>M</italic> = 0.03, <italic>SD</italic> = 0.03), (<italic>t</italic>(245) = 53.83, p &lt; 0.001, <italic>d</italic> = 4.99). Thus, each task (with the exception of working memory) replicated previously-reported condition effects or showed the predicted pattern, confirming they can be robustly combined within a large-scale battery.</p>
</sec>
<sec id="s4c">
<label>3.3.</label><title>Factor structure</title>
<p>We first inspected bivariate correlations between core variables from each task (reaction time/accuracy in the cognitive control and working memory; reaction time in the gap; reaction time and proportion of trials retained in the non-social contingency; % looking and peak look to faces in the pop-out, dancing ladies and fifty faces tasks; accuracy in the visual search). This showed strong intercorrelations between variables from the same task, and weaker correlations between tasks. Based on this information and literature-based hypotheses, we conducted a theoretically-motivated structural equation model of the visual attention battery (using sem in Lavaan). We initially tested a three-factor structure with three variables from each of three tasks per construct (proportion looking to faces, peak look and mean look duration to faces from face pop-out, dancing ladies and fifty faces), exogenous orienting (saccadic task reaction times in the three gap-overlap conditions, fixation reaction time in the three conditions of the non-social contingency task, mu, sigma and tau parameters of the exGaussian modelling of reaction times) and endogenous attention (working memory accuracy and reaction time to correct and all trials, cognitive control accuracy in the learning conditions and reaction time during learning, and search accuracy across three conditions in the visual search; <xref rid="fig2" ref-type="fig">Figure 2a</xref>). This produced a good fit to the data n=194 RMSEA = 0.034 (CI = 0.019-0.045, p = .99); CFI = 0.975; TLI = 0.9656; AIC = 11551; BIC = 12012; (ξ2 (294) = 358, p =0.006, ξ <sup>2</sup>/df=1.21. However, model comparisons showed that a two-factor structure with the exogenous and endogenous factors combined was not a significantly worse fit to the data (ξ2 diff(2)= 1.61, p = 0.45) and was more parsimonious; we thus retained the two-factor model, which separated social and non-social attention. Two data quality measures were also included based on a factor analytic decomposition of accuracy and precision and core data quality measures from each task; this revealed two factors reflecting accuracy and precision (21% of the variance) and median lost samples in the free viewing tasks (20% of the variance). The model produced a good fit using both multiple imputation to derive scores from all children (n=350 RMSEA = 0.033 (CI = 0.025-0.040, p = 1); CFI = 0.969; TLI = 0.958; AIC = 19340; BIC = 19988; ξ <sup>2</sup>(296) = 408, p &lt; 0.001, (ξ2/df=1.38) and complete cases only (n=194 RMSEA = 0.034 (CI = 0.02-0.046, p = .99); CFI = 0.974; TLI = 0.965; AIC = 11552; BIC = 12006; (ξ2 (296) = 363, p =0.005, ξ <sup>2</sup>/df=1.22; (<xref rid="fig3" ref-type="fig">Figure 3</xref>). Further, this model was a significant improvement on a model with only one latent factor (n=194 RMSEA = 0.037 (CI = 0.024-0.048, p = .98); CFI = 0.970; TLI = 0.959; AIC = 11563; BIC = 12014; (ξ2 (297) = 376, p = 0.001, ξ <sup>2</sup>/df=1.27; ξ2 diff(3)= 13.13, p = 0.0003). The two latent variables were not associated (n=194 B=0.087 SE=0.084 z=1.04, p = 0.3). Higher scores for social attention represent more interest in social content; higher scores for non-social attention represent slower and less accurate cognitive responses (<xref rid="fig2" ref-type="fig">Figure 2b</xref>). These summary scores may prove useful for investigators wishing to represent distinct sources of variance in the battery as a whole.</p>
</sec>
</sec>
<sec id="s5">
<title>Discussion</title>
<p>Although much existing research has used eye-tracking to assess visual attention in infants and young children, little work has considered the feasibility of conducting large-scale eye-tracking studies in toddlers. In this study, we collected data from a battery of eight eye-tracking tasks completed by over three hundred 18-month-olds. Data were successfully collected from 99.1% (347) of toddlers on at least one task. Analyses found expected condition effects in seven out of eight eye-tracking tasks, with only the working memory task not finding the expected effects at the group level. A hypothesis-driven SEM provided a good fit to the data, indicating that in addition to indices from individual tasks, the battery can be used to extract global measures of social and non-social attention. Using maximum likelihood imputation allows latent variable measures to be extracted for all infants in the sample. Overall, the current eye-tracking battery provides a feasible and objective measure of visual attention in 18-month-old children.</p>
<sec id="s5a">
<title>Overall feasibility</title>
<p>Overall feasibility of the study was good. For every task at least 70% of children provided enough data to be included in analyses; this was 90% for four of the first five tasks in the battery. After cut-off criteria were applied, at least 64% of the whole sample were retained; over 86% for the four tasks with highest retention. Fewer than 10% of children failed to meet cut-off criteria, indicating that the majority of participants who completed the task produced valid data and supporting the feasibility of the eye-tracking tasks in this age range. Furthermore, all but one condition effects were the same if children who did and did not meet typical cut-offs were included, indicating that patterns of performance reported in this age range do not just capture effects seen in children with stronger attentiveness.</p>
<p>Despite validation procedures, data quality still impacted extracted metrics. For free-viewing tasks, data duration was associated with looking to faces, and the degree to which looking to faces was greater than comparison stimuli. One potential explanation is that children who spend less time looking at the stimuli have less time to process the content and show less differential endogenously-mediated attention to stimulus features. Alternatively, it may be that children who are less interested in faces and people spend less time looking at the videos, which feature prominent social content. Such considerations are important to balance when interpreting relations between variables of interest and data quality. Metrics which were associated with either accuracy or precision metrics at a level of &gt;0.2 (suggesting they should be controlled in research using these tasks) include gap reaction time in the Gap-Overlap task, reaction times in the non-social contingency task, singleton nine visual search accuracy, percentage looking to faces and multiple peak look duration variables in the dancing ladies and percentage looking and peak look duration to faces in the Fifty Faces free viewing video. This is likely because even with expanded areas of interest, lower accuracy and precision leads to less certainty in gaze sample classification. Loadings in the model suggested precision and accuracy measures may be better captured as one data quality factor, with median lost samples another; as such, controlling for these factors may be the most parsimonious approach.</p>
</sec>
<sec id="s5b">
<title>Task robustness</title>
<p>The majority of tasks in the current battery produced robust condition effects in the expected direction. These included competition effects in visual orienting, the ability to use memory to find a video, set size and pop-out effects on search for a visual target and the preference for faces in early development. These findings indicate that it is possible to robustly measure many previously observed effects, even within a large battery of tasks. This has important implications for the replicability and validity of these findings, strengthening support for their generalisability and robustness. In the light of increasing concerns about the reproducibility of cognitive psychology (<xref ref-type="bibr" rid="c30">Huber et al., 2019</xref>), this set of replications provides important reassurance that developmental science has produced a range of robust observations about the developing infant attention system.</p>
<p>The one exception was the working memory task, which only provided some weak evidence of working memory at the group level when trial-level reaction time data were analysed. This may reflect the fact that this skill is relatively fragile at this age; indeed, there are few robust demonstrations of single-trial level working memory success at 18 months (<xref ref-type="bibr" rid="c25">Hendry et al., 2016</xref>). One important task feature may be that the child had to actively ‘find’ the object with their gaze, rather than passively watch sequences of objects as has previously been used in other working memory tasks with younger infants (<xref ref-type="bibr" rid="c1">Ahmed &amp; Ruffman, 1998</xref>; <xref ref-type="bibr" rid="c5">Baillargeon et al., 1985</xref>). Indeed, <xref ref-type="bibr" rid="c28">Hood et al. (2003)</xref> found that the same two-and-a-half-year-olds demonstrated ability to recognise impossible locations of an object but failed to complete active search and retrieval of the object, indicating a disparity between active search and passive observation. Although children typically succeed at passive visual tasks at a substantially younger age than behavioural analogues (<xref ref-type="bibr" rid="c1">Ahmed &amp; Ruffman, 1998</xref>; <xref ref-type="bibr" rid="c4">Baillargeon &amp; Graber, 1988</xref>; <xref ref-type="bibr" rid="c27">Hofstadter &amp; Reznick, 1996</xref>), it may be that this active component makes tasks more difficult.</p>
</sec>
<sec id="s5c">
<title>Structure of visual attention</title>
<p>Once strong within-task associations were accounted for, a hypothesis-driven SEM provided a good fit to the data. The most parsimonious structure indicated two latent variables that can be interpreted as social attention (interest in people) and non-social attention (speed and accuracy of saccades). We used overall measures of accuracy and reaction time in this model because condition difference scores can be unstable (combining the noise inherent in both variables). Indeed, <xref ref-type="bibr" rid="c13">Draheim et al. (2019)</xref> suggested that the lack of correlation between attention measures in many studies may be due to methodological issues with attention capture tasks, rather than that attention not representing an unified concept.</p>
<p>They conclude that accuracy-based measures may provide more reliable and valid measures of attention control than reaction times and difference scores. This assessment was based on a number of judgements, including how well metrics from various tasks inter-correlated with one another; notably, considerable correlations were found even between metrics from tasks which make markedly different demands on the participant.</p>
<p>The lack of correlations between the two latent variables in our SEM is consistent with theoretical models in which social attention represents a distinct construct. The large sample size in the current work implies this result is not due to lack of statistical power. Based on this, eye-tracking metrics should be carefully chosen in future work in this field, to ensure that tasks measure an appropriate facet of visual attention. Although many models of attention distinguish exogenous and endogenous orienting, inclusion of this distinction in our modelling did not explain significantly more variance than a two-factor model. Neuroimaging studies have indicated that both endogenous and exogenous attention shifts are mediated by the same large-scale fronto-parietal networks, indicating they may be closely related constructs (<xref ref-type="bibr" rid="c38">Peelen et al., 2004</xref>). However, other studies have shown distinct impacts of environmental variation (such as screen exposure) on exogenous and endogenous attention shifting (<xref ref-type="bibr" rid="c39">Portugal et al., 2021</xref>). A model incorporating the endogenous/exogenous distinction did provide an adequate fit to the data, and may be preferred by investigators who wish to distinguish these components of visual attention at the individual level.</p>
<p>Though this work draws strengths from its large sample size and substantial task battery, there are some limitations. Whilst it is assumed that eye-tracking tasks provide a direct measure of attention, assumptions are made about specifically what aspect of attention tasks are tapping. Most tasks in the current battery have been widely used previously; the current study serves to establish previously-found condition effects in this large toddler sample. Yet, inaccurate assumptions about the processes captured by different tasks may lead to mistakes in how effects are interpreted. Additionally, processing of data has been carried out in line with other work using these tasks, but this could be problematic. Measures calculated directly from the eye-tracker remove an element of subjective assessment, but there remain numerous options for processing; some of these decisions may impact the results found in later analyses. Such limitations ought to be considered by eye-tracking researchers. Indeed, there is a continuing effort to establish reliable and valid eye-tracking batteries in young children (<xref ref-type="bibr" rid="c42">van Baar et al., 2020</xref>); the current work contributes to this field by establishing the robustness of these eye-tracking tasks.</p>
<p>The current study demonstrates that this large eye-tracking battery can be successfully used with 18-month-olds and produces both task-based and cross-task indices that may be useful for large-scale assessment of visual attention in toddlers. Our dataset is embedded in a large-scale study of child development (<ext-link ext-link-type="uri" xlink:href="http://www.developingconnectome.org/">http://www.developingconnectome.org/</ext-link>) that also includes measures of neonatal brain structure (<xref ref-type="bibr" rid="c7">Bozek et al., 2018</xref>) and function (<xref ref-type="bibr" rid="c16">Eyre et al., 2021</xref>) and clinical and developmental assessments in toddlerhood. This context provides a strong foundation for investigators to examine the relation between individual differences in attentional processing and other aspects of child development. Further, investigators can access our task battery (built in TaskEngine; <ext-link ext-link-type="uri" xlink:href="https://sites.google.com/site/taskenginedoc/">https://sites.google.com/site/taskenginedoc/</ext-link>) for use in their own large-scale cohort studies, facilitating greater robustness and comparability in large scale studies of child development; ongoing efforts include assessments in infants with a family history of autism (<xref ref-type="bibr" rid="c32">Jones et al., 2019</xref>), infants with a history of heart problems and premature infants. In summary, we provide a resource for investigators to further probe the role of individual differences in visual attention in toddlerhood, and their relation to structural brain development and clinical outcomes.</p>
</sec>
</sec>
<sec id="d1e1316" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1437">
<label>Supplementary Materials</label>
<media xlink:href="supplements/534573_file02.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We are grateful for all the families who kindly agreed to participate in the project and recognize their particular commitment in remaining engaged with the programme during the COVID-19 Pandemic. We also acknowledge the support of the Neonatal Intensive Care Unit and the Newborn Imaging Centre at Evelina London Children’s Hospital. We also thank the external advisory board for their expert advice and contribution across the dHCP project: David van Essen, Arthur Toga, Richard Frackowiak, Dan Marcus, Petra Huppi, Essa Yacoub, and John Ashburner.</p>
</ack>
<sec id="s6">
<title>Funding</title>
<p>The Developing Human Connectome Project was funded by the European Research Council under the European Union Seventh Framework Programme (FP/20072013)/ERC Grant Agreement no. 319456. This work was supported by the NIHR Biomedical Research Centres at Guys and St Thomas NHS Trust and the South London and Maudsley NHS Trust; the ESPRC/Wellcome Centre for Medical Engineering; and the MRC Centre for Neurodevelopmental Disorders. This research was supported by awards from the Medical Research Council (MR/K021389/1; MR/T003057/1, EJ, MHJ, TC),ESRC grant no. ES/R009368/1, MQ (MQ14PP_83, MHJ, EJHJ, TC), Further, this work was also supported by the EU-AIMS and AIMS-2-TRIALS programmes funded by the Innovative Medicines Initiative (IMI) Joint Undertaking Grant Nos. 115300 (MHJ, TC) and No. 777394 (MHJ, EJHJ and TC; European Union’s FP7 and Horizon 2020, respectively). This Joint Undertaking receives support from the European Union’s Horizon 2020 research and innovation programme, with in-kind contributions from the European Federation of Pharmaceutical Industries and Associations (EFPIA) companies and funding from Autism Speaks, Autistica and SFARI. Any views expressed are those of the author(s) and not necessarily those of the funders.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Ahmed</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Ruffman</surname>, <given-names>T</given-names></string-name>. (<year>1998</year>). <article-title>Why do infants make A not B errors in a search task, yet show memory for the location of hidden objects in a nonsearch task?</article-title> <source>Developmental Psychology</source>, <volume>34</volume>(<issue>3</issue>), <fpage>441</fpage>–<lpage>453</lpage>. <pub-id pub-id-type="doi">10.1037/0012-1649.34.3.441</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Allen</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Poggiali</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Whitaker</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Marshall</surname>, <given-names>T. R.</given-names></string-name>, <string-name><surname>Langen</surname>, <given-names>J. van</given-names></string-name>, &amp; <string-name><surname>Kievit</surname>, <given-names>R. A.</given-names></string-name> (<year>2021</year>). <article-title><italic>Raincloud plots: A multi-platform tool for robust data visualization</italic> (4:63)</article-title>. <source>Wellcome Open Research</source>. <pub-id pub-id-type="doi">10.12688/wellcomeopenres.15191.2</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Amso</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Scerif</surname>, <given-names>G</given-names></string-name>. (<year>2015</year>). <article-title>The attentive brain: Insights from developmental cognitive neuroscience</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>16</volume>(<fpage>10</fpage>), Article 10. <pub-id pub-id-type="doi">10.1038/nrn4025</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Baillargeon</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Graber</surname>, <given-names>M</given-names></string-name>. (<year>1988</year>). <article-title>Evidence of location memory in 8-month-old infants in a nonsearch AB task</article-title>. <source>Developmental Psychology</source>, <volume>24</volume>(<issue>4</issue>), <fpage>502</fpage>–<lpage>511</lpage>. <pub-id pub-id-type="doi">10.1037/0012-1649.24.4.502</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Baillargeon</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Spelke</surname>, <given-names>E. S.</given-names></string-name>, &amp; <string-name><surname>Wasserman</surname>, <given-names>S</given-names></string-name>. (<year>1985</year>). <article-title>Object permanence in five-month-old infants</article-title>. <source>Cognition</source>, <volume>20</volume>(<issue>3</issue>), <fpage>191</fpage>–<lpage>208</lpage>. <pub-id pub-id-type="doi">10.1016/0010-0277(85)90008-3</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="other"><string-name><surname>Best</surname>, <given-names>J</given-names></string-name>. (<year>2020</year>). <article-title>Physical Fitness and Age-Related Differences in Cognition and Cortical Thickness in Young Adulthood</article-title>. <pub-id pub-id-type="doi">10.31234/osf.io/6yc7t</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Bozek</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Makropoulos</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schuh</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Fitzgibbon</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Wright</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Coalson</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>O’Muircheartaigh</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hutter</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Price</surname>, <given-names>A. N.</given-names></string-name>, <string-name><surname>Cordero-Grande</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Teixeira</surname>, <given-names>R. P. A. G.</given-names></string-name>, <string-name><surname>Hughes</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Tusor</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Baruteau</surname>, <given-names>K. P.</given-names></string-name>, <string-name><surname>Rutherford</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Edwards</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Hajnal</surname>, <given-names>J. V.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, … <string-name><surname>Robinson</surname>, <given-names>E. C</given-names></string-name>. (<year>2018</year>). <article-title>Construction of a neonatal cortical surface atlas using Multimodal Surface Matching in the Developing Human Connectome Project</article-title>. <source>NeuroImage</source>, <volume>179</volume>, <fpage>11</fpage>–<lpage>29</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.06.018</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Braithwaite</surname>, <given-names>E. K.</given-names></string-name>, <string-name><surname>Gui</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Jones</surname>, <given-names>E. J. H</given-names></string-name>. (<year>2020</year>). <article-title>Social attention: What is it, how can we measure it, and what can it tell us about autism and ADHD?</article-title> <source>In Progress in Brain Research. Elsevier</source>. <pub-id pub-id-type="doi">10.1016/bs.pbr.2020.05.007</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Chevallier</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kohls</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Troiani</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Brodkin</surname>, <given-names>E. S.</given-names></string-name>, &amp; <string-name><surname>Schultz</surname>, <given-names>R. T</given-names></string-name>. (<year>2012</year>). <article-title>The Social Motivation Theory of Autism</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>16</volume>(<issue>4</issue>), <fpage>231</fpage>–<lpage>239</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2012.02.007</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Connor</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Egeth</surname>, <given-names>H. E.</given-names></string-name>, &amp; <string-name><surname>Yantis</surname>, <given-names>S</given-names></string-name>. (<year>2004</year>). <article-title>Visual Attention: Bottom-Up Versus Top-Down</article-title>. <source>Current Biology</source>, <volume>14</volume>(<issue>19</issue>), <fpage>R850</fpage>–<lpage>R852</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2004.09.041</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Daehler</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bukatko</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Benson</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Myers</surname>, <given-names>N</given-names></string-name>. (<year>1976</year>). <article-title>The effects of size and color cues on the delayed response of very young children</article-title>. <source>Bulletin of the Psychonomic Society</source>, <volume>7</volume>(<issue>1</issue>), <fpage>65</fpage>–<lpage>68</lpage>. <pub-id pub-id-type="doi">10.3758/BF03337124</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Dawson</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Webb</surname>, <given-names>S. J.</given-names></string-name>, &amp; <string-name><surname>McPartland</surname>, <given-names>J</given-names></string-name>. (<year>2005</year>). <article-title>Understanding the Nature of Face Processing Impairment in Autism: Insights From Behavioral and Electrophysiological Studies</article-title>. <source>Developmental Neuropsychology</source>, <volume>27</volume>, <fpage>403</fpage>–<lpage>424</lpage>. <pub-id pub-id-type="doi">10.1207/s15326942dn2703_6</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="other"><string-name><surname>Draheim</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Tsukahara</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Martin</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Mashburn</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Engle</surname>, <given-names>R. W</given-names></string-name>. (<year>2019</year>). <source>A toolbox approach to improving the measurement of attention control</source>. PsyArXiv. <pub-id pub-id-type="doi">10.31234/osf.io/q985d</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Elsabbagh</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gliga</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Pickles</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hudry</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Charman</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Johnson</surname>, <given-names>M. H</given-names></string-name>. (<year>2013</year>). <article-title>The development of face orienting mechanisms in infants at-risk for autism</article-title>. <source>Behavioural Brain Research</source>, <volume>251</volume>, <fpage>147</fpage>–<lpage>154</lpage>. <pub-id pub-id-type="doi">10.1016/j.bbr.2012.07.030</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Elsabbagh</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Volein</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Holmboe</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Tucker</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Csibra</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Baron-Cohen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bolton</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Charman</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Baird</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Johnson</surname>, <given-names>M. H.</given-names></string-name> (<year>2009</year>). <article-title>Visual orienting in the early broader autism phenotype: Disengagement and facilitation</article-title>. <source>Journal of Child Psychology and Psychiatry</source>, <volume>50</volume>(<issue>5</issue>), <fpage>637</fpage>–<lpage>642</lpage>. <pub-id pub-id-type="doi">10.1111/j.1469-7610.2008.02051.x</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Eyre</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Fitzgibbon</surname>, <given-names>S. P.</given-names></string-name>, <string-name><surname>Ciarrusta</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Cordero-Grande</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Price</surname>, <given-names>A. N.</given-names></string-name>, <string-name><surname>Poppe</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Schuh</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hughes</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>O’Keeffe</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Brandon</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Cromb</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Vecchiato</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Andersson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Duff</surname>, <given-names>E. P.</given-names></string-name>, <string-name><surname>Counsell</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Rueckert</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hajnal</surname>, <given-names>J. V.</given-names></string-name>, <string-name><surname>Arichi</surname>, <given-names>T.</given-names></string-name>, … <string-name><surname>Edwards</surname>, <given-names>A. D</given-names></string-name>. (<year>2021</year>). <article-title>The Developing Human Connectome Project: Typical and disrupted perinatal functional connectivity</article-title>. <source>Brain</source>, <volume>144</volume>(<issue>7</issue>), <fpage>2199</fpage>–<lpage>2213</lpage>. <pub-id pub-id-type="doi">10.1093/brain/awab118</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Falck-Ytter</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hamrefors</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sanches</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Portugal</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Taylor</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Viktorsson</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Hardiansyah</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Myers</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Westberg</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Bölte</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tammimies</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Ronald</surname>, <given-names>A</given-names></string-name>. (<year>2021</year>). <article-title>The Babytwins Study Sweden (BATSS): A Multi-Method Infant Twin Study of Genetic and Environmental Factors Influencing Infant Brain and Behavioral Development</article-title>. <source>Twin Research and Human Genetics</source>, <volume>24</volume>(<issue>4</issue>), <fpage>217</fpage>–<lpage>227</lpage>. <pub-id pub-id-type="doi">10.1017/thg.2021.34</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Farroni</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>M. H.</given-names></string-name>, <string-name><surname>Menon</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Zulian</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Faraguna</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Csibra</surname>, <given-names>G</given-names></string-name>. (<year>2005</year>). <article-title>Newborns’ preference for face-relevant stimuli: Effects of contrast polarity</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>102</volume>(<issue>47</issue>), <fpage>17245</fpage>–<lpage>17250</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0502205102</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Fox</surname>, <given-names>S. E.</given-names></string-name>, <string-name><surname>Levitt</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Iii</surname>, <given-names>C. A. N</given-names></string-name>. (<year>2010</year>). <article-title>How the Timing and Quality of Early Experiences Influence the Development of Brain Architecture</article-title>. <source>Child Development</source>, <volume>81</volume>(<issue>1</issue>), <fpage>28</fpage>–<lpage>40</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-8624.2009.01380.x</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Frank</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Amso</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Johnson</surname>, <given-names>S. P</given-names></string-name>. (<year>2014</year>). <article-title>Visual search and attention to faces during early infancy</article-title>. <source>Journal of Experimental Child Psychology</source>, <volume>118</volume>, <fpage>13</fpage>–<lpage>26</lpage>. <pub-id pub-id-type="doi">10.1016/j.jecp.2013.08.012</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Gerhardstein</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Rovee-Collier</surname>, <given-names>C</given-names></string-name>. (<year>2002</year>). <article-title>The Development of Visual Search in Infants and Very Young Children</article-title>. <source>Journal of Experimental Child Psychology</source>, <volume>81</volume>(<issue>2</issue>), <fpage>194</fpage>–<lpage>215</lpage>. <pub-id pub-id-type="doi">10.1006/jecp.2001.2649</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Gliga</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Elsabbagh</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Andravizou</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Johnson</surname>, <given-names>M</given-names></string-name>. (<year>2009</year>). <article-title>Faces Attract Infants’ Attention in Complex Displays</article-title>. <source>Infancy</source>, <volume>14</volume>(<issue>5</issue>), <fpage>550</fpage>–<lpage>562</lpage>. <pub-id pub-id-type="doi">10.1080/15250000903144199</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Gui</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mason</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Gliga</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hendry</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Begum Ali</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Pasco</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Shephard</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Curtis</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Charman</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>M. H.</given-names></string-name>, <string-name><surname>Meaburn</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>E. J. H.</given-names></string-name>, &amp; <collab>the BASIS-STAARS team</collab>. (<year>2020</year>). <article-title>Look duration at the face as a developmental endophenotype: Elucidating pathways to autism and ADHD</article-title>. <source>Development and Psychopathology</source>, <volume>32</volume>(<issue>4</issue>), <fpage>1303</fpage>–<lpage>1322</lpage>. <pub-id pub-id-type="doi">10.1017/S0954579420000930</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Hendry</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>E. J. H.</given-names></string-name>, &amp; <string-name><surname>Charman</surname>, <given-names>T</given-names></string-name>. (<year>2016</year>). <article-title>Executive function in the first three years of life: Precursors, predictors and patterns</article-title>. <source>Developmental Review</source>, <volume>42</volume>, <fpage>1</fpage>–<lpage>33</lpage>. <pub-id pub-id-type="doi">10.1016/j.dr.2016.06.005</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Hessels</surname>, <given-names>R. S.</given-names></string-name>, &amp; <string-name><surname>Hooge</surname>, <given-names>I. T. C</given-names></string-name>. (<year>2019</year>). <article-title>Eye tracking in developmental cognitive neuroscience – The good, the bad and the ugly</article-title>. <source>Developmental Cognitive Neuroscience</source>, <volume>40</volume>, <fpage>100710</fpage>. <pub-id pub-id-type="doi">10.1016/j.dcn.2019.100710</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Hofstadter</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Reznick</surname>, <given-names>J. S</given-names></string-name>. (<year>1996</year>). <article-title>Response Modality Affects Human Infant Delayed-Response Performance</article-title>. <source>Child Development</source>, <volume>67</volume>(<issue>2</issue>), <fpage>646</fpage>–<lpage>658</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-8624.1996.tb01757.x</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Hood</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Cole-Davies</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Dias</surname>, <given-names>M</given-names></string-name>. (<year>2003</year>). <article-title>Looking and search measures of object knowledge in preschool children</article-title>. <source>Developmental Psychology</source>, <volume>39</volume>(<issue>1</issue>), <fpage>61</fpage>–<lpage>70</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Hood</surname>, <given-names>B. M.</given-names></string-name>, &amp; <string-name><surname>Atkinson</surname>, <given-names>J</given-names></string-name>. (<year>1993</year>). <article-title>Disengaging visual attention in the infant and adult</article-title>. <source>Infant Behavior and Development</source>, <volume>16</volume>(<issue>4</issue>), <fpage>405</fpage>–<lpage>422</lpage>. <pub-id pub-id-type="doi">10.1016/0163-6383(93)80001-O</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Huber</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Potter</surname>, <given-names>K. W.</given-names></string-name>, &amp; <string-name><surname>Huszar</surname>, <given-names>L. D</given-names></string-name>. (<year>2019</year>). <article-title>Less “Story” and more “Reliability” in cognitive neuroscience</article-title>. <source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source>, <volume>113</volume>, <fpage>347</fpage>–<lpage>349</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2018.10.030</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Hutchinson</surname>, <given-names>J. B.</given-names></string-name>, &amp; <string-name><surname>Turk-Browne</surname>, <given-names>N. B</given-names></string-name>. (<year>2012</year>). <article-title>Memory-guided attention: Control from multiple memory systems</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>16</volume>(<issue>12</issue>), <fpage>576</fpage>–<lpage>579</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2012.10.003</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Jones</surname>, <given-names>E. J. H.</given-names></string-name>, <string-name><surname>Mason</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Begum Ali</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>van den Boomen</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Braukmann</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Cauvet</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Demurie</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Hessels</surname>, <given-names>R. S.</given-names></string-name>, <string-name><surname>Ward</surname>, <given-names>E. K.</given-names></string-name>, <string-name><surname>Hunnius</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bolte</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tomalski</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kemner</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Warreyn</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Roeyers</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Buitelaar</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Falck-Ytter</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Charman</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Johnson</surname>, <given-names>M. H.</given-names></string-name> (<year>2019</year>). <article-title>Eurosibs: Towards robust measurement of infant neurocognitive predictors of autism across Europe</article-title>. <source>Infant Behavior and Development</source>, <volume>57</volume>, <fpage>101316</fpage>. <pub-id pub-id-type="doi">10.1016/j.infbeh.2019.03.007</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Kidd</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Piantadosi</surname>, <given-names>S. T.</given-names></string-name>, &amp; <string-name><surname>Aslin</surname>, <given-names>R. N</given-names></string-name>. (<year>2012</year>). <article-title>The Goldilocks Effect: Human Infants Allocate Attention to Visual Sequences That Are Neither Too Simple Nor Too Complex</article-title>. <source>PLoS ONE</source>, <volume>7</volume>(<issue>5</issue>), <fpage>e36399</fpage>–<lpage>e36399</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Klin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Shultz</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Jones</surname>, <given-names>W</given-names></string-name>. (<year>2015</year>). <article-title>Social visual engagement in infants and toddlers with autism: Early developmental transitions and a model of pathogenesis</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, <volume>50</volume>, <fpage>189</fpage>–<lpage>203</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2014.10.006</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Knudsen</surname>, <given-names>E. I.</given-names></string-name>, <string-name><surname>Heckman</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Cameron</surname>, <given-names>J. L.</given-names></string-name>, &amp; <string-name><surname>Shonkoff</surname>, <given-names>J. P</given-names></string-name>. (<year>2006</year>). <article-title>Economic, neurobiological, and behavioral perspectives on building America’s future workforce</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>103</volume>(<issue>27</issue>), <fpage>10155</fpage>–<lpage>10162</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0600888103</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Leppänen</surname>, <given-names>J. M</given-names></string-name>. (<year>2016</year>). <article-title>Using Eye Tracking to Understand Infants’ Attentional Bias for Faces</article-title>. <source>Child Development Perspectives</source>, <volume>10</volume>(<issue>3</issue>), <fpage>161</fpage>–<lpage>165</lpage>. <pub-id pub-id-type="doi">10.1111/cdep.12180</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Miller</surname>, <given-names>E. K.</given-names></string-name>, &amp; <string-name><surname>Cohen</surname>, <given-names>J. D</given-names></string-name>. (<year>2001</year>). <article-title>An integrative theory of prefrontal cortex function</article-title>. <source>Annual Review of Neuroscience</source>, <volume>24</volume>, <fpage>167</fpage>–<lpage>202</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.167</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>Heslenfeld</surname>, <given-names>D. J.</given-names></string-name>, &amp; <string-name><surname>Theeuwes</surname>, <given-names>J</given-names></string-name>. (<year>2004</year>). <article-title>Endogenous and exogenous attention shifts are mediated by the same large-scale neural network</article-title>. <source>NeuroImage</source>, <volume>22</volume>(<issue>2</issue>), <fpage>822</fpage>–<lpage>830</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.01.044</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Portugal</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Bedford</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Cheung</surname>, <given-names>C. H. M.</given-names></string-name>, <string-name><surname>Mason</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>T. J</given-names></string-name>. (<year>2021</year>). <article-title>Longitudinal touchscreen use across early development is associated with faster exogenous and reduced endogenous attention control</article-title>. <source>Scientific Reports</source>, <volume>11</volume>(<fpage>1</fpage>), Article 1. <pub-id pub-id-type="doi">10.1038/s41598-021-81775-7</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Sarter</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Givens</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Bruno</surname>, <given-names>J. P</given-names></string-name>. (<year>2001</year>). <article-title>The cognitive neuroscience of sustained attention: Where top-down meets bottom-up</article-title>. <source>Brain Research Reviews</source>, <volume>35</volume>(<issue>2</issue>), <fpage>146</fpage>–<lpage>160</lpage>. <pub-id pub-id-type="doi">10.1016/S0165-0173(01)00044-3</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Shultz</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Klin</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Jones</surname>, <given-names>W</given-names></string-name>. (<year>2018</year>). <article-title>Neonatal Transitions in Social Behavior and Their Implications for Autism</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>22</volume>(<issue>5</issue>), <fpage>452</fpage>–<lpage>469</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2018.02.012</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>van Baar</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>de Jong</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Maat</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hooge</surname>, <given-names>I. T. C.</given-names></string-name>, <string-name><surname>Bogičević</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Verhoeven</surname>, <given-names>M.</given-names></string-name> (<year>2020</year>). <article-title>Reliability and Validity of the Utrecht Tasks for Attention in Toddlers Using Eye Tracking (UTATE)</article-title>. <source>Frontiers in Psychology</source>, <volume>11</volume>. <pub-id pub-id-type="doi">10.3389/fpsyg.2020.01179</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Wass</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Porayska-Pomsta</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Johnson</surname>, <given-names>M. H</given-names></string-name>. (<year>2011</year>). <article-title>Training Attentional Control in Infancy</article-title>. <source>Current Biology</source>, <volume>21</volume>(<issue>18</issue>), <fpage>1543</fpage>–<lpage>1547</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2011.08.004</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87566.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Spering</surname>
<given-names>Miriam</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>The University of British Columbia</institution>
</institution-wrap>
<city>Vancouver</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study, based on a large-scale data set and established behavioural tasks, has the potential to provide a <bold>valuable</bold> contribution to the literature if the authors could combine and correlate their behavioural evaluations with neural data and/or clinical assessments. As a standalone dataset, however, the current study appears <bold>incomplete</bold> because it does not go beyond merely replicating existing findings in a large cohort of children. In order to elevate their study, the authors are encouraged to publish their full dataset and explore the relationship between behavioural and neural or clinical data.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87566.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors provide a large-scale study of 18-month-olds, tested on a battery of tests (7 tasks designed to study attention, 1 to study working memory). Most of these tasks are already well-established, and the authors provide an additional replication.</p>
<p>
They further show that the variability in toddler's behavior (in terms of accuracy and reaction time) can be best and most parsimoniously accounted by two variable, one would correspond to attention, and the second one to social attention (i.e., the well-known interest for faces across the lifespan). Additionally, the authors find no evidence for a distinction between endogenous and exogenous attention. One may however argue that it is unclear whether any of the tasks actually tap endogenous attention. More detailed discussion of the cognitive functions involved in each of the tasks would enrich the paper.</p>
<p>Arguably, the working memory task is the task that is most likely to involve endogenous attention, as the behavior being tested is a spontaneous interest for a hidden object, hence attention triggered by an internal mental representation. Unfortunately, this task yielded null results and did not replicate previous findings.</p>
<p>Altogether, these findings provide an interesting method. Its value will be assessed when these behavioral evaluations will be combined with neural data and clinical assessment, as teased by the authors towards the end of the paper.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87566.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The manuscript describes a large scale study of 8 eye tracking tasks in a large cohort of 18 month old children. The dataset is impressive and allows a comparison across children in different tasks that assess social, endogenous, and exogenous attention tasks. As such, it provides a benchmark for future studies that examine eye movements within different cohorts of children and across development and offers exciting possibilities to correlate these measures with behavior, other measures of motor and neural development, and to compare these measures with children diagnosed with neurodevelopmental disorders.</p>
<p>It does seem like additional insights can be gained from the study that could potentially address important topics in development, attention, and eye movements. Which components of attention are similar and in what way? The distinction between social vs non social is interesting but not ground breaking (e.g., the preference of toddlers to attend to faces); maybe looking at specific sub-tasks and clusters of participants the study can reveal new insights about the differences and similarities across tasks. The manuscript describes the importance of characterizing profiles of attention and individual differences, what kind of profiles are found in the study? Are there different profiles among this large cohort?</p>
<p>
Moreover, to allow comparison across analysis methods, ages, and neurodevelopmental disorders, it is important that the full dataset will be available online (i.e., all eye tracking data not just the metrics) as well as the software to run tasks that should also be made available to encourage using the battery across different research communities.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87566.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Braithwaite et al. present data from a comprehensive large-scale study of 18-month-old's visual attention. The authors leverage a battery of well-known visual attention tasks to replicate canonical effects found in the literature and assess the latent structure of these tasks. They find that, while controlling for eye tracking precision and accuracy, two factors best fit the data - attention to social and non-social stimuli.</p>
<p>Strengths:</p>
<p>
The current study represents what amounts to years of hard work collecting data from a population that is challenging to work with - young children. The authors have diligently attended to data cleaning and sample size throughout the manuscript. Not only do they provide a large-scale replication of several well-known tasks, but they use advanced statistical modeling to discover the structure of visual attention in these 18-month-olds. Overall, this is a valuable contribution to the literature and provides a useful framework for studying visual attention development.</p>
<p>Weaknesses:</p>
<p>
While the study is clearly a valuable addition to the extant literature, I have several concerns that might be addressed to improve the manuscript. These primarily center around clarity and conciseness. First, the introduction seems to lack clarity at times. For example, the first paragraph seems to introduce several ideas (e.g., brain and cognitive development, direct and indirect measures of cognition, eyetracking, etc) that make it hard to understand where the paper is going. The authors might consider homing in on 2 main points to motivate eye tracking as a tool. Second, there are many different eye tracking measures may make it difficult for the reader to track which measures were used for each task and which were relevant for the larger model. This may be remedied by adding a section to the methods that briefly describes how each measure was calculated and perhaps a table that lists each task, the measure, and how it was calculated. Third, the results are exciting but hard to visualize in the supplementary figures. I commend them on using raincloud plots to visualize the individual data, but I would strongly encourage the authors to rethink how they display the data. For example, I find the supplementary images hard to see and as a result the effects reported are hard to discern in the image. Fourth, I believe the current data warrant a deeper discussion of what these findings mean. For example, given the developmental nature of the current study, it would be valuable for the authors to discuss how the structure visual attention might change or stay the same across development. For example, do the authors believe the current two factor model would replicate in older children, or would exogenous and endogenous attention emerge as separable components? How do these predictions relate to the extensive research in the adult literature?</p>
</body>
</sub-article>
</article>