<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">105081</article-id>
<article-id pub-id-type="doi">10.7554/eLife.105081</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.105081.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Movie reconstruction from mouse visual cortex activity</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5858-166X</contrib-id>
<name>
<surname>Bauer</surname>
<given-names>Joel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>joel.bauer@ucl.ac.uk</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5526-4578</contrib-id>
<name>
<surname>Margrie</surname>
<given-names>Troy W</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4507-8648</contrib-id>
<name>
<surname>Clopath</surname>
<given-names>Claudia</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Sainsbury Wellcome Centre, University College London</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041kmwe10</institution-id><institution>Bioengineering Dept., Imperial College</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Denison</surname>
<given-names>Rachel</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/05qwgg493</institution-id><institution>Boston University</institution>
</institution-wrap>
<city>Boston</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>*</label><p>These authors contributed equally as last authors</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-03-25">
<day>25</day>
<month>03</month>
<year>2025</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-12-18">
<day>18</day>
<month>12</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP105081</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-12-02">
<day>02</day>
<month>12</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-11-11">
<day>11</day>
<month>11</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.06.19.599691"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2025-03-25">
<day>25</day>
<month>03</month>
<year>2025</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.105081.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.105081.1.sa4">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.105081.1.sa3">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.105081.1.sa2">Reviewer #2 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.105081.1.sa1">Reviewer #3 (Public review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.105081.1.sa0">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Bauer et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Bauer et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-105081-v2.pdf"/>
<abstract>
<p>The ability to reconstruct images represented by the brain has the potential to give us an intuitive understanding of what the brain sees. Reconstruction of visual input from human fMRI data has garnered significant attention in recent years. Comparatively less focus has been directed towards vision reconstruction from single-cell recordings, despite its potential to provide a more direct measure of the information represented by the brain. Here, we achieve high-quality reconstructions of natural movies presented to mice, from the activity of neurons in their visual cortex for the first time. Using our method of video optimization via backpropagation through a state-of-the-art dynamic neural encoding model we reliably reconstruct 10-second movies at 30 Hz from two-photon calcium imaging data. We achieve a pixel-level correction of 0.57 between the ground truth movie and the reconstructions from single-trial neural responses. We find that critical for high-quality reconstructions are the number of neurons in the dataset and the use of model ensembling. This paves the way for movie reconstruction to be used as a tool to investigate a variety of visual processing phenomena.</p>
</abstract>
<funding-group>
<award-group id="par-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id>
<institution>Wellcome Trust (WT)</institution>
</institution-wrap>
</funding-source>
<award-id award-id-type="doi">10.35802/219627</award-id>
<principal-award-recipient>
<name>
<surname>Margrie</surname>
<given-names>Troy W</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-2">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id>
<institution>Wellcome Trust (WT)</institution>
</institution-wrap>
</funding-source>
<award-id award-id-type="doi">10.35802/306384</award-id>
<principal-award-recipient>
<name>
<surname>Margrie</surname>
<given-names>Troy W</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-3">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000324</institution-id>
<institution>Gatsby Charitable Foundation (GATSBY)</institution>
</institution-wrap>
</funding-source>
<award-id>GAT3755</award-id>
<principal-award-recipient>
<name>
<surname>Margrie</surname>
<given-names>Troy W</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-4">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004410</institution-id>
<institution>European Molecular Biology Organization (EMBO)</institution>
</institution-wrap>
</funding-source>
<award-id>ALTF 415-2024</award-id>
<principal-award-recipient>
<name>
<surname>Bauer</surname>
<given-names>Joel</given-names>
</name>
</principal-award-recipient>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>We have made changes to the manuscript to address comments by reviewers at eLife.
(1) We introduce pixel error maps in Figure 6.
(2) We address how image masking improves reconstruction performance and add receptive field mapping in Supplementary Figures 2 &amp; 3.
(3) We add analysis in Supplementary Figures 3 &amp; 4 to examine how other factors, such as Gaussian smoothing, contrast/luminance adjustment, gradient ensembling, and encoder activity prediction performance affect reconstruction performance.
(4) In Figure 4 and Supplementary Figure 5, we elaborate on the limits of model performance using Gaussian noise reconstruction to test what happens beyond the reconstruction limits when stimuli are phase inverted.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>One fundamental aim of neuroscience is to eventually gain insight into the ongoing perceptual experience of humans and animals. Reconstruction of visual perception directly from brain activity has the potential to give us a deeper understanding of how the brain represents visual information. Over the past decade, there have been considerable advances in reconstructing images and videos from human brain activity [<xref ref-type="bibr" rid="c31">Nishimoto et al., 2011</xref>, <xref ref-type="bibr" rid="c47">Shen et al., 2019a</xref>,<xref ref-type="bibr" rid="c48">b</xref>, <xref ref-type="bibr" rid="c38">Rakhimberdina et al., 2021</xref>, <xref ref-type="bibr" rid="c41">Ren et al., 2021</xref>, <xref ref-type="bibr" rid="c52">Takagi and Nishimoto, 2023</xref>, <xref ref-type="bibr" rid="c33">Ozcelik and VanRullen, 2023</xref>, <xref ref-type="bibr" rid="c18">Ho et al., 2023</xref>, <xref ref-type="bibr" rid="c46">Scotti et al., 2023</xref>, <xref ref-type="bibr" rid="c7">Chen et al., 2023</xref>, <xref ref-type="bibr" rid="c2">Benchetrit et al., 2023</xref>, <xref ref-type="bibr" rid="c26">Kupershmidt et al., 2022</xref>]. These advances have largely leveraged deep learning techniques to interpret fMRI or MEG recordings, taking advantage of the fact that spatially separated clusters of neurons have distinct visual and semantic response properties [<xref ref-type="bibr" rid="c38">Rakhimberdina et al., 2021</xref>]. Due to the low resolution of fMRI and MEG, relative to single neurons, the most successful models heavily rely on extracting semantic content and use diffusion models to generate semantically similar images and videos. Some approaches combine low-level perceptual (retinotopic) and semantic information in separate modules to achieve even better image similarity [<xref ref-type="bibr" rid="c41">Ren et al., 2021</xref>, <xref ref-type="bibr" rid="c33">Ozcelik and VanRullen, 2023</xref>, <xref ref-type="bibr" rid="c46">Scotti et al., 2023</xref>]. However, the pixel-level similarities are still relatively low. These methods are highly useful in humans, but their focus on semantic content may make them less useful when applied to non-human subjects or when using the reconstructed images to investigate visual processing.</p>
<p>Less attention has been given to image reconstruction from non-human brains. This is surprising given the advantages of large-scale single-cell-resolution recording techniques available in animal models, particularly mice. In the past, reconstructions using linear summation of receptive fields or Gabor filters have shown some success using responses from retinal ganglion cells [<xref ref-type="bibr" rid="c3">Brackbill et al., 2020</xref>], thalamo-cortical neurons in lateral geniculate nucleus [<xref ref-type="bibr" rid="c50">Stanley et al., 1999</xref>], and primary visual cortex [<xref ref-type="bibr" rid="c15">Garasto et al., 2019</xref>, <xref ref-type="bibr" rid="c62">Yoshida and Ohki, 2020</xref>]. Recently, deep nonlinear neural networks have been used with promising results to reconstruct static images from mouse retina [<xref ref-type="bibr" rid="c63">Zhang et al., 2020</xref>, <xref ref-type="bibr" rid="c27">Li et al., 2023</xref>] and visual cortex [<xref ref-type="bibr" rid="c9">Cobos et al., 2022</xref>](bioRxiv), and in particular from monkey V4 extracellular recordings [<xref ref-type="bibr" rid="c27">Li et al., 2023</xref>, <xref ref-type="bibr" rid="c35">Pierzchlewicz et al., 2023</xref>].</p>
<p>Here, we present a method for the reconstruction of 10-second movie clips using two-photon calcium imaging data recorded in mouse V1 [<xref ref-type="bibr" rid="c54">Turishcheva et al., 2023</xref>, <xref ref-type="bibr" rid="c55">2024</xref>]. Our method takes advantage of a state-of-the-art (SOTA) dynamic neural encoding model (DNEM) [<xref ref-type="bibr" rid="c1">Baikulov, 2023</xref>] which predicts neuronal activity based on video input as well as behavior. Our method allows us to successfully reconstruct videos despite the fact that V1 neuronal activity in awake mice is heavily modulated by behavioral factors such as running speed [<xref ref-type="bibr" rid="c30">Niell and Stryker, 2010</xref>] and pupil diameter (correlated with arousal; [<xref ref-type="bibr" rid="c40">Reimer et al., 2014</xref>]). We then quantify the spatio-temporal limits of this reconstruction approach and identify key aspects of our method necessary for optimal performance.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Video reconstruction using state-of-the-art dynamic neural encoding models</title>
<p>We used publicly available data provided by the Sensorium 2023 competition [<xref ref-type="bibr" rid="c54">Turishcheva et al., 2023</xref>, <xref ref-type="bibr" rid="c55">2024</xref>]. The data included movies that were presented to mice and the evoked activity of V1 neurons along with pupil position, pupil diameter, and running speed. The neuronal activity was measured using two-photon imaging of GCaMP6s [<xref ref-type="bibr" rid="c5">Chen et al., 2013</xref>] fluorescence from 10 mice, with ≈8000 neurons from each mouse. In total, we reconstructed ten 10s natural movies from 5 mice.</p>
<p>We used the winning model of the Sensorium 2023 competition which achieved a score of 0.301 ([<xref ref-type="bibr" rid="c1">Baikulov, 2023</xref>, <xref ref-type="bibr" rid="c55">Turishcheva et al., 2024</xref>] single-trial correlation between predicted and ground truth neuronal activity; <xref rid="fig1" ref-type="fig">Figure 1A</xref> and <xref rid="figS1" ref-type="fig">Figure S1A-C</xref>). This state-of-the-art (SOTA) dynamic neural encoding model (DNEM) was composed of three parts: core, cortex and readout. The model takes the video as input with the behavioral data (pupil position, pupil diameter, and running speed) broadcast to four additional channels of the video. The original model weights were not used to avoid reconstructing movies the model was trained on. Instead, we retrained 7 instances of the model using the same training data, which did not include the movies reserved for reconstruction. Beyond this point the weights of the model were frozen, i.e. not influenced by future movie presentations.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Video reconstruction from neuronal activity in mouse V1 (data provided by the Sensorium 2023 competition; [<xref ref-type="bibr" rid="c54">Turishcheva et al., 2023</xref>, <xref ref-type="bibr" rid="c55">2024</xref>]) using a state-of-the-art (SOTA) dynamic neural encoding model (DNEM; [<xref ref-type="bibr" rid="c1">Baikulov, 2023</xref>]).</title>
<p>A) DNEMs predict neuronal activity from mouse primary visual cortex, given a video and behavioural input. B) We use a SOTA DNEM to reconstruct part of the input video given neuronal population activity, using gradient descent to optimize the input. C) Poisson negative log likelihood loss across training steps between ground truth neuronal activity and predicted neuronal activity in response to videos. Left: all 50 videos from 5 mice for one model. Right: average loss across all videos for 7 model instances. D) Spatio-temporal (pixel-by-pixel) correlation between reconstructed video and ground truth video.</p></caption>
<graphic xlink:href="599691v4_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To reconstruct the videos presented to mice we iteratively optimized an initially blank input video to the SOTA DNEM until the predicted activity in response to this input matched the ground truth recorded neuronal activity. In effect, we optimized the input video to be perceptually similar with respect to the recorded neurons. To achieve this we used an input optimization through gradient descent approach inspired by the optimization of maximally exciting images [<xref ref-type="bibr" rid="c57">Walker et al., 2019</xref>] and the reconstruction of static images [<xref ref-type="bibr" rid="c9">Cobos et al., 2022</xref>](bioRxiv) [<xref ref-type="bibr" rid="c35">Pierzchlewicz et al., 2023</xref>]. The input videos were initialized as uniform gray values and the behavioral parameters (<xref rid="figS1" ref-type="fig">Figure S1A</xref>) were added as additional channels, i.e. these were not reconstructed but given. The neuronal activity in response to the input video was predicted using the SOTA DNEM for a sliding window of 32 frames (1.067 sec) with a stride of 8 frames. We saw slightly better results with a stride of 2 frames but, in our case, this did not warrant the increase in training time. For each window, the difference between the predicted and ground truth responses was calculated and this loss backpropagated to the pixels of the input video to get the gradient of the loss with respect to each pixel. In effect, the input pixels were thus treated as if they were model weights. The gradients for each pixel were then averaged across all windows and the pixels of the input video updated accordingly (See Supplementary Algorithm 1).</p>
<p>The data from the Sensorium competition provided the activity of neurons within a 630 by 630 <italic>µ</italic>m field of view for each mouse, i.e. covering roughly one-fifth of mouse V1. Due to the retinotopic organization of V1 we therefore did not expect to get good reconstructions of the entire video frame. However, gradients still propagated to the full video frame and produced non-sensical results along the periphery of the video frames (<xref rid="figS3" ref-type="fig">Figure S3</xref>). Inspired by previous work [<xref ref-type="bibr" rid="c29">Mordvintsev et al., 2018</xref>] [<xref ref-type="bibr" rid="c60">Willeke et al., 2023b</xref>](bioRxiv) we therefore decided to apply a mask during training and evaluation. To generate these masks, we optimized a transparency layer placed at the input to the SOTA DNEM. High values are given to pixels that contribute to the accurate prediction of neuronal activity and represent the collective receptive field of the neural population. None of the reconstructed movies were used in the optimization of this transparency mask. The transparency masks are aligned with with but not identical to the On-Off receptive field distribution maps using sparse-noise (<xref rid="figS2" ref-type="fig">Figure S2</xref>). This mask was applied during the optimization of the reconstructed movies (training mask: binarized with threshold <italic>α</italic> = 0.5) and applied again to the final reconstruction (evaluation mask: binarized with threshold <italic>α</italic> = 1) (See Supplementary Algorithm 2). Applying the mask in two stages first boosts the performance of reconstruction itself and separately allows evaluation of the reconstruction in a region of high confidence, given the neural population available (<xref rid="figS3" ref-type="fig">Figure S3</xref>).</p>
<p>As the loss between predicted (<xref rid="figS1" ref-type="fig">Figure S1D</xref>) and ground truth responses (<xref rid="figS1" ref-type="fig">Figure S1B</xref>) decreased, the similarity between the reconstructed and ground truth input video increased (<xref rid="fig1" ref-type="fig">Figure 1C-D</xref>). We generated 7 separate reconstructions from 7 neural encoding models (trained on the same data) and averaged them. Finally, we applied a 3D Gaussian filter with sigma 0.5 pixels to remove the remaining static noise (<xref rid="figS3" ref-type="fig">Figure S3</xref>) and applied the evaluation mask. When presenting videos in this paper we normalize the mean and standard deviation of the reconstructions to the average and standard deviation of the corresponding ground truth movie before applying the evaluation masks, but this is not done for quantification except in <xref rid="figS3" ref-type="fig">Figure S3D</xref>. The Gaussian filter was not applied when evaluating spatial or temporal resolution (<xref rid="fig4" ref-type="fig">Figure 4</xref>, <xref rid="figS6" ref-type="fig">Figure S6</xref>, <xref rid="figS5" ref-type="fig">Figure S5</xref>).</p>
<sec id="s2a">
<label>2.1</label>
<title>High-quality video reconstruction</title>
<p>As can be seen in <xref rid="fig2" ref-type="fig">Figure 2</xref> and <xref ref-type="supplementary-material" rid="video1">Supplementary Video 1</xref>, the reconstructed videos capture much of the spatial and temporal dynamics of the original input video. Because our optimization of the movies was based on a perceptual loss function, we were interested in how closely these movies matched the originals on the pixel level. To evaluate performance of the video reconstructions we therefore correlated either all pixels from all time points between ground truth and reconstructed videos (Pearson’s correlation r = 0.569; to quantify temporal and spatial similarity), or the average correlation between all sets of frames (Pearson’s correlation r = 0.512; to quantify just spatial similarity)(<xref rid="fig2" ref-type="fig">Figure 2B</xref> and <xref rid="figS1" ref-type="fig">Figure S1E</xref>). This represents a ≈ 2x higher pixel-level correlation over previous single-trial static image reconstructions from V1 in awake mice (image correlation 0.238 +/− 0.054 s.e.m for awake mice) [<xref ref-type="bibr" rid="c62">Yoshida and Ohki, 2020</xref>] over a similar retinotopic area (≈ 43° × 43°) while also capturing temporal dynamics. However, we would like to stress that directly comparing static image reconstruction methods with movie reconstruction approaches is fundamentally problematic, as they rely on different data types both during training and evaluation (temporally averaged vs continuous neural activity, images flashed at fixed intervals vs continuous movies).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Reconstruction performance.</title>
<p>A) Three reconstructions of 10s videos from different mice (see <xref ref-type="supplementary-material" rid="video1">Supplementary Video 1</xref>). Reconstructions have been luminance (mean pixel value across video) and contrast (standard deviation of pixel values across video) matched to ground truth. B) The reconstructed videos have high correlation to ground truth in both spatiotemporal correlation (mean Pearson’s correlation r = 0.569 with 95% CIs 0.542 to 0.596, t-test between ground truth and random video p = 6.69 * 10<sup><italic>−</italic>49</sup>, n = 50 videos from 5 mice) and mean frame correlation (mean Pearson’s correlation r = 0.512 with 95% CIs 0.481 to 0.543, t-test between ground truth and random video p = 4.29 * 10<sup><italic>−</italic>45</sup>, n = 50 videos from 5 mice).</p></caption>
<graphic xlink:href="599691v4_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Reconstruction quality, however, was not consistent across movies (<xref rid="fig2" ref-type="fig">Figure 2B</xref>) or constant throughout the 10 second videos (<xref rid="figS1" ref-type="fig">Figure S1E</xref>). We therefore investigated what factors may cause these fluctuations by correlating video motion energy, contrast and luminance, as well as running speed, pupil diameter and eye movement with frame correlation. We found that contrast correlated with frame correlation, but only to a moderate degree. Video motion energy shows a trend but was not significant (<xref rid="figS4" ref-type="fig">Figure S4</xref>). We also found that the ability of the SOTA DNEM to predict neural activity correlated with reconstruction performance. This could be because some frames are harder to reconstruct due to their content (high temporal and spatial frequencies) or because neural activity in these moments is influenced by factors the model cannot take into account.</p>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Ensembling</title>
<p>We found that the 7 instances of the SOTA DNEMs by themselves performed similarly in terms of reconstructed video correlation (<xref rid="fig1" ref-type="fig">Figure 1D</xref>), but that this correlation was significantly increased by taking the average across reconstructions from different models (<xref rid="fig3" ref-type="fig">Figure 3</xref>) – A technique known as bagging, and more generally ensembling [<xref ref-type="bibr" rid="c4">Breiman, 1996</xref>]. We averaged over 7 model instances, which gave a performance increase of 28.0%, but the largest gain in performance, 13.7%, came from averaging across just 2 models (<xref rid="fig3" ref-type="fig">Figure 3</xref>). Doubling the number of models to 4 increased the performance by another 8.32%. Individual models produced reconstructions with high-frequency noise in the temporal and spatial domains. We therefore think the increase in performance from ensembling is mostly an effect of averaging out this high-frequency noise. On-the-other-hand, it is possible that averaging over separately optimized reconstructions degrades high-frequency information. We therefore tested whether averaging pixel gradients from all models at each iteration rather than averaging the final movies yields higher performance, but we observed no improvement (<xref rid="figS3" ref-type="fig">Figure S3C</xref>). Overall, although ensembling over models trained on separate data splits is a computationally expensive method, it substantially improved reconstruction quality.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Model ensembling.</title>
<p>Mean video correlation is improved when predictions from multiple models are averaged. Dashed lines are individual animals, solid line is mean. One-way repeated measures ANOVA p = 1.11 * 10<sup><italic>−</italic>16</sup>. Bonferroni corrected paired t-test outcomes between consecutive ensemble sizes are all p&lt; 0.001, n = 5 mice.</p></caption>
<graphic xlink:href="599691v4_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Reconstruction of Gaussian noise across the spatial and temporal spectrum using predicted activity.</title>
<p>A) Example Gaussian noise stimulus set with evaluation mask for one mouse. Shown is the last frame of a 2 second video. B) Reconstructed Gaussian stimuli with SOTA DNEM predicted neuronal activity as the target (see also <xref ref-type="supplementary-material" rid="video2">Supplementary Video 2</xref>). C) Pearson’s correlation between ground truth (A) and reconstructed videos (B) across the range of spatial and temporal length constants. For each stimulus type the average correlation across 5 movies reconstructed from the SOTA DNEM of 3 mice is given. D) Pearson’s correlation between reconstructions from phase-inverted Gaussian noise stimuli.</p></caption>
<graphic xlink:href="599691v4_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Not all spatial and temporal frequencies are reconstructed equally</title>
<p>While the reconstructed videos achieve high correlation to ground truth, it is not entirely clear if the remaining deviations are due to the limitations of the model or arise from the recorded neurons themselves. To assess the resolution limits of our reconstruction process, we assessed the model’s ability to reconstruct synthetic stimuli at varying spatial and temporal resolutions in a noise-free scenario.</p>
<p>To quantify which spatial and temporal frequencies our reconstruction approach is able to capture we used a Gaussian noise stimulus set generated using a Gaussian process (<ext-link ext-link-type="uri" xlink:href="https://github.com/TomGeorge1234/gp_video">https://github.com/TomGeorge1234/gp_video</ext-link>; <xref rid="fig4" ref-type="fig">Figure 4A</xref>). The dataset consisted of 49, 2 second, 36 by 36 pixel videos at 30 Hz, which varied in the spatial and temporal length constants. As we did not have ground truth neuronal activity in response to this stimulus set, we first predicted the neuronal responses given these videos using the ensembled SOTA DNEMs. We then used gradient descent to reconstruct the original input using these predicted neuronal responses as the target. In this way, we generated reconstructions in an ideal case with no biological noise and assuming the SOTA DNEM perfectly predicts neuronal activity (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). This means the video reconstruction quality loss reflects the inefficiency of the reconstruction process itself without the additional loss or transformation of information by processes such as top-down modulation, e.g. predictive coding or selective feature attention (see Discussion). We found that the reconstruction process failed at high spatial frequencies (&lt; 1 pixel, or &lt; 3.4° retinotopy) and performed worse at high temporal frequencies (&lt; 1 frame, or &gt; 30 Hz)(<xref rid="fig4" ref-type="fig">Figure 4C</xref> and <xref ref-type="supplementary-material" rid="video2">Supplementary Video 2</xref>). We repeated this analysis using full-field high-contrast square gratings drifting in the four cardinal directions and similarly found that high-spatial and temporal frequencies were not reconstructed as well as low-spatial and temporal frequency gratings (<xref rid="figS6" ref-type="fig">Figure S6</xref>). We also found that beyond the spatial reconstruction limit, the reconstructions from phase-inverted Gaussian noise stimuli had higher correlation with each other than with their ground truth stimuli (<xref rid="fig4" ref-type="fig">Figure 4D</xref>). Never-the-less, even when the reconstructions were not captured on the pixel level, they did capture some of the spatial entropy and motion energy of the ground truth stimuli (<xref rid="figS5" ref-type="fig">Figure S5</xref>).</p>
<p>To test if model ensembling improves Gaussian noise reconstruction quality across all spatial and temporal length constants uniformly, we subtracted the average video correlation across the seven model instances from the video correlation of the average video (i.e. ensembled video reconstruction minus unensembled video reconstruction; <xref rid="figS5" ref-type="fig">Figure S5C</xref>). We found that, in particular, short temporal and spatial length constant stimuli improved in correlation, supporting our hypothesis that ensembling mitigates the high-frequency noise we observed in the reconstruction from individual models.</p>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Neuronal population size</title>
<p>In order to design future <italic>in vivo</italic> experiments to investigate visual processing using our video reconstruction approach, it would be useful to know how reconstruction performance scales with the number of recorded neurons. This is vital for prioritizing experimental parameters such as weighing between sampling density within a similar retinotopic area and retinotopic coverage to maximize both video reconstruction quality and visual coverage. We therefore performed an <italic>in silico</italic> ablation experiment, dropping either 50, 75% or 87.5% of the total recorded population of ≈8000 neurons per mouse by setting their activity to 0 (<xref rid="fig5" ref-type="fig">Figure 5</xref>). We found that dropping 50% of the neurons reduced the video correlation by only 9.96% while dropping 75% reduced the performance by 24.9%. We would therefore argue that ≈4000-8000 neurons within a 630 by 630 <italic>µ</italic>m area (≈10000-20000 neurons/mm<sup>2</sup>) of mouse V1 would provide a balance when compromising between density and 2D coverage.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Video reconstruction using fewer neurons (i.e. population ablation) leads to lower reconstruction quality.</title>
<p>Dashed lines are individual animals, solid line is mean. One-way repeated measures ANOVA p = 5.70 * 10<sup><italic>−</italic>13</sup>. Bonferroni corrected paired t-test outcomes between consecutive drops in population size are all p &lt; 0.001, n = 5 mice.</p></caption>
<graphic xlink:href="599691v4_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2e">
<label>2.5</label>
<title>Visualization of Reconstruction Error</title>
<p>One advantage of stimulus reconstruction compared to stimulus identity decoding (i.e. classification) is that it is possible to visualize the deviation of the reconstructed stimuli from what is expected. This is interesting because reconstruction performance is not stable over time but fluctuates (<xref rid="fig6" ref-type="fig">Figure 6A</xref>), likely due to the fact that the DNEM does not have access to all possible factors that influence neural activity. When using our reconstruction method it is not the input stimulus similarity that is optimized, but the evoked activity of the stimulus. As a consequence, the predicted neural response from the reconstructed movie is more similar to the experimental neural response compared to the predicted neural response evoked by the original ground truth movie (<xref rid="fig6" ref-type="fig">Figure 6B</xref>). It is possible to visualize this deviation on a pixel level by subtracting the experimentally derived movie reconstruction (i.e., based on measured neural responses) from the <italic>in silio</italic> simulation derived movie reconstruction (i.e., first predict activity based on the ground truth video and then reconstruct the movie based on the resulting simulated neural activity)(<xref rid="fig6" ref-type="fig">Figure 6B-C</xref>). With the current dataset it is not possible to test if these deviations reflect failures of the encoding model to predict neural activity given the sensory stimulus or true deviations of the images represented by the neural population from the sensory stimulus, but this approach may be an interesting method for investigating when and why model predictions of neural activity deviate from the experimentally measured activity.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Comparison of reconstructions from experimental responses vs expected responses and their visualization as error maps.</title>
<p>A) Frame-by-frame correlation between reconstructed and ground truth video for mouse 1 trial 7 (same as <xref ref-type="fig" rid="figS1">Figure S1</xref>). B) From left to right: experimental (ground truth) neural activity <italic>y</italic>, neural activity predicted by DNEM from ground truth video <inline-formula id="inline-eqn-1"><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="599691v4_inline7.gif"/></inline-formula>, neural activity predicted by DNEM based on reconstructed movie <inline-formula id="inline-eqn-2"><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="599691v4_inline8.gif"/></inline-formula>. C) Difference between correlation of true neural response <italic>y</italic> with predicted neural response from the ground truth movie <inline-formula id="inline-eqn-3"><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="599691v4_inline9.gif"/></inline-formula>, and correlation of true neural response <italic>y</italic> with predicted neural response from the reconstructed movie <inline-formula id="inline-eqn-4"><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="599691v4_inline10.gif"/></inline-formula>. D) 9 frames from mouse 1 trial 7. From top to bottom: reconstructed movie <inline-formula id="inline-eqn-5"><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="599691v4_inline11.gif"/></inline-formula>, reconstructed movie from predicted neural response to ground truth movie <inline-formula id="inline-eqn-6"><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="599691v4_inline12.gif"/></inline-formula>, ground truth movie <italic>x</italic> with overlayed heatmap of the difference between <inline-formula id="inline-eqn-7"><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="599691v4_inline13.gif"/></inline-formula> and <inline-formula id="inline-eqn-8"><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="599691v4_inline14.gif"/></inline-formula> (error map). E) Error map of one frame from all 50 movie clips. Each row is 10 trials from one mouse. Aee also <xref ref-type="supplementary-material" rid="video4">Supplementary Video 4</xref>)</p></caption>
<graphic xlink:href="599691v4_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<sec id="s3a">
<label>3.1</label>
<title>Stimulus identification vs reconstruction</title>
<p>Stimulus identification, i.e. identifying the most likely stimulus from a constrained set, has been a popular approach for quantifying whether a population of neurons encodes the identity of a particular stimulus [<xref ref-type="bibr" rid="c14">Földiák, 1993</xref>, <xref ref-type="bibr" rid="c23">Kay et al., 2008</xref>]. This approach has, for instance, been used to decode frame identity within a movie [<xref ref-type="bibr" rid="c10">Deitch et al., 2021</xref>, <xref ref-type="bibr" rid="c61">Xia et al., 2021</xref>, <xref ref-type="bibr" rid="c44">Schneider et al., 2023</xref>, <xref ref-type="bibr" rid="c6">Chen et al., 2024</xref>]. Some of these approaches have also been used to reorder the frames of the ground truth movie [<xref ref-type="bibr" rid="c44">Schneider et al., 2023</xref>] based on the decoded frame identity. Importantly, stimulus identification methods are distinct from stimulus reconstruction where the aim is to recreate what the sensory content of a neuronal code is in a way that generalizes to new sensory stimuli [<xref ref-type="bibr" rid="c38">Rakhimberdina et al., 2021</xref>]. This is inherently a more demanding task because the range of possible solutions is much larger. Although stimulus identification is a valuable tool for understanding the information content of a population code, stimulus reconstruction could provide a more generalizable approach, because it can be applied to novel stimuli.</p>
</sec>
<sec id="s3b">
<label>3.2</label>
<title>Comparison to other reconstruction methods</title>
<p>There has recently been a growing number of publications in the field of image reconstruction, primarily from fMRI data, and a comprehensive review of all the approaches is outside the scope of this paper. However, we will briefly summarize the most common approaches and how they relate to our own method. In general, image reconstruction methods can be categorized into one of four groups: direct decoding models, encoder-decoder models, invertible encoding models, and encoder model input optimization.</p>
<p>Direct decoders, directly decode the input image/videos from neuronal activity with deep neuronal networks [<xref ref-type="bibr" rid="c47">Shen et al., 2019a</xref>, <xref ref-type="bibr" rid="c63">Zhang et al., 2020</xref>, <xref ref-type="bibr" rid="c27">Li et al., 2023</xref>]. When training direct decoders, the decoders can be pretrained [<xref ref-type="bibr" rid="c41">Ren et al., 2021</xref>] or additional constraints can be added to the loss function to encourage the decoder to produce images that adhere to learned image statistics [<xref ref-type="bibr" rid="c47">Shen et al., 2019a</xref>, <xref ref-type="bibr" rid="c26">Kupershmidt et al., 2022</xref>]. A direct decoder approach has been used for video reconstruction in mice [<xref ref-type="bibr" rid="c6">Chen et al., 2024</xref>], but in that case, the training and test movies were the same, meaning it is unclear if out-of-training set generalization was achieved (a key distinction between sensory reconstruction and stimulus identification, see previous section).</p>
<p>In encoder-decoder models the aim is to combine separately trained brain encoders (brain activity to latent space) and decoders (latent space to image/video). Recently, this approach has become particularly popular because it allows the use of SOTA generative image models such as stable diffusion [<xref ref-type="bibr" rid="c42">Rombach et al., 2021</xref>, <xref ref-type="bibr" rid="c52">Takagi and Nishimoto, 2023</xref>, <xref ref-type="bibr" rid="c46">Scotti et al., 2023</xref>, <xref ref-type="bibr" rid="c7">Chen et al., 2023</xref>, <xref ref-type="bibr" rid="c2">Benchetrit et al., 2023</xref>]. The encoder part of the models are first trained to translate brain activity into a latent space that the pretrained generative networks can interpret. Because these latent spaces are often conditioned on semantic information, this lends itself to separate processing of low-level visual and high-level semantic information from brain activity [<xref ref-type="bibr" rid="c46">Scotti et al., 2023</xref>].</p>
<p>Invertible encoding models are encoding models which, once trained to predict neuronal activity, can implicitly be inverted to predict sensory input given brain activity. We would also include those models in this class which first compute the receptive field or preferred stimulus of neurons (or voxels) and reconstruct the input as the weighted sum of the receptive fields by their activity [<xref ref-type="bibr" rid="c50">Stanley et al., 1999</xref>, <xref ref-type="bibr" rid="c53">Thirion et al., 2006</xref>, <xref ref-type="bibr" rid="c15">Garasto et al., 2019</xref>, <xref ref-type="bibr" rid="c3">Brackbill et al., 2020</xref>, <xref ref-type="bibr" rid="c62">Yoshida and Ohki, 2020</xref>, <xref ref-type="bibr" rid="c31">Nishimoto et al., 2011</xref>]. The down-side of this approach is that invertible linear models generally under perform in terms of capturing the coding properties of neurons compared to more complex deep neural networks [<xref ref-type="bibr" rid="c59">Willeke et al., 2023a</xref>].</p>
<p>Encoder input optimization, also involves first training an encoder which predicts the activity of neurons or voxels given sensory input. Once trained the encoder is fixed and the input to the network is optimized using backpropagation until the predicted activity matches the observed activity [<xref ref-type="bibr" rid="c35">Pierzchlewicz et al., 2023</xref>]. Unlike with invertible encoding models any SOTA neuronal encoding model can be used. But like invertible models, the networks are not specifically trained to reconstruct images so they may be less likely to extrapolate information encoded by the brain by learning general image statistics. There is some evidence to support this, static image reconstructions which where optimized to evoke similar <italic>in silico</italic> predicted neural activity also evoke more similar neural responses <italic>in vivo</italic> compared to other methods which optimized image similarity directly [<xref ref-type="bibr" rid="c9">Cobos et al., 2022</xref>](bioRxiv).</p>
<p>Although outlined here as 4 distinct classes these approaches can be combined. For instance, encoder input optimization can be combined with image diffusion [<xref ref-type="bibr" rid="c35">Pierzchlewicz et al., 2023</xref>] and in principle invertible models could also be combined in such a way.</p>
<p>We chose to pursue a pure encoder input optimization approach for single cell mouse visual cortex activity for two reasons. First, there have been considerable advances in the performance of neuronal encoding models for dynamic visual stimuli [<xref ref-type="bibr" rid="c49">Sinz et al., 2018</xref>, <xref ref-type="bibr" rid="c58">Wang et al., 2025</xref>, <xref ref-type="bibr" rid="c55">Turishcheva et al., 2024</xref>] and we aimed to take advantage of these developments. Second, the addition of a generative decoder trained to producing high quality images brings with it the risk of extrapolating information based on general image statistics rather than interpreting what the brain is representing. In some cases, the brain may not be encoding coherent images and in those cases we would argue image reconstruction should fail, rather than producing an image when only the semantic information is present.</p>
</sec>
<sec id="s3c">
<label>3.3</label>
<title>Key contributions and limitations</title>
<p>We demonstrate high-quality video reconstruction from mouse V1 using SOTA DNEMs to iteratively optimize the input video to match the resulting predicted activity with the recorded neuronal activity. Key to achieving high-quality reconstructions is model ensembling and using a large enough number of recorded neurons over a given retinotopic area.</p>
<p>While we averaged the video reconstructions from several models, an alternative method would be to average the gradients calculated by multiple models at each epoch, as has been done for the generation of maximally exciting images in the past [<xref ref-type="bibr" rid="c57">Walker et al., 2019</xref>]. When using video models this can be an impractical solution due to the amount of GPU memory required, but in principle there might be situations in which averaging gradients yields better reconstructions. For instance, there may be multiple solutions for the activation pattern of a neural population, e.g. if their responses are translation/phase invariant [<xref ref-type="bibr" rid="c19">Ito et al., 1995</xref>, <xref ref-type="bibr" rid="c51">Tacchetti et al., 2018</xref>]. In such a case, averaging ‘misaligned’ reconstructions from multiple models might degrade overall quality. However, we observed no performance improvement when ensembling with gradients instead of ensembling with reconstructions.</p>
<p>The SOTA DNEM we used takes video data at an angular resolution of 3.4 °/pixels at the center of the screen which is about 3x worse than the visual acuity of mice (≈0.5 cycles/° [<xref ref-type="bibr" rid="c37">Prusky and Douglas, 2004</xref>]). As our model can reconstruct Gaussian noise stimuli down to a spatial length constant of 1 pixel, and drifting gratings up to a spatial frequency of 0.071 cycles/°, there is still some potential for improving spatial resolution. To close this gap and achieve reconstructions equivalent to the limit of mouse visual acuity, a different dataset and model would likely need to be developed. However, the frame rate of the videos the SOTA DNEM takes as input (30 Hz) is faster than the flicker fusion frequency of mice (14 Hz [<xref ref-type="bibr" rid="c32">Nomura et al., 2019</xref>]) and our tests with Gaussian noise and drifting grating stimuli show that the temporal resolution of reconstruction is close to this expected limit. Future efforts should therefore focus on the spatial resolution of video reconstruction rather than the temporal resolution.</p>
<p>It is, however, unclear how closely the representation of vision by the brain is expected to match the actual input. There are a number of visual processing phenomena that have previously been identified which leads us to suspect that some deviations between video reconstructions and ground truth input are to be expected. One such phenomenon is predictive coding [<xref ref-type="bibr" rid="c39">Rao and Ballard, 1999</xref>, <xref ref-type="bibr" rid="c13">Fiser et al., 2016</xref>]. It is possible that the unexpected parts of visual stimuli are sharper and have higher contrast compared to the expected parts when reconstructed from neuronal activity. Alternatively, perceptual learning is a phenomenon where visual stimulus detection or discriminability is enhanced through prolonged training [<xref ref-type="bibr" rid="c28">Li, 2015</xref>] and is associated with changes in the tuning distribution of neurons in the visual system [<xref ref-type="bibr" rid="c17">Goltstein et al., 2013</xref>, <xref ref-type="bibr" rid="c36">Poort et al., 2015</xref>, <xref ref-type="bibr" rid="c20">Jurjut et al., 2017</xref>, <xref ref-type="bibr" rid="c45">Schumacher et al., 2022</xref>]. Similarly, selective feature attention can modulate the response amplitude of neurons that have a preference for the features that are currently being attended to [<xref ref-type="bibr" rid="c22">Kanamori and Mrsic-Flogel, 2022</xref>]. Visual task engagement and training could therefore alter the accuracy and biases of what features of a video can accurately be reconstructed from the neuronal activity. Visualizing differences between movie reconstructions from experimentaly derived recordings to those from predicted activity, as we have done, may be an interesting approach.</p>
<p>Although fMRI-based reconstruction techniques are starting to be used to investigate visual phenomena in humans (such as illusions [<xref ref-type="bibr" rid="c8">Cheng et al., 2023</xref>] and mental imagery <xref ref-type="bibr" rid="c48">Shen et al. [2019b]</xref>, <xref ref-type="bibr" rid="c25">Koide-Majima et al. [2024]</xref>, <xref ref-type="bibr" rid="c21">Kalantari et al. [2025]</xref>), visual processing phenomena are likely difficult to investigate using existing fMRI-based reconstruction approaches, due to the low spatial and temporal resolution of the data. Additionally, many of these fMRI-based reconstruction approaches rely on the use of pretrained generative diffusion models to achieve more naturalistic and semantically interpretable images [<xref ref-type="bibr" rid="c52">Takagi and Nishimoto, 2023</xref>, <xref ref-type="bibr" rid="c33">Ozcelik and VanRullen, 2023</xref>, <xref ref-type="bibr" rid="c46">Scotti et al., 2023</xref>, <xref ref-type="bibr" rid="c7">Chen et al., 2023</xref>], but very likely at the cost of introducing information that may not be present in the actual neuronal representation. In contrast, our video reconstruction approach using single-trial single-cell resolution recordings, without a pretrained generative model, provides a more accurate method to investigate visual processing phenomena such as predictive coding, perceptual learning, and selective feature attention.</p>
<p>In conclusion, we reconstruct videos presented to mice based on single-trial activity of neurons in the mouse visual cortex. This paves the way to using movie reconstruction as a tool to investigate a variety of visual processing phenomena.</p>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Methods</title>
<sec id="s4a">
<label>4.1</label>
<title>Source data</title>
<p>The data was provided by the Sensorium 2023 competition [<xref ref-type="bibr" rid="c54">Turishcheva et al., 2023</xref>, <xref ref-type="bibr" rid="c55">2024</xref>] and downloaded from <ext-link ext-link-type="uri" xlink:href="https://gin.g-node.org/pollytur/Sensorium2023Data">https://gin.g-node.org/pollytur/Sensorium2023Data</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://gin.g-node.org/pollytur/sensorium_2023_dataset">https://gin.g-node.org/pollytur/sensorium_2023_dataset</ext-link>. The data included grayscale movies presented to the mice at 30 Hz on a 31.8 by 56.5 cm monitor 15 cm from and perpendicular to the left eye. The movies were provided as spatially downsampled versions of the original screen resolution to 36 by 64 pixels, corresponding to an angular resolution of 3.4 °/pixel at the center of the screen. The pupil position and diameter were recorded at 20 Hz and the running at 100 Hz. The neuronal activity was measured using two-photon imaging [<xref ref-type="bibr" rid="c11">Denk et al., 1990</xref>] of GCaMP6s [<xref ref-type="bibr" rid="c5">Chen et al., 2013</xref>] fluorescence at 8 Hz, extracted and deconvolved using the CAIMAN pipeline [<xref ref-type="bibr" rid="c16">Giovannucci et al., 2019</xref>]. For each of the 10 mice, the activity of ≈8000 neurons was provided. The different data types were resampled to 30 Hz.</p>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>State-of-the-art dynamic neural encoding model</title>
<p>We used the winning model of the Sensorium 2024 competition, DwiseNeuro [<xref ref-type="bibr" rid="c54">Turishcheva et al., 2023</xref>, <xref ref-type="bibr" rid="c55">2024</xref>]. The code for the SOTA DNEM was downloaded from <ext-link ext-link-type="uri" xlink:href="https://github.com/lRomul/sensorium">https://github.com/lRomul/sensorium</ext-link>. This model achieved an average single-trial correlation between predicted and ground truth neural activity of 0.291 during the competition, this was later improved to 0.301. The competition benchmark models achieved 0.106, 0.164 and 0.197 single-trial correlation, while the second and third place models achieved 0.265 and 0.243. Across the models, a variety of architectural components were used, including 2D and 3D convolutional layers, recurrent layers, and transformers, to name just a few. The winning model consists of 3 main components: core, cortex, and readout. The core largely consisted of factorized 3D convolution blocks with residual connections, positional encoding [<xref ref-type="bibr" rid="c56">Vaswani et al., 2017</xref>] and SiLU activations [<xref ref-type="bibr" rid="c12">Elfwing et al., 2017</xref>] followed by spatial average pooling. The cortex consisted of three fully connected layers. The readout consisted of a 1D convolution for each mouse with a final Softplus nonlinearity, that gives activity predictions for all neurons of each mouse. The kernel of the input layer had size 16 with a dilation of 2 in the time dimension, so spanned 32 video frames.</p>
<p>The original ensemble of models consisted of 7 model instances trained on a 7-fold cross-validation split of all available Sensorium 2023 competition data (≈1 hour of training data and ≈ 8 min of cross-validation data per fold from each mouse). Each model instance was trained on 6 of 7 data folds, with different validation data excluded from training for each model. To allow ensembled reconstructions of videos without test set contamination we instead retrained the models with a shared validation fold, i.e. we retrained the models leaving out the same validation data for all 7 model instances. The only other difference in the training procedure was that we retrained the models using a batch size of 24 instead of 32, this did not change the performance of neuronal response prediction on the withheld data folds (mean validation fold predicted vs ground truth response correlation for original weights: 0.293; and retrained weights: 0.291). We also did not use model distillation, while the original model did (see <ext-link ext-link-type="uri" xlink:href="https://github.com/lRomul/sensorium">https://github.com/lRomul/sensorium</ext-link>).</p>
<p>We chose the first 10 movies in data fold 0 (assigned as part of the DNEM code using a video hashing function) for reconstructions. We additionally excluded 9 movies which were incorrectly assigned to fold 0 and replaced them with other movie clips from fold 0.</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Additional visual stimuli</title>
<p>The Gaussian noise stimuli were downloaded from <ext-link ext-link-type="uri" xlink:href="https://github.com/TomGeorge1234/gp_video">https://github.com/TomGeorge1234/gp_video</ext-link> and spanned a range of 0 to 32 pixels in spatial length constant and 0 to 32 frames in temporal length constant used in the Gaussian process. 5 separately generated movies of 2 seconds each were generated and combined with their phase-inverted versions to give a total of 10 trials.</p>
<p>The drifting grating stimuli were produced using PsychoPy [<xref ref-type="bibr" rid="c34">Peirce et al., 2019</xref>] and ranged from 0.5 to 0.062 cycles/degree and 0.5 to 0 cycles/second, with 2 seconds of movie for each cardinal direction. These ranges were chosen to avoid aliasing effects in the 36 by 64 pixel videos. The highest temporal frequency corresponds to a flicker stimulus.</p>
<p>The receptive field mapping stimulus, i.e. sparse noise stimulus, consisted of a pre-stimulus gray (gray value 127) screen period of 0.5 seconds, a 0.5 second stimulus period where one pixel was set to either 0 (Off) or 255 (On), and a 0.5 second post-stimulus gray screen period. The full stimulus set consisted of 4608 stimuli, one On and one Off stimulus for every pixel of the 36 by 64 movie.</p>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>Mask training</title>
<p>To generate the transparency masks we used an alpha blending approach inspired by [<xref ref-type="bibr" rid="c29">Mordvintsev et al., 2018</xref>] [<xref ref-type="bibr" rid="c60">Willeke et al., 2023b</xref>](bioRxiv). A transparency layer was placed at the input to the SOTA DNEM. This transparency layer was used to alpha blend the true video <italic>V</italic> with another randomly selected background video <italic>BG</italic> from the data:
<disp-formula id="eqn1">
<graphic xlink:href="599691v4_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>α</italic> is the 2D transparency mask and <italic>V</italic><sub><italic>BG</italic></sub> is the blended input video. This mask was optimized using stochastic gradient descent (for 1000 epochs with learning rate 10) with mean squared error (<italic>MSE</italic>) loss between the true responses <italic>y</italic> and the predicted responses <italic>ŷ</italic> scaled by the average weight of the transparency mask <inline-formula id="inline-eqn-9"><inline-graphic xlink:href="599691v4_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>:
<disp-formula id="eqn2">
<graphic xlink:href="599691v4_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn3">
<graphic xlink:href="599691v4_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>n</italic> is the total number of neurons. The mask was initialized as uniform noise between 0 and 0.05. At each epoch the neuronal activity in response to a randomly selected 32 frame video segment from the training set was predicted and the gradients of the loss (<xref ref-type="disp-formula" rid="eqn3">Equation 3</xref>) with respect to the pixels in the transparency mask <italic>α</italic> were calculated for each video frame. The gradients were normalized by their matrix norm, clipped to between −1 and 1 and averaged across frames. The gradients were smoothed with a 2D Gaussian kernel of <italic>σ</italic> = 5 and subtracted from the transparency mask. The transparency mask was only calculated using one SOTA DNEM instance using its validation fold. See Supplementary Algorithm 2.</p>
<p>The transparency mask was thresholded and binarized at 0.5 for the masked gradients ∇<sub><italic>masked</italic></sub> or 1 for the masked videos for evaluation <italic>V</italic><sub><italic>eval</italic></sub>:
<disp-formula id="eqn4">
<graphic xlink:href="599691v4_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn5">
<graphic xlink:href="599691v4_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where ∇ is the gradients of the loss with respect to each pixel in the video and <italic>V</italic> is the reconstructed video before masking. These masks were trained independently for each mouse using one model instance with the original weights of the model <ext-link ext-link-type="uri" xlink:href="https://github.com/lRomul/sensorium">https://github.com/lRomul/sensorium</ext-link>, not the retrained models used in the rest of this paper to reconstruct the videos.</p>
</sec>
<sec id="s4e">
<label>4.5</label>
<title>Video reconstruction</title>
<p>To reconstruct the input video we initialized the video as uniform gray values and concatenated the ground truth behavioral parameters. The SOTA DNEM took 32 frames at a time and we shifted this window by 8 frames until all frames of the whole 10s video were covered. For each 32-frame window, the Poisson negative log-likelihood loss between the predicted and true neuronal responses was calculated:
<disp-formula id="eqn6">
<graphic xlink:href="599691v4_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>ŷ</italic> are the predicted responses and <italic>y</italic> are the ground truth responses. The gradients of the loss with respect to each pixel of the input video were calculated for each window of frames and averaged across all windows. The gradients for each pixel were normalized by the matrix norm across all gradients and clipped to between −1 and 1. The gradients were masked (<xref ref-type="disp-formula" rid="eqn4">Equation 4</xref>) and applied to the input video using Adam without second order momentum [<xref ref-type="bibr" rid="c24">Kingma and Ba, 2014</xref>] (<italic>β</italic><sub>1</sub> = 0.9) for 1000 epochs and a learning rate of 1000, with a learning rate warm-up for the first 10 epochs. After each epoch, the video was clipped to between 0 and 255. The optimization was run for 1000 epochs. 7 reconstructions from 7 model instances were averaged, denoised with a 3D Gaussian filter <italic>σ</italic> = 0.5 (unless specified otherwise), and masked with the evaluation mask. See Supplementary Algorithm 1. Optimizing each 10-second video with one model instance for 1000 epochs took ≈ 60 min using a desktop with an RTX4070 GPU.</p>
</sec>
<sec id="s4f">
<label>4.6</label>
<title>Reconstruction quality assessment</title>
<p>To evaluate the similarity between reconstructed and ground truth videos, we used the mean Pearson’s correlation between pixels of corresponding frames to evaluate spatial similarity:
<disp-formula id="eqn7">
<graphic xlink:href="599691v4_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>f</italic> is the number of frames, and <italic>x</italic><sub><italic>i</italic></sub> and <inline-formula id="inline-eqn-10"><inline-graphic xlink:href="599691v4_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are the ground truth and reconstructed frames. To evaluate temporal and spatial similarity between ground truth and reconstructed videos we used the Pearson’s correlation between all pixels of the whole movie:
<disp-formula id="eqn8">
<graphic xlink:href="599691v4_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
To calculate the Shannon entropy, we first computed the intensity histogram of the pixels inside the evaluation mask for every frame (25 bins between 0 and 255). Shannon entropy of one frame (<italic>H</italic><sub><italic>f</italic></sub>) was then calculated as:
<disp-formula id="eqn9">
<graphic xlink:href="599691v4_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>p</italic><sub><italic>k</italic></sub> is the normalized histogram count of bin <italic>k</italic> (only including non-zero bins). For each movie the average Shannon entropy across frames is taken. <italic>n</italic> is the total number of non-zero bins.</p>
<p>The motion energy of a frame (<italic>E</italic><sub><italic>f</italic></sub>) is calculated as:
<disp-formula id="eqn10">
<graphic xlink:href="599691v4_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>V</italic><sub><italic>f,i</italic></sub> is the intensity value for one pixel <italic>i</italic> inside the evaluation mask at frame <italic>f, n</italic> is the total number of pixels inside the mask.</p>
</sec>
<sec id="s4g">
<label>4.7</label>
<title>Retinotopic mapping</title>
<p>To calculate the receptive fields of neurons <italic>in silico</italic> we predicted each neurons response to the full sparse noise stimulus set using the ensembles prediction of 7 SOTA DNEM instances. The response map across pixels for each neuron (<italic>OnR</italic><sub><italic>h,w,n</italic></sub>) was defined as:
<disp-formula id="eqn11">
<graphic xlink:href="599691v4_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where h and w denote the position of the pixel on the screen, and <italic>n</italic> the neuron. <italic>R</italic><sub><italic>stim</italic></sub> is the predicted response of the neuron during the stimulus period and <italic>R</italic><sub><italic>pre</italic></sub> during the pre-stimulus period. <italic>OnR</italic><sub><italic>h,w,n</italic></sub> was thresholded at 0.1. The same procedure was done to calculate <italic>Off R</italic><sub><italic>h,w,n</italic></sub>. The <italic>OnR</italic> and <italic>Off R</italic> maps where smoothed using a 2D Gaussian filter with <italic>σ</italic> = 2 and then normalized by the maximum value for each neuron. The On and Off receptive field centers were defined as the pixel with the maximum value for each neuron. We calculate the On-Off receptive fields for example neurons as:
<disp-formula id="eqn12">
<graphic xlink:href="599691v4_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and calculate the population On or Off response as:
<disp-formula id="eqn13">
<graphic xlink:href="599691v4_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s4h">
<label>4.8</label>
<title>Reconstruction area calculation</title>
<p>To calculate the retinotopic diameter of a mask we first computed the retinotopic area of each pixel of the movie based on the screen size (31.8 cm by 56.5 cm) and distance from the mouse eye (15 cm). Strictly speaking this is the visuotopic area as it does not take eye position into account, but we refer to it as retinotopic for simplicity. We then take the sum of all pixel areas for an evaluation mask with a given <italic>α</italic> threshold. Then we define the retinotopic diameter of this area (<italic>A</italic>) as:
<disp-formula id="eqn14">
<graphic xlink:href="599691v4_eqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s4i">
<label>4.9</label>
<title>Error map calculation</title>
<p>To calculate the error maps we reconstruct movie clips either from the experimental neural responses or the predicted neural responses given the ground truth movie and took the difference:
<disp-formula id="eqn15">
<graphic xlink:href="599691v4_eqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn16">
<graphic xlink:href="599691v4_eqn16.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>x</italic> is the ground truth video, <italic>y</italic> is the experimental neural activity, <inline-formula id="inline-eqn-11"><inline-graphic xlink:href="599691v4_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the reconstructed movie from <inline-formula id="inline-eqn-12"><inline-graphic xlink:href="599691v4_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the predicted neural activity from <inline-formula id="inline-eqn-13"><inline-graphic xlink:href="599691v4_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the predicted neural activity from <italic>x</italic>, and <inline-formula id="inline-eqn-14"><inline-graphic xlink:href="599691v4_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the reconstructed movie from <italic>ŷ</italic>. Using Fiji [<xref ref-type="bibr" rid="c43">Schindelin et al., 2012</xref>] the positive error (LUT: red hot, range 25 to 75), negative error (LUT: cyan hot, range −25 to −75), and ground truth video (LUT: gray scale, range 0 to 255) were then combined into a composite image.</p>
</sec>
</sec>
</body>
<back>
<sec id="das" sec-type="data-availability">
<title>Data availability</title>
<p>The code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Joel-Bauer/movie_reconstruction_code">https://github.com/Joel-Bauer/movie_reconstruction_code</ext-link>.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We would like to thank Emmanuel Bauer, Sandra Reinert and the anonymous reviewers for useful input and discussions, and Tom George for the Gaussian noise stimulus set. T.W.M. is funded by The Wellcome Trust (219627/Z/19/Z; 306384/Z/23/Z) and Gatsby Charitable Foundation (GAT3755) and J.B. is funded by EMBO (ALTF 415-2024).</p>
</ack>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="video1">
<label>Supplementary Video 1.</label>
<caption>
<title>Reconstructed natural videos from mouse brain activity.</title>
<p>Odd rows are ground truth (GT) movie clips presented to mice. Even rows are the reconstructed movies from the activity of ≈ 8000 V1 neurons. Reconstructed movies are smoothed (<italic>σ</italic> = 0.5 pixels), masked, and contrast (std) and luminance (mean) matched to ground truth movies.</p>
</caption>
<media mimetype="video" mime-subtype="mp4" xlink:href="video1.mp4"/>
</supplementary-material>
<supplementary-material id="video2">
<label>Supplementary Video 2.</label>
<caption>
<title>Gaussian noise stimuli and reconstructions.</title>
<p>Odd rows are ground truth (GT) video inputs to the model. Even rows are the reconstructed videos from the predicted neuronal activity for 1 mouse. Reconstructed movies are masked, and contrast (std) and luminance (mean) matched to ground truth videos.</p>
</caption>
<media mimetype="video" mime-subtype="mp4" xlink:href="video2.mp4"/>
</supplementary-material>
<supplementary-material id="video3">
<label>Supplementary Video 3.</label>
<caption>
<title>Drifting grating stimuli and reconstructions.</title>
<p>Odd rows are ground truth (GT) video inputs to the model. Even rows are the reconstructed videos from the predicted neuronal activity for 1 mouse. Reconstructed movies are masked, and contrast (std) and luminance (mean) matched to ground truth videos.</p>
</caption>
<media mimetype="video" mime-subtype="mp4" xlink:href="video3.mp4"/>
</supplementary-material>
<supplementary-material id="video4">
<label>Supplementary Video 4.</label>
<caption>
<title>Reconstruction error maps.</title>
<p>Pixel error = reconstructions from experimental neural activity – reconstructions from expected neural activity. Over-&amp; underestimations of pixel values as hot &amp; cold heat maps, respectively. Ground truth movies in gray. Each row is 10 trials from 1 mouse played at half speed.</p>
</caption>
<media mimetype="video" mime-subtype="mp4" xlink:href="video4.mp4"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><given-names>Ruslan</given-names> <surname>Baikulov</surname></string-name></person-group>. <article-title>Solution for Sensorium 2023 Competition</article-title> (v<version>23.11.22</version>). <source>Zenodo</source>, <year>2023</year>. <pub-id pub-id-type="doi">10.5281/zenodo.10155151</pub-id>, <month>nov</month> <year>2023</year>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Yohann</given-names> <surname>Benchetrit</surname></string-name>, <string-name><given-names>Hubert</given-names> <surname>Banville</surname></string-name>, and <string-name><given-names>Jean-Rémi</given-names> <surname>King</surname></string-name></person-group>. <article-title>Brain decoding: toward real-time reconstruction of visual perception</article-title>. <source>arXiv</source>, <year>2023</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.2310.19812</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nora</given-names> <surname>Brackbill</surname></string-name>, <string-name><given-names>Colleen</given-names> <surname>Rhoades</surname></string-name>, <string-name><given-names>Alexandra</given-names> <surname>Kling</surname></string-name>, <string-name><given-names>Nishal P</given-names> <surname>Shah</surname></string-name>, <string-name><given-names>Alexander</given-names> <surname>Sher</surname></string-name>, <string-name><given-names>Alan M</given-names> <surname>Litke</surname></string-name>, and <string-name><given-names>EJ</given-names> <surname>Chichilnisky</surname></string-name></person-group>. <article-title>Reconstruction of natural images from responses of primate retinal ganglion cells</article-title>. <source>eLife</source>, <volume>9</volume>: <elocation-id>e58516</elocation-id>, <year>2020</year>. doi: <pub-id pub-id-type="doi">10.7554/elife.58516</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Leo</given-names> <surname>Breiman</surname></string-name></person-group>. <article-title>Stacked regressions</article-title>. <source>Machine Learning</source>, <volume>24</volume>(<issue>1</issue>):<fpage>49</fpage>–<lpage>64</lpage>, <year>1996</year>. ISSN <issn>0885-6125</issn>. doi: <pub-id pub-id-type="doi">10.1007/bf00117832</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Tsai-Wen</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Trevor J.</given-names> <surname>Wardill</surname></string-name>, <string-name><given-names>Yi</given-names> <surname>Sun</surname></string-name>, <string-name><given-names>Stefan R.</given-names> <surname>Pulver</surname></string-name>, <string-name><given-names>Sabine L.</given-names> <surname>Renninger</surname></string-name>, <string-name><given-names>Amy</given-names> <surname>Baohan</surname></string-name>, <string-name><given-names>Eric R.</given-names> <surname>Schreiter</surname></string-name>, <string-name><given-names>Rex A.</given-names> <surname>Kerr</surname></string-name>, <string-name><given-names>Michael B.</given-names> <surname>Orger</surname></string-name>, <string-name><given-names>Vivek</given-names> <surname>Jayaraman</surname></string-name>, <string-name><given-names>Loren L.</given-names> <surname>Looger</surname></string-name>, <string-name><given-names>Karel</given-names> <surname>Svoboda</surname></string-name>, and <string-name><given-names>Douglas S.</given-names> <surname>Kim</surname></string-name></person-group>. <article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title>. <source>Nature</source>, <volume>499</volume>(<issue>7458</issue>):<fpage>295</fpage>–<lpage>300</lpage>, <year>2013</year>. ISSN <issn>0028-0836</issn>. doi: <pub-id pub-id-type="doi">10.1038/nature12354</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ye</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Beech</surname></string-name>, <string-name><given-names>Ziwei</given-names> <surname>Yin</surname></string-name>, <string-name><given-names>Shanshan</given-names> <surname>Jia</surname></string-name>, <string-name><given-names>Jiayi</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Zhaofei</given-names> <surname>Yu</surname></string-name>, and <string-name><given-names>Jian K.</given-names> <surname>Liu</surname></string-name></person-group>. <article-title>Decoding dynamic visual scenes across the brain hierarchy</article-title>. <source>PLOS Computational Biology</source>, <volume>20</volume>(<issue>8</issue>):<fpage>e1012297</fpage>, <year>2024</year>. ISSN <issn>1553-734X</issn>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1012297</pub-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Zijiao</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Jiaxin</given-names> <surname>Qing</surname></string-name>, and <string-name><given-names>Juan Helen</given-names> <surname>Zhou</surname></string-name></person-group>. <article-title>Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity</article-title>. <source>arXiv</source>, <year>2023</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.2305.11675</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Fan L</given-names> <surname>Cheng</surname></string-name>, <string-name><given-names>Tomoyasu</given-names> <surname>Horikawa</surname></string-name>, <string-name><given-names>Kei</given-names> <surname>Majima</surname></string-name>, <string-name><given-names>Misato</given-names> <surname>Tanaka</surname></string-name>, <string-name><given-names>Mohamed</given-names> <surname>Abdelhack</surname></string-name>, <string-name><given-names>Shuntaro C</given-names> <surname>Aoki</surname></string-name>, <string-name><given-names>Jin</given-names> <surname>Hirano</surname></string-name>, and <string-name><given-names>Yukiyasu</given-names> <surname>Kamitani</surname></string-name></person-group>. <article-title>Reconstructing visual illusory experiences from human brain activity</article-title>. <source>Science Advances</source>, <volume>9</volume>(<issue>46</issue>):<fpage>eadj3906</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Erick</given-names> <surname>Cobos</surname></string-name>, <string-name><given-names>Taliah</given-names> <surname>Muhammad</surname></string-name>, <string-name><given-names>Paul G</given-names> <surname>Fahey</surname></string-name>, <string-name><given-names>Zhiwei</given-names> <surname>Ding</surname></string-name>, <string-name><given-names>Zhuokun</given-names> <surname>Ding</surname></string-name>, <string-name><given-names>Jacob</given-names> <surname>Reimer</surname></string-name>, <string-name><given-names>Fabian H</given-names> <surname>Sinz</surname></string-name>, and <string-name><given-names>Andreas S</given-names> <surname>Tolias</surname></string-name></person-group>. <article-title>It takes neurons to understand neurons: Digital twins of visual cortex synthesize neural metamers</article-title>. <source>bioRxiv</source>, <year>2022</year>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Daniel</given-names> <surname>Deitch</surname></string-name>, <string-name><given-names>Alon</given-names> <surname>Rubin</surname></string-name>, and <string-name><given-names>Yaniv</given-names> <surname>Ziv</surname></string-name></person-group>. <article-title>Representational drift in the mouse visual cortex</article-title>. <source>Current Biology</source>, <volume>31</volume>(<issue>19</issue>):<fpage>4327</fpage>–<lpage>4339.e6,</lpage> <year>2021</year>. ISSN <issn>0960-9822</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2021.07.062</pub-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Winifried</given-names> <surname>Denk</surname></string-name>, <string-name><given-names>James H.</given-names> <surname>Strickler</surname></string-name>, and <string-name><given-names>Watt W.</given-names> <surname>Webb</surname></string-name></person-group>. <article-title>Two-Photon Laser Scanning Fluorescence Microscopy</article-title>. <source>Science</source>, <volume>248</volume>(<issue>4951</issue>):<fpage>73</fpage>–<lpage>76</lpage>, <year>1990</year>. ISSN <issn>0036-8075</issn>. doi: <pub-id pub-id-type="doi">10.1126/science.2321027</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Stefan</given-names> <surname>Elfwing</surname></string-name>, <string-name><given-names>Eiji</given-names> <surname>Uchibe</surname></string-name>, and <string-name><given-names>Kenji</given-names> <surname>Doya</surname></string-name></person-group>. <article-title>Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning</article-title>. <source>arXiv</source>, <year>2017</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.1702.03118</pub-id>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Aris</given-names> <surname>Fiser</surname></string-name>, <string-name><given-names>David</given-names> <surname>Mahringer</surname></string-name>, <string-name><given-names>Hassana K</given-names> <surname>Oyibo</surname></string-name>, <string-name><given-names>Anders V</given-names> <surname>Petersen</surname></string-name>, <string-name><given-names>Marcus</given-names> <surname>Leinweber</surname></string-name>, and <string-name><given-names>Georg B</given-names> <surname>Keller</surname></string-name></person-group>. <article-title>Experience-dependent spatial expectations in mouse visual cortex</article-title>. <source>Nature Neuroscience</source>, <volume>19</volume>(<issue>12</issue>):<fpage>1658</fpage>–<lpage>1664</lpage>, <year>2016</year>. ISSN <issn>1097-6256</issn>. doi: <pub-id pub-id-type="doi">10.1038/nn.4385</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Peter</given-names> <surname>Földiák</surname></string-name></person-group>. <chapter-title>The ‘Ideal Homunculus’: Statistical Inference from Neural Population Responses</chapter-title> In <person-group person-group-type="editor"><string-name><given-names>FH</given-names> <surname>Eeckman</surname></string-name> &amp; <string-name><given-names>JM</given-names> <surname>Bower</surname></string-name></person-group> <source>Computation and Neural Systems</source>. pages <fpage>55</fpage>–<lpage>60</lpage>, <year>1993</year>. <publisher-name>Springer New York</publisher-name>, <publisher-loc>NY</publisher-loc>. doi: <pub-id pub-id-type="doi">10.1007/978-1-4615-3254-5_9</pub-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Stef</given-names> <surname>Garasto</surname></string-name>, <string-name><given-names>Wilten</given-names> <surname>Nicola</surname></string-name>, <string-name><given-names>Anil A.</given-names> <surname>Bharath</surname></string-name>, and <string-name><given-names>Simon R.</given-names> <surname>Schultz</surname></string-name></person-group>. <article-title>Neural Sampling Strategies for Visual Stimulus Reconstruction from Two-photon Imaging of Mouse Primary Visual Cortex</article-title>. <conf-name>2019 9th International IEEE/EMBS Conference on Neural Engineering (NER)</conf-name>, <volume>00</volume>:<fpage>566</fpage>–<lpage>570</lpage>, <year>2019</year>. doi: <pub-id pub-id-type="doi">10.1109/ner.2019.8716934</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Andrea</given-names> <surname>Giovannucci</surname></string-name>, <string-name><given-names>Johannes</given-names> <surname>Friedrich</surname></string-name>, <string-name><given-names>Pat</given-names> <surname>Gunn</surname></string-name>, <string-name><given-names>Jérémie</given-names> <surname>Kalfon</surname></string-name>, <string-name><given-names>Brandon L</given-names> <surname>Brown</surname></string-name>, <string-name><given-names>Sue Ann</given-names> <surname>Koay</surname></string-name>, <string-name><given-names>Jiannis</given-names> <surname>Taxidis</surname></string-name>, <string-name><given-names>Farzaneh</given-names> <surname>Najafi</surname></string-name>, <string-name><given-names>Jeffrey L</given-names> <surname>Gauthier</surname></string-name>, <string-name><given-names>Pengcheng</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>Baljit S</given-names> <surname>Khakh</surname></string-name>, <string-name><given-names>David W</given-names> <surname>Tank</surname></string-name>, <string-name><given-names>Dmitri B</given-names> <surname>Chklovskii</surname></string-name>, and <string-name><given-names>Eftychios A</given-names> <surname>Pnevmatikakis</surname></string-name></person-group>. <article-title>CaImAn an open source tool for scalable calcium imaging data analysis</article-title>. <source>eLife</source>, <volume>8</volume>:<elocation-id>e38173</elocation-id>, <year>2019</year>. doi: <pub-id pub-id-type="doi">10.7554/elife.38173</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Pieter M.</given-names> <surname>Goltstein</surname></string-name>, <string-name><given-names>Emily B. J.</given-names> <surname>Coffey</surname></string-name>, <string-name><given-names>Pieter R.</given-names> <surname>Roelfsema</surname></string-name>, and <string-name><given-names>Cyriel M. A.</given-names> <surname>Pennartz</surname></string-name></person-group>. <article-title>In Vivo Two-Photon Ca2+ Imaging Reveals Selective Reward Effects on Stimulus-Specific Assemblies in Mouse Visual Cortex</article-title>. <source>The Journal of Neuroscience</source>, <volume>33</volume>(<issue>28</issue>):<fpage>11540</fpage>–<lpage>11555</lpage>, <year>2013</year>. ISSN <issn>0270-6474</issn>. doi: <pub-id pub-id-type="doi">10.1523/jneurosci.1341-12.2013</pub-id>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jun Kai</given-names> <surname>Ho</surname></string-name>, <string-name><given-names>Tomoyasu</given-names> <surname>Horikawa</surname></string-name>, <string-name><given-names>Kei</given-names> <surname>Majima</surname></string-name>, <string-name><given-names>Fan</given-names> <surname>Cheng</surname></string-name>, and <string-name><given-names>Yukiyasu</given-names> <surname>Kamitani</surname></string-name></person-group>. <article-title>Inter-individual deep image reconstruction via hierarchical neural code conversion</article-title>. <source>NeuroImage</source>, <volume>271</volume>:<fpage>120007</fpage>, <year>2023</year>. ISSN <issn>1053-8119</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2023.120007</pub-id>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Ito</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Tamura</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Fujita</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Tanaka</surname></string-name></person-group>. <article-title>Size and position invariance of neuronal responses in monkey inferotemporal cortex</article-title>. <source>Journal of Neurophysiology</source>, <volume>73</volume>(<issue>1</issue>):<fpage>218</fpage>–<lpage>226</lpage>, <year>1995</year>. ISSN <issn>0022-3077</issn>. doi: <pub-id pub-id-type="doi">10.1152/jn.1995.73.1.218</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ovidiu</given-names> <surname>Jurjut</surname></string-name>, <string-name><given-names>Petya</given-names> <surname>Georgieva</surname></string-name>, <string-name><given-names>Laura</given-names> <surname>Busse</surname></string-name>, and <string-name><given-names>Steffen</given-names> <surname>Katzner</surname></string-name></person-group>. <article-title>Learning Enhances Sensory Processing in Mouse V1 before Improving Behavior</article-title>. <source>The Journal of Neuroscience</source>, <volume>37</volume>(<issue>27</issue>):<fpage>6460</fpage>–<lpage>6474</lpage>, <year>2017</year>. ISSN <issn>0270-6474</issn>. doi: <pub-id pub-id-type="doi">10.1523/jneurosci.3485-16.2017</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Fatemeh</given-names> <surname>Kalantari</surname></string-name>, <string-name><given-names>Karim</given-names> <surname>Faez</surname></string-name>, <string-name><given-names>Hamidreza</given-names> <surname>Amindavar</surname></string-name>, and <string-name><given-names>Soheila</given-names> <surname>Nazari</surname></string-name></person-group>. <article-title>Improved image reconstruction from brain activity through automatic image captioning</article-title>. <source>Scientific Reports</source>, <volume>15</volume>(<issue>1</issue>):<fpage>4907</fpage>, <year>2025</year>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Takahiro</given-names> <surname>Kanamori</surname></string-name> and <string-name><given-names>Thomas D.</given-names> <surname>Mrsic-Flogel</surname></string-name></person-group>. <article-title>Independent response modulation of visual cortical neurons by attentional and behavioral states</article-title>. <source>Neuron</source>, <volume>110</volume>(<issue>23</issue>):<fpage>3907</fpage>–<lpage>3918</lpage>, <year>2022</year>. ISSN <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2022.08.028</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Kendrick N.</given-names> <surname>Kay</surname></string-name>, <string-name><given-names>Thomas</given-names> <surname>Naselaris</surname></string-name>, <string-name><given-names>Ryan J.</given-names> <surname>Prenger</surname></string-name>, and <string-name><given-names>Jack L.</given-names> <surname>Gallant</surname></string-name></person-group>. <article-title>Identifying natural images from human brain activity</article-title>. <source>Nature</source>, <volume>452</volume>(<issue>7185</issue>):<fpage>352</fpage>–<lpage>355</lpage>, <year>2008</year>. ISSN <issn>0028-0836</issn>. doi: <pub-id pub-id-type="doi">10.1038/nature06713</pub-id>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Diederik P</given-names> <surname>Kingma</surname></string-name> and <string-name><given-names>Jimmy</given-names> <surname>Ba</surname></string-name></person-group>. <article-title>Adam: A Method for Stochastic Optimization</article-title>. <source>arXiv</source>, <year>2014</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.1412.6980</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Naoko</given-names> <surname>Koide-Majima</surname></string-name>, <string-name><given-names>Shinji</given-names> <surname>Nishimoto</surname></string-name>, and <string-name><given-names>Kei</given-names> <surname>Majima</surname></string-name></person-group>. <article-title>Mental image reconstruction from human brain activity: Neural decoding of mental imagery via deep neural network-based bayesian estimation</article-title>. <source>Neural Networks</source>, <volume>170</volume>:<fpage>349</fpage>–<lpage>363</lpage>, <year>2024</year>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Ganit</given-names> <surname>Kupershmidt</surname></string-name>, <string-name><given-names>Roman</given-names> <surname>Beliy</surname></string-name>, <string-name><given-names>Guy</given-names> <surname>Gaziv</surname></string-name>, and <string-name><given-names>Michal</given-names> <surname>Irani</surname></string-name></person-group>. <article-title>A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of Natural Movies from Brain Activity</article-title>. <source>arXiv</source>, <year>2022</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.2206.03544</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Wenyi</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Shengjie</given-names> <surname>Zheng</surname></string-name>, <string-name><given-names>Yufan</given-names> <surname>Liao</surname></string-name>, <string-name><given-names>Rongqi</given-names> <surname>Hong</surname></string-name>, <string-name><given-names>Chenggang</given-names> <surname>He</surname></string-name>, <string-name><given-names>Weiliang</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Chunshan</given-names> <surname>Deng</surname></string-name>, and <string-name><given-names>Xiaojian</given-names> <surname>Li</surname></string-name></person-group>. <article-title>The brain-inspired decoder for natural visual image reconstruction</article-title>. <source>Frontiers in Neuroscience</source>, <volume>17</volume>:<fpage>1130606</fpage>, <year>2023</year>. ISSN <issn>1662-4548</issn>. doi: <pub-id pub-id-type="doi">10.3389/fnins.2023.1130606</pub-id>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Wu</given-names> <surname>Li</surname></string-name></person-group>. <article-title>Perceptual Learning: Use-Dependent Cortical Plasticity</article-title>. <source>Annual Review of Vision Science</source>, <volume>2</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>22</lpage>, <year>2015</year>. ISSN <issn>2374-4642</issn>. doi: <pub-id pub-id-type="doi">10.1146/annurev-vision-111815-114351</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alexander</given-names> <surname>Mordvintsev</surname></string-name>, <string-name><given-names>Nicola</given-names> <surname>Pezzotti</surname></string-name>, <string-name><given-names>Ludwig</given-names> <surname>Schubert</surname></string-name>, and <string-name><given-names>Chris</given-names> <surname>Olah</surname></string-name></person-group>. <article-title>Differentiable image parameterizations</article-title>. <source>Distill</source>, <volume>3</volume>(<issue>7</issue>), <month>July</month> <year>2018</year>. ISSN <issn>2476-0757</issn>. doi: <pub-id pub-id-type="doi">10.23915/distill.00012</pub-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Cristopher M.</given-names> <surname>Niell</surname></string-name> and <string-name><given-names>Michael P.</given-names> <surname>Stryker</surname></string-name></person-group>. <article-title>Modulation of Visual Responses by Behavioral State in Mouse Visual Cortex</article-title>. <source>Neuron</source>, <volume>65</volume>(<issue>4</issue>):<fpage>472</fpage>–<lpage>479</lpage>, <year>2010</year>. ISSN <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Shinji</given-names> <surname>Nishimoto</surname></string-name>, <string-name><given-names>An T.</given-names> <surname>Vu</surname></string-name>, <string-name><given-names>Thomas</given-names> <surname>Naselaris</surname></string-name>, <string-name><given-names>Yuval</given-names> <surname>Benjamini</surname></string-name>, <string-name><surname>Bin</surname> <given-names>Yu</given-names></string-name>, and <string-name><given-names>Jack L.</given-names> <surname>Gallant</surname></string-name></person-group>. <article-title>Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies</article-title>. <source>Current Biology</source>, <volume>21</volume>(<issue>19</issue>):<fpage>1641</fpage>–<lpage>1646</lpage>, <year>2011</year>. ISSN <issn>0960-9822</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2011.08.031</pub-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yuichiro</given-names> <surname>Nomura</surname></string-name>, <string-name><given-names>Shohei</given-names> <surname>Ikuta</surname></string-name>, <string-name><given-names>Satoshi</given-names> <surname>Yokota</surname></string-name>, <string-name><given-names>Junpei</given-names> <surname>Mita</surname></string-name>, <string-name><given-names>Mami</given-names> <surname>Oikawa</surname></string-name>, <string-name><given-names>Hiroki</given-names> <surname>Matsushima</surname></string-name>, <string-name><given-names>Akira</given-names> <surname>Amano</surname></string-name>, <string-name><given-names>Kazuhiro</given-names> <surname>Shimonomura</surname></string-name>, <string-name><given-names>Yasuhiro</given-names> <surname>Seya</surname></string-name>, and <string-name><given-names>Chieko</given-names> <surname>Koike</surname></string-name></person-group>. <article-title>Evaluation of critical flicker-fusion frequency measurement methods using a touchscreen-based visual temporal discrimination task in the behaving mouse</article-title>. <source>Neuroscience Research</source>, <volume>148</volume>:<fpage>28</fpage>–<lpage>33</lpage>, <year>2019</year>. ISSN <issn>0168-0102</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neures.2018.12.001</pub-id>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Furkan</given-names> <surname>Ozcelik</surname></string-name> and <string-name><given-names>Rufin</given-names> <surname>VanRullen</surname></string-name></person-group>. <article-title>Natural scene reconstruction from fMRI signals using generative latent diffusion</article-title>. <source>Scientific Reports</source>, <volume>13</volume>(<issue>1</issue>):<fpage>15666</fpage>, <year>2023</year>. doi: <pub-id pub-id-type="doi">10.1038/s41598-023-42891-8</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jonathan</given-names> <surname>Peirce</surname></string-name>, <string-name><given-names>Jeremy R</given-names> <surname>Gray</surname></string-name>, <string-name><given-names>Sol</given-names> <surname>Simpson</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>MacAskill</surname></string-name>, <string-name><given-names>Richard</given-names> <surname>Höchenberger</surname></string-name>, <string-name><given-names>Hiroyuki</given-names> <surname>Sogo</surname></string-name>, <string-name><given-names>Erik</given-names> <surname>Kastman</surname></string-name>, and <string-name><given-names>Jonas</given-names> <surname>Kristoffer Lindeløv</surname></string-name></person-group>. <article-title>Psychopy2: Experiments in behavior made easy</article-title>. <source>Behavior research methods</source>, <volume>51</volume>:<fpage>195</fpage>–<lpage>203</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Pawel</given-names> <surname>Pierzchlewicz</surname></string-name>, <string-name><given-names>Konstantin</given-names> <surname>Willeke</surname></string-name>, <string-name><given-names>Arne</given-names> <surname>Nix</surname></string-name>, <string-name><given-names>Pavithra</given-names> <surname>Elumalai</surname></string-name>, <string-name><given-names>Kelli</given-names> <surname>Restivo</surname></string-name>, <string-name><given-names>Tori</given-names> <surname>Shinn</surname></string-name>, <string-name><given-names>Cate</given-names> <surname>Nealley</surname></string-name>, <string-name><given-names>Gabrielle</given-names> <surname>Rodriguez</surname></string-name>, <string-name><given-names>Saumil</given-names> <surname>Patel</surname></string-name>, <string-name><given-names>Katrin</given-names> <surname>Franke</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Energy guided diffusion for generating neurally exciting images</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>36</volume>:<fpage>32574</fpage>–<lpage>32601</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jasper</given-names> <surname>Poort</surname></string-name>, <string-name><given-names>Adil G.</given-names> <surname>Khan</surname></string-name>, <string-name><given-names>Marius</given-names> <surname>Pachitariu</surname></string-name>, <string-name><given-names>Abdellatif</given-names> <surname>Nemri</surname></string-name>, <string-name><given-names>Ivana</given-names> <surname>Orsolic</surname></string-name>, <string-name><given-names>Julija</given-names> <surname>Krupic</surname></string-name>, <string-name><given-names>Marius</given-names> <surname>Bauza</surname></string-name>, <string-name><given-names>Maneesh</given-names> <surname>Sahani</surname></string-name>, <string-name><given-names>Georg B.</given-names> <surname>Keller</surname></string-name>, <string-name><given-names>Thomas D.</given-names> <surname>Mrsic-Flogel</surname></string-name>, and <string-name><given-names>Sonja B.</given-names> <surname>Hofer</surname></string-name></person-group>. <article-title>Learning Enhances Sensory and Multiple Non-sensory Representations in Primary Visual Cortex</article-title>. <source>Neuron</source>, <volume>86</volume>(<issue>6</issue>):<fpage>1478</fpage>–<lpage>1490</lpage>, <year>2015</year>. ISSN <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2015.05.037</pub-id>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.T.</given-names> <surname>Prusky</surname></string-name> and <string-name><given-names>R.M.</given-names> <surname>Douglas</surname></string-name></person-group>. <article-title>Characterization of mouse cortical spatial vision</article-title>. <source>Vision Research</source>, <volume>44</volume>(<issue>28</issue>): <fpage>3411</fpage>–<lpage>3418</lpage>, <year>2004</year>. ISSN <issn>0042-6989</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.visres.2004.09.001</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Zarina</given-names> <surname>Rakhimberdina</surname></string-name>, <string-name><given-names>Quentin</given-names> <surname>Jodelet</surname></string-name>, <string-name><given-names>Xin</given-names> <surname>Liu</surname></string-name>, and <string-name><given-names>Tsuyoshi</given-names> <surname>Murata</surname></string-name></person-group>. <article-title>Natural Image Reconstruction From fMRI Using Deep Learning: A Survey</article-title>. <source>Frontiers in Neuroscience</source>, <volume>15</volume>:<fpage>795488</fpage>, <year>2021</year>. ISSN <issn>1662-4548</issn>. doi: <pub-id pub-id-type="doi">10.3389/fnins.2021.795488</pub-id>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Rajesh P. N.</given-names> <surname>Rao</surname></string-name> and <string-name><given-names>Dana H.</given-names> <surname>Ballard</surname></string-name></person-group>. <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nature Neuroscience</source>, <volume>2</volume>(<issue>1</issue>):<fpage>79</fpage>–<lpage>87</lpage>, <year>1999</year>. ISSN <issn>1097-6256</issn>. doi: <pub-id pub-id-type="doi">10.1038/4580</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jacob</given-names> <surname>Reimer</surname></string-name>, <string-name><given-names>Emmanouil</given-names> <surname>Froudarakis</surname></string-name>, <string-name><given-names>Cathryn R.</given-names> <surname>Cadwell</surname></string-name>, <string-name><given-names>Dimitri</given-names> <surname>Yatsenko</surname></string-name>, <string-name><given-names>George H.</given-names> <surname>Denfield</surname></string-name>, and <string-name><given-names>Andreas S.</given-names> <surname>Tolias</surname></string-name></person-group>. <article-title>Pupil Fluctuations Track Fast Switching of Cortical States during Quiet Wakefulness</article-title>. <source>Neuron</source>, <volume>84</volume>(<issue>2</issue>):<fpage>355</fpage>–<lpage>362</lpage>, <year>2014</year>. ISSN <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.033</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ziqi</given-names> <surname>Ren</surname></string-name>, <string-name><given-names>Jie</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Xuetong</given-names> <surname>Xue</surname></string-name>, <string-name><given-names>Xin</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Fan</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>Zhicheng</given-names> <surname>Jiao</surname></string-name>, and <string-name><given-names>Xinbo</given-names> <surname>Gao</surname></string-name></person-group>. <article-title>Reconstructing seen image from brain activity by visually-guided cognitive representation and adversarial learning</article-title>. <source>NeuroImage</source>, <volume>228</volume>: <fpage>117602</fpage>, <year>2021</year>. ISSN <issn>1053-8119</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117602</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Robin</given-names> <surname>Rombach</surname></string-name>, <string-name><given-names>Andreas</given-names> <surname>Blattmann</surname></string-name>, <string-name><given-names>Dominik</given-names> <surname>Lorenz</surname></string-name>, <string-name><given-names>Patrick</given-names> <surname>Esser</surname></string-name>, and <string-name><given-names>Björn</given-names> <surname>Ommer</surname></string-name></person-group>. <article-title>High-Resolution Image Synthesis with Latent Diffusion Models</article-title>. <source>arXiv</source>, <year>2021</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.2112.10752</pub-id>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Johannes</given-names> <surname>Schindelin</surname></string-name>, <string-name><given-names>Ignacio</given-names> <surname>Arganda-Carreras</surname></string-name>, <string-name><given-names>Erwin</given-names> <surname>Frise</surname></string-name>, <string-name><given-names>Verena</given-names> <surname>Kaynig</surname></string-name>, <string-name><given-names>Mark</given-names> <surname>Longair</surname></string-name>, <string-name><given-names>Tobias</given-names> <surname>Pietzsch</surname></string-name>, <string-name><given-names>Stephan</given-names> <surname>Preibisch</surname></string-name>, <string-name><given-names>Curtis</given-names> <surname>Rueden</surname></string-name>, <string-name><given-names>Stephan</given-names> <surname>Saalfeld</surname></string-name>, <string-name><given-names>Benjamin</given-names> <surname>Schmid</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Fiji: an open-source platform for biological-image analysis</article-title>. <source>Nature methods</source>, <volume>9</volume>(<issue>7</issue>):<fpage>676</fpage>–<lpage>682</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Steffen</given-names> <surname>Schneider</surname></string-name>, <string-name><given-names>Jin Hwa</given-names> <surname>Lee</surname></string-name>, and <string-name><given-names>Mackenzie Weygandt</given-names> <surname>Mathis</surname></string-name></person-group>. <article-title>Learnable latent embeddings for joint behavioural and neural analysis</article-title>. <source>Nature</source>, <volume>12</volume>(<issue>1</issue>):<fpage>5170</fpage>, <year>2023</year>. ISSN <issn>0028-0836</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41586-023-06031-6</pub-id>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Joseph W.</given-names> <surname>Schumacher</surname></string-name>, <string-name><given-names>Matthew K.</given-names> <surname>McCann</surname></string-name>, <string-name><given-names>Katherine J.</given-names> <surname>Maximov</surname></string-name>, and <string-name><given-names>David</given-names> <surname>Fitzpatrick</surname></string-name></person-group>. <article-title>Selective enhancement of neural coding in V1 underlies fine-discrimination learning in tree shrew</article-title>. <source>Current Biology</source>, <year>2022</year>. ISSN <issn>0960-9822</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2022.06.009</pub-id>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Paul S</given-names> <surname>Scotti</surname></string-name>, <string-name><given-names>Atmadeep</given-names> <surname>Banerjee</surname></string-name>, <string-name><given-names>Jimmie</given-names> <surname>Goode</surname></string-name>, <string-name><given-names>Stepan</given-names> <surname>Shabalin</surname></string-name>, <string-name><given-names>Alex</given-names> <surname>Nguyen</surname></string-name>, <string-name><given-names>Ethan</given-names> <surname>Cohen</surname></string-name>, <string-name><given-names>Aidan J</given-names> <surname>Dempster</surname></string-name>, <string-name><given-names>Nathalie</given-names> <surname>Verlinde</surname></string-name>, <string-name><given-names>Elad</given-names> <surname>Yundler</surname></string-name>, <string-name><given-names>David</given-names> <surname>Weisberg</surname></string-name>, <string-name><given-names>Kenneth A</given-names> <surname>Norman</surname></string-name>, and <string-name><given-names>Tanishq Mathew</given-names> <surname>Abraham</surname></string-name></person-group>. <article-title>Reconstructing the Mind’s Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors</article-title>. <source>arXiv</source>, <year>2023</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.2305.18274</pub-id>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Guohua</given-names> <surname>Shen</surname></string-name>, <string-name><given-names>Kshitij</given-names> <surname>Dwivedi</surname></string-name>, <string-name><given-names>Kei</given-names> <surname>Majima</surname></string-name>, <string-name><given-names>Tomoyasu</given-names> <surname>Horikawa</surname></string-name>, and <string-name><given-names>Yukiyasu</given-names> <surname>Kamitani</surname></string-name></person-group>. <article-title>End-to-End Deep Image Reconstruction From Human Brain Activity</article-title>. <source>Frontiers in Computational Neuroscience</source>, <volume>13</volume>:<fpage>21</fpage>, <year>2019a</year>. ISSN <issn>1662-5188</issn>. doi: <pub-id pub-id-type="doi">10.3389/fncom.2019.00021</pub-id>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Guohua</given-names> <surname>Shen</surname></string-name>, <string-name><given-names>Tomoyasu</given-names> <surname>Horikawa</surname></string-name>, <string-name><given-names>Kei</given-names> <surname>Majima</surname></string-name>, and <string-name><given-names>Yukiyasu</given-names> <surname>Kamitani</surname></string-name></person-group>. <article-title>Deep image reconstruction from human brain activity</article-title>. <source>PLoS Computational Biology</source>, <volume>15</volume>(<issue>1</issue>):<fpage>e1006633</fpage>, <year>2019b</year>. ISSN <issn>1553-734X</issn>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006633</pub-id>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Fabian</given-names> <surname>Sinz</surname></string-name>, <string-name><given-names>Alexander S</given-names> <surname>Ecker</surname></string-name>, <string-name><given-names>Paul</given-names> <surname>Fahey</surname></string-name>, <string-name><given-names>Edgar</given-names> <surname>Walker</surname></string-name>, <string-name><given-names>Erick</given-names> <surname>Cobos</surname></string-name>, <string-name><given-names>Emmanouil</given-names> <surname>Froudarakis</surname></string-name>, <string-name><given-names>Dimitri</given-names> <surname>Yatsenko</surname></string-name>, <string-name><given-names>Zachary</given-names> <surname>Pitkow</surname></string-name>, <string-name><given-names>Jacob</given-names> <surname>Reimer</surname></string-name>, and <string-name><given-names>Andreas</given-names> <surname>Tolias</surname></string-name></person-group>. <article-title>Stimulus domain transfer in recurrent models for large scale cortical population prediction on video</article-title>. <source>Advances in neural information processing systems</source>, <volume>31</volume>, <year>2018</year>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Garrett B</given-names> <surname>Stanley</surname></string-name>, <string-name><given-names>Fei F</given-names> <surname>Li</surname></string-name>, and <string-name><given-names>Yang</given-names> <surname>Dan</surname></string-name></person-group>. <article-title>Reconstruction of Natural Scenes from Ensemble Responses in the Lateral Geniculate Nucleus</article-title>. <source>The Journal of Neuroscience</source>, <volume>19</volume>(<issue>18</issue>):<fpage>8036</fpage>–<lpage>8042</lpage>, <year>1999</year>. ISSN <issn>0270-6474</issn>. doi: <pub-id pub-id-type="doi">10.1523/jneurosci.19-18-08036.1999</pub-id>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Andrea</given-names> <surname>Tacchetti</surname></string-name>, <string-name><given-names>Leyla</given-names> <surname>Isik</surname></string-name>, and <string-name><given-names>Tomaso A.</given-names> <surname>Poggio</surname></string-name></person-group>. <article-title>Invariant Recognition Shapes Neural Representations of Visual Input</article-title>. <source>Annual Review of Vision Science</source>, <volume>4</volume>(<issue>1</issue>):<fpage>403</fpage>–<lpage>422</lpage>, <year>2018</year>. ISSN <issn>2374-4642</issn>. doi: <pub-id pub-id-type="doi">10.1146/annurev-vision-091517-034103</pub-id>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Yu</given-names> <surname>Takagi</surname></string-name> and <string-name><given-names>Shinji</given-names> <surname>Nishimoto</surname></string-name></person-group>. <article-title>High-resolution image reconstruction with latent diffusion models from human brain activity</article-title>. In <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>, pages <fpage>14453</fpage>–<lpage>14463</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Bertrand</given-names> <surname>Thirion</surname></string-name>, <string-name><given-names>Edouard</given-names> <surname>Duchesnay</surname></string-name>, <string-name><given-names>Edward</given-names> <surname>Hubbard</surname></string-name>, <string-name><given-names>Jessica</given-names> <surname>Dubois</surname></string-name>, <string-name><given-names>Jean-Baptiste</given-names> <surname>Poline</surname></string-name>, <string-name><given-names>Denis</given-names> <surname>Lebihan</surname></string-name>, and <string-name><given-names>Stanislas</given-names> <surname>Dehaene</surname></string-name></person-group>. <article-title>Inverse retinotopy: Inferring the visual content of images from brain activation patterns</article-title>. <source>NeuroImage</source>, <volume>33</volume>(<issue>4</issue>):<fpage>1104</fpage>–<lpage>1116</lpage>, <year>2006</year>. ISSN <issn>1053-8119</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.06.062</pub-id>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Polina</given-names> <surname>Turishcheva</surname></string-name>, <string-name><given-names>Paul G</given-names> <surname>Fahey</surname></string-name>, <string-name><given-names>Laura</given-names> <surname>Hansel</surname></string-name>, <string-name><given-names>Rachel</given-names> <surname>Froebe</surname></string-name>, <string-name><given-names>Kayla</given-names> <surname>Ponder</surname></string-name>, <string-name><given-names>Michaela</given-names> <surname>Vystrčilová</surname></string-name>, <string-name><given-names>Konstantin F</given-names> <surname>Willeke</surname></string-name>, <string-name><given-names>Mohammad</given-names> <surname>Bashiri</surname></string-name>, <string-name><given-names>Eric</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Zhiwei</given-names> <surname>Ding</surname></string-name>, <string-name><given-names>Andreas S</given-names> <surname>Tolias</surname></string-name>, <string-name><given-names>Fabian H</given-names> <surname>Sinz</surname></string-name>, and <string-name><given-names>Alexander S</given-names> <surname>Ecker</surname></string-name></person-group>. <article-title>The Dynamic Sensorium competition for predicting large-scale mouse visual cortex activity from videos</article-title>. <source>arXiv</source>, <year>2023</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.2305.19654</pub-id>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Polina</given-names> <surname>Turishcheva</surname></string-name>, <string-name><given-names>Paul</given-names> <surname>Fahey</surname></string-name>, <string-name><given-names>Michaela</given-names> <surname>Vystrčilová</surname></string-name>, <string-name><given-names>Laura</given-names> <surname>Hansel</surname></string-name>, <string-name><given-names>Rachel</given-names> <surname>Froebe</surname></string-name>, <string-name><given-names>Kayla</given-names> <surname>Ponder</surname></string-name>, <string-name><given-names>Yongrong</given-names> <surname>Qiu</surname></string-name>, <string-name><given-names>Konstantin</given-names> <surname>Willeke</surname></string-name>, <string-name><given-names>Mohammad</given-names> <surname>Bashiri</surname></string-name>, <string-name><given-names>Ruslan</given-names> <surname>Baikulov</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Retrospective for the dynamic sensorium competition for predicting large-scale mouse primary visual cortex activity from videos</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>37</volume>:<fpage>118907</fpage>–<lpage>118929</lpage>, <year>2024</year>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Ashish</given-names> <surname>Vaswani</surname></string-name>, <string-name><given-names>Noam</given-names> <surname>Shazeer</surname></string-name>, <string-name><given-names>Niki</given-names> <surname>Parmar</surname></string-name>, <string-name><given-names>Jakob</given-names> <surname>Uszkoreit</surname></string-name>, <string-name><given-names>Llion</given-names> <surname>Jones</surname></string-name>, <string-name><given-names>Aidan N</given-names> <surname>Gomez</surname></string-name>, <string-name><given-names>Lukasz</given-names> <surname>Kaiser</surname></string-name>, and <string-name><given-names>Illia</given-names> <surname>Polosukhin</surname></string-name></person-group>. <article-title>Attention Is All You Need</article-title>. <source>arXiv</source>, <year>2017</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.1706.03762</pub-id>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Edgar Y.</given-names> <surname>Walker</surname></string-name>, <string-name><given-names>Fabian H.</given-names> <surname>Sinz</surname></string-name>, <string-name><given-names>Erick</given-names> <surname>Cobos</surname></string-name>, <string-name><given-names>Taliah</given-names> <surname>Muhammad</surname></string-name>, <string-name><given-names>Emmanouil</given-names> <surname>Froudarakis</surname></string-name>, <string-name><given-names>Paul G.</given-names> <surname>Fahey</surname></string-name>, <string-name><given-names>Alexander S.</given-names> <surname>Ecker</surname></string-name>, <string-name><given-names>Jacob</given-names> <surname>Reimer</surname></string-name>, <string-name><given-names>Xaq</given-names> <surname>Pitkow</surname></string-name>, and <string-name><given-names>Andreas S.</given-names> <surname>Tolias</surname></string-name></person-group>. <article-title>Inception loops discover what excites neurons most using deep predictive models</article-title>. <source>Nature Neuroscience</source>, <volume>22</volume>(<issue>12</issue>):<fpage>2060</fpage>–<lpage>2065</lpage>, <year>2019</year>. ISSN <issn>1097-6256</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41593-019-0517-x</pub-id>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Eric Y</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Paul G</given-names> <surname>Fahey</surname></string-name>, <string-name><given-names>Zhuokun</given-names> <surname>Ding</surname></string-name>, <string-name><given-names>Stelios</given-names> <surname>Papadopoulos</surname></string-name>, <string-name><given-names>Kayla</given-names> <surname>Ponder</surname></string-name>, <string-name><given-names>Marissa A</given-names> <surname>Weis</surname></string-name>, <string-name><given-names>Andersen</given-names> <surname>Chang</surname></string-name>, <string-name><given-names>Taliah</given-names> <surname>Muhammad</surname></string-name>, <string-name><given-names>Saumil</given-names> <surname>Patel</surname></string-name>, <string-name><given-names>Zhiwei</given-names> <surname>Ding</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Foundation model of neural activity predicts response to new stimulus types</article-title>. <source>Nature</source>, <volume>640</volume>(<issue>8058</issue>):<fpage>470</fpage>–<lpage>477</lpage>, <year>2025</year>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Konstantin F</given-names> <surname>Willeke</surname></string-name>, <string-name><given-names>Paul G</given-names> <surname>Fahey</surname></string-name>, <string-name><given-names>Mohammad</given-names> <surname>Bashiri</surname></string-name>, <string-name><given-names>Laura</given-names> <surname>Hansel</surname></string-name>, <string-name><given-names>Christoph</given-names> <surname>Blessing</surname></string-name>, <string-name><given-names>Konstantin-Klemens</given-names> <surname>Lurz</surname></string-name>, <string-name><given-names>Max F</given-names> <surname>Burg</surname></string-name>, <string-name><given-names>Santiago A</given-names> <surname>Cadena</surname></string-name>, <string-name><given-names>Zhiwei</given-names> <surname>Ding</surname></string-name>, <string-name><given-names>Kayla</given-names> <surname>Ponder</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Retrospective on the sensorium 2022 competition</article-title>. In <conf-name>NeurIPS 2022 Competition Track</conf-name>, pages <fpage>314</fpage>–<lpage>333</lpage>. <publisher-name>PMLR</publisher-name>, <year>2023a</year>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Konstantin F.</given-names> <surname>Willeke</surname></string-name>, <string-name><given-names>Kelli</given-names> <surname>Restivo</surname></string-name>, <string-name><given-names>Katrin</given-names> <surname>Franke</surname></string-name>, <string-name><given-names>Arne F.</given-names> <surname>Nix</surname></string-name>, <string-name><given-names>Santiago A.</given-names> <surname>Cadena</surname></string-name>, <string-name><given-names>Tori</given-names> <surname>Shinn</surname></string-name>, <string-name><given-names>Cate</given-names> <surname>Nealley</surname></string-name>, <string-name><given-names>Gabrielle</given-names> <surname>Rodriguez</surname></string-name>, <string-name><given-names>Saumil</given-names> <surname>Patel</surname></string-name>, <string-name><given-names>Alexander S.</given-names> <surname>Ecker</surname></string-name>, <string-name><given-names>Fabian H.</given-names> <surname>Sinz</surname></string-name>, and <string-name><given-names>Andreas S.</given-names> <surname>Tolias</surname></string-name></person-group>. <article-title>Deep learningdriven characterization of single cell tuning in primate visual area V4 unveils topological organization</article-title>. <source>bioRxiv</source>, page 2023.05.12.540591, <year>2023b</year>. doi: <pub-id pub-id-type="doi">10.1101/2023.05.12.540591</pub-id>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ji</given-names> <surname>Xia</surname></string-name>, <string-name><given-names>Tyler D.</given-names> <surname>Marks</surname></string-name>, <string-name><given-names>Michael J.</given-names> <surname>Goard</surname></string-name>, and <string-name><given-names>Ralf</given-names> <surname>Wessel</surname></string-name></person-group>. <article-title>Stable representation of a naturalistic movie emerges from episodic activity with gain variability</article-title>. <source>Nature Communications</source>, <volume>12</volume>(<issue>1</issue>):<fpage>5170</fpage>, <year>2021</year>. doi: <pub-id pub-id-type="doi">10.1038/s41467-021-25437-2</pub-id>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Takashi</given-names> <surname>Yoshida</surname></string-name> and <string-name><given-names>Kenichi</given-names> <surname>Ohki</surname></string-name></person-group>. <article-title>Natural images are reliably represented by sparse and variable populations of neurons in visual cortex</article-title>. <source>Nature Communications</source>, <volume>11</volume>(<issue>1</issue>):<fpage>872</fpage>, <year>2020</year>. doi: <pub-id pub-id-type="doi">10.1038/s41467-020-14645-x</pub-id>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yichen</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Shanshan</given-names> <surname>Jia</surname></string-name>, <string-name><given-names>Yajing</given-names> <surname>Zheng</surname></string-name>, <string-name><given-names>Zhaofei</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>Yonghong</given-names> <surname>Tian</surname></string-name>, <string-name><given-names>Siwei</given-names> <surname>Ma</surname></string-name>, <string-name><given-names>Tiejun</given-names> <surname>Huang</surname></string-name>, and <string-name><given-names>Jian K.</given-names> <surname>Liu</surname></string-name></person-group>. <article-title>Reconstruction of natural visual scenes from neural spikes with deep neural networks</article-title>. <source>Neural Networks</source>, <volume>125</volume>:<fpage>19</fpage>–<lpage>30</lpage>, <year>2020</year>. ISSN <issn>0893-6080</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neunet.2020.01.033</pub-id>.</mixed-citation></ref>
<ref id="dataref1"><mixed-citation publication-type="data" specific-use="analyzed"><person-group person-group-type="author"><string-name><surname>Fahey</surname> <given-names>P</given-names></string-name>, <string-name><surname>Turishcheva</surname> <given-names>P</given-names></string-name>, <string-name><surname>Hansel</surname> <given-names>L</given-names></string-name>, <string-name><surname>Froebe</surname> <given-names>R</given-names></string-name>, <string-name><surname>Ponder</surname> <given-names>K</given-names></string-name>, <string-name><surname>Vystrcilová</surname> <given-names>M</given-names></string-name>, <string-name><surname>Qiu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Willeke</surname> <given-names>K</given-names></string-name>, <string-name><surname>Bashiri</surname> <given-names>M</given-names></string-name>, <string-name><surname>Tolias</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sinz</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ecker</surname> <given-names>A</given-names></string-name></person-group> (<year iso-8601-date="2023">2023</year>) <article-title>The Dynamic Sensorium competition for predicting large-scale mouse visual cortex activity from videos - Dataset</article-title>. <source>G-Node Gin</source>. <pub-id pub-id-type="accession" xlink:href="https://gin.g-node.org/pollytur/Sensorium2023Data">Sensorium2023Data</pub-id></mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1">
<title>Supplemental material</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Supplementary Figure S1:</label>
<caption><title>Summary ethogram of SOTA DNEM inputs, output predictions, and video reconstruction over time for three videos from three mice (same as <xref ref-type="fig" rid="fig2">Figure 2A</xref>).</title>
<p>A) Top: motion energy of the input video. Bottom: pupil diameter and running speed of the mouse during the video. B) Ground truth neuronal activity. C) Predicted neuronal activity in response to input video and behavioural parameters. D) predicted neuronal activity given reconstructed video and ground truth behaviour as input. E) Frame by frame correlation between reconstructed and ground truth video.</p></caption>
<graphic xlink:href="599691v4_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Supplementary Figure S2:</label>
<caption><title>Receptive fields and transparency masks.</title>
<p>A) One example receptive field for one neuron from each mouse mapped using on &amp; off patch stimuli <italic>in silico</italic>. B) Average population receptive fields from each mouse. C) Distribution of on and off receptive field centers for each moue. D) Unthresholded alpha masks, i.e. transparency masks, for each mouse. E) Pixel-wise temporal correlation between ground truth and reconstructed videos with either the training or the evaluation mask applied. Dashed lines in C-E indicate retinotopic eccentricity in steps of 10°. Plot limits correspond to screen size.</p></caption>
<graphic xlink:href="599691v4_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Supplementary Figure S3:</label>
<caption><title>Variations on reconstruction method.</title>
<p>A) Example movie frames (from mouse 4 trial 1) using variations of the reconstruction method. B) Same as A but using different training mask thresholds. No evaluation mask is applied here. C) Left: reconstruction performance for different reconstruction versions. Version without contrast and luminance adjustment is not included because video correlation is always calculated before contrast and luminance adjustment. Reconstruction from predicted vs standard method (34.7% increase; paired t-test p = 1.5 * 10<sup><italic>−</italic></sup>5 n = 5 mice), standard method vs gradient ensembling (3.00% decrease; paired t-test p = 0.0198, n = 5 mice), standard method vs no Gaussian smoothing (6.28% decrease; paired t-test p = 6.26 * 10<sup><italic>−</italic>6</sup>, n = 5 mice). Middle: reconstruction performance with different evaluation mask thresholds compared across the three training mask thresholds shown in B. Right: same as middle but plotting the mask diameter for each evaluation mask threshold. D) Neural activity prediction performance for different movie inputs. Left: Poisson loss, used to train the DNEM and movie reconstruction. Predicted activity from full video vs masked with alpha = 0.5 (5.02% increase, t-test p = 3.71*10<sup><italic>−</italic></sup>6, n = 5 mice), reconstruction vs reconstruction after contrast &amp; luminance matching to ground truth video (0.227% increase, paired t-test p = 0.776, n = 5 mice). Right: correlation across all neurons and frames (note this is a different metric to the one used in the Sensorium competition). Predicted activity from full video vs masked with alpha = 0.5 (3.73% increase, t-test p = 2.49 * 10<sup><italic>−</italic></sup>5, n = 5 mice), reconstruction vs reconstruction after contrast &amp; luminance matching to ground truth video (2.49% increase, paired t-test p = 0.0195, n = 5 mice). In C-D dashed lines are single mice, solid lines are means across mice.</p></caption>
<graphic xlink:href="599691v4_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Supplementary Figure S4:</label>
<caption><title>Reconstruction performance correlates with frame contrast but not with behavioral parameters.</title>
<p>A) Pearson’s correlation between mean frame correlation per movie, and 3 movie parameters and 3 behavioral parameters. Linear fit as black line. B) Left: Pearson’s correlation between activity prediction accuracy and movie reconstruction accuracy. Right: cross-correlation plot of frame-by-frame activity prediction accuracy and video frame correlation. In other words, the more predictable the neural activity, the better the reconstruction performance.</p></caption>
<graphic xlink:href="599691v4_figS4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Supplementary Figure S5:</label>
<caption><title>Gaussian Noise reconstruction.</title>
<p>A) Average frame Shannon entropy (a measure of variance in the spatial domain) across Gaussian noise stimuli with various spatial and temporal Gaussian length constants. Left: ground truth stimuli. Right: reconstructions from predicted activity. B) Same as A but for motion energy (a measure of variance in the temporal domain). C) Ensembling effect for each stimulus. Video correlation for ensembled prediction (average videos from 7 model instances) minus the mean video correlation across the 7 individual model instances.</p></caption>
<graphic xlink:href="599691v4_figS5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Supplementary Figure S6:</label>
<caption><title>Reconstruction of drifting grating stimuli with different spatial and temporal frequencies using predicted activity.</title>
<p>A) Example drifting grating stimuli (rightwards moving) masked with the evaluation mask for one mouse. Shown is the 31st frame of a 2 second video. B) Reconstructed drifting grating stimuli with SOTA DNEM predicted neuronal activity as the target (see also <xref ref-type="supplementary-material" rid="video3">Supplementary Video 3</xref>). C) Pearson’s correlation between ground truth (A) and reconstructed videos (B) across the range of spatial and temporal frequencies. For each stimulus type the average correlation across 4 directions (up, down, left, right) reconstructed from the SOTA DNEM of 1 mouse is given. Interestingly, video correlation at 15 cycles/second (half the video frame rate of 30 Hz) is much higher than 7.5 cycles/second. This is an artifact of using predicted responses rather than true neural responses. The DNEM input layer convolution has dilation 2. The predicted activity is therefore based on every second frame, with the effect that the activity is predicted as the response of 2 static images which are then interleaved.</p></caption>
<graphic xlink:href="599691v4_figS6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<statement id="alg1">
<label>Algorithm 1:</label>
<p>Movie reconstruction</p>
<p><fig id="alg1a" position="float" fig-type="figure">
<graphic xlink:href="599691v4_alg1.tif" mimetype="image" mime-subtype="tiff"/>
</fig></p>
</statement>
<statement id="alg2">
<label>Algorithm 2:</label>
<p>Mask training</p>
<p><fig id="alg2a" position="float" fig-type="figure">
<graphic xlink:href="599691v4_alg2.tif" mimetype="image" mime-subtype="tiff"/>
</fig></p>
</statement>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105081.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Denison</surname>
<given-names>Rachel</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/05qwgg493</institution-id><institution>Boston University</institution>
</institution-wrap>
<city>Boston</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study uses state-of-the-art neural encoding and video reconstruction methods to achieve a substantial improvement in video reconstruction quality from mouse neural data. It provides a <bold>convincing</bold> demonstration of how reconstruction performance can be improved by combining these methods. The goal of the study was improving reconstruction performance rather than advancing theoretical understanding of neural processing, so the results will be of practical interest to the brain decoding community.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105081.2.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This is an interesting study exploring methods for reconstructing visual stimuli from neural activity in the mouse visual cortex. Specifically, it uses a competition dataset (published in the Dynamic Sensorium benchmark study) and a recent winning model architecture (DNEM, dynamic neural encoding model) to recover visual information stored in ensembles of mouse visual cortex.</p>
<p>Strengths:</p>
<p>This is a great start for a project addressing visual reconstruction. It is based on physiological data obtained at a single-cell resolution, the stimulus movies were reasonably naturalistic and representative of the real world, the study did not ignore important correlates such as eye position and pupil diameter, and of course, the reconstruction quality exceeded anything achieved by previous studies. There appear to be no major technical flaws in the study, and some potential confounds were addressed upon revision. The study is an enjoyable read.</p>
<p>Weaknesses:</p>
<p>The study is technically competent and benchmark-focused, but without significant conceptual or theoretical advances. The inclusion of neuronal data broadens the study's appeal, but the work does not explore potential principles of neural coding, which limits its relevance for neuroscience and may create some disappointment to some neuroscientists. The authors are transparent that their goal was methodological rather than explanatory, but this raises the question of why neuronal data were necessary at all, as more significant reconstruction improvements might be achievable using noise-less artificial video encoders alone (network-to-network decoding approaches have been done well by teams such as Han, Poggio, and Cheung, 2023, ICML). Yet, even within the methodological domain, the study does not articulate clear principles or heuristics that could guide future progress. The finding that more neurons improve reconstruction aligns with well-established results in the literature that show that higher neuronal numbers improve decoding in general (for example, Hung, Kreiman, Poggio, and DiCarlo, 2005) and thus may not constitute a novel insight.</p>
<p>Specific issues:</p>
<p>(1) The study showed that it could achieve high-quality video reconstructions from mouse visual cortex activity using a neural encoding model (DNEM), recovering 10-second video sequences and approaching a two-fold improvement in pixel-by-pixel correlation over attempts. As a reader, I was left with the question: okay, does this mean that we should all switch to DNEM for our investigations of mouse visual cortex? What makes this encoding model special? It is introduced as &quot;a winning model of the Sensorium 2023 competition which achieved a score of 0.301...single trial correlation between predicted and ground truth neuronal activity,&quot; but as someone who does not follow this competition (most eLife readers are not likely to do so, either), I do not know how to gauge my response. Is this impressive? What is the best theoretical score, given noise and other limitations? Is the model inspired by the mouse brain in terms of mechanisms or architecture, or was it optimized to win the competition by overfitting it to the nuances of the data set? Of course, I know that as a reader, I am invited to read the references, but the study would stand better on its own, if it clarified how its findings depended on this model.</p>
<p>The revision helpfully added context to the Methods about the range of scores achieved by other models, but this information remains absent from the Abstract and other important sections. For instance, the Abstract states, &quot;We achieve a pixel-level correlation of 0.57 between the ground truth movie and the reconstructions from single-trial neural responses,&quot; yet this point estimate (presented without confidence intervals or comparisons to controls) lacks meaning for readers who are not told how it compares to prior work or what level of performance would be considered strong. Without such context, the manuscript undercuts potentially meaningful achievements.</p>
<p>(2) Along those lines, the authors conclude that &quot;the number of neurons in the dataset and the use of model ensembling are critical for high-quality reconstructions.&quot; If true, these principles should generalize across network architectures. I wondered whether the same dependencies would hold for other network types, as this could reveal more general insights. The authors replied that such extensions are expected (since prior work has shown similar effects for static images) but argued that testing this explicitly would require &quot;substantial additional work,&quot; be &quot;impractical,&quot; and likely not produce &quot;surprising results.&quot; While practical difficulty alone is not a sufficient reason to leave an idea untested, I agree that the idea that &quot;more neurons would help&quot; would be unsurprising. The question then becomes: given that this is a conclusion already in the field, what new principle or understanding has been gained in this study?</p>
<p>(3) One major claim was that the quality of the reconstructions depended on the number of neurons in the dataset. There were approximately 8000 neurons recorded per mouse. The correlation difference between the reconstruction achieved by 1000 neurons and 8000 neurons was ~0.2. Is that a lot or a little? One might hypothesize that 7000 additional neurons could contribute more information, but perhaps, those neurons were redundant if their receptive fields are too close together or if they had the same orientation or spatiotemporal tuning. How correlated were these neurons in response to a given movie? Why did so many neurons offer such a limited increase in correlation? Originally, this question was meant to prompt deeper analysis of the neural data, but the authors did not engage with it, suggesting a limited understanding of the neuronal aspects of the dataset.</p>
<p>(4) We appreciated the experiments testing the capacity of the reconstruction process, by using synthetic stimuli created under a Gaussian process in a noise-free way. But this originally further raised questions: what is the theoretical capability for reconstruction of this processing pipeline, as a whole? Is 0.563 the best that one could achieve given the noisiness and/or neuron count of the Sensorium project? What if the team applied the pipeline to reconstruct the activity of a given artificial neural network's layer (e.g., some ResNet convolutional layer), using hidden units as proxies for neuronal calcium activity? In the revision, this concern was addressed nicely in the review in Supplementary Figure 3C. Also, one appreciates that as a follow up, the team produced error maps (New Figure 6) that highlight where in the frames the reconstruction are likely to fail. But the maps went unanalyzed further, and I am not sure if there was a systematic trend in the errors.</p>
<p>(5) I was encouraged by Figure 4, which shows how the reconstructions succeeded or failed across different spatial frequencies. The authors note that &quot;the reconstruction process failed at high spatial frequencies,&quot; yet it also appears to struggle with low spatial frequencies, as the reconstructed images did not produce smooth surfaces (e.g., see the top rows of Figures 4A and 4B). In regions where one would expect a single continuous gradient, the reconstructions instead display specular, high-frequency noise. This issue is difficult to overlook and might deserve further discussion.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105081.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper presents a method for reconstructing input videos shown to a mouse from the simultaneously recorded visual cortex activity (two-photon calcium imaging data). The publicly available experimental dataset is taken from a recent brain-encoding challenge, and the (publicly available) neural network model that serves to reconstruct the videos is the winning model from that challenge (by distinct authors). The present study applies gradient-based input optimization by backpropagating the brain-encoding error through this selected model (a method that has been proposed in the past, with other datasets). The main contribution of the paper is, therefore, the choice of applying this existing method to this specific dataset with this specific neural network model. The quantitative results appear to go beyond previous attempts at video input reconstruction (although measured with distinct datasets). The conclusions have potential practical interest for the field of brain decoding, and theoretical interest for possible future uses in functional brain exploration.</p>
<p>Strengths:</p>
<p>The authors use a validated optimization method on a recent large-scale dataset, with a state-of-the-art brain encoding model. The use of an ensemble of 7 distinct model instances (trained on distinct subsets of the dataset, with distinct random initializations) significantly improves the reconstructions. The exploration of the relation between reconstruction quality and number of recorded neurons will be useful to those planning future experiments.</p>
<p>Weaknesses:</p>
<p>The main contribution is methodological, and the methodology combines pre-existing components without any new original component.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105081.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bauer</surname>
<given-names>Joel</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5858-166X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Margrie</surname>
<given-names>Troy W</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5526-4578</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Clopath</surname>
<given-names>Claudia</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4507-8648</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the current reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Summary:</p>
<p>This is an interesting study exploring methods for reconstructing visual stimuli from neural activity in the mouse visual cortex. Specifically, it uses a competition dataset (published in the Dynamic Sensorium benchmark study) and a recent winning model architecture (DNEM, dynamic neural encoding model) to recover visual information stored in ensembles of mouse visual cortex.</p>
<p>Strengths:</p>
<p>This is a great start for a project addressing visual reconstruction. It is based on physiological data obtained at a single-cell resolution, the stimulus movies were reasonably naturalistic and representative of the real world, the study did not ignore important correlates such as eye position and pupil diameter, and of course, the reconstruction quality exceeded anything achieved by previous studies. There appear to be no major technical flaws in the study, and some potential confounds were addressed upon revision. The study is an enjoyable read.</p>
<p>Weaknesses:</p>
<p>The study is technically competent and benchmark-focused, but without significant conceptual or theoretical advances. The inclusion of neuronal data broadens the study's appeal, but the work does not explore potential principles of neural coding, which limits its relevance for neuroscience and may create some disappointment to some neuroscientists. The authors are transparent that their goal was methodological rather than explanatory, but this raises the question of why neuronal data were necessary at all, as more significant reconstruction improvements might be achievable using noise-less artificial video encoders alone (network-to-network decoding approaches have been done well by teams such as Han, Poggio, and Cheung, 2023, ICML). Yet, even within the methodological domain, the study does not articulate clear principles or heuristics that could guide future progress. The finding that more neurons improve reconstruction aligns with well-established results in the literature that show that higher neuronal numbers improve decoding in general (for example, Hung, Kreiman, Poggio, and DiCarlo, 2005) and thus may not constitute a novel insight.</p>
</disp-quote>
<p>We thank the reviewer for this second round of comments and hope we were able to address the remaining points below.</p>
<p>Indeed, using surrogate noiseless data is interesting and useful when developing such methods, or to demonstrate that they work in principle. But in order to evaluate if they really work in practice, we need to use real neuronal data. While we did not try movie reconstruction from layers within artificial neural networks as surrogate data, in Supplementary Figure 3C we provide the performance of our method using simulated/predicted neuronal responses from the dynamic neural encoding model alongside real neuronal responses.</p>
<disp-quote content-type="editor-comment">
<p>Specific issues:</p>
<p>(1)The study showed that it could achieve high-quality video reconstructions from mouse visual cortex activity using a neural encoding model (DNEM), recovering 10-second video sequences and approaching a two-fold improvement in pixel-by-pixel correlation over attempts. As a reader, I was left with the question: okay, does this mean that we should all switch to DNEM for our investigations of mouse visual cortex? What makes this encoding model special? It is introduced as &quot;a winning model of the Sensorium 2023 competition which achieved a score of 0.301...single trial correlation between predicted and ground truth neuronal activity,&quot; but as someone who does not follow this competition (most eLife readers are not likely to do so, either), I do not know how to gauge my response. Is this impressive? What is the best theoretical score, given noise and other limitations? Is the model inspired by the mouse brain in terms of mechanisms or architecture, or was it optimized to win the competition by overfitting it to the nuances of the data set? Of course, I know that as a reader, I am invited to read the references, but the study would stand better on its own, if it clarified how its findings depended on this model.</p>
<p>The revision helpfully added context to the Methods about the range of scores achieved by other models, but this information remains absent from the Abstract and other important sections. For instance, the Abstract states, &quot;We achieve a pixel-level correlation of 0.57 between the ground truth movie and the reconstructions from single-trial neural responses,&quot; yet this point estimate (presented without confidence intervals or comparisons to controls) lacks meaning for readers who are not told how it compares to prior work or what level of performance would be considered strong. Without such context, the manuscript undercuts potentially meaningful achievements.</p>
</disp-quote>
<p>We appreciate that the additional information about the performance of the SOTA DNEM to predict neural responses could be made more visible in the paper and will therefore move it from the methods to the results section instead:</p>
<p>Line 348 “This model achieved an average single-trial correlation between predicted and ground truth neural activity of 0.291 during the competition, this was later improved to 0.301. The competition benchmark models achieved 0.106, 0.164 and 0.197 single-trial correlation, while the third and second place models achieved 0.243 and 0.265. Across the models, a variety of architectural components were used, including 2D and 3D convolutional layers, recurrent layers, and transformers, to name just a few.” will be moved to the results.</p>
<p>With regard to the lack of context for the performance of our reconstruction in the abstract, we may have overcorrected in the previous revision round and have tried to find a compromise which gives more context to the pixel-level correlation value:</p>
<p>Abstract: “We achieve a pixel-level correlation of 0.57 (95% CI [0.54, 0.60]) between ground-truth movies and single-trial reconstructions. Previous reconstructions based on awake mouse V1 neuronal responses to static images achieved a pixel-level correlation of 0.238 over a similar retinotopic area.”</p>
<disp-quote content-type="editor-comment">
<p>(2) Along those lines, the authors conclude that &quot;the number of neurons in the dataset and the use of model ensembling are critical for high-quality reconstructions.&quot; If true, these principles should generalize across network architectures. I wondered whether the same dependencies would hold for other network types, as this could reveal more general insights. The authors replied that such extensions are expected (since prior work has shown similar effects for static images) but argued that testing this explicitly would require &quot;substantial additional work,&quot; be &quot;impractical,&quot; and likely not produce &quot;surprising results.&quot; While practical difficulty alone is not a sufficient reason to leave an idea untested, I agree that the idea that &quot;more neurons would help&quot; would be unsurprising. The question then becomes: given that this is a conclusion already in the field, what new principle or understanding has been gained in this study?</p>
</disp-quote>
<p>As mentioned in our previous round of revisions, we chose not to pursue the comparison of reconstructions using different model architectures in this manuscript because we did not think it would add significant insights to the paper given the amount of work it would require, and we are glad the reviewer agrees.</p>
<p>While the fact that more neurons result in better reconstructions is unsurprising, how quickly performance drops off will depend on the robustness of the method, and on the dimensionality of the decoding/reconstruction task (decoding grating orientation likely requires fewer neurons than gray scale image reconstruction, which in turn likely requires fewer neurons than full color movie reconstruction). How dependent input optimization based image/movie reconstruction is on population size has not been shown, so we felt it was useful for readers to know how well movie reconstruction works with our method when recording from smaller numbers of neurons.</p>
<disp-quote content-type="editor-comment">
<p>(3) One major claim was that the quality of the reconstructions depended on the number of neurons in the dataset. There were approximately 8000 neurons recorded per mouse. The correlation difference between the reconstruction achieved by 1000 neurons and 8000 neurons was ~0.2. Is that a lot or a little? One might hypothesize that 7000 additional neurons could contribute more information, but perhaps, those neurons were redundant if their receptive fields are too close together or if they had the same orientation or spatiotemporal tuning. How correlated were these neurons in response to a given movie? Why did so many neurons offer such a limited increase in correlation? Originally, this question was meant to prompt deeper analysis of the neural data, but the authors did not engage with it, suggesting a limited understanding of the neuronal aspects of the dataset.</p>
</disp-quote>
<p>We apologize that we did not engage with this comment enough in the previous round. We assumed that the question arose because there was a misunderstanding about figure 5: 1000 not 1 neuron is sufficient to reconstruct the movies to a pixel-level correlation of 0.344. Of course, the fact that increasing the number of neurons from 1000 to 8000 only increased the reconstruction performance from 0.344 to 0.569 (65% increase in correlation) is still worth discussing. To illustrate this drop in performance qualitatively, we show 3 example frames from movie reconstructions using 1000-8000 neurons in Author response image 1.</p>
<fig id="sa3fig1">
<label>Author response image 1.</label>
<caption>
<title>3 example frames from reconstructions using different numbers of neurons.</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-105081-sa3-fig1.jpg" mimetype="image"/>
</fig>
<p>As the reviewer points out, the diminishing returns of additional neurons to reconstruction performance is at least partly because there is redundancy in how a population of neurons represents visual stimuli. In supplementary figure S2, we inferred the on-off receptive fields of the neurons and show that visual space is oversampled in terms of the receptive field positions in panel C. However, the exact slope/shape of the performance vs population size curve we show in Figure 5 will also depend on the maximum performance of our reconstruction method, which is limited in spatial resolution (Figure 4 &amp; Supplementary Figure S5). It is possible that future reconstruction approaches will require fewer neurons than ours, so we interpret this curve rather as a description of the reconstruction method itself than a feature of the underlying neuronal code. For that reason, we chose caution and refrained from making any claims about neuronal coding principles based on this plot.</p>
<disp-quote content-type="editor-comment">
<p>(4) We appreciated the experiments testing the capacity of the reconstruction process, by using synthetic stimuli created under a Gaussian process in a noise-free way. But this originally further raised questions: what is the theoretical capability for reconstruction of this processing pipeline, as a whole? Is 0.563 the best that one could achieve given the noisiness and/or neuron count of the Sensorium project? What if the team applied the pipeline to reconstruct the activity of a given artificial neural network's layer (e.g., some ResNet convolutional layer), using hidden units as proxies for neuronal calcium activity? In the revision, this concern was addressed nicely in the review in Supplementary Figure 3C. Also, one appreciates that as a follow up, the team produced error maps (New Figure 6) that highlight where in the frames the reconstruction are likely to fail. But the maps went unanalyzed further, and I am not sure if there was a systematic trend in the errors.</p>
</disp-quote>
<p>We are happy to hear that we were able to answer the reviewers’ question of what the maximum theoretical performance of our reconstruction process is in figure 3C. Regarding systematic trends in the error maps, we also did not observe any clear systematic trends. If anything, we noticed that some moving edges were shifted, but we do not think we can quantify this effect with this particular dataset.</p>
<disp-quote content-type="editor-comment">
<p>(5) I was encouraged by Figure 4, which shows how the reconstructions succeeded or failed across different spatial frequencies. The authors note that &quot;the reconstruction process failed at high spatial frequencies,&quot; yet it also appears to struggle with low spatial frequencies, as the reconstructed images did not produce smooth surfaces (e.g., see the top rows of Figures 4A and 4B). In regions where one would expect a single continuous gradient, the reconstructions instead display specular, high-frequency noise. This issue is difficult to overlook and might deserve further discussion.</p>
</disp-quote>
<p>Thank you for pointing this out, this is indeed true. The reconstructions do have high frequency noise. We mention this briefly in line 102 “Finally, we applied a 3D Gaussian filter with sigma 0.5 pixels to remove the remaining static noise (Figure S3) and applied the evaluation mask.” In revisiting this sentence, we think it is more appropriate to replace “remove” with “reduce”. This noise is more visible in the Gaussian noise stimuli (Figure 4) because we did not apply the 3D Gaussian filter to these reconstructions, in case it interfered with the estimates of the reconstruction resolution limits.</p>
<p>Given that the Gaussian noise and drifting grating stimuli reconstructions were from predicted activity (“noise-free”), this high-frequency noise is not biological in origin and must therefore come from errors in our reconstruction process. This kind of high-frequency noise has previously been observed in feature visualization (optimizing input to maximize the activity of a specific node within a neural network to visualize what that node encodes; Olah, et al., &quot;Feature Visualization&quot;, <ext-link ext-link-type="uri" xlink:href="https://distill.pub/2017/feature-visualization/">https://distill.pub/2017/feature-visualization/</ext-link>, 2017). It is caused by a kind of overfitting, whereby a solution to the optimization is found that is not “realistic”. Ways of combating this kind of noise include gradient smoothing, image smoothing, and image transformations during optimization, but these methods can restrict the resolution of the features that are recovered. Since we were more interested in determining the maximum resolution of stimuli that can be reconstructed in Figure 4 and Supplementary Figures 5-6, we chose not to apply these methods.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public review):</bold></p>
<p>Summary:</p>
<p>This paper presents a method for reconstructing input videos shown to a mouse from the simultaneously recorded visual cortex activity (two-photon calcium imaging data). The publicly available experimental dataset is taken from a recent brain-encoding challenge, and the (publicly available) neural network model that serves to reconstruct the videos is the winning model from that challenge (by distinct authors). The present study applies gradient-based input optimization by backpropagating the brain-encoding error through this selected model (a method that has been proposed in the past, with other datasets). The main contribution of the paper is, therefore, the choice of applying this existing method to this specific dataset with this specific neural network model. The quantitative results appear to go beyond previous attempts at video input reconstruction (although measured with distinct datasets). The conclusions have potential practical interest for the field of brain decoding, and theoretical interest for possible future uses in functional brain exploration.</p>
<p>Strengths:</p>
<p>The authors use a validated optimization method on a recent large-scale dataset, with a state-of-the-art brain encoding model. The use of an ensemble of 7 distinct model instances (trained on distinct subsets of the dataset, with distinct random initializations) significantly improves the reconstructions. The exploration of the relation between reconstruction quality and number of recorded neurons will be useful to those planning future experiments.</p>
<p>Weaknesses:</p>
<p>The main contribution is methodological, and the methodology combines pre-existing components without any new original component.</p>
</disp-quote>
<p>We thank the reviewer for their balanced assessment of our manuscript.</p>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Summary:</p>
<p>This paper presents a method for reconstructing videos from mouse visual cortex neuronal activity using a state-of-the-art dynamic neural encoding model. The authors achieve high-quality reconstructions of 10-second movies at 30 Hz from two-photon calcium imaging data, reporting a 2-fold increase in pixel-by-pixel correlation compared to previous methods. They identify key factors for successful reconstruction including the number of recorded neurons and model ensembling techniques.</p>
<p>Strengths:</p>
<p>(1) A comprehensive technical approach combining state-of-the-art neural encoding models with gradient-based optimization for video reconstruction.</p>
<p>(2) Thorough evaluation of reconstruction quality across different spatial and temporal frequencies using both natural videos and synthetic stimuli.</p>
<p>(3) Detailed analysis of factors affecting reconstruction quality, including population size and model ensembling effects.</p>
<p>(4) Clear methodology presentation with well-documented algorithms and reproducible code.</p>
<p>(5) Potential applications for investigating visual processing phenomena like predictive coding and perceptual learning.</p>
</disp-quote>
<p>We thank the reviewer for taking the time to provide this valuable feedback. We would like to add that in our eyes one additional main contribution is the step of going from reconstruction of static images to dynamic videos. We trust that in the revised manuscript, we have now made the point more explicit that static image reconstruction relies on temporally averaged responses, which negates the necessity of having to account for temporal dynamics altogether.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>The main metric of success (pixel correlation) may not be the most meaningful measure of reconstruction quality:</p>
<p>High correlation may not capture perceptually relevant features.</p>
<p>Different stimuli producing similar neural responses could have low pixel correlations The paper doesn't fully justify why high pixel correlation is a valuable goal</p>
</disp-quote>
<p>This is a very relevant point. In retrospect, perhaps we did not justify this enough. Sensory reconstruction typically aims to reconstruct sensory input based on brain activity as faithfully as possible. A brain-to-image decoder might therefore be trained to produce images as close to the original input as possible. The loss function to train the decoder would therefore be image similarity on the pixel level. In that case, evaluating reconstruction performance based on pixel correlation is somewhat circular.</p>
<p>However, when reconstructing videos, we optimize the input video in terms of its perceptual similarity to the original video and only then evaluate pixel-level similarity. The perceptual similarity metric we optimize for is the estimate of how the neurons in mouse V1 respond to that video. We then evaluate the similarity of this perceptually optimized video to the original input video with pixel-level correlation. In other words, we optimize for perceptual similarity and then evaluate pixel similarity. If our method optimized pixel-level similarity, then we would agree that perceptual similarity is a more relevant evaluation metric. We do not think it was clear in our original submission that our optimization loss function is a perceptual loss function, and have now made this clearer in Figure 1C-D and have clarified this in the results section, line 70:</p>
<p>“In effect, we optimized the input video to be perceptually similar with respect to the recorded neurons.”</p>
<p>And in line 110:</p>
<p>“Because our optimization of the movies was based on a perceptual loss function, we were interested in how closely these movies matched the originals on the pixel level.”</p>
<p>We chose to use pixel correlation to measure pixel-level similarity for several reasons. 1) It has been used in the past to evaluate reconstruction performance (Yoshida et al., 2020), 2) It is contrast and luminance insensitive, 3) correlation is a common metric so most readers will have an intuitive understanding of how it relates to the data.</p>
<p>To further highlight why pixel similarity might be interesting to visualize, we have included additional analysis in Figure 6 illustrating pixel-level differences between reconstructions from experimentally recorded activity and predicted activity.</p>
<p>We expect that the type of perceptual similarity the reviewer is alluding to is pretrained neural network image embedding similarity (Zhang et al., 2018: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1801.03924">https://doi.org/10.48550/arXiv.1801.03924</ext-link>). While these metrics seem to match human perceptual similarity, it is unclear if they reflect mouse vision. We did try to compare the embedding similarity from pretrained networks such as VGG16, but got results suggesting the reconstructed frames were no more similar to the ground truth than random frames, which is obviously not true. This might be because the ground truth videos were too different in resolution from the training data of these networks and because these metrics are typically very sensitive to decreases in resolution.</p>
<p>The best alternative approach to evaluate mouse perceptual similarity would be to show the reconstructed videos to the same animals while recording the same neurons and to compare these neural activation patterns to those evoked by the original ground truth videos. This has been done for static images in the past: Cobos et al., bioRxiv 2022, found that static image reconstructions generated using gradient descent evoked more similar trial-averaged (40 trials) responses to those evoked by ground truth images compared to other reconstruction methods. Unfortunately, we are currently not able to perform these in vivo experiments, which is why we used publicly available data for the current paper. We plan to use this method in the future. But this method is also not flawless as it assumes that the average response to an image is the best reflection of how that image is represented, which may not be the case for an individual trial.</p>
<p>As far as we are aware, there is currently no method that, given a particular activity pattern in response to an image/video, can produce an image/video that induces a neural activity pattern that is closer to the original neural response than simply showing the same image/video again. Hypothetically, such a stimulus exists because of various visual processing phenomena we mention in our discussion (e.g., predictive coding and selective attention), which suggest that the image that is represented by a population of neurons likely differs from the original sensory input. In other words, what the brain represents is an interpretation of reality not a pure reflection. Experimentally verifying this is difficult, as these variations might be present on a single trial level. The first step towards establishing a method that captures the visual representation of a population of neurons is sensory reconstruction, where the aim is to get as close as possible to the original sensory input. We think pixel-level correlation is a stringent and interpretable metric for this purpose, particularly when optimizing for perceptual similarity rather than image similarity directly.</p>
<disp-quote content-type="editor-comment">
<p>Comparison to previous work (Yoshida et al.) has methodological concerns: Direct comparison of correlation values across different datasets may be misleading; Large differences in the number of recorded neurons (10x more in the current study); Different stimulus types (dynamic vs static) make comparison difficult; No implementation of previous methods on the current dataset or vice versa.</p>
</disp-quote>
<p>Yes, we absolutely agree that direct comparison to previous static image reconstruction methods is problematic. We primarily do so because we think it is standard practice to give related baselines. We agree that direct comparison of the performance of video reconstruction methods to image reconstruction methods is not really possible. It does not make sense to train and apply a dynamic model on a static image data set where neural activity is time-averaged, as the temporal kernels could not be learned. Conversely, for a static model, which expects a single image as input and predicts time averaged responses, it does not make sense to feed it a series of temporally correlated movie frames and to simply concatenate the resulting activity perdition. The static model would need to be substantially augmented to incorporate temporal dynamics, which in turn would make it a new method. This puts us in the awkward position of being expected to compare our video reconstruction performance to previous image reconstruction methods without a fair way of doing so. We have now added these caveats in line 119:</p>
<p>“However, we would like to stress that directly comparing static image reconstruction methods with movie reconstruction approaches is fundamentally problematic, as they rely on different data types both during training and evaluation (temporally averaged vs continuous neural activity, images flashed at fixed intervals vs continuous movies).”</p>
<p>We have also toned down the language, emphasising the comparison to previous image reconstruction performance in the abstract, results, and conclusion.</p>
<p>Abstract: We removed “We achieve a ~2-fold increase in pixel-by-pixel correlation compared to previous state-of-the-art reconstructions of static images from mouse V1, while also capturing temporal dynamics.” and replaced with “We achieve a pixel-level correction of 0.57 between the ground truth movie and the reconstructions from single-trial neural responses.”</p>
<p>Discussion: we removed “In conclusion, we reconstruct videos presented to mice based on the activity of neurons in the mouse visual cortex, with a ~2-fold improvement in pixel-by-pixel correlation compared to previous static image reconstruction methods.” and replaced with “In conclusion, we reconstruct videos presented to mice based on single-trial activity of neurons in the mouse visual cortex.”</p>
<p>We have also removed the performance table and have instead added supplementary figure 3 with in-depth comparison across different versions of our reconstruction method (variations of masking, ensembling, contrast &amp; luminance matching, and Gaussian blurring).</p>
<disp-quote content-type="editor-comment">
<p>Limited exploration of how the reconstruction method could provide insights into neural coding principles beyond demonstrating technical capability.</p>
</disp-quote>
<p>The aim of this paper was not to reveal principles of neural coding. Instead, we aimed to achieve the best possible performance of video reconstructions and to quantify the limitations. But to highlight its potential we have added two examples of how sensory reconstruction has been applied in human vision research in line 321:</p>
<p>“Although fMRI-based reconstruction techniques are starting to be used to investigate visual phenomena in humans (such as illusions [Cheng et al., 2023] and mental imagery [Shen et al., 2019; Koide-Majima et al., 2024; Kalantari et al., 2025]), visual processing phenomena are likely difficult to investigate using existing fMRI-based reconstruction approaches, due to the low spatial and temporal resolution of the data.”</p>
<p>We have also added a demonstration of how this method could be used to investigate which parts of a reconstruction from a single trial response differs from the model's prediction (Figure  6). We do this by calculating pixel-level differences between reconstructions from the recorded neural activity and reconstructions from the expected neural activity (predicted activity by the neural encoding model). Although difficult to interpret, this pixel-by-pixel error map could represent trial-by-trial deviations of the neural code from pure sensory representation. But at this point we cannot know whether these errors are nothing more than errors in the reconstruction process. To derive meaningful interpretations of these maps would require a substantial amount of additional work and in vivo experiments and so is outside the scope of this paper, but we include this additional analysis now to highlight a) why pixel-level similarity might be interesting to quantify and visualize and b) to demonstrate how video reconstruction could be used to provide insights into neural coding, namely as a tool to identify how sensory representations differ from a pure reflection of the visual input.</p>
<disp-quote content-type="editor-comment">
<p>The claim that &quot;stimulus reconstruction promises a more generalizable approach&quot; (line 180) is not well supported with concrete examples or evidence.</p>
</disp-quote>
<p>What we mean by generalizable is the ability to apply reconstruction to novel stimuli, which is not possible for stimulus classification. We now explain this better in the paragraph in line 211:</p>
<p>“Stimulus identification, i.e. identifying the most likely stimulus from a constrained set, has been a popular approach for quantifying whether a population of neurons encodes the identity of a particular stimulus [Földiák, 1993, Kay et al., 2008]. This approach has, for instance, been used to decode frame identity within a movie [Deitch et al., 2021, Xia et al., 2021, Schneider et al., 2023, Chen et al.,2024]. Some of these approaches have also been used to reorder the frames of the ground truth movie [Schneider et al., 2023] based on the decoded frame identity. Importantly, stimulus identification methods are distinct from stimulus reconstruction where the aim is to recreate what the sensory content of a neuronal code is in a way that generalizes to new sensory stimuli [Rakhimberdina et al., 2021]. This is inherently a more demanding task because the range of possible solutions is much larger. Although stimulus identification is a valuable tool for understanding the information content of a population code, stimulus reconstruction could provide a more generalizable approach, because it can be applied to novel stimuli.”</p>
<p>All the stimuli we reconstructed were not in the training set of the model, i.e., novel. We have also downed down the claim: we have replaced “promises” with “could provide”.</p>
<disp-quote content-type="editor-comment">
<p>The paper would benefit from addressing how the method handles cases where different stimuli produce similar neural responses, particularly for high-speed moving stimuli where phase differences might be lost in calcium imaging temporal resolution.</p>
</disp-quote>
<p>Thank you for this suggestion, we think this is a great question. Calcium dynamics are slow and some of the high temporal frequency information could indeed be lost, particularly phase information. In other words, when the stimulus has high temporal frequency information, it is harder to decode spatial information because of the slow calcium dynamics. Ideally, we would look at this effect using the drifting grating stimuli; however, this is problematic because we rely on predicted activity from the SOTA DNEM, and due to the dilation of the first convolution, the periodic grating stimulus causes aliasing. At 15Hz, when the temporal frequency of the stimulus is half the movie frame rate, the model is actually being given two static images, and so the predicted activity is the interleaved activity evoked by two static images. We therefore do not think using the grating stimuli is a good idea. But we have used the Gaussian stimuli as it is not periodic, and is therefore less of a problem.</p>
<p>We have now also reconstructed phase-inverted Gaussian noise stimuli and plotted the video correlation between the reconstructions from activity evoked by phase-inverted stimuli. On the one hand, we find that even for the fastest changing stimuli, the correlation between the reconstructions from phase inverted stimuli are negative, meaning phase information is not lost at high temporal frequencies. On the other hand, for the highest spatial frequency stimuli, the correlation is negative. So, the predicted neural activity (and therefore the reconstructions) are phase-insensitive when the spatial frequency is higher than the reconstruction resolution limit we identified (spatial length constant of 1 pixel, or 3.38 degrees). Beyond this limit, the DNEM predicts activity in response to phase-inverted stimuli, which, when used for reconstruction, results in movies which are more similar to each other than the stimulus that actually evokes them.</p>
<p>However, not all information is lost at these high spatial frequencies. If we plot the Shannon entropy in the spatial domain or the motion energy in the temporal domain, we find that even when the reconstructions fail to capture the stimulus at a pixel-specific level (spatial length constant of 1 pixel, or 3.38 degrees), they do capture the general spatial and temporal qualities of the videos.</p>
<p>We have added these additional analyses to Figure 4 and Supplementary Figure 5.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>This is an interesting study exploring methods for reconstructing visual stimuli from neural activity in the mouse visual cortex. Specifically, it uses a competition dataset (published in the Dynamic Sensorium benchmark study) and a recent winning model architecture (DNEM, dynamic neural encoding model) to recover visual information stored in ensembles of the mouse visual cortex.</p>
<p>This is a great project - the physiological data were measured at a single-cell resolution, the movies were reasonably naturalistic and representative of the real world, the study did not ignore important correlates such as eye position and pupil diameter, and of course, the reconstruction quality exceeded anything achieved by previous studies. Overall, it is great that teams are working towards exploring image reconstruction. Arguably, reconstruction may serve as an endgame method for examining the information content within neuronal ensembles - an alternative to training interminable numbers of supervised classifiers, as has been done in other studies. Put differently, if a reconstruction recovers a lot of visual features (maybe most of them), then it tells us a lot about what the visual brain is trying to do: to keep as much information as possible about the natural world in which its internal motor circuits may act consequently.</p>
<p>While we enjoyed reading the manuscript, we admit that the overall advance was in the range of those that one finds in a great machine learning conference proceedings paper. More specifically, we found no major technical flaws in the study, only a few potential major confounds (which should be addressable with new analyses), and the manuscript did not make claims that were not supported by its findings, yet the specific conceptual advance and significance seemed modest. Below, we will go through some of the claims, and ask about their potential significance.</p>
</disp-quote>
<p>We thank the reviewer for the positive feedback on our paper.</p>
<disp-quote content-type="editor-comment">
<p>(1) The study showed that it could achieve high-quality video reconstructions from mouse visual cortex activity using a neural encoding model (DNEM), recovering 10-second video sequences and approaching a two-fold improvement in pixel-by-pixel correlation over attempts. As a reader, I am left with the question: okay, does this mean that we should all switch to DNEM for our investigations of the mouse visual cortex? What makes this encoding model special? It is introduced as &quot;a winning model of the Sensorium 2023 competition which achieved a score of 0.301... single-trial correlation between predicted and ground truth neuronal activity,&quot; but as someone who does not follow this competition (most eLife readers are not likely to do so, either), I do not know how to gauge my response. Is this impressive? What is the best achievable score, in theory, given data noise? Is the model inspired by the mouse brain in terms of mechanisms or architecture, or was it optimized to win the competition by overfitting it to the nuances of the data set? Of course, I know that as a reader, I am invited to read the references, but the study would stand better on its own if clarified how its findings depended on this model.</p>
</disp-quote>
<p>This is a very good point. We do not think that everyone should switch to using this particular DNEM to investigate the mouse visual cortex, but we think DNEMs and stimulus reconstruction in general has a lot of potential. We think static neural encoding models have already been demonstrated to be an extremely valuable tool to investigate visual coding (Walker et al., 2019; Yoshida et al., 2021; Willeke et al., bioRxiv 2023). DNEMs are less common, largely because they are very large and are technically more demanding to train and use. That makes static encoding models more practical for some applications, but they do not have temporal kernels and are therefore only used for static stimuli. They cannot, for instance, encode direction tuning, only orientation tuning. But both static and dynamic encoding models have advantages over stimulus classification methods which we outline in our discussion. Here we provide the first demonstration that previous achievements in static image reconstruction are transferable to movies.</p>
<p>It has been shown in the past for static neural encoding models that choosing a better-performing model produces reconstructed static images that are closer to the original image (Pierzchlewicz et al., 2023). The factors in choosing this particular DNEM were its capacity to predict neural activity (benchmarked against other models), it was open source, and the data it was designed for was also available.</p>
<p>To give more context to the model used in the paper, we have included the following, line 348:</p>
<p>“This model achieved an average single-trial correlation between predicted and ground truth neural activity of 0.291 during the competition, this was later improved to 0.301. The competition benchmark models achieved 0.106, 0.164 and 0.197 single-trial correlation, while the third and second place models achieved 0.243 and 0.265. Across the models, a variety of architectural components were used, including 2D and 3D convolutional layers, recurrent layers, and transformers, to name just a few.”</p>
<p>Concerning biologically inspired model design. The winning model contained 3 fully connected layers comprising the “Cortex” just before the final readout of neural activity, but we would consider this level of biological inspiration as minor. We do not think that the exact architecture of the model is particularly important, as the crucial aspect of such neural encoders is their ability to predict neural activity irrespective of how they achieve it. There has been a move towards creating foundation models of the brain (Wang et al., 2025) and the priority so far has been on predictive performance over mechanistic interpretability or similarity to biological structures and processes.</p>
<p>Finally, we would like to note that we do not know what the maximum theoretical score for single-trial responses might be, and don't think there is a good way of estimating it in this context.</p>
<disp-quote content-type="editor-comment">
<p>(2) Along those lines, two major conclusions were that &quot;critical for high-quality reconstructions are the number of neurons in the dataset and the use of model ensembling.&quot; If true, then these principles should be applicable to networks with different architectures. How well can they do with other network types?</p>
</disp-quote>
<p>This is a good question. Our method critically relies on the accurate prediction of neural activity in response to new videos. It is therefore expected that a model that better predicts neural responses to stimuli will also be better at reconstructing those stimuli given population activity. This was previously shown for static images (Pierzchlewicz et al., 2023). It is also expected that whenever the neural activity is accurately predicted, the corresponding reconstructed frames will also be more similar to the ground truth frames. We have now demonstrated this relationship between prediction accuracy and reconstruction accuracy in supplementary figure 4.</p>
<p>Although it would be interesting to compare the movie reconstruction performance of many different models with different architectures and activity prediction performances, this would involve quite substantial additional work because movie reconstruction is very resource- and time-intensive. Finding optimal hyperparameters to make such a comparison fair and informative would therefore be impractical and likely not yield surprising results.</p>
<p>We also think it is unlikely that ensembling would not improve reconstruction performance in other models because ensembling across model predictions is a common way of improving single-model performance in machine learning. Likewise, we think it is unlikely that the relationship between neural population size and reconstruction performance would differ substantially when using different models, because using more neurons means that a larger population of noisy neurons is “voting” on what the stimulus is. However, we would expect that if the model were worse at predicting neural activity, then more neurons are needed for an equivalent reconstruction performance. In general, we would recommend choosing the best possible DNEM available, in terms of neural activity prediction performance, when reconstructing movies using input optimization through gradient descent.</p>
<disp-quote content-type="editor-comment">
<p>(3) One major claim was that the quality of the reconstructions depended on the number of neurons in the dataset. There were approximately 8000 neurons recorded per mouse. The correlation difference between the reconstruction achieved by 1 neuron and 8000 neurons was ~0.2. Is that a lot or a little? One might hypothesize that ~7,999 additional neurons could contribute more information, but perhaps, those neurons were redundant if their receptive fields were too close together or if they had the same orientation or spatiotemporal tuning. How correlated were these neurons in response to a given movie? Why did so many neurons offer such a limited increase in correlation?</p>
</disp-quote>
<p>In the population ablation experiments, we compared the performance using ~1000, ~2000, ~4000, ~8000 neurons, and found an attenuation of 39.5% in video correlation when dropping 87.5% of the neurons (~1000 neurons remaining), we did not try reconstruction using just 1 neuron.</p>
<disp-quote content-type="editor-comment">
<p>(4) On a related note, the authors address the confound of RF location and extent. The study resorted to the use of a mask on the image during reconstruction, applied during training and evaluation (Line 87). The mask depends on pixels that contribute to the accurate prediction of neuronal activity. The problem for me is that it reads as if the RF/mask estimate was obtained during the very same process of reconstruction optimization, which could be considered a form of double-dipping (see the &quot;Dead salmon&quot; article, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S1053-8119(09)71202-9">https://doi.org/10.1016/S1053-8119(09)71202-9</ext-link>). This could inflate the reconstruction estimate. My concern would be ameliorated if the mask was obtained using a held-out set of movies or image presentations; further, the mask should shift with eye position, if it indeed corresponded to the &quot;collective receptive field of the neural population.&quot; Ideally, the team would also provide the characteristics of these putative RFs, such as their weight and spatial distribution, and whether they matched the biological receptive fields of the neurons (if measured independently).</p>
</disp-quote>
<p>We can reassure the reviewer that there is no double-dipping. We would like to clarify that the mask was trained only on videos from the training set of the DNEM and not the videos which were reconstructed. We have added the sentence, line 91:</p>
<p>“None of the reconstructed movies were used in the optimization of this transparency mask.”</p>
<p>Making the mask dependent on eye position would be difficult to implement with the current DNEM, where eye position is fed to the model as an additional channel. When using a model where the image is first transformed into retinotopic coordinates in an eye position-dependent manner (such as in Wang et al., 2025) the mask could be applied in retinotopic coordinates and therefore be dependent on eye position.</p>
<p>Effectively, the alpha mask defines the relative level of influence each pixel contributes to neural activity prediction. We agree it is useful to compare the shape of the alpha mask with the location of traditional on-off receptive fields (RFs) to clarify what the alpha mask represents and characterise the neural population available for our reconstructions. We therefore presented the DNEM with on-off patches to map the receptive fields of single neurons in an in silico experiment (the experimentally derived RF are not available). As expected, there is a rough overlap between the alpha mask (Supplementary Figure 2D), the average population receptive field (Supplementary Figure 2B), and the location of receptive field peaks (Supplementary Figure 2C). In principle, all three could be used during training or evaluation for masking, but we think that defining a mask based on the general influence of images on neural activity, rather than just on off patch responses, is a more elegant solution.</p>
<p>One idea of how to go a step further would be to first set the alpha mask threshold during training based on the % loss of neural activity prediction performance that threshold induces (in our case alpha=0.5 corresponds to ~3% loss in correlation between predicted vs recorded neural responses, see Supplementary Figure 3D), and second base the evaluation mask on a pixel correlation threshold (see example pixel correlation map in Supplementary Figure 2E) instead to avoid evaluating areas of the image with low image reconstruction confidence.</p>
<p>We referred to this figure in the result section, line 83:</p>
<p>“The transparency masks are aligned with but not identical to the On-Off receptive field distribution maps using sparse-noise (Figure S2).”</p>
<p>We have also done additional analysis on the effect of masking during training and evaluation with different thresholds in Supplementary Figure 3.</p>
<disp-quote content-type="editor-comment">
<p>(5) We appreciated the experiments testing the capacity of the reconstruction process, by using synthetic stimuli created under a Gaussian process in a noise-free way. But this further raised questions: what is the theoretical capability for the reconstruction of this processing pipeline, as a whole? Is 0.563 the best that one could achieve given the noisiness and/or neuron count of the Sensorium project? What if the team applied the pipeline to reconstruct the activity of a given artificial neural network's layer (e.g., some ResNet convolutional layer), using hidden units as proxies for neuronal calcium activity?</p>
</disp-quote>
<p>That’s a very interesting point. It is very hard to know what the theoretical best reconstruction performance of the model would be. Reconstruction performance could be decreased due to neural variability, experimental noise, the temporal kernel of the calcium indicator and the imaging frame rate, information compression along the visual hierarchy, visual processing phenomena (such as predictive coding and selective attention), failure of the model to predict neural activity correctly, or failure of the reconstruction process to find the best possible image which explains the neural activity. We don't think we can disentangle the contribution of all these sources, but we can provide a theoretical maximum assuming that the model and the reconstruction process are optimal. To that end, we performed additional simulations and reconstructed the natural videos using the predicted activity of the neurons in response to the natural videos as the target (similar to the synthetic stimuli) and got a correlation of 0.766. So, the single trial performance of 0.569 is ~75% of this theoretical maximum. This difference can be interpreted as a combination of the losses due to neuronal variability, measurement noise, and actual deviations in the images represented by the brain compared to reality.</p>
<p>We thank the reviewer for this suggestion, as it gave us the idea of looking at error maps (Figure 6), where the pixel-level deviation of the reconstructions from recorded vs predicted activity is overlaid on the ground truth movie.</p>
<disp-quote content-type="editor-comment">
<p>(6) As the authors mentioned, this reconstruction method provided a more accurate way to investigate how neurons process visual information. However, this method consisted of two parts: one was the state-of-the-art (SOTA) dynamic neural encoding model (DNEM), which predicts neuronal activity from the input video, and the other part reconstructed the video to produce a response similar to the predicted neuronal activity. Therefore, the reconstructed video was related to neuronal activity through an intermediate model (i.e., SOTA DNEM). If one observes a failure in reconstructing certain visual features of the video (for example, high-spatial frequency details), the reader does not know whether this failure was due to a lack of information in the neural code itself or a failure of the neuronal model to capture this information from the neural code (assuming a perfect reconstruction process). Could the authors address this by outlining the limitations of the SOTA DNEM encoding model and disentangling failures in the reconstruction from failures in the encoding model?</p>
</disp-quote>
<p>To test if a better neural prediction by the DNEM would result in better reconstructions, we ran additional simulations and now show that neural activity prediction performance correlates with reconstruction performance (Supplementary Figure 4B). This is consistent with Pierzchlewicz et al., (2023) who showed that static image reconstructions using better encoding models leads to better reconstruction performance. As also mentioned in the answer to the previous comment, untangling the relative contributions of reconstruction losses is hard, but we think that improvements to the DNEM performance are key. Two suggestions to improving the DNEM we used would be to translate the input image into retinotopic coordinates and shift this image relative to eye position before passing it to the first convolutional layer (as is done in Wang et al. 2025), to use movies which are not spatially down sampled as heavily, to not use a dilation of 2 in the temporal convolution of the first layer and to train on a larger dataset.</p>
<disp-quote content-type="editor-comment">
<p>(7) The authors mentioned that a key factor in achieving high-quality reconstructions was model assembling. However, this averaging acts as a form of smoothing, which reduces the reconstruction's acuity and may limit the high-frequency content of the videos (as mentioned in the manuscript). This averaging constrains the tool's capacity to assess how visual neurons process the low-frequency content of visual input. Perhaps the authors could elaborate on potential approaches to address this limitation, given the critical importance of high-frequency visual features for our visual perception.</p>
</disp-quote>
<p>This is exactly what we also thought. To answer this point more specifically, we ran additional simulations where we also reconstruct the movies using gradient ensembling instead of reconstruction ensembling. Here, the gradients of the loss with respect to each pixel of the movie is calculated for each of the model instances and are averaged at every iteration of the reconstruction optimization. In essence, this means that one reconstruction solution is found, and the averaging across reconstructions, which could degrade high-frequency content, is skipped. The reconstructions from both methods look very similar, and the video correlation is, if anything, slightly worse (Supplemental Figure 3A&amp;C). This indicates that our original ensembling approach did not limit reconstruction performance, but that both approaches can be used, depending on what is more convenient given hardware restrictions.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public review):</bold></p>
<p>Summary:</p>
<p>This paper presents a method for reconstructing input videos shown to a mouse from the simultaneously recorded visual cortex activity (two-photon calcium imaging data). The publicly available experimental dataset is taken from a recent brain-encoding challenge, and the (publicly available) neural network model that serves to reconstruct the videos is the winning model from that challenge (by distinct authors). The present study applies gradient-based input optimization by backpropagating the brain-encoding error through this selected model (a method that has been proposed in the past, with other datasets). The main contribution of the paper is, therefore, the choice of applying this existing method to this specific dataset with this specific neural network model. The quantitative results appear to go beyond previous attempts at video input reconstruction (although measured with distinct datasets). The conclusions have potential practical interest for the field of brain decoding, and theoretical interest for possible future uses in functional brain exploration.</p>
<p>Strengths:</p>
<p>The authors use a validated optimization method on a recent large-scale dataset, with a state-of-the-art brain encoding model. The use of an ensemble of 7 distinct model instances (trained on distinct subsets of the dataset, with distinct random initializations) significantly improves the reconstructions. The exploration of the relation between reconstruction quality and the number of recorded neurons will be useful to those planning future experiments.</p>
<p>Weaknesses:</p>
<p>The main contribution is methodological, and the methodology combines pre-existing components without any new original components.</p>
</disp-quote>
<p>We thank the reviewer for taking the time to review our paper and for their overall positive assessment. We would like to emphasise that combining pre-existing machine learning techniques to achieve top results in a new modality does require iteration and innovation. While gradient-based input optimization by backpropagating the brain-encoding error through a neural encoding model has been used in 2D static image optimization to generate maximally exciting images and reconstruct static images, we are the first to have applied it to movies which required accounting for the time domain. Previous methods used time averaged responses and were limited to the reconstruction of static images presented with fixed image intervals.</p>
<disp-quote content-type="editor-comment">
<p>The movie reconstructions include a learned &quot;transparency mask&quot; to concentrate on the most informative area of the frame; it is not clear how this choice impacts the comparison with prior experiments. Did they all employ this same strategy? If not, shouldn't the quantitative results also be reported without masking, for a fair comparison?</p>
</disp-quote>
<p>Yes, absolutely. All reconstruction approaches limit the field of view in some way, whether this is due to the size of the screen, the size of the image on the screen, or cropping of the presented/reconstructed images during analysis due to the retinotopic coverage of the recorded neurons. Note that we reconstruct a larger field of view than Yoshida et al. In Yoshida et al., the reconstructed field of view was 43 by 43 retinal degrees. we show the size of an example evaluation mask in comparison.</p>
<p>To address the reviewer’s concern more specifically, we performed additional simulations and now also show the performance using a variety of different training and evaluation masks, including different alpha thresholds for training and evaluation masks as well as the effective retinotopic coverage at different alpha thresholds. Despite these comparisons, we would also like to highlight that the comparison to the benchmark is problematic itself. This is because image and movie reconstruction are not directly comparable. It does not make sense to train and apply a dynamic model on a static image dataset where neural activity is time averaged. Conversely, it does not make sense to train or apply a static model that expects time-averaged neural responses on continuous neural activity unless it is substantially augmented to incorporate temporal dynamics, which in turn would make it a new method. This puts us in the awkward position of being expected to compare our video reconstruction performance to previous image reconstruction methods without a fair way of doing so. We have therefore de-emphasised the phrasing comparing our method to previous publications in the abstract, results, and discussion.</p>
<p>Abstract: “We achieve a ~2-fold increase in pixel-by-pixel correlation compared to previous state-of-the-art reconstructions of static images from mouse V1, while also capturing temporal dynamics.” with “We achieve a pixel-level correction of 0.57 between the ground truth movie and the reconstructions from single-trial neural responses.”</p>
<p>Results: “This represents a ~2x higher pixel-level correlation over previous single-trial static image reconstructions from V1 in awake mice (image correlation 0.238 +/- 0.054 s.e.m for awake mice) [Yoshida et al., 2020] over a similar retinotopic area (~43° x 43°) while also capturing temporal dynamics. However, we would like to stress that directly comparing static image reconstruction methods with movie reconstruction approaches is fundamentally problematic, as they rely on different data types both during training and evaluation (temporally averaged vs continuous neural activity, images flashed at fixed intervals vs continuous movies).”</p>
<p>Discussion: “In conclusion, we reconstruct videos presented to mice based on the activity of neurons in the mouse visual cortex, with a ~2-fold improvement in pixel-by-pixel correlation compared to previous static image reconstruction methods.” with “In conclusion, we reconstruct videos presented to mice based on single-trial activity of neurons in the mouse visual cortex.”</p>
<p>We have also removed the performance table and have instead added supplementary figure 3 with in-depth comparison across different versions of our reconstruction method (variations of masking, ensembling, contrast &amp; luminance matching, and Gaussian blurring).</p>
<p>We believe that we have given enough information in our paper now so that readers can make an informed decision whether our movie reconstruction method is appropriate for the questions they are interested in.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewer #2 (Recommendations for the authors):</bold></p>
<p>(1) &quot;Reconstructions have been luminance (mean pixel value across video) and contrast (standard deviation of pixel values across video) matched to ground truth.&quot; This was not clear: was it done by the investigating team? I imagine that one of the most easily captured visual features is luminance and contrast, why wouldn't the optimization titrate these well?</p>
</disp-quote>
<p>The contrast and luminance matching of the reconstructions to the ground truth videos was done by us, but this was only done to help readers assess the quality of the reconstructions by eye. Our performance metrics (frame and video correlation) are contrast and luminance insensitive. To clarify this, we have also added examples of non-adjusted frames in Supplementary Figure 3A, and added a sentence in the results, line 103:</p>
<p>“When presenting videos in this paper we normalize the mean and standard deviation of the reconstructions to the average and standard deviation of the corresponding ground truth movie before applying the evaluation masks, but this is not done for quantification except in Supplementary Figure 3D.”</p>
<p>We were also initially surprised that contrast and luminance are not captured well by our reconstruction method, but this makes sense as V1 is largely luminance invariant (O’Shea et al., 2025 <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.celrep.2024.115217">https://doi.org/10.1016/j.celrep.2024.115217</ext-link> ) and contrast only has a gain effect on V1 activity (Tring et al., 2024 <ext-link ext-link-type="uri" xlink:href="https://journals.physiology.org/doi/full/10.1152/jn.00336.2024">https://journals.physiology.org/doi/full/10.1152/jn.00336.2024</ext-link>). Decoding absolute contrast is likely unreliable because it is probably not the only factor modulating the overall gain of the neural population.</p>
<p>To address the reviewer’s comment more fully, we ran additional experiments. More specifically, to test why contrast and luminance are not recovered in the reconstructions, we checked how the predicted activity between the reconstruction and the contrast/luminance corrected reconstructions differs. Contrast and luminance adjustment had little impact on predicted response similarity on average. This makes the reconstruction optimization loss function insensitive to overall contrast and luminance so it cannot be decoded. There is a small effect on activity correlation, however, so we cannot completely rule out that contrast and luminance could be reconstructed with a different loss function.</p>
<disp-quote content-type="editor-comment">
<p>(2) The authors attempted to investigate the variability in reconstruction quality across different movies and 10-second snippets of a movie by correlating various visual features, such as video motion energy, contrast, luminance, and behavioral factors like running speed, pupil diameter, and eye movement, with reconstruction success. However, it would also be beneficial if the authors correlated the response loss (Poisson loss between neural responses) with reconstruction quality (video correlation) for individual videos, as these metrics are expected to be correlated if the reconstruction captures neural variance.</p>
</disp-quote>
<p>We thank the reviewer for this suggestion. We have now included this analysis and find that if the neural activity was better predicted by the DNEM then the reconstruction of the video was also more similar to the ground truth video. We further found that this effect is shift-dependent (in time), meaning the prediction of activity based on proximal video frames is more influential on reconstruction performance.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations for the authors):</bold></p>
<p>(1) I was confused about the choice of applying a transparency mask thresholded with alpha&gt;0.5 during training and alpha&gt;1 during evaluation. Why treat the two situations differently? Also, shouldn't we expect alpha to be in the [0,1] range, in which case, what is the meaning of alpha&gt;1? (And finally, as already described in &quot;Weaknesses&quot;, how does this choice impact the comparison with prior experiments? Did they also employ a similar masking strategy?)</p>
</disp-quote>
<p>We found that applying a mask during training increased performance regardless of the size of the evaluation mask. Using a less stringent mask during training than during evaluation increases performance slightly, but also allows inspection of the reconstruction in areas where the model will be less confident without sacrificing performance, if this is desired. The thresholds of 0.5 and 1 were chosen through trial and error, but the exact values do not hold intrinsic meaning. The alpha mask values can go above 1 during their optimization. We could have clipped alpha during the training procedure (algorithm 1), but we decided this was not worth redoing at this stage, as the alphas used for testing were not above 1. All reconstruction approaches in previous publications limit the field of view in some form, whether this is due to the size of the screen, the size of the image on the screen, or the cropping of the presented/reconstructed images during analysis.</p>
<p>To address the reviewer’s comment in detail, we have added extensive additional analysis to evaluate the coverage of the reconstruction achieved in this paper and how different masking strategies affect performance, as well as how the mask relates to more traditional receptive field mapping.</p>
<disp-quote content-type="editor-comment">
<p>(2) I would not use the word &quot;imagery&quot; in the first sentence of the abstract, because this might be interpreted by some readers as reconstruction of mental imagery, a very distinct question.</p>
</disp-quote>
<p>We changed imagery to images in the abstract.</p>
<disp-quote content-type="editor-comment">
<p>(3) Line 145-146: &quot;&lt;1 frame, or &lt;30Hz&quot; should be &quot;&lt;1 frame, or &gt;30Hz&quot;.</p>
</disp-quote>
<p>We have corrected the error.</p>
<disp-quote content-type="editor-comment">
<p>(4) Algorithm 1, Line 5, a subscript variable 'g' should be changed to 'h'</p>
</disp-quote>
<p>We have corrected the error.</p>
<p>Additional Changes</p>
<p>(1) Minor grammatical errors</p>
<p>(2) Addition of citations: We were previously not aware of a bioRxiv preprint from 2022 (Cobos et al., 2022), which used gradient descent-based input optimization to reconstruct static images but without the addition of a diffusion model. Instead, we had cited for this method Pierzchlewicz et al., 2023 bioRxiv/NeurIPS. In Cobos et al., 2022, they compare static image reconstruction similarity to ground truth images and the similarity of the in vivo evoked activity across multiple reconstruction methods. Performance values are only given for reconstructions from trial-averaged responses across ~40 trials (in the absence of original data or code we are also not able to retrospectively calculate single-trial performance). The authors find that optimizing for evoked activity rather than image similarity produces image reconstructions that evoke more similar in vivo responses compared to reconstructions optimized for image similarity itself. We have now added and discussed the citation in the main text.</p>
<p>(3) Workaround for error in the open-source code from <ext-link ext-link-type="uri" xlink:href="https://github.com/lRomul/sensorium">https://github.com/lRomul/sensorium</ext-link> for video hashing function in the SOTA DNEM: By checking the most correlated first frame for each reconstructed movie, we discovered there was a bug in the open-source code and 9/50 movies we originally used for reconstruction were not properly excluded from the training data between DNEM instances. The reason for this error was that some of the movies are different by only a few pixels, and the video hashing function used to split training and test set folds in the original DNEM code classified these movies as different and split them across folds. We have replaced these 9 movies and provide a figure below showing the next closest first frame for every movie clip we reconstruct. This does not affect our claims. Excluding these 9 movie clips, did not affect the reconstruction performance (video correlation went from 0.563 to 0.568), so there was no overestimation of performance due to test set contamination. However, they should still be removed so some of the values in the paper have changed slightly. The only statistical test that was affected was the correlation between video correlation and mean motion energy (Supplementary Figure 4A), which went from p = 0.043 to 0.071.</p>
<fig id="sa3fig2">
<label>Author response image 2.</label>
<caption>
<title>exclusion of movie clips with duplicates in the DNEM training data.</title>
<p>A) example frame of a reconstructed movie (ground truth) and the most correlated first frame from the training data. b) all movie clips and their corresponding most correlated clip from the training data. Red boxes indicate excluded duplicates.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-105081-sa3-fig2.jpg" mimetype="image"/>
</fig>
</body>
</sub-article>
</article>