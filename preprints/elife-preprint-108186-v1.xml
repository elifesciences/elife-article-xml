<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108186</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108186</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108186.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Biochemistry and Chemical Biology</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories><title-group>
<article-title>Squidly: Enzyme Catalytic Residue Prediction Harnessing a Biology-Informed Contrastive Learning Framework</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0005-9574-3468</contrib-id>
<name>
<surname>Rieger</surname>
<given-names>William JF</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3548-268X</contrib-id>
<name>
<surname>Boden</surname>
<given-names>Mikael</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4027-364X</contrib-id>
<name>
<surname>Arnold</surname>
<given-names>Frances</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1331-8192</contrib-id>
<name>
<surname>Mora</surname>
<given-names>Ariane</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<email>amora@caltech.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00rqy9422</institution-id><institution>School of Chemistry and Molecular Biology, University of Queensland</institution></institution-wrap>, <city>Brisbane</city>, <country country="AU">Australia</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05dxps055</institution-id><institution>Division of Chemistry and Chemical Engineering, California Institute of Technology</institution></institution-wrap>, <city>Pasadena</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Graña</surname>
<given-names>Martin</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Institut Pasteur de Montevideo</institution>
</institution-wrap>
<city>Montevideo</city>
<country country="UY">Uruguay</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Cui</surname>
<given-names>Qiang</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Boston University</institution>
</institution-wrap>
<city>Boston</city>
<country country="US">United States</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-10-02">
<day>02</day>
<month>10</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP108186</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-06-28">
<day>28</day>
<month>06</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-06-20">
<day>20</day>
<month>06</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.06.13.659624"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Rieger et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Rieger et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108186-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Enzymes present a sustainable alternative to traditional chemical industries, drug synthesis, and bioremediation applications. Because catalytic residues are the key amino acids that drive enzyme function, their accurate prediction facilitates enzyme function prediction. Sequence similarity-based approaches such as BLAST are fast but require previously annotated homologs. Machine learning approaches aim to overcome this limitation; however, current gold-standard machine learning (ML)-based methods require high-quality 3D structures limiting their application to large datasets. To address these challenges, we developed Squidly, a sequence-only tool that leverages contrastive representation learning with a biology-informed, rationally designed pairing scheme to distinguish catalytic from non-catalytic residues using per-token Protein Language Model embeddings. Squidly surpasses state-of-the-art ML annotation methods in catalytic residue prediction while remaining sufficiently fast to enable wide-scale screening of databases. We ensemble Squidly with BLAST to provide an efficient tool that annotates catalytic residues with high precision and recall for both in- and out-of-distribution sequences.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Enzymes are efficient, non-toxic and sustainable alternatives to traditional chemical catalysts in applications such as bio-remediation, pharmaceutical and biofuel production<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. Catalytic residues refer to the amino acids in the protein sequence directly involved in the mechanism of action. Catalytic residues are involved both directly and indirectly in catalysis; for example, they can stabilize transition states <sup><xref ref-type="bibr" rid="c2">2</xref></sup>, act as direct electron donors, or interact with secondary residues, water molecules, or cofactors that are then directly involved in the catalytic mechanism. As catalytic activity depends on specific structural conformations and sequence context, identifying catalytic residues remains a challenge.</p>
<p>Catalytic residue annotations can be used to infer protein function prediction and enable insights into mechanisms of action and evolutionary relationships <sup><xref ref-type="bibr" rid="c3">3</xref>–<xref ref-type="bibr" rid="c5">5</xref></sup>. Annotations are used to facilitate enzyme engineering methods such as directed evolution and rational design <sup><xref ref-type="bibr" rid="c6">6</xref></sup>, and can help construct the theoretical catalytic sites known as theozymes that are used to condition <italic>de novo</italic> enzyme design <sup><xref ref-type="bibr" rid="c7">7</xref></sup>. Experimental methods to determine catalytic residues are time-consuming, labour-intensive, and costly, as they often require experimentally determined enzyme-substrate structures. To expedite annotation, computational approaches have been developed to predict catalytic residues from protein sequences or predicted structures <sup><xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c11">11</xref></sup>, using annotations in databases such as UniProt <sup><xref ref-type="bibr" rid="c12">12</xref></sup> and M-CSA, a manually curated enzyme mechanism database <sup><xref ref-type="bibr" rid="c13">13</xref></sup>. Existing computational methods can be broadly grouped into three categories: 1) similarity-based methods and sequence alignment, 2) structural methods, or 3) statistical and machine learning approaches.</p>
<p>Similarity-based approaches are effective when homologous sequences have been previously annotated. Structure-based approaches use a variety of geometric search algorithms to find potential binding pockets and then map sequence conservation scores to these regions to predict catalytic residues with higher specificity <sup><xref ref-type="bibr" rid="c14">14</xref>–<xref ref-type="bibr" rid="c16">16</xref></sup>. Geometric methods improve the accuracy of approaches reliant on homology, while the limitation of requiring homologous sequences remains. Data-driven approaches such as statistical models or machine learning algorithms offer a more flexible framework <sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup> but are restricted by the quality and diversity of datasets available for training. The current machine learning (ML) state-of-the-art (SOTA) tools apply multimodal deep learning strategies and often emphasise the value of the structural modality. Shen et al. <sup><xref ref-type="bibr" rid="c10">10</xref></sup> combined an adaptive edge-gated graph attention neural network (AEGAN) with graphs constructed using the 3D structure and sequence-derived positional-specific scoring matrices. SCREEN leverages protein sequence and structure by combining graphical convolutional neural networks and contrastive learning on Protein Language Models (PLMs) <sup><xref ref-type="bibr" rid="c9">9</xref></sup>. EasIFA, developed by Wang et al. <sup><xref ref-type="bibr" rid="c11">11</xref></sup>, predicts both catalytic residues and binding sites by incorporating graph attention representations of chemical reactions with PLM information and structure representations. Despite the reported improvements on sequence based methods by multi-modal approaches, current benchmarks inherently obscure the reliance on structural prediction as they often contain sequences with known structures, which are more likely to be predicted accurately by tools such as AlphaFold <sup><xref ref-type="bibr" rid="c18">18</xref></sup>. This bias may limit the evaluation of generalisability to unseen data, i.e. enzymes without experimentally derived structures. Furthermore computing structures remains computationally intensive.</p>
<p>In comparison to structural models PLMs learn the distribution of amino acids across protein sequences. PLMs embed biologically meaningful representations that capture complex dependencies between residues within a protein sequence <sup><xref ref-type="bibr" rid="c19">19</xref>–<xref ref-type="bibr" rid="c21">21</xref></sup>. These models are trained in an unsupervised manner on large numbers (billions) of diverse sequences <sup><xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref></sup>. Remote homology search and alignment algorithms based on PLM embeddings have been reported to outperform sequence similarity-based methods at low sequence identities <sup><xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c25">25</xref></sup>. In enzyme function prediction, models highlight the utility of PLM embeddings in combination with contrastive learning, to achieve SOTA ML performance in the prediction of enzyme function (by proxy of enzyme classification numbers) and associated reactions catalysed <sup><xref ref-type="bibr" rid="c26">26</xref>–<xref ref-type="bibr" rid="c28">28</xref></sup>; however, similarity-based approaches (e.g. BLAST or reaction distance) perform comparably <sup><xref ref-type="bibr" rid="c28">28</xref></sup>. We hypothesised that using PLM embeddings to predict catalytic residues would complement sequence similarity approaches in low homology settings.</p>
<p>Contrastive learning is a discriminative modelling approach that facilitates learning by distinguishing between paired observations in data that are “similar” or “different”, thus enabling information to be extracted from dense PLM embeddings <sup><xref ref-type="bibr" rid="c29">29</xref></sup>. Data engineering, such as choosing a dataset to enhance specific relationships <sup><xref ref-type="bibr" rid="c30">30</xref></sup>, enables domain expertise to inform pair schemes for training. For example, it is common to explicitly incorporate and maximise “hard negatives” (samples that are dissimilar but close in the latent space) to refine the model’s ability to discriminate subtle differences in the feature space <sup><xref ref-type="bibr" rid="c31">31</xref>–<xref ref-type="bibr" rid="c33">33</xref></sup>.</p>
<p>Although pair schemes can be generated automatically in contrastive learning frameworks <sup><xref ref-type="bibr" rid="c30">30</xref></sup>, we posited that the physical principles of enzyme functions could be used to maximise hard negatives and create an accurate catalytic residue prediction algorithm, as in ref <sup><xref ref-type="bibr" rid="c28">28</xref></sup>. Enzyme function can be grouped into different classes using the domain standard Enzyme Class (EC) ontology, which classifies enzymes into four levels based on the reactions they catalyse. The first level represents the general type of reaction (e.g., oxidoreductases), while subsequent levels add specificity, capturing information about substrates, cofactors, and products <sup><xref ref-type="bibr" rid="c34">34</xref></sup>. This hierarchical structure provides as basis for relationships between enzyme functions and mechanisms that can be used to create biologically meaningful pairs.</p>
<p>To overcome the compute limitations with structural methods and existing benchmarks, we developed Squidly and a new benchmark for low sequence identity catalytic residue predictions. We combined a contrastive learning framework on PLM per-token embeddings with a rationally designed hierarchical pair scheme to create a sequence-based catalytic residue predictor that is accurate, scalable, and able to generalise in the absence of sequence similarity. Squidly exceeds the performance of both BLAST and SOTA ML tools in existing benchmarks achieving a F1 of greater than 0.85 across enzyme families, in addition to generalising to low sequence identity enzymes, with an F1 of 0.64 on sequences with less than 30% identity. Squidly is over 50x faster than folding a structure, enabling it to be used to annotate existing databases, in metagenomics pipelines, and to filter <italic>de novo</italic> designed proteins. Finally, owing to the added interpretability of sequence similarity-based approaches we provide Squidly as an ensemble with BLAST, and suggest using Squidly where the sequence identity is less than 50%. We envision Squidly will be broadly applicable in the enzyme engineering field and will complement existing sequence homology and structure-based methods.</p>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>AEGAN training and test datasets</title>
<p>To ensure comparability with the SOTA tools for catalytic residue prediction, the same training, test, and benchmark datasets (Uni14230, Uni3750) were utilised as described in prior work <sup><xref ref-type="bibr" rid="c10">10</xref></sup>. These were originally sourced from UniProt and M-CSA databases. Uni14230 and Uni3750 datasets also include computational and manually curated predictions from SwissProt. To reduce redundancy, the data was filtered to retain only sequences with less than 60% sequence identity, resulting in 8,784 sequences in the training set and 1,955 sequences in the test set. Additionally, Squidly was evaluated against six previously reported benchmark datasets <sup><xref ref-type="bibr" rid="c35">35</xref>–<xref ref-type="bibr" rid="c38">38</xref></sup>. The EF family, superfamily, fold, and HA superfamily datasets were specifically designed to include one representative sequence from each enzyme family, fold, or superfamily present in existing databases, and are thus not designed to predict “novel” active sites <sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup>. The NN and PC datasets were both constructed to be structurally and functionally heterogeneous based on SCOP and CATH classifications <sup><xref ref-type="bibr" rid="c37">37</xref>,<xref ref-type="bibr" rid="c38">38</xref></sup>. These datasets were generated prior to 2007 and are therefore limited in their catalytic diversity compared to the current databases. Both AEGAN and SCREEN were originally benchmarked against distinct subsets of these datasets, however, we found that they contained sequences with high identity to their respective training data, as detailed in <xref rid="tblS2" ref-type="table">Supplementary Table 2</xref>. Squidly’s reported performance with these datasets is based on the Uni14230 training set to ensure a fair comparison with AEGAN and given the lower overall sequence similarity between the training and test sets.</p>
</sec>
<sec id="s2b">
<title>New benchmark: CataloDB</title>
<p>To address the shortcomings of existing benchmark datasets for the evaluation of catalytic residue predictions, we introduce a benchmarking dataset named CataloDB. CataloDB collects the annotation data available for enzymes in the SwissProt database. The total set of enzymes includes experimentally determined annotations in addition to reviewed sequences with known structures, see Appendix A.1 for more details. Sequences with greater than 90% sequence similarity, based on shared overlap, were removed using mmSeqs2 <sup><xref ref-type="bibr" rid="c39">39</xref></sup>. Structural and sequence-based clustering was performed on the filtered dataset to create a test set of 232 sequences with less than 30% sequence and structural identity to the training set (the remaining 5357 sequences). Structural identity was calculated using FoldSeek <sup><xref ref-type="bibr" rid="c40">40</xref></sup>. Sequences that were in any of the six commonly used benchmarks were omitted from the training data to ensure compatibility between CataloDB and existing benchmarks. Additional detailed descriptions of the data are available in the Appendix A.1, and see <xref rid="figS1" ref-type="fig">Supplementary Figures S1</xref>-<xref rid="figS6" ref-type="fig">S6</xref> for distribution of amino acids and EC classes in the benchmark dataset.</p>
<p>To enable direct comparison with the SOTA tool SCREEN, we retrained a lightweight version of the model using the CataloDB benchmark training data. This version of SCREEN, provided by the original authors, omits costly evolutionary features which are cumbersome for larger datasets. The authors report that these features have minimal impact on performance <sup><xref ref-type="bibr" rid="c9">9</xref></sup>. Minor changes were made to the source code to improve model training stability. Notably, we allowed optimal stopping to occur after 5 epochs of training, rather than after 200 epochs in the original source code as we observed exploding gradients at later epochs when trained on CataloDB.</p>
</sec>
<sec id="s2c">
<title>ESM2 embeddings</title>
<p>Each sequence was encoded by either the 3 billion parameter or 15 billion ESM2 model <sup><xref ref-type="bibr" rid="c23">23</xref></sup>. Pre-trained model weights were used without fine-tuning and per-token embeddings were extracted from the final layer of the encoder. These embeddings contain a vector of length 2560 (3B) or 5120 (15B) for each amino acid in the sequence. No normalization was applied to the embeddings before downstream use.</p>
</sec>
<sec id="s2d">
<title>Contrastive model</title>
<p>The supervised contrastive model is a network that takes per-residue embeddings as input (2560 or 5120 length vectors) and is trained to output a new embedding (N=128) of the residue. The model contains an initial dropout layer connected to the inputs, with a dropout rate of 10%, and two subsequent hidden layers of 1280 and 640 size each. During training, batches of 16,000 pairs are passed through the model and the distance between the output representations of each pair is calculated. The following cosine embedding loss function implemented by PyTorch is used to calculate the loss based on the distances:
<disp-formula>
<graphic xlink:href="659624v1_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Where <italic>x</italic><sub>1</sub> and <italic>x</italic><sub>2</sub> are input vectors and <italic>y</italic> is the label, with <italic>y</italic> = 1 indicating a positive pair and <italic>y</italic> = <italic>−</italic>1 indicating a negative pair. When <italic>y</italic> = 1, the loss function rewards the model for maximising the cosine similarity, cos(<italic>x</italic><sub>1</sub><italic>, x</italic><sub>2</sub>). The loss function minimises the cosine similarity when <italic>y</italic> = <italic>−</italic>1. Importantly, the pair selection scheme optimises for hard-negative pairs, and thus the margin is set equal to 0 when defining the loss function.</p>
</sec>
<sec id="s2e">
<title>Reaction informed pair-mining</title>
<p>Positive and negative pairs were created to enable contrastive comparisons where in the simplest form positives are pairs of exclusively catalytic or exclusively non-catalytic residues while negatives include a catalytic and a non-catalytic residue. As all possible pair combinations of amino acids becomes intractable we are required to reduce this to make training feasible, hence we test three methods to subsample from this pool of possible pairs. Specifically, we used information about the EC number of the sequence and amino acid characters to refine the pair selection process. For further information, see Appendix A.2.</p>
<p>Three pair schemes were created to test for the effect of the pair scheme, herein referred to as “reaction-informed pair-mining”. See <xref rid="fig1" ref-type="fig">Figure 1</xref> for a visual description.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 1:</label>
<caption><title>Representation of the pair-mining schemes depicting the negative and positive classes for each scheme.</title><p>Scheme 1 is the most permissive and is amino acid and EC <italic>unaware</italic>. Schemes 2 and 3 are more restrictive, requiring both the amino acids and the EC classes to be shared or different in the pair-mining scheme. Scheme 3 included additional negative pairs to separate EC numbers in the latent space.</p></caption>
<graphic xlink:href="659624v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><bold>Scheme 1</bold> samples pairs across all sequences and residues such that positive pairs are pairs of residues that are either both catalytic or both non-catalytic.</p>
<p><bold>Scheme 2</bold> samples positive and negative pairs much like in scheme 1, except that each pair of residues must be the same amino acid and must have come from a sequence which has the same EC number.</p>
<p><bold>Scheme 3</bold> performs the same pair sampling as in scheme 2; however, scheme 3 also includes additional negative pairs that are either exclusively catalytic residues or exclusively non-catalytic and come from different EC numbers or amino acids.</p>
<p>To combat the inherent bias in the data towards certain EC numbers, sequences were grouped by EC number at the 2<italic><sup>nd</sup></italic> level, and an upper limit of 600 catalytic residues and 600 non-catalytic residues were sampled from each EC number. An equal number of positive and negative pairs were generated for each EC number. A parameter called the sample limit was set to limit the total number of pairs made for each group. The sample limit chosen was 16,000, providing a total of 8-10 million pairs for schemes 2 and 3. Scheme 1 was then trained with an equal total number of pairs.</p>
</sec>
<sec id="s2f">
<title>LSTM classifier</title>
<p>All LSTM classification models were trained using the same parameters. The models are bidirectional LSTM networks designed for binary classification of catalytic versus non-catalytic residues in protein sequences. The model takes as input 128-dimensional per-residue embeddings (from the contrastive model) that are processed by a 2-layer LSTM (hidden size: 128) with a dropout rate of 0.2, followed by a linear layer mapping the output to a single logit score, see <xref rid="fig2" ref-type="fig">Figure 2</xref>. Class imbalance is addressed by weighting the misclassification of catalytic residues 100 times higher during loss computation. The models were trained using a 90:10 train-validation split, with a batch size of 400 sequences, a learning rate of 0.01, and early stopping of 5 epochs was used. These parameters were not optimised. See <xref rid="fig2" ref-type="fig">Figure 2</xref> for a depiction of the model architecture.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 2:</label>
<caption><title>Overview of the model, depicting a sequence being translated into per-token embeddings via ESM2, each per-token embedding is then translated into a smaller representation space by the MLP trained with contrastive loss (CL).</title><p>Each residue from the CL model is then predicted as catalytic or not catalytic via a bidirectional LSTM network.</p></caption>
<graphic xlink:href="659624v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2g">
<title>Squidly ensemble and uncertainty quantification</title>
<p>For each training dataset, five models were ensembled by taking the per-residue output from the LSTM and then combining these to compute the mean score, along with the predictive entropy for each residue<sup><xref ref-type="bibr" rid="c41">41</xref></sup>. Users can specify a threshold for classifying a residue as catalytic (between 0 to 1.0): if the mean score exceeds this threshold, the residue is designated as “catalytic”. Users can optionally use the uncertainty to further filter for confidence in predictions.</p>
</sec>
<sec id="s2h">
<title>BLAST similarity-based search</title>
<p>BLAST is a local alignment search tool able to identify homologous sequences from a reference database<sup><xref ref-type="bibr" rid="c42">42</xref></sup>. For our study, we utilised diamond BLAST to search the training set for similar sequences to each test set <sup><xref ref-type="bibr" rid="c43">43</xref></sup>. The ultra-sensitive flag was used to identify sequences with low sequence similarities. Upon retrieval of results, the sequence with the highest sequence similarity was retained. This sequence was then aligned against the query using the alignment tool ClustalOmega <sup><xref ref-type="bibr" rid="c44">44</xref></sup>. The gapped alignment was used to infer the catalytic residues of the query sequence.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>ESM2 per-token embeddings contain catalytic residue specific variation</title>
<p>To illustrate the information content of per-residue embeddings for catalytic residue differentiation, we performed principal component analysis (PCA) on equally sampled catalytic and non-catalytic residues from EC 3.1 and 2.7, <xref rid="fig3" ref-type="fig">Figure 3</xref>. For EC 3.1, the first two principal components explained 5.24% and 1.86% of the variance for all residues, respectively. Although the explained variance in PCs 1 and 2 is minimal, the separation in PCA space indicates that the largest two signatures of linear variation are related to catalytic roles, with visible clusters of catalytic (red-coloured) and non-catalytic (blue-coloured) residues, <xref rid="fig3" ref-type="fig">Figure 3</xref>. The separability from a linear transformation suggests that ESM2 per-residue embeddings could be used to capture catalytic-specific signals.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 3:</label>
<caption><title>Principal component analysis of per-residue embeddings from EC numbers 3.1 and 2.7.</title><p>In the upper row, red colours indicate a catalytic residue, and blue, non-catalytic. In the lower row, residues are coloured by amino acid class which represent their structural and chemical properties. The x and y axes represent the first and second principal components in each subplot. A clear separation of catalytic and non-catalytic residues is seen in the principal components which explain the most variance in our data.</p></caption>
<graphic xlink:href="659624v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3b">
<title>Reaction informed pair-mining improves upon contrastive learning and state of the art prediction performance</title>
<p>We hypothesised that contrastive learning would present an effective approach to leverage the rich feature space of ESM2 embeddings to distinguish between catalytic and non-catalytic roles. We implemented a biologically informed hierarchical contrastive learning pair selection framework to select pairs which are informative at the per-residue level. Three datasets were created to compare the performance of different pair scheme selection criteria (see Methods for details).</p>
</sec>
<sec id="s3c">
<title>Squidly performance on the Uni3175 dataset</title>
<p>Scheme 1, see <xref rid="fig1" ref-type="fig">Figure 1</xref>, is a “control” scheme and uses a standard contrastive loss function with random pair selection. We observed that scheme 1 performs poorly overall on the Uni3175 dataset, <xref rid="fig4" ref-type="fig">Figure 4</xref>. Across all datasets, Scheme 1 achieved relatively low F1 scores, with a mean F1 score of 0.49 and 0.43 for the 3B and 15B ESM2 models. Scheme 1 failed to surpass the LSTM classification model using unchanged ESM2 embeddings, which had a mean F1 score of 0.73, <xref rid="fig4" ref-type="fig">Figure 4</xref>. Schemes 2 and 3, both of which employ reaction-informed pair-mining, exhibited improved performance relative to both Scheme 1 and the ESM2 baseline. Scheme 2 achieved mean F1 scores of 0.80 and 0.82 using the 3B and 15B ESM2 models, respectively. Scheme 3 improves upon scheme 2 by contrasting catalytic and non-catalytic sites based on distinct EC and amino acid labels, <xref rid="fig1" ref-type="fig">Figure 1</xref>. This resulted in mean F1 scores of 0.79 and 0.84. The best performing model had an F1 score of 0.86 achieved using Scheme 3 and the 15B ESM2 model.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 4:</label>
<caption><title>Comparison of the Squidly contrastive learning framework pair schemes on the Uni3175 dataset.</title><p>We directly compare the schemes with AEGAN, the tool by Shen et al., the authors of this dataset <sup><xref ref-type="bibr" rid="c10">10</xref></sup>. Schemes 2 and 3 (S1 and S2) using the largest ESM2 model (15 billion parameters) embeddings slightly improved upon AEGAN’s performance. The smaller ESM2 (3 billion parameters) model still performs competitively, at a heavily reduced computation cost. In comparison to the rationally informed pair schemes, the Scheme 1 and raw embedding LSTM models performed poorly.</p></caption>
<graphic xlink:href="659624v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The results from Uni3175 benchmark show that Squidly improved marginally upon the performance of previous work and the current SOTA ML prediction model AEGAN by Shen et al. <sup><xref ref-type="bibr" rid="c10">10</xref></sup> Squidly uses only sequence through ESM2 embeddings for prediction, which, compared to the homology and structural modalities included in AEGAN, makes for a more flexible predictor without compromising performance. Overall, the results on the Uni3175 dataset indicate that the rationally informed pair-mining strategy is an effective method for discriminative tasks using per-token ESM2 embeddings. It also indicates that sequence-based models using PLM embeddings can compete with models using structural modalities. See Methods for details on each scheme design.</p>
</sec>
<sec id="s3d">
<title>Squidly outperforms other ML-based methods yet works best in conjunction with sequence similarity methods</title>
<p>Squidly was evaluated against six previously reported benchmark datasets, namely the EF fold, family, and superfamily benchmarks, as well as the HA superfamily, NN, and PC benchmarks, see <xref rid="tblS1" ref-type="table">Supplementary Table 1</xref> and Appendix 3 for description of each dataset, see <xref rid="fig5" ref-type="fig">Figure 5</xref> for sequence identity between test and train sets. Squidly’s performance exceeded those of SOTA models AEGAN and SCREEN, <xref rid="tbl1" ref-type="table">Table 1</xref>. When compared to the baseline of BLAST, we find that BLAST outperforms all other ML models except Squidly, likely due to the homologous sequences in the test and train sets, see Appendix A3, and <xref rid="tblS2" ref-type="table">Supplementary Table 2</xref> for details.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 5:</label>
<caption><p>Sequence similarity based on sequence identity to the closest sequence in the training set between each family.</p></caption>
<graphic xlink:href="659624v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>F1 scores of tools on 6 common benchmark datasets.</title></caption>
<graphic xlink:href="659624v1_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>We hypothesised that BLAST would work best in instances where a similar sequence existed while Squidly would excel in low sequence similarity situations. To test for the optimal conditions for integration we varied the rate at which either predictor was used, <xref rid="fig6" ref-type="fig">Figure 6</xref>. For sequences with low sequence identity (&lt;50%), using Squidly improves the F1 score. However, Squidly has a higher likelihood than BLAST to record false positives where similar sequences exist while BLAST is more precise. The cutoff of 50% sequence identity was used to compare the performance of the ensemble method to existing tools. On all datasets using this cutoff an ensemble of BLAST and Squidly performs SOTA, <xref rid="tbl1" ref-type="table">Table 1</xref>.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 6:</label>
<caption><title>F1, recall, and precision for the ensemble of Squidly and BLAST with different sequence identity cut-offs.</title><p>The cut-offs indicate the point at which to transition from a Squidly to a BLAST prediction. At 100% only Squidly is used for predictions, and at 0%, BLAST is used, unless there are no homologous sequences. For sequences with less than 50% sequence identity (dashed black line), Squidly predictions are used. The dashed horizontal line represents the total score for BLAST on the specific dataset, while the dots represent the combined BLAST and Squidly ensemble approach. Squidly reports higher scores for recall, while BLAST is more precise. The performance is best when used as an ensemble.</p></caption>
<graphic xlink:href="659624v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3e">
<title>Squidly is scalable and fast</title>
<p>Squidly’s sequence-based approach reduces the overall computational cost of prediction to enable the screening of large databases. SOTA ML models rely on accurate structures as input, and thus structural prediction must be done prior to inference when screening databases such as metagenomic samples. Squidly’s smaller 3B model can be run locally and can predict roughly 6 sequences a second in our tests using an NVIDIA H100 GPU. In comparison to the current widely used structural generation tool Chai <sup><xref ref-type="bibr" rid="c45">45</xref></sup>, Squidly is approximately 50-fold faster (e.g. N=108, Squidly=108s, Chai=5757s). Note this under-represents the time for the structural tools to run, as they also require predictions which also add time, <xref rid="tbl2" ref-type="table">Table 2</xref>.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>Resource usage measurements for Squidly models when predicting 1000 sequences with an average length of 401 residues.</title></caption>
<graphic xlink:href="659624v1_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s3f">
<title>Squidly generalises to low identity sequences</title>
<p>To evaluate the capacity of Squidly to generalise to low identity sequences, we developed a benchmark dataset, CataloDB with low structural and sequence identity, see Methods and <xref rid="fig7" ref-type="fig">Figure 7</xref>. A, B for similarities between the test and train sets. This benchmark serves as an alternative test set that takes structural and sequence similarity into account, allowing for fair comparison between future structural and sequence-based tools and testing in low identity settings. Squidly 3B and 15B models showed similar performance on the benchmark with most sequences (N=148/232, N=154/232), recording at least one correct catalytic residue. For comparison, BLAST identifies only 68 sequences that have a catalytic residue recovered correctly. Both Squidly models performed similarly with F1 scores of (0.64, 0.68), precision of (0.80, 0.82), and recall of (0.53, 0.58) for the 15B and 3B models (respectively), while BLAST records an F1 of 0.37, recall of 0.24, and precision of 0.72, see <xref rid="fig8" ref-type="fig">Figure 8</xref>. We also retrain SCREEN, with performances recorded above BLAST however still below Squidly, <xref rid="fig7" ref-type="fig">Figure 7</xref>. C, see Methods for details. Reflecting the bias of public data, CataloDB has a bias towards well-represented EC numbers and amino acids. For example, EC 6 was represented by only three sequences in the test data; and EC 7 had no representation. Overall, these results suggest that Squidly can generalise to low identity settings and be used in predictions where traditional homology transfer is not possible. However, caution should be taken when applied to poorly represented enzyme classes such as EC 6 and 7.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 7:</label>
<caption><title>Distribution of the catalytic residues in the proposed benchmark dataset.</title><p>A. The distribution of the identity of the catalytic residues is not even and represents the uneven distribution of the catalytic mechanism within the benchmark dataset. B Almost 50% of the data points are in the well characterised class of EC 3, which are hydrolases. Neither EC 7 nor EC 6 is represented, which are translocases and ligases, respectively.</p></caption>
<graphic xlink:href="659624v1_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 8:</label>
<caption><p>A. BLAST sequence similarity to the training set for low identity CataloDB test set sequences. B. Structural similarity for the final test set, filtered on both structure and sequence. C. F1, recall, and precision for Squidly, SCREEN and BLAST on CataloDB.</p></caption>
<graphic xlink:href="659624v1_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>Rationally constraining pair generation in contrastive learning can improve model performance on biological data. Using publicly available EC number annotations and amino acid sequences, we categorised pairs of catalytic residues into distinct groups enriched with hard negatives to enable sequence-only prediction of catalytic residues. This approach led to substantial improvements compared to a standard LSTM classifier and typical pair-mining strategies. Squidly significantly outperformed other ML-based sequence methods and showed marginal improvement to SOTA ML-based catalytic residue prediction models that use structural data while running in a fraction of the time. Furthermore, Squidly performs comparably to BLAST when homologous sequences exist, yet also shows good performance in low-identity settings, where BLAST is unable to identify any similar sequences. Given the complementarity of BLAST and Squidly, we provide Squidly as an ensemble with sequence similarity, leveraging the interpretability of sequence similarity-based methods where homologous sequences exist, and Squidly, in out-of-distribution settings. The focus of this work was catalytic site prediction; however, of similar utility are combined binding and catalytic prediction models, such as EasIFA <sup><xref ref-type="bibr" rid="c11">11</xref></sup>, and therefore future work should additionally consider binding sites.</p>
<p>A limitation of this study is the inherent bias in publicly available data towards well studied enzyme classes. The bias likely confounds the perceived performance on less studied mechanisms, and in these instances the results should be considered with caution, for example, in EC classes 6 and 7. A further challenge is the lack of a consolidated benchmark for catalytic residue prediction, making direct and fair comparison between tools difficult. Notably, the multiclass classification objective and benchmarks used to evaluate EasIFA <sup><xref ref-type="bibr" rid="c11">11</xref></sup> made it infeasible to compare performance for the binary catalytic residue prediction task. Existing benchmark datasets do not capture the diversity of enzyme catalysis. Although we have provided a new benchmark that improves upon previous datasets by using structural identity filtering, the core issues remain. CataloDB and current benchmarks are small and include entire enzyme families. While low sequence and structural similarity samples are held out from training, they may not accurately represent the generalisability of these methods in out-of-distribution settings. Furthermore, the datasets often consist of sequences with known structures in the PDB, many of which were likely part of the AlphaFold training set or structurally similar clusters represented in the set. Tools also evaluate their performance using known, rather than predicted structures <sup><xref ref-type="bibr" rid="c10">10</xref></sup>. Consequently, SOTA tools that rely on AlphaFold predicted structures tend to perform better in these benchmarks but are likely to underperform in practical applications involving truly unique and uncharacterised sequences. This issue also applies, albeit to a lesser extent, to the method proposed in this study, which utilises ESM2 representations. While ESM2’s training set is more diverse and comprehensive, it may not generalise to distant out-of-distribution sequences compared to the well studied sequences used in the benchmarks.</p>
<p>Squidly and other data-driven approaches to catalytic residue prediction should be used with caution when applied to retrieve understudied catalytic mechanisms. Currently, there exists a limited set of experimentally validated annotations <sup><xref ref-type="bibr" rid="c13">13</xref></sup>. Datasets such as Uni14230 and CataloDB used for training in this study include SwissProt sequences with reviewed computational annotations to increase the size of the training data. However, this does not improve the diversity of catalytic mechanisms and enzyme sequences, as these methods reflect the inherent biases in databases towards well represented EC classes. Squidly, having learnt from this biased data, may indeed replicate this bias during inference and is therefore less likely to provide insights into novel or understudied catalytic mechanisms. Despite the limited data for catalytic residue prediction, we show that an ensemble approach of machine learning and sequence-similarity-based methods performs SOTA across multiple benchmark sets. By leveraging contrastive learning, we find that ML sequence-based predictions can generalize to low sequence/structural similarity settings. Finally, the methods reported in this paper that leverage rational data engineering for contrastive learning have broad implications for learning across biological data. Biological sequences are generated through evolutionary processes occurring over billions of years, resulting in hierarchically structured data that has significant variance between evolutionarily distant sequences. Moreover, mechanisms in biochemistry are often similar for related chemical reactions. By leveraging ontologies such as EC numbers, pairs can be selected rationally to improve training on proteins that are not only evolutionarily related but also share broader biochemical functions. A wide range of alternative ontologies exist that can capture different structures within biological data, suggesting that this approach is likely to be applicable across other biological classification tasks, particularly in studies utilising contrastive frameworks for per-token in-sequence classification <sup><xref ref-type="bibr" rid="c46">46</xref></sup>. Future studies should explore and compare additional ontologies to better understand the utility of such data structures.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>A.M. is supported by the Schmidt Science Fellows, in partnership with the Rhodes Trust. W.R. is supported by the Australian Government Research Training Program (RTP) Scholarship. The authors thank Charlie Trimble for his generous contributions to the project.</p>
</ack>
<sec>
<title>Additional information</title>
<sec id="s5" sec-type="data-availability">
<title>Code and data availability</title>
<p>All source code is available at Github (<ext-link ext-link-type="uri" xlink:href="https://github.com/WRiegs/Squidly">https://github.com/WRiegs/Squidly</ext-link>). All associated data including CataloDB is available at Zenodo (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.15541320">https://doi.org/10.5281/zenodo.15541320</ext-link>).</p>
</sec>
<sec id="s7">
<title>Author contributions statement</title>
<p>W.R. and A.M. conceived the experiment(s), W.R. designed and implemented the model, W.R. and A.M. ran the experiment(s), analysed the results, and wrote the manuscript. M.B. and F.H.A reviewed the manuscript and provided input to the research.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>(1)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reisenbauer</surname>, <given-names>J. C.</given-names></string-name>; <string-name><surname>Sicinski</surname>, <given-names>K. M.</given-names></string-name>; <string-name><surname>Arnold</surname>, <given-names>F. H</given-names></string-name></person-group>. <article-title>Catalyzing the Future: Recent Advances in Chemical Synthesis Using Enzymes</article-title>. <source>Current Opinion in Chemical Biology</source> <year>2024</year>, <volume>83</volume>, <fpage>102536</fpage>. <pub-id pub-id-type="doi">10.1016/j.cbpa.2024.102536</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>(2)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ribeiro</surname>, <given-names>A. J. M.</given-names></string-name>; <string-name><surname>Tyzack</surname>, <given-names>J. D.</given-names></string-name>; <string-name><surname>Borkakoti</surname>, <given-names>N.</given-names></string-name>; <string-name><surname>Holliday</surname>, <given-names>G. L.</given-names></string-name>; <string-name><surname>Thornton</surname>, <given-names>J. M</given-names></string-name></person-group>. <article-title>A Global Analysis of Function and Conservation of Catalytic Residues in Enzymes</article-title>. <source>Journal of Biological Chemistry</source> <year>2020</year>, <volume>295</volume> (<issue>2</issue>), <fpage>314</fpage>–<lpage>324</lpage>. <pub-id pub-id-type="doi">10.1074/jbc.REV119.006289</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>(3)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Precord</surname>, <given-names>T. W.</given-names></string-name>; <string-name><surname>Ramesh</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Dommaraju</surname>, <given-names>S. R.</given-names></string-name>; <string-name><surname>Harris</surname>, <given-names>L. A.</given-names></string-name>; <string-name><surname>Kille</surname>, <given-names>B. L.</given-names></string-name>; <string-name><surname>Mitchell</surname>, <given-names>D. A</given-names></string-name></person-group>. <article-title>Catalytic Site Proximity Profiling for Functional Unification of Sequence-Diverse Radical S-Adenosylmethionine Enzymes</article-title>. <source>ACS Bio &amp; Med Chem Au</source> <year>2023</year>, <volume>3</volume> (<issue>3</issue>), <fpage>240</fpage>–<lpage>251</lpage>. <pub-id pub-id-type="doi">10.1021/acsbiomedchemau.2c00085</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>(4)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martínez-Martínez</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Coscolín</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Santiago</surname>, <given-names>G.</given-names></string-name>; <string-name><surname>Chow</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Stogios</surname>, <given-names>P. J.</given-names></string-name>; <string-name><surname>Bargiela</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Gertler</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Navarro-Fernández</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Bollinger</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Thies</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Méndez-García</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Popovic</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Brown</surname>, <given-names>G.</given-names></string-name>; <string-name><surname>Chernikova</surname>, <given-names>T. N.</given-names></string-name>; <string-name><surname>García-Moyano</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Bjerga</surname>, <given-names>G. E. K.</given-names></string-name>; <string-name><surname>Pérez-García</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Hai</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Del Pozo</surname>, <given-names>M. V.</given-names></string-name>; <string-name><surname>Stokke</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Steen</surname>, <given-names>I. H.</given-names></string-name>; <string-name><surname>Cui</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Xu</surname>, <given-names>X.</given-names></string-name>; <string-name><surname>Nocek</surname>, <given-names>B. P.</given-names></string-name>; <string-name><surname>Alcaide</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Distaso</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Mesa</surname>, <given-names>V.</given-names></string-name>; <string-name><surname>Peláez</surname>, <given-names>A. I.</given-names></string-name>; <string-name><surname>Sánchez</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Buchholz</surname>, <given-names>P. C. F.</given-names></string-name>; <string-name><surname>Pleiss</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Fernández-Guerra</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Glöckner</surname>, <given-names>F. O.</given-names></string-name>; <string-name><surname>Golyshina</surname>, <given-names>O. V.</given-names></string-name>; <string-name><surname>Yakimov</surname>, <given-names>M. M.</given-names></string-name>; <string-name><surname>Savchenko</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Jaeger</surname>, <given-names>K.-E.</given-names></string-name>; <string-name><surname>Yakunin</surname>, <given-names>A. F.</given-names></string-name>; <string-name><surname>Streit</surname>, <given-names>W. R.</given-names></string-name>; <string-name><surname>Golyshin</surname>, <given-names>P. N.</given-names></string-name>; <string-name><surname>Guallar</surname>, <given-names>V.</given-names></string-name>; <string-name><surname>Ferrer</surname>, <given-names>M</given-names></string-name></person-group>.<article-title>; The INMARE Consortium. Determinants and Prediction of Esterase Substrate Promiscuity Patterns</article-title>. <source>ACS Chem. Biol</source>. <year>2018</year>, <volume>13</volume> (<issue>1</issue>), <fpage>225</fpage>–<lpage>234</lpage>. <pub-id pub-id-type="doi">10.1021/acschembio.7b00996</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>(5)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>Z.</given-names></string-name>; <string-name><surname>Yin</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Lee</surname>, <given-names>J. S.</given-names></string-name>; <string-name><surname>Parasuram</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Somarowthu</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Ondrechen</surname>, <given-names>M. J</given-names></string-name></person-group>. <article-title>Protein Function Annotation with Structurally Aligned Local Sites of Activity (SALSAs)</article-title>. <source>BMC Bioinformatics</source> <year>2013</year>, <volume>14</volume> (<issue>3</issue>), <fpage>S13</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-14-S3-S13</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>(6)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Ma</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Dalby</surname>, <given-names>P. A</given-names></string-name></person-group>. <article-title>Hot Spots-Making Directed Evolution Easier</article-title>. <source>Biotechnology Advances</source> <year>2022</year>, <volume>56</volume>, <fpage>107926</fpage>. <pub-id pub-id-type="doi">10.1016/j.biotechadv.2022.107926</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>(7)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Ahern</surname>, <given-names>W.</given-names></string-name>; <string-name><surname>Yim</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Tischer</surname>, <given-names>D.</given-names></string-name>; <string-name><surname>Salike</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Woodbury</surname>, <given-names>S. M.</given-names></string-name>; <string-name><surname>Kim</surname>, <given-names>D.</given-names></string-name>; <string-name><surname>Kalvet</surname>, <given-names>I.</given-names></string-name>; <string-name><surname>Kipnis</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Coventry</surname>, <given-names>B.</given-names></string-name>; <string-name><surname>Altae-Tran</surname>, <given-names>H. R.</given-names></string-name>; <string-name><surname>Bauer</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Barzilay</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Jaakkola</surname>, <given-names>T. S.</given-names></string-name>; <string-name><surname>Krishna</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Baker</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Atom Level Enzyme Active Site Scaffolding Using RFdiffusion2</article-title>. <source>bioRxiv</source> <year>2025</year></mixed-citation></ref>
<ref id="c8"><label>(8)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Zhang</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Chen</surname>, <given-names>K.</given-names></string-name>; <string-name><surname>Shen</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Ruan</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Kurgan</surname>, <given-names>L</given-names></string-name></person-group>. <article-title>Accurate Sequence-Based Prediction of Catalytic Residues</article-title>. <source>Bioinformatics</source> <year>2008</year>, <volume>24</volume> (<issue>20</issue>), <fpage>2329</fpage>–<lpage>2338</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btn433</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>(9)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pan</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Bi</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>; <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Webb</surname>, <given-names>G. I.</given-names></string-name>; <string-name><surname>Gasser</surname>, <given-names>R. B.</given-names></string-name>; <string-name><surname>Kurgan</surname>, <given-names>L.</given-names></string-name>; <string-name><surname>Song</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>SCREEN: A Graph-Based Contrastive Learning Tool to Infer Catalytic Residues and Assess Enzyme Mutations</article-title>. <source>Genomics, Proteomics &amp; Bioinformatics</source> <year>2024</year>, <fpage>qzae094</fpage>. <pub-id pub-id-type="doi">10.1093/gpbjnl/qzae094</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>(10)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname>, <given-names>X.</given-names></string-name>; <string-name><surname>Zhang</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Long</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Chen</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Wang</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Cui</surname>, <given-names>Z.</given-names></string-name>; <string-name><surname>Chen</surname>, <given-names>B.</given-names></string-name>; <string-name><surname>Tan</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>A Highly Sensitive Model Based on Graph Neural Networks for Enzyme Key Catalytic Residue Prediction</article-title>. <source>Journal of Chemical Information and Modeling</source> <year>2023</year>, <volume>63</volume> (<issue>14</issue>), <fpage>4277</fpage>–<lpage>4290</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.3c00273</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>(11)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>; <string-name><surname>Yin</surname>, <given-names>X.</given-names></string-name>; <string-name><surname>Jiang</surname>, <given-names>D.</given-names></string-name>; <string-name><surname>Zhao</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Wu</surname>, <given-names>Z.</given-names></string-name>; <string-name><surname>Zhang</surname>, <given-names>O.</given-names></string-name>; <string-name><surname>Wang</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Deng</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Liu</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Luo</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Han</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Hou</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Yao</surname>, <given-names>X.</given-names></string-name>; <string-name><surname>Hsieh</surname>, <given-names>C.-Y</given-names></string-name></person-group>. <article-title>Multi-Modal Deep Learning Enables Efficient and Accurate Annotation of Enzymatic Active Sites</article-title>. <source>Nat Commun</source> <year>2024</year>, <volume>15</volume> (<issue>1</issue>), <fpage>7348</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-024-51511-6</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>(12)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Consortium</surname>, <given-names>T. U</given-names></string-name></person-group>. <article-title>UniProt: The Universal Protein Knowledgebase in 2023</article-title>. <source>Nucleic Acids Research</source> <year>2022</year>, <volume>51</volume> (<issue>D1</issue>), <fpage>D523</fpage>–<lpage>D531</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkac1052</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>(13)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ribeiro</surname>, <given-names>A. J. M.</given-names></string-name>; <string-name><surname>Holliday</surname>, <given-names>G. L.</given-names></string-name>; <string-name><surname>Furnham</surname>, <given-names>N.</given-names></string-name>; <string-name><surname>Tyzack</surname>, <given-names>J. D.</given-names></string-name>; <string-name><surname>Ferris</surname>, <given-names>K.</given-names></string-name>; <string-name><surname>Thornton</surname>, <given-names>J. M</given-names></string-name></person-group>. <article-title>Mechanism and Catalytic Site Atlas (M-CSA): A Database of Enzyme Reaction Mechanisms and Active Sites</article-title>. <source>Nucleic Acids Research</source> <year>2018</year>, <volume>46</volume> (<issue>D1</issue>), <fpage>D618</fpage>–<lpage>D623</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkx1012</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>(14)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glaser</surname>, <given-names>F.</given-names></string-name>; <string-name><surname>Pupko</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Paz</surname>, <given-names>I.</given-names></string-name>; <string-name><surname>Bell</surname>, <given-names>R. E.</given-names></string-name>; <string-name><surname>Bechor-Shental</surname>, <given-names>D.</given-names></string-name>; <string-name><surname>Martz</surname>, <given-names>E.</given-names></string-name>; <string-name><surname>Ben-Tal</surname>, <given-names>N</given-names></string-name></person-group>. <article-title>ConSurf: Identification of Functional Regions in Proteins by Surface-Mapping of Phylogenetic Information</article-title>. <source>Bioinformatics</source> <year>2003</year>, <volume>19</volume> (<issue>1</issue>), <fpage>163</fpage>–<lpage>164</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/19.1.163</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>(15)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mayrose</surname>, <given-names>I.</given-names></string-name>; <string-name><surname>Graur</surname>, <given-names>D.</given-names></string-name>; <string-name><surname>Ben-Tal</surname>, <given-names>N.</given-names></string-name>; <string-name><surname>Pupko</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>Comparison of Site-Specific Rate-Inference Methods for Protein Sequences: Empirical Bayesian Methods Are Superior</article-title>. <source>Molecular Biology and Evolution</source> <year>2004</year>, <volume>21</volume> (<issue>9</issue>), <fpage>1781</fpage>–<lpage>1791</lpage>. <pub-id pub-id-type="doi">10.1093/molbev/msh194</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>(16)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>del Sol Mesa</surname></string-name>, <string-name><given-names>F.</given-names>; <surname>Pazos</surname></string-name>, <string-name><given-names>A.</given-names>; <surname>Valencia</surname></string-name></person-group>, <article-title>Automatic Methods for Predicting Functionally Important Residues</article-title>. <source>Journal of Molecular Biology</source> <year>2003</year>, <volume>326</volume> (<issue>4</issue>), <fpage>1289</fpage>–<lpage>1302</lpage>. <pub-id pub-id-type="doi">10.1016/S0022-2836(02)01451-1</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>(17)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Song</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Li</surname>, <given-names>F.</given-names></string-name>; <string-name><surname>Takemoto</surname>, <given-names>K.</given-names></string-name>; <string-name><surname>Haffari</surname>, <given-names>G.</given-names></string-name>; <string-name><surname>Akutsu</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Chou</surname>, <given-names>K.-C.</given-names></string-name>; <string-name><surname>Webb</surname>, <given-names>G. I</given-names></string-name></person-group>. <article-title>PREvaIL, an Integrative Approach for Inferring Catalytic Residues Using Sequence, Structural, and Network Features in a Machine-Learning Framework</article-title>. <source>Journal of Theoretical Biology</source> <year>2018</year>, <volume>443</volume>, <fpage>125</fpage>–<lpage>137</lpage>. <pub-id pub-id-type="doi">10.1016/j.jtbi.2018.01.023</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>(18)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jumper</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Evans</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Pritzel</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Green</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Figurnov</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Ronneberger</surname>, <given-names>O.</given-names></string-name>; <string-name><surname>Tunyasuvunakool</surname>, <given-names>K.</given-names></string-name>; <string-name><surname>Bates</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Žídek</surname>, <given-names>A</given-names></string-name>.; <string-name><surname>Potapenko</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>Highly Accurate Protein Structure Prediction with AlphaFold</article-title>. <source>Nature</source> <year>2021</year>, <volume>596</volume> (<issue>7873</issue>), <fpage>583</fpage>–<lpage>589</lpage>.</mixed-citation></ref>
<ref id="c19"><label>(19)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Tule</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Foley</surname>, <given-names>G.</given-names></string-name>; <string-name><surname>Boden</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Do Protein Language Models Learn Phylogeny?</article-title> <source>bioRxiv</source>, <year>2024</year>, p 2024.09.23.614642. <pub-id pub-id-type="doi">10.1101/2024.09.23.614642</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>(20)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Rao</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Meier</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Sercu</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Ovchinnikov</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Rives</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>Transformer Protein Language Models Are Unsupervised Structure Learners</article-title>. <source>bioRxiv</source> <year>2020</year>, 2020.12.15.422761. <pub-id pub-id-type="doi">10.1101/2020.12.15.422761</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>(21)</label><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Vig</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Madani</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Varshney</surname>, <given-names>L. R.</given-names></string-name>; <string-name><surname>Xiong</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Socher</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Rajani</surname>, <given-names>N. F.</given-names></string-name></person-group> <article-title>BERTology Meets Biology: Interpreting Attention in Protein Language Models</article-title>, <year>2021</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2006.15222">https://arxiv.org/abs/2006.15222</ext-link>.</mixed-citation></ref>
<ref id="c22"><label>(22)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Heinzinger</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Weissenow</surname>, <given-names>K.</given-names></string-name>; <string-name><surname>Sanchez</surname>, <given-names>J. G.</given-names></string-name>; <string-name><surname>Henkel</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Mirdita</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Steinegger</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Rost</surname>, <given-names>B</given-names></string-name></person-group>. <article-title>Bilingual Language Model for Protein Sequence and Structure</article-title>. <source>bioRxiv</source> <year>2024</year>, 2023.07.23.550085. <pub-id pub-id-type="doi">10.1101/2023.07.23.550085</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>(23)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname>, <given-names>Z.</given-names></string-name>; <string-name><surname>Akin</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Rao</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Hie</surname>, <given-names>B.</given-names></string-name>; <string-name><surname>Zhu</surname>, <given-names>Z.</given-names></string-name>; <string-name><surname>Lu</surname>, <given-names>W.</given-names></string-name>; <string-name><surname>Smetanin</surname>, <given-names>N.</given-names></string-name>; <string-name><surname>Verkuil</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Kabeli</surname>, <given-names>O.</given-names></string-name>; <string-name><surname>Shmueli</surname>, <given-names>Y</given-names></string-name>.; <string-name><surname>dos Santos Costa</surname>, <given-names>A</given-names></string-name>.; <string-name><surname>Fazel-Zarandi</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Sercu</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Candido</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Rives</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Evolutionary-Scale Prediction of Atomic-Level Protein Structure with a Language Model</article-title>. <source>Science</source> <year>2023</year>, <volume>379</volume> (<issue>6637</issue>), <fpage>1123</fpage>–<lpage>1130</lpage>. <pub-id pub-id-type="doi">10.1126/science.ade2574</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>(24)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Kulikova</surname>, <given-names>A. V.</given-names></string-name>; <string-name><surname>Parker</surname>, <given-names>J. K.</given-names></string-name>; <string-name><surname>Davies</surname>, <given-names>B. W.</given-names></string-name>; <string-name><surname>Wilke</surname>, <given-names>C. O</given-names></string-name></person-group>. <article-title>Semantic Search Using Protein Large Language Models Detects Class II Microcins in Bacterial Genomes</article-title>. <source>bioRxiv</source> <year>2023</year>, 2023.11.15.567263. <pub-id pub-id-type="doi">10.1101/2023.11.15.567263</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>(25)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McWhite</surname>, <given-names>C. D.</given-names></string-name>; <string-name><surname>Armour-Garb</surname>, <given-names>I.</given-names></string-name>; <string-name><surname>Singh</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Leveraging Protein Language Models for Accurate Multiple Sequence Alignments</article-title>. <source>Genome Research</source> <year>2023</year>, <volume>33</volume> (<issue>7</issue>), <fpage>1145</fpage>–<lpage>1153</lpage>. <pub-id pub-id-type="doi">10.1101/gr.277675.123</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>(26)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Mikhael</surname>, <given-names>P. G.</given-names></string-name>; <string-name><surname>Chinn</surname>, <given-names>I.</given-names></string-name>; <string-name><surname>Barzilay</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>CLIPZyme: Reaction-Conditioned Virtual Screening of Enzymes</article-title>. <source>arXiv</source> <month>February</month>, <year>2024</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2402.06748</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>(27)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Cui</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Li</surname>, <given-names>J. C.</given-names></string-name>; <string-name><surname>Luo</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Jiang</surname>, <given-names>G.</given-names></string-name>; <string-name><surname>Zhao</surname>, <given-names>H</given-names></string-name></person-group>. <article-title>Enzyme Function Prediction Using Contrastive Learning</article-title>. <source>Science</source> <year>2023</year>, <volume>379</volume> (<issue>6639</issue>), <fpage>1358</fpage>–<lpage>1363</lpage>. <pub-id pub-id-type="doi">10.1126/science.adf2465</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>(28)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Yang</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Mora</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Liu</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Wittmann</surname>, <given-names>B. J.</given-names></string-name>; <string-name><surname>Anandkumar</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Arnold</surname>, <given-names>F. H.</given-names></string-name>; <string-name><surname>Yue</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>CARE: A Benchmark Suite for the Classification and Retrieval of Enzymes</article-title>. <source>arXiv</source>, <year>2025</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2406.15669</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>(29)</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Chopra</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Hadsell</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name></person-group> <article-title>Learning a Similarity Metric Discriminatively, with Application to Face Verification</article-title>. In <conf-name>2005 IEEE computer society conference on computer vision and pattern recognition</conf-name>, <year>2005</year>; Vol. <volume>1</volume>, pp <fpage>539</fpage>–<lpage>546</lpage>.</mixed-citation></ref>
<ref id="c30"><label>(30)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Le-Khac</surname>, <given-names>P. H.</given-names></string-name>; <string-name><surname>Healy</surname>, <given-names>G.</given-names></string-name>; <string-name><surname>Smeaton</surname>, <given-names>A. F</given-names></string-name></person-group>. <article-title>Contrastive Representation Learning: A Framework and Review</article-title>. <source>IEEE Access</source> <year>2020</year>, <volume>8</volume>, <fpage>193907</fpage>–<lpage>193934</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3031549</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>(31)</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kalantidis</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Sariyildiz</surname>, <given-names>M. B.</given-names></string-name>; <string-name><surname>Pion</surname>, <given-names>N.</given-names></string-name>; <string-name><surname>Weinzaepfel</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Larlus</surname>, <given-names>D.</given-names></string-name></person-group> <chapter-title>Hard Negative Mixing for Contrastive Learning</chapter-title>. In <source>Advances in Neural Information Processing Systems</source>; <person-group person-group-type="editor"><string-name><surname>Larochelle</surname>, <given-names>H</given-names></string-name>, <string-name><surname>Ranzato</surname>, <given-names>M</given-names></string-name>, <string-name><surname>Hadsell</surname>, <given-names>R</given-names></string-name>, <string-name><surname>Balcan</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>H</given-names></string-name></person-group>, Eds.; <publisher-name>Curran Associates, Inc.</publisher-name>, <year>2020</year>; Vol. <volume>33</volume>, pp <fpage>21798</fpage>–<lpage>21809</lpage>.</mixed-citation></ref>
<ref id="c32"><label>(32)</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Khosla</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Teterwak</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Wang</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Sarna</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Tian</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Isola</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Maschinot</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Liu</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Krishnan</surname>, <given-names>D.</given-names></string-name></person-group> <chapter-title>Supervised Contrastive Learning</chapter-title>. In <source>Advances in Neural Information Processing Systems</source>; <publisher-name>Curran Associates, Inc.</publisher-name>, <year>2020</year>; Vol. <volume>33</volume>, pp <fpage>18661</fpage>–<lpage>18673</lpage>.</mixed-citation></ref>
<ref id="c33"><label>(33)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Lin</surname>, <given-names>N.</given-names></string-name>; <string-name><surname>Qin</surname>, <given-names>G.</given-names></string-name>; <string-name><surname>Wang</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Yang</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Zhou</surname>, <given-names>D</given-names></string-name></person-group>. <article-title>An Effective Deployment of Contrastive Learning in Multi-Label Text Classification</article-title>. <source>arXiv</source> <year>2022</year>, <pub-id pub-id-type="doi">10.48550/arXiv.2212.00552</pub-id>.</mixed-citation></ref>
<ref id="c34"><label>(34)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDonald</surname>, <given-names>A. G.</given-names></string-name>; <string-name><surname>Tipton</surname>, <given-names>K. F</given-names></string-name></person-group>. <article-title>Enzyme Nomenclature and Classification: The State of the Art</article-title>. <source>The FEBS Journal</source> <year>2023</year>, <volume>290</volume> (<issue>9</issue>), <fpage>2214</fpage>–<lpage>2231</lpage>. <pub-id pub-id-type="doi">10.1111/febs.16274</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>(35)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Youn</surname>, <given-names>E.</given-names></string-name>; <string-name><surname>Peters</surname>, <given-names>B.</given-names></string-name>; <string-name><surname>Radivojac</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Mooney</surname>, <given-names>S. D</given-names></string-name></person-group>. <article-title>Evaluation of Features for Catalytic Residue Prediction in Novel Folds</article-title>. <source>Protein Science</source> <year>2007</year>, <volume>16</volume> (<issue>2</issue>), <fpage>216</fpage>–<lpage>226</lpage>. <pub-id pub-id-type="doi">10.1110/ps.062523907</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>(36)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chea</surname>, <given-names>E.</given-names></string-name>; <string-name><surname>Livesay</surname>, <given-names>D. R</given-names></string-name></person-group>. <article-title>How Accurate and Statistically Robust Are Catalytic Site Predictions Based on Closeness Centrality?</article-title> <source>BMC Bioinformatics</source> <year>2007</year>, <volume>8</volume> (<issue>1</issue>), <fpage>153</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-8-153</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>(37)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bartlett</surname>, <given-names>G. J.</given-names></string-name>; <string-name><surname>Porter</surname>, <given-names>C. T.</given-names></string-name>; <string-name><surname>Borkakoti</surname>, <given-names>N.</given-names></string-name>; <string-name><surname>Thornton</surname>, <given-names>J. M</given-names></string-name></person-group>. <article-title>Analysis of Catalytic Residues in Enzyme Active Sites</article-title>. <source>Journal of Molecular Biology</source> <year>2002</year>, <volume>324</volume> (<issue>1</issue>), <fpage>105</fpage>–<lpage>121</lpage>. <pub-id pub-id-type="doi">10.1016/S0022-2836(02)01036-7</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>(38)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petrova</surname>, <given-names>N. V.</given-names></string-name>; <string-name><surname>Wu</surname>, <given-names>C. H</given-names></string-name></person-group>. <article-title>Prediction of Catalytic Residues Using Support Vector Machine with Selected Protein Sequence and Structural Properties</article-title>. <source>BMC Bioinformatics</source> <year>2006</year>, <volume>7</volume> (<issue>1</issue>), <fpage>312</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-7-312</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>(39)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steinegger</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Söding</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>MMseqs2 Enables Sensitive Protein Sequence Searching for the Analysis of Massive Data Sets</article-title>. <source>Nature Biotechnology</source> <year>2017</year>, <volume>35</volume> (<issue>11</issue>), <fpage>1026</fpage>–<lpage>1028</lpage>. <pub-id pub-id-type="doi">10.1038/nbt.3988</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>(40)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Kempen</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Kim</surname>, <given-names>S. S.</given-names></string-name>; <string-name><surname>Tumescheit</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Mirdita</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Lee</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Gilchrist</surname>, <given-names>C. L. M.</given-names></string-name>; <string-name><surname>Söding</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Steinegger</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Fast and Accurate Protein Structure Search with Foldseek</article-title>. <source>Nat Biotechnol</source> <year>2024</year>, <volume>42</volume> (<issue>2</issue>), <fpage>243</fpage>–<lpage>246</lpage>. <pub-id pub-id-type="doi">10.1038/s41587-023-01773-0</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>(41)</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Lakshminarayanan</surname>, <given-names>B.</given-names></string-name>; <string-name><surname>Pritzel</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Blundell</surname>, <given-names>C.</given-names></string-name></person-group> <chapter-title>Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles</chapter-title>. In <source>Advances in Neural Information Processing Systems</source>; <publisher-name>Curran Associates, Inc.,</publisher-name> <year>2017</year>; Vol. <volume>30</volume>.</mixed-citation></ref>
<ref id="c42"><label>(42)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Altschul</surname>, <given-names>S. F.</given-names></string-name>; <string-name><surname>Gish</surname>, <given-names>W.</given-names></string-name>; <string-name><surname>Miller</surname>, <given-names>W.</given-names></string-name>; <string-name><surname>Myers</surname>, <given-names>E. W.</given-names></string-name>; <string-name><surname>Lipman</surname>, <given-names>D. J</given-names></string-name></person-group>. <article-title>Basic Local Alignment Search Tool</article-title>. <source>J Mol Biol</source> <year>1990</year>, <volume>215</volume> (<issue>3</issue>), <fpage>403</fpage>–<lpage>410</lpage>. <pub-id pub-id-type="doi">10.1016/S0022-2836(05)80360-2</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>(43)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buchfink</surname>, <given-names>B.</given-names></string-name>; <string-name><surname>Xie</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Huson</surname>, <given-names>D. H</given-names></string-name></person-group>. <article-title>Fast and Sensitive Protein Alignment Using DIAMOND</article-title>. <source>Nat Methods</source> <year>2015</year>, <volume>12</volume> (<issue>1</issue>), <fpage>59</fpage>–<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.3176</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>(44)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sievers</surname>, <given-names>F.</given-names></string-name>; <string-name><surname>Wilm</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Dineen</surname>, <given-names>D.</given-names></string-name>; <string-name><surname>Gibson</surname>, <given-names>T. J.</given-names></string-name>; <string-name><surname>Karplus</surname>, <given-names>K.</given-names></string-name>; <string-name><surname>Li</surname>, <given-names>W.</given-names></string-name>; <string-name><surname>Lopez</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>McWilliam</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Remmert</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Söding</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Thompson</surname>, <given-names>J. D.</given-names></string-name>; <string-name><surname>Higgins</surname>, <given-names>D. G.</given-names></string-name></person-group> <article-title>Fast, Scalable Generation of High- quality Protein Multiple Sequence Alignments Using Clustal Omega</article-title>. <source>Molecular Systems Biology</source> <year>2011</year>, <volume>7</volume> (<issue>1</issue>), <fpage>539</fpage>. <pub-id pub-id-type="doi">10.1038/msb.2011.75</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>(45)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><collab>Chai Discovery</collab></person-group>. <article-title>Chai-1: Decoding the Molecular Interactions of Life</article-title>. <source>bioRxiv</source> <year>2024</year>. <pub-id pub-id-type="doi">10.1101/2024.10.10.615955</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>(46)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Tian</surname>, <given-names>B</given-names></string-name></person-group>. <article-title>Protein-Small Molecule Binding Site Prediction Based on a Pre-Trained Protein Language Model with Contrastive Learning</article-title>. <source>Journal of Cheminformatics</source> <year>2024</year>, <volume>16</volume> (<issue>1</issue>), <fpage>125</fpage>. <pub-id pub-id-type="doi">10.1186/s13321-024-00920-2</pub-id>.</mixed-citation></ref>
</ref-list>
<app-group>
<title>Appendix</title>
<app id="app1">
<label>A.1</label>
<title>Rationale for developing CataloDB</title>
<p>To enable comparisons to existing work, we utilized the AEGAN training and test datasets as described above. However, the limitations identified motivated us to create a new benchmark, CataloDB, for future evaluations. The pre-existing benchmarks were published prior to 2007 and were developed by selecting proteins with active site annotations representing unique protein families, superfamilies, or folds. Their construction lacked standardized metrics for ensuring appropriate similarity separation between training and test data. The continued use of these historical benchmarks likely persists to maintain backward compatibility in comparisons with earlier methods. However, these six benchmarks predominantly represent well-studied folds and families, making it challenging to create properly separated training and test sets.</p>
<p>Our results demonstrate that BLAST already performs comparably or better when predicting catalytic residues in sequences with greater than 0.35 identity. Therefore, we believe machine learning methods should aim to develop tools that generalize effectively to low-identity sequences. CataloDB addresses this challenge, and the concerns by containing test sequences with strictly less than 0.3 sequence and structure identity to the training set. This benchmark is more extensive than previous collections while still allowing for a larger training set with a higher redundancy threshold (90%), enabling machine learning methods to better learn from the underlying data distribution.</p>
<sec id="s8a1">
<title>Evaluation of Swissprot data for use in CataloDB</title>
<p>To overcome the limited availability of experimentally validated catalytic residue annotations, annotations from sequences which have been reviewed by Swissprot’s expert review panel have been included in training and test data in recent machine learning tools <sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c10">10</xref></sup>. However, the assumption that these annotations are valid is not a given. Swissprot provides limited information to clarify how their review process for catalytic mechanisms is conducted <sup><xref ref-type="bibr" rid="c12">12</xref></sup>. These annotations may have been generated manually or by computational predictions and likely contain the same bias present in the original experimentally validated set. Here we present initial analysis done, using an earlier version of Squidly, to determine whether reviewed sequences improved performance, and thus, the quality of training data.</p>
<p>Three datasets were curated to evaluate the impact of reviewed sequences on training catalytic residue prediction models. Dataset 1 contained 2,209 sequences with only experimentally validated catalytic residue annotations. Dataset 2 included 6,437 sequences, including experimentally validated annotations and sequences with catalytic residues supported by known structures. Dataset 3 included 99,926 sequences comprising all catalytic residue annotations in the SwissProt database. Redundancy reduction removed sequences with over 90% sequence identity. After filtering, Dataset 1 was reduced to 2,030 sequences, Dataset 2 to 5,921 sequences, and Dataset 3 had the highest proportion of redundant sequences with only 57,698 sequences left after filtering. See <xref rid="tbl2" ref-type="table">Table 2</xref> for details.</p>
<p>Contrary to the expectation that increasing data size would improve model performance, we found that increasing the available training data by including reviewed sequences from SwissProt does not improve the overall bias or variance of the available experimental data. <xref rid="figS2" ref-type="fig">Figures S2</xref>, <xref rid="figS4" ref-type="fig">S4</xref> and <xref rid="figS6" ref-type="fig">S6</xref> show the EC distributions (EC numbers up to the second tier) across the datasets. All three datasets are biased toward hydrolases (EC 3), followed by classes 2, 1, 4, 5, 6, and 7. The relative abundance of EC class 2 increases between dataset 1, dataset 2, and dataset 3, while other classes remain relatively constant. This trend suggests that the additional sequences in the reviewed datasets likely originate from similarity-based prediction methods that incorporate experimental data, thus maintaining the relative abundance in EC representation despite increasing dataset size.</p>
<p><xref rid="figS1" ref-type="fig">Figures S1</xref>, <xref rid="figS3" ref-type="fig">S3</xref> and <xref rid="figS5" ref-type="fig">S5</xref> compare the distributions of amino acids acting as catalytic residues across the datasets. The distributions are dominated by histidine, aspartic acid, glutamic acid, cysteine, and serine, in line with the dominant classes being hydrolases. Dataset 2 and dataset 3 show a relative increase in aspartic acid abundance compared to dataset 1. Despite this, and the much larger size of datasets 2 and 3, their amino acid distributions closely resemble dataset 1, indicating that reviewed datasets contribute little diversity in rare catalytic residues, which may correspond to mechanisms that are less studied. Overall, these findings suggest that training the models using the additional data will not greatly improve the performance of the models when generalising to sequences with low identity in the experimental ground truth set. Based on our analyses we use dataset 2 for CataloDB.</p>
</sec>
</app>
<app id="app2">
<label>A.2</label>
<title>Reaction-informed pair schemes</title>
<p>By performing contrastive learning with pairs, we move from having too few samples to having too many samples. The large pool of amino acid pairs in our dataset varies in how informative each pair is for the model. Therefore, we must find the most effective ways to down sample and select the most informative pairs for our model to learn from. Particularly, we want to maximise the proportion of hard negative pairs in the training data.</p>
<p>The Uni14320 training dataset contains 8,735 non-redundant sequences from Dataset 1 (experimentally validated) with an average sequence length of 407 residues. Given the total pool of 3,555,145 residues, a total number of 6.32 × 10<sup>12</sup> unique pairwise comparisons can be made, as determined by the binomial coefficient <inline-formula><inline-graphic xlink:href="659624v1_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We developed pair schemes 2 and 3 to capture the most informative pairs from this available data. Maximising hard negatives improves the model’s ability to discriminate catalytic and non-catalytic residues in a more generalisable way. If two pairs of residue embeddings are inherently very distinct from one another, the model will be unlikely to learn much from them. This is not just because our loss function will penalise the model less during training, but because the relationships extracted from the feature space that are used to separate these two inputs might not necessarily be related to our primary objective.</p>
<p>Reaction informed pair-mining assumes that there must be key biological contexts which create harder negative pairs for the model to differentiate. First, we make the basic assumption that the model will be unable to correctly discriminate between similar amino acids pairs. It is likely that a contrastive model will find pairs individual amino acids, such as histidine, harder to contrast, because the unique structure and properties of each amino acid determine the catalytic or non-catalytic roles in which these amino acids play within enzymes. Therefore, we decided to implicitly include the amino acid character of each catalytic site into the pair sampling scheme.</p>
<p>Another assumption we made was inspired by the use of contrastive learning in another catalytic site prediction tool developed by Tong Pan et al. <sup><xref ref-type="bibr" rid="c9">9</xref></sup>. Their contrastive model creates EC specific representations of sequences for further use in a convolutional graph neural network classification model. Their contrastive model, however, did not try to use these contexts to better differentiate between catalytic and non-catalytic sites in their model, but as a support to ensure the information about the sequence EC is propagated through their multimodal system. EC numbers separate enzymes based on the reactions they perform. Not only that, but lower-level EC numbers group enzymes by the types of substrate bonds they cleave, or mechanisms by which they catalyse reactions. Inspired by this idea, we leveraged EC numbers to sample hard negative pairs. We specifically chose to represent sequences with only the first two levels of their EC-numbers so that there is sufficient variety in the data that represents each label.</p>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Table S1.</label>
<caption><title>Brief description of methods compared to Squidly in common benchmarks.</title></caption>
<graphic xlink:href="659624v1_tblS1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</app>
<app id="app3">
<label>A.3</label>
<title>Evaluation of Existing Benchmarks</title>
<p>The six standard benchmarks traditionally used for catalytic residue prediction exhibit methodological inconsistencies in the literature. In our comparative analysis, we discovered that state-of-the-art methods benchmarked in this study – specifically SCREEN and AEGAN utilized different independent subsets of these benchmarks, complicating direct performance comparisons. According to the SCREEN authors, data preparation involved clustering the EF family and M-CSA training data to less than 0.4 sequence identity, selecting unique cluster representatives for training and validation. The authors reported that they “excluded enzymes from these five test datasets from our training/validation dataset.”</p>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>Table S2.</label>
<caption><title>Benchmark datasets in prior work.</title></caption>
<graphic xlink:href="659624v1_tblS2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>Our analysis identified that up to 85.4% of the test sequences had greater than 0.9 identity to the training set in the already filtered subsets provided by the authors. Notably, the high sequence similarity appears reduced in the EF superfamily and EF fold datasets, likely resulting from the authors’ use of the closely related EF family dataset during training set curation. The test results published by SCREEN align with our observations, showing a marked increase in F1 score when comparing the HA, NN, and PC datasets with the EF datasets, corresponding to their differing levels of data leakage.</p>
<p>Additionally, the curation process employed in SCREEN for the training data appears unnecessarily stringent, with the average closest training sequence similarity measured at only 0.21. This sparse training data likely impacts machine learning models attempting to fit such data distributions effectively. We hypothesize this factor may explain why AEGAN reportedly performed poorly when retrained using this data in the SCREEN authors’ experiments.</p>
<fig id="figS1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1:</label>
<caption><title>Amino acid distribution of Catalytic Residues in Dataset 1.</title><p>Dataset 1 contains 3,873 catalytic residues, with 15 possible amino acids. The database is skewed towards certain amino acids which are commonly involved in catalysis.</p></caption>
<graphic xlink:href="659624v1_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2:</label>
<caption><title>Dataset 1 EC Distribution.</title><p>Dataset 1 is made up of 2,210 sequences from Swissprot. The sequences have experimental validation for the catalytic residue annotations. Validation sets are derived exclusively from this distribution. A clear bias exists for hydrolases (EC 3.X).</p></caption>
<graphic xlink:href="659624v1_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" orientation="portrait" fig-type="figure">
<label>Figure S3:</label>
<caption><title>Amino acid distribution of Catalytic Residues in Dataset 2.</title><p>Catalytic residue distribution. Dataset 2 contains 10,564 catalytic residues, with 18 possible amino acids. Amino acids M, V and F are additions not seen in dataset 1, although they exist in very few numbers.</p></caption>
<graphic xlink:href="659624v1_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS4" position="float" orientation="portrait" fig-type="figure">
<label>Figure S4:</label>
<caption><title>Dataset 3 EC Distribution.</title><p>Dataset 3 is made up of 48,625 sequences. The sequences are taken from all the available proteins with catalytic site annotations in Swissprot. A very similar distribution is seen between datasets 3 and 2, with a notable increase in EC 2 again.</p></caption>
<graphic xlink:href="659624v1_figS4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5" position="float" orientation="portrait" fig-type="figure">
<label>Figure S5:</label>
<caption><title>Amino acid distribution of Catalytic Residues in Dataset 3.</title><p>Dataset 3 contains 78,966 catalytic residues, with 19 possible catalytic amino acids. Isoleucine is an additional amino acid not seen in dataset 1 or 2. The distribution seen here is very similar to that of dataset 1 and 2.</p></caption>
<graphic xlink:href="659624v1_figS5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS6" position="float" orientation="portrait" fig-type="figure">
<label>Figure S6:</label>
<caption><title>Dataset 3 EC Distribution.</title><p>Dataset 3 is made up of 48,625 sequences. The sequences are taken from all the available proteins with catalytic site annotations in Swissprot. A very similar distribution is seen between datasets 3 and 2, with a notable increase in EC 2 again.</p></caption>
<graphic xlink:href="659624v1_figS6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108186.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Graña</surname>
<given-names>Martin</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Institut Pasteur de Montevideo</institution>
</institution-wrap>
<city>Montevideo</city>
<country>Uruguay</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>The authors make an <bold>important</bold> advance in enzyme annotation by fusing biochemical knowledge with language‑model-based learning to predict catalytic residues from sequence alone. Squidly, a new ML method, outperforms existing tools on standard benchmarks and on the CataloDB dataset. The work has <bold>solid</bold> support, yet clarifications on dataset biases, ablation analyses, and uncertainty filtering would strengthen its efficiency claims.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108186.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this well-written and timely manuscript, Rieger et al. introduce Squidly, a new deep learning framework for catalytic residue prediction. The novelty of the work lies in the aspect of integrating per-residue embeddings from large protein language models (ESM2) with a biology-informed contrastive learning scheme that leverages enzyme class information to rationally mine hard positive/negative pairs. Importantly, the method avoids reliance on the use of predicted 3D structures, enabling scalability, speed, and broad applicability. The authors show that Squidly outperforms existing ML-based tools and even BLAST in certain settings, while an ensemble with BLAST achieves state-of-the-art performance across multiple benchmarks. Additionally, the introduction of the CataloDB benchmark, designed to test generalization at low sequence and structural identity, represents another important contribution of this work.</p>
<p>I have only some minor comments:</p>
<p>(1) The manuscript acknowledges biases in EC class representation, particularly the enrichment for hydrolases. While CataloDB addresses some of these issues, the strong imbalance across enzyme classes may still limit conclusions about generalization. Could the authors provide per-class performance metrics, especially for underrepresented EC classes?</p>
<p>(2) An ablation analysis would be valuable to demonstrate how specific design choices in the algorithm contribute to capturing catalytic residue patterns in enzymes.</p>
<p>(3) The statement that users can optionally use uncertainty to filter predictions is promising but underdeveloped. How should predictive entropy values be interpreted in practice? Is there an empirical threshold that separates high- from low-confidence predictions? A demonstration of how uncertainty filtering shifts the trade-off between false positives and false negatives would clarify the practical utility of this feature.</p>
<p>(4) The excerpt highlights computational efficiency, reporting substantial runtime improvements (e.g., 108 s vs. 5757 s). However, the comparison lacks details on dataset size, hardware/software environment, and reproducibility conditions. Without these details, the speedup claim is difficult to evaluate. Furthermore, it remains unclear whether the reported efficiency gains come at the expense of predictive performance.</p>
<p>(5) Given the well-known biases in public enzyme databases, the dataset is likely enriched for model organisms (e.g., E. coli, yeast, human enzymes) and underrepresents enzymes from archaea, extremophiles, and diverse microbial taxa. Would this limit conclusions about Squidly's generalisability to less-studied lineages?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108186.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors aim to develop Squidly, a sequence-only catalytic residue prediction method. By combining protein language model (ESM2) embedding with a biologically inspired contrastive learning pairing strategy, they achieve efficient and scalable predictions without relying on three-dimensional structure. Overall, the authors largely achieved their stated objectives, and the results generally support their conclusions. This research has the potential to advance the fields of enzyme functional annotation and protein design, particularly in the context of screening large-scale sequence databases and unstructured data. However, the data and methods are still limited by the biases of current public databases, so the interpretation of predictions requires specific biological context and experimental validation.</p>
<p>Strengths:</p>
<p>The strengths of this work include the innovative methodological incorporation of EC classification information for &quot;reaction-informed&quot; sample pairing, thereby enhancing the discriminative power of contrastive learning. Results demonstrate that Squidly outperforms existing machine learning methods on multiple benchmarks and is significantly faster than structure prediction tools, demonstrating its practicality.</p>
<p>Weaknesses:</p>
<p>Disadvantages include the lack of a systematic evaluation of the impact of each strategy on model performance. Furthermore, some analyses, such as PCA visualization, exhibit low explained variance, which undermines the strength of the conclusions.</p>
</body>
</sub-article>
</article>