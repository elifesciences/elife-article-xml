<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">105394</article-id>
<article-id pub-id-type="doi">10.7554/eLife.105394</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.105394.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A high-throughput approach for the efficient prediction of perceived similarity of natural objects</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1460-1267</contrib-id>
<name>
<surname>Kaniuth</surname>
<given-names>Philipp</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>kaniuth@cbs.mpg.de</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0001-0031-964X</contrib-id>
<name>
<surname>Mahner</surname>
<given-names>Florian P</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Perkuhn</surname>
<given-names>Jonas</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7257-428X</contrib-id>
<name>
<surname>Hebart</surname>
<given-names>Martin N</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
<email>hebart@cbs.mpg.de</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Vision and Computational Cognition Group, Max Planck Institute for Human Cognitive and Brain Sciences</institution></institution-wrap>, <city>Leipzig</city>, <country country="DE">Germany</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/033eqas34</institution-id><institution>Computational Cognitive Neuroscience and Quantitative Psychiatry, Department of Medicine, Justus Liebig University Giessen</institution></institution-wrap>, <city>Giessen</city>, <country country="DE">Germany</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Neural Coding Lab, Donders Institute for Brain, Cognition and Behavior</institution></institution-wrap>, <city>Nijmegen</city>, <country country="NL">Netherlands</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01rdrb571</institution-id><institution>Center for Mind, Brain and Behavior (CMBB), Universities of Marburg, Giessen, and Darmstadt</institution></institution-wrap>, <city>Marburg</city>, <country country="DE">Germany</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Bracci</surname>
<given-names>Stefania</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Trento</institution>
</institution-wrap>
<city>Rovereto</city>
<country>Italy</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-04-22">
<day>22</day>
<month>04</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP105394</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-12-20">
<day>20</day>
<month>12</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-12-22">
<day>22</day>
<month>12</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.06.28.601184"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Kaniuth et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Kaniuth et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-105394-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Perceived similarity offers a window into the mental representations underlying our ability to make sense of our visual world, yet, the collection of similarity judgments quickly becomes infeasible for larger datasets, limiting their generality. To address this challenge, here we introduce a computational approach that predicts perceived similarity from neural network activations through a set of 49 interpretable dimensions learned on 1.46 million triplet odd-one-out judgments. The approach allowed us to predict separate, independently-sampled similarity scores with an accuracy of up to 0.898. Combining this approach with human ratings of the same dimensions led only to small improvements, indicating that the neural network used similar information as humans in this task. Predicting the similarity of highly homogeneous image classes revealed that performance critically depends on the granularity of the training data. Our approach allowed us to improve the brain-behavior correspondence in a large-scale neuroimaging dataset and visualize candidate image features humans use for making similarity judgments, thus highlighting which image parts may carry behaviorally-relevant information. Together, our results demonstrate that current neural networks carry information sufficient for capturing broadly-sampled similarity scores, offering a pathway towards the automated collection of similarity scores for natural images.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>representations</kwd>
<kwd>similarity</kwd>
<kwd>embedding</kwd>
<kwd>behavior</kwd>
<kwd>deep neural networks</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Added a statement in the acknowledgement section stating who funded Open Access</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Understanding the nature of human mental representations is a key aim of the cognitive sciences<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. A central approach for gaining access to these representations has been the collection of perceived similarity judgments<sup><xref ref-type="bibr" rid="c2">2</xref></sup>, with a long history across the fields of psychology, linguistics, neuroscience, and computer science<sup><xref ref-type="bibr" rid="c3">3</xref>–<xref ref-type="bibr" rid="c6">6</xref></sup>. Similarity judgments allow us to improve our understanding of a variety of cognitive processes, including object recognition, categorization, decision making, and semantic memory<sup><xref ref-type="bibr" rid="c6">6</xref>–<xref ref-type="bibr" rid="c13">13</xref></sup>. In addition, they offer a convenient means for relating mental representations to representations in the human brain<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c15">15</xref></sup> and other domains<sup><xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup>.</p>
<p>To collect similarity judgments, a diverse set of behavioral similarity tasks has been developed, including pairwise ratings<sup><xref ref-type="bibr" rid="c18">18</xref>–<xref ref-type="bibr" rid="c20">20</xref></sup>, pile sorting<sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup>, object arrangements<sup><xref ref-type="bibr" rid="c23">23</xref>–<xref ref-type="bibr" rid="c25">25</xref></sup>, triplet judgments<sup><xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c26">26</xref>–<xref ref-type="bibr" rid="c28">28</xref></sup>, or speeded, more indirect similarity tasks<sup><xref ref-type="bibr" rid="c29">29</xref>–<xref ref-type="bibr" rid="c31">31</xref></sup>. While each task has its unique advantage, a key challenge with the acquisition of similarity ratings is their complexity, which in many cases grows quadratically or cubically with the number of items (superlinear complexity, <xref rid="fig1" ref-type="fig">Figure 1</xref>). This challenge not only limits our understanding of the similarity between images to small stimulus sets; it also limits the direct comparability between studies that rely on different images. A more efficient approach for sampling similarity scores mirroring these judgments is thus highly desirable and could even be applied retrospectively to a large number of existing datasets that did not collect similarity scores. By overcoming the data sampling challenge, such an approach would thus open novel data-driven and hypothesis-driven avenues for understanding mental representations at scale.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Cost functions and different perceived similarity sampling regimes.</title>
<p>(a) For a stimulus set with <italic>n</italic> stimuli (x-axis) the cost of sampling all similarities (y-axis) grows differently for different sampling regimes. (b) For most classical behavioral similarity tasks like the pairwise rating task (left), cost grows superlinearly. Ideally, though, behavioral tasks should have linear cost increases like the task we present in this study (middle). Flat costs (right) are only attainable when utilizing deep neural networks (DNNs) and applying an automatic method like the one we explore in this study with which the whole embedding for a given image set is predicted in one go which can then be used to compute the whole representational similarity matrix.</p></caption>
<graphic xlink:href="601184v4_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>An efficient alternative to directly sampling similarity judgements is offered by feature-based rating approaches (<xref rid="fig1" ref-type="fig">Figure 1b</xref>, middle). Here, stimuli are first scored along multiple dimensions that can be both visual (e.g., shape features) or semantic (e.g., animacy) in nature. Then, those dimensions are combined to compute an overall similarity score between pairs of stimuli<sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c32">32</xref>–<xref ref-type="bibr" rid="c34">34</xref></sup>. While this leads to increased efficiency that scales linearly with the number of items (<xref rid="fig1" ref-type="fig">Figure 1a</xref>), the predictive accuracy of this approach has been rather limited<sup><xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c35">35</xref>, but see <xref ref-type="bibr" rid="c36">36</xref></sup>. More recently, computational models have been used (<xref rid="fig1" ref-type="fig">Figure 1b</xref>, right), based on which predictions of similarity can be automated, including the use of raw features of computational models<sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c37">37</xref></sup> or the indirect use of large language models<sup><xref ref-type="bibr" rid="c38">38</xref>–<xref ref-type="bibr" rid="c40">40</xref></sup>. Similar to behavioral feature ratings, prediction accuracy of these approaches has remained limited or was restricted to small-scale comparisons, and comparisons were often done on image sets where the tested categories in the training and testing set overlapped<sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c41">41</xref></sup>. This leaves open the question to what degree results would generalize to a broader set of independent stimuli<sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c42">42</xref>,<xref ref-type="bibr" rid="c43">43</xref></sup>.</p>
<p>To overcome existing challenges of predicting perceived similarity judgments, here we present an efficient automated approach that is based on predicting a small set of human interpretable dimensions derived from similarity ratings to 1,854 diverse object images. This approach allows us to retrieve similarity scores, which we can validate with independently sampled behavioral ground-truth similarity. We showcase our approach on different image sets across various neural network models. Our results show that the approach is capable of generating accurate perceived similarity scores for various broadly-sampled stimulus sets. Combining this automated approach with human judgments of the same dimensions led only to small improvements, indicating that the neural network relied on largely similar information as humans for predicting representational dimensions. Having established an end-to-end mapping of input images to perceived similarity that generalizes to a large set of natural images, we showcase the potential of this approach by (1) revealing image regions that are predicted to be most relevant for similarity judgements and (2) improving the brain-behavior correspondence in a large-scale neuroimaging dataset. Together, the results demonstrate that the prediction of interpretable representational dimensions offers a powerful approach for the efficient prediction of perceived similarity, opening the avenue to automatically sample perceived similarity scores for broadly-sampled image categories.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>To automate the generation of similarity judgments, we tested the degree to which off-the-shelf deep neural networks (DNNs) contain representations sufficient for capturing perceived similarity. Instead of directly using raw DNN features to predict similarity<sup><xref ref-type="bibr" rid="c41">41</xref></sup>, we developed a two-step procedure (<xref rid="fig2" ref-type="fig">Figure 2</xref>, top). In the first step, we predicted a human interpretable similarity embedding for each stimulus from DNN representations of these images. In the second step, we derived perceived similarity scores from this predicted embedding. For the first step, as a similarity embedding we used the Sparse Positive Similarity Embedding (SPoSE), which consists of 49 human interpretable dimensions<sup><xref ref-type="bibr" rid="c36">36</xref></sup>. SPoSE had been trained on 1.46 million behavioral triplet odd-one-out judgments across 1,854 object images from the THINGS database, a sizable database containing thousands of images that comprehensively cover objects that are nameable in the American English language<sup><xref ref-type="bibr" rid="c44">44</xref></sup>. The behavioral dataset has been shown to accurately predict similarity judgments in held-out data close to the noise ceiling<sup><xref ref-type="bibr" rid="c36">36</xref></sup>, making it ideally suited for the prediction of perceived similarity. To predict the similarity embedding, we extracted DNN activations for each of the 1,854 reference images. Next, we trained a linear encoding model on this DNN embedding to predict values on each of the 49 dimensions separately. By breaking down the problem of predicting stimulus similarity into smaller units, we allow the DNN to focus on one dimension at a time rather than having to account for all possible dimensions in parallel. Pairing this approach with a large-scale similarity dataset offers a promising avenue for predicting embedding dimensions with high precision.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Overview of our approach.</title>
<p>For a given image set, we extracted image activations from various image-computable neural network models (top). Based on these activations, we trained an L2-penalized multi-output multiple regression encoding model to predict a 49-dimensional Sparse Positive Similarity Embedding (SPoSE). We then used this embedding to construct a DNN-predicted representational similarity matrix (RSM) which we validated with a behavioral ground-truth RSM. To compare our DNN-based approach to human performance, we conducted crowdsourcing to receive human-generated ratings of all images on the same embedding (bottom). Based on this embedding, we computed a human-constructed RSM which we validated with the ground-truth RSM. Images in this figure were taken from the THINGSplus dataset and are used for illustrative purposes only<sup><xref ref-type="bibr" rid="c79">79</xref></sup>.</p></caption>
<graphic xlink:href="601184v4_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Having predicted the similarity embedding from DNN activations, in the second step we used these predicted dimensions to construct perceived similarity between all pairs of stimuli. To achieve this, we mimicked the cognitive process of constructing similarity judgments assumed for the generation of SPoSE similarities. For a given pair <italic>i</italic> and <italic>j</italic>, similarity was constructed by computing all possible triplet odd-one-out judgments from the embedding involving both stimuli and all possible contexts imposed by a third stimulus <italic>k</italic>. Across all possible <italic>k</italic>, similarity for <italic>i</italic> and <italic>j</italic> was defined as the probability of predicting stimuli <italic>i</italic> and <italic>j</italic> as belonging together and stimulus <italic>k</italic> being the odd-one-out. Repeating this procedure for all possible pairs of stimuli allowed us to construct a predicted representational similarity matrix (RSM), which we could later validate with a ground-truth RSM that we obtained independently.</p>
<sec id="s2a">
<title>Accurate prediction of perceived similarity for broadly-sampled natural object images</title>
<p>To achieve high prediction performance for human similarity, we sampled activations from a broad selection of DNN architectures that had been trained with different objectives and training datasets (see <italic>Methods</italic>). This breadth additionally allowed us to avoid that results were a mere idiosyncrasy of the chosen network and reveal the extent to which the networks vary in their prediction of human similarity judgments. The results of the comparison of DNNs is shown in <xref rid="fig3" ref-type="fig">Figure 3</xref>. As a baseline, we used (1) classical representational similarity analysis (cRSA)<sup><xref ref-type="bibr" rid="c17">17</xref></sup>, which directly computes similarity from DNN embeddings and is an established method for relating DNN representations with perceived similarity, and (2) feature-reweighted RSA (FR-RSA)<sup><xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c41">41</xref>,<xref ref-type="bibr" rid="c45">45</xref>–<xref ref-type="bibr" rid="c47">47</xref></sup>, which computes similarity from linearly-reweighted features learned from the original similarity matrix and has been shown to achieve much stronger correspondence to similarity judgments<sup><xref ref-type="bibr" rid="c41">41</xref>,<xref ref-type="bibr" rid="c45">45</xref></sup>. Across DNNs, the prediction of perceived similarity varied strongly. However, for a number of DNNs, the predictive performance was excellent, achieving correlations with ground truth of <italic>r</italic> = 0.887 for the visual encoder layer of the multimodal network OpenCLIP-ResNet50 trained on 400 million image-text pairs, and <italic>r</italic> = 0.888 for OpenCLIP-ViT-g-14 trained on 2 billion image-text pairs. All of the best neural networks were based on a variant of CLIP, which indicates the benefit of using large, diverse training data, or a training objective that includes much broader semantics based on text descriptions of images<sup><xref ref-type="bibr" rid="c48">48</xref>–<xref ref-type="bibr" rid="c52">52</xref></sup>. Between the poorer models and the CLIP models was a range of other network architectures with different training objectives, indicating that architecture and objective were not dominant factors at predicting similarity. DimPred clearly outperformed the baseline approaches, yielding improved predictive accuracy for 52 of 53 computational models.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Predictive performance of all computational models for perceived similarity.</title>
<p>For the 1,854 reference image set, separately for each computational model (x-axis) we Pearson-correlated the predicted and ground-truth similarity scores (y-axis). Across models, DimPred outperformed established representational similarity methods that relate representational spaces to each other (cRSA and FR-RSA). Amongst all computational models, Open-CLIP models performed exceptionally well. An ensemble DimPred model consisting of the best 8 computational models performed even better (denoted by a star symbol). For a mapping of computational model name abbreviations to full model names please see Supplement 4.</p></caption>
<graphic xlink:href="601184v4_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The computational models also successfully predicted individual embedding dimensions (see Supplement 1).</p>
<p>Since many individual models performed well, we wondered to what degree their combination would improve the prediction of similarity. To this end, we combined values predicted for a specific dimension from different computational models and evaluated whether this led to improved predicted similarity scores. Specifically, we combined the predicted values of the best <italic>n</italic> models for each dimension, creating an ensemble model of predicted perceived similarity. The ensemble model exhibited an even greater predictive accuracy than the best model individually, producing correlations of predicted similarity with ground-truth of up to 0.926 when combining the best 8 computational models (for further details, see Supplement 2).</p>
</sec>
<sec id="s2b">
<title>DimPred generalizes to out-of-set images and other similarity tasks</title>
<p>To test the degree to which the results generalize beyond the distribution of the 1,854 reference images and the exact similarity task used, we sought to test the performance of DimPred by predicting similarity for independent stimulus sets that had been collected with a variety of different behavioral similarity tasks. Predicting similarity for external validation sets is usually not done when applying cRSA or FR-RSA<sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c46">46</xref>,<xref ref-type="bibr" rid="c53">53</xref></sup>, which at least in the case of FR-RSA might lead to overestimating generalization performance. In addition, even though we labeled the similarity matrix of the 1,854 images as “ground truth”, it was not sampled empirically but was the result of similarity prediction of the original similarity embedding, which may slightly overestimate the performance of this approach on external data. To address these issues, we applied DimPred to five additional image sets, each consisting of images from various categories. One validation set consisted of 48 images of categories that are part of the 1,854 THINGS categories that DimPred had been trained on, but contained new images, thus testing generalization at the image level (48- image-generalization image set). Another validation set consisted of 48 images, but this time of concepts not included in THINGS, thus testing generalization to new images and new concepts (48-concept-generalization image set). The three other validation sets consisted of various images sampled in previously published data and collected with different similarity tasks, containing 92 images without natural background sampled with the multi-arrangement task (Mur-92)<sup><xref ref-type="bibr" rid="c54">54</xref>–<xref ref-type="bibr" rid="c56">56</xref></sup>, 118 images natural images sampled with the same task (Cichy-118)<sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c57">57</xref></sup>, and 120 images from a mix of categories sampled with a pairwise similarity task (Peterson-various image set)<sup><xref ref-type="bibr" rid="c41">41</xref></sup>. We again estimated the performance of DimPred across all computational models and included cRSA and FR-RSA as a baseline.</p>
<p>The results are presented in <xref rid="fig4" ref-type="fig">Figure 4</xref>, with a direct contrast against the baselines cRSA and FR-RSA. DimPred again showed strong overall predictive performance on both 48 image sets, with predictive accuracy reaching <italic>r</italic> = 0.858, while it was slightly lower in the additional three image sets (with <italic>r</italic> up to 0.809), indicating a slight drop in performance when generalizing across similarity tasks. When applying the ensemble model of predicted perceived similarity with the best training performance for the 1,854 reference image set, predictive accuracy on the 48 image sets reached up to <italic>r</italic> = 0.898, and for the additional three image sets up to <italic>r</italic> = 0.843 (see Supplement 2 for further details). For all image sets, DimPred was clearly superior to the baseline approaches. Together, this demonstrates that DimPred generalizes well to other images as well as other concepts while also generalizing well to other similarity tasks, underscoring the generality of the approach for broad image sets.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Predictive performance of all computational models for out-of-sample heterogeneous image sets.</title>
<p>Our dimension prediction approach (DimPred) yielded similarity scores that correlated very highly with ground-truth similarity scores across five different heterogeneous validation sets. Across image sets, DimPred outperformed either classical (cRSA; left) or feature-reweighted RSA (FR-RSA; right) in 256 and 231 of 265 cases, respectively. The best computational model for each image set is indicated using the model abbreviation from Supplement 4. An ensemble DimPred model consisting of the best 8 computational models performed even better (denoted by a star symbol for each image set in the left panel).</p></caption>
<graphic xlink:href="601184v4_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2c">
<title>Humans can successfully rate images on embedding dimensions - but rely on information similar to those of DNNs</title>
<p>While DNN-based predictions yielded excellent performance, we asked whether humans could solve the prediction of dimensions with similar or better efficiency, and if so, whether combining humans with DNNs would lead to further improvements in predictive performance. To this end, we utilized crowdsourcing to gather human ratings of images on the 49 dimensions (see <xref rid="fig2" ref-type="fig">Figure 2</xref>, top). In the crowdsourcing task, after training, participants had to rate the images on each of the 49 SpoSE dimensions separately using a visual Likert-type scale. Based on these ratings, we computed an RSM which we then compared to a behavioral ground-truth RSM (for further details see <italic>Methods</italic>). As target images, we used the 48-image-generalization and 48-concept-generalization image sets.</p>
<p>Our results showed that humans were successful at providing sensible dimensional ratings of images; the resulting similarity scores correlated highly with ground-truth similarity scores for the two image sets (image set 1: <italic>r</italic> = 0.804, image set 2: <italic>r</italic> = 0.838) (see <xref rid="fig5" ref-type="fig">Figure 5</xref> and Supplement 3). Next, we combined the human-based similarity scores with the similarity scores yielded by DimPred using a single neural network (OpenCLIP-RN50×64, OpenAI, visual) by averaging the predicted similarity scores of both. Compared to human-only predictions, this combination improved performance notably (image set 1: <italic>r</italic> = 0.861, <italic>p</italic><sub>diff</sub> = 0.002; image set 2: <italic>r</italic> = 0.884, <italic>p</italic><sub>diff</sub> = 0.002). Also in comparison to DNN-only predictions, performance was improved (original <italic>r</italic>s = 0.811 and 0.848, <italic>p</italic><sub>diff</sub> = 0.014 and 0.005, respectively). When combining the human-based similarity scores with the similarity scores yielded by the top-8 ensemble DimPred model, performance improved even more compared to human-only predictions (<italic>r</italic>s = 0.878 and 0.900, and <italic>p</italic><sub>diff</sub> &lt; 0.001, respectively), while performance improvement was smaller and non-significant in comparison to ensemble-only predictions (image set 1: <italic>r</italic> = 0.861, <italic>p</italic><sub>diff</sub> = 0.869; image set 2: <italic>r</italic> = 0.898, <italic>p</italic><sub>diff</sub> = 0.244; all <italic>p</italic>-values uncorrected). This demonstrates that in comparison to the ensemble approach, combining DimPred with human predictions did not yield a statistically significant benefit, and the small improvement in absolute prediction accuracy may not justify the additional overhead of collecting human dimension ratings.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Correspondence between human-predicted, human &amp; best single DNN combined or human &amp; ensemble-DNN combined similarity matrices and ground-truth similarity matrices.</title>
<p>For the two image sets a) the 48-image-generalization and b) the 48-concept-generalization image set, the representational similarity matrix (RSM) solely based on human ratings of image dimension values (top row), an average of that RSM and the best single DNN- predicted RSM (middle row), and an average of the human-based RSM and the top-8 ensemble DimPred model RSM (bottom row) was correlated with the respective ground-truth RSM. While humans provided sensible dimensional ratings of images evidenced by RSMs correlating highly with ground-truth RSMs, enriching them with RSMs yielded by DimPred for OpenCLIP-RN50×64 (OpenAI, visual) or with RSMs yielded by the ensemble DimPred model increased correspondence with ground-truth RSMs notably compared to only using human-derived RSMs.</p></caption>
<graphic xlink:href="601184v4_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2d">
<title>Predictive accuracy depends on granularity of an image set’s similarity structure</title>
<p>The image sets investigated so far contained many different object categories (e.g., aardvark, ambulance, bed). However, DimPred had been trained on similarity data sampled for a broad set of categories but with only very sparse sampling within higher-level categories (e.g., animals). We thus wondered what the limits of the performance of DimPred are when applying it to image sets from much more homogeneous image classes (e.g., pictures of furniture only). To test this, we utilized five more validation sets containing images from Peterson and colleagues<sup><xref ref-type="bibr" rid="c41">41</xref></sup> that each consisted of 120 images of either exclusively animals, automobiles, fruits, furniture, vegetables and again used cRSA and FR-RSA as baseline approaches.</p>
<p>The respective results are presented in <xref rid="fig6" ref-type="fig">Figure 6</xref>. As expected, DimPred performed poorly on these image sets, with the ensemble model showing only slightly increased performance for most image sets. For all homogeneous image sets, DimPred was better only in 73 (<xref rid="fig6" ref-type="fig">Figure 6</xref>, left panel) or 59 (<xref rid="fig6" ref-type="fig">Figure 6</xref>, right panel) of 265 cases when compared to cRSA or FR-RSA as a baseline, respectively, reaching predictive accuracy of only up to <italic>r</italic> = 0.541.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Predictive performance of all computational models for out-of-sample homogeneous image sets.</title>
<p>Dimpred yielded similarity scores that did not correlate well with ground-truth similarity scores for five validation sets containing homogeneous images. Across image sets, DimPred was outperformed by either classical (cRSA; left) or feature-reweighted RSA (FR-RSA; right) in 73 and 59 of 265 cases, respectively. An ensemble DimPred model consisting of the best 8 computational models sometimes performed slightly better (denoted by a star symbol for each image set in the left panel).</p></caption>
<graphic xlink:href="601184v4_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2e">
<title>Revealing SPoSE-dimension specific areas in images important for similarity derivation</title>
<p>Despite the challenges of applying DimPred to the prediction of similarity judgments at a more fine-grained level, the prediction of similarity at scale for broadly-sampled natural images opens up many potential avenues. To showcase this potential, we first asked whether we could use the dimension predictions to highlight the likely image features underlying similarity judgments in humans. DimPred yields end-to-end prediction from images to individual dimensions, allowing us to observe the change in dimension prediction when occluding parts of the original image<sup><xref ref-type="bibr" rid="c58">58</xref></sup>. To this end, we used the iterative masking approach RISE<sup><xref ref-type="bibr" rid="c59">59</xref></sup> that repeatedly occludes random image regions while observing the effect on the prediction performance of the model (see <italic>Methods</italic>). This approach allowed us to generate heatmaps that indicate the importance of image regions for a given SPoSE dimension. Beyond the prediction of individual dimensions, this approach allows us to compute an overall relevance score for the image regions involved in similarity judgments. Specifically, multiplying the predicted heatmaps with the dimension scores and averaging across dimensions provides us with a single heatmap of image region importance for similarity judgments. Example heatmaps are shown in <xref rid="fig7" ref-type="fig">Figure 7</xref>. For each of the example images, heatmaps for the global relevance as well as for the three most important image-specific SPoSE dimensions are shown. Indeed, the visualization method produces interpretable results, highlighting not only the image regions in line with the individual predicted dimensions but also which image regions are behaviorally-relevant for similarity and categorization judgments. Together, combining DimPred with visualization techniques offers a window into understanding which parts of an image may underlie its mental representations.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Visualization of image regions relevant for predicting different dimensions underlying similarity judgments.</title>
<p>For each of the example images, the three most important SPoSE dimensions are shown. In addition, the relevance for similarity prediction reflects the weighted integration across all dimensions, highlighting candidate image regions that are behaviorally relevant for similarity judgments and categorization behavior.</p></caption>
<graphic xlink:href="601184v4_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2f">
<title>DimPred improves brain-behavior correspondence</title>
<p>To showcase the potential of DimPred for the application to external datasets, we applied DimPred to a publicly available functional MRI dataset of three participants who each saw 8,740 object images from 720 categories<sup><xref ref-type="bibr" rid="c60">60</xref></sup>. As a baseline, we fit a voxel encoding model of all 49 dimensions. Since dimension scores were available only for one image per category<sup><xref ref-type="bibr" rid="c36">36</xref></sup>, for the baseline model, we used the same value for each image of the same category and estimated predictive performance using cross-validation. To test the degree to which image-specific effects contributed to the prediction, we used DimPred to attain predicted dimension scores for each image and fit another encoding model that was evaluated separately. As a computational model, we used OpenCLIP-RN50×64. The relative improvement for each participant is shown in <xref rid="fig8" ref-type="fig">Figure 8</xref>. As can be seen, regions across the visual system show consistent improvements in predictive performance, demonstrating clear image-specific effects across the entire visual system, including high-level visual cortex. We have applied this approach in practice for the prediction of functional MRI<sup><xref ref-type="bibr" rid="c61">61</xref></sup> and MEG data<sup><xref ref-type="bibr" rid="c62">62</xref></sup>. Together, these results highlight the potential of applying DimPred to existing dataset to gain novel insights into the functional organization of the visual system.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
<caption><title>Prediction accuracy improvement of an fMRI voxel-wise encoding model based on the DimPred embedding.</title>
<p>For three subjects, we computed an encoding model to predict voxel-wise fMRI activity, either using a coarser category-insensitive object image embedding or using the category-sensitive image-specific embedding as produced by DimPred based on activations from OpenCLIP-RN50×64. The relative improvement in encoding-model prediction accuracy when using DimPred is color-coded separately for each subject on a cortical flat map.</p></caption>
<graphic xlink:href="601184v4_fig8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Perceived similarity is central to understanding mental representations and how they relate to brain activity and behavior<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c13">13</xref></sup>. In addition, it plays a crucial role in evaluating computational models for their potential to explain neural or behavioral data<sup><xref ref-type="bibr" rid="c41">41</xref></sup>. In this study, we present an image-computable model of perceived similarity that uses the activations of a deep neural network (DNN) to predict meaningful similarity scores for thousands of object images. Our approach, DimPred, demonstrated a notable correspondence between the predicted similarity scores and ground-truth similarity across various image sets containing diverse categories. Given the high out-of-sample accuracy, this method suggests that predicted similarity scores can serve as a substitute for perceived similarity judgments of broadly-sampled, diverse object image sets, especially when the collection of such judgments is too effortful or costly. Our results highlight examples of how DimPred can be used in practice to yield new insights into the mental and neural representations of objects.</p>
<p>Much previous work combining artificial neural networks with human similarity judgments has focused on revealing the correspondence between artificial neural networks and human similarity judgments. Thus, explicitly or implicitly, many of these studies have attempted to automate the estimation of perceived similarity of stimuli. Most prior research fitted embeddings of off-the-shelf DNN layers directly to predict pairwise similarity judgments<sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c37">37</xref>,<xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c41">41</xref></sup>, which required fitting all relevant features used in similarity comparisons simultaneously. In contrast, our dimension-based prediction framework DimPred breaks down this problem by separately predicting individual interpretable dimensions. In addition to the complexity of the prediction, much previous work relied on small similarity datasets<sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c42">42</xref>,<xref ref-type="bibr" rid="c43">43</xref></sup>, raising questions about their generalizability for predicting perceived similarity. Our results demonstrate that DimPred’s predictive performance surpasses direct prediction approaches for datasets including broadly-sampled object categories and even generalized to datasets using other similarity tasks. In this context, it is important to note that many previous approaches, despite their seemingly impressive performance on existing datasets<sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c41">41</xref>,<xref ref-type="bibr" rid="c42">42</xref></sup>, were not genuinely validated out-of-sample. Validation data often included several examples from the same object used during training, or performance was estimated in a non-independent manner by not leaving out objects but similarity judgments. In contrast, DimPred was evaluated both across category and across similarity tasks and continued to yield notable out-of-sample performance.</p>
<p>While DimPred showed strong performance on broad image sets, for within-category similarity prediction, performance was more limited. While this result might be due to the lack of relevant information in the neural networks used for training the approach, recent work successfully predicted representational dimensions for the category of manipulable objects, demonstrating that within-category dimension prediction from DNN activations is feasible<sup><xref ref-type="bibr" rid="c63">63</xref></sup>. This suggests that the performance of DimPred critically depends on the granularity of the similarity judgments from which the dimensions are derived. Since DimPred was trained on broad comparisons of many objects spanning diverse categories, it emphasized comparisons at a less fine-grained level than within-category comparisons. The recent success in predicting representational dimensions within categories<sup><xref ref-type="bibr" rid="c63">63</xref></sup> highlights DimPred’s potential for predicting similarity ratings at varying levels of granularity. Future work could involve training representational embeddings at multiple levels, both within and across categories, to determine the ideal level of granularity for a given set of similarity comparisons, thereby maximizing predictive accuracy.</p>
<p>Predicting dimensions from human ratings yielded an accuracy very similar to that of DimPred. Combining both human and DNN ratings did not yield notable improvements over the performance found in an ensemble model of DNNs alone, but the addition of DNNs still slightly improved human ratings. This comparison might underestimate human performance, as human dimension ratings were based on a limited number of participants, which is typical of empirical studies. Nevertheless, the strong similarity in predictive accuracy between humans and DNNs demonstrates that DNNs captured similar information to that used by humans when rating dimensions. This indicates that much of the information needed to explain perceived similarity at the level of object images in this study may already be present in the best-performing DNNs.</p>
<p>The dimensions that span the representational space of objects capture both visual and semantic aspects of our visual world<sup><xref ref-type="bibr" rid="c36">36</xref></sup>. Interestingly, the best performing computational models were those that combined visual and semantic information (i.e., the OpenCLIP models) compared to classical deep neural networks that encode visual information only. Determining whether this performance is due to dataset size, dataset diversity, multimodal training, or the effective number of prediction targets is an important question for future work.<sup><xref ref-type="bibr" rid="c49">49</xref>,<xref ref-type="bibr" rid="c50">50</xref>,<xref ref-type="bibr" rid="c63">63</xref></sup></p>
<p>Given the fact that DimPred can predict similarity judgments for broadly-sampled object image sets at scale, this opens the door to many novel applications, some of which we highlighted in the present work. First, in the past, it has remained unclear which image regions are used for similarity judgments, and only recent efforts have highlighted behaviorally relevant regions in images<sup><xref ref-type="bibr" rid="c64">64</xref>,<xref ref-type="bibr" rid="c65">65</xref></sup>. In the present work, we not only identified which image regions were potentially behaviorally relevant for similarity judgments. We also revealed what image regions were potentially relevant for individual representational dimensions (e.g., an animal-related dimension). Second, we generated image-specific predictions to improve encoding models of the visual system, which we highlighted with functional MRI data<sup><xref ref-type="bibr" rid="c61">61</xref></sup> but which has also been applied to improve the prediction of MEG data<sup><xref ref-type="bibr" rid="c62">62</xref></sup>. This highlights that DimPred could be applied not only to future dataset but could even be applied post-hoc to already existing data, thus enabling others to answer new questions with already existing data. Furthermore, DimPred may allow teasing apart image-specific from general category-specific visual representations and thus reveal the distribution of visual and semantic features relevant for the representation of individual objects.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Image sets</title>
<p>We used 11 different image sets that consisted of images either of broadly-sampled objects from diverse high-level categories or of images within individual superordinate categories (e.g., vegetables). The first three image sets were taken from the THINGS dataset<sup><xref ref-type="bibr" rid="c44">44</xref></sup>, which consists of 1,854 object categories and 26,107 images of these objects. For the first image set, we used the reference image of each THINGS object category, resulting in 1,854 images. The second image set (48-image-generalization image set) consisted of 48 THINGS images. The objects had been used in a validation set in a previous study<sup><xref ref-type="bibr" rid="c36">36</xref></sup> but here we used different object examples to test the generalization of DimPred to novel images. These objects had been chosen to be semantically diverse, and thus we anticipated that their prediction would be more challenging than a random selection of objects. The third image set consisted of object concepts that were not part of THINGS (48-concept-generalization image set) and were chosen to test the generalization to novel images and novel objects. The fourth image set<sup><xref ref-type="bibr" rid="c54">54</xref>–<xref ref-type="bibr" rid="c56">56</xref></sup> consisted of 92 images of natural and artificial objects as well as human and non-human faces and bodies (Mur-92 image set). The fifth image set<sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c57">57</xref></sup> consisted of 118 natural images (Cichy-118 image set). Both image sets were chosen to test DimPred’s generalization to another similarity task (object arrangement). The last six image sets<sup><xref ref-type="bibr" rid="c41">41</xref></sup> consisted of 120 images of either animals, automobiles, fruits, furniture, vegetables or a mix of these categories and were chosen to test the performance of DimPred within category or, for the last image set, the generalization to another similarity task (pairwise ratings).</p>
</sec>
<sec id="s4b">
<title>Layer activations from chosen deep neural networks</title>
<p>We extracted activations from various deep neural networks (DNNs) as computational models for all image sets. For a detailed overview of the DNNs and the specific modules we used, please see Supplement 4. Overall, we deployed various kinds of models: fully connected layers of convolutional networks, convolutional layers from convolutional networks, unsupervised networks, transformers, and CLIP models. Where applicable, we used versions of the DNNs that had been pre-trained on the 1,000 ImageNet object classes<sup><xref ref-type="bibr" rid="c66">66</xref></sup>. After extraction, we flattened activation vectors. To ease computational load, for each vector exceeding 4,096 units, we used sparse random projection to reduce the dimensionality of the layer to 4,096.</p>
</sec>
<sec id="s4c">
<title>The DimPred approach</title>
<p>DimPred works in two steps. In the first step, an L2-regularized multiple multi-output regression is fitted to a given DNN module’s image activations to predict a value for each image on each of the dimensions of a similarity embedding derived from human behavior. In the second step, the predicted embedding is used to build a predicted similarity matrix. For step 1, as a prediction target we used the Sparse Positive Similarity Embedding (SPoSE)<sup><xref ref-type="bibr" rid="c67">67</xref></sup>, which consists of 49 human-interpretable visual and semantic dimensions across images of 1,854 natural objects. SPoSE was derived from 1.46 million triplet odd-one-out similarity judgments and has been shown to capture human similarity judgments in this task close to the human noise ceiling<sup><xref ref-type="bibr" rid="c36">36</xref></sup>. For the prediction of this embedding, for the 1,854 reference image set and a given computational neural network model, we first extracted activations for the images. We then split these image activations into ten folds for an outer cross-validation. Hyperparameter tuning was carried out using three-fold inner cross-validation on the training set. We used fractional ridge regression<sup><xref ref-type="bibr" rid="c68">68</xref></sup>, with 70 different L2 hyperparameter candidates linearly spaced between 0.1 (high regularization) and 1 (no regularization). We opted for L2 regularization for increased stability and to avoid overfitting. Since SPoSE dimensions were non-negative, predicted dimension scores below zero were set to zero. This led to a predicted SPoSE embedding. In step 2, we computed a predicted representational similarity matrix by predicting the choice probabilities for each pair of stimuli in the context of all possible combinations of triplets. Finally, we used Pearson correlation to relate the predicted similarity matrix with a ground-truth similarity matrix. For the other image sets, we used the entire 1,854 reference image set for training, determined the hyperparameter with cross-validation, and predicted the similarity matrices for external datasets accordingly. Since the Mur-92 and Cichy-118 datasets were based on dissimilarities sampled using the multiple arrangement task, in this case dissimilarities were derived by computing Euclidean distances from the embeddings directly. To facilitate future use of DimPred, we provide a toolbox to run DimPred in Python (<ext-link ext-link-type="uri" xlink:href="https://github.com/ViCCo-Group/dimpred">https://github.com/ViCCo-Group/dimpred</ext-link>).</p>
</sec>
<sec id="s4d">
<title>Ground-truth similarity matrices</title>
<p>The similarity matrix for the 1,854 reference image set was taken from Hebart and colleagues<sup><xref ref-type="bibr" rid="c36">36</xref></sup>. Please note that, for simplicity, we refer to the similarity matrix derived from this embedding as “ground-truth”, even though this is only a predicted similarity. Since the performance of DimPred may reflect overfitting to the prediction of SPoSE rather than perceived similarity predicted by SPoSE, we validated DimPred out-of-sample with the other datasets. The ground-truth similarity scores for the 48-image-generalization image set and the 48-concept-generalization image set were sampled in a separate online behavioral study (see below). Ground-truth behavioral (dis-)similarity scores for the other image sets were available online or provided by the authors of the respective original article.</p>
</sec>
<sec id="s4e">
<title>Comparison to classical and feature-reweighted RSA</title>
<p>To put the performance of DimPred into perspective, we compared it to classical and feature-reweighted RSA (cRSA and FR-RSA, respectively). Both are established methods used frequently to relate representational spaces to each other. Classical RSA is computed based on the raw activations of a given computational model for a given image set. Representational similarity matrices (RSMs) are compared by flattening the upper or lower triangular part of the similarity matrix and correlating it to the respective part of the ground-truth similarity matrix. As a comparison metric, we used Pearson correlation. Mur-92 and Cichy-118, we computed Euclidean distance scores based on the raw activations. For all of the other image sets, we computed Pearson similarity scores based on the raw activations.</p>
<p>FR-RSA is similar to classical RSA but reweights individual features using cross-validation, with the aim to maximize the linear correspondence with another representational space. By combining reweighted feature-specific similarities, FR-RSA achieves much greater power when relating representational spaces to each other<sup><xref ref-type="bibr" rid="c45">45</xref></sup>. For the image sets Mur-92 and Cichy-118, we computed squared Euclidean distances within the predicting features. For all of the other image sets, we computed dot-product similarities within the predicting features.</p>
</sec>
<sec id="s4f">
<title>Behavioral experiments</title>
<p>We conducted two online behavioral experiments with human research participants. In the first experiment, we sampled dimensional values for images while in the second experiment, we sampled odd-one-out data for images.</p>
</sec>
<sec id="s4g">
<title>Experiment 1</title>
<sec id="s4g1">
<title>Participants</title>
<p>We recruited 1,097 participants via Prolific (<ext-link ext-link-type="uri" xlink:href="https://www.prolific.com">www.prolific.com</ext-link>) residing in the United States. Participants who were overly fast or with overly repetitive responses or who did not complete the task were excluded, leaving 763 participants (566 female, 189 male, 8 other, mean age: 29.42, range: 18 - 74, SD = 10.08). Participants were compensated with an equivalent of 3 GBP per session and provided informed consent as well as agreed to the data storage policy before beginning with the experiment. The experiment was approved by the ethics commission of Leipzig University’s Faculty of Medicine (211/21-ek) and we have complied with all relevant ethical regulations.</p>
</sec>
<sec id="s4g2">
<title>Design and Stimuli</title>
<p>The experiment was run online via Pavlovia<sup><xref ref-type="bibr" rid="c69">69</xref></sup> within each participant’s browser. In the first experiment, we let participants rate images of natural objects on each of the 49 SpoSE dimensions separately. As test images, we used the 48-image-generalization and the 48-concept-generalization image sets (and a third image set containing 200 images, see Supplement 3) making it a total of 296 images that were rated by participants. Participants were allowed to take part several times by rating different dimensions. Most participants (83.49%) rated one dimension only. Dimensions were presented visually using Likert-type scales, with example images arranged along the scale based on their scores for each dimension. Participants were required to rate different images based on the presented dimensions.</p>
<p>The rating scale consisted of seven points, where the lowest point (“not at all”), the second to lowest point (“very low”) and the highest point (“very high”) were labeled. Additionally, we used anchor images to visualize each point of the scale. Those anchor images were sampled from the reference image set but were not part of the rated images or the validation sets. Each dimension scale was labeled with the dimension labels from Hebart and colleagues<sup><xref ref-type="bibr" rid="c36">36</xref></sup>. Participants were able to click anywhere on the scale. Scales were percentile-based and were later converted back to continuous dimension scores.</p>
<p>Participants were trained on the task and received feedback. During the test phase, each participant rated all 296 test images on a given dimension. The final dataset after participant exclusion consisted of an average of 19.31 ratings per dimension (946 dimension ratings across the 49 dimensions).</p>
</sec>
</sec>
<sec id="s4h">
<title>Experiment 2</title>
<sec id="s4h1">
<title>Participants</title>
<p>498 online workers (mean age: 41.2, SD = 12.7, 275 female, 223 male) from the crowdsourcing platform Amazon Mechanical Turk took part in this experiment. Participants received 0.1 USD per completed session and provided informed consent as well as agreed to the data storage policy before beginning with the experiment. The experiment was approved by the ethics commission of Leipzig University’s Faculty of Medicine (211/21-ek) and we have complied with all relevant ethical regulations.</p>
</sec>
<sec id="s4h2">
<title>Design and Stimuli</title>
<p>The experiment was conducted on Amazon Mechanical Turk using a triplet odd-one-out task. In each trial, participants were presented with three images of natural objects and asked to select the image that was least similar to the other two. Each session consisted of 20 trials, and participants could participate in as many sessions as they wished. The images were taken from the 48-image-generalization and 48-concept-generalization sets, but images from the two sets were not mixed. We collected a total of 69,200 trials, which covered all possible triplet combinations of both image sets twice. Participants whose responses were too repetitive or too quick were excluded, leaving 63,440 trials. From these triplet ratings, we computed similarity scores, which served as ground-truth similarity matrices for downstream analyses.</p>
</sec>
</sec>
<sec id="s4i">
<title>Heatmap generation</title>
<p>To identify which image features were most relevant for predicting SPoSE dimensions, we used the image explanation method Randomized Input Sampling For Explanation (RISE<sup><xref ref-type="bibr" rid="c59">59</xref></sup>). RISE is an image occlusion technique for generating heatmaps that highlight the relevance of different image parts to an embedding dimension. In brief, RISE applies a series of random masks to the original image. Each mask probabilistically determines whether each pixel is preserved, reduced in intensity, or set to zero. These masked images are then used to predict the SPoSE embeddings. The differences between the predicted values for the masked images and the original image are recorded. By averaging the results from many such masks, we obtain a relevance score indicating each pixel’s contribution to each embedding dimension.</p>
</sec>
<sec id="s4j">
<title>Statistical analyses</title>
<p>We calculated Pearson’s <italic>r</italic> between predicted and ground-truth similarity scores and, where applicable, tested Fisher’s z transformed differences between Pearson’s <italic>r</italic> values (two-sided).</p>
</sec>
<sec id="s4k">
<title>Software packages</title>
<p>For the DimPred implementation and all ensuing analyses, we used Python (Python Software Foundation. Python Language Reference, available at <ext-link ext-link-type="uri" xlink:href="http://www.python.org">http://www.python.org</ext-link>) as well as the following Python main libraries: fracridge<sup><xref ref-type="bibr" rid="c68">68</xref></sup>, joblib (<ext-link ext-link-type="uri" xlink:href="https://github.com/joblib/joblib">https://github.com/joblib/joblib</ext-link>), matplotlib<sup><xref ref-type="bibr" rid="c70">70</xref></sup>, numba<sup><xref ref-type="bibr" rid="c71">71</xref></sup>, numpy<sup><xref ref-type="bibr" rid="c72">72</xref></sup>, pandas<sup><xref ref-type="bibr" rid="c73">73</xref></sup>, seaborn<sup><xref ref-type="bibr" rid="c74">74</xref></sup>, scikit-image<sup><xref ref-type="bibr" rid="c75">75</xref></sup>, scikit-learn<sup><xref ref-type="bibr" rid="c76">76</xref></sup>, scipy<sup><xref ref-type="bibr" rid="c77">77</xref></sup> and thingsvision<sup><xref ref-type="bibr" rid="c78">78</xref></sup>. We used the FR-RSA toolbox to conduct FR-RSA<sup><xref ref-type="bibr" rid="c45">45</xref></sup>.</p>
</sec>
</sec>
</body>
<back>
<sec id="s5" sec-type="data-availability">
<title>Data availability</title>
<p>All result files pertaining to this study will be made available upon publication via an OSF repository (<ext-link ext-link-type="uri" xlink:href="https://osf.io/jtekq">https://osf.io/jtekq</ext-link>).</p>
</sec>
<sec id="s6">
<title>Code availability</title>
<p>All analysis scripts pertaining to this study will be made available upon publication via an GitHub repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/ViCCo-Group/dimpred_paper">https://github.com/ViCCo-Group/dimpred_paper</ext-link>). The toolbox to run DimPred itself in Python is available via GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/ViCCo-Group/dimpred">https://github.com/ViCCo-Group/dimpred</ext-link>).</p>
</sec>
<sec id="s9">
<title>Materials &amp; Correspondence</title>
<p>Correspondence and material requests should be addressed to Philipp Kaniuth (<email>kaniuth@cbs.mpg.de</email>) or Martin Hebart (<email>hebart@cbs.mpg.de</email>)</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank Oliver Contier for his help with setting up the encoding models, Johannes Roth and Magdalena Gippert for helpful discussions, and Joshua Peterson and Ian Charest for sharing the similarity datasets used for validation. This work was supported by a research group grant by the Max Planck Society awarded to M.N.H., the ERC Starting Grant project COREDIM (ERC- StG-2021-101039712) awarded to M.N.H., and the Hessian Ministry of Higher Education, Science, Research and Art (LOEWE Start Professorship to M.N.H. and Excellence Program “The Adaptive Mind”). Open access funding provided by Max Planck Society. The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.</p>
</ack>
<sec id="d1e1311" sec-type="additional-information">
<title>Additional information</title>
<sec id="s7">
<title>Credit authorship contribution statement</title>
<p>Philipp Kaniuth: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing – original draft, Writing – review &amp; editing, Visualization</p>
<p>Florian P. Mahner: Formal analysis, Visualization, Writing – review &amp; editing</p>
<p>Jonas Perkuhn: Investigation, Writing – review &amp; editing</p>
<p>Martin N. Hebart: Conceptualization, Methodology, Resources, Writing – review &amp; editing, Supervision, Project administration, Funding acquisition</p>
</sec>
</sec>
<sec id="suppd1e1311" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e1302">
<label>supplementary_material</label>
<media xlink:href="supplements/601184_file02.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Pitt</surname>, <given-names>D.</given-names></string-name></person-group> <chapter-title>Mental Representation</chapter-title>. <source>Stanford Encyclopedia of Philosophy</source>, <publisher-name>Stanford University</publisher-name> (<year>2000</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Goldstone</surname>, <given-names>R. L.</given-names></string-name> &amp; <string-name><surname>Son</surname>, <given-names>J. Y.</given-names></string-name></person-group>, <year>2012</year>).<chapter-title>Similarity</chapter-title>. in <person-group person-group-type="editor"><string-name><surname>Holyoak</surname>, <given-names>K. J.</given-names></string-name> &amp; <string-name><surname>Morrison</surname>, <given-names>R. G.</given-names></string-name></person-group><source>The Oxford handbook of thinking and reasoning</source> (eds.  &amp; ) <fpage>155</fpage>–<lpage>176</lpage> (<publisher-name>Oxford University Press</publisher-name>, ).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deerwester</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Dumais</surname>, <given-names>S. T.</given-names></string-name>, <string-name><surname>Furnas</surname>, <given-names>G. W.</given-names></string-name>, <string-name><surname>Landauer</surname>, <given-names>T. K.</given-names></string-name> &amp; <string-name><surname>Harshman</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>Indexing by latent semantic analysis</article-title>. <source>J. Am. Soc. Inf. Sci</source>. <volume>41</volume>, <fpage>391</fpage>–<lpage>407</lpage> (<year>1990</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Edelman</surname>, <given-names>S</given-names></string-name></person-group>. <article-title>Representation is representation of similarities</article-title>. <source>Behav. Brain Sci</source>. <volume>21</volume>, <fpage>449</fpage>–<lpage>467</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Salton</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Wong</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Yang</surname>, <given-names>C. S</given-names></string-name></person-group>. <article-title>A vector space model for automatic indexing</article-title>. <source>Commun. ACM</source> <volume>18</volume>, <fpage>613</fpage>–<lpage>620</lpage> (<year>1975</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shepard</surname>, <given-names>R. N</given-names></string-name></person-group>. <article-title>Toward a Universal Law of Generalization for Psychological Science</article-title>. <source>Science</source> <volume>237</volume>, <fpage>1317</fpage>–<lpage>1323</lpage> (<year>1987</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chaffin</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Herrmann</surname>, <given-names>D. J</given-names></string-name></person-group>. <article-title>Effects of Relation Similarity on Part-Whole Decisions</article-title>. <source>J. Gen. Psychol</source>. <volume>115</volume>, <fpage>131</fpage>–<lpage>139</lpage> (<year>1988</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ferko</surname>, <given-names>K. M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Activity in perirhinal and entorhinal cortex predicts perceived visual similarities among category exemplars with highest precision</article-title>. <source>eLife</source> <volume>11</volume>, <elocation-id>e66884</elocation-id> (<year>2022</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hahn</surname>, <given-names>U.</given-names></string-name> and <string-name><surname>Ramscar</surname>, <given-names>M.</given-names></string-name></person-group> <source>Similarity and Categorization</source>. (<publisher-name>Oxford University Press</publisher-name>:<publisher-loc>Oxford</publisher-loc>, <year>2001</year>). doi:<pub-id pub-id-type="doi">10.1093/acprof:oso/9780198506287.001.0001</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nosofsky</surname>, <given-names>R. M</given-names></string-name></person-group>. <article-title>Attention, similarity, and the identification–categorization relationship</article-title>. <source>J. Exp. Psychol. Gen</source>. <volume>115</volume>, <fpage>39</fpage>–<lpage>57</lpage> (<year>1986</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Op De Beeck</surname> <given-names>H. P.</given-names></string-name>, <string-name><surname>Torfs</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Wagemans</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Perceived Shape Similarity among Unfamiliar Objects and the Organization of the Human Object Vision Pathway</article-title>. <source>J. Neurosci</source>. <volume>28</volume>, <fpage>10111</fpage>–<lpage>10123</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name> &amp; <string-name><surname>Griffiths</surname>, <given-names>T. L.</given-names></string-name></person-group> <article-title>Generalization, similarity, and Bayesian inference</article-title>. <source>Behav. Brain Sci</source>. <volume>24</volume>, <fpage>629</fpage>–<lpage>640</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tversky</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Features of similarity</article-title>. <source>Psychol. Rev</source>. <volume>84</volume>, <fpage>327</fpage>–<lpage>352</lpage> (<year>1977</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bankson</surname>, <given-names>B. B.</given-names></string-name>, <string-name><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name>, <string-name><surname>Groen</surname>, <given-names>I. I. A.</given-names></string-name> &amp; <string-name><surname>Baker</surname>, <given-names>C. I</given-names></string-name></person-group>. <article-title>The temporal evolution of conceptual object representations revealed through models of behavior, semantics and deep neural networks</article-title>. <source>NeuroImage</source> <volume>178</volume>, <fpage>172</fpage>–<lpage>182</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Jozwik</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>van den Bosch</surname>, <given-names>J. J. F.</given-names></string-name> &amp; <string-name><surname>Charest</surname>, <given-names>I.</given-names></string-name></person-group> <article-title>The spatiotemporal neural dynamics underlying perceived similarity for real-world objects</article-title>. <source>NeuroImage</source> <volume>194</volume>, <fpage>12</fpage>–<lpage>24</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jozwik</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Storrs</surname>, <given-names>K. R.</given-names></string-name> &amp; <string-name><surname>Mur</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Deep Convolutional Neural Networks Outperform Feature-Based But Not Categorical Models in Explaining Object Similarity Judgments</article-title>. <source>Front. Psychol</source>. <volume>8</volume>, 1726 (<year>2017</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Bandettini</surname>, <given-names>P. A</given-names></string-name></person-group>. <article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title>. <source>Front. Syst. Neurosci</source>. <volume>2</volume>, (<year>2008</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ekman</surname>, <given-names>G</given-names></string-name></person-group>. <article-title>Dimensions of Color Vision</article-title>. <source>J. Psychol</source>. <volume>38</volume>, <fpage>467</fpage>–<lpage>474</lpage> (<year>1954</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shepard</surname>, <given-names>R. N.</given-names></string-name> &amp; <string-name><surname>Arabie</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>Additive clustering: Representation of similarities as combinations of discrete overlapping properties</article-title>. <source>Psychol. Rev</source>. <volume>86</volume>, <fpage>81</fpage>–<lpage>123</lpage> (<year>1979</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tversky</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Krantz</surname>, <given-names>D. H</given-names></string-name></person-group>. <article-title>Similarity of schematic faces: A test of interdimensional additivity</article-title>. <source>Percept. Psychophys</source>. <volume>5</volume>, <fpage>124</fpage>–<lpage>128</lpage> (<year>1969</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rao</surname>, <given-names>V. R.</given-names></string-name> &amp; <string-name><surname>Katz</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>Alternative Multidimensional Scaling Methods for Large Stimulus Sets</article-title>. <source>J. Mark. Res</source>. <volume>8</volume>, <fpage>488</fpage>–<lpage>494</lpage> (<year>1971</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsogo</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Masson</surname>, <given-names>M. H.</given-names></string-name> &amp; <string-name><surname>Bardot</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>Multidimensional Scaling Methods for Many-Object Sets: A Review</article-title>. <source>Multivar. Behav. Res</source>. <volume>35</volume>, <fpage>307</fpage>–<lpage>319</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goldstone</surname>, <given-names>R. L</given-names></string-name></person-group>. <article-title>An efficient method for obtaining similarity data</article-title>. <source>Behav. Res. Methods Instrum. Comput</source>. <volume>26</volume>, <fpage>381</fpage>–<lpage>386</lpage> (<year>1994</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hout</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Goldinger</surname>, <given-names>S. D.</given-names></string-name> &amp; <string-name><surname>Ferguson</surname>, <given-names>R. W</given-names></string-name></person-group>. <article-title>The versatility of SpAM: A fast, efficient, spatial method of data collection for multidimensional scaling</article-title>. <source>J. Exp. Psychol. Gen</source>. <volume>142</volume>, <fpage>256</fpage>–<lpage>281</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Mur</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Inverse MDS: Inferring Dissimilarity Structure from Multiple Item Arrangements</article-title>. <source>Front. Psychol</source>. <volume>3</volume>, (<year>2012</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lorusso</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Pulina</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Grosso</surname>, <given-names>E</given-names></string-name></person-group>. <article-title>The Measure of Perceived Similarity Between Faces: Old Issues for a New Method</article-title>. <source>Rev. Philos. Psychol</source>. <volume>6</volume>, <fpage>317</fpage>–<lpage>339</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Messick</surname>, <given-names>S. J</given-names></string-name></person-group>. <article-title>Some Recent Theoretical Developments in Multidimensional Scaling</article-title>. <source>Educ. Psychol. Meas</source>. <volume>16</volume>, <fpage>82</fpage>–<lpage>100</lpage> (<year>1956</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Navarro</surname>, <given-names>D. J.</given-names></string-name> &amp; <string-name><surname>Griffiths</surname>, <given-names>T. L</given-names></string-name></person-group>. <article-title>Latent Features in Similarity Judgments: A Nonparametric Bayesian Approach</article-title>. <source>Neural Comput</source>. <volume>20</volume>, <fpage>2597</fpage>–<lpage>2628</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Alvarez</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Nakayama</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Konkle</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>Visual search for object categories is predicted by the representational architecture of high-level visual cortex</article-title>. <source>J. Neurophysiol</source>. <volume>117</volume>, <fpage>388</fpage>–<lpage>402</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Magri</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Konkle</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>Comparing facets of behavioral object representation: implicit perceptual similarity matches brains and models</article-title>. in <conf-name>2019 Conference on Cognitive Computational Neuroscience</conf-name> (<publisher-name>Cognitive Computational Neuroscience</publisher-name>, <publisher-loc>Berlin, Germany</publisher-loc>, <year>2019</year>). doi:<pub-id pub-id-type="doi">10.32470/CCN.2019.1395-0</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mohan</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Arun</surname>, <given-names>S. P</given-names></string-name></person-group>. <article-title>Similarity relations in visual search predict rapid visual categorization</article-title>. <source>J. Vis</source>. <volume>12</volume>, <issue>19</issue> (<year>2012</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Biederman</surname>, <given-names>I</given-names></string-name></person-group>. <article-title>Recognition-by-components: A theory of human image understanding</article-title>. <source>Psychol. Rev</source>. <volume>94</volume>, <fpage>115</fpage>–<lpage>147</lpage> (<year>1987</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Osherson</surname>, <given-names>D. N.</given-names></string-name>, <string-name><surname>Stern</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wilkie</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Stob</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Smith</surname>, <given-names>E. E.</given-names></string-name></person-group> <article-title>Default Probability</article-title>. <source>Cogn. Sci</source>. <volume>15</volume>, <fpage>251</fpage>–<lpage>269</lpage> (<year>1991</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tversky</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Hemenway</surname>, <given-names>K.</given-names></string-name></person-group> <article-title>Objects, parts, and categories</article-title>. <source>J. Exp. Psychol. Gen</source>. <volume>113</volume>, <fpage>169</fpage>–<lpage>193</lpage> (<year>1984</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Marjieh</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Words are all you need? Language as an approximation for human similarity judgments</article-title>. <source>arXiv</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2206.04105">http://arxiv.org/abs/2206.04105</ext-link> (<year>2023</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name>, <string-name><surname>Zheng</surname>, <given-names>C. Y.</given-names></string-name>, <string-name><surname>Pereira</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Baker</surname>, <given-names>C. I</given-names></string-name></person-group>. <article-title>Revealing the multidimensional mental representations of natural objects underlying human similarity judgements. <italic>Nat</italic></article-title>. <source>Hum. Behav</source>. <volume>4</volume>, <fpage>1173</fpage>–<lpage>1185</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Günther</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Marelli</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Tureski</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Petilli</surname>, <given-names>M. A</given-names></string-name></person-group>. <article-title>ViSpa (Vision Spaces): A computer-vision-based representation system for individual images and concept prototypes, with large-scale evaluation</article-title>. <source>Psychol. Rev</source>. <volume>130</volume>, <fpage>896</fpage>–<lpage>934</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Marjieh</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sucholutsky</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Sumers</surname>, <given-names>T. R.</given-names></string-name>, <string-name><surname>Jacoby</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Griffiths</surname>, <given-names>T. L.</given-names></string-name></person-group> <article-title>Predicting Human Similarity Judgments Using Large Language Models</article-title>. <source>arXiv</source> (<year>2022</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marjieh</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sucholutsky</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>van Rijn</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Jacoby</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Griffiths</surname>, <given-names>T. L</given-names></string-name></person-group>. <article-title>Large language models predict human sensory judgments across six modalities</article-title>. <source>Preprint at</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2302.01308">http://arxiv.org/abs/2302.01308</ext-link> (<year>2023</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hansen</surname>, <given-names>H. J.</given-names></string-name> &amp; <string-name><surname>Hebart</surname>, <given-names>M. N</given-names></string-name></person-group>. <article-title>Automatic generation of semantic feature norms of objects using GPT-3</article-title>. <source>J. Vis</source>. <volume>22</volume>, 3461 (<year>2022</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peterson</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Abbott</surname>, <given-names>J. T.</given-names></string-name> &amp; <string-name><surname>Griffiths</surname>, <given-names>T. L</given-names></string-name></person-group>. <article-title>Evaluating (and Improving) the Correspondence Between Deep Neural Networks and Human Representations</article-title>. <source>Cogn. Sci</source>. <volume>42</volume>, <fpage>2648</fpage>–<lpage>2669</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jha</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Peterson</surname>, <given-names>J. C.</given-names></string-name> &amp; <string-name><surname>Griffiths</surname>, <given-names>T. L</given-names></string-name></person-group>. <article-title>Extracting Low-Dimensional Psychological Representations from Convolutional Neural Networks</article-title>. <source>Cogn. Sci</source>. <volume>47</volume>, (<year>2023</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sanders</surname>, <given-names>C. A.</given-names></string-name> &amp; <string-name><surname>Nosofsky</surname>, <given-names>R. M</given-names></string-name></person-group>. <article-title>Training Deep Networks to Construct a Psychological Feature Space for a Natural-Object Category Domain. <italic>Comput</italic></article-title>. <source>Brain Behav</source>. <volume>3</volume>, <fpage>229</fpage>–<lpage>251</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>THINGS: A database of 1,854 object concepts and more than 26,000 naturalistic object images</article-title>. <source>PLOS One</source> <volume>14</volume>, <fpage>e0223792</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kaniuth</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Hebart</surname>, <given-names>M. N</given-names></string-name></person-group>. <article-title>Feature-reweighted representational similarity analysis: A method for improving the fit between computational models, brains, and behavior</article-title>. <source>NeuroImage</source> <volume>257</volume>, <issue>119294</issue> (<year>2022</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khaligh-Razavi</surname>, <given-names>S.-M.</given-names></string-name> &amp; <string-name><surname>Kriegeskorte</surname>, <given-names>N</given-names></string-name></person-group>. <article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation</article-title>. <source>PLoS Comput. Biol</source>. <volume>10</volume>, <fpage>e1003915</fpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Storrs</surname>, <given-names>K. R.</given-names></string-name>, <string-name><surname>Kietzmann</surname>, <given-names>T. C.</given-names></string-name>, <string-name><surname>Walther</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mehrer</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Kriegeskorte</surname>, <given-names>N</given-names></string-name></person-group>. <article-title>Diverse Deep Neural Networks All Predict Human Inferior Temporal Cortex Well, After Training and Fitting</article-title>. <source>J. Cogn. Neurosci</source>. <volume>33</volume>, <fpage>2044</fpage>–<lpage>2064</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Geirhos</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal></person-group>., <year>2021</year>). <article-title>Partial success in closing the gap between human and machine vision</article-title>. in <person-group person-group-type="editor"><string-name><surname>Ranzato</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Beygelzimer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dauphin</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Liang</surname>, <given-names>P. S.</given-names></string-name> &amp; <string-name><surname>Vaughan</surname>, <given-names>J. W.</given-names></string-name></person-group> (eds.) <conf-name>Advances in Neural Information Processing Systems</conf-name> vol. <volume>34</volume> <fpage>23885</fpage>–<lpage>23899</lpage> (<publisher-name>Curran Associates, Inc</publisher-name>., ).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>A. Y.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name> &amp; <string-name><surname>Wehbe</surname>, <given-names>L</given-names></string-name></person-group>. <article-title>Better models of human high-level visual cortex emerge from natural language supervision with a large and diverse dataset. <italic>Nat</italic></article-title>. <source>Mach. Intell</source>. <volume>5</volume>, <fpage>1415</fpage>–<lpage>1426</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Conwell</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Prince</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Alvarez</surname>, <given-names>G. A.</given-names></string-name> &amp; <string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name></person-group> <article-title>What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?</article-title> <source>bioRxiv</source> <elocation-id>2022.03.28.485868</elocation-id>. <pub-id pub-id-type="doi">10.1101/2022.03.28.485868</pub-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Muttenthaler</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Dippel</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Linhardt</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Vandermeulen</surname>, <given-names>R. A.</given-names></string-name> &amp; <string-name><surname>Kornblith</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Human alignment of neural network representations</article-title>. <source>arXiv</source> <pub-id pub-id-type="doi">10.48550/arXiv.2211.01201</pub-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Doerig</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Semantic scene descriptions as an objective of human vision</article-title>. <source>arXiv</source> <pub-id pub-id-type="doi">10.48550/arXiv.2209.11737</pub-id> (<year>2022</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jozwik</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Mur</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Visual features as stepping stones toward semantics: Explaining object similarity in IT and perception with non-negative least squares</article-title>. <source>Neuropsychologia</source> <volume>83</volume>, <fpage>201</fpage>–<lpage>226</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Pantazis</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Oliva</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>Resolving human object recognition in space and time</article-title>. <source>Nat. Neurosci</source>. <volume>17</volume>, <fpage>455</fpage>–<lpage>462</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey</article-title>. <source>Neuron</source> <volume>60</volume>, <fpage>1126</fpage>–<lpage>1141</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Human Object-Similarity Judgments Reflect and Transcend the Primate-IT Object Representation</article-title>. <source>Front. Psychol</source>. <volume>4</volume>, (<year>2013</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Khosla</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pantazis</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Torralba</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Oliva</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title>. <source>Sci. Rep</source>. <volume>6</volume>, <issue>27755</issue> (<year>2016</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zeiler</surname>, <given-names>M. D.</given-names></string-name> &amp; <string-name><surname>Fergus</surname>, <given-names>R.</given-names></string-name></person-group>, <year>2014</year>).<chapter-title>Visualizing and Understanding Convolutional Networks</chapter-title>. in <person-group person-group-type="editor"><string-name><surname>Fleet</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Pajdla</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Schiele</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Tuytelaars</surname>, <given-names>T.</given-names></string-name></person-group><source>Computer Vision – ECCV</source> 2014 (eds. , ,  &amp; ) vol. 8689 818–833 (<publisher-name>Springer International Publishing</publisher-name>, <publisher-loc>Cham</publisher-loc>, ).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Petsiuk</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Das</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Saenko</surname>, <given-names>K.</given-names></string-name></person-group> <article-title>RISE: Randomized Input Sampling for Explanation of Black-box Models</article-title>. <source>arXiv</source> <pub-id pub-id-type="doi">10.48550/arXiv.1806.07421</pub-id> (<year>2018</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior</article-title>. <source>eLife</source> <volume>12</volume>, <elocation-id>e82580</elocation-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Contier</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name> &amp; <string-name><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name></person-group> <article-title>Distributed representations of behaviorally-relevant object dimensions in the human visual system</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2023.08.23.553812</pub-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Teichmann</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name> &amp; <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name></person-group> <article-title>Multidimensional object properties are dynamically represented in the human brain</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2023.09.08.556679</pub-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Walbrin</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sossounov</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mahdiani</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Vaz</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Almeida</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Fine-grained knowledge about manipulable objects is well-predicted by CLIP</article-title>. <source>PsyArXiv</source> <pub-id pub-id-type="doi">10.31234/osf.io/qym4s</pub-id> (<year>2024</year>).</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Linsley</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Shiebler</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Eberhardt</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Serre</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>Learning what and where to attend</article-title>. <source>Preprint at</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1805.08819">http://arxiv.org/abs/1805.08819</ext-link> (<year>2019</year>).</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Kummerer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wallis</surname>, <given-names>T. S. A.</given-names></string-name>, <string-name><surname>Gatys</surname>, <given-names>L. A.</given-names></string-name> &amp; <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>Understanding Low-and High-Level Contributions to Fixation Prediction</article-title>. in <conf-name>2017 IEEE International Conference on Computer Vision (ICCV)</conf-name> <fpage>4789</fpage>–<lpage>4798</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Russakovsky</surname>, <given-names>O.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>ImageNet Large Scale Visual Recognition Challenge</article-title>. <source>Int. J. Comput. Vis</source>. <volume>115</volume>, <fpage>211</fpage>–<lpage>252</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zheng</surname>, <given-names>C. Y.</given-names></string-name>, <string-name><surname>Pereira</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name> &amp; <string-name><surname>Hebart</surname>, <given-names>M. N</given-names></string-name></person-group>. <article-title>Revealing interpretable object representations from human behavior</article-title>. <source>ArXi</source><issue><italic>v190102915</italic></issue> <italic>Cs Q-Bio Stat</italic> (<year>2019</year>).</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rokem</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Kay</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>Fractional ridge regression: a fast, interpretable reparameterization of ridge regression</article-title>. <source>GigaScience</source> <volume>9</volume>, (<year>2020</year>).</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="other"><source>Pavlovia Surveys. Open Science Tools</source>. <year>no date</year></mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hunter</surname>, <given-names>J. D</given-names></string-name></person-group>. <article-title>Matplotlib: A 2D graphics environment</article-title>. <source>Comput. Sci. Eng</source>. <volume>9</volume>, <fpage>90</fpage>–<lpage>95</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Lam</surname>, <given-names>S. K.</given-names></string-name>, <string-name><surname>Pitrou</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Seibert</surname>, <given-names>S</given-names></string-name></person-group>. <article-title>Numba: a LLVM-based Python JIT compiler</article-title>. in <conf-name>Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC</conf-name> <fpage>1</fpage>–<lpage>6</lpage> (<publisher-name>ACM</publisher-name>, <publisher-loc>Austin Texas</publisher-loc>, <year>2015</year>). doi:<pub-id pub-id-type="doi">10.1145/2833157.2833162</pub-id>.</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harris</surname>, <given-names>C. R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Array programming with NumPy</article-title>. <source>Nature</source> <volume>585</volume>, <fpage>357</fpage>–<lpage>362</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><collab>The pandas development team</collab></person-group>. <article-title>pandas-dev/pandas: Pandas</article-title>. <source>Zenodo</source> <pub-id pub-id-type="doi">10.5281/zenodo.10697587</pub-id> (<year>2024</year>).</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Waskom</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>seaborn: statistical data visualization</article-title>. <source>J. Open Source Softw</source>. <volume>6</volume>, 3021 (<year>2021</year>).</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van der Walt</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>scikit-image: image processing in Python</article-title>. <source>PeerJ</source> <volume>2</volume>, <fpage>e453</fpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Scikit-learn: Machine Learning in Python</article-title>. <source>J. Mach. Learn. Res</source>. <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Virtanen</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</article-title>. <source>Nat. Methods</source> <volume>17</volume>, <fpage>261</fpage>–<lpage>272</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Muttenthaler</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Hebart</surname>, <given-names>M. N</given-names></string-name></person-group>. <article-title>THINGSvision: A Python Toolbox for Streamlining the Extraction of Activations From Deep Neural Networks. <italic>Front</italic></article-title>. <source>Neuroinformatics</source> <volume>15</volume>, <issue>679838</issue> (<year>2021</year>).</mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stoinski</surname>, <given-names>L. M.</given-names></string-name>, <string-name><surname>Perkuhn</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Hebart</surname>, <given-names>M. N</given-names></string-name></person-group>. <article-title>THINGSplus: New norms and metadata for the THINGS database of 1854 object concepts and 26,107 natural object images</article-title>. <source>Behav Res</source> <volume>56</volume>, <fpage>1583</fpage>–<lpage>1603</lpage> (<year>2023</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105394.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bracci</surname>
<given-names>Stefania</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Trento</institution>
</institution-wrap>
<city>Rovereto</city>
<country>Italy</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>The authors show that an automated approach using artificial neural networks, which focuses on behaviourally relevant dimensions, can predict human similarity data up to a certain level of granularity. This study has the potential to be a <bold>valuable</bold> contribution to the broader field of cognitive computational neuroscience, as it provides a tool for the automated collection of similarity judgments under certain conditions. However, as of now, the significance of this method is somewhat limited because of its inability to generalise beyond between-category distinctions and the limited model evaluation. In terms of broader implications, the degree to which this work provides insights into DNN-brain alignment and a better understanding of the functional organisation of the visual system is supported by <bold>incomplete</bold> evidence.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105394.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This manuscript addresses the challenge of understanding and capturing the similarity among large numbers of visual images. The authors show that an automated approach using artificial neural networks that focuses upon the embedding of similarity through behaviorally relevant dimensions can predict human similarity data up to a certain level of granularity.</p>
<p>Strengths:</p>
<p>The manuscript starts with a very useful introduction that sets the stage with an insightful Figure 1. The methods are state of the art and well thought off, and the data are compelling. The authors demonstrate the added value of their approach in several directions, resulting in a manuscript that is highly relevant for different domains. The authors also explore its limitations (e.g., granularity).</p>
<p>Weaknesses:</p>
<p>Although this manuscript and the work it describes are already of high quality, I see several ways in which it could be further improved. Below I rank these suggestions tentatively in order of importance.</p>
<p>Predictions obtain correlations above 0.80, often close to correlations of 0.90. The performance of DimPred is not trivial, given how much better it performs relative to classic RSA and feature reweighting. Yet, the ceiling is not sufficiently characterized. What is the noise ceiling in the main and additional similarity sets that are used? If the noise ceiling is higher than the prediction correlations, then can the authors try to find the stimulus pairs for which the approach systematically fails to capture similarity? Or is the mismatch very distributed across the full stimulus set?</p>
<p>Also in the section on p. 8-p.9, it is crucial to provide information on the noise ceiling of the various datasets.</p>
<p>This consideration of noise ceiling brings me to another consideration. Arguments have been made that a focus on overall prediction accuracy might mask important differences in underlying processes that can be demonstrated in more specific, experimental situations (Bowers et al., 2023). Can the authors exclude the possibility that their automatic approach would fail dramatically in specifically engineered situations? Some examples can be found in the 2024 challenge of the BrainScore platform. How can future users of this approach know whether they are in such a situation or not?</p>
<p>The authors demonstrated one limitation of the DimPred approach to capture fine-grained similarity among highly similar stimuli. The implications of this finding were not clear to me from the Abstract etc, because it is not sufficiently highlighted in the summaries that in this case DimPred performs even worse, and much worse, than more simple approaches like feature reweighting and even than classic RSA. I would discuss this outcome more in detail. With hindsight, this problem might not be so surprising given that DimPred relies upon the embedding with a few tens dimensions that mostly capture between-category differences. To me, this seems like a more fundamental limitation than a mere problem of granularity or lack of data, as suggested in the abstract.</p>
<p>The DimPred approach is based on the dimensions of a similarity embedding derived from human behavior. What is important here is (i) that DimPred is based upon an approach that tries to capture latent dimensions; or (ii) that these dimensions are behaviorally relevant? There are a lot of dimension-focused approaches. Generic ones are PCA, MDS, etc. More domain-specific approaches in cogneuro include the following: (i) for two-dimensional shape representations, good results have been obtained with image-computable dimensions of various levels of complexity (Morgenstern et al., 2021, PLOS Comput. Biol.); (ii) another dimension-focused approach has focused upon identifying dimensions that are universal across networks &amp; human representations (Chen &amp; Bonner, 2024, arXiv). Would such generic or more specific approaches work as well as DimPred?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105394.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this paper, the authors successfully incorporated the 49 dimensions found in a human similarity judgment task to better train DNNs to perform accurate human-like object similarity judgments. The results of the model performance are impressive but I am not totally convinced that the present modeling approach may bring new insights regarding the mental and neural representations of visual objects in the human brain. I have a few thoughts that I would like the authors to consider.</p>
<p>(1) Can the authors provide a detailed description of what these off-the-shelf DNNs are trained on? For models trained on visual images only, because semantic information was never present during training, it is not surprising they fail to capture such information, even with additional DimPred training. For the CLIP models, because visual-sematic associations were included during training, it again comes as no surprise that these models can do better even without DimPred training. Similarly, the results of homogenous image sets are not particularly surprising. In this regard, I am finding the paper reports many obvious results. Better motivations should be used to justify why particular models and analyses were performed, what predictions can be made, and how the results may be informative beyond what we already know.</p>
<p>(2) I am curious as to what DimPred training is doing exactly. If you create an arbitrary similarity structure (i.e., not the one derived from human similarity judgment) by, e.g., shuffling the values during training or creating 49 arbitrary dimensions, can the models be trained to follow this new arbitrary structure? In other words, do the models intrinsically contain a human-like structure, but we just have to find the right parameters to align them with the human structure or do we actually impose/force the human similarity structure onto the model with DimPred training?</p>
<p>Is it also an issue that you are including more parameters during DimPred training and that increased parameters alone can increase performance?</p>
<p>(3) There is very little information on how Figure 8 is generated. I couldn't find in the Methods any detailed descriptions of how the values were calculated. Are results from both the category-insensitive and category-sensitive embedding obtained from the same OpenCLIP-RN50x64? Figure 8 reports the relative improvement. What do the raw activation maps look like for the category-insensitive and category-sensitive embedding? I am surprised that the improvement is seen primarily in the early visual cortex (EVC) and higher visual areas but not more extensively in association areas sensitive to semantics. Why should EVC show such large improvements, given that category information is stored elsewhere?</p>
<p>Related to this point, how do other DNN models account for human brain fMRI responses in the present study? Many prior studies have documented the similarities and differences between DNN and human fMRI visual object representations. Do category-sensitive CLIP models outperform other DNN models? It is important to report the full results. Even though category-sensitive CLIP models outperform category-insensitive CLIP ones, if the overall model performance is low compared to the other DNNs, the results would not be very meaningful/impressive. I am just wondering if, in the process of achieving better human-like similarity judgment performance, these models lose some of the ability to account for visual object representations in the human ventral visual cortex.</p>
<p>(4) I am wondering how precisely the present results may yield new insights into the mental and neural representations of visual objects in the human brain. Prior human studies have already identified 49 dimensions that can capture human similarity judgment. Beyond predicting performance for new pairs of objects, how would the present modeling approach help us understand more about the human brain? The authors discussed this, but I am not sure the arguments are convincing.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105394.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors compare how well their automatic dimension prediction approach (DimPred) can support similarity judgements and compare it to more standard RSA approaches. The authors show that the DimPred approach does better when assessing out-of-sample heterogeneous image sets, but worse for out-of-sample homogeneous image sets. DimPred also does better at predicting brain-behaviour correspondences compared to an alternative approach. The work appears to be well done, but I'm left unsure what conclusions the authors are drawing.</p>
<p>In the abstract, the authors write: &quot;Together, our results demonstrate that current neural networks carry information sufficient for capturing broadly-sampled similarity scores, offering a pathway towards the automated collection of similarity scores for natural images&quot;. If that is the main claim, then they have done a reasonable job supporting this conclusion. However the importance of automating this process for broadly-sampled object categories is not made so clear.</p>
<p>But the authors also highlight the importance that similarity judgements have been for theories of cognition and brain, such as in the first paragraph of the paper they write: &quot;Similarity judgments allow us to improve our understanding of a variety of cognitive processes, including object recognition, categorization, decision making, and semantic memory6-13. In addition, they offer a convenient means for relating mental representations to representations in the human brain14,15 and other domains16,17&quot;. The fact that the authors also assess how well a CLIP model using DimPred can predict brain activation suggests that their work is not just about automating similarity judgements, but highlighting how their approach reveals that ANNs are more similar to brains than previously assessed.</p>
<p>My main concern is with regards to the claim that DimPred is revealing better similarities between ANNs and brains (a claim that the authors may not be making, but this should be clarified). The fact that predictions are poor for homogenous images is problematic for this claim, and I expect their DimPred scores would be very poor under many conditions, such as when applied to line drawings of objects, or a variety of addition out-of-sample stimuli that are easily identified by humans. The fact that so many different models get such similar prediction scores (Fig 3) also raises questions as to the inferences you can make about ANN-brain similarity based on the results. Do the authors want to claim that CLIP models are more like brains?</p>
<p>With regards to the brain prediction results, why is the DimPred approach doing so much better in V1? I would not think the 49 interpretable categories are encoded in V1, and the ability to predict would likely reflect a confound rather than V1 encoding these categories (e.g., if a category was &quot;things that are burning&quot; then DNN might predict V1 activation based on the encoding of colour).</p>
<p>In addition, more information is needed on the baseline model, as it is hard to appreciate whether we should be impressed by the better performance of DimPred based on what is provided: &quot;As a baseline, we fit a voxel encoding model of all 49 dimensions. Since dimension scores were available only for one image per category36, for the baseline model, we used the same value for each image of the same category and estimated predictive performance using cross-validation&quot;. Is it surprising that predictions are not good with one image per category? Is this a reasonable comparison?</p>
<p>Relatedly, what was the ability of the baseline model to predict? (I don't think that information was provided). Did the authors attempt to predict outside the visual brain areas? What would it mean if predictions were still better there?</p>
<p>Minor points:</p>
<p>The authors write: &quot;Please note that, for simplicity, we refer to the similarity matrix derived from this embedding as &quot;ground-truth&quot;, even though this is only a predicted similarity&quot;. Given this, it does not seem a good idea to use &quot;ground truth&quot; as this clarification will be lost in future work citing this article.</p>
<p>It would be good to have the 49 interpretable dimensions listed in the supplemental materials rather than having to go to the original paper.</p>
<p>Strengths:</p>
<p>The experiments seem well done.</p>
<p>Weaknesses:</p>
<p>It is not clear what claims are being made.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105394.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kaniuth</surname>
<given-names>Philipp</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1460-1267</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Mahner</surname>
<given-names>Florian P</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0001-0031-964X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Perkuhn</surname>
<given-names>Jonas</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hebart</surname>
<given-names>Martin N</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7257-428X</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We wish to express our gratitude to the reviewers for their insightful and constructive comments on the initial version of our manuscript. We greatly value their observations and have every intention of addressing their remarks in a thorough and constructive manner. Based on the editors’ and reviewers’ feedback, we realize that it was not entirely clear that we intended this work primarily to be a resource and not yield strong insights into DNN-human alignment. Since our method also covers the broad range of natural objects - as used in the vast majority of studies on object processing - we also feel we did not sufficiently highlight the breadth of the tool. Based on the editors’ assessment, our explorations into the limits of the method - which we saw as a strength, not a weakness of our work - perhaps overshadowed the otherwise broad applicability somewhat. We hope to clarify this in the revised manuscript. Beyond these general points, we would like to address the following four points:</p>
<p>• Where feasible, we intend to undertake additional analyses and refine existing ones. For instance, we plan to provide noise ceilings for all datasets where such calculations are possible, and we plan to give careful consideration to implementing a permutation or label-shuffling test to explore some of the ideas shared by the reviewers.</p>
<p>• We plan to discuss more thoroughly several topics raised by the reviewers (e.g., how our approach might contend with different experimental situations such when using line drawings as stimuli).</p>
<p>• We aim to enhance the clarity of our manuscript throughout. This will include refining the wording of our abstract and offering a more detailed explanation of the methods employed in the fMRI analyses.</p>
<p>• We plan to elaborate further on our line of reasoning by addressing potential sources of misunderstanding—such as clarifying what we mean by a “lack of data” and providing greater detail regarding the nature of the 49-dimensional embedding.</p>
</body>
</sub-article>
</article>