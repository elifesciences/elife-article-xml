<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">91415</article-id>
<article-id pub-id-type="doi">10.7554/eLife.91415</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.91415.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Sensitive remote homology search by local alignment of small positional embeddings from protein language models</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8261-9015</contrib-id>
<name>
<surname>Johnson</surname>
<given-names>Sean R.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Peshwa</surname>
<given-names>Meghana</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6106-5356</contrib-id>
<name>
<surname>Sun</surname>
<given-names>Zhiyi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>New England Biolabs Inc.</institution>, 240 County Road, Ipswich, MA 01938, <country>United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Moses</surname>
<given-names>Alan M</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Toronto</institution>
</institution-wrap>
<city>Toronto</city>
<country>Canada</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Landry</surname>
<given-names>Christian R</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Université Laval</institution>
</institution-wrap>
<city>Québec</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Correspondence: <email>sjohnson@neb.com</email>, or <email>sunz@neb.com</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-10-27">
<day>27</day>
<month>10</month>
<year>2023</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2024-02-26">
<day>26</day>
<month>02</month>
<year>2024</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP91415</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-08-08">
<day>08</day>
<month>08</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-07-29">
<day>29</day>
<month>07</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.07.26.550718"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2023-10-27">
<day>27</day>
<month>10</month>
<year>2023</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91415.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.91415.1.sa2">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.91415.1.sa1">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.91415.1.sa0">Reviewer #2 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Johnson et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Johnson et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-91415-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Accurately detecting distant evolutionary relationships between proteins remains an ongoing challenge in bioinformatics. Search methods based on primary sequence struggle to accurately detect homology between sequences with less than 20% amino acid identity. Profile- and structure-based strategies extend sensitive search capabilities into this twilight zone of sequence similarity but require slow pre-processing steps. Recently, whole-protein and positional embeddings from deep neural networks have shown promise for providing sensitive sequence comparison and annotation at long evolutionary distances. Embeddings are generally faster to compute than profiles and predicted structures but still suffer several drawbacks related to the ability of whole-protein embeddings to discriminate domain-level homology, and the database size and search speed of methods using positional embeddings. In this work, we show that low-dimensionality positional embeddings can be used directly in speed-optimized local search algorithms. As a proof of concept, we use the ESM2 3B model to convert primary sequences directly into the 3Di alphabet or amino acid profiles and use these embeddings as input to the highly optimized Foldseek, HMMER3, and HH-suite search algorithms. Our results suggest that positional embeddings as small as a single byte can provide sufficient information for dramatically improved sensitivity over amino acid sequence searches without sacrificing search speed.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Just changing the email of the corresponding author, which was entered incorrectly on thefirst submission.</p></fn>
</fn-group>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/8174960">https://zenodo.org/record/8174960</ext-link>
</p></fn>
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/seanrjohnson/esmologs">https://github.com/seanrjohnson/esmologs</ext-link>
</p></fn>
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/seanrjohnson/hmmer3di">https://github.com/seanrjohnson/hmmer3di</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>A common method for assigning a putative function to a protein sequence is to find sequences with experimentally determined functions that have similarities in sequence, structure, or evolutionary origin to the unannotated sequence (<xref ref-type="bibr" rid="c22">Loewenstein et al., 2009</xref>). Direct comparisons of primary sequence, for example using BLASTP (<xref ref-type="bibr" rid="c6">Camacho et al., 2009</xref>), are fast and reliable but show poor ability to detect homologs with less than about 20% identity to the query (<xref ref-type="bibr" rid="c29">Rost, 1999</xref>). Popular approaches for higher sensitivity sequence searches involve using sequence profiles, for example with PSI-BLAST (<xref ref-type="bibr" rid="c1">Altschul et al., 1997</xref>), HMMER3 (<xref ref-type="bibr" rid="c9">Eddy, 2011</xref>), HH-suite3 (<xref ref-type="bibr" rid="c32">Steinegger et al., 2019</xref>), or MMseqs2 (<xref ref-type="bibr" rid="c33">Steinegger and Söding, 2017</xref>). Sequence profiles are derived from multiple sequence alignments (MSAs) and are often modeled as profile hidden Markov models (HMMs), for example in HMMER and HH-suite. Profile HMMs model each position as the probability of each amino acid at the position together with insertion and deletion probabilities. Because of their reliance on the construction of MSAs, profile-based methods can have high computational overhead for database construction, query preparation, or both.</p>
<p>Protein structure searches also show higher sensitivity than sequence searches (<xref ref-type="bibr" rid="c16">Jambrich et al., 2023</xref>). Until recently, the utility of structure searches for protein annotation was limited by the lack of extensive reference databases and the inability to predict structures quickly and reliably for sequences lacking experimentally determined structures. In the past several years accurate protein structure prediction programs such as AlphaFold2 (<xref ref-type="bibr" rid="c18">Jumper et al., 2021</xref>) and ESMFold (<xref ref-type="bibr" rid="c21">Lin et al., 2023</xref>) have led to a massive increase in the size of databases of predicted protein structures. Coupled with fast structure search algorithms such as Foldseek (<xref ref-type="bibr" rid="c34">van Kempen et al., 2023</xref>), RUPEE (<xref ref-type="bibr" rid="c2">Ayoub and Lee, 2019</xref>), and Dali (<xref ref-type="bibr" rid="c15">Holm, 2022</xref>), structure prediction programs provide another powerful tool for remote homology detection. Foldseek achieves fast structure search by encoding the tertiary interactions of each amino acid in the 20-letter 3D interaction (3Di) alphabet. By using a structure alphabet of the same size as the amino acid alphabet, Foldseek can leverage optimized sequence search algorithms originally developed for amino acid sequences (<xref ref-type="bibr" rid="c33">Steinegger and Söding, 2017</xref>; <xref ref-type="bibr" rid="c34">van Kempen et al., 2023</xref>). Structure search methods suffer from some of the same drawbacks as profile-based methods, including the computational cost of converting primary sequences to predicted structures.</p>
<p>Emerging methods for protein annotation and remote homology detection rely on deep neural networks taking protein sequences as inputs and producing either a classification from a controlled vocabulary (<xref ref-type="bibr" rid="c4">Bileschi et al., 2022</xref>; <xref ref-type="bibr" rid="c30">Sanderson et al., 2023</xref>), a natural language description (<xref ref-type="bibr" rid="c11">Gane et al., 2022</xref>), positional embeddings, or a sequence embedding. Positional embeddings are fixed length vectors for each amino acid position of the protein. Positional embeddings produced by popular protein language models (pLMs) usually have large dimensions such as 1024 for ProtT5-XL-U50 (<xref ref-type="bibr" rid="c10">Elnaggar et al., 2021</xref>) and 2560 for ESM-2 3B (<xref ref-type="bibr" rid="c21">Lin et al., 2023</xref>). Sequence embeddings represent an entire sequence and are often calculated by element-wise averaging of the positional embeddings. Positional and sequence embeddings can be used for remote homology detection by using them to calculate substitution matrices in pairwise local alignments (<xref ref-type="bibr" rid="c19">Kaminski et al., 2023</xref>; <xref ref-type="bibr" rid="c25">Pantolini et al., 2022</xref>; <xref ref-type="bibr" rid="c37">Ye and Iovino, 2023</xref>), or by k-nearest neighbors searches (<xref ref-type="bibr" rid="c12">Hamamsy et al., 2022</xref>; <xref ref-type="bibr" rid="c31">Schütze et al., 2022</xref>), respectively.</p>
<p>While each of these emerging methods shows promise for improving sensitivity of protein search and annotation, they suffer various limitations. Classification models and methods relying on sequence embeddings struggle at discriminating individual domains of multi-domain proteins (<xref ref-type="bibr" rid="c4">Bileschi et al., 2022</xref>). Methods relying on large positional embeddings are space inefficient, and current search implementations are slow compared to other methods. Smaller positional embeddings would be more amenable to algorithmic optimizations using single instruction multiple data (SIMD) capabilities of central processing units (CPUs) that contribute to the speed of optimized sequence search algorithms (<xref ref-type="bibr" rid="c5">Buchfink et al., 2021</xref>; <xref ref-type="bibr" rid="c9">Eddy, 2011</xref>; <xref ref-type="bibr" rid="c32">Steinegger et al., 2019</xref>).</p>
<p>An embedding is an alternative vector representation of the input data which, preferably, makes the input data more suitable for some downstream task. We recognized that profile HMMs and 3Di sequences are types of protein positional embeddings with dimensionality as low as 1 (3Di sequences) to about 25 (profile HMMs, including amino acid frequencies and state transition probabilities), that are more suitable than amino acid sequences for the task of remote homology detection. We tested the hypothesis that the ESM-2 3B pLM (<xref ref-type="bibr" rid="c21">Lin et al., 2023</xref>) could be used to directly convert primary amino acid sequences into profile HMMs compatible with HMMER3 or HH-suite, and 3Di sequences compatible with Foldseek, providing a sequence search workflow leveraging the speed and sensitivity advantages of profile and structure search algorithms with an accelerated query preparation step enabled by the pLM.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Using ESM-2 3B to generate small positional embeddings</title>
<p>ESM-2 was already pretrained on the masked language modeling task (<xref ref-type="bibr" rid="c8">Devlin et al., 2019</xref>; <xref ref-type="bibr" rid="c21">Lin et al., 2023</xref>) of predicting amino acid distributions at masked positions of input sequences, therefore no additional fine-tuning was necessary to induce it to produce probabilities compatible with profile HMM tools (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). Positional amino acid frequencies predicted by ESM-2 3B resembled those found in MSAs built from sequence searches; <xref rid="fig2" ref-type="fig">Figure 2</xref> shows an example comparison of logos of HMM profiles (<xref ref-type="bibr" rid="c36">Wheeler et al., 2014</xref>) derived from the 4HBT Pfam profile and the YBGC_HELPY____14-90 sequence from the seed alignment. The example is cherry picked in terms of the profile length, to be short enough to look nice as a figure, but it is not cherry picked for agreement between the profiles; it was the first short one we looked at.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Schematics of embedding models and the experimental design.</title>
<p>(<bold>A</bold>) ESM-2 3B can be directly used to predict amino acid probability distributions at masked positions. Our implementation uses seven passes. The second pass is shown in the figure. (<bold>B</bold>) ESM-2 3B 3Di, a fine-tuned ESM-2 3B with a small CNN top model can be used to predict 3Di sequences from amino acid sequences. (<bold>C</bold>) Data flow from amino acid sequences through embedding models and other programs to produce files used in homology searches. Bold words correspond to line labels in <xref rid="fig3" ref-type="fig">Figure 3</xref>.</p></caption>
<graphic xlink:href="550718v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Logos related to the example test sequence YBGC_HELPY____14-90 from the 4HBT family.</title>
<p>(<bold>A</bold>) 4HBT family hmm from Pfam 32. (<bold>B</bold>) hmmbuild with default settings on an MSA of the top 100 hits supplied by an online blast search (<ext-link ext-link-type="uri" xlink:href="http://blast.ncbi.nlm.nih.gov">blast.ncbi.nlm.nih.gov</ext-link>) of YBGC_HELPY____14-90 against the NCBI clustered nr database. (<bold>C</bold>) hmmbuild with default settings on the MSA sampled from the ESM-2 3B positional probabilities for YBGC_HELPY____14-90. (<bold>D</bold>) and (<bold>E</bold>) hmmbuild with Dirichlet priors disabled on the same MSAs as for (<bold>B</bold>) and (<bold>C</bold>), respectively. All logos were generated by uploading the corresponding .hmm file to <ext-link ext-link-type="uri" xlink:href="http://skylign.org">skylign.org</ext-link> (<xref ref-type="bibr" rid="c36">Wheeler et al., 2014</xref>).</p></caption>
<graphic xlink:href="550718v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To produce Foldseek-compatible 3Di sequences, we trained a two-layer 1D convolutional neural network (CNN) to convert positional embeddings from the last transformer layer of ESM-2 3B into 3Di sequences, we then unfroze the last transformer layer and fine-tuned it together with the CNN. The fine-tuned model, which we call ESM-2 3B 3Di (<xref rid="fig1" ref-type="fig">Figure 1B</xref>), converted amino acids to 3Di with an accuracy of 64% compared to a test set of 3Di sequences derived from AlphaFold2 predicted structures. Training and test sets were derived from a random split of the Foldseek AlphaFold2 UniProt50 dataset (<xref ref-type="bibr" rid="c18">Jumper et al., 2021</xref>; <xref ref-type="bibr" rid="c34">van Kempen et al., 2023</xref>; <xref ref-type="bibr" rid="c35">Varadi et al., 2022</xref>), a reduced-redundancy subset of the UniProt AlphaFold2 structures (see Methods for details).</p>
</sec>
<sec id="s2b">
<title>Comparison of newly developed embedding methods</title>
<p>To evaluate the capacity of small embeddings to improve search sensitivity, we generated predicted profiles and 3Di sequences from clustered Pfam 32 splits (<xref ref-type="bibr" rid="c4">Bileschi et al., 2022</xref>) and converted them into formats compatible with various search tools (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). Pfam (<xref ref-type="bibr" rid="c24">Mistry et al., 2021</xref>) is a set of manually curated multiple sequence alignments of families of homologous protein domains. Some families presumed to have a common evolutionary origin are further grouped into clans. In the clustered splits, each family is divided into train and test groups such that each sequence in the test group has less than 25% identity to the most similar protein in the train group (<xref ref-type="bibr" rid="c4">Bileschi et al., 2022</xref>). The sensitivity of a search algorithm is evaluated by the ability to match sequences from the test groups to their corresponding train group at the family or clan level.</p>
<p>Methods using predicted profiles (<xref rid="fig3" ref-type="fig">Figure 3A,B</xref>) and those using predicted 3Di sequences (<xref rid="fig3" ref-type="fig">Figure 3C,D</xref>) both showed greater accuracy than phmmer (amino acid to amino acid) searches across all identity bins, and hmmscan (amino acid to profile HMM) searches on test sequences below 20% identity to the closest train sequence. Converting query sequences to hhsuite compatible profiles using ESM-2 3B and searching against databases built from ESM-2 predicted profiles of training sequences (<xref rid="fig3" ref-type="fig">Figure 3A,B</xref>, line 6) or profiles built from family-wise MSAs of the training sequences themselves (<xref rid="fig3" ref-type="fig">Figure 3A,B</xref>, line 5) gave improved accuracy at low identity bins compared to phmmer and hmmscan.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Homology detection accuracy.</title>
<p>Test sequences were binned based on percent identity to the closest training sequence in the same family and annotated based on the top scoring hit from a search against the entire set of training sequences or training sequence family profiles, depending on the algorithm. (<bold>A</bold>) and (<bold>C</bold>) family recovery accuracy by bin. (<bold>B</bold>) and (<bold>D</bold>) clan recovery accuracy. (<bold>A</bold>) and (<bold>B</bold>) compare amino acid profile-based methods. (<bold>C</bold>) and (<bold>D</bold>) compare Foldseek-based methods. Dashed lines are controls. There are a total of 21,293 test sequences. 12,246 test sequences have clan assignments.</p></caption>
<graphic xlink:href="550718v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Foldseek searches where both the queries and database consisted of 3Di sequences produced by ESM-2 3B 3Di (<xref rid="fig3" ref-type="fig">Figure 3C,D</xref>, line 7) performed the best overall of all new methods tested. Foldseek considers both 3Di and amino acid sequences in its alignments and can therefore be conceptualized as using a 2-byte embedding. Running Foldseek in 3Di-only mode (line 8), a 1-byte embedding, led to a decrease in accuracy but still outperformed phmmer across all identity bins, and hmmscan on bins below 20% identity. We also tried creating HMMER3 profiles from predicted 3Di sequences (line 9). These performed worse than single-sequence Foldseek searches. Patching HMMER3 to use 3Di-derived background frequencies and Dirichlet priors (line 10) did not improve performance. Furthermore, we noticed that HMMER3 runs very slowly on 3Di sequences and profiles, presumably because the prefiltering steps were not optimized with 3Di sequences in mind.</p>
<p>While faster than MSA construction or full structure prediction, pLM embedding still has non-trivial computational overhead. This limits the possibility of using methods based on pLM embeddings to perform sensitive homology searches against large metagenomic databases such as the 2.4 billion sequence Mgnify database (<xref ref-type="bibr" rid="c28">Richardson et al., 2023</xref>). It would be desirable to have search methods where the database can remain as amino acid sequences or other cheaply-calculated representations and the queries can be processed with more expensive methods. To this end, we tested hmmsearch using ESM-2 3B generated profiles as queries against amino acid databases (<xref rid="fig3" ref-type="fig">Figure 3A,B</xref>, lines 3 and 4). This is similar to a two-iteration PSI-BLAST (<xref ref-type="bibr" rid="c1">Altschul et al., 1997</xref>) or JackHMMER (<xref ref-type="bibr" rid="c17">Johnson et al., 2010</xref>) search where the first search and MSA-building step is replaced by a pLM embedding step. Curiously, hmmsearch using profiles built directly from the pLM probabilities (line 3) had the lowest accuracy of any algorithm. Nevertheless, profiles processed with hmmbuild from HMMER3, applying HMMER3 Dirichlet priors on top of the pLM probabilities (line 4), had better family prediction accuracy than phmmer at all but the highest identity bin, and accuracy on par with hmmscan at 18% identity and lower.</p>
</sec>
<sec id="s2c">
<title>Benchmarking of ESM-2 3B 3Di against emerging and established search methods</title>
<p>ESM-2 3B 3Di coupled to Foldseek search performed the best out of all the new methods we proposed on the clustered Pfam benchmark. To examine the performance of ESM-2 3B 3Di based search against other emerging methods, we used the SCOPe40 benchmark (<xref rid="fig4" ref-type="fig">Figure 4</xref>) (<xref ref-type="bibr" rid="c7">Chandonia et al., 2019</xref>; <xref ref-type="bibr" rid="c34">van Kempen et al., 2023</xref>), which has previously been used to evaluate Foldseek-based search methods (<xref ref-type="bibr" rid="c13">Heinzinger et al., 2023</xref>; <xref ref-type="bibr" rid="c34">van Kempen et al., 2023</xref>). The SCOPe40 benchmark is convenient because the dataset is much smaller than the clustered Pfam dataset, 11,211 vs 1,339,083 sequences. The small size of the dataset allowed us to compare compute-intensive methods, such as predicting AlphaFold2 structures for the entire dataset and then running Foldseek. In the SCOPe40 benchmark, the criteria for true positives (TPs) vary depending on the level of classification: for family, TPs are matches within the same family; for superfamily, TPs are matches the same superfamily but not the same family; and for fold, TPs are matches that share the same fold but are not in the same superfamily. Any hits from different folds (but not from different families or superfamilies) are considered false positives (FPs). Calculating the metric in this way has the effect that “Family” is a metric of homolog detection at the closest evolutionary distance (<xref rid="fig4" ref-type="fig">Figure 4A</xref>), “Superfamily” is a metric of detection of more distant homologs (<xref rid="fig4" ref-type="fig">Figure 4B</xref>), and “Fold” is a metric of very distant homolog detection or detection of evolutionarily unrelated proteins with similar folded structures (<xref rid="fig4" ref-type="fig">Figure 4C</xref>). All 11,211 proteins were used as queries for timing, but for sensitivity calculations, only the 3,566 proteins with at one other family member, at least one other superfamily member, and at least one other fold member were considered.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>SCOPe40 benchmark.</title>
<p>Cumulative distribution plots of the number of queries attaining each level of sensitivity to the first false positive fold at the (<bold>A</bold>) family, (<bold>B</bold>) superfamily, and (<bold>C</bold>) fold level. (<bold>D</bold>) Data preparation, search times, and average sensitivity at the superfamily level. (<bold>E</bold>) Comparison of average sensitivity at the superfamily level of Foldseek run with queries and databases prepared by different methods.</p></caption>
<graphic xlink:href="550718v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We benchmarked against Foldseek using experimentally determined structures, 3Di sequences predicted by ProstT5 (<xref ref-type="bibr" rid="c13">Heinzinger et al., 2023</xref>), and 3Di sequences derived from AlphaFold2 (<xref ref-type="bibr" rid="c18">Jumper et al., 2021</xref>; <xref ref-type="bibr" rid="c23">Mirdita et al., 2022</xref>) predicted structures. We also benchmarked a method based on local alignment of large positional embeddings, pLM-BLAST (<xref ref-type="bibr" rid="c19">Kaminski et al., 2023</xref>). Protein sequence search with pHMMER search was included as a control.</p>
<p>The Foldseek-based methods all performed similarly on the SCOPe40 benchmark at all three homology levels, with all outperforming pLM-BLAST and far outperforming pHMMER (<xref rid="fig4" ref-type="fig">Figure 4</xref>). At the superfamily level, AlphaFold2-predicted structures provided the best average sensitivity. 3Di sequences generated by different methods could be used as queries against databases built from 3Di sequences generated by other methods, but with some degradation of sensitivity compared to searches where both query and database 3Di sequences were generated by the same method (<xref rid="fig4" ref-type="fig">Figure 4C</xref>).</p>
<p>For each method we also timed the data preparation time and search time (<xref rid="fig4" ref-type="fig">Figure 4D</xref>). pHMMER uses protein sequences directly and does not require a database building step, so database and query preparation are not needed. For data preparation time, AlphaFold2 structure prediction was by far the slowest, taking nearly two weeks on our hardware (Methods), and ProstT5 was the fastest, converting all protein sequences to 3Di sequences in just 6 minutes. For search time, the Foldseek-based methods were the fastest. pLM-BLAST was much slower than Foldseek-based methods, which makes sense because it is doing the same types of Smith-Waterman alignments as Foldseek, but with positional encodings more than 500 times as large. Pre-filtering is used in both methods, but evidently the pre-filtering in pLM-BLAST is not enough to compensate for the slowness of the alignment step.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We tested several schemes for using the ESM-2 3B pLM to recode protein sequences into formats compatible with profile HMM or structure search tools with the hope of enhancing search sensitivity compared to primary sequence searches, but with less computational overhead than MSA construction or full structure prediction. The most successful of our efforts came from fine-tuning ESM-2 3B to convert protein sequence into 3Di sequences and running Foldseek structure search.</p>
<p>Compared to other emerging methods for neural-network-driven remote homology detection, ESM-2 3B 3Di coupled to Foldseek enabled faster and more sensitive search than pLM-BLAST (<xref ref-type="bibr" rid="c19">Kaminski et al., 2023</xref>), and was slower but had similar sensitivity to ProstT5 (<xref ref-type="bibr" rid="c13">Heinzinger et al., 2023</xref>) coupled to Foldseek. Furthermore, 3Di sequences predicted by ESM-2 3B 3Di or by ProstT5 seem to be compatible with Foldseek searches against experimental or AlphaFold2-generated structures (<xref rid="fig4" ref-type="fig">Figure 4E</xref>), enabling a sequence annotation workflow where large numbers of proteins, for example from a newly sequenced genome, can be quickly annotated with high sensitivity by pLM-based conversion to 3Di sequences followed by Foldseek search against existing databases of experimental or predicted structures, for example the protein data bank (<xref ref-type="bibr" rid="c3">Berman et al., 2000</xref>) or the AlphaFold Protein Structure Database (<xref ref-type="bibr" rid="c35">Varadi et al., 2022</xref>).</p>
<p>There are many possible directions for future development of improved embeddings, faster conversion and search programs, and comprehensive reference databases. It is significant that a 1-byte embedding, the predicted 3Di sequences run in 3Di-only Foldseek searches, was among the top performing models. This result suggests that positional embeddings as small as a single byte can provide sufficient information for dramatically improved sensitivity over amino acid sequence searches. The 3Di alphabet was not developed to maximize remote homology search sensitivity, but to model tertiary interactions in protein structures. In future work, small positional embeddings optimal for local alignment could be learned directly from a differentiable alignment algorithm (<xref ref-type="bibr" rid="c27">Petti et al., 2023</xref>) instead of relying on proxy tasks of amino acid or 3Di prediction.</p>
<p>Finally, asymmetric architectures where embedding a database sequence is cheaper than embedding a query, analogous to searches with profile HMM queries against primary sequence databases, could be a powerful method for improving search sensitivity against large and growing reference databases. Some first steps in that direction could be conversion to profile HMM-style embeddings in a single pass, rather than the seven we required, and predicting state transition probabilities, which may lead to improved search performance. We’ve made our model training, search, and data analysis code publicly available. We hope our results and code will serve as a springboard for further exploration of the utility of low-dimensionality positional embeddings of protein sequences.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Alignments</title>
<p>Unless otherwise noted, multiple sequence alignments were made using MAFFT (v7.505) (<xref ref-type="bibr" rid="c20">Katoh and Standley, 2013</xref>) with options --anysymbol --maxiterate 1000 --globalpair. Protein alignments used the BLOSUM62 substitution matrix (<xref ref-type="bibr" rid="c14">Henikoff and Henikoff, 1992</xref>). 3Di alignments used the 3Di substitution matrix from Foldseek (<xref ref-type="bibr" rid="c34">van Kempen et al., 2023</xref>) (<ext-link ext-link-type="uri" xlink:href="https://github.com/steineggerlab/foldseek/blob/master/data/mat3di.out">https://github.com/steineggerlab/foldseek/blob/master/data/mat3di.out</ext-link>).</p>
</sec>
<sec id="s4b">
<title>Patching HMMER3 with background frequencies and Dirichlet priors for 3Di</title>
<p>We created a fork of the HMMER3 program, replacing amino acid background frequencies and Dirichlet priors with values calculated from the 3Di alphabet instead of the amino acid alphabet (<ext-link ext-link-type="uri" xlink:href="https://github.com/seanrjohnson/hmmer3di">https://github.com/seanrjohnson/hmmer3di</ext-link>). To generate a set of 3Di MSAs, we converted the AlphaFold UniProt Foldseek database (<xref ref-type="bibr" rid="c18">Jumper et al., 2021</xref>; <xref ref-type="bibr" rid="c34">van Kempen et al., 2023</xref>; <xref ref-type="bibr" rid="c35">Varadi et al., 2022</xref>) to a 3Di fasta file. We then looked up every sequence name from the Pfam 35 seed file in the UniProt 3Di fasta file and, for cases where the corresponding sequence was identifiable, extracted the sub-sequence corresponding to the Pfam 35 seed. 3Di seeds from each profile were aligned using MAFFT. MSA columns with more than 10 rows were used to calculate background frequencies and Dirichlet priors using the HMMER3 program esl-mixdchlet fit with options -s 17 9 20. Amino acid background frequencies and Dirichlet priors in the HMMER3 source code were then replaced with the newly calculated 3Di background frequencies and Dirichlet priors. We call the patched HMMER3 as HMMER3Di.</p>
</sec>
<sec id="s4c">
<title>Fine tuning ESM-2 3B to convert amino acid sequences into 3Di sequences</title>
<p>A 1D CNN was added on top of ESM-2 3B. The CNN takes as input position-wise embeddings from the last transformer layer of ESM-2 3B. The CNN consists of two layers, the first layer has 2560 input channels (the size of the embeddings from ESM-2 3B), and 300 output channels, kernel size 5, stride 1, padding 3. The second layer has 300 input channels and 21 output channels (one for each 3Di symbol plus a padding symbol), kernel size 5, stride 1, padding 1. The model was trained with a weighted cross-entropy loss function using weights of 0.1 * the diagonal from 3Di substitution matrix. The neural network was implemented in PyTorch (<xref ref-type="bibr" rid="c26">Paszke et al., 2019</xref>).</p>
<p>Training data was derived from the Foldseek AlphaFold2 UniProt50 dataset (<xref ref-type="bibr" rid="c18">Jumper et al., 2021</xref>; <xref ref-type="bibr" rid="c34">van Kempen et al., 2023</xref>; <xref ref-type="bibr" rid="c35">Varadi et al., 2022</xref>), a reduced-redundancy subset of the UniProt AlphaFold2 structures. The Foldseek database was downloaded using the Foldseek “databases” command line program, converted into protein and 3Di fasta files, filtered to remove sequences smaller than 120 amino acids and larger than 1000 amino acids, and split into train, validation, and test subsets, 90%:5%:5%, (33,924,764: 1,884,709: 1,884,710 sequences)</p>
<p>With ESM-2 layers frozen, the CNN was trained on the task of converting amino acids to 3Di sequences using the AdamW optimizer with learning rate 0.001, weight decay 0.001, and exponential learning rate decrease (gamma 0.98, applied every 100 batches). Training sequences were randomly selected in batches of 15 sequences. Training proceeded for 1301 batches, leading to a training accuracy of about 58%.</p>
<p>The last transformer layer of ESM-2 was then unfrozen and training restarted from the saved weights, with the same training parameters. Training continued for another 24001 batches of 10 random training sequences. Accuracy on the final training batch was 65%. Using the final trained weights, test sequences were converted to 3Di at an accuracy of 64.4%. We call the fine-tuned model ESM-2 3B 3Di.</p>
<p>The trained weights are available on Zenodo (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.8174959">https://doi.org/10.5281/zenodo.8174959</ext-link>). The training code is available on Github (<ext-link ext-link-type="uri" xlink:href="https://github.com/seanrjohnson/esmologs">https://github.com/seanrjohnson/esmologs</ext-link>). The model training code should be useful for fine-tuning ESM-2 to convert amino acid sequences to various other kinds of sequences, such as secondary structure codes, etc.</p>
</sec>
<sec id="s4d">
<title>Pfam 32 clustered splits</title>
<p>Pfam 32 clustered splits (<xref ref-type="bibr" rid="c4">Bileschi et al., 2022</xref>) were downloaded from: <ext-link ext-link-type="uri" xlink:href="https://console.cloud.google.com/storage/browser/brain-genomics-public/research/proteins/pfam/clustered_split">https://console.cloud.google.com/storage/browser/brain-genomics-public/research/proteins/pfam/clustered_split</ext-link>. Data for mapping of individual Pfam 32 families to clans was downloaded from: <ext-link ext-link-type="uri" xlink:href="https://ftp.ebi.ac.uk/pub/databases/Pfam/releases/Pfam32.0/">https://ftp.ebi.ac.uk/pub/databases/Pfam/releases/Pfam32.0/</ext-link>. The sequences from the train, dev, and test splits were sorted into unaligned fasta files according to their split and family (<bold>aa seq</bold>).</p>
</sec>
<sec id="s4e">
<title>Predicting 3Di sequences</title>
<p>Each training and test sequence was converted into a predicted 3Di sequence (<bold>pred 3Di</bold>) using the ESM-2 3B 3Di model described above. MAFFT alignments were made of both the amino acid and predicted 3Di training and test sequences for each family. HMMER3 profiles were built from the alignments using either unaltered HMMER3 (<bold>pred 3Di hmmer profile</bold>), or HMMER3Di (<bold>pred 3Di hmmer3Di profile</bold>).</p>
</sec>
<sec id="s4f">
<title>Predicting profiles to generate HH-suite hhm files and HMMER3 hmm files from single sequences</title>
<p>Positional amino acid probabilities were calculated for unaligned train and test sequence using pre-trained ESM-2 3B in the following algorithm.</p>
<list list-type="order">
<list-item><p>Prepend sequence with M. This is because ESM-2 3B has a strong bias towards predicting M as the first amino acid in every sequence, and most Pfam domains don’t start with M.</p></list-item>
<list-item><p>Mask the first, eighth, etc. position in the sequence</p></list-item>
<list-item><p>Run a forward pass of ESM-2 3B over the masked sequence.</p></list-item>
<list-item><p>Save the logits for each of the 20 amino acid tokens for each masked position.</p></list-item>
<list-item><p>Shift the masks one position to the right.</p></list-item>
<list-item><p>Repeat steps 3 through 5 another 6 times until logits have been saved for every position.</p></list-item>
<list-item><p>Use the softmax function to calculate the amino acid probabilities at each position from the logits.</p></list-item>
</list>
<p>Note that in our actual implementation, a single forward pass was run on a batch of seven copies of the input sequence, each with different masking.</p>
<p>The hyperparameter of 7 passes over the input sequence was chosen semi-empirically. Ideally, masking would be done one position at a time, such that each position benefits from the context of the rest of the sequence. In practice, masking individual positions is prohibitively slow. We experimented with different masking distances in a non-systematic way and found that probabilities derived from 7 passes gave similar probabilities as masking each position individually. In addition, in the original ESM-2 pre-training, 15%, approximately 1/7, of positions were masked at each training step, so our masking of every 7<sup>th</sup> position resembles the training conditions.</p>
<p>The positional probabilities were written directly as an HH-suite compatible .hhm file (<bold>pred hhsuite profile</bold>). A 40 sequence fasta MSA file was written where each sequence was randomly sampled from the probability distribution. Hmmbuild was run with default settings on the sampled MSA (<bold>pred hmmer profile dchlet</bold>) and with the -pnone setting, which disables adjustments to the probability distribution based on Dirichlet priors (<bold>pred hmmer profile</bold>).</p>
</sec>
<sec id="s4g">
<title>Building HH-suite hhm and HMMER3 hmm profiles from train amino acid MSAs</title>
<p>The amino acid MSA for each training family were converted into an HH-suite database (<bold>aa hhsuite profile</bold>) with the following bash script:</p>
<preformat>
echo ‘#’$profile_name &gt; msa/${profile_name}.a3m
hhfilter -i $MSA_FASTA -a msa/${profile_name}.a3m -M 50 -id 90;
hhconsensus -i msa/${profile_name}.a3m -o consensus/${profile_name}.a3m
hhmake -name $base -i consensus/${profile_name}.a3m -o hhm/${base}.hhm
ffindex_build -s db_hhm.ffdata db_hhm.ffindex hhm
ffindex_build -s db_a3m.ffdata db_a3m.ffindex consensus
cstranslate -f -x 0.3 -c 4 -I a3m -i db_a3m -o db_cs219
</preformat>
<p>HMMER3 profiles were built by calling hmmbuild with default settings on the training family amino acid MSAs (<bold>aa hmmer profile</bold>).</p>
</sec>
<sec id="s4h">
<title>Building HH-suite databases from predicted profiles</title>
<p>HH-suite databases were built from predicted profiles .hhm files and the corresponding sampled 40 sequence MSA fasta files using the following bash script:</p>
<preformat>
ffindex_build -s db_hhm.ffdata db_hhm.ffindex esm2_3B_profiles
ffindex_build -s db_a3m.ffdata db_a3m.ffindex esm2_3B_sampled_msas
cstranslate -f -x 0.3 -c 4 -I a3m -i db_a3m -o db_cs219
</preformat>
</sec>
<sec id="s4i">
<title>Building Foldseek databases from predicted 3Di sequences</title>
<p>Amino acid and predicted 3Di fasta files were converted into Foldseek-compatible databases using a new script, fasta2foldseek.py, available from the esmologs python package (see below).</p>
</sec>
<sec id="s4j">
<title>Hmmscan, phmmer, and hmmsearch HMMER3 searches</title>
<p>In an attempt to mimic the Top pick HMM strategy reported by (<xref ref-type="bibr" rid="c4">Bileschi et al., 2022</xref>) we ran all HMMER3 searches in up to two iterations. The first iteration was run with default settings. For test sequences where no hits were detected among the training sequences or profiles, depending on the program, a second iteration was run with the addition of parameters intended to maximize sensitivity at the expense of search speed:</p>
<preformat>
--max -Z 1 --domZ 1 -E 1000000 --domE 1000000
</preformat>
<p>It should be noted that while our phmmer results are directly comparable to the phmmer results reported by Bileschi et al., our hmmscan results are not directly comparable to the reported “Top Pick HMM” results because we re-aligned the training sequences for each family instead of using the Pfam seed alignments. Still our results were very similar. We observed a 17.6% error rate (3744 test sequences with mispredicted family assignments) by hmmscan, compared to the reported 18.1% error rate (3844 mispredictions).</p>
</sec>
<sec id="s4k">
<title>3Di_hmmscan HMMER3Di searches</title>
<p>3Di_hmmscan searches were performed using the same two iteration method described above for searches using standard HMMER3 programs.</p>
</sec>
<sec id="s4l">
<title>Hhblits HH-suite searches</title>
<p>Hhbilts was run with the options:</p>
<preformat>
-tags -n 1 -v 0
</preformat>
</sec>
<sec id="s4m">
<title>Foldseek searches</title>
<p>After converting both query and target amino acid and predicted 3Di fasta files into Foldseek compatible databases (see above), Foldseek searches were run with the following commands:</p>
<preformat>
foldseek search test_db train_db foldseek_results tmpFolder
foldseek convertalis test_db train_db foldseek_results hits.tsv --format-
output query,target,bits
</preformat>
<p>For 3Di-only searches, the option <monospace>--alignment-type 0</monospace> was added to the search call.</p>
</sec>
<sec id="s4n">
<title>SCOPe40 benchmark</title>
<p>The SCOPe40 benchmark was conducted as previously described (<xref ref-type="bibr" rid="c13">Heinzinger et al., 2023</xref>; <xref ref-type="bibr" rid="c34">van Kempen et al., 2023</xref>). For timings, GPU operations were run on an Nvidia A100 GPU with 40 Gb of VRAM. CPU operations were run on 16 cores on an Intel Xeon Gold 6258R.</p>
<p>AlphaFold2 structures were computed using local ColabFold (<xref ref-type="bibr" rid="c18">Jumper et al., 2021</xref>; <xref ref-type="bibr" rid="c23">Mirdita et al., 2022</xref>), with the command:</p>
<preformat>
colabfold_batch –num-recycle 3 –num-models 5 peptides.fasta colabfold
</preformat>
<p>ProstT5 embedding was run with default settings of the script:</p>
<p><ext-link ext-link-type="uri" xlink:href="https://github.com/mheinzinger/ProstT5/blob/main/scripts/predict_3Di_encoderOnly.py">https://github.com/mheinzinger/ProstT5/blob/main/scripts/predict_3Di_encoderOnly.py</ext-link></p>
<p>Foldseek was always run with the command:</p>
<preformat>
Foldseek search queryDB targetDB aln3 tmpFolder -s 9.5 -e 10 --max-seqs 2000 -
-threads 16
</preformat>
<p>pLM-BLAST for SCOPe40</p>
<p>For pLM-BLAST, the version from Oct 30, 2023 was used (<ext-link ext-link-type="uri" xlink:href="https://github.com/labstructbioinf/pLM-BLAST/tree/76832b6078d64234591cedeab8c39b222450111a">https://github.com/labstructbioinf/pLM-BLAST/tree/76832b6078d64234591cedeab8c39b222450111a</ext-link>). We had to make a few trivial changes to the code to make it not crash when running the following operations.</p>
<p>The pLM-BLAST database was built with the entire SCOPe40 protein set (11,211 sequences), using the commands:</p>
<preformat>
python pLM-BLAST/embeddings.py start pLM-blastDB.fasta pLM-blastDB -embedder
pt --gpu -bs 0 --asdir -t 1500
python pLM-BLAST/scripts/dbtofile.py pLM-blastDB
</preformat>
<p>For the sake of speed, for embedding and searching queries, the SCOPe40 was divided into 10 equal partitions. Each partition was converted to embeddings with the command:</p>
<preformat>
python pLM-BLAST/embeddings.py start peptides_partition_[num].fasta
peptides_partition_[num].pt --gpu -bs 0 -t 1500
</preformat>
<p>Timings for database prep were the sum of timings for all the embedding calls. Each query partition was searched against the database with the command:</p>
<preformat>
python pLM-BLAST/scripts/plmblast.py pLM-blastDB peptides_partition_[num]
[num]_hits.csv --use_chunks -workers 16
</preformat>
<p>Timings for the search were the sum of the timings of all of the plmblast.py calls.</p>
</sec>
</sec>
<sec id="s5">
<title>Data and Code availability</title>
<p>HMMER3 patched with 3Di background frequencies and Dirichlet priors: <ext-link ext-link-type="uri" xlink:href="https://github.com/seanrjohnson/hmmer3di">https://github.com/seanrjohnson/hmmer3di</ext-link></p>
<p>Code for neural network training, sequence searches, and data analysis: <ext-link ext-link-type="uri" xlink:href="https://github.com/seanrjohnson/esmologs">https://github.com/seanrjohnson/esmologs</ext-link></p>
<p>Model weights for ESM-2 3B 3Di, predicted profiles and 3Di sequences from the Pfam 32 clustered splits, and other data necessary to reproduce the analyses: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.8174959">https://doi.org/10.5281/zenodo.8174959</ext-link></p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank Sergey Ovchinnikov for helpful discussion about protein language models, Sean Eddy for helpful discussion about HMMER3, and Gary Smith for IT support.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Altschul</surname> <given-names>SF</given-names></string-name>, <string-name><surname>Madden</surname> <given-names>TL</given-names></string-name>, <string-name><surname>Schäffer</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>W</given-names></string-name>, <string-name><surname>Lipman</surname> <given-names>DJ</given-names></string-name>. <year>1997</year>. <article-title>Gapped BLAST and PSI-BLAST: a new generation of protein database search programs</article-title>. <source>Nucleic Acids Research</source> <volume>25</volume>:<fpage>3389</fpage>–<lpage>3402</lpage>. doi:<pub-id pub-id-type="doi">10.1093/nar/25.17.3389</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Ayoub</surname> <given-names>R</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>Y.</given-names></string-name> <year>2019</year>. <article-title>RUPEE: A fast and accurate purely geometric protein structure search</article-title>. <source>PLOS ONE</source> <volume>14</volume>:<fpage>e0213712</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0213712</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Berman</surname> <given-names>HM</given-names></string-name>, <string-name><surname>Westbrook</surname> <given-names>J</given-names></string-name>, <string-name><surname>Feng</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Gilliland</surname> <given-names>G</given-names></string-name>, <string-name><surname>Bhat</surname> <given-names>TN</given-names></string-name>, <string-name><surname>Weissig</surname> <given-names>H</given-names></string-name>, <string-name><surname>Shindyalov</surname> <given-names>IN</given-names></string-name>, <string-name><surname>Bourne</surname> <given-names>PE</given-names></string-name>. <year>2000</year>. <article-title>The Protein Data Bank</article-title>. <source>Nucleic Acids Research</source> <volume>28</volume>:<fpage>235</fpage>–<lpage>242</lpage>. doi:<pub-id pub-id-type="doi">10.1093/nar/28.1.235</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="other"><string-name><surname>Bileschi</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Belanger</surname> <given-names>D</given-names></string-name>, <string-name><surname>Bryant</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Sanderson</surname> <given-names>T</given-names></string-name>, <string-name><surname>Carter</surname> <given-names>B</given-names></string-name>, <string-name><surname>Sculley</surname> <given-names>D</given-names></string-name>, <string-name><surname>Bateman</surname> <given-names>A</given-names></string-name>, <string-name><surname>DePristo</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Colwell</surname> <given-names>LJ</given-names></string-name>. <year>2022</year>. <article-title>Using deep learning to annotate the protein universe</article-title>. <source>Nat Biotechnol</source> <fpage>1</fpage>–<lpage>6</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41587-021-01179-w</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Buchfink</surname> <given-names>B</given-names></string-name>, <string-name><surname>Reuter</surname> <given-names>K</given-names></string-name>, <string-name><surname>Drost</surname> <given-names>H-G.</given-names></string-name> <year>2021</year>. <article-title>Sensitive protein alignments at tree-of-life scale using DIAMOND</article-title>. <source>Nat Methods</source> <volume>18</volume>:<fpage>366</fpage>–<lpage>368</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41592-021-01101-x</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Camacho</surname> <given-names>C</given-names></string-name>, <string-name><surname>Coulouris</surname> <given-names>G</given-names></string-name>, <string-name><surname>Avagyan</surname> <given-names>V</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>N</given-names></string-name>, <string-name><surname>Papadopoulos</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bealer</surname> <given-names>K</given-names></string-name>, <string-name><surname>Madden</surname> <given-names>TL</given-names></string-name>. <year>2009</year>. <article-title>BLAST+: architecture and applications</article-title>. <source>BMC Bioinformatics</source> <volume>10</volume>:<fpage>421</fpage>. doi:<pub-id pub-id-type="doi">10.1186/1471-2105-10-421</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Chandonia</surname> <given-names>J-M</given-names></string-name>, <string-name><surname>Fox</surname> <given-names>NK</given-names></string-name>, <string-name><surname>Brenner</surname> <given-names>SE</given-names></string-name>. <year>2019</year>. <article-title>SCOPe: classification of large macromolecular structures in the structural classification of proteins—extended database</article-title>. <source>Nucleic Acids Research</source> <volume>47</volume>:<fpage>D475</fpage>–<lpage>D481</lpage>. doi:<pub-id pub-id-type="doi">10.1093/nar/gky1134</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="other"><string-name><surname>Devlin</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>M-W</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>K</given-names></string-name>, <string-name><surname>Toutanova</surname> <given-names>K.</given-names></string-name> <year>2019</year>. <source>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</source>. arXiv:181004805 [cs].</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Eddy</surname> <given-names>SR</given-names></string-name>. <year>2011</year>. <article-title>Accelerated Profile HMM Searches</article-title>. <source>PLOS Computational Biology</source> <volume>7</volume>:<fpage>e1002195</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1002195</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="other"><string-name><surname>Elnaggar</surname> <given-names>A</given-names></string-name>, <string-name><surname>Heinzinger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Dallago</surname> <given-names>C</given-names></string-name>, <string-name><surname>Rehawi</surname> <given-names>G</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>W</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>L</given-names></string-name>, <string-name><surname>Gibbs</surname> <given-names>T</given-names></string-name>, <string-name><surname>Feher</surname> <given-names>T</given-names></string-name>, <string-name><surname>Angerer</surname> <given-names>C</given-names></string-name>, <string-name><surname>Steinegger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bhowmik</surname> <given-names>D</given-names></string-name>, <string-name><surname>Rost</surname> <given-names>B.</given-names></string-name> <year>2021</year>. <article-title>ProtTrans: Towards Cracking the Language of Lifes Code Through Self-Supervised Deep Learning and High Performance Computing</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source> <fpage>1</fpage>–<lpage>1</lpage>. doi:<pub-id pub-id-type="doi">10.1109/TPAMI.2021.3095381</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="web"><string-name><surname>Gane</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bileschi</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Dohan</surname> <given-names>D</given-names></string-name>, <string-name><surname>Speretta</surname> <given-names>E</given-names></string-name>, <string-name><surname>Héliou</surname> <given-names>A</given-names></string-name>, <string-name><surname>Meng-Papaxanthos</surname> <given-names>L</given-names></string-name>, <string-name><surname>Zellner</surname> <given-names>H</given-names></string-name>, <string-name><surname>Brevdo</surname> <given-names>E</given-names></string-name>, <string-name><surname>Parikh</surname> <given-names>A</given-names></string-name>, <string-name><surname>Orchard</surname> <given-names>S.</given-names></string-name> <year>2022</year>. <source>ProtNLM: Model-based Natural Language Protein Annotation</source>. <ext-link ext-link-type="uri" xlink:href="https://www.uniprot.org/help/ProtNLM">https://www.uniprot.org/help/ProtNLM</ext-link></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="other"><string-name><surname>Hamamsy</surname> <given-names>T</given-names></string-name>, <string-name><surname>Morton</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Berenberg</surname> <given-names>D</given-names></string-name>, <string-name><surname>Carriero</surname> <given-names>N</given-names></string-name>, <string-name><surname>Gligorijevic</surname> <given-names>V</given-names></string-name>, <string-name><surname>Blackwell</surname> <given-names>R</given-names></string-name>, <string-name><surname>Strauss</surname> <given-names>CEM</given-names></string-name>, <string-name><surname>Leman</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Cho</surname> <given-names>K</given-names></string-name>, <string-name><surname>Bonneau</surname> <given-names>R.</given-names></string-name> <year>2022</year>. <source>TM-Vec: template modeling vectors for fast homology detection and alignment</source>. doi:<pub-id pub-id-type="doi">10.1101/2022.07.25.501437</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="other"><string-name><surname>Heinzinger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Weissenow</surname> <given-names>K</given-names></string-name>, <string-name><surname>Sanchez</surname> <given-names>JG</given-names></string-name>, <string-name><surname>Henkel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Steinegger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Rost</surname> <given-names>B.</given-names></string-name> <year>2023</year>. <source>ProstT5: Bilingual Language Model for Protein Sequence and Structure</source>. doi:<pub-id pub-id-type="doi">10.1101/2023.07.23.550085</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Henikoff</surname> <given-names>S</given-names></string-name>, <string-name><surname>Henikoff</surname> <given-names>JG</given-names></string-name>. <year>1992</year>. <article-title>Amino acid substitution matrices from protein blocks</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>89</volume>:<fpage>10915</fpage>–<lpage>10919</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="other"><string-name><surname>Holm</surname> <given-names>L.</given-names></string-name> <year>2022</year>. <article-title>Dali server: structural unification of protein families</article-title>. <source>Nucleic Acids Research gkac387</source>. doi:<pub-id pub-id-type="doi">10.1093/nar/gkac387</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="other"><string-name><surname>Jambrich</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Tusnady</surname> <given-names>GE</given-names></string-name>, <string-name><surname>Dobson</surname> <given-names>L.</given-names></string-name> <year>2023</year>. <source>How AlphaFold shaped the structural coverage of the human transmembrane proteome</source>. doi:<pub-id pub-id-type="doi">10.1101/2023.04.18.537193</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Johnson</surname> <given-names>LS</given-names></string-name>, <string-name><surname>Eddy</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Portugaly</surname> <given-names>E.</given-names></string-name> <year>2010</year>. <article-title>Hidden Markov model speed heuristic and iterative HMM search procedure</article-title>. <source>BMC Bioinformatics</source> <volume>11</volume>:<fpage>431</fpage>. doi:<pub-id pub-id-type="doi">10.1186/1471-2105-11-431</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="other"><string-name><surname>Jumper</surname> <given-names>J</given-names></string-name>, <string-name><surname>Evans</surname> <given-names>R</given-names></string-name>, <string-name><surname>Pritzel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Green</surname> <given-names>T</given-names></string-name>, <string-name><surname>Figurnov</surname> <given-names>M</given-names></string-name>, <string-name><surname>Ronneberger</surname> <given-names>O</given-names></string-name>, <string-name><surname>Tunyasuvunakool</surname> <given-names>K</given-names></string-name>, <string-name><surname>Bates</surname> <given-names>R</given-names></string-name>, <string-name><surname>Žídek</surname> <given-names>A</given-names></string-name>, <string-name><surname>Potapenko</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bridgland</surname> <given-names>A</given-names></string-name>, <string-name><surname>Meyer</surname> <given-names>C</given-names></string-name>, <string-name><surname>Kohl</surname> <given-names>SAA</given-names></string-name>, <string-name><surname>Ballard</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Cowie</surname> <given-names>A</given-names></string-name>, <string-name><surname>Romera-Paredes</surname> <given-names>B</given-names></string-name>, <string-name><surname>Nikolov</surname> <given-names>S</given-names></string-name>, <string-name><surname>Jain</surname> <given-names>R</given-names></string-name>, <string-name><surname>Adler</surname> <given-names>J</given-names></string-name>, <string-name><surname>Back</surname> <given-names>T</given-names></string-name>, <string-name><surname>Petersen</surname> <given-names>S</given-names></string-name>, <string-name><surname>Reiman</surname> <given-names>D</given-names></string-name>, <string-name><surname>Clancy</surname> <given-names>E</given-names></string-name>, <string-name><surname>Zielinski</surname> <given-names>M</given-names></string-name>, <string-name><surname>Steinegger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Pacholska</surname> <given-names>M</given-names></string-name>, <string-name><surname>Berghammer</surname> <given-names>T</given-names></string-name>, <string-name><surname>Bodenstein</surname> <given-names>S</given-names></string-name>, <string-name><surname>Silver</surname> <given-names>D</given-names></string-name>, <string-name><surname>Vinyals</surname> <given-names>O</given-names></string-name>, <string-name><surname>Senior</surname> <given-names>AW</given-names></string-name>, <string-name><surname>Kavukcuoglu</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kohli</surname> <given-names>P</given-names></string-name>, <string-name><surname>Hassabis</surname> <given-names>D.</given-names></string-name> <year>2021</year>. <article-title>Highly accurate protein structure prediction with AlphaFold</article-title>. <source>Nature</source> <fpage>1</fpage>–<lpage>11</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Kaminski</surname> <given-names>K</given-names></string-name>, <string-name><surname>Ludwiczak</surname> <given-names>J</given-names></string-name>, <string-name><surname>Pawlicki</surname> <given-names>K</given-names></string-name>, <string-name><surname>Alva</surname> <given-names>V</given-names></string-name>, <string-name><surname>Dunin-Horkawicz</surname> <given-names>S.</given-names></string-name> <year>2023</year>. <article-title>pLM-BLAST: distant homology detection based on direct comparison of sequence representations from protein language models</article-title>. <source>Bioinformatics</source> <volume>39</volume>:<fpage>btad579</fpage>. doi:<pub-id pub-id-type="doi">10.1093/bioinformatics/btad579</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Katoh</surname> <given-names>K</given-names></string-name>, <string-name><surname>Standley</surname> <given-names>DM</given-names></string-name>. <year>2013</year>. <article-title>MAFFT Multiple Sequence Alignment Software Version 7: Improvements in Performance and Usability</article-title>. <source>Molecular Biology and Evolution</source> <volume>30</volume>:<fpage>772</fpage>–<lpage>780</lpage>. doi:<pub-id pub-id-type="doi">10.1093/molbev/mst010</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Lin</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Akin</surname> <given-names>H</given-names></string-name>, <string-name><surname>Rao</surname> <given-names>R</given-names></string-name>, <string-name><surname>Hie</surname> <given-names>B</given-names></string-name>, <string-name><surname>Zhu</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>W</given-names></string-name>, <string-name><surname>Smetanin</surname> <given-names>N</given-names></string-name>, <string-name><surname>Verkuil</surname> <given-names>R</given-names></string-name>, <string-name><surname>Kabeli</surname> <given-names>O</given-names></string-name>, <string-name><surname>Shmueli</surname> <given-names>Y</given-names></string-name>, <string-name><surname>dos Santos Costa</surname> <given-names>A</given-names></string-name>, <string-name><surname>Fazel-Zarandi</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sercu</surname> <given-names>T</given-names></string-name>, <string-name><surname>Candido</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rives</surname> <given-names>A.</given-names></string-name> <year>2023</year>. <article-title>Evolutionary-scale prediction of atomic-level protein structure with a language model</article-title>. <source>Science</source> <volume>379</volume>:<fpage>1123</fpage>–<lpage>1130</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.ade2574</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Loewenstein</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Raimondo</surname> <given-names>D</given-names></string-name>, <string-name><surname>Redfern</surname> <given-names>OC</given-names></string-name>, <string-name><surname>Watson</surname> <given-names>J</given-names></string-name>, <string-name><surname>Frishman</surname> <given-names>D</given-names></string-name>, <string-name><surname>Linial</surname> <given-names>M</given-names></string-name>, <string-name><surname>Orengo</surname> <given-names>C</given-names></string-name>, <string-name><surname>Thornton</surname> <given-names>J</given-names></string-name>, <string-name><surname>Tramontano</surname> <given-names>A.</given-names></string-name> <year>2009</year>. <article-title>Protein function annotation by homology-based inference</article-title>. <source>Genome Biology</source> <volume>10</volume>:<fpage>207</fpage>. doi:<pub-id pub-id-type="doi">10.1186/gb-2009-10-2-207</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Mirdita</surname> <given-names>M</given-names></string-name>, <string-name><surname>Schütze</surname> <given-names>K</given-names></string-name>, <string-name><surname>Moriwaki</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Heo</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ovchinnikov</surname> <given-names>S</given-names></string-name>, <string-name><surname>Steinegger</surname> <given-names>M.</given-names></string-name> <year>2022</year>. <article-title>ColabFold: making protein folding accessible to all</article-title>. <source>Nat Methods</source> <volume>19</volume>:<fpage>679</fpage>–<lpage>682</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41592-022-01488-1</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Mistry</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chuguransky</surname> <given-names>S</given-names></string-name>, <string-name><surname>Williams</surname> <given-names>L</given-names></string-name>, <string-name><surname>Qureshi</surname> <given-names>M</given-names></string-name>, <string-name><surname>Salazar</surname> <given-names>GA</given-names></string-name>, <string-name><surname>Sonnhammer</surname> <given-names>ELL</given-names></string-name>, <string-name><surname>Tosatto</surname> <given-names>SCE</given-names></string-name>, <string-name><surname>Paladin</surname> <given-names>L</given-names></string-name>, <string-name><surname>Raj</surname> <given-names>S</given-names></string-name>, <string-name><surname>Richardson</surname> <given-names>LJ</given-names></string-name>, <string-name><surname>Finn</surname> <given-names>RD</given-names></string-name>, <string-name><surname>Bateman</surname> <given-names>A.</given-names></string-name> <year>2021</year>. <article-title>Pfam: The protein families database in 2021</article-title>. <source>Nucleic Acids Research</source> <volume>49</volume>:<fpage>D412</fpage>–<lpage>D419</lpage>. doi:<pub-id pub-id-type="doi">10.1093/nar/gkaa913</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="other"><string-name><surname>Pantolini</surname> <given-names>L</given-names></string-name>, <string-name><surname>Studer</surname> <given-names>G</given-names></string-name>, <string-name><surname>Pereira</surname> <given-names>J</given-names></string-name>, <string-name><surname>Durairaj</surname> <given-names>J</given-names></string-name>, <string-name><surname>Schwede</surname> <given-names>T.</given-names></string-name> <year>2022</year>. <source>Embedding-based alignment: combining protein language models and alignment approaches to detect structural similarities in the twilight-zone</source>. doi:<pub-id pub-id-type="doi">10.1101/2022.12.13.520313</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="other"><string-name><surname>Paszke</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>S</given-names></string-name>, <string-name><surname>Massa</surname> <given-names>F</given-names></string-name>, <string-name><surname>Lerer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bradbury</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chanan</surname> <given-names>G</given-names></string-name>, <string-name><surname>Killeen</surname> <given-names>T</given-names></string-name>, <string-name><surname>Lin</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Gimelshein</surname> <given-names>N</given-names></string-name>, <string-name><surname>Antiga</surname> <given-names>L</given-names></string-name>, <string-name><surname>Desmaison</surname> <given-names>A</given-names></string-name>, <string-name><surname>Köpf</surname> <given-names>A</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>E</given-names></string-name>, <string-name><surname>DeVito</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Raison</surname> <given-names>M</given-names></string-name>, <string-name><surname>Tejani</surname> <given-names>A</given-names></string-name>, <string-name><surname>Chilamkurthy</surname> <given-names>S</given-names></string-name>, <string-name><surname>Steiner</surname> <given-names>B</given-names></string-name>, <string-name><surname>Fang</surname> <given-names>L</given-names></string-name>, <string-name><surname>Bai</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chintala</surname> <given-names>S.</given-names></string-name> <year>2019</year>. <source>PyTorch: An Imperative Style, High-Performance Deep Learning Library</source>. doi:<pub-id pub-id-type="doi">10.48550/arXiv.1912.01703</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Petti</surname> <given-names>S</given-names></string-name>, <string-name><surname>Bhattacharya</surname> <given-names>N</given-names></string-name>, <string-name><surname>Rao</surname> <given-names>R</given-names></string-name>, <string-name><surname>Dauparas</surname> <given-names>J</given-names></string-name>, <string-name><surname>Thomas</surname> <given-names>N</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>J</given-names></string-name>, <string-name><surname>Rush</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Koo</surname> <given-names>P</given-names></string-name>, <string-name><surname>Ovchinnikov</surname> <given-names>S.</given-names></string-name> <year>2023</year>. <article-title>End-to-end learning of multiple sequence alignments with differentiable Smith–Waterman</article-title>. <source>Bioinformatics</source> <volume>39</volume>:<fpage>btac724</fpage>. doi:<pub-id pub-id-type="doi">10.1093/bioinformatics/btac724</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Richardson</surname> <given-names>L</given-names></string-name>, <string-name><surname>Allen</surname> <given-names>B</given-names></string-name>, <string-name><surname>Baldi</surname> <given-names>G</given-names></string-name>, <string-name><surname>Beracochea</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bileschi</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Burdett</surname> <given-names>T</given-names></string-name>, <string-name><surname>Burgin</surname> <given-names>J</given-names></string-name>, <string-name><surname>Caballero-Pérez</surname> <given-names>J</given-names></string-name>, <string-name><surname>Cochrane</surname> <given-names>G</given-names></string-name>, <string-name><surname>Colwell</surname> <given-names>LJ</given-names></string-name>, <string-name><surname>Curtis</surname> <given-names>T</given-names></string-name>, <string-name><surname>Escobar-Zepeda</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gurbich</surname> <given-names>TA</given-names></string-name>, <string-name><surname>Kale</surname> <given-names>V</given-names></string-name>, <string-name><surname>Korobeynikov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Raj</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rogers</surname> <given-names>AB</given-names></string-name>, <string-name><surname>Sakharova</surname> <given-names>E</given-names></string-name>, <string-name><surname>Sanchez</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wilkinson</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Finn</surname> <given-names>RD</given-names></string-name>. <year>2023</year>. <article-title>MGnify: the microbiome sequence data analysis resource in 2023</article-title>. <source>Nucleic Acids Research</source> <volume>51</volume>:<fpage>D753</fpage>–<lpage>D759</lpage>. doi:<pub-id pub-id-type="doi">10.1093/nar/gkac1080</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Rost</surname> <given-names>B.</given-names></string-name> <year>1999</year>. <article-title>Twilight zone of protein sequence alignments</article-title>. <source>Protein Engineering, Design and Selection</source> <volume>12</volume>:<fpage>85</fpage>–<lpage>94</lpage>. doi:<pub-id pub-id-type="doi">10.1093/protein/12.2.85</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Sanderson</surname> <given-names>T</given-names></string-name>, <string-name><surname>Bileschi</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Belanger</surname> <given-names>D</given-names></string-name>, <string-name><surname>Colwell</surname> <given-names>LJ</given-names></string-name>. <year>2023</year>. <article-title>ProteInfer, deep neural networks for protein functional inference</article-title>. <source>eLife</source> <volume>12</volume>:<fpage>e80942</fpage>. doi:<pub-id pub-id-type="doi">10.7554/eLife.80942</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Schütze</surname> <given-names>K</given-names></string-name>, <string-name><surname>Heinzinger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Steinegger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Rost</surname> <given-names>B.</given-names></string-name> <year>2022</year>. <article-title>Nearest neighbor search on embeddings rapidly identifies distant protein relations</article-title>. <source>Frontiers in Bioinformatics</source> <volume>2</volume>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Steinegger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Meier</surname> <given-names>M</given-names></string-name>, <string-name><surname>Mirdita</surname> <given-names>M</given-names></string-name>, <string-name><surname>Vöhringer</surname> <given-names>H</given-names></string-name>, <string-name><surname>Haunsberger</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Söding</surname> <given-names>J.</given-names></string-name> <year>2019</year>. <article-title>HH-suite3 for fast remote homology detection and deep protein annotation</article-title>. <source>BMC Bioinformatics</source> <volume>20</volume>:<fpage>473</fpage>. doi:<pub-id pub-id-type="doi">10.1186/s12859-019-3019-7</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Steinegger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Söding</surname> <given-names>J.</given-names></string-name> <year>2017</year>. <article-title>MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</article-title>. <source>Nat Biotechnol</source> <volume>35</volume>:<fpage>1026</fpage>–<lpage>1028</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nbt.3988</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="other"><string-name><surname>van Kempen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>SS</given-names></string-name>, <string-name><surname>Tumescheit</surname> <given-names>C</given-names></string-name>, <string-name><surname>Mirdita</surname> <given-names>M</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>J</given-names></string-name>, <string-name><surname>Gilchrist</surname> <given-names>CLM</given-names></string-name>, <string-name><surname>Söding</surname> <given-names>J</given-names></string-name>, <string-name><surname>Steinegger</surname> <given-names>M.</given-names></string-name> <year>2023</year>. <article-title>Fast and accurate protein structure search with Foldseek</article-title>. <source>Nat Biotechnol</source> <fpage>1</fpage>–<lpage>4</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41587-023-01773-0</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Varadi</surname> <given-names>M</given-names></string-name>, <string-name><surname>Anyango</surname> <given-names>S</given-names></string-name>, <string-name><surname>Deshpande</surname> <given-names>M</given-names></string-name>, <string-name><surname>Nair</surname> <given-names>S</given-names></string-name>, <string-name><surname>Natassia</surname> <given-names>C</given-names></string-name>, <string-name><surname>Yordanova</surname> <given-names>G</given-names></string-name>, <string-name><surname>Yuan</surname> <given-names>D</given-names></string-name>, <string-name><surname>Stroe</surname> <given-names>O</given-names></string-name>, <string-name><surname>Wood</surname> <given-names>G</given-names></string-name>, <string-name><surname>Laydon</surname> <given-names>A</given-names></string-name>, <string-name><surname>Žídek</surname> <given-names>A</given-names></string-name>, <string-name><surname>Green</surname> <given-names>T</given-names></string-name>, <string-name><surname>Tunyasuvunakool</surname> <given-names>K</given-names></string-name>, <string-name><surname>Petersen</surname> <given-names>S</given-names></string-name>, <string-name><surname>Jumper</surname> <given-names>J</given-names></string-name>, <string-name><surname>Clancy</surname> <given-names>E</given-names></string-name>, <string-name><surname>Green</surname> <given-names>R</given-names></string-name>, <string-name><surname>Vora</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lutfi</surname> <given-names>M</given-names></string-name>, <string-name><surname>Figurnov</surname> <given-names>M</given-names></string-name>, <string-name><surname>Cowie</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hobbs</surname> <given-names>N</given-names></string-name>, <string-name><surname>Kohli</surname> <given-names>P</given-names></string-name>, <string-name><surname>Kleywegt</surname> <given-names>G</given-names></string-name>, <string-name><surname>Birney</surname> <given-names>E</given-names></string-name>, <string-name><surname>Hassabis</surname> <given-names>D</given-names></string-name>, <string-name><surname>Velankar</surname> <given-names>S.</given-names></string-name> <year>2022</year>. <article-title>AlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space with high-accuracy models</article-title>. <source>Nucleic Acids Research</source> <volume>50</volume>:<fpage>D439</fpage>–<lpage>D444</lpage>. doi:<pub-id pub-id-type="doi">10.1093/nar/gkab1061</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Wheeler</surname> <given-names>TJ</given-names></string-name>, <string-name><surname>Clements</surname> <given-names>J</given-names></string-name>, <string-name><surname>Finn</surname> <given-names>RD</given-names></string-name>. <year>2014</year>. <article-title>Skylign: a tool for creating informative, interactive logos representing sequence alignments and profile hidden Markov models</article-title>. <source>BMC Bioinformatics</source> <volume>15</volume>:<fpage>7</fpage>. doi:<pub-id pub-id-type="doi">10.1186/1471-2105-15-7</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="other"><string-name><surname>Ye</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Iovino</surname> <given-names>BG</given-names></string-name>. <year>2023</year>. <source>Protein Embedding based Alignment (preprint). Preprints</source>. doi:<pub-id pub-id-type="doi">10.22541/au.168534397.72964200/v1</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.91415.2.sa1</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Moses</surname>
<given-names>Alan M</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Toronto</institution>
</institution-wrap>
<city>Toronto</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study addresses the problem of detecting weak similarity between protein sequences, a procedure commonly used to infer homology or assign putative functions to uncharacterized proteins. The authors present a <bold>convincing</bold> approach that combines recently developed protein language models with well-established methods. The benchmarks provided show that the proposed tool is fast and accurate for remote homology detection, making this paper of general interest to all researchers working in the fields of protein evolution and genome annotation.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.91415.2.sa0</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This paper describes a new method for sequence-based remote homology detection. Such methods are essential for the annotation of uncharacterized proteins and for studies of protein evolution.</p>
<p>The main strength and novelty of the proposed approach lies in the idea of combining state-of-the-art sequence-based (HHpred and HMMER) and structure-based (Foldseek) homology detection methods with protein language models (the ESM2 model was used). The authors show that high-dimensional, information-rich representations extracted from the ESM2 model can be efficiently combined with the aforementioned tools.</p>
<p>The benchmarking of the new approach is convincing and shows that it is suitable for homology detection at very low sequence similarity. The method is also fast because it does not require the computation of multiple sequence alignments for profile calculation or structure prediction.</p>
<p>Overall, this is an interesting and useful paper that proposes an alternative direction for the problem of distant homology detection.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.91415.2.sa2</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Johnson</surname>
<given-names>Sean R.</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8261-9015</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Peshwa</surname>
<given-names>Meghana</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sun</surname>
<given-names>Zhiyi</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6106-5356</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>This work describes a new method for sequence-based remote homology detection. Such methods are essential for the annotation of uncharacterized proteins and for studies of protein evolution.</p>
<p>Strengths:</p>
<p>The main strength and novelty of the proposed approach lies in the idea of combining stateof-the-art sequence-based (HHpred and HMMER) and structure-based (Foldseek) homology detection methods with recent developments in the field of protein language models (the ESM2 model was used). The authors show that features extracted from high-dimensional, information-rich ESM2 sequence embeddings can be suitable for efficient use with the aforementioned tools.</p>
<p>The reduced features take the form of amino acid occurrence probability matrices estimated from ESM2 masked-token predictions, or structural descriptors predicted by a modified variant of the ESM2 model. However, we believe that these should not be called &quot;embeddings&quot; or &quot;representations&quot;. This is because they don't come directly from any layer of these networks, but rather from their final predictions.</p>
</disp-quote>
<p>We agree that there is some room for discussion about whether the amino acid probabilities returned by pre-trained ESM-2 and the 3Di sequences returned by ESM-2 3B 3Di can be properly referred to as “embeddings”. The term “embedding” doesn’t have a formal definition, other than some kind of alternative vector representation of the input data which, preferably, makes the input data more suitable for some downstream task. In that simple sense of the word “embedding”, amino acid probabilities and 3Di sequences output by our models are, indeed, types of embeddings. We posed the question on Twitter
(<ext-link ext-link-type="uri" xlink:href="https://twitter.com/TrichomeDoctor/status/1715051012162220340">https://twitter.com/TrichomeDoctor/status/1715051012162220340</ext-link>) and nobody responded, so we are left to conclude that the community is largely ambivalent about the precise definition of “embedding”.</p>
<p>We’ve added language in our introduction to make it more clear that this is our working definition of an “embedding”, and why that definition can apply to profile HMMs and 3Di sequences.</p>
<disp-quote content-type="editor-comment">
<p>The benchmarks presented suggest that the approach improves sensitivity even at very low sequence identities &lt;20%. The method is also expected to be faster because it does not require the computation of multiple sequence alignments (MSAs) for profile calculation or structure prediction.</p>
<p>Weaknesses:</p>
<p>The benchmarking of the method is very limited and lacks comparison with other methods. Without additional benchmarks, it is impossible to say whether the proposed approach really allows remote homology detection and how much improvement the discussed method brings over tools that are currently considered state-of-the-art.</p>
</disp-quote>
<p>We thank the reviewer for the comment. To address the question, we’ve expanded the results by adding a new benchmark and added a new figure, Figure 4. In this new content, we use the SCOPe40 benchmark, originally proposed in the Foldseek paper (van Kempen et al., 2023), to compare our best method, ESM-2 3B 3Di coupled to Foldseek, with several other recent methods.  We find our method to be competitive with the other methods.</p>
<p>We are hesitant to claim that any of our proposed methods are state-of-the-art because of the lack of a widely accepted standard benchmark for remote homology detection, and because of the rapid pace of advancement of the field in recent years, with many groups finding innovative uses of pLMs and other neural-network models for protein annotation and homology detection.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary:</p>
<p>The authors present a number of exploratory applications of current protein representations for remote homology search. They first fine-tune a language model to predict structural alphabets from sequence and demonstrate using these predicted structural alphabets for fast remote homology search both on their own and by building HMM profiles from them. They also demonstrate the use of residue-level language model amino acid predicted probabilities to build HMM profiles. These three implementations are compared to traditional profile-based remote homology search.</p>
<p>Strengths:</p>
<list list-type="bullet">
<list-item><p>Predicting structural alphabets from a sequence is novel and valuable, with another approach (ProstT5) also released in the same time frame further demonstrating its application for the remote homology search task.</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>Using these new representations in established and battle-tested workflows such as MMSeqs, HMMER, and HHBlits is a great way to allow researchers to have access to the state-of-the-art methods for their task.</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>Given the exponential growth of data in a number of protein resources, approaches that
allow for the preparation of searchable datasets and enable fast search is of high relevance.</p>
</list-item></list>
<p>Weaknesses:</p>
<list list-type="bullet">
<list-item><p>The authors fine-tuned ESM-2 3B to predict 3Di sequences and presented the fine-tuned model ESM-2 3B 3Di with a claimed accuracy of 64% compared to a test set of 3Di sequences derived from AlphaFold2 predicted structures. However, the description of this test set is missing, and I would expect repeating some of the benchmarking efforts described in the Foldseek manuscript as this accuracy value is hard to interpret on its own.</p>
</list-item></list>
</disp-quote>
<p>The preparation of training and test sets are described in the methods under the heading “Fine tuning ESM-2 3B to convert amino acid sequences into 3Di sequences”. Furthermore, there is code in our github repository to reproduce the splits, and the entire model training process:
<ext-link ext-link-type="uri" xlink:href="https://github.com/seanrjohnson/esmologs#train-esm-2-3b-3di-starting-from-the-esm-2-3bpre-trained-weights">https://github.com/seanrjohnson/esmologs#train-esm-2-3b-3di-starting-from-the-esm-2-3bpre-trained-weights</ext-link></p>
<p>We didn’t include the training/validation/test splits in the Zenodo repository because they are very large: train 33,924,764; validation 1,884,709; test 1,884,710 sequences, times 2 because there are both amino acid and 3Di sequences. It comes out to about 30 Gb total, and is easily rebuilt from the same sources we built it from.</p>
<p>We’ve added the following sentence to the main text to clarify:</p>
<p>“Training and test sets were derived from a random split of the Foldseek AlphaFold2 UniProt50 dataset (Jumper et al., 2021; van Kempen et al., 2023; Varadi et al., 2022), a reducedredundancy subset of the UniProt AlphaFold2 structures (see Methods for details).”</p>
<p>To address the concern about comparing to Foldseek using the same benchmark, we’ve expanded the results section and added a new figure, Figure 4 using the SCOPe40 benchmark originally presented in the Foldseek paper, and subsequently in the ProstT5 paper to compare Foldseek with ESM-2 3B 3Di to Foldseek with ProstT5, AlphaFold2, and experimental structures.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Given the availability of predicted structure data in AFDB, I would expect to see a comparison between the searches of predicted 3Di sequences and the &quot;true&quot; 3Di sequences derived from these predicted structures. This comparison would substantiate the innovation claimed in the manuscript, demonstrating the potential of conducting new searches solely based on sequence data on a structural database.</p>
</list-item></list>
</disp-quote>
<p>See response above. We’ve now benchmarked against both ProstT5 and AF2.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The profile HMMs built from predicted 3Di appear to perform sub-optimally, and those from the ESM-2 3B predicted probabilities also don't seem to improve traditional HMM results significantly. The HHBlits results depicted in lines 5 and 6 in the figure are not discussed at all, and a comparison with traditional HHBlits is missing. With these results and presentation, the advantages of pLM profile-based searches are not clear, and more justification over traditional methods is needed.</p>
</list-item></list>
</disp-quote>
<p>We thank the reviewer for pointing out the lack of clarity in the discussion of lines 5 and 6.</p>
<p>We’ve re-written that section of the discussion, and reformatted Figure 3 to enhance clarity.</p>
<p>We agree, a comparison to traditional HHBlits could be interesting, but we don’t expect to see stronger performance from the pLM-predicted profiles than from traditional HHBlits, just as we don’t see stronger performance from pLM-hmmscan or pLM-Foldseek than from the traditional variants. We think that the advantages of pLM based amino acid hmm searches are primarily speed. There are many variables that can influence speed of generating an MSA and HMM profile, but in general we expect that it will be much slower than generating an HMM profile from a pLM.</p>
<p>We don’t know why making profiles of 3Di sequences doesn’t improve search sensitivity, we just think it’s an interesting result that is worth presenting to the community. Perhaps someone can figure out how to make it work better.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Figure 3 and its associated text are hard to follow due to the abundance of colors and abbreviations used. One figure attempting to explain multiple distinct points adds to the confusion. Suggestion: Splitting the figure into two panels comparing (A) Foldseek-derived searches (lines 7-10) and (B) language-model derived searches (line 3-6) to traditional methods could enhance clarity. Different scatter markers could also help follow the plots more easily.</p>
</list-item></list>
</disp-quote>
<p>We thank the reviewer for this helpful comment. We’ve reformatted Figure 3 as suggested, and we think it is much easier to read now.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The justification for using Foldseek without amino acids (3Di-only mode) is not clear. Its utility should be described, or it should be omitted for clarity.</p>
</list-item></list>
</disp-quote>
<p>To us, the use of 3Di-only mode is of great theoretical interest. From our perspective, this is one of our most significant results. Previous methods, such as pLM-BLAST and related methods, have made use of very large positional embeddings to achieve sensitive remote homology search.  We show that with the right embedding, you don’t need very many bits per position to get dramatically improved search sensitivity from Smith-Waterman, compared to amino acid searches. We also doubt that predicted 3Di sequences are the optimal small encoding for remote homology detection. This result and observation opens up an exciting avenue for future research in developing small, learned positional embeddings that are optimal for remote homology detection and amenable to SIMD-optimized pre-filtering and Smith-Waterman alignment steps.</p>
<p>We’ve expanded the discussion, explaining why we are excited about this result.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Figure 2 is not described, unclear what to read from it.</p>
</list-item></list>
</disp-quote>
<p>It's just showing that ESM-2-derived amino acid probabilities closely resemble amino acid frequencies in MSAs. We think it gives readers some visual intuition about why predicted profile HMMs perform as well as they do. We’ve added some additional explanation of it in the text.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>The paper would mainly benefit from a more comprehensive benchmark:</p>
<p>We suggest that the authors extend the benchmark by including the reference methods (HHpred and Foldseek) run with their original representations, i.e., MSAs obtained with 2-3 iterations of hhblits (for HHpred) and experimental or predicted structures (for Foldseek). HHpred profile-profile comparisons and Foldseek structure-structure comparisons would be important reference points for assessing the applicability of the proposed approach in distant homology detection. It is also essential to compare the method with other emerging tools such as EBA (DOI: 10.1101/2022.12.13.520313), pLM-BLAST (DOI: 10.1101/2022.11.24.517862), DEDAL (DOI: 10.1038/s41592-022-01700-2), etc.</p>
<p>We also suggest using an evolutionary-oriented database for the benchmark, such as ECOD or CATH (these databases classify protein domains with known structures, which is important in the context of including Foldseek in the benchmark). We ran a cursory benchmark using the ECOD database and generated HH-suite .hhm files (using the single_seq_to_hmm.py and hhsearch_multiple.py scripts). Precision and recall appear to be significantly lower compared to &quot;vanilla&quot; hhsearch runs with MSA-derived profiles. It would also be interesting to see benchmarks for speed and alignment quality.</p>
<p>The pLM-based methods for homology detection are an emerging field, and it would be important to evaluate them in the context of distinguishing between homology and analogy. In particular, the predicted Foldseek representations may be more likely to capture structural similarity than homology. This could be investigated, for example, using the ECOD classification (do structurally similar proteins from different homology groups produce significant matches?) and/or resources such as MALISAM that catalog examples of analogy.</p>
</disp-quote>
<p>We’ve added the SCOPe40 benchmark, which we think at least partially addresses these comments, adding a comparison to pLM-BLAST, ProstT5, and AF2 followed by Foldseek. The question of Analogy vs homology is an interesting one. It could be argued that the SCOPe40 benchmark addresses this in the difference between Superfamily (distant homology) and Fold (analogy, or very distant homology).</p>
<p>Our focus is on remote homology detection applications rather than alignment quality, so we don’t benchmark alignment quality, although we agree that those benchmarks would be interesting.</p>
<disp-quote content-type="editor-comment">
<p>Page 2, lines 60-67. This paragraph would benefit from additional citations and explanations to support the superiority of the proposed approach. The fact that flattened embeddings are not suitable for annotating multidomain proteins seems obvious. Also, the claim that &quot;current search implementations are slow compared to other methods&quot; should be supported (tools such as EBA or pLM-BLAST have been shown to be faster than standard MSA-based methods). Also, as we mentioned in the main review, we believe that the generated pseudo-profiles and fine-tuned ESM2 predictions should not be called &quot;smaller positional embeddings&quot;.</p>
</disp-quote>
<p>Discriminating subdomains was a major limitation of the influential and widely-cited PfamN paper (Bileschi et al., 2022), we’ve added a citation to that paper in that paragraph for readers interested in diving deeper.</p>
<p>To address the question of speed, we’ve included data preparation and search benchmarks as part of our presentation of the SCOPe40 benchmark.</p>
<disp-quote content-type="editor-comment">
<p>Finally, we were not sure why exactly every 7th residue is masked in a single forward pass. Traditionally, pseudo-log likelihoods are generated by masking every single token and predicting probabilities from logits given the full context - e.g. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/1910.14659.pdf">https://arxiv.org/pdf/1910.14659.pdf</ext-link>. Since this procedure is crucial in the next steps of the pipeline, it would be important to either experiment with this hyperparameter or explain the logic used to choose the mask spacing.</p>
</disp-quote>
<p>We’ve added discussion of the masking distance to the Methods section.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<list list-type="bullet">
<list-item><p>While the code and data for the benchmark are available, the generation of searchable databases using the methods described for a popular resource such as Pfam, AFDB, SCOP/CATH which can be used by the community would greatly boost the impact of this work.</p>
</list-item></list>
</disp-quote>
<p>3Di sequences predicted by ESM-2 3B 3Di can easily be used as queries against any Foldseek database, such as PDB, AFDB, etc. We’ve added Figure 4E to demonstrate this possibility, and added some related discussion.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Minor: In line 114, the text should likely read &quot;compare lines 7 and 8&quot; instead of &quot;compare lines 6 and 7.&quot;</p>
</list-item></list>
</disp-quote>
<p>We’ve clarified the discussion of Figure 3.</p>
</body>
</sub-article>
</article>