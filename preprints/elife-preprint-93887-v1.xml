<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">93887</article-id>
<article-id pub-id-type="doi">10.7554/eLife.93887</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.93887.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.6</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Individuals with anxiety and depression use atypical decision strategies in an uncertain world</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8091-4413</contrib-id>
<name>
<surname>Fang</surname>
<given-names>Zeming</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2238-0513</contrib-id>
<name>
<surname>Zhao</surname>
<given-names>Meihua</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1590-9025</contrib-id>
<name>
<surname>Xu</surname>
<given-names>Ting</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Yuhang</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1133-8544</contrib-id>
<name>
<surname>Xie</surname>
<given-names>Hanbo</given-names>
</name>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5416-536X</contrib-id>
<name>
<surname>Quan</surname>
<given-names>Peng</given-names>
</name>
<xref ref-type="aff" rid="a7">7</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6115-807X</contrib-id>
<name>
<surname>Geng</surname>
<given-names>Haiyang</given-names>
</name>
<xref ref-type="aff" rid="a8">8</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0654-715X</contrib-id>
<name>
<surname>Zhang</surname>
<given-names>Ru-Yuan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a9">9</xref>
<email xlink:href="mailto:ruyuanzhang@sjtu.edu.cn">ruyuanzhang@sjtu.edu.cn</email>
<xref ref-type="corresp" rid="cor1"/>
</contrib>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Liljeholm</surname>
<given-names>Mimi</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of California, Irvine</institution>
</institution-wrap>
<city>Irvine</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Frank</surname>
<given-names>Michael J</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Brown University</institution>
</institution-wrap>
<city>Providence</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<aff id="a1"><label>1</label><institution>Shanghai Mental Health, School of Medicine, Shanghai Jiao Tong University</institution>, <addr-line>Shanghai, 200030</addr-line>, <country>China</country></aff>
<aff id="a2"><label>2</label><institution>Institute of Psychology and Behavioral Science, Antai College of Economics and Management, Shanghai Jiao Tong University</institution>, <addr-line>Shanghai, 200030</addr-line>, <country>China</country></aff>
<aff id="a3"><label>3</label><institution>School of Psychology, South China Normal University</institution>, <addr-line>Guangzhou, 510631</addr-line>, <country>China</country></aff>
<aff id="a4"><label>4</label><institution>The Center of Psychosomatic Medicine, Sichuan Provincial Center for Mental Health, Sichuan Provincial People’s Hospital, University of Electronic Science and Technology of China</institution>, <addr-line>Chengdu, 611731</addr-line>, <country>China</country></aff>
<aff id="a5"><label>5</label><institution>Centre of Centre for Cognitive and Brain Sciences, Institute of Collaborative Innovation, University of Macau</institution>, <addr-line>Macau, 999078</addr-line>, <country>China</country></aff>
<aff id="a6"><label>6</label><institution>Department of Psychology, University of Arizona</institution>, <addr-line>Tucson, Arizona</addr-line>, <country>USA</country></aff>
<aff id="a7"><label>7</label><institution>Research Center for Quality of Life and Applied Psychology, Guangdong Medical University</institution>, <addr-line>Dongguan</addr-line>, <country>China</country></aff>
<aff id="a8"><label>8</label><institution>Tianqiao and Chrissy Chen Institute for Translational Research</institution>, <addr-line>Shanghai, 200040</addr-line>, <country>China</country></aff>
<aff id="a9"><label>9</label><institution>Shanghai Key Laboratory of Mental Health and Psychological Crisis Intervention, School of Psychology and Cognitive Science, East China Normal University</institution>, <addr-line>Shanghai, 200241</addr-line>, <country>China</country></aff>
<author-notes>
<corresp id="cor1">Corresponding: Ru-Yuan Zhang, Institute of Psychology and Behavioral Sciences and Shanghai Mental Health Center, Shanghai Jiao Tong University, Email: <email xlink:href="mailto:ruyuanzhang@sjtu.edu.cn">ruyuanzhang@sjtu.edu.cn</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>24</day>
<month>10</month>
<year>2023</year>
</pub-date>
<pub-date date-type="original-publication" iso-8601-date="2024-02-02">
<day>02</day>
<month>02</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP93887</elocation-id>
<history><date date-type="sent-for-review" iso-8601-date="2023-11-10">
<day>10</day>
<month>11</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-10-24">
<day>24</day>
<month>10</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.31234/osf.io/wymuc"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Fang et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Fang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-93887-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>The theory of optimal learning proposes that an agent should increase or decrease the learning rate in environments where reward conditions are relatively volatile or stable, respectively. Deficits in such flexible learning rate adjustment have been shown to be associated with several psychiatric disorders. However, this flexible learning rate (FLR) account attributes all behavioral differences across volatility contexts solely to differences in learning rate. Here, we propose instead that different learning behaviors across volatility contexts arise from the mixed use of multiple decision strategies. Accordingly, we develop a hybrid mixture-of-strategy (MOS) model that incorporates the optimal strategy, which maximizes expected utility but is computationally expensive, and two additional heuristic strategies, which merely emphasize reward magnitude or repeated decisions but are computationally simpler. We tested our model on a dataset in which 54 healthy controls and 32 individuals with anxiety and depression performed a probabilistic reversal learning task with varying volatility conditions. Our MOS model outperforms several previous FLR models. Parameter analyses suggest that individuals with anxiety and depression prefer suboptimal heuristics over the optimal strategy. The relative strength of these two strategies also predicts individual variation in symptom severity. These findings underscore the importance of considering mixed strategy use in human learning and decision making and suggest atypical strategy preference as a potential mechanism for learning deficits in psychiatric disorders.</p>
</abstract>
<kwd-group>
<title>Keywords</title>
<kwd>environmental volatility</kwd>
<kwd>probabilistic reversal learning</kwd>
<kwd>mixed decision strategy</kwd>
<kwd>anxiety</kwd>
<kwd>depression</kwd>
<kwd>heuristic strategy</kwd>
<kwd>optimal decision</kwd>
</kwd-group>

</article-meta>
</front>
<body>
<sec id="s1" sec-type="intro">
<title>Introduction</title>
<p>Intelligent behavior requires the ability to adapt to a constantly changing environment. For example, foraging animals must be able to track the changing abundance or scarcity of food resources in different locations and at different timescales. Motor control demands the ability to control limbs that constantly vary in their dynamics (due to fatigue, injury, growth, etc.). Human competitors in games or sports of all varieties must be able to learn and adapt to the changing strategies of their opponents.</p>
<p>To understand the mechanisms of these abilities, researchers have examined how (and how well) human agents can learn option values and track the dynamic changes in values in a volatile reversal learning task (<xref ref-type="bibr" rid="c3">Behrens et al., 2007</xref>). Unlike the traditional probabilistic reversal learning task where reward probabilities of two options only switch once (<xref ref-type="bibr" rid="c9">Cools et al., 2002</xref>), this paradigm includes two volatility conditions (see <xref ref-type="fig" rid="fig1">Fig. 1B</xref>): the reward probabilities of the two options keep constant in one condition (i.e., the <italic>stable</italic> condition) and switch periodically in the other (i.e., the <italic>volatile</italic> condition).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1</label>
<caption><title>Schematic diagram of the experimental task in <xref ref-type="bibr" rid="c14">Gagne et al. (2020)</xref>.</title>
<p><bold><italic>A.</italic></bold> On each trial, the participants were shown two stimuli (with potential reward presented) and were instructed to choose one of them to receive feedback. Only one stimulus results in a reward.</p>
<p><bold><italic>B.</italic></bold> Each task is evenly divided into two blocks: a stable and a volatile block. During the stable block, the environmental probability does not change, while in the volatile block, the probability flips every 20 trials.</p></caption>
<graphic xlink:href="wymucv6_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Previous studies often summarized human behaviors in this paradigm using the parameter of <italic>learning rate</italic>, a description of the efficiency with which current information is used to promote learning. The learning rate in essence serves as an abstract description of human learning behaviors, often exhibiting locality (<xref ref-type="bibr" rid="c3">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="c4">Boyd &amp; Vandenberghe, 2004</xref>). Hence, the analyses of learning rates are usually contingent on the context, where researchers fit specific learning rates to each context. Using this method, previous studies have found that humans are able to flexibly adapt to the change in environmental volatility, which is exhibited by increasing and reducing the learning rate in response to volatile and stable conditions. Impaired flexibility in adjusting the learning rate according to environmental volatility has also been observed in individuals with several psychiatric diseases, including anxiety and depression (<xref ref-type="bibr" rid="c3">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="c5">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="c14">Gagne et al., 2020</xref>). This hallmark can suggest atypical behaviors (<xref ref-type="bibr" rid="c5">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="c14">Gagne et al., 2020</xref>), psychosis (<xref ref-type="bibr" rid="c26">Powers et al., 2017</xref>), and autism spectrum disorder (<xref ref-type="bibr" rid="c20">Lawson et al., 2017</xref>). Nevertheless, there are two limitations to this context-dependent method. First, as the number of contexts increases, the number of parameters can grow dramatically, increasing the risk of over-parameterization. Second, it can be challenging to interpret the learning rate in terms of its normative meaning. The quality of a learning rate never grows monotonically with its value but rather peaks at a moderate range. A higher learning rate is not always better. Apart from these, so far, there is no idea what the learning rate is associated with in the human brain.</p>
<p>The goal of the present work is to offer an alternative explanation of human reinforcement learning that is relatively context-independent. Instead of attributing the behavioral difference to the learning rate, we focus on a less-examined subcognitive process: <italic>decision</italic>. The decision process describes how individuals strategically utilize their knowledge of the environment to generate responses. We realize this process by introducing a hybrid model, referred to as the <italic>mixture-of-strateg</italic>y (MOS), which weights and sums over various strategies. The weighting parameters reflect subjects’ decision preferences (<xref ref-type="bibr" rid="c10">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="c13">Fan et al., 2023</xref>). As we will soon show, this model offers a parsimonious explanation for human behavioral responses across varying levels of environmental volatility, using a consistent set of weighting parameters across different contexts.</p>
<p>We base the MOS model on the principle of resource-rationality, which posits that humans’ decision-making should tradeoff reward maximization and the consumption of cognitive resources (<xref ref-type="bibr" rid="c16">Gershman et al., 2015</xref>; <xref ref-type="bibr" rid="c18">Griffiths et al., 2015</xref>). Three strategies are included in the decision pool. First, we consider the optimal strategy, <italic>Expected Utility</italic> (EU), which guides decision-making based on the expected utility of each option (calculated as the probability multiplied by the reward magnitude) (<xref ref-type="bibr" rid="c34">Von Neumann &amp; Morgenstern, 1947</xref>). This EU strategy yields the maximum amount of reward, but utility calculation <italic>per se</italic> consumes substantial cognitive resources. Alternatively, humans may choose simpler strategies. For example, the <italic>magnitude-oriented</italic> (MO) strategy, where only reward magnitude was considered during the decision process, and the <italic>habitual</italic> (HA) strategy, where people simply repeat choices frequently made in the past regardless of reward magnitude (<xref ref-type="bibr" rid="c36">Wood &amp; Runger, 2016</xref>). These heuristic strategies certainly sacrifice the potential reward but come with the benefit of reducing cognitive costs in decision-making processes. We use the preference for these decision strategies to roughly estimate participants’ reward-effort tradeoff in the volatile reversal task. Choosing to heavily rely on the EU strategy is more cognitively demanding than any combination of strategies. We expected that individuals with psychiatric diseases are less likely to use the EU strategy because they are known to have shrunk cognitive resources (<xref ref-type="bibr" rid="c8">Cohen et al., 2014</xref>; <xref ref-type="bibr" rid="c19">Harvey et al., 2005</xref>; <xref ref-type="bibr" rid="c21">Levens et al., 2009</xref>; <xref ref-type="bibr" rid="c23">Moran, 2016</xref>).</p>
<p>In this study, we apply and examine the MOS model on a dataset previously reported by <xref ref-type="bibr" rid="c14">Gagne et al. (2020)</xref>. Our analysis reveals that, compared to healthy controls, patients with anxiety and depression exhibit a weaker tendency for the optimal EU strategy and a stronger preference for the simpler MO strategy, consistent with the reduced-resource hypothesis in psychiatric diseases. Furthermore, we demonstrate that this pattern of strategy preference readily accounts for several learning phenomena observed in prior research. Our work offers an alternative explanation for the effects of environmental volatility on human learning. Meanwhile, it underscores the importance of identifying behavioral markers to differentiate between explanations related to learning rate and decision strategy.</p>
</sec>
<sec id="s2" sec-type="methods|materials">
<title>Methods and Materials</title>
<sec id="s2.1"> 
<title>Datasets</title>
<p>We focused on the data from Experiment 1 reported in <xref ref-type="bibr" rid="c14">Gagne et al. (2020)</xref>. The data is publicly available via (<ext-link ext-link-type="uri" xlink:href="https://osf.io/8mzuj/">https://osf.io/8mzuj/</ext-link>). The original study included data from two experiments. The data from Experiment 2 was not used here because it was implemented on Amazon’s Mechanical Turk with no information about the participants’ clinical diagnoses. Here, we provide critical information about Experiment 1 (see <xref ref-type="bibr" rid="c14">Gagne, et al. (2020)</xref> for more technical details).</p>
<sec id="s2.1.1">
<title>Participants</title>
<p>Eighty-six participants took part in this experiment. The pool includes 20 patients with a major depressive disorder (MDD), 12 patients with a generalized anxiety disorder (GAD), and 24 healthy control participants. The diagnosis was made through a phone screen, an in-person screening session, and the structured clinical interview following DSM-IV-TR (SCID). Thirty additional participants who reported no history of psychiatric or neurological conditions were recruited without SCID. In this article, we regrouped the MDD and GAD individuals into a patient (PAT) group and the remaining 54 participants into a healthy control (HC) group. The detailed difference between MDD and GAD is not the focus of this paper. We will show later that the general factor behind MDD and GAD is the only factor that predicts learning behavior (see next section for details), a similar result reported in the original study (<xref ref-type="bibr" rid="c14">Gagne et al., 2020</xref>).</p>
<sec id="s2.1.1.1">
<title>Clinical measures</title>
<p>The severity of anxiety and depression in all participants was measured by several standard clinical questionnaires, including the Spielberger State-Trait Anxiety Inventory (STAI form Y; <xref ref-type="bibr" rid="c32">Spielberger CD, 1983</xref>), the Beck Depression Inventory (BDI; <xref ref-type="bibr" rid="c2">Beck et al., 1961</xref>), the Mood and Anxiety Symptoms Questionnaire (MASQ; <xref ref-type="bibr" rid="c7">Clark &amp; Watson, 1991</xref>; <xref ref-type="bibr" rid="c35">Watson &amp; Clark, 1991</xref>), the Penn State Worry Questionnaire (<xref ref-type="bibr" rid="c22">Meyer et al., 1990</xref>), the Center for Epidemiologic Studies Depression Scale (CESD; <xref ref-type="bibr" rid="c27">Radloff, 2016</xref>), and the Eysenck Personality Questionnaire (EPQ; <xref ref-type="bibr" rid="c12">Eysenck &amp; Eysenck, 1975</xref>). An exploratory bifactor analysis was then applied to item-level responses to disentangle the variance that is common to GAD and MDD or unique to each. The results of this analysis summarized participants’ symptoms into three orthogonal factors: a general factor (g) explaining the common symptoms, a depression-specific factor (f1), and an anxiety-specific factor (f2). Similar to the original study, here we used the same three factors to indicate the participants’ severity of their psychiatric symptoms.</p>
</sec>
<sec id="s2.1.1.2">
<title>Stimuli and behavioral task</title>
<p>This task is a volatile reversal learning task (see <xref ref-type="fig" rid="fig1">Fig. 1A</xref>). On each trial, participants were instructed to choose between two stimuli in order to receive feedback. There were two types of feedback. Participants received points or money in the reward condition and an electric shock in the punishment condition. The potential amount of reward or the intensity of electric shock (i.e., feedback magnitude) was presented together with the stimuli, but only one of the two stimuli would yield the feedback. The participant received feedback only after choosing the correct stimulus and received nothing else. The magnitude of the feedback, ranged between (1-99), is sampled uniformly for each shape from trial to trial. Each run consisted of 180 trials evenly divided into a stable and a volatile block (<xref ref-type="fig" rid="fig1">Fig. 1B</xref>). In the stable block, the dominant stimulus (i.e., the stimulus induces the feedback with a higher probability) provided a feedback with a fixed probability of 0.75, while the other one yielded a feedback with a probability of 0.25. In the volatile block, the dominant stimulus’s feedback probability was 0.8, but the dominant stimulus switched between the two every 20 trials. Hence, this design required participants to actively learn and infer the changing stimulus-feedback contingency in the volatile block. The whole experiment included two runs each for the two feedback conditions. 79 participants completed both feedback conditions. 4 participants only completed the reward condition, and 3 participants completed the punishment conditions.</p>
</sec>
</sec>
</sec>
<sec id="s2.2">
<title>Computational Modeling</title>
<p>Each participant in the experiment must address two fundamental challenges: 1) decision-making, by adhering to a strategy that determines the action to maximize benefit; and 2) learning, by figuring out the untold feedback probability via feedback.</p>
<p>Before formalizing each challenge, we introduce our notation system. We denote each stimulus <italic>s</italic> as one of two possible states <italic>s</italic> ∈ {<italic>s</italic><sub>1</sub>, <italic>s</italic><sub>2</sub>}, <italic>s</italic><sub>1</sub> refers to the left stimulus, and <italic>s</italic><sub>2</sub> the right one. The labeled feedback magnitude (i.e., reward points or shock intensity) of the stimulus is <italic>m</italic>(<italic>s</italic>), and the feedback probability is <italic>ψ</italic>(<italic>s</italic>). Following the convention in reinforcement learning (<xref ref-type="bibr" rid="c33">Sutton &amp; Barto, 2018</xref>), we presume that the decision is made from a policy <italic>π</italic> that maps the observed magnitudes <italic>m</italic> and currently maintained feedback probabilities <italic>ψ</italic> to a distribution over stimuli, <italic>π</italic>(<italic>s</italic>|<italic>m</italic>, <italic>ψ</italic>). The construct of the policy varies between models (see below).</p>
<sec id="s2.2.1">
<title>The mixture-of-strategy (MOS) model</title>
<p>The key signature of the hybrid MOS model is that its policy consists of a mixture of three strategies: expected utility (EU), magnitude-oriented (MO), and habitual (HA). The EU strategy postulates that human agents rationally calculate the value of each stimulus and use the softmax rule to select an action. In this case, the value of a stimulus should be its expected utility: <italic>m</italic>(<italic>s</italic>)<italic>ψ</italic>(<italic>s</italic>).</p>
<p>The probability of choosing a stimulus s thus follows a softmax function.
<disp-formula id="ed1"><alternatives><mml:math display="block" id="e1"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mtext>EU</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>|</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="wymucv6_eqn1.tif" mimetype="image" mime-subtype="tiff"/></alternatives><label>(1)</label></disp-formula>
where <italic>β</italic> is the inverse temperature that is used to round the policy to a Bernoulli distribution. For simplicity, we rewrite <xref ref-type="disp-formula" rid="ed1">Eq. 1</xref> in the following form:
<disp-formula id="ed2"><alternatives><mml:math display="block" id="e2"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mtext>EU</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>|</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="wymucv6_eqn2.tif" mimetype="image" mime-subtype="tiff"/></alternatives><label>(2)</label></disp-formula></p>
<p>Different from the EU strategy, the MO strategy postulates that observers only focus on feedback magnitude <italic>m</italic>(<italic>s</italic>), disregarding feedback probability <italic>ψ</italic>(<italic>s</italic>). This is certainly an irrational strategy but more economical in terms of cognitive efforts. Feedback magnitudes are explicitly shown with the stimuli in each trial and readily available for any related cognitive computation. But feedback probability, as a latent variable, requires trial-by-trial learning and inference, which is more cognitively demanding. The MO strategy is defined as,
<disp-formula id="ed3"><alternatives><mml:math display="block" id="e3"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mtext>MO</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="wymucv6_eqn3.tif" mimetype="image" mime-subtype="tiff"/></alternatives><label>(3)</label></disp-formula></p>
<p>Like the EU strategy, the MO strategy is converted to a Bernoulli distribution after passing through a softmax function. This processing is necessary because a hybrid model is uninterpretable when its components follow heterogeneous distributions. The softmax function enhances the model’s interpretability.</p>
<p>Unlike EU and MO, the HA strategy depends on neither feedback magnitude <italic>m</italic>(<italic>s</italic>) nor feedback probability <italic>ψ</italic>(<italic>s</italic>). The HA strategy reflects the tendency to repeat previous frequent choices. This reinforcement reflects the habit of choosing a stimulus, a phenomenon sometimes called hot-hand bias (<xref ref-type="bibr" rid="c17">Gilovich et al., 1985</xref>) or perseveration (<xref ref-type="bibr" rid="c15">Gershman, 2020</xref>; <xref ref-type="bibr" rid="c36">Wood &amp; Runger, 2016</xref>) in literature. For example, if an agent chooses the left stimulus more often in past trials, she forms a preference for the left stimulus in future trials. We constructed it as a Bernoulli distribution (henceforth called habitual distribution) over the two stimuli <italic>π</italic><sub>HA</sub> (<italic>s</italic>). The trial-by-trial update rule of <italic>π</italic><sub>HA</sub> (<italic>s</italic>) will be detailed in <xref ref-type="disp-formula" rid="ed5">Eqs. 5</xref>–<xref ref-type="disp-formula" rid="ed6">6</xref> below.</p>
<p>We implemented the hybrid policy of a linear mixture of the three strategies following the methods used in <xref ref-type="bibr" rid="c10">Daw et al. (2011)</xref>,
<disp-formula id="ed4"><alternatives><mml:math display="block" id="e4"><mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>|</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mtext>HA</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>EU</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mtext>EU</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>|</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>MO</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mtext>MO</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>HA</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mtext>HA</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math><graphic xlink:href="wymucv6_eqn4.tif" mimetype="image" mime-subtype="tiff"/></alternatives><label>(4)</label></disp-formula>
where <italic>w</italic><sub>EU</sub>, <italic>w</italic><sub>MO</sub>, and <italic>w</italic><sub>HA</sub> are the weighting parameters of each strategy. The three weighting parameters should be summed to 1, i.e., <italic>w</italic><sub>EU</sub> + <italic>w</italic><sub>MO</sub> + <italic>w</italic><sub>HA</sub> = 1. We can thus describe the policy an observer adopted just by examining the weighting parameters.</p>
<p>Next, we modeled the second challenge — the probabilistic learning process. Two distributions — the feedback probability and the habitual probability — are learned and updated in a trial-by-trial fashion. We updated the feedback probability according to the outcome of the left stimulus <italic>s</italic><sub>1</sub>:
<disp-formula id="ed5"><alternatives><mml:math display="block" id="e5"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>ψ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="wymucv6_eqn5.tif" mimetype="image" mime-subtype="tiff"/></alternatives><label>(5)</label></disp-formula>
where <italic>α<sub>ψ</sub></italic> is the learning rate. <italic>O</italic>(⋅) is an indicator function that returns 1 at the true feedback stimulus or 0 otherwise. Intuitively, the stimulus that induced a reward would be reinforced and its feedback probability was enhanced. This update equation is the standard format of the well-known Rescorla-Wagner model (<xref ref-type="bibr" rid="c28">Rescorla, 1972</xref>). To keep consistent with <xref ref-type="bibr" rid="c14">Gagne et al., (2020)</xref>, we also explored the valence-specific learning rate in some models,
<disp-formula id="ed5a"><alternatives><mml:math display="block" id="e5a"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>ψ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> for </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> for </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math><graphic xlink:href="wymucv6_eqn5a.tif" mimetype="image" mime-subtype="tiff"/></alternatives></disp-formula></p>
<p>The habitual distribution is updated in a similar manner.
<disp-formula id="ed6"><alternatives><mml:math display="block" id="e6"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mtext>HA</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mtext>HA</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>π</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mtext>HA</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mtext>HA</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mtext>HA</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="wymucv6_eqn6.tif" mimetype="image" mime-subtype="tiff"/></alternatives><label>(6)</label></disp-formula>
where <italic>α<sub>π</sub></italic> is the learning rate. <italic>A</italic>(⋅) is also an indicator function that returns 1 for the stimulus chosen at the current trial. Intuitively, the stimulus chosen in each trial regardless of its feedback will be reinforced via <xref ref-type="disp-formula" rid="ed6">Eq. 6</xref>.</p>
<p>We developed two variants for each model, a context-free and a context-dependent variant. The context-free MOS6 has six parameters <italic>ξ</italic> = {<italic>β</italic>, <italic>α</italic><sub>HA</sub>, <italic>α</italic><sub><italic>ψ</italic></sub>, <italic>w</italic><sub>EU</sub>, <italic>w</italic><sub>MO</sub>, <italic>w</italic><sub>HA</sub>}. This variant does not include the value-specific learning rate design. The context dependent variant MOS22 has 22 parameters. Among them <italic>β</italic> and <italic>α</italic><sub>HA</sub> are context-free parameters that holds the same for all contexts. Parameters {<italic>α</italic><sub><italic>ψ+</italic></sub>, <italic>α</italic><sub><italic>ψ−</italic></sub>, <italic>w</italic><sub>EU</sub>, <italic>w</italic><sub>MO</sub>, <italic>w</italic><sub>HA</sub>} are context-dependent parameters that are fitted specifically to each context. We will further discuss the model fitting details in the later section.</p>
</sec>
<sec id="s2.2.2">
<title>The flexible learning rate (FLR) model</title>
<p>The FLR model refers to Model 11 (i.e., the best-fitting model) in <xref ref-type="bibr" rid="c14">Gagne et al. (2020)</xref>. Here, we describe the FLR model using the same notation system with the published paper, slightly different from the notations in the MOS model. The FLR model models the probability of selecting the left stimulus <italic>s</italic><sub>1</sub> as,
<disp-formula id="ed7"><alternatives><mml:math display="block" id="e7"><mml:mrow><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mtext>HA</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mtext>HA</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mtext>HA</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mtext>HA</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="wymucv6_eqn7.tif" mimetype="image" mime-subtype="tiff"/></alternatives><label>(7)</label></disp-formula>
where <italic>β</italic> and <italic>β</italic><sub>HA</sub> are the inverse temperature parameters of the value of the left stimulus and the HA strategy, respectively. The value of the left stimulus <italic>v</italic> represents the advantage of <italic>s</italic><sub>1</sub> over <italic>s</italic><sub>2</sub>,
<disp-formula id="ed8"><alternatives><mml:math display="block" id="e8"><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>sign</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo>|</mml:mo><mml:mi>r</mml:mi></mml:msup></mml:mrow></mml:math><graphic xlink:href="wymucv6_eqn8.tif" mimetype="image" mime-subtype="tiff"/></alternatives><label>(8)</label></disp-formula>
where <italic>λ</italic> is the weighting parameter balancing the two terms. The first term <italic>ψ</italic>(<italic>s</italic><sub>1</sub>) − <italic>ψ</italic>(<italic>s</italic><sub>2</sub>) indicates the feedback probability difference between the two options. The second term, sign(<italic>m</italic> (<italic>s</italic><sub>1</sub>) − <italic>m</italic>(<italic>s</italic><sub>2</sub>))|<italic>m</italic>(<italic>s</italic><sub>1</sub>) − <italic>m</italic>(<italic>s</italic><sub>2</sub>)|<sup><italic>r</italic></sup>, indicates the feedback magnitude differences scaled by a non-linear factor <italic>r</italic>. Intuitively, the value <italic>v</italic> of <italic>s</italic><sub>1</sub> can be understood as the weighted sum of the two terms. We write this nonlinear scaling in a slightly different form with Eq. 1b in <xref ref-type="bibr" rid="c14">Gagne et al. (2020)</xref> to better replicate their coding implementation.</p>
<p>During the learning stage, the FLR model learns the feedback probability using the same equations in the MOS model (<xref ref-type="disp-formula" rid="ed5">Eqs. 5</xref>–<xref ref-type="disp-formula" rid="ed6">6</xref>). The context-free variant FLR6 has 6 parameters <italic>ξ</italic> = {<italic>α</italic><sub>HA</sub>, <italic>β</italic><sub>HA</sub>, <italic>r</italic>, <italic>α</italic><sub><italic>ψ</italic></sub>, <italic>β</italic>, <italic>λ</italic>}. The context-dependent variant FLR19 considers {<italic>α</italic><sub>HA</sub>, <italic>β</italic><sub>HA</sub>, <italic>r</italic>} as context-free parameters; {<italic>α</italic><sub><italic>ψ</italic>+</sub>, <italic>α</italic><sub><italic>ψ</italic>−</sub>, <italic>β</italic>, <italic>λ</italic>} as context-dependent parameters.</p>
</sec>
<sec id="s2.2.3">
<title>The risk-sensitive (RS) model</title>
<p>We adopted the RS model from <xref ref-type="bibr" rid="c3">Behrens, et al. (2007)</xref>. The RS model assumes that participants apply the EU policy but with a subjectively distorted feedback probability <inline-formula id="i1"><alternatives><mml:math display="inline" id="im1"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="wymucv6_ieq1.tif" mimetype="image" mime-subtype="tiff"/></alternatives></inline-formula>,
<disp-formula id="ed9"><alternatives><mml:math display="block" id="e9"><mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mi>ψ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>ψ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="wymucv6_eqn9.tif" mimetype="image" mime-subtype="tiff"/></alternatives><label>(9)</label></disp-formula>
where <italic>β</italic> is the inverse temperature. The distorted probability is calculated by,
<disp-formula id="ed10"><alternatives><mml:math display="block" id="e10"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mi>ψ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>min</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mi>ψ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>ψ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="wymucv6_eqn10.tif" mimetype="image" mime-subtype="tiff"/></alternatives><label>(10)</label></disp-formula>
where the <italic>γ</italic> indicates participants’ risk sensitivity. When <italic>γ</italic> = 1, a participant has an optimal risk balance. <italic>γ</italic> &lt; 1 and <italic>γ</italic> &gt; 1 indicate risk-seeking and risk-aversive tendencies, respectively.</p>
<p>The RS model learns the feedback probability the same as the MOS and FLR models (i.e., <xref ref-type="disp-formula" rid="ed5">Eq. 5</xref>). The model did not include the HA strategy. The context-free variant RS3 has 3 parameters <italic>ξ</italic> = {<italic>β</italic>, <italic>α</italic><sub><italic>ψ</italic></sub>, <italic>γ</italic>}. The context-dependent variant RS12 considers {<italic>β</italic>, <italic>α</italic><sub><italic>ψ</italic>+</sub>, <italic>α</italic><sub><italic>ψ</italic>−</sub>, <italic>γ</italic>} as context-dependent parameters.</p>
</sec>
<sec id="s2.2.4">
<title>The Pearce-Hall (PH) model</title>
<p>That people use different learning rates in different contexts implies that people adaptively adjust the learning rate during the learning process. To constitute this hypothesis, we adopt the PH model, an adaptive learning rate model, from <xref ref-type="bibr" rid="c24">Pearce and Hall (1980)</xref>. The PH model posits that an adaptive learning rate in <xref ref-type="disp-formula" rid="ed5">Eq. 5</xref>,
<disp-formula id="ed11"><alternatives><mml:math display="block" id="e11"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mi>ψ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="wymucv6_eqn11.tif" mimetype="image" mime-subtype="tiff"/></alternatives><label>(11)</label></disp-formula>
where <italic>k</italic> is a scale factor of the learning rate. Each trial the learning rate is updated in accordance with the absolute prediction error,
<disp-formula id="ed12"><alternatives><mml:math display="block" id="e12"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>ψ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>ψ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mi>ψ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="wymucv6_eqn12.tif" mimetype="image" mime-subtype="tiff"/></alternatives><label>(12)</label></disp-formula>
where <italic>η</italic> is the step size for the learning rate. We have no knowledge of participants’ learning rate values before the experiment, so we need to also fit the initial learning rate value, <inline-formula id="i2"><alternatives><mml:math display="inline" id="im2"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>ψ</mml:mi><mml:mn>0</mml:mn></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="wymucv6_ieq2.tif" mimetype="image" mime-subtype="tiff"/></alternatives></inline-formula>. The PH model generate a choice through a sigmoid function,
<disp-formula id="ed13"><alternatives><mml:math display="block" id="e13"><mml:mrow><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="wymucv6_eqn13.tif" mimetype="image" mime-subtype="tiff"/></alternatives><label>(13)</label></disp-formula></p>
<p>The context-free variant PH4 has 4 parameters <inline-formula id="i3"><alternatives><mml:math display="inline" id="im3"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msubsup><mml:mi>α</mml:mi><mml:mi>ψ</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>η</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="wymucv6_ieq3.tif" mimetype="image" mime-subtype="tiff"/></alternatives></inline-formula>. The context-dependent variant PH17 considers {<inline-formula id="i4"><alternatives><mml:math display="inline" id="im4"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>ψ</mml:mi><mml:mn>0</mml:mn></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="wymucv6_ieq2.tif" mimetype="image" mime-subtype="tiff"/></alternatives></inline-formula>} as context-free parameters; {<italic>k</italic><sub>+</sub>, <italic>k</italic><sub>−</sub>, <italic>η</italic>, <italic>γ</italic>} as context-dependent parameters.</p>
</sec>
</sec>
<sec id="s2.3">
<title>Model fitting</title>
<p>To characterize participants’ behavioral patterns in different experimental context <italic>c</italic>, we fit the context-dependent parameters to each context following a 2-by-2 factorial structure (<xref ref-type="table" rid="tab1">Table 1</xref>). For example, in the MOS model, we were only interested in the learning rate parameters <italic>α</italic><sub><italic>ψ</italic></sub> and three strategies weights <italic>w</italic><sub>EU</sub>, <italic>w</italic><sub>MO</sub>, <italic>w</italic><sub>HA</sub> and fit them separately to each context. The remaining two parameters {<italic>β</italic>, <italic>α</italic><sub>HA</sub>} were held constant across all four experimental contexts for each participant. Thus, there were 22 free parameters (2 context-free parameters + 5 context-dependent parameters × 4 conditions) of the MOS model in each participant. In contrast, the context-free variant (MOS6), we fit the same set of parameters to all contexts.</p>
<table-wrap id="tab1">
<label>Table 1</label>
<caption><title>Four experimental contexts</title></caption>
<alternatives>
<graphic xlink:href="wymucv6_tab1.tif" mimetype="image" mime-subtype="tiff"/>
<table frame="hsides" rules="all">
<thead>
<tr>
<th align="left"/>
<th align="left">Reward</th>
<th align="left">Aversive</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Stable</td>
<td align="left">Reward-Stable</td>
<td align="left">Aversive-Stable</td>
</tr>
<tr>
<td align="left">Volatile</td>
<td align="left">Reward-Volatile</td>
<td align="left">Aversive-Stable</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Parameters were estimated for each participant via the maximum a posteriori (MAP) method. The objective function to maximize is:
<disp-formula id="ed11a"><alternatives><mml:math display="block" id="e11a"><mml:mrow><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>ξ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mi>log</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mtext>M</mml:mtext><mml:mo>,</mml:mo><mml:mi>ξ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ξ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math><graphic xlink:href="wymucv6_eqn11a.tif" mimetype="image" mime-subtype="tiff"/></alternatives><label>(11)</label></disp-formula>
where <italic>ξ</italic>(<italic>c</italic>) means the model parameters under condition <italic>c</italic>. <italic>M</italic> is the model and <italic>N</italic> refers to the number of trials of the participant’s behavioral data in condition <italic>c</italic>. <italic>m</italic><sub><italic>i</italic></sub>, <italic>O</italic><sub><italic>i</italic></sub>, and <italic>s</italic><sub><italic>i</italic></sub> are the presented magnitude, true feedback probability, and participants’ responses recorded in each trial.</p>
<p>Parameter estimation was performed using the <italic>Broyden-Fletcher-Goldfarb-Shanno (BFGS)</italic> algorithm in the <italic>scipy.optimize</italic> module in Python. This algorithm provides an approximation of the inverse Hessian matrix for the parameter, a critical component that can be employed in Bayesian model selection (<xref ref-type="bibr" rid="c29">Rigoux et al., 2014</xref>). For each participant, we ran the optimization with 40 randomly chosen initial parameters to avoid local minima.</p>
<p>In order to use the BFGS algorithm, we reparametrized the model, thereby transforming the original fitting problem into an unconstrained optimization problem (<xref ref-type="sec" rid="s6">Supplemental material Note 1</xref>). Importantly, to fit the weighting parameters (<italic>w</italic><sub>EU</sub>, <italic>w</italic><sub>MO</sub>, <italic>w</italic><sub>HA</sub>) and ensure them summed to 1, we parameterized the weighting parameters as outputs of a softmax function,
<disp-formula id="ed14"><alternatives><mml:math display="block" id="e14"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>sofmax</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>∀</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mtext>EU</mml:mtext><mml:mo>,</mml:mo><mml:mtext>MO</mml:mtext><mml:mo>,</mml:mo><mml:mtext>HA</mml:mtext><mml:mo>}</mml:mo></mml:mrow></mml:math><graphic xlink:href="wymucv6_eqn14.tif" mimetype="image" mime-subtype="tiff"/></alternatives><label>(14)</label></disp-formula>
and fit the logits <italic>λ</italic><sub><italic>i</italic></sub> of the weights. All logits were assumed to be normally distributed with a prior <italic>N</italic> (0, 10). Due to its normality, we also used the logits as participants’ strategy preferences in statistical analyses in the result section. To provide better intuition, we will use the terms “weighting parameters” and “logits” interchangeably in the following section. Specifically, we may refer to the logits <italic>λ</italic><sub>EU</sub>, <italic>λ</italic><sub>MO</sub>, and <italic>λ</italic><sub>HA</sub> as the weighting parameters.</p>
</sec>
<sec id="s2.4">
<title>Simulate to explain the previous learning rate effects</title>
<p>In the result section, <italic>Explain the previous learning rate effects using the strategy preferences</italic>, we illustrate how we explicate some classical learning rate phenomena on the weighting parameters of the MOS model, referred to as the strategy preferences. Here are some technical details.</p>
<p>We simulate to show that the strategy preferences alone can explain the slower learning curve in the patient group. Each simulation task included 90 stable trials, followed by 90 volatile trials. The parameters used for simulations are <italic>β</italic> = 8.536, <italic>α</italic><sub>HA</sub> = 0.403, <italic>α</italic><sub><italic>ψ</italic></sub> = 0.460, <italic>λ</italic><sub>EU</sub> = 0.712, <italic>λ</italic><sub>MO</sub> = −0.988, <italic>λ</italic><sub>HA</sub> = 0.276. We outputted the predicted probability of the left action <italic>a</italic><sub>1</sub> for each strategy along with the learning trials to generate <xref ref-type="fig" rid="fig4">Fig. 4B</xref>. We simulated the policy for the healthy control group using the averaged parameters except for replacing the three weighting parameters with <inline-formula id="i5"><alternatives><mml:math display="inline" id="im5"><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>EU</mml:mtext></mml:mrow><mml:mrow><mml:mtext>HC</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.873</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>MO</mml:mtext></mml:mrow><mml:mrow><mml:mtext>HC</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1.575</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>HA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>HC</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.701</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="wymucv6_ieq4.tif" mimetype="image" mime-subtype="tiff"/></alternatives></inline-formula> (<xref ref-type="fig" rid="fig4">Fig. 4A</xref>, HC curve). The same method was applied HC HC HC to generate the PAT curve using parameters <inline-formula id="i6"><alternatives><mml:math display="inline" id="im6"><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>EU</mml:mtext></mml:mrow><mml:mrow><mml:mtext>PAT</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.109</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>MO</mml:mtext></mml:mrow><mml:mrow><mml:mtext>PAT</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.072</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>HA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>PAT</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.037</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="wymucv6_ieq5.tif" mimetype="image" mime-subtype="tiff"/></alternatives></inline-formula>.</p>
<p>Demonstrating the remaining two effects is equivalent to establishing the following assertion: when compared to the MO strategy, a preference for the EU strategy results in a qualitatively greater extent of increase from stable to volatile conditions, whereas a preference for the MO strategy corresponds to a lesser increase extent. We generated 10 blocks of synthesized data on the MOS model with reward feedback using <italic>β</italic> = 8.536, <italic>α</italic><sub>HA</sub> = 0.403, <italic>α</italic><sub><italic>ψ</italic></sub> = 0.460, 5 blocks each for the EU and MO strategies. For the EU strategy, we set weighting parameters <italic>λ</italic><sub>EU</sub> = 10, <italic>λ</italic><sub>MO</sub> = 0, <italic>λ</italic><sub>HA</sub> = 0, which yields <italic>w</italic><sub>EU</sub> ≈ 1; Similarly, we set the weighting parameters to <italic>λ</italic><sub>EU</sub> = 0, <italic>λ</italic><sub>MO</sub> = 10, <italic>λ</italic><sub>HA</sub> = 0 to synthesize the learning curve for the MO strategy, resulting in <italic>w</italic><sub>MO</sub> ≈ 1. We then fit the FLR and the RS models to these synthesized data, controlling all parameters except for the learning rate.</p>
<p>All parameter values introduced here are reparametrized rather than the raw values.</p>
</sec>
<sec id="s2.5">
<title>Parameter recovery and model recovery analyses</title>
<p>We conducted a parameter recovery analysis to validate the fitting of the MOS models. We generated 80 synthetic datasets varying the four parameters of interest {<italic>α</italic><sub><italic>ψ</italic></sub>, <italic>λ</italic><sub>EU</sub>, <italic>λ</italic><sub>MO</sub>, <italic>λ</italic><sub>HA</sub>}. The remaining parameters were fixed to the averaged fitted weighting parameters <italic>β</italic> = 8.804, <italic>α</italic><sub>HA</sub> = 0.366. Each dataset in both cases contained ten blocks. For each dataset, we fit our MOS6 model to the data and compared the fitted parameters and the ground-truth parameters. Parameter recovery analysis aims to exclude the exchangeability between the learning rate and weighting parameters.</p>
<p>To further differentiate models, we also performed a model recovery analysis. We generated 40 synthetic ten-block datasets from the MOS6 model, using the parameters fitted to each participant. We fit all six models to each dataset and examined whether the MOS6 model, as the generative model, was still the best-fitting model on the synthetic datasets.</p>
</sec>
</sec>
<sec id="s3" sec-type="results">
<title>Results</title>
<p>To illustrate that the mixture of strategies provides parsimonious alternative explanations, we first demonstrate that the context-free MOS6 model can quantitatively capture human learning behaviors and predict individual psychiatric syndromes. Furthermore, we simulate to show that the MOS6 model, without its parameters held constant, can explain certain human behavioral phenomena, as previously evidenced by context-dependent learning rates.</p>
<sec id="s3.1">
<title>The mixture-of-policy model quantitatively captures learning behaviors</title>
<p>We fit a total of eight models to the behavioral data reported in <xref ref-type="bibr" rid="c14">Gagne et al. (2020)</xref>. Model fitting and comparison results are summarized in Table 2. To quantify goodness-of-fit, we calculated negative total likelihood (NLL), Akaike Information Criterion (AIC; <xref ref-type="bibr" rid="c1">Akaike, 1974</xref>), and Bayesian Information Criterion (BIC; <xref ref-type="bibr" rid="c30">Schwarz, 1978</xref>) for each individual participant and performed Bayesian model selection at the group level (<xref ref-type="bibr" rid="c29">Rigoux et al., 2014</xref>).</p>
<p>Modeling fitting reveals that the MOS framework accurately accounts for human behaviors. MOS6 and MOS22 were the best-fitting models in terms of BIC and AIC, respectively. The different results based on AIC and BIC may be due to the different degrees of penalty on model complexity (i.e., number of free parameters). The group-level Bayesian model comparisons suggested MOS6 as the best-fitting model. These model comparisons underscore that the MOS framework outperforms existing models, FLR and RS, in the previous literature (<xref ref-type="bibr" rid="c3">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="c14">Gagne et al., 2020</xref>). Importantly, an analysis of the parameters in the MOS22 model revealed no significant differences across different experimental contexts (discussed later). This suggests that MOS22 and MOS6 are not qualitatively distinct. Therefore, we conclude that the MOS6 model can effectively account for human behaviors in a relatively context-free manner.</p>
<p>The difference in learning rates between stable and volatile conditions highlights the human capacity to flexibly adapt their learning rate in response to environmental volatility. To explore this adaptability, we applied a model with a built-in adaptive learning rate known as the Pearce-Hall model (PH4, PH17). However, our findings indicate that this model does not provide a better explanation than the MOS6 model. This suggests that there may be behavioral variations that cannot be fully accounted for by the parameters of learning rate.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2</label>
<caption><title>Model comparison results. The reds indicate the target models.</title>
<p><italic><bold>A-C.</bold></italic> Averaged relative increase in negative log-likelihood (NLL), Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC) subtracted by the lowest values. Lower scores indicate better models. Error bars indicate the standard deviation for the estimated mean across 86 participants.</p>
<p><italic><bold>D.</bold></italic> Protected exceedance probability (PXP) for the group-level Bayesian model selection.</p></caption>
<graphic xlink:href="wymucv6_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3.2">
<title>MDD and GAD patients favor simpler and more irrational strategies</title>
<p>The MOS model assumes that each participant’s response is a result of a weighted combination of three strategies, EU, MO, and HA. We therefore can summarize participants’ decision preferences using the weighting parameters <italic>w</italic>. For example, a larger weight <italic>w</italic><sub>EU</sub> for the EU strategy indicates the participant’s tendency of using the rational strategy in value computation and action selection. For the significant testing, we used the logit (<italic>λ</italic>) of the weight parameters (<italic>w</italic>) as indicators of decision preference for significant testing. This is because the weighting parameters are not normally distributed, but their logits are approximately subject to the normal assumption. We performed a <italic>Welch’s t-test</italic> (<xref ref-type="bibr" rid="c11">Delacre et al., 2017</xref>) on the MOS6 model to ensure a reliable analysis of the unequal population for the health control and patient groups. We found that the patient group exhibited a weaker tendency for the rational EU strategy (<italic>t</italic>(57.980) = 2.195, <italic>p</italic> = 0.032, Cohen’s d = 0.508) and the HA strategy (<italic>t</italic>(59.032) = 2.389, <italic>p</italic> = 0.020, Cohen’s d = 0.550), but stronger tendency for the MO strategy(<italic>t</italic>(63.746) = −3.479, <italic>p</italic> = 0.001, Cohen’s d = 0.783) (<xref ref-type="fig" rid="fig3">Fig. 3A</xref>). However, there was no group difference in (log) learning rate (<italic>t</italic>(72.041) = 0.678, <italic>p</italic> = 0.500, Cohen’s d = 0.147).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3</label>
<caption><title>Parameter analyses of the MOS6 model.</title>
<p><bold><italic>A.</italic></bold> The weighting parameters for the healthy control participants (HC, purple) and the patients (PAT, pink) diagnosed with MDD and GAD. The y-axis means averaged preference over different volatility levels (volatile and stable) and feedback types (reward and aversive). Error bars reflect the standard deviation across 86 participants. Significance symbol conventions are: *: <italic>p</italic> &lt; 0.05; <italic>p</italic> &lt; 0.01; <italic>p</italic> &lt; 0.001; n.s.: non-significant.</p>
<p><bold><italic>B.</italic></bold> Decision preferences predict participants’ general factor score (<italic>g</italic> score) in the bifactor analysis reported in <xref ref-type="bibr" rid="c14">Gagne, et al. (2020)</xref>. The y-axis indicates the averaged preference over different volatility levels (volatile and stable) and feedback types (reward and aversive). This average operation is permitted here because the logit of the weight is normally distributed. The shaded areas reflect 95% confidence intervals of the regression prediction.</p></caption>
<graphic xlink:href="wymucv6_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>For completeness, we also examined whether the decision preferences and the learning rates varied as a function of volatility levels (stable/volatile) and feedback types (reward/aversive) using the MOS22. We conducted three 2 × 2 × 2 ANOVAs on the logit of the three weighting parameters of MOS22 and found no significant relationship between different volatility levels or feedback types (all <italic>p</italic>s &gt; 0.149) except that participants are more likely to use EU strategy under the reward condition (<italic>F</italic>(1, 300) = 13.426, <italic>p</italic> = 0.021, <italic>η</italic><sup>2</sup> = 0.016; see more details in <xref ref-type="sec" rid="s7">Supplemental Note 2</xref>). In addition, the between-group (HC/PAT) analyses of the decision preferences in MOS22 are mostly consistent with the MOS6 results shown above (<xref ref-type="fig" rid="figS2">Supplemental Fig. S2</xref>). For the (log) learning rate parameters, we also examined the outcome valence (higher/lower reward than expectation) apart from the volatility levels or feedback types, finding no general significance (all <italic>p</italic>s &gt; 0.046; <xref ref-type="sec" rid="s7">Supplemental material Note 2</xref>). All these results suggest that one set of parameters is sufficient for the MOS model to describe the human behavioral dataset.</p>
</sec>
<sec id="s3.3">
<title>Decision preferences predict the general severity of anxiety and depression</title>
<p>We investigated the relationship between decision preferences and psychiatric symptom severity (<xref ref-type="fig" rid="fig3">Fig. 3B</xref>). To measure symptom severity, we used the bifactor analysis approach described by <xref ref-type="bibr" rid="c14">Gagne et al., (2020)</xref> which decomposed measurements of symptom severity into the factors specific to anxiety and depression, and the general factor (<italic>g</italic> score) indicating the common symptoms shared by them. Our findings indicate that patients with severe symptoms exhibit a weaker tendency of using the optimal EU strategy (Pearson’s <italic>r</italic> = −0.221, <italic>p</italic> = 0.040) but a stronger tendency of using the MO strategy (Pearson’s <italic>r</italic> = 0.360, <italic>p</italic> = 0.001). Additionally, there was a significant correlation between symptom severity and the preference for the HA strategy (Pearson’s <italic>r</italic> = −0.285, <italic>p</italic> = 0.007). In other words, the participants with severer anxiety tend to use a less accurate but simpler strategy for probabilistic learning. Again, we suspect that this is because anxiety and depression reduce cognitive resources in patients, and they have to choose less resource-consuming strategies. We will return to this point in the discussion.</p>
</sec>
<sec id="s3.4">
<title>Explain learning rate effects using the strategy preferences</title>
<p>Three ubiquitous observations have been documented in probabilistic learning tasks. First, individuals with anxiety and depression often exhibit a slower learning curve over the course of learning, as evidenced by a smaller fitted learning rate (<xref ref-type="bibr" rid="c6">Chen et al., 2015</xref>; <xref ref-type="bibr" rid="c25">Pike &amp; Robinson, 2022</xref>) (<xref ref-type="bibr" rid="c6">Chen et al., 2015</xref>). Second, to adapt to the high environmental volatility, subjects increase their learning rate to generate a faster learning curve (<xref ref-type="bibr" rid="c3">Behrens et al., 2007</xref>). Third, the extent of the learning rate increment from the stable to the volatile condition is smaller in the patient group, a hallmark of their learning deficits (<xref ref-type="bibr" rid="c5">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="c14">Gagne et al., 2020</xref>). Here, we demonstrate that the MOS model can qualitatively reproduce all three effects by only attributing them to strategy preferences without resorting to the learning rate parameter.</p>
<p>We first averaged over data from 43 human participants (26 health control and 17 patients) in an experimental context and showed that the patients exhibited slower convergence to the true feedback probability than the healthy control group (<xref ref-type="fig" rid="fig4">Fig. 4A</xref>). Next, we used the MOS model to simulate the learning behaviors of the two groups using the averaged weighting parameters {<italic>w</italic><sub>EU</sub>, <italic>w</italic><sub>MO</sub>, <italic>w</italic><sub>HA</sub>} of each group. Meanwhile, we controlled the learning rate effect by fixing the parameters {<italic>β</italic>, <italic>α</italic><sub>HA</sub>, <italic>α</italic><sub><italic>ψ</italic></sub>} to their averaged values across all participants for both HA and PAT groups, volatility levels (stable/volatile), and feedback types (reward/punishment).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4</label>
<caption><title>Simulated learning behavior of the two groups and the three strategies. The black dashed lines indicate the ground truth feedback probability. The simulated curves in B-C were generated by averaging 500 simulations.</title>
<p><bold><italic>A.</italic></bold> The human learning curves of the two groups were produced using the data in the aversive context (see the reward context in <xref ref-type="fig" rid="figS3">Fig. S3</xref>) and smoothed by a Gaussian kernel with a window size of 5 trials and a s.t.d. of 2 trials.</p>
<p><bold><italic>B.</italic></bold> Simulated the learning curves of the two groups using their fitted parameters in the MOS model.</p></caption>
<p><bold><italic>C.</italic></bold> The simulated learning curve for each strategy.</p>
<graphic xlink:href="wymucv6_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Without introducing the learning rate difference between healthy controls and patients, we observed the same slower learning curve in the patient group in the simulations (<xref ref-type="fig" rid="fig4">Fig. 4B</xref>). To gain insights, we visualized their respective learning curves throughout the learning task (<xref ref-type="fig" rid="fig4">Fig. 4C</xref>). The EU strategy, which is theoretically optimal, quickly approximates the true feedback probability and exhibits a faster learning curve. The HA strategy can also adapt to the volatile environment at a slower adaptation speed and with longer delays, resulting in a slower learning curve. This is intuitively reasonable because shaping a habit usually takes a longer time. The MO strategy is not adaptive to environmental volatility at all and exhibits a flat learning curve throughout the entire course of learning. As previously mentioned, the patients tend to be more magnitude-oriented (MO) possibly because of their inability to afford the effort-consuming EU strategy. Their preference for the slowest decision strategy induces a flattened learning curve in the probabilistic learning task. Therefore, we conclude that strategy preference can explain the slower learning curves in the patient group.</p>
<p>Next, we investigate whether the strategy preferences can account for the remaining two effects. Based on our earlier conclusion that health participants prefer the EU strategy while patients favor the MO strategy, we equate this problem as showing that a preference for the EU strategy results in a greater extent of increase in the learning rate from stable to volatile condition, whereas a preference for the MO strategy corresponds to a lesser extent of increase. To this end, we fit the FLR and RS models to the simulated data generated by each strategy in the MOS model. We controlled all parameters except for the learning rate parameters across the two strategies (see Methods <xref ref-type="sec" rid="s2.4">Simulate to explain the previous learning rate effects</xref> for details). We observed that, for the EU strategy, there was an elevation in the fitted learning rate from the stable to the volatile condition (<xref ref-type="fig" rid="fig5">Fig. 5</xref> <bold>Learning rate</bold>), which mirrors the findings of faster learning curves in the volatile condition. The MO strategy displayed almost no increase. Furthermore, we also found that the increase in the learning rate was smaller for the MO strategy (<xref ref-type="fig" rid="fig5">Fig. 5</xref> <bold>Learning rate: volatile - stable</bold>), indicating that patients would display a smaller increase in the learning rate. These results suggest that strategy preferences alone can provide a natural explanation for patients’ maladaptive learning behaviors in response to environmental volatility.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5</label>
<caption><title>Fitted learning rate and the learning rate differences between the stable and volatile conditions. The simulated data are generated by the MOS model and fitted by the FLR (<italic><bold>A</bold></italic>) and the RS (<italic><bold>B</bold></italic>) model.</title></caption>
<graphic xlink:href="wymucv6_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In summary, the MOS model can effectively explain the three well-established learning curve effects in previous literature. It is important to note that, in contrast to the FLR or RS models, the apparent differences in learning curves in the MOS model originate from the weighting differences in strategy rather than learning rate <italic>per se</italic>. This means that the MOS model provides a key theoretical interpretation that differs from that in the majority of literature.</p>
</sec>
<sec id="s3.5">
<title>Model and parameter recovery analyses support model and parameter identifiability in MOS</title>
<p>It is intriguing that the MOS model can reproduce the classic learning curve effects only by adjusting strategy preferences without altering the learning rates. However, there are two potential confounding factors to consider. First, it is possible that adjusting the learning rate, rather than strategy preferences, could produce the same behavioral outcomes that are indistinguishable by the model fitting. If this holds, the MOS model might be problematic, as all learning rate differences may be automatically attributed to strategy preferences because of some unknown idiosyncratic model fitting mechanisms. Second, the fact that the MOS framework outperforms the other two frameworks may be partly due to an unknown bias in the model design. It is possible that the MOS model always wins, irrespective of how the data is generated.</p>
<p>To circumvent these issues, we performed parameter and model recovery analyses to investigate the identifiability of true parameters and models. The parameter recovery results demonstrate that the true parameters that generate synthetic datasets can be correctly estimated and identified (all Pearson’s <italic>r</italic>s &gt; 0.720), demonstrating that the effects of learning rate and weighting parameters are not interchangeable in the MOS6 model.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6</label>
<caption><title>Parameter and model recovery analyses</title>
<p><italic><bold>A.</bold></italic> Parameter recovery for the MOS6 model. Each recovered parameter is averaged over ten samples.</p>
<p><italic><bold>B-C</bold></italic>. Model recovery analyses. The MOS6 model is still the best-fitting model on synthetic data generated by MOS6 <italic>per se</italic>, according to the AIC (<italic>B</italic>) and BIC (<italic>C</italic>) comparisons. Error bars indicate the standard deviation of the mean value across 79 synthetic data points.</p></caption>
<graphic xlink:href="wymucv6_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>For model recovery, we fit all six models to the synthetic data generated by MOS6 and found MOS6, as the generative model, was still the best-fitting model based on the lowest averaged AIC and BIC (<bold>Fig. 7</bold>). Both parameter and model recovery analyses suggest that our modeling approach is reliable and the MOS6 model is distinguishable. We ensured that differences in decision preferences between patients and HC were not the result of idiosyncratic model design or fitting procedures. Remark that we excluded the NLL and PXP from the evaluation. The NLL always favors models with more parameters, while the use of PXP, which is designed for group-level comparisons, is not an appropriate metric here since we knew in advance that all data were generated from one identical model in this model recovery.</p>
</sec>
</sec>
<sec id="s4" sec-type="discussion">
<title>Discussion</title>
<p>In this article, we propose a mixture-of-strategy model assuming that human agents’ decision policy consists of three distinct components: the EU, the MO, and the HA strategies. The EU strategy is optimal in terms of maximizing reward. The MO and HA are simpler heuristic strategies that are cognitively demanding. We applied the MOS model to a public dataset and found that it outperformed existing models in capturing human behaviors. We summarized human behaviors using the estimated parameters of the target model and reported three primary conclusions. First, individuals with MDD and GAD tended to favor more irrational policies (i.e., a stronger preference for the MO strategy). Second, individual decision preferences predict the general severity of anxiety and depression. Third, decision preferences explain several learning rate phenomena that have been studied before. All conclusions suggest that a mixture of strategies provides an effective and parsimonious explanation for human learning behaviors in volatile reversal tasks.</p>
<sec id="s4.1">
<title>The attempt of decision analysis in the previous studies</title>
<p>We are not the first to examine the human decision process. Numerous previous studies have also explored this cognitive process, although they did produce particular findings.</p>
<p>The well-established finding that humans apply flexible learning rates in different experimental blocks is a successful case study of the ideal observer analysis. <xref ref-type="bibr" rid="c3">Behrens et al. (2007)</xref> constructed a hierarchical ideal Bayesian observer that dynamically models how higher-order environmental volatility influences the speed of updating the lower-order feedback probability. Because of the hierarchical interaction, the model predicts a faster updating speed for the feedback probability in a volatile environment. The ideal Bayesian model proposes an optimal manner of processing new information, like how an agent should behave. Human behavioral data was better accounted for by the RS model, which updates feedback probability in the classical Rescorla-Wagner format. Interestingly, the key prediction of the ideal Bayesian model has been preserved in the RS implementation: human subjects had a significantly higher learning rate in the volatile than the stable environment. The success of the RS model seems to suggest that humans can flexibly adjust learning rates according to environmental volatility. The view is better established as more studies replicate this learning rate effect (e.g., <xref ref-type="bibr" rid="c5">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="c14">Gagne et al., 2020</xref>).</p>
<p>Despite this, some attention has nevertheless been paid to understanding the decision process. <xref ref-type="bibr" rid="c5">Browning et al. (2015)</xref> studied the decision process of the RS9 model. They examined and compared the risk-sensitive parameter <italic>γ</italic> and inverse temperature <italic>β</italic> but found no significant difference between different degrees of trait anxiety and volatility. <xref ref-type="bibr" rid="c14">Gagne et al. (2020)</xref> constructed 13 models in a stepwise manner to find the best-fitting description of human decision-making in the volatile reversal learning task. However, the study did not attempt to connect the decision process to anxiety and depression traits, possibly due to the best-fit model, the FLR18 model implemented here, being too complex to analyze.</p>
<p>The problems in both attempts are straightforward. The RS9 model might have an inaccurate description of the human decision process, and the FLR18 model is not understandable. The MOS model developed here relieves both issues, providing a competitive fit and being constructed in an easy-to-understand form. Additionally, the MOS model also provides a parsimonious description of the behavioral data, using a set of parameters to capture the data in four experimental contexts. However, the model yields a contradictory explanation. It shows no significant difference in terms of learning rate. In other words, the apparent differences in learning curves may arise from decision processes (i.e., decision preferences) rather than learning processes. Note that we reproduced this finding using the same model in the <xref ref-type="bibr" rid="c14">Gagne et al. (2020)</xref> dataset, so this difference is not introduced by replacing the Bayesian estimation with the MAP parameter estimation. The good quantitative performance of the MOS model and the qualitative explanation of the adaptation effect without introducing a flexible learning rate seem to challenge a range of previous results.</p>
<p>We emphasize that previous results and ours may not be mutually exclusive and may coexist. We argue that the current experimental paradigm is insufficient to disassociate the two possible accounts. Although the MOS framework quantitatively wins in model comparisons, it requires examining their qualitatively distinct predictions to further differentiate the two accounts. We will discuss this issue in future directions below.</p>
</sec>
<sec id="s4.2">
<title>The normative interpretation of the mixed strategies</title>
<p>The normative interpretation of learning rate can be elusive. On one hand, the quality of a learning rate does not monotonically increase with its value. Consequently, one’s cognitive ability cannot be directly assessed based on their fitted learning rate, unless compared to the theoretically optimal learning rate. On the other hand, the optimal learning rate is highly context-dependent and can even vary from trial to trial (<xref ref-type="bibr" rid="c3">Behrens et al., 2007</xref>). This can pose challenges when assessing participants’ performance across different cognitive tasks.</p>
<p>Based on the principle of resource rationality, the MOS model demonstrates stronger normative characteristics. The model suggests that preference towards the three strategies can be used to qualitatively approximate this reward-effort tradeoff. Especially, the EU strategy is (defined to be) the most rewarding strategy (<xref ref-type="bibr" rid="c34">Von Neumann &amp; Morgenstern, 1947</xref>), but also cognitively demanding (<xref ref-type="bibr" rid="c16">Gershman et al., 2015</xref>). Hence, a higher preference for the EU strategy typically signifies better cognitive ability and capacity. Individuals with psychiatric diseases exhibit a significantly lower preference for this EU strategy compared with healthy individuals, which implies that their cognitive resources might be disrupted. The MO and HA strategies are more computationally economical, though they yield fewer rewards. It is worth noting that the patient group exhibits a greater preference for the MO strategy, which may imply mental impairments beyond limited cognitive resources. According to the resource-rationality principle (<xref ref-type="bibr" rid="c15">Gershman, 2020</xref>) and <xref ref-type="fig" rid="fig4">Fig. 4C</xref>: The HA strategy is a cost-efficient strategy that brings more rewards than the MO strategy. This may prompt further investigation into the underlying reasons behind these preferences.</p>
<p>This framework can be extended to understand human behaviors in paradigms beyond the volatile reversal task. The key lies in identifying heuristics that contrast with the EU strategy. For instance, when employing the MOS model in a volatile reversal task with fixed reward magnitude signals set to 1, we can exclude the MO strategy from the pool and preserve EU and HA. A higher preference for the EU strategy still implies better cognitive ability.</p>
</sec>
<sec id="s4.3">
<title>Atypical learning speed in psychiatric diseases</title>
<p>In the present work, we found that patients with depression and anxiety display slower learning speed in the probabilistic learning tasks (shown in <xref ref-type="fig" rid="fig4">Fig. 4A</xref>). We attributed the very observation to participants’ decision preferences. However, in conventional Rescorla-Wagner modeling, the learning speed is primarily indicated by the parameter of learning rate. For example, <xref ref-type="bibr" rid="c6">Chen (2015)</xref> conducted a systematic review of reinforcement learning in patients with depression and identified 10 out of 11 behavioral datasets showing either comparable or slower learning rates in depressive patients. Nonetheless, depressive patients may not always have a slower learning rate. In a recent meta-analysis that summarized 27 articles with 3085 participants, including 1242 with depression and/or anxiety, <xref ref-type="bibr" rid="c25">Pike and Robinson (2022)</xref> found a reduced reward but enhanced punishment learning rate. This finding yields two practical implications. First, the heterogeneous findings in the literature may arise from heterogeneous pathologies in depression and anxiety. Second, the learning rate as an indicator of human learning and decision-making is not yet perfect and needs to be revised. The mixture decision strategy model may provide useful complementary explanations of the consequences of a spectrum of symptoms.</p>
</sec>
<sec id="s4.4">
<title>Limitations and future directions</title>
<p>The MOS model provides relative context-free interpretations for some learning rate phenomena, but not all of them. One among them is the value-specific learning rate differences, where the learning rates for positive outcomes are higher than the negative ones (<xref ref-type="bibr" rid="c6">Chen et al., 2015</xref>; <xref ref-type="bibr" rid="c14">Gagne et al., 2020</xref>; <xref ref-type="bibr" rid="c25">Pike &amp; Robinson, 2022</xref>). It’s worth noting that there is no difference between value-specific learning rates, even in MOS 22, where value-specific learning rates are incorporated (<xref ref-type="sec" rid="s7">Supplementary Note 2</xref>). This suggests that at least the effect of the value-specific learning rate is modest in this dataset. Future studies may consider exploring explicit behavioral markers for value-specific learning that do not rely on specific computational models, rather than merely estimating the learning rate value from noisy behavioral data.</p>
<p>We propose an experimental paradigm that can potentially disassociate the learning rate from the mixture of strategy at behavioral level. The idea is to verify the cognitive constraints hypothesis by manipulating participants’ cognitive loads. The volatile reversal learning task can be implemented with a secondary task (e.g., asking participants to remember words through headphones). We expect to identify a preference shift from the EU strategy to the MO strategy from participants (a decreasing <italic>w</italic><sub>EU</sub> and an increasing <italic>w</italic><sub>MO</sub>) because human agents should compromise to a simpler and irrational strategy due to resource constraints induced by the secondary task. In general, we expect this line of research to include more experimental paradigms such that we can gain a complete picture of human learning behavior.</p>
<p>We will also explore why individuals with mental disorders prefer simpler strategies when making decisions. One possible explanation is that individuals with depression exhibit a maladaptive emotion regulation behavior called <italic>rumination</italic>, suffering from irresistible and persistent negative thoughts (<xref ref-type="bibr" rid="c31">Song et al., 2022</xref>; <xref ref-type="bibr" rid="c37">Yan et al., 2022</xref>). It is likely that the presence of negative thoughts consumes some cognitive resources, such that the participants fail to utilize the complicated but rewarding EU strategy.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We thank the authors of <xref ref-type="bibr" rid="c14">Gagne et al. (2020)</xref> for sharing their data. This work was supported by the National Natural Science Foundation of China (32100901), Shanghai Pujiang Program (21PJ1407800), Natural Science Foundation of Shanghai (21ZR1434700), the Research Project of Shanghai Science and Technology Commission (20dz2260300) and the Fundamental Research Funds for the Central Universities (to R.-Y.Z.)</p>
</ack>
<sec sec-type="competing-interest-statement" id="coi">
<title>Conflict of Interests</title>
<p>The authors declare no competing financial interests.</p>
</sec>
<sec id="s5">
<title>Author Contributions</title>
<p>Z. F. and R-Y.Z. conceived and designed the study. Z. F., M. Z., T.X., Y.L., H.X., and P.Q. processed the data. Z. F. implemented the computational models. Z. F., and R-Y.Z. made the first draft of the manuscript. All authors provide valuable feedback on the final manuscript.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Akaike</surname>, <given-names>H.</given-names></string-name></person-group> (<year>1974</year>). <article-title>A new look at the statistical model identification</article-title>. <source>IEEE transactions on automatic control</source>, <volume>19</volume>(<issue>6</issue>), <fpage>716</fpage>–<lpage>723</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beck</surname>, <given-names>A. T.</given-names></string-name>, <string-name><surname>Ward</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Mendelson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Mock</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Erbaugh</surname>, <given-names>J.</given-names></string-name></person-group> (<year>1961</year>). <article-title>Beck depression inventory (BDI)</article-title>. <source>Arch Gen Psychiatry</source>, <volume>4</volume>(<issue>6</issue>), <fpage>561</fpage>–<lpage>571</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Behrens</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Walton</surname>, <given-names>M. E.</given-names></string-name>, &amp; <string-name><surname>Rushworth</surname>, <given-names>M. F.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Learning the value of information in an uncertain world</article-title>. <source>Nat Neurosci</source>, <volume>10</volume>(<issue>9</issue>), <fpage>1214</fpage>–<lpage>1221</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Boyd</surname>, <given-names>S. P.</given-names></string-name>, &amp; <string-name><surname>Vandenberghe</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2004</year>). <source>Convex optimization</source>. <publisher-name>Cambridge university press</publisher-name>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Browning</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Jocham</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>O’Reilly</surname>, <given-names>J. X.</given-names></string-name>, &amp; <string-name><surname>Bishop</surname>, <given-names>S. J.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Anxious individuals have difficulty learning the causal statistics of aversive environments</article-title>. <source>Nat Neurosci</source>, <volume>18</volume>(<issue>4</issue>), <fpage>590</fpage>–<lpage>596</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Takahashi</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Nakagawa</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Inoue</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Kusumi</surname>, <given-names>I.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Reinforcement learning in depression: A review of computational research</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, <volume>55</volume>, <fpage>247</fpage>–<lpage>267</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clark</surname>, <given-names>L. A.</given-names></string-name>, &amp; <string-name><surname>Watson</surname>, <given-names>D.</given-names></string-name></person-group> (<year>1991</year>). <article-title>Tripartite model of anxiety and depression: psychometric evidence and taxonomic implications</article-title>. <source>J Abnorm Psychol</source>, <volume>100</volume>(<issue>3</issue>), <fpage>316</fpage>–<lpage>336</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>McGovern</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Dinzeo</surname>, <given-names>T. J.</given-names></string-name>, &amp; <string-name><surname>Covington</surname>, <given-names>M. A.</given-names></string-name></person-group> (<year>2014</year>). Speech deficits in serious mental illness: a cognitive resource issue? <source>Schizophr Res</source>, <volume>160</volume>(<issue>1–3</issue>), <fpage>173</fpage>–<lpage>179</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cools</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Clark</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Owen</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name><surname>Robbins</surname>, <given-names>T. W.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Defining the neural mechanisms of probabilistic reversal learning using event-related functional magnetic resonance imaging</article-title>. <source>J Neurosci</source>, <volume>22</volume>(<issue>11</issue>), <fpage>4563</fpage>–<lpage>4567</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Daw</surname>, <given-names>N. D.</given-names></string-name>, <string-name><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Seymour</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Dolan</surname>, <given-names>R. J.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Model-based influences on humans’ choices and striatal prediction errors</article-title>. <source>Neuron</source>, <volume>69</volume>(<issue>6</issue>), <fpage>1204</fpage>–<lpage>1215</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Delacre</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lakens</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Leys</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Why Psychologists Should by Default Use Welch’s t-test Instead of Student’s t-test</article-title>. <source>International Review of Social Psychology</source>, <volume>30</volume>(<issue>1</issue>).</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Eysenck</surname>, <given-names>H. J.</given-names></string-name>, &amp; <string-name><surname>Eysenck</surname>, <given-names>S. B.</given-names></string-name></person-group> (<year>1975</year>). <source>Eysenck personality questionnaire (junior &amp; adult)</source>. <publisher-name>EdITS/Educational and Industrial Testing Service</publisher-name>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fan</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name>, &amp; <string-name><surname>Phelps</surname>, <given-names>E. A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Trait somatic anxiety is associated with reduced directed exploration and underestimation of uncertainty</article-title>. <source>Nature Human Behaviour</source>, <volume>7</volume>(<issue>1</issue>), <fpage>102</fpage>–<lpage>113</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gagne</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Zika</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Bishop</surname>, <given-names>S. J.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Impaired adaptation of learning to contingency volatility in internalizing psychopathology</article-title>. <source>Elife</source>, <volume>9</volume>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Origin of perseveration in the trade-off between reward and complexity</article-title>. <source>Cognition</source>, <volume>204</volume>, <fpage>104394</fpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Horvitz</surname>, <given-names>E. J.</given-names></string-name>, &amp; <string-name><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Computational rationality: A converging paradigm for intelligence in brains, minds, and machines</article-title>. <source>Science</source>, <volume>349</volume>(<issue>6245</issue>), <fpage>273</fpage>–<lpage>278</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gilovich</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Vallone</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Tversky</surname>, <given-names>A.</given-names></string-name></person-group> (<year>1985</year>). <article-title>The hot hand in basketball: On the misperception of random sequences</article-title>. <source>Cognitive psychology</source>, <volume>17</volume>(<issue>3</issue>), <fpage>295</fpage>–<lpage>314</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Griffiths</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Lieder</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Goodman</surname>, <given-names>N. D.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Rational use of cognitive resources: levels of analysis between the computational and the algorithmic</article-title>. <source>Top Cogn Sci</source>, <volume>7</volume>(<issue>2</issue>), <fpage>217</fpage>–<lpage>229</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harvey</surname>, <given-names>P. O.</given-names></string-name>, <string-name><surname>Fossati</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Pochon</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Levy</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Lebastard</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Lehericy</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Allilaire</surname>, <given-names>J. F.</given-names></string-name>, &amp; <string-name><surname>Dubois</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Cognitive control and brain resources in major depression: an fMRI study using the n-back task</article-title>. <source>Neuroimage</source>, <volume>26</volume>(<issue>3</issue>), <fpage>860</fpage>–<lpage>869</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lawson</surname>, <given-names>R. P.</given-names></string-name>, <string-name><surname>Mathys</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Rees</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Adults with autism overestimate the volatility of the sensory environment</article-title>. <source>Nat Neurosci</source>, <volume>20</volume>(<issue>9</issue>), <fpage>1293</fpage>–<lpage>1299</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Levens</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Muhtadie</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Gotlib</surname>, <given-names>I. H.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Rumination and impaired resource allocation in depression</article-title>. <source>J Abnorm Psychol</source>, <volume>118</volume>(<issue>4</issue>), <fpage>757</fpage>–<lpage>766</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meyer</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Metzger</surname>, <given-names>R. L.</given-names></string-name>, &amp; <string-name><surname>Borkovec</surname>, <given-names>T. D.</given-names></string-name></person-group> (<year>1990</year>). <article-title>Development and validation of the Penn State Worry Questionnaire</article-title>. <source>Behav Res Ther</source>, <volume>28</volume>(<issue>6</issue>), <fpage>487</fpage>–<lpage>495</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moran</surname>, <given-names>T. P.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Anxiety and working memory capacity: A meta-analysis and narrative review</article-title>. <source>Psychol Bull</source>, <volume>142</volume>(<issue>8</issue>), <fpage>831</fpage>–<lpage>864</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pearce</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>Hall</surname>, <given-names>G.</given-names></string-name></person-group> (<year>1980</year>). <article-title>A model for Pavlovian learning: variations in the effectiveness of conditioned but not of unconditioned stimuli</article-title>. <source>Psychological review</source>, <volume>87</volume>(<issue>6</issue>), <fpage>532</fpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pike</surname>, <given-names>A. C.</given-names></string-name>, &amp; <string-name><surname>Robinson</surname>, <given-names>O. J.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Reinforcement Learning in Patients With Mood and Anxiety Disorders vs Control Individuals: A Systematic Review and Meta-analysis</article-title>. <source>JAMA psychiatry</source>, <volume>79</volume>(<issue>4</issue>), <fpage>313</fpage>–<lpage>322</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Powers</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Mathys</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Corlett</surname>, <given-names>P. R.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Pavlovian conditioning-induced hallucinations result from overweighting of perceptual priors</article-title>. <source>Science</source>, <volume>357</volume>(<issue>6351</issue>), <fpage>596</fpage>–<lpage>600</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Radloff</surname>, <given-names>L. S.</given-names></string-name></person-group> (<year>2016</year>). <article-title>The CES-D Scale</article-title>. <source>Applied psychological measurement</source>, <volume>1</volume>(<issue>3</issue>), <fpage>385</fpage>–<lpage>401</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rescorla</surname>, <given-names>R. A.</given-names></string-name></person-group> (<year>1972</year>). <article-title>A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement</article-title>. <source>Current research and theory</source>, <fpage>64</fpage>–<lpage>99</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rigoux</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Stephan</surname>, <given-names>K. E.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, &amp; <string-name><surname>Daunizeau</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Bayesian model selection for group studies—revisited</article-title>. <source>Neuroimage</source>, <volume>84</volume>, <fpage>971</fpage>–<lpage>985</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwarz</surname>, <given-names>G.</given-names></string-name></person-group> (<year>1978</year>). <article-title>Estimating the dimension of a model</article-title>. <source>The annals of statistics</source>, <fpage>461</fpage>–<lpage>464</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Song</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Long</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Lee</surname>, <given-names>T. M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>The inter-relationships of the neural basis of rumination and inhibitory control: neuroimaging-based meta-analyses</article-title>. <source>Psychoradiology</source>, <volume>2</volume>(<issue>1</issue>), <fpage>11</fpage>–<lpage>22</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spielberger CD</surname>, <given-names>G. R.</given-names></string-name>, <string-name><surname>Lushene</surname> <given-names>R</given-names></string-name>, <string-name><surname>Vagg</surname> <given-names>PR</given-names></string-name>, <string-name><surname>Jacobs</surname> <given-names>GA</given-names></string-name></person-group>. (<year>1983</year>). <article-title>Manual for the State-Trait Anxiety Inventory</article-title>. <source>Consulting Psychologists Press</source>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sutton</surname>, <given-names>R. S.</given-names></string-name>, &amp; <string-name><surname>Barto</surname>, <given-names>A. G.</given-names></string-name></person-group> (<year>2018</year>). <source>Reinforcement learning: An introduction</source>. <publisher-name>MIT press</publisher-name>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Von Neumann</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Morgenstern</surname>, <given-names>O.</given-names></string-name></person-group> (<year>1947</year>). <source>Theory of games and economic behavior</source>, <edition>2nd rev.</edition></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Watson</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Clark</surname>, <given-names>L. A.</given-names></string-name></person-group> (<year>1991</year>). <article-title>Mood and anxiety symptom questionnaire</article-title>. <source>Journal of Behavior Therapy and Experimental Psychiatry</source>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wood</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Runger</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Psychology of Habit</article-title>. <source>Annu Rev Psychol</source>, <volume>67</volume>(<issue>1</issue>), <fpage>289</fpage>–<lpage>314</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yan</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Gao</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Yuan</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Emotion regulation choice in internet addiction: less reappraisal, lower frontal alpha asymmetry</article-title>. <source>Clinical EEG and Neuroscience</source>, <volume>53</volume>(<issue>4</issue>), <fpage>278</fpage>–<lpage>286</lpage>.</mixed-citation></ref>
</ref-list>
<sec id="s6">
<title>Supplemental Information</title>
<sec id="s6.1">
<title>Supplemental Note 1: the priors for reparametrized parameters</title>
<p>We fit the models using the BFGS method, which requires us to first turn the constrained optimization problem (in terms of the parameter range) into an unconstrained one. To do so, we applied the reparameterization tricks. For example, we passed the raw parameter values through the sigmoid function <inline-formula id="i7"><alternatives><mml:math display="inline" id="im7"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mtext>raw</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math><inline-graphic xlink:href="wymucv6_ieq6.tif" mimetype="image" mime-subtype="tiff"/></alternatives></inline-formula> to create parameters with range (0, 1). For parameters with range (0, ∞), we used the exponential function <italic>ξ</italic> = exp(<italic>ξ</italic><sub>raw</sub>). The raw parameter values are all sampled from a Gaussian space.</p>
<p>We carefully tuned the raw parameter priors to ensure the parameters have a reasonable prior or a prior that is consistent with other published research in the reparametrized space (not the raw value space) (<xref ref-type="fig" rid="figS1">Fig. S1</xref>). For parameters with range (0, 1), we approximate the uniform distribution Uniform(0, 1); for parameters with range (0, ∞), we approximate Gamma(3, 3).</p>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1</label>
<caption><title>The reparametrized priors for parameters.</title>
<p><bold><italic>A.</italic></bold> For parameters with a range of (0, 1), the raw values were sampled from <italic>N</italic>(0, 1.55) and passed through the sigmoid function.</p>
<p><bold><italic>B.</italic></bold> For parameters with a range of (0, ∞), the raw values were sampled from <italic>N</italic>(2, 1) and passed through the exponential function.</p>
<p><bold><italic>C.</italic></bold> For parameters with a range (−∞, ∞), the raw values were sampled from <italic>N</italic>(0, 10).</p></caption>
<graphic xlink:href="wymucv6_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s7">
<title>Supplemental Note 2: the complete statistical results of MOS18</title>
<p>We performed multiple 2 × 2 × 2 ANOVAs with the logit of the three weight parameters, dubbed decision preferences, as the dependent variable, group (health control/patients) as a between-subject factor, volatility level (stable/volatile) and feedback types (reward/aversive) as within-subject factors.</p>
<p>For the <bold>weighting parameters for the EU strategy <italic>w</italic><sub>EU</sub></bold>, the patient group exhibited a weaker tendency for the rational EU strategy (<italic>F</italic>(1, 300) = 27.195, <italic>p</italic> &lt; 0.001, <italic>η</italic><sup>2</sup> = 0.076). People also showed a higher tendency for the EU strategy for the reward feedback than the aversive one (<italic>F</italic>(1, 300) = 5.368, <italic>p</italic> = 0.021, <italic>η</italic><sup>2</sup> = 0.016). No significant main effects of volatility level (<italic>F</italic>(1, 300) = 0.022, <italic>p</italic> = 0.926, <italic>η</italic><sup>2</sup> = 0.006) and significant interaction effects were found (all <italic>p</italic>s &gt; 0.149).</p>
<p>For the <bold>weighting parameters for the MO strategy <italic>w</italic><sub>MO</sub></bold>, the patient group exhibited a stronger tendency for the MO strategy (<italic>F</italic>(1, 300) = 10.652, <italic>p</italic> &lt; 0.001, <italic>η</italic><sup>2</sup> = 0.031). No significant main effects of volatility level (<italic>F</italic>(1, 300) = 0.537, <italic>p</italic> = 0.464, <italic>η</italic><sup>2</sup> &lt; 0.001) and feedback types (<italic>F</italic>(1, 300) = 0.431, <italic>p</italic> = 0.512, <italic>η</italic><sup>2</sup> = 0.002) as well as significant interaction effects were found (all <italic>p</italic>s &gt; 0.420).</p>
<p>For the <bold>weighting parameters for the HA strategy <italic>w</italic><sub>HA</sub></bold>, the two groups exhibited no significant differences in the preference for the HA strategy (<italic>F</italic>(1, 300) = 0.434, <italic>p</italic> = 0.511, <italic>η</italic><sup>2</sup> = 0.001). No significant main effects of volatility level (<italic>F</italic>(1, 300) = 0.872, <italic>p</italic> = 0.351, <italic>η</italic><sup>2</sup> = 0.003)
and feedback types (<italic>F</italic>(1, 300) = 1.484, <italic>p</italic> = 0.224, <italic>η</italic><sup>2</sup> = 0.004) as well as significant interaction effects were found (all <italic>p</italic>s &gt; 0.357).</p>
<p>For the <bold>log learning rates log <italic>α</italic><sub><italic>ψ</italic></sub></bold>, there were no significant main effects of participant groups (<italic>F</italic>(1, 300) = 1.489, <italic>p</italic> = 0.223, <italic>η</italic><sup>2</sup> = 0.002), feedback types (<italic>F</italic>(1, 300) = 0.002, <italic>p</italic> = 0.961, <italic>η</italic><sup>2</sup> = 0.000), and volatility levels (<italic>F</italic>(1, 300) = 1.280, <italic>p</italic> = 0.258, <italic>η</italic><sup>2</sup> = 0.002). We also examined the value-specific learning rate effect and not significant difference (<italic>F</italic>(1, 300) = 0.006, <italic>p</italic> = 0.937, <italic>η</italic><sup>2</sup> = 0.000). There is a weak significance in the interaction patients group × volatility levels × feedback types (<italic>F</italic>(1, 300) = 3.998, <italic>p</italic> = 0.046, <italic>η</italic><sup>2</sup> = 0.006). No other significant interaction effects were found (all <italic>p</italic>s &gt; 0.258).</p>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2</label>
<caption><title>Parameter analyses of MOS22.</title>
<p><bold><italic>A.</italic></bold> The weighting parameters for the healthy control participants (HC) and the patients (PAT) diagnosed with MDD and GAD. The y-axis means averaged preference over different volatility levels (volatile and stable) and feedback types (reward and aversive). Error bars reflect the standard error of the mean value across participants ×experimental conditions× feedback types.</p>
<p><bold><italic>B.</italic></bold> Decision preferences predict participants’ general factor score (<italic>g</italic> score) in the bifactor analysis reported in <xref ref-type="bibr" rid="c14">Gagne, et al. (2020)</xref>. The y-axis indicates the averaged preference over different volatility levels (volatile and stable) and feedback types (reward and aversive). This average operation is permitted here because the logit of the weight is normally distributed. The shaded areas reflect 95% confidence intervals of the regression prediction.</p></caption>
<graphic xlink:href="wymucv6_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3</label>
<caption><title>The human learning behaviors of the two groups in the reward condition.</title></caption>
<graphic xlink:href="wymucv6_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93887.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Liljeholm</surname>
<given-names>Mimi</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of California, Irvine</institution>
</institution-wrap>
<city>Irvine</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This paper provides a <bold>valuable</bold> alternative explanation for the influence of environmental volatility on learning, attributing such effects to a mixture of strategies (MoS), rather than changes in the learning rate. The authors demonstrate that the MoS model provides a superior fit to previously published data, and suggest that atypical learning in individuals with anxiety and depression might reflect their use of a suboptimal strategy. While the approach should be of interest to researchers across decision sciences, the evidence is <bold>incomplete</bold>, limiting its potential impact.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93887.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
This paper describes a reanalysis of data collected by Gagne et al. (2020), who investigated how human choice behaviour differs in response to changes in environmental volatility. Several studies to date have demonstrated that individuals appear to increase their learning rate in response to greater volatility and that this adjustment is reduced amongst individuals with anxiety and depression. The present authors challenge this view and instead describe a novel Mixture of Strategies (MOS) model, that attributes individual differences in choice behaviour to different weightings of three distinct decision-making strategies. They demonstrate that the MOS model provides a superior fit to the data and that the previously observed differences between patients and healthy controls may be explained by patients opting for a less cognitively demanding, but suboptimal, strategy.</p>
<p>Strengths:</p>
<p>
The authors compare several models (including the original winning model in Gagne et al., 2020) that could feasibly fit the data. These are clearly described and are evaluated using a range of model diagnostics. The proposed MOS model appears to provide a superior fit across several tests.</p>
<p>The MOS model output is easy to interpret and has good face validity. This allows for the generation of clear, testable, hypotheses, and the authors have suggested several lines of potential research based on this.</p>
<p>Weaknesses:</p>
<p>
The authors justify this reanalysis by arguing that learning rate adjustment (which has previously been used to explain choice behaviour on volatility tasks) is likely to be too computationally expensive and therefore unfeasible. It is unclear how to determine how &quot;expensive&quot; learning rate adjustment is, and how this compares to the proposed MOS model (which also includes learning rate parameters), which combines estimates across three distinct decision-making strategies.</p>
<p>As highlighted by the authors, the model is limited in its explanation of previously observed learning differences based on outcome value. It's currently unclear why there would be a change in learning across positive/negative outcome contexts, based on strategy choice alone.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93887.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
Previous research shows that humans tend to adjust learning in environments where stimulus-outcome contingencies become more volatile. This learning rate adaptation is impaired in some psychiatric disorders, such as depression and anxiety. In this study, the authors reanalyze previously published data on a reversal-learning task with two volatility levels. Through a new model, they provide some evidence for an alternative explanation whereby the learning rate adaptation is driven by different decision-making strategies and not learning deficits. In particular, they propose that adjusting learning can be explained by deviations from the optimal decision-making strategy (based on maximizing expected utility) due to response stickiness or focus on reward magnitude. Furthermore, a factor related to the general psychopathology of individuals with anxiety and depression negatively correlated with the weight on the optimal strategy and response stickiness, while it correlated positively with the magnitude strategy (a strategy that ignores the probability of outcome).</p>
<p>Strengths:</p>
<p>
The main strength of the study is a novel and interesting explanation of an otherwise well-established finding in human reinforcement learning. This proposal is supported by rigorously conducted parameter retrieval and the comparison of the novel model to a wide range of previously published models.</p>
<p>Weaknesses:</p>
<p>
My main concern is that the winning model (MOS6) does not have an error term (inverse temperature parameter beta is fixed to 8.804).</p>
<p>1. It is not clear why the beta is not estimated and how were the values presented here chosen. It is reported as being an average value but it is not clear from which parameter estimation. Furthermore, with an average value for participants that would have lower values of inverse temperature (more stochastic behaviour) the model is likely overfitting.</p>
<p>2. In the absence of a noise parameter, the model will have to classify behaviour that is not explained by the optimal strategy (where participants simply did not pay attention or were not motivated) as being due to one of the other two strategies.</p>
<p>3. A model comparison among models with inverse temperature and variable subsets of the three strategies (EU + MO, EU + HA) would be interesting to see. Similarly, comparison of the MOS6 model to other models where the inverse temperature parameter is fixed to 8.804).</p>
<p>This is an important limitation because the same simulation as with the MOS model in Figure 3b can be achieved by a more parsimonious (but less interesting) manipulation of the inverse temperature parameter.</p>
<p>Furthermore, the claim that the EU represents an optimal strategy is a bit overstated. The EU strategy is the only one of the three that assumes participants learn about the stimulus-outcomes contingencies. Higher EU strategy utilisation will include participants that are more optimal (in maximum utility maximisation terms), but also those that just learned better and completely ignored the reward magnitude.</p>
<p>Other minor issues that I have are the following:</p>
<p>
The mixture strategies model is an interesting proposal, but seems to be a very convoluted way to ask: to what degree are decisions of subjects affected by reward, what they've learned, and response stickiness? It seems to me that the same set of questions could be addressed with a simpler model that would define choice decisions through a softmax with a linear combination of the difference in rewards, the difference in probabilities, and a stickiness parameter.</p>
<p>Learning rate adaptation was also shown with tasks where decision-making strategies play a less important role, such as the Predictive Inference task (see for instance Nassar et al, 2010). When discussing the merit of the findings of this study on learning rate adaptation across volatility blocks, this work would be essential to mention.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93887.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
This paper presents a new formulation of a computational model of adaptive learning amid environmental volatility. Using a behavioral paradigm and data set made available by the authors of an earlier publication (Gagne et al., 2020), the new model is found to fit the data well. The model's structure consists of three weighted controllers that influence decisions on the basis of (1) expected utility, (2) potential outcome magnitude, and (3) habit. The model offers an interpretation of psychopathology-related individual differences in decision-making behavior in terms of differences in the relative weighting of the three controllers.</p>
<p>Strengths:</p>
<p>
The newly proposed &quot;mixture of strategies&quot; (MOS) model is evaluated relative to the model presented in the original paper by Gagne et al., 2020 (here called the &quot;flexible learning rate&quot; or FLR model) and two other models. Appropriate and sophisticated methods are used for developing, parameterizing, fitting, and assessing the MOS model, and the MOS model performs well on multiple goodness-of-fit indices. The parameters of the model show decent recoverability and offer a novel interpretation for psychopathology-related individual differences. Most remarkably, the model seems to be able to account for apparent differences in behavioral learning rates between high-volatility and low-volatility conditions even with no true condition-dependent change in the parameters of its learning/decision processes. This finding calls into question a class of existing models that attribute behavioral adaptation to adaptive learning rates.</p>
<p>Weaknesses:</p>
<p>
1. Some aspects of the paper, especially in the methods section, lacked clarity or seemed to assume context that had not been presented. I found it necessary to set the paper down and read Gagne et al., 2020 in order to understand it properly.</p>
<p>2. There is little examination of why the MOS model does so well in terms of model fit indices. What features of the data is it doing a better job of capturing? One thing that makes this puzzling is that the MOS and FLR models seem to have most of the same qualitative components: the FLR model has parameters for additive weighting of magnitude relative to probability (akin to the MOS model's magnitude-only strategy weight) and for an autocorrelative choice kernel (akin to the MOS model's habit strategy weight). So it's not self-evident where the MOS model's advantage is coming from.</p>
<p>3. One of the paper's potentially most noteworthy findings (Figure 5) is that when the FLR model is fit to synthetic data generated by the expected utility (EU) controller with a fixed learning rate, it recovers a spurious difference in learning rate between the volatile and stable environments. Although this is potentially a significant finding, its interpretation seems uncertain for several reasons:</p>
<p>- According to the relevant methods text, the result is based on a simulation of only 5 task blocks for each strategy. It would be better to repeat the simulation and recovery multiple times so that a confidence interval or error bar can be estimated and added to the figure.</p>
<p>- It makes sense that learning rates recovered for the magnitude-oriented (MO) strategy are near zero, since behavior simulated by that strategy would have no reason to show any evidence of learning. But this makes it perplexing why the MO learning rate in the volatile condition is slightly positive and slightly greater than in the stable condition.</p>
<p>- The pure-EU and pure-MO strategies are interpreted as being analogous to the healthy control group and the patient group, respectively. However, the actual difference in estimated EU/MO weighting between the two participant groups was much more moderate. It's unclear whether the same result would be obtained for a more empirically plausible difference in EU/MO weighting.</p>
<p>- The fits of the FLR model to the simulated data &quot;controlled all parameters except for the learning rate parameters across the two strategies&quot; (line 522). If this means that no parameters except learning rate were allowed to differ between the fits to the pure-EU and pure-MO synthetic data sets, the models would have been prevented from fitting the difference in terms of the relative weighting of probability and magnitude, which better corresponds to the true difference between the two strategies. This could have interfered with the estimation of other parameters, such as learning rate.</p>
<p>- If, after addressing all of the above, the FLR model really does recover a spurious difference in learning rate between stable and volatile blocks, it would be worth more examination of why this is happening. For example, is it because there are more opportunities to observe learning in those blocks?</p>
<p>4. Figure 4C shows that the habit-only strategy is able to learn and adapt to changing contingencies, and some of the interpretive discussion emphasizes this. (For instance, line 651 says the habit strategy brings more rewards than the MO strategy.) However, the habit strategy doesn't seem to have any mechanism for learning from outcome feedback. It seems unlikely it would perform better than chance if it were the sole driver of behavior. Is it succeeding in this example because it is learning from previous decisions made by the EU strategy, or perhaps from decisions in the empirical data?</p>
<p>5. For the model recovery analysis (line 567), the stated purpose is to rule out the possibility that the MOS model always wins (line 552), but the only result presented is one in which the MOS model wins. To assess whether the MOS and FLR models can be differentiated, it seems necessary also to show model recovery results for synthetic data generated by the FLR model.</p>
<p>6. To the best of my understanding, the MOS model seems to implement valence-specific learning rates in a qualitatively different way from how they were implemented in Gagne et al., 2020, and other previous literature. Line 246 says there were separate learning rates for upward and downward updates to the outcome probability. That's different from using two learning rates for &quot;better&quot;- and &quot;worse&quot;-than-expected outcomes, which will depend on both the direction of the update and the valence of the outcome (reward or shock). Might this relate to why no evidence for valence-specific learning rates was found even though the original authors found such evidence in the same data set?</p>
<p>7. The discussion (line 649) foregrounds the finding of greater &quot;magnitude-only&quot; weights with greater &quot;general factor&quot; psychopathology scores, concluding it reflects a shift toward simplifying heuristics. However, the picture might not be so straightforward because &quot;habit&quot; weights, which also reflect a simplifying heuristic, correlated negatively with the psychopathology scores.</p>
</body>
</sub-article>
</article>