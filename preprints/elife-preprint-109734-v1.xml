<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">109734</article-id>
<article-id pub-id-type="doi">10.7554/eLife.109734</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.109734.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Involuntary feedback responses reflect a representation of partner actions</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Sullivan</surname>
<given-names>Seth R</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>sethsullivan99@gmail.com</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Buggeln</surname>
<given-names>John H</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Calalo</surname>
<given-names>Jan A</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ngo</surname>
<given-names>Truc T</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2812-7789</contrib-id>
<name>
<surname>Semrau</surname>
<given-names>Jennifer A</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Carter</surname>
<given-names>Michael J</given-names>
</name>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<name>
<surname>Cashaback</surname>
<given-names>Joshua GA</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
<email>cashabackjga@gmail.com</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sbq1a82</institution-id><institution>Department of Biomedical Engineering, University of Delaware</institution></institution-wrap>, <city>Newark</city>, <country country="US">United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sbq1a82</institution-id><institution>Department of Mechanical Engineering, University of Delaware</institution></institution-wrap>, <city>Newark</city>, <country country="US">United States</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sbq1a82</institution-id><institution>Biomechanics and Movement Science Program, University of Delaware</institution></institution-wrap>, <city>Newark</city>, <country country="US">United States</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sbq1a82</institution-id><institution>Department of Kinesiology &amp; Applied Physiology, University of Delaware</institution></institution-wrap>, <city>Newark</city>, <country country="US">United States</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sbq1a82</institution-id><institution>Interdisciplinary Neuroscience Graduate Program, University of Delaware</institution></institution-wrap>, <city>Newark</city>, <country country="US">United States</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02fa3aq29</institution-id><institution>Department of Kinesiology, McMaster University</institution></institution-wrap>, <city>Hamilton</city>, <country country="CA">Canada</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Wei</surname>
<given-names>Kunlin</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>King</surname>
<given-names>Andrew J</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5180-7179</contrib-id><role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>†</label><p>co-senior authors</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2026-01-22">
<day>22</day>
<month>01</month>
<year>2026</year>
</pub-date>
<volume>15</volume>
<elocation-id>RP109734</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-11-11">
<day>11</day>
<month>11</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-10-24">
<day>24</day>
<month>10</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.07.29.667240"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2026, Sullivan et al</copyright-statement>
<copyright-year>2026</copyright-year>
<copyright-holder>Sullivan et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-109734-v1.pdf"/>
<abstract>
<p>We have a remarkable ability to seamlessly and rapidly coordinate actions with others, from double dutch to dancing. Humans use high-level partner representations to jointly control voluntary actions, while other work shows lower-level involuntary feedback responses to sudden visual perturbations. Yet, it is unknown if a high-level partner representation can be rapidly expressed through lower-level involuntary sensorimotor circuitry. Here we test the idea that a partner representation influences involuntary visuomotor feedback responses during a cooperative sensorimotor task. Using two experiments and dynamic game theory predictions, we show that involuntary visuomotor feedback responses reflect a partner representation and consideration of a partner’s movement cost (i.e., accuracy and energy). Collectively, our results suggest there is top-down modulation from high-level partner representations to lower-level sensorimotor circuits, enabling fast and flexible feedback responses during jointly coordinated actions.</p>
</abstract>
<abstract abstract-type="summary">
<title>Significance Statement</title>
<p>Humans have an adept ability to rapidly coordinate their movements with others. Yet it is unknown how fast the sensorimotor system can use a representation of others to jointly control movement. Remarkably, ‘intelligent reflexes’ (i.e., involuntary visuomotor feedback responses) consider high-level partner representations within 180 - 230 ms. Further, these involuntary visuomotor feedback responses show that the sensorimotor system is willing sacrifice energy to help a partner.</p></abstract>
<funding-group>
<award-group id="par-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id>
<institution>National Science Foundation (NSF)</institution>
</institution-wrap>
</funding-source>
<award-id>2146888</award-id>
<principal-award-recipient>
<name>
<surname>Cashaback</surname>
<given-names>Josh</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-2">
<funding-source>
<institution-wrap>
<institution>National Sciences and Engineering Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>RGPIN-2018- 05589</award-id>
<principal-award-recipient>
<name>
<surname>Carter</surname>
<given-names>Michael J</given-names>
</name>
</principal-award-recipient>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Moved trajectories to supplementary. Figure 2 revised.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>To successfully coordinate voluntary actions, humans form a representation of others to consider their partner’s goals<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c3">3</xref></sup> and movement costs (i.e., accuracy and energy).<sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c5">5</xref></sup> Other work has shown that the sensorimotor system modulates involuntary feedback responses based on the structure of the individual’s own goal.<sup><xref ref-type="bibr" rid="c6">6</xref></sup> Yet it is unknown if the sensorimotor system uses a partner representation to tune these rapid and involuntary feedback responses. Investigating the influence of high-level partner representations on lower-level involuntary sensorimotor responses is crucial to understanding how humans achieve coordinated interactions during rapid movements.</p>
<p>A representation of a partner to consider both their goals and costs has been shown to influence voluntary movements. Behavioural work examining human-human sensorimotor interactions has suggested that a partner representation influences reaction time,<sup><xref ref-type="bibr" rid="c7">7</xref></sup> action planning,<sup><xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c5">5</xref></sup> and reaching movements.<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c10">10</xref></sup> While work with a single individual has shown that humans minimize a self movement cost,<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c14">14</xref></sup> work with multiple individuals suggests that humans will select voluntary actions that minimize a joint cost that considers both self and a partner.<sup><xref ref-type="bibr" rid="c5">5</xref></sup> While these past works have broadened our understanding of voluntary coordinated actions, it remains unknown if a high-level partner representation and consideration of the partner’s cost can influence lower-level involuntary sensorimotor feedback responses.</p>
<p>Elegant work has shown that the sensorimotor system has a remarkable ability to generate rapid and involuntary feedback responses—prior to voluntary control—that are tuned by task dynamics and goals.<sup><xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup> Nashed and colleagues (2012) had participants reach to either a narrow target (task-relevant) or wide target (task-irrelevant).<sup><xref ref-type="bibr" rid="c6">6</xref></sup> The narrow target was task-relevant since participants needed to correct for lateral deviations to successfully hit the target. The wide target was task-irrelevant since participants did not need to correct for lateral deviations to hit their target. As early as 70 ms following a mechanical perturbation, they found greater muscular feedback responses when reaching to a narrow task-relevant target compared to a wide task-irrelevant target. Likewise, pioneering work by Franklin and Wolpert (2008) demonstrated that sensorimotor circuits also generate involuntary feedback responses to visual perturbations between 180 - 230 ms.<sup><xref ref-type="bibr" rid="c17">17</xref></sup> To measure involuntary feedback responses, they laterally constrained a participant’s hand within a rigid force channel and recorded the lateral hand force in response to a lateral cursor jump. They found that these rapid and involuntary visuomotor feedback responses are also tuned according to relevant and irrelevant task demands. While considerable work has examined visuomotor feedback responses of a human acting alone, it is unknown whether the sensorimotor system uses a partner representation to tune involuntary visuomotor feedback responses.</p>
<p>Across two experiments, we tested the overarching idea that a high-level partner representation influences lower-level involuntary sensorimotor circuits. Human pairs were required to move a jointly controlled cursor into their own target. We manipulated the width of both participant targets to be either task-relevant or task-irrelevant. We measured visuomotor feedback responses following either a cursor (Experiment 1) or target jump (Experiment 2). We made <italic>a priori</italic> predictions using four unique dynamic game theory models. Each of these models tested a specific hypothesis on whether visuomotor feedback responses reflect: i) a partner representation and, if so, ii) a weighting of the partner cost. Collectively, our empirical and computational work provides novel insights into how humans rapidly control their actions with others.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Experimental Design</title>
<p>In Experiment 1 (n = 48) and Experiment 2 (n = 48), participants completed a joint reaching task with a partner (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>). Participants had vision of their own cursor, a partner’s cursor, and a center cursor. The center cursor was at the midpoint of their own cursor and their partner’s cursor. They also viewed their own self target and their partner’s target on their screen. Participants were instructed to move and stabilize the center cursor in the self target within a time constraint. Participants received the message ‘Good’, ‘Too Slow’, or ‘Too Fast’ if they stabilized within their self target between 1400 - 1600 ms, &gt; 1600 ms, or &lt;1400 ms respectively. They were explicitly informed that their success in the task was determined by moving and stabilizing the center cursor only within the self target. Therefore, the instructions and timing constraints did not enforce participants to work together.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Experimental Design.</title>
<p><bold>A)</bold> In both experiments, each participant in the pair grasped the handle of a robotic manipulandum and made reaching movements in the horizontal plane. An LCD projected images (start position, targets, cursors) onto a semi-silvered mirror. Each trial began with each participant’s hand (dark grey circle) within their respective start position (white circle). After a short and random time delay, the self target appeared as a filled dark grey rectangle and the partner target appeared as an unfilled light grey rectangle. Simultaneously, the center cursor (green circle) and partner cursor (light grey circle) also appeared on the screen. After a constant time delay of 500 ms, participants heard a tone that cued them to begin their reach. Participants were instructed to move the center cursor into their own target. Each participant received independent feedback once the center cursor was stabilized within their own target. B) Experimental Conditions. We manipulated the width of both the self and partner targets to be either narrow (task-relevant) or wide (task-irrelevant). The narrow target is task-relevant since participants would need to correct for lateral deviations to successfully complete their task. The wide target is taskirrelevant since participants do not need to correct for lateral deviations to successfully complete their task. Human pairs performed four blocked experimental conditions: i) <italic>partner-irrelevant/self-irrelevant</italic> ii) <italic>partner-relevant/self-irrelevant</italic> iii) <italic>partner-irrelevant/self-relevant</italic> iv) <italic>partner-relevant/self-relevant</italic>. <bold>C-D) Perturbation Trials</bold>. On a subset of trials, the center cursor in (<bold>C</bold>) Experiment 1 or both targets in (<bold>D</bold>) Experiment 2 jumped 3 cm laterally to the left or right. <bold>E-F) Visuomotor Probe Trials</bold>. On a subset of trials, the center cursor in Experiment 1 (<bold>E</bold>) or both targets in Experiment 2 (<bold>F</bold>) jumped 3 cm laterally for 225 ms, then jumped 3 cm back to the original lateral position. During these probe trials, the hand of both participants in the pair was constrained to a force channel. Here we measured each participant’s visuomotor feedback responses as the force (N) they applied to the wall of the stiff force channel.</p></caption>
<graphic xlink:href="667240v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We manipulated the width of both the self and partner target (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>) to be either narrow (task-relevant) or wide (task-irrelevant). The narrow target is task-relevant since participants would need to correct for lateral deviations to hit their target. The wide target is task-irrelevant since participants do not need to correct for lateral deviations to hit their target. Thus, we used a 2 (Partner Irrelevant or Partner Relevant) x 2 (Self Irrelevant or Self Relevant) repeated measures experimental design with four blocked experimental conditions: i) <italic>partner-irrelevant/self-irrelevant</italic>, ii) <italic>partner-relevant/self-irrelevant</italic>, iii) <italic>partner-irrelevant/self-relevant</italic>, and iv) <italic>partner-relevant/self-relevant</italic>.</p>
<p>The goal of Experiment 1 and Experiment 2 was to determine if a representation of a partner and consideration of their costs influences involuntary visuomotor feedback responses. To address this goal, we had participants perform non-perturbation trials, perturbation trials, and probe trials in each experimental condition. In both the non-perturbation trials and perturbation trials, participants reached freely in the lateral and forward dimensions. However, in perturbation trials (<xref rid="fig1" ref-type="fig">Fig. 1C-D</xref>) either the center cursor (Experiment 1) or both targets (Experiment 2) jumped 3 cm to the right or left when the center cursor moved 25% of the forward distance to the targets. In probe trials (<xref rid="fig1" ref-type="fig">Fig. 1E-F</xref>), both participants were constrained by a force channel and could only move along the forward dimension. Here they experienced the cursor or target jump for 250 ms before returning to the original lateral position. Critically, as a metric of visuomotor feedback responses, we measured the lateral force participants applied against the channel in response to center cursor or target jumps.</p>
</sec>
<sec id="s2b">
<title>Dynamic Game Theory Model</title>
<p>We generated <italic>a priori</italic> predictions of hand trajectories and visuomotor feedback responses for each of the experimental conditions using a dynamic game theory model (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>). We modelled our task as a linear quadratic game of the form</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Control Model Framework and Hypotheses.</title>
<p><bold>A) Control Model</bold>. Human pairs were modelled as controllers within a dynamic game theory framework. Here we depict the feedback control loop from the perspective of one participant (i.e. the self). The self and partner control policy each generate a motor command to produce jointly controlled movement. An efference copy of the motor command passes through an internal model (representation of dynamics) to generate predicted states. Each controller also receives noisy and delayed sensory feedback on the states (e.g., position of the self and partner hand, center cursor, and self and partner targets). Both the self and partner controllers have a state estimator that combines the predicted state and sensory feedback in a statistically optimal manner to produce estimated states. The estimated states are used by the control policy to generate motor commands on each time step. <bold>B) Hypotheses</bold>. The dynamic game theory framework allowed us to test four distinct hypotheses. The hypotheses test whether the control policy: i) has a representation of a partner, and ii) considers only a self cost or joint (self + partner) cost of accuracy and energy. <bold>No Partner Representation &amp; Self Cost Hypothesis:</bold> The sensorimotor system has a control policy that does not use a representation of a partner, and only considers a self cost. <bold>Partner Representation &amp; Self Cost Hypothesis:</bold> The sensorimotor system has a control policy that uses a representation of a partner, but only considers a self cost. <bold>Partner Representation &amp; Equal Joint Cost Hypothesis:</bold> The sensorimotor system has a control policy that uses a representation of a partner, and equally considers both a self cost and partner cost (i.e., equal joint cost). <bold>Partner Representation &amp; Weighted Joint Cost Hypothesis:</bold> The sensorimotor system has a control policy that uses a representation of a partner, and that weights the self cost greater than the partner cost (i.e., weighted joint cost). Each of the four hypotheses generate unique predictions of human hand movement (<xref ref-type="fig" rid="fig3">Fig. 3A-P</xref>) and visuomotor feedback responses (<xref ref-type="fig" rid="fig4">Fig. 4</xref>).</p></caption>
<graphic xlink:href="667240v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>
<disp-formula id="eqn1">
<graphic xlink:href="667240v3_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<italic>x</italic><sub><italic>k</italic></sub> is the state (e.g., position) of the system at time step <italic>k, A</italic> represents the task dynamics, <italic>u</italic><sub>1</sub> and <italic>u</italic><sub>2</sub> are the control signals, and <italic>B</italic><sub>1</sub> and <italic>B</italic><sub>2</sub> converts the control signals to a force that produces movement. Here the subscripts 1 and 2 respectively refers to controller 1 and 2, representing a pair of participants in our task. Throughout, we describe the model with controller 1 as the self and controller 2 as the partner.</p>
<p>Controller 1 and 2 select their own control signal <inline-formula id="inline-eqn-1"><inline-graphic xlink:href="667240v3_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula> or <inline-formula id="inline-eqn-2"><inline-graphic xlink:href="667240v3_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, which considers their respective costs. We can define individual cost functions <italic>J</italic><sub>1</sub> and <italic>J</italic><sub>2</sub> as:
<disp-formula id="eqn2">
<graphic xlink:href="667240v3_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn3">
<graphic xlink:href="667240v3_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here <italic>J</italic><sub>1</sub> is the individual cost for controller 1 (e.g., self) and <italic>J</italic><sub>2</sub> is the individual cost for controller 2 (e.g., partner). <italic>N</italic> is the final step, which represents the end of a trial. The term <italic>Q</italic> penalizes deviations of the center cursor relative to each target.</p>
<p>Depending on the experimental condition, we modelled i) a task-relevant target using a higher value of <italic>Q</italic>, and ii) a task-irrelevant target using a lower value of <italic>Q</italic>. The term <italic>R</italic> penalizes the control signal (<italic>u</italic>), which would relate to an energetic cost. Further, we define a joint cost function as:
<disp-formula id="eqn4">
<graphic xlink:href="667240v3_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5">
<graphic xlink:href="667240v3_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>α</italic><sub><italic>i</italic></sub> ∈ [0, 1] determines the degree to which controller <italic>i</italic> considers their partner’s cost function.</p>
<p>The optimal control signal for controller <inline-formula id="inline-eqn-3"><inline-graphic xlink:href="667240v3_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and controller <inline-formula id="inline-eqn-4"><inline-graphic xlink:href="667240v3_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is determined by the time-varying feedback gains <italic>F</italic><sub>1</sub> and <italic>F</italic><sub>2</sub> that minimize the joint cost function <inline-formula id="inline-eqn-5"><inline-graphic xlink:href="667240v3_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula id="inline-eqn-6"><inline-graphic xlink:href="667240v3_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula> respectively:
<disp-formula id="eqn6">
<graphic xlink:href="667240v3_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn7">
<graphic xlink:href="667240v3_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here, <inline-formula id="inline-eqn-7"><inline-graphic xlink:href="667240v3_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for <italic>i</italic> = {1, 2} is controller <italic>i</italic>’s posterior estimate of the state (see Methods). The feedback gains <italic>F</italic><sub>1</sub> and <italic>F</italic><sub>2</sub> constitute a Nash equilibrium solution to the linear quadratic game defined by <xref ref-type="disp-formula" rid="eqn1">eqs. 1</xref>-<xref ref-type="disp-formula" rid="eqn5">5</xref>. Throughout, the feedback gains determine hand movement and visuomotor feedback responses. The Nash equilibrium solution <italic>F</italic><sub>1</sub> that minimizes <inline-formula id="inline-eqn-8"><inline-graphic xlink:href="667240v3_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula> can utilize knowledge of the partner’s control policy <italic>F</italic><sub>2</sub> through the coupled algebraic Riccati equations (see Supplementary C).</p>
</sec>
<sec id="s2c">
<title>Modelling partner representation</title>
<p>A partner representation is defined as knowledge of the partner’s control policy <italic>F</italic><sub>2</sub>. That is, a person accounts for their partner’s actions. No partner representation would reflect the case where <italic>F</italic><sub>1</sub> is selected under the assumption that <italic>F</italic><sub>2</sub> = 0. More simply, a person does not account for their partner’s actions.</p>
</sec>
<sec id="s2d">
<title>Modelling self and partner cost</title>
<p>We also modelled the degree to which a person considers their self cost, or some joint cost of both self and partner. In <xref ref-type="disp-formula" rid="eqn19">Eq. 19</xref>, <italic>α</italic><sub>1</sub> determines the degree to which controller 1 considers its partner’s cost. <italic>α</italic><sub>1</sub> = 0 reflects only a self cost, which would imply a person does not consider their partner’s cost. Conversely, <italic>α</italic><sub>1</sub> = 1 reflects an equal joint cost that would imply a person considers their self cost and partner cost equally. Finally, <italic>α</italic><sub>1</sub> = 0.5 reflects a higher weighting on the self cost than the partner cost, implying that a person primarily considers their own cost and to a lesser extent their partner’s cost.</p>
<p>Through our computational framework, we considered four alternative hypotheses, each testing how a partner representation and consideration of a partner’s cost influences sensorimotor behaviour (<xref rid="fig2" ref-type="fig">Fig. 2B-E</xref>): i) No Partner Representation &amp; Self Cost, ii) Partner Representation &amp; Self Cost, iii) Partner Representation &amp; Equal Joint Cost, iv) Partner Representation &amp; Weighted Joint Cost. For each experimental condition, we used these four models to make <italic>a priori</italic> predictions of reaching trajectories (<xref rid="fig3" ref-type="fig">Fig. 3</xref>) and visuomotor feedback responses (<xref rid="fig4" ref-type="fig">Fig. 4</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Experiment 1 Hand and Center Cursor Trajectories.</title>
<p>Collectively, the self cursor in models with only a self cost do not laterally deviate to correct for the cursor jump in the <italic>partner-relevant/self-irrelevant</italic> condition. In contrast, the self cursor in models that consider a self and partner cost laterally deviate to correct for the cursor jump in the <italic>partner-relevant/self-irrelevant</italic> condition. <bold>A-D)</bold> Individual hand and center cursor positions of an exemplar pair for each condition in Experiment 1. Thin traces represent each trial. Thick traces represent the average across trials for the human pair. <bold>E-H)</bold> Group average hand and center cursor positions in Experiment 1. Traces represent the mean and shaded regions reflect <italic>±</italic>1 standard error of the mean.</p></caption>
<graphic xlink:href="667240v3_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Model Visuomotor Feedback Responses.</title>
<p>Model predictions of visuomotor feedback responses (y-axis) over the time from probe onset (x-axis) for each condition considering the <bold>(A)</bold> No Partner Representation &amp; Self Cost, <bold>(B)</bold> Partner Representation &amp; Self Cost, <bold>(C)</bold> Partner Representation &amp; Equal Joint Cost, and <bold>(D)</bold> Partner Representation &amp; Weighted Joint Cost models. Solid lines reflect the average visuomotor feedback response to probe trials and shaded error bars reflect <italic>±</italic>1 standard deviation of the mean. The inset axis shows the mean visuomotor feedback response between (180 - 230 ms), which aligns with the involuntary time epoch.<sup><xref ref-type="bibr" rid="c17">17</xref></sup> Across the different models, a greater visuomotor feedback response in the <italic>partner-relevant/self-irrelevant</italic> condition compared to <italic>partner-irrelevant/self-irrelevant</italic> condition implies that there is a partner representation and a consideration of the partner’s cost. Likewise, a lower feedback response in the <italic>partner-relevant/self-relevant</italic> condition relative to the <italic>partner-irrelevant/self-relevant</italic> condition would indicate a partner representation, as well as a higher weighting of the self cost compared to the partner cost.</p></caption>
<graphic xlink:href="667240v3_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<sec id="s2d1">
<title>Hand and Center Cursor Trajectories</title>
<p>An exemplar pair (<xref rid="fig3" ref-type="fig">Fig. 3A-D</xref>) and group average (<xref rid="fig3" ref-type="fig">Fig. 3E-H</xref>) hand and center cursor trajectories are shown for each experimental condition in Experiment 1. Note that while both participants in the pair began each trial to the right of the center cursor (see <xref rid="fig1" ref-type="fig">Fig. 1A</xref>), we refer to one of the participants as the ‘self’ and the other participant as the ‘partner’ (see Methods for details). In the <italic>partner-irrelevant/self-irrelevant</italic> condition (<xref rid="fig3" ref-type="fig">Fig. 3A,E</xref>), neither participant laterally deviated to correct for the cursor jump since both targets were irrelevant. In the <italic>partner-relevant/self-irrelevant</italic> condition (<xref rid="fig3" ref-type="fig">Fig. 3B,F</xref>), the self cursor laterally deviated less than the partner cursor. In the <italic>partner-irrelevant/self-relevant</italic> condition (<xref rid="fig3" ref-type="fig">Fig. 3C,G</xref>), the self cursor laterally deviated more than the partner cursor. Finally, in the <italic>partner-relevant/self-relevant</italic> condition (<xref rid="fig3" ref-type="fig">Fig. 3D,H</xref>), both the self and partner cursor had a similar amount of lateral deviation.</p>
<p>The group average trajectories in both Experiment 1, Experiment 2 (see Supplementary A), and final lateral hand deviation (see Supplementary B) aligned closest with the Partner Representation &amp; Weighted Joint Cost model (see <xref ref-type="fig" rid="figS1">Supplementary Fig. S1M-P</xref>). Together, the model predictions and empirical hand trajectories support the notion that voluntary sensorimotor control reflects a partner representation and a consideration of the partner’s cost.</p>
</sec>
<sec id="s2d2">
<title>Visuomotor Feedback Responses</title>
<p>In these experiments, we were primarily interested in the involuntary feedback responses to visual probes. We modeled these visuomotor feedback responses computationally and measured them experimentally using cursor and target jumps.</p>
</sec>
</sec>
<sec id="s2e">
<title>Model Visuomotor Feedback Responses</title>
<p>We also simulated probe trials by constraining the models to a force channel and calculating the force the models produce in response to the cursor jump (see Methods: Dynamic Game Theory Model). For each model and condition, <xref rid="fig4" ref-type="fig">Fig. 4</xref> shows the visuomotor feedback responses over time in response to cursor jump probe trials. The inset within each of the subplots displays the average visuomotor response between 180-230 ms, which aligns with the involuntary time epoch.<sup><xref ref-type="bibr" rid="c17">17</xref></sup></p>
<p>Models that only consider the self cost predict no change in visuomotor feedback responses between the <italic>partner-relevant/self-irrelevant</italic> and <italic>partner-irrelevant/self-irrelevant</italic> condition (<xref rid="fig4" ref-type="fig">Fig. 4A,B</xref>). Thus, models that only consider a self cost do not help their partner achieve their goal. Conversely, models that consider both a self and partner cost predict a greater visuomotor feedback response in the <italic>partner-relevant/self-irrelevant</italic> condition compared to the <italic>partner-irrelevant/self-irrelevant</italic> condition (<xref rid="fig4" ref-type="fig">Fig. 4C,D</xref>). That is, models that consider a joint cost attempt to help a partner achieve their goal. If the involuntary sensorimotor circuits leverage a partner representation and consideration of partner costs, we would expect to see an increased visuomotor feedback response in the <italic>partner-relevant/self-irrelevant</italic> condition compared to the <italic>partner-irrelevant/selfirrelevant</italic> condition.</p>
<p>If there is a partner representation, there are different visuomotor feedback response predictions when the self controller has an equal joint cost versus a weighted joint cost. In the Partner Representation &amp; Equal Joint Cost model, the self controller is willing to spend the same amount of energy to help their partner or itself achieve a goal. As a result, this model predicts no difference between the <italic>partner-relevant/self-irrelevant, partner-irrelevant/self-relevant</italic> and <italic>partner-relevant/self-relevant</italic> conditions (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>).</p>
<p>On the contrary, the self controller in the Partner Representation &amp; Weighted Joint Cost model primarily spends energy to achieve its own goal, while spending comparatively less energy to help a partner achieve their goal. During the <italic>partner-irrelevant/self-relevant</italic> condition, the self controller is only expecting a partial visuomotor feedback response from the partner since the partner has an irrelevant target. But in the <italic>partner-relevant/self-relevant</italic> condition, the self controller is expecting a comparatively greater visuomotor feedback response from the partner since the partner also has a relevant target. Therefore, the Partner Representation &amp; Weighted Joint Cost model predicts a greater visuomotor feedback response in the <italic>partner-irrelevant/self-relevant</italic> condition compared to the <italic>partner-relevant/self-relevant</italic> condition (<xref rid="fig4" ref-type="fig">Fig. 4D</xref>).</p>
</sec>
<sec id="s2f">
<title>Experiment 1 Visuomotor Feedback Responses</title>
<p>Here we show group level visuomotor feedback responses over time (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>), and the average visuomotor feedback response during the involuntary (180-230 ms), semi-involuntary (230-300 ms), and voluntary (300-400 ms) epochs.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Visuomotor Feedback Responses in Experiment 1.</title>
<p><bold>A)</bold> Visuomotor feedback response (yaxis) over time (x-axis), where 0 ms corresponds to the initial cursor jump. Solid lines represent the group average visuomotor feedback response for each condition. Shaded regions represent <italic>±</italic>1 standard error. Vertical grey lines separate involuntary (180 - 230 ms), semi-involuntary (230 - 300 ms), and voluntary (300-400 ms) visuomotor feedback responses. Average <bold>B)</bold> involuntary, <bold>C)</bold> semi-involuntary, and <bold>D)</bold> and voluntary visuomotor feedback response for each condition. Box and whisker plots show 25%, 50%, and 75% quartiles. <bold>B)</bold> We see significant differences in involuntary visuomotor feedback responses between each condition, matching the predictions of the Partner Representation &amp; Weighted Joint Cost model (see <xref ref-type="fig" rid="fig4">Fig. 4D</xref>). Crucially, a greater involuntary visuomotor feedback response in the <italic>partner-relevant/self-irrelevant</italic> condition compared to the <italic>partner-irrelevant/self-irrelevant</italic> condition (p &lt; 0.001) suggests a partner representation and some consideration of the partner’s cost. Further, a smaller involuntary visuomotor feedback response in the <italic>partner-relevant/self-relevant</italic> condition compared to the <italic>partner-irrelevant/self-relevant</italic> condition (p = 0.002) suggests a higher weighting of the self cost compared to the partner cost. Taken together, our results support the idea that involuntary visuomotor feedback responses express a representation of a partner, while using a joint cost that more heavily weights the self cost over the partner cost.</p></caption>
<graphic xlink:href="667240v3_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>There was a significant interaction between self target and partner target (F[1,47] = 61.61, p &lt; 0.001) on involuntary visuomotor feedback responses in Experiment 1. Interestingly, we found a significantly greater involuntary visuomotor feedback responses in the <italic>partner-relevant/self-irrelevant</italic> condition compared to the <italic>partner-irrelevant/self-irrelevant</italic> condition <inline-formula id="inline-eqn-9"><inline-graphic xlink:href="667240v3_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Crucially, these results support the idea that the involuntary sensorimotor circuits have a partner representation and a consideration of the partner’s cost.</p>
<p>Further, there was a significantly different involuntary visuomotor feedback response between the <italic>partner-relevant/self-relevant</italic> and <italic>partner-irrelevant/self-relevant</italic> conditions <inline-formula id="inline-eqn-10"><inline-graphic xlink:href="667240v3_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. A lower involuntary visuomotor feedback response in the <italic>partner-relevant/self-relevant</italic> condition compared to the <italic>partner-irrelevant/self-relevant</italic> condi-tion further suggests a partner representation, as well as a greater weighting of the self cost compared to the partner cost.</p>
<p><xref rid="fig5" ref-type="fig">Fig. 5C</xref> and <xref rid="fig5" ref-type="fig">Fig. 5D</xref> show the semi-involuntary and voluntary visuomotor feedback responses. We also found a significant interaction between self target and partner target for semi-involuntary (F[1,47] = 79.76, p &lt; 0.001) and voluntary (F[1,47] = 79.85, p &lt; 0.001) visuomotor feedback responses. Follow-up mean comparisons showed the same significant differences in both the semi-involuntary and voluntary visuomotor feedback responses, as seen in the involuntary visuomotor feedback responses.</p>
<p>The involuntary, semi-involuntary, and voluntary visuomotor feedback responses in each condition closely match the predictions of the Partner Representation &amp; Weighted Joint Cost model (compare <xref rid="fig4" ref-type="fig">Fig. 4D</xref> to <xref rid="fig5" ref-type="fig">Fig. 5</xref>). Remarkably, the results in Experiment 1 suggest that a partner representation and consideration of a partner’s cost not only influence voluntary behaviour, but also involuntary sensorimotor circuits.</p>
</sec>
<sec id="s2g">
<title>Experiment 2 Visuomotor Feedback Responses</title>
<p>Here we show group level visuomotor feedback responses over time (<xref rid="fig6" ref-type="fig">Fig. 6A</xref>), and the average visuomotor feedback response during the involuntary (180-230 ms), semi-involuntary (230-300 ms), and voluntary (300-400 ms) epochs.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Visuomotor Feedback Responses in Experiment 2.</title>
<p><bold>A)</bold> Visuomotor feedback response (yaxis) over time (x-axis), where 0 ms corresponds to the initial target jump. Solid lines represent the group average visuomotor feedback response for each condition. Shaded regions represent <italic>±</italic>1 standard error. Vertical grey lines separate involuntary (180 - 230 ms), semi-involuntary (230 - 300 ms), and voluntary (300-400 ms) visuomotor feedback responses. Average <bold>B)</bold> involuntary, <bold>C)</bold> semi-involuntary, and <bold>D)</bold> and voluntary visuomotor feedback response for each condition. Box and whisker plots show 25%, 50%, and 75% quartiles. <bold>B)</bold> Critically, a greater involuntary visuomotor feedback response in the <italic>partner-relevant/selfirrelevant</italic> condition compared to the <italic>partner-irrelevant/self-irrelevant</italic> condition (p &lt; 0.001) suggests a partner representation and some consideration of the partner’s cost.</p></caption>
<graphic xlink:href="667240v3_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We found a significant interaction between self target and partner target (F[1,47] = 20.54, p &lt; 0.001) for involuntary visuomotor feedback responses in Experiment 2. Follow-up mean comparisons again showed a significant increase in the visuomotor feedback response in the <italic>partner-relevant/self-irrelevant</italic> condition compared to the <italic>partnerirrelevant/self-irrelevant</italic> condition <inline-formula id="inline-eqn-11"><inline-graphic xlink:href="667240v3_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. As shown in Experiment 1, these Experiment 2 results further support the idea that the involuntary sensorimotor circuits have a partner representation and a consideration of the partner’s cost.</p>
<p>Between the <italic>partner-irrelevant/self-relevant</italic> condition and the <italic>partner-relevant/self-relevant</italic> conditions, we did not find a significant difference <inline-formula id="inline-eqn-12"><inline-graphic xlink:href="667240v3_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Nevertheless, the involuntary visuomotor feedback responses in each condition still most closely matched the predictions of the Partner Representation &amp; Weighted Joint Cost model. Further, we also found a significant interaction between self target and partner target for the semi-involuntary (F[1,47] = 68.82, p &lt; 0.001) and voluntary (F[1,47] = 133.04, p &lt; 0.001) visuomotor feedback responses. Aligning with the results from Experiment 1, we found a significant difference between the <italic>partner-irrelevant/self-relevant</italic> condition and <italic>partner-relevant/self-relevant</italic> condition for the semi-involuntary (<xref rid="fig6" ref-type="fig">Fig. 6C</xref>; <inline-formula id="inline-eqn-13"><inline-graphic xlink:href="667240v3_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula>) and voluntary (<xref rid="fig6" ref-type="fig">Fig. 6D</xref>; <inline-formula id="inline-eqn-14"><inline-graphic xlink:href="667240v3_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula>) visuomotor feedback responses. Taken together, the visuomotor feedback responses in Experiment 1 and Experiment 2 closely match the Partner Representation &amp; Weighted Joint Cost model predictions. Remarkably, the involuntary visuomotor feedback responses across two experiments supports our hypothesis that a high-level partner representation and a consideration of a partner’s cost influences low-level involuntary sensorimotor circuits.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Our primary finding across two experiments was that a partner representation and consideration of a partner’s cost influences involuntary visuomotor feedback responses. Specifically, involuntary visuomotor feedback responses closely matched the hypothesis that the sensorimotor system uses a partner representation and weighted joint cost, where the self cost is prioritized more than the partner cost. Taken together, our empirical results and computational modelling support the idea that a high-level partner representation and a joint cost influences lower-level involuntary sensorimotor circuits.</p>
<p>In this paper, we demonstrated how a representation of a partner and consideration of their costs influences rapid and involuntary visuomotor feedback responses during a cooperative sensorimotor reaching task. In Experiments 1 and 2, we found that participants displayed increased involuntary visuomotor feedback responses when there was a relevant partner target and an irrelevant self target, compared to when both targets were irrelevant. Aligned with model predictions, these findings suggest that involuntary feedback responses reflect a partner representation and a joint cost. In Experiment 1, we found a significant decrease in involuntary visuomotor feedback responses to cursor jumps when both the self and partner target was relevant, compared to the condition with an irrelevant partner target and relevant self target. The different involuntary visuomotor feedback responses between these conditions suggests that the sensorimotor system uses a partner representation and weighted joint cost to modulate involuntary visuomotor feedback responses. Interestingly, this result suggests that the sensorimotor system modulates involuntary visuomotor feedback responses based on a prediction of a partner’s control policy. Further, it highlights that high-level partner representations modulate lower-level sensorimotor circuits and are rapidly expressed via involuntary visuomotor feedback responses.</p>
<p>In Experiment 1 and Experiment 2 we found the same significant differences between conditions for the semi-involuntary and voluntary visuomotor feedback responses. However, in Experiment 2, we did not see a decrease in involuntary visuomotor feedback responses to target jumps when both self and partner targets were relevant, compared to an irrelevant partner target and relevant self target. One possibility for this finding is that there may be longer visuomotor feedback response latencies to target jumps compared to cursor jumps.<sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c20">20</xref></sup> Interestingly, it may also be the case that visuomotor feedback responses to a self target jump are expressed at a different latency than responses to a partner target jump.</p>
<p>Overall, we found greater involuntary visuomotor feedback responses for a relevant self target compared to an irrelevant self target. This finding aligns with single-person studies that examined how the relevancy of a mechanical or visual perturbation to the behavioural goal influences rapid feedback responses, prior to volitional control. Nashed and colleagues (2012) showed larger long-latency muscular responses (50-100 ms) to mechanical perturbations when reaching to a narrow (circular) relevant target compared to a wide (rectangular) irrelevant target.<sup><xref ref-type="bibr" rid="c6">6</xref></sup> Further, Franklin and colleagues (2008) showed greater involuntary visuomotor feedback responses to a relevant visual perturbation compared to an irrelevant visual perturbation.<sup><xref ref-type="bibr" rid="c17">17</xref></sup> Both of these prior studies suggest that the sensorimotor system can tune involuntary feedback responses based on higher-level task goals. Our novel experimental paradigm has extended these findings to understand how humans integrate their own goal with their partner’s goal during jointly controlled actions. Importantly, we found that involuntary visuomotor processes can express not only an individual goal, but also an integrated representation of both the self and partner goals.</p>
<p>Optimal feedback control has been a powerful framework to understand how the nervous system selects movements.<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup> Past work, including our own,<sup><xref ref-type="bibr" rid="c23">23</xref></sup> has extended optimal feedback control to human-human interaction by having two separate optimal feedback controllers interact.<sup><xref ref-type="bibr" rid="c24">24</xref></sup> In these works the control policy for each of the controllers was selected in isolation. That is, the controllers do not select a control policy using knowledge of the partner’s control policy (i.e., partner representation). The dynamic game theory framework further extends the separate feedback controller approach by allowing each controller to select a control policy using a partner representation. This dynamic game theory framework has successfully been used to model human-robot<sup><xref ref-type="bibr" rid="c25">25</xref></sup> and human-human sensorimotor interactions.<sup><xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c27">27</xref></sup> The aforementioned studies have suggested people form a partner representation in their control policy to produce voluntary movements. Critically, we are the first to our knowledge to measure a proxy of the control policy, assessing how a partner representation influences rapid and involuntary visuomotor feedback responses. Our dynamic game theory model supports the hypothesis that involuntary visuomotor feedback responses reflect a partner representation and joint cost. It would also be interesting to investigate whether other rapid feedback responses, such as the long-latency stretch response, can also express a partner representation.</p>
<p>Both the optimal feedback control and dynamic game theory frameworks view human movement as a process of minimizing a cost function. This cost function is designed such that the controller (i.e., sensorimotor system) achieves some goal state, such as accurately hitting a target, while minimizing an energetic cost. Not correcting for deviations along an irrelevant dimension reduces energetic cost. In our paper, we extended this concept to understand not only how the sensorimotor system considers its own self cost, but also a joint cost that considers both the self cost and partner cost. In both experiments we found increased involuntary visuomotor feedback responses in the relevant partner target and irrelevant self target condition compared to both targets being irrelevant. That is, we found that participant’s visuomotor feedback responses reflected a consideration of not only the relevancy of their own self target (i.e, self cost), but also that of their partner (i.e., partner cost). This result suggests that involuntary visuomotor feedback responses reflect the sensorimotor system’s willingness to sacrifice energy to help a partner.</p>
<p>Classic and contemporary theories of action selection, such as Gibson’s theory of affordances<sup><xref ref-type="bibr" rid="c28">28</xref></sup> and the affordance competition hypothesis,<sup><xref ref-type="bibr" rid="c29">29</xref></sup> propose that the sensorimotor system selects movements based on opportunities for action that emerge from the fit between an individual’s capabilities and surrounding environment. Our finding that humans sacrifice energetic cost to support a partner’s goal extends this perspective by suggesting that the sensorimotor system may also consider “social affordances”, which depend not only on one’s own goals but also those of others. An interesting future direction would be to explore how the overlap of the self and partner goals might influence the degree to which humans help one another during collaborative, cooperative, and competitive sensorimotor interactions.</p>
<p>The nervous system can form representations of both self and others. Research studying reaching movements for a single individual has shown that the nervous system forms a representation of one’s own limb dynamics<sup><xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c32">32</xref></sup> and environment,<sup><xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup> which are expressed prior to and following volitional control. Additionally, it has been wellestablished that the human sensorimotor system can form representations of others.<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup> Ramnani and Miall (2004) showed evidence using fMRI that the human brain even has a dedicated system to predict the actions of others.<sup><xref ref-type="bibr" rid="c36">36</xref></sup> Behavioural evidence for these partner representations have been shown across cognitive<sup><xref ref-type="bibr" rid="c37">37</xref>,<xref ref-type="bibr" rid="c38">38</xref></sup> and perceptual<sup><xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c40">40</xref></sup> decisionmaking, response time,<sup><xref ref-type="bibr" rid="c7">7</xref></sup> and reaching<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c41">41</xref></sup> tasks. Behaviourally, Schmitz and colleagues (2017) observed that an obstacle in the partner’s movement path influenced one’s own voluntary reach trajectory.<sup><xref ref-type="bibr" rid="c1">1</xref></sup> Computational and empirical work from Chackochan and Sanguinetti (2019) suggested that humans use a representation of their partner to select movement trajectories during a reaching task where they are haptically connected to a partner.<sup><xref ref-type="bibr" rid="c27">27</xref></sup> While neural data, behavioural experiments, and computational modelling have suggested that partner representations influence voluntary reaching movements, to our knowledge none have examined whether a representation of others can be expressed at an involuntary timescale.</p>
<p>The neural basis of upper limb control has been well-studied. For the upper limb, neural recordings in monkeys have shown that activity in the primary motor cortex (M1) reflects visuospatial representations including target goals.<sup><xref ref-type="bibr" rid="c42">42</xref></sup> High-level visuospatial representations can be rapidly expressed via the muscular long-latency reflex and involuntary visuomotor feedback responses. These lower-level sensorimotor feedback responses are prior to volitional control. The long-latency reflex involves a transcortical pathway with contributions from likely both cortical and subcortical circuitry.<sup><xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c44">44</xref></sup> It has also been shown that both cortical and subcortical (e.g., superior colliculus) regions are involved when responding to visual perturbations.<sup><xref ref-type="bibr" rid="c45">45</xref>,<xref ref-type="bibr" rid="c46">46</xref>,<xref ref-type="bibr" rid="c47">47</xref>,<xref ref-type="bibr" rid="c48">48</xref></sup> Collectively these studies suggest top-down projections from high-level cortical representations to lower-level sensorimotor circuits,<sup><xref ref-type="bibr" rid="c49">49</xref></sup> enabling fast and flexible feedback responses.</p>
<p>Just as the sensorimotor system forms representations of its own actions and goals, it has also been shown to represent the actions of others. Observing the actions of others increases neural activity in motor regions such as primary motor and the dorsal premotor cortex.<sup><xref ref-type="bibr" rid="c41">41</xref>,<xref ref-type="bibr" rid="c50">50</xref>,<xref ref-type="bibr" rid="c51">51</xref>,<xref ref-type="bibr" rid="c52">52</xref>,<xref ref-type="bibr" rid="c53">53</xref></sup> These so-called ‘mirror neurons’ may help the sensorimotor system understand the actions of others<sup><xref ref-type="bibr" rid="c54">54</xref>,<xref ref-type="bibr" rid="c55">55</xref></sup> (but see Hayes et al. (2010)<sup><xref ref-type="bibr" rid="c56">56</xref></sup> for an alternative perspective). Importantly, activation of primary motor and premotor regions have also been shown during the prediction of others’ actions, even without directly observing the movement.<sup><xref ref-type="bibr" rid="c36">36</xref></sup> Other work has shown that the cerebellum, which uses a self representation (i.e., internal model) to predict future motor actions,<sup><xref ref-type="bibr" rid="c57">57</xref>,<xref ref-type="bibr" rid="c58">58</xref>,<xref ref-type="bibr" rid="c59">59</xref></sup> may also form an internal model of others to predict their future actions.<sup><xref ref-type="bibr" rid="c60">60</xref>,<xref ref-type="bibr" rid="c61">61</xref></sup> Therefore, the neural circuitry for the representations of others’ actions and for the control of movement seems to be tightly linked.<sup><xref ref-type="bibr" rid="c62">62</xref>,<xref ref-type="bibr" rid="c63">63</xref></sup> In light of these findings, our work suggests that there is top-down modulation from high-level circuits involved with partner representations to lower-level sensorimotor circuitry. That is, the nervous system appears to leverage high-level partner representations in lower-level sensorimotor circuits to anticipate and respond to a partner’s future actions. Future work could use neural recordings while non-human primates perform a cooperative sensorimotor interaction task to further understand how a representation of others might influence the control of movement. From an evolutionary perspective,<sup><xref ref-type="bibr" rid="c64">64</xref>,<xref ref-type="bibr" rid="c65">65</xref></sup> it would be interesting to know where along the phylogenetic history a high-level representation of others regulates lower-level sensorimotor circuits involved with rapid and involuntary feedback responses.</p>
<p>Across two experiments and a computational model, we showed that involuntary visuomotor feedback responses reflect a partner representation and consideration of a partner’s cost. Our novel results suggest that high-level partner representations influence lower-level involuntary sensorimotor circuitry. Our paradigm offers a powerful new window to probe how human sensorimotor interactions are influenced by cognitive processes, theory of mind, and social dynamics.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>96 participants participated across two experiments. 24 pairs (48 individuals) participated in Experiment 1 and 24 pairs (48 individuals) participated in Experiment 2. All participants reported they were free from musculoskeletal injuries, neurological conditions, or sensory impairments. In addition to a base compensation of $5.00, we informed them they would receive a performance-based compensation of up to $5.00. Each participant received the full $10.00 once they completed the experiment irrespective of their performance. All participants provided written informed consent to participate in the experiment and the procedures were approved by the University of Delaware’s Institutional Review Board.</p>
</sec>
<sec id="s4b">
<title>Apparatus</title>
<p>For both experiments we used two end-point KINARM robots (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>; BKIN Technologies, Kingston, ON). Each participant was seated on an adjustable chair in front of one of the end-point robots. Each participant grasped the handle of a robotic manipulandum and made reaching movements in the horizontal plane. A semi-silvered mirror blocked the vision of the upper limb, and also reflected virtual images (e.g., targets, cursors) from an LCD to the horizontal plane of hand motion. In all experiments the participant’s own (self) cursor was aligned with the position of their hand. Kinematic data were recorded at 1,000 Hz and stored offline for data analysis.</p>
</sec>
<sec id="s4c">
<title>Experimental Design</title>
<p>We designed two experiments where participants used knowledge of both their own and partner’s target to successfully complete a jointly coordinated reaching task. During both experiments, each participant viewed a self cursor that was aligned with their hand and another cursor that represented their partner’s position. They also saw a center cursor at the midpoint between their cursor and their partner’s cursor. Finally, they also observed both their own target and their partner’s target. The center cursor and both targets were laterally aligned to the center of each participant’s screen. Both targets were 25 cm forward from the start position.</p>
<p>Both participants began each trial with their hand placed 13 cm to the right of the center cursor. Each participant observed their partner’s cursor, which was reflected over the center line that intersected the center cursor and the targets. Thus, both participants viewed a mirrored position of their partner. By mirroring both partners, this allowed each participant to view themselves on the right side of the center cursor and their partner on the left side of the center cursor. Further, it allowed for control of the center cursor in a smooth and intuitive manner as if their partner was sitting beside them.</p>
<p>The center cursor was at the midpoint between the participant’s hands, except during Experiment 1 perturbation and probe trials when the center cursor was laterally jumped (see further below). The movement of each participant contributed to half the movement of the center cursor. For example, if one participant moved forward 6 cm and their partner did not move, then the center cursor would move 3 cm forward. Likewise, if one participant moved 6 cm to the right and their partner did not move, then the center cursor would move 3 cm to the right.</p>
<p>At the start of a trial, the robot guided each participant’s self cursor to their respective start circle. The white start circle (diameter 2 cm) was displayed 13 cm to the right of the initial center cursor location for both participants. After a constant delay of 700 ms the partner cursor, center cursor, self target, and partner target appeared on the screen (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>). Then after a constant delay of 750 ms, participants heard a tone that indicated they should begin their reach. Both participants in the pair were instructed to move the center cursor into their self target. To complete a trial, each participant had to stabilize the center cursor within their self target for 500 ms. Participants received timing feedback based on the time between the start tone and completing the trial. Participants received the message ‘Good’, ‘Too Slow’, or ‘Too Fast’ if they stabilized within their self target between 1400 - 1600 ms, &gt; 1600 ms, or &lt;1400 ms respectively. They were explicitly informed that their timing feedback depended on the center cursor entering and stabilizing within <italic>only</italic> their own target. For example, if the center cursor entered and stabilized within the participant’s self target at 1500 ms, but entered and stabilized within the partner target at 1700 ms, then the participant would receive “Good” feedback and their partner would receive “Too Slow” feedback. Therefore, the instructions and timing constraints did not enforce participants to work together.</p>
<p>The goal of Experiment 1 and Experiment 2 was to study how a representation of a partner’s goal influences involuntary visuomotor feedback responses. Therefore, in experimental blocks we manipulated the width of both the self and partner goal to be either narrow (1.05 cm) or wide (20 cm). The narrow target reflects a task-relevant goal because a participant must correct for lateral deviations of the center cursor to successfully complete their task. The wide target reflects a task-irrelevant goal because a participant does not have to correct for lateral deviations of the center cursor to successfully complete their task. Both targets had a height of 1.25 cm and were aligned horizontally and vertically throughout both experiments. Human pairs performed four blocked conditions in a two-way, repeated measures experimental design: i) <italic>partner-irrelevant/selfirrelevant</italic> ii) <italic>partner-relevant/self-irrelevant</italic> iii) <italic>partner-irrelevant/self-relevant</italic> iv) <italic>partnerrelevant/self-relevant</italic> (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>). The order of the experimental conditions was fully counterbalanced in both Experiment 1 and Experiment 2.</p>
<p>For both experiments, participants first performed a familiarization block of trials, and then 4 experimental blocks that were separated by a washout block. The self and partner targets were 10 cm in width and 1.25 cm in height in both familiarization and washout blocks. Participants performed 50 non-perturbation trials (see below) during the familiarization block. They performed 25 non-perturbation trials during each of the three washout blocks</p>
<p>Each human pair completed four experimental blocks, where for a block they experienced the i) <italic>partner-irrelevant/self-irrelevant</italic>, ii) <italic>partner-relevant/self-irrelevant</italic>, iii) <italic>partner-irrelevant/self-relevant</italic>, or iv) <italic>partner-relevant/self-relevant</italic> condition. In the experimental blocks, participants experienced 81 non-perturbation trials, 40 perturbation trials, and 30 probe trials.</p>
</sec>
<sec id="s5">
<title>Non-Perturbation Trials</title>
<p>During non-perturbation trials, the cursor center was always at the midpoint between the human pair. There was neither center cursor (Experiment 1) nor target jumps (Experiment 2).</p>
<sec id="s5a">
<title>Perturbation Trials</title>
<p>During perturbation trials within an experimental block, the center cursor (Experiment 1) or both targets (Experiment 2) jumped to either the left (20 trials) or right (20 trials) once the center cursor crossed 25% of the distance to the goals (6.25 cm forward from the start position; <xref rid="fig1" ref-type="fig">Fig. 1C-D</xref>). The cursor or target jump was a 3 cm linear shift in the lateral position over 25 ms. The center cursor remained laterally displaced for the duration of the trial. Thus, participants were required to correct for the center cursor or target jump to successfully complete their task when they had a self-relevant target. However, they would not have to correct for the center cursor or target jump to successfully complete their task when they had a self-irrelevant target.</p>
</sec>
<sec id="s5b">
<title>Probe Trials</title>
<p>During probe trials within an experimental block, the center cursor (Experiment 1) or both targets (Experiment 2) jumped to either the left (10 trials) or the right (10 trials) once the center cursor crossed 25% of the distance to the goals (6.25 cm forward from the start position; <xref rid="fig1" ref-type="fig">Fig. 1E-F</xref>). We also included 10 null probe trials where the center cursor or both targets did not jump. Here, both participants in the pair were constrained to a force channel that allowed forward hand movement, but prevented lateral hand movement.</p>
<p>The center cursor or target jump was a 3 cm linear shift in the lateral position over 25 ms. The center cursor or target remained displaced for 200 ms and then linearly shifted back to the original lateral position over 25 ms. Critically, as a metric of visuomotor feedback responses, we measured the lateral force participants applied against the channel in response to center cursor or target jumps.</p>
<p>During an experimental block, the non-perturbation trials, perturbation trials, and probe trials were randomly interleaved such that each set of 15 trials contained 8 non-perturbation trials, 2 left perturbation trials, 2 right perturbation trials, 1 left probe trial, 1 right probe trial, and 1 neutral probe trial. Participants performed 10 sets of trials within a block. We also ensured the first trial of an experimental condition was not a probe trial by adding a non-perturbation trial to the start of each experimental condition. In total, participants performed 729 reaches consisting of 50 non-perturbation trials in the familiarization block, 75 non-perturbation trials across the three washout blocks, as well as 480 non-perturbation trials, 240 perturbation trials, and 120 probe trials across the four experimental blocks.</p>
<sec id="s5b1">
<title>Dynamic Game Theory Model</title>
<p>We used a dynamic game theory model to predict movement behaviour and visuomotor feedback responses of human pairs. Dynamic game theory is a multi-controller extension of the typical optimal feedback control framework that describes a single controller.<sup><xref ref-type="bibr" rid="c11">11</xref></sup> This framework has previously been used to model human movement during collaborative tasks.<sup><xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c66">66</xref></sup> Here, we modelled our experiments as a linear-quadratic game with two players (controllers).<sup><xref ref-type="bibr" rid="c67">67</xref></sup> Each controller had direct control of its own hand and attempted to move the center cursor toward its own self target.</p>
</sec>
</sec>
<sec id="s5c">
<title>System Dynamics</title>
<p>Each hand in the linear-quadratic game was modelled as a point mass. Throughout, the subscript <italic>i</italic> refers to each controller, where <italic>i</italic> = {1, 2}. We describe the model with controller 1 as the self and controller 2 as the partner. The continuous-time dynamics of the point mass representing the hand of controller 1 were as follows:
<disp-formula id="eqn8">
<graphic xlink:href="667240v3_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn9">
<graphic xlink:href="667240v3_eqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>m</italic><sub>1</sub> is the mass of the hand, <italic>p</italic><sub>1</sub> is the two-dimensional position vector of the point mass, <italic>b</italic> is the viscous constant, <italic>f</italic><sub>1</sub> is the two-dimensional controlled forces, and <italic>u</italic><sub>1</sub> is the two-dimensional control signal for controller 1. <italic>m</italic> was set to 1.5 kg, <italic>b</italic> was set to 0.1 N · s · m<sup>−1</sup> and the time constant of the linear filter (<italic>τ</italic>) was set to 20 ms. These parameters were identical for controller 1 and controller 2. The parameters were selected so that the model visuomotor feedback response magnitudes closely matched the measured visuomotor feedback response magnitudes.</p>
<p>Controllers 1 and 2 each move their hand and interact to move the center cursor (<italic>cc</italic>). The dynamics of the center cursor are:
<disp-formula id="eqn10">
<graphic xlink:href="667240v3_eqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The state vector <italic>x</italic> is
<disp-formula id="eqn11">
<graphic xlink:href="667240v3_eqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where each element in the vector contains an <italic>x</italic> and <italic>y</italic> dimension. <italic>T</italic> is the transpose operator. The system dynamics were transformed into a system of first order differential equations and discretized. The linear-quadratic state space model is
<disp-formula id="eqn12">
<graphic xlink:href="667240v3_eqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here <italic>x</italic><sub><italic>k</italic></sub> is the state vector at time <italic>k</italic> and <italic>A</italic> is the dynamics matrix. <italic>B</italic><sub><italic>i</italic></sub> maps the control vector <italic>u</italic><sub><italic>i,k</italic></sub> of player <italic>i</italic> to muscle force <italic>f</italic><sub><italic>i,k</italic></sub> at time <italic>k. A, B</italic><sub>1</sub>, and <italic>B</italic><sub>2</sub> are fully defined in Supplementary C.</p>
</sec>
<sec id="s5d">
<title>State Feedback Design</title>
<p>Each controller receives delayed sensory feedback of its own hand position, velocity, and force, as well as the partner’s hand position and velocity. Further, each controller receives delayed sensory feedback of the center cursor position and target position. To incorporate sensory delays, we augmented the state vector with previous states:<sup><xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c68">68</xref></sup>
<disp-formula id="eqn13">
<graphic xlink:href="667240v3_eqn13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here <italic>δv</italic> = 110 ms (corresponding to <italic>n</italic><sub><italic>δv</italic></sub> = 11 time steps when discretized) to reflect the transmission delay associated with vision and aligned the model and experimental visuomotor response onset times. The sensory states available to controller 1 are
<disp-formula id="eqn14">
<graphic xlink:href="667240v3_eqn14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>y</italic><sub>1</sub> is the vector of delayed state observations and <italic>ω</italic><sub>1,<italic>k</italic></sub> is a sensory noise vector. <inline-formula id="inline-eqn-15"><inline-graphic xlink:href="667240v3_inline15.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is an observation matrix designed to selectively observe some of the delayed states. The observation matrices <inline-formula id="inline-eqn-16"><inline-graphic xlink:href="667240v3_inline16.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula id="inline-eqn-17"><inline-graphic xlink:href="667240v3_inline17.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and noise vector <italic>ω</italic><sub>1,<italic>k</italic></sub> are fully defined in Supplementary C. We drop the superscript <italic>aug</italic> to minimize extra notation going forward.</p>
<p>Like previous work, we used a linear Kalman filter to model participants sensory estimates of the state variables. The posterior state estimate <inline-formula id="inline-eqn-18"><inline-graphic xlink:href="667240v3_inline18.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of controller 1 is obtained using an online filter of the form:
<disp-formula id="eqn15">
<graphic xlink:href="667240v3_eqn15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn16">
<graphic xlink:href="667240v3_eqn16.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here <inline-formula id="inline-eqn-19"><inline-graphic xlink:href="667240v3_inline19.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the prior prediction of the state. That is, we assume the sensorimotor system obtains a prior prediction of the states using an accurate internal model of the state dynamics, which includes a prediction of the partner’s motor command. The prior prediction uses the previous posterior estimate <inline-formula id="inline-eqn-20"><inline-graphic xlink:href="667240v3_inline20.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the efference copy (<italic>u</italic><sub>1</sub>), and the prediction of the partner’s motor command (<italic>u</italic><sub>2</sub>).</p>
<p>The prior prediction of the state is updated using sensory measurements to obtain the posterior estimate <inline-formula id="inline-eqn-21"><inline-graphic xlink:href="667240v3_inline21.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref ref-type="disp-formula" rid="eqn15">Eq. 15</xref>). The sequence of Kalman gains <italic>K</italic><sub>1</sub> and <italic>K</italic><sub>2</sub> were updated recursively (Supplementary C).</p>
</sec>
<sec id="s5e">
<title>Control Design</title>
<p>The goal of each controller <italic>i</italic> is to move the state of the system from an initial state <italic>x</italic><sub>0</sub> to a target state <italic>x</italic><sup><italic>target</italic></sup> at the final time step <italic>N</italic> by each minimizing a quadratic cost functional <italic>J</italic><sub><italic>i</italic></sub>:
<disp-formula id="eqn17">
<graphic xlink:href="667240v3_eqn17.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn18">
<graphic xlink:href="667240v3_eqn18.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here <italic>J</italic><sub>1</sub> is the individual cost for controller 1 (e.g. self) and <italic>J</italic><sub>2</sub> is the individual cost for controller 2 (e.g. partner). The quadratic costs penalize deviations from the target state at the final step (<italic>Q</italic><sub><italic>i,N</italic></sub>) and controller <italic>i</italic>’s control signals (<italic>R</italic><sub><italic>ii</italic></sub>). We then define the joint cost functions as
<disp-formula id="eqn19">
<graphic xlink:href="667240v3_eqn19.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn20">
<graphic xlink:href="667240v3_eqn20.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>α</italic><sub><italic>i</italic></sub> ∈ [0, 1] determines the degree to which controller <italic>i</italic> considers their partner’s costs.</p>
<p>The optimal control signal for controllers 1 and 2 is defined as
<disp-formula id="eqn21">
<graphic xlink:href="667240v3_eqn21.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn22">
<graphic xlink:href="667240v3_eqn22.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula id="inline-eqn-22"><inline-graphic xlink:href="667240v3_inline22.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the posterior estimate and <italic>F</italic><sub><italic>i,k</italic></sub> is the time-varying feedback gain for controller <italic>i</italic>. The feedback gains <italic>F</italic><sub><italic>i,k</italic></sub>, also known as the control policy, are the Nash equilibrium solution to the linear quadratic game described by <xref ref-type="disp-formula" rid="eqn11">Eq. 11</xref> and <xref ref-type="disp-formula" rid="eqn16">Eqs. 16</xref>-<xref ref-type="disp-formula" rid="eqn19">19</xref>.<sup><xref ref-type="bibr" rid="c67">67</xref></sup> See Supplementary C for details.</p>
</sec>
<sec id="s5f">
<title>Modelling Different Control Policies</title>
<p>We tested four different control policies, each reflecting a hypothesis about how a partner representation and consideration of a partner’s cost influences visuomotor feedback responses. In our modelling framework, a partner representation indicates knowledge of the partner’s control policy. Further, we can also vary whether a controller considers only their own self cost, or both a self and partner cost (i.e., joint cost). We tested the following four models: i) No Partner Representation &amp; Self Cost, ii) Partner Representation &amp; Self Cost, iii) Partner Representation &amp; Equal Joint Cost, and iv) Partner</p>
<sec id="s5f1">
<title>Representation &amp; Weighted Joint Cost</title>
<p>The No Partner Representation &amp; Self Cost model implies that the sensorimotor system does not use a control policy that has a representation of a partner. Mathematically, we set <italic>F</italic><sub>2,<italic>k</italic></sub> = 0 for all <italic>t</italic> when calculating the feedback gains for controller 1. That is, if there is no partner representation, then controller 1 does not account for the partner’s control policy when selecting its own control policy. Since there is no partner representation, the model can only consider a self cost (i.e., <italic>α</italic><sub>1</sub> = 0).</p>
<p>The Partner Representation &amp; Self Cost model suggests that the sensorimotor system uses a control policy that has a partner representation, but only considers a self cost. That is, controller 1 will produce movements using knowledge of how their partner will move. Further, controller 1 will only produce movements that lead to a minimal self cost, without consideration of the partner’s cost. A self cost is obtained by setting <italic>α</italic><sub>1</sub> = 0 in <xref ref-type="disp-formula" rid="eqn19">Eq. 19</xref>.</p>
<p>The Partner Representation &amp; Equal Joint Cost model implies that the sensorimotor system uses a control policy that has a partner representation, and equally weights the self and partner costs. Here controller 1 will produce movements that uses knowledge of how their partner will move. Further, controller 1 will produce movements that lead to an equal minimization of both the self and partner cost. That is, one is willing to potentially spend additional energy so that a partner reaches their goal. An equal joint cost is obtained by setting <italic>α</italic><sub>1</sub> = 1.0 in <xref ref-type="disp-formula" rid="eqn19">Eq. 19</xref>.</p>
<p>The Partner Representation &amp; Weighted Joint Cost model implies that the sensorimotor system uses a control policy that has a partner representation, and partially weights the partner cost. Again, controller 1 will produce movements that uses knowledge of how their partner will move. However, controller 1 will produce movements that primarily minimize the self cost and to a lesser extent the partner cost. That is, one will mostly spend energy to reach their own goal, but will still spend some energy to help their partner. A weighted joint cost that weighs the self cost higher than the partner cost is obtained by setting <italic>α</italic><sub>1</sub> = 0.5 in <xref ref-type="disp-formula" rid="eqn19">Eq. 19</xref>.</p>
</sec>
</sec>
<sec id="s5g">
<title>Model Simulations</title>
<p>We simulated each of the self and partner target structures from the experiment: i) <italic>partner-irrelevant/self-irrelevant</italic>, ii) <italic>partner-relevant/self-irrelevant</italic>, iii) <italic>partner-irrelevant/selfrelevant</italic>, and iv) <italic>partner-relevant/self-relevant</italic>. For relevant targets, we set the x-dimension of the center cursor position in the final state cost matrix <italic>Q</italic><sub><italic>i,N</italic></sub> to 40000 for <italic>i</italic> = {1, 2}. That is, controller <italic>i</italic> incurs a cost and will correct for lateral deviations of the center cursor away from a relevant target. For irrelevant targets, we set the x-dimension of the center cursor position in the final state cost matrix <italic>Q</italic><sub><italic>i,N</italic></sub> to 100 for <italic>i</italic> = {1, 2}. That is, controller <italic>i</italic> does not incur a cost for lateral deviations of the center cursor if their target is irrelevant. For a full description of <italic>Q</italic><sub>1</sub> and <italic>Q</italic><sub>2</sub>, see Supplementary C</p>
<p>We simulated 100 perturbation trials per condition to predict the position trajectories. Perturbation trials were simulated by jumping the center cursor laterally to the left by 3 cm once the center cursor reached 25% of the forward distance to the target.</p>
<p>We also simulated 100 probe trials per condition to predict visuomotor feedback responses. Probe trials were simulated by jumping the center cursor 3 cm laterally for 250 ms, then jumping it back to the original lateral position. To simulate a force channel, we set the x-force element in the <italic>B</italic><sub>1</sub> and <italic>B</italic><sub>2</sub> matrices to 0. Thus, the controllers could only move the center cursor in the forward dimension. We were able to calculate the applied force for controller 1 in the lateral dimension using the original <italic>B</italic><sub>1</sub> matrix. That is, the applied force traces shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref> is <italic>B</italic><sub>1</sub><italic>u</italic><sub>1</sub> in the x-dimension over time. Aligned with the literature on involuntary visuomotor feedback responses, we calculated the average feedback response from the model during the 180 - 230 ms epoch.</p>
<sec id="s5g1">
<title>Data Analysis</title>
<p>We analyzed the results from the non-perturbation, perturbation, and probe trials for the experimental conditions. We recorded both hand positions and the center cursor position during all trials, as well as the force applied by the robot to the hand during the probe trials. All kinematic and kinetic data were filtered with a 5th-order, low-pass Butterworth filter with a 14-hz cutoff frequency.</p>
</sec>
</sec>
<sec id="s5h">
<title>Visuomotor Feedback Responses</title>
<p>The recorded forces applied by both participants in the pair during visual probe trials were time-aligned with the cursor or target jump onset. The delay of the LCD for presentation of visual feedback was determined to be 42 ms. Visuomotor feedback responses in this study are presented relative to the onset of the actual perturbation time. That is, the LCD delay has been taken into account such that visuomotor feedback responses were aligned relative to the time the visual signal was actually presented to participants on their display. In Experiment 1, we define the visuomotor feedback response (N) as the difference between the recorded force during a left cursor jump probe and a right cursor jump probe.<sup><xref ref-type="bibr" rid="c17">17</xref></sup> In Experiment 2, we define the visuomotor feedback response (N) as the difference between the recorded force during a right target jump probe and a left target jump probe.</p>
<p>To investigate the involuntary visuomotor feedback response, we calculated the average force response for each participant during the 180 - 230 ms time window.<sup><xref ref-type="bibr" rid="c17">17</xref></sup> We also calculated the average force response during the 230 - 300 ms window and 300 - 400 ms window. The 230 - 300 ms window may contain a mixture of involuntary and voluntary responses, which we term as the semi-involuntary visuomotor feedback response. The 300 - 400 ms window is the voluntary visuomotor feedback response.</p>
</sec>
<sec id="s5i">
<title>Final Lateral Hand Deviation</title>
<p>We calculated the final lateral hand deviation as a metric of a participant’s voluntary corrective response to perturbation trials. Final lateral hand position was determined as the x-position of each participant at the end of each trial. To calculate final lateral hand deviation, we took the difference between the average final lateral hand position during non-perturbation trials and the average final lateral hand position during perturbation trials (see Supplementary B).</p>
<sec id="s5i1">
<title>Statistical Analysis</title>
<p>For both experiments, we used a 2 (Self Irrelevant or Self Relevant) x 2 (Partner Irrelevant or Partner Relevant) repeated-measures ANOVA for each dependent variable. We followed up the omnibus tests with mean comparisons using nonparametric bootstrap hypothesis tests (n = 1,000,000).<sup><xref ref-type="bibr" rid="c69">69</xref>,<xref ref-type="bibr" rid="c70">70</xref>,<xref ref-type="bibr" rid="c71">71</xref>,<xref ref-type="bibr" rid="c72">72</xref>,<xref ref-type="bibr" rid="c73">73</xref></sup> Mean comparisons were Holm-Bonferroni corrected to account for multiple comparisons. We computed the common language effect sizes <inline-formula id="inline-eqn-23"><inline-graphic xlink:href="667240v3_inline23.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for all mean comparisons. Significance threshold was set at <italic>α</italic> = 0.05.</p>
</sec>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="das" sec-type="data-availability">
<title>Data availability</title>
<p>All behavioural data have been deposited at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.30132088">https://doi.org/10.6084/m9.figshare.30132088</ext-link>.</p>
</sec>
<app-group>
<app id="app1">
<title>Appendix</title>
<sec id="s6">
<title>Supplementary A: Model and Experiment 2 Trajectories</title>
<p>To make <italic>a priori</italic> predictions of voluntary motor behaviour, we simulated center cursor jump perturbation trials. Note that center cursor jumps and target jumps result in identical behaviour in model simulations. The hand trajectory predictions for each of the four models on leftward cursor jump perturbation trials are shown for each condition in <xref ref-type="fig" rid="figS1">Supplementary Fig. S1A-P</xref>.</p>
<p>In the <italic>partner-relevant/self-irrelevant</italic> condition, we predicted that the self controller in the No Partner Representation &amp; Self Cost (<xref ref-type="fig" rid="figS1">Supplementary Fig. S1B</xref>) and Partner Representation &amp; Self Cost model (<xref ref-type="fig" rid="figS1">Supplementary Fig. S1F</xref>) would not make a lateral correction for a cursor jump. Conversely, in the same <italic>partner-relevant/self-irrelevant</italic> condition, we predicted that the self controller in the Partner Representation &amp; Equal Joint Cost (<xref ref-type="fig" rid="figS1">Supplementary Fig. S1J</xref>) and Partner Representation &amp; Weighted Joint Cost model (<xref ref-type="fig" rid="figS1">Supplementary Fig. S1Ns</xref>) would make a lateral correction for a cursor jump.</p>
<fig id="figS1" position="float" fig-type="figure">
<label>Supplementary Figure 1:</label>
<caption><title>Model Hand and Center Cursor Trajectories.</title>
<p>Predicted hand and center cursor positions during left cursor jumps for each condition and model: <bold>A-D)</bold> No Partner Representation &amp; Self Cost, <bold>E-H)</bold> Partner Representation &amp; Self Cost, <bold>I-L)</bold> Partner Representation &amp; Equal Joint Cost, and <bold>M-P)</bold> Partner Representation &amp; Weighted Joint Cost. Collectively, the self cursor in models with only a self cost do not laterally deviate to correct for the cursor jump in the <italic>partner-relevant/self-irrelevant</italic> condition. In contrast, the self cursor in models that consider a self and partner cost laterally deviate to correct for the cursor jump in the <italic>partner-relevant/self-irrelevant</italic> condition.</p></caption>
<graphic xlink:href="667240v3_figS1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Supplementary Figure 2:</label>
<caption><title>Experiment 2 Trajectories.</title>
<p><bold>A-D)</bold> Individual hand and center cursor positions of an exemplar pair for each condition in Experiment 2. Thin traces represent each trial. Thick traces represent the average across trials for the human pair. <bold>E-F)</bold> Group average hand and center cursor positions in Experiment 2. Traces represent the mean and shaded regions reflect <italic>±</italic>1 standard error of the mean. The group average behaviour in Experiment 2 closely aligns with the Partner Representation &amp; Weighted Joint Cost model (<xref ref-type="fig" rid="figS1">Supplementary Fig. S1</xref>), suggesting that voluntary behaviour reflects a partner representation and consideration of a partner’s cost.</p></caption>
<graphic xlink:href="667240v3_figS2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s7">
<title>Supplementary B: Final Lateral Hand Deviation</title>
<p>We calculated final lateral hand deviation for each participant as the average absolute difference between the endpoint hand position on regular trials and perturbation trials. <xref ref-type="fig" rid="figS3">Supplementary Fig. S3</xref> shows the predicted final lateral hand deviation from each of the four models. <xref ref-type="fig" rid="figS4">Supplementary Fig. S4</xref> shows the final lateral hand deviation in Experiment 1 (<xref ref-type="fig" rid="figS4">Supplementary Fig. S4A</xref>) and Experiment 2 (<xref ref-type="fig" rid="figS4">Supplementary Fig. S4B</xref>). We found a significant interaction between partner target and self target for both Experiment 1 (F[1,47] = 60.99, p &lt; 0.001) and Experiment 2 (F[1,47] = 42.66, p &lt; 0.001). Follow-up mean comparisons showed the same significant differences for Experiment 1 and Experiment 2. The empirical results of final lateral hand position most closely match the Partner Representation &amp; Weighted Joint Cost model, suggesting that participants form a partner representation and consider a weighted joint cost.</p>
<fig id="figS3" position="float" fig-type="figure">
<label>Supplementary Figure 3:</label>
<caption><title>Model Final Hand Lateral Deviation.</title>
<p>Final lateral hand deviation for <bold>A)</bold> No Partner Representation &amp; Self Cost <bold>B)</bold> Partner Representation &amp; Self Cost <bold>C)</bold> Partner Representation &amp; Equal Joint Cost <bold>D)</bold> Partner Representation &amp; Weighted Joint Cost. The final lateral hand deviation was calculated as the mean of the absolute value of the difference between the final hand position on non-perturbation trials and the final hand position on perturbation trials.</p></caption>
<graphic xlink:href="667240v3_figS3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Supplementary Figure 4:</label>
<caption><title>Experimental Final Hand Lateral Deviation.</title>
<p>Final lateral hand deviation for <bold>A)</bold> Experiment 1 and <bold>B)</bold> Experiment 2. The final lateral hand deviation was calculated as the mean of the absolute value of the difference between the final hand position on non-perturbation trials and the final hand position on perturbation trials. We saw significant differences between each of our statistical comparisons for both Experiment 1 and Experiment 2. The results for both experiments closely match the Partner Representation &amp; Weighted Joint Cost model (<xref ref-type="fig" rid="figS3">Supplementary Fig. S3D</xref>), showing that voluntary behaviour considers a partner representation and a weighted joint cost.</p></caption>
<graphic xlink:href="667240v3_figS4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s8">
<title>Supplementary C: Modelling</title>
<sec id="s8a">
<title>Linear Quadratic Game</title>
<p>As a reminder, we modelled our task as a linear quadratic game of the form
<disp-formula id="eqn1a">
<graphic xlink:href="667240v3_eqn1a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here the subscripts 1 and 2 respectively refers to controller 1 and 2, representing a pair of participants in our task. <italic>x</italic><sub><italic>k</italic></sub> is the state (e.g., position) of the system at time step <italic>k, A</italic> represents the task dynamics, <italic>u</italic><sub>1</sub> and <italic>u</italic><sub>2</sub> are the control signals, and <italic>B</italic><sub>1</sub> and <italic>B</italic><sub>2</sub> converts the control signals to a force that produces movement. Σ<sub><italic>u</italic></sub> is a covariance matrix that inputs additive noise to the system, representing noisy control signals (defined further below). Throughout, we describe the model with controller 1 as the self and controller 2 as the partner.</p>
<p>Controller 1 and 2 select their own control signal <inline-formula id="inline-eqn-24"><inline-graphic xlink:href="667240v3_inline24.gif" mime-subtype="gif" mimetype="image"/></inline-formula> or <inline-formula id="inline-eqn-25"><inline-graphic xlink:href="667240v3_inline25.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, which considers their respective costs. We can define an individual cost function <italic>J</italic><sub>1</sub> and <italic>J</italic><sub>2</sub> as:
<disp-formula id="eqn2a">
<graphic xlink:href="667240v3_eqn2a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn3a">
<graphic xlink:href="667240v3_eqn3a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here <italic>J</italic><sub>1</sub> is the individual cost for controller 1 (e.g., self) and <italic>J</italic><sub>2</sub> is the individual cost for controller 2 (e.g., partner). <italic>N</italic> is the final step, which represents the end of a trial. The term <italic>Q</italic> penalizes deviations of the center cursor relative to each target. We modelled i) a task-relevant target using a higher value of Q, and ii) a task-irrelevant target using a lower value of Q. The term <italic>R</italic><sub><italic>ii</italic></sub> penalizes controller <italic>i</italic>’s control signals, which can be thought of as an energetic cost. See further below and <xref rid="tbl1" ref-type="table">Table 1</xref> for the fully defined matrices <italic>Q</italic> and <italic>R</italic>. We define a joint cost function as:</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Model Parameters</title></caption>
<graphic xlink:href="667240v3_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<p>
<disp-formula id="eqn4a">
<graphic xlink:href="667240v3_eqn4a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5a">
<graphic xlink:href="667240v3_eqn5a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>α</italic><sub><italic>i</italic></sub> ∈ [0, 1] determines the degree to which controller <italic>i</italic> considers their partner’s cost function. Specifically, the term <italic>α</italic><sub><italic>i</italic></sub> was applied to both the partner’s state cost and energetic cost:
<disp-formula id="eqn6a">
<graphic xlink:href="667240v3_eqn6a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn7a">
<graphic xlink:href="667240v3_eqn7a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn8a">
<graphic xlink:href="667240v3_eqn8a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn9a">
<graphic xlink:href="667240v3_eqn9a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<inline-formula id="inline-eqn-26"><inline-graphic xlink:href="667240v3_inline26.gif" mime-subtype="gif" mimetype="image"/></inline-formula> reflects how much controller 1 considers controller 2’s state cost (i.e., controller 1’s partner stabilizing the center cursor in the partner target). <inline-formula id="inline-eqn-27"><inline-graphic xlink:href="667240v3_inline27.gif" mime-subtype="gif" mimetype="image"/></inline-formula> reflects how much controller 2 considers controller 1’s state cost (i.e., controller 2’s partner stabilizing the center cursor in the partner target). Specifically, we reasoned that each controller would only consider their partner’s state cost if their partner’s state cost was higher (i.e. higher value of an element in <italic>Q</italic>). We implemented this logic with the max function in <xref ref-type="disp-formula" rid="eqn6">Eqs. 6</xref>-<xref ref-type="disp-formula" rid="eqn7">7</xref>. That is, for controller 1, if the self target was relevant then <inline-formula id="inline-eqn-28"><inline-graphic xlink:href="667240v3_inline28.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and if the self target was irrelevant and partner target was relevant then <inline-formula id="inline-eqn-29"><inline-graphic xlink:href="667240v3_inline29.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Additionally, <italic>R</italic><sub>12</sub> reflects how much controller 1 considers controller 2’s energetic cost of movement by multiplying alpha by its own energetic cost. Likewise, <italic>R</italic><sub>21</sub> reflects how much controller 2 considers controller 1’s energetic cost of movement. We can rewrite the joint cost functions in <xref ref-type="disp-formula" rid="eqn4">Eqs. 4</xref>-<xref ref-type="disp-formula" rid="eqn5">5</xref> in the quadratic form as:
<disp-formula id="eqn10a">
<graphic xlink:href="667240v3_eqn10a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn11a">
<graphic xlink:href="667240v3_eqn11a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s8b">
<title>Linear Quadratic Game – Matrices</title>
<p>
<disp-formula id="ueqn1">
<graphic xlink:href="667240v3_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<italic>x</italic><sub>0</sub> reflects the initial state at the start of each simulation, matching the experimental design.
<disp-formula id="ueqn2">
<graphic xlink:href="667240v3_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="ueqn2a">
<graphic xlink:href="667240v3_ueqn2a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="ueqn2b">
<graphic xlink:href="667240v3_ueqn2b.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Writing out <italic>xQ</italic><sub><italic>i</italic></sub><italic>x</italic> from the cost function <italic>J</italic><sub><italic>i</italic></sub> gives the state cost:
<disp-formula id="ueqn3">
<graphic xlink:href="667240v3_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The control costs for controller 1 and controller 2 are:
<disp-formula id="ueqn4">
<graphic xlink:href="667240v3_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Note that all the parameter weightings for the state and control costs are held constant across all experimental conditions except for <inline-formula id="inline-eqn-30"><inline-graphic xlink:href="667240v3_inline30.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula id="inline-eqn-31"><inline-graphic xlink:href="667240v3_inline31.gif" mime-subtype="gif" mimetype="image"/></inline-formula> which reflected the width of the target. For values of all parameters, see <xref rid="tbl1" ref-type="table">Table 1</xref>.</p>
<sec id="s8b1">
<title>Control Policy</title>
<p>The optimal control signal for controller <inline-formula id="inline-eqn-32"><inline-graphic xlink:href="667240v3_inline32.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and controller <inline-formula id="inline-eqn-33"><inline-graphic xlink:href="667240v3_inline33.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is determined by the time-varying feedback gains <italic>F</italic><sub>1</sub> and <italic>F</italic><sub>2</sub> that minimize the joint cost function <inline-formula id="inline-eqn-34"><inline-graphic xlink:href="667240v3_inline34.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula id="inline-eqn-35"><inline-graphic xlink:href="667240v3_inline35.gif" mime-subtype="gif" mimetype="image"/></inline-formula> respectively:
<disp-formula id="eqn12a">
<graphic xlink:href="667240v3_eqn12a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn13a">
<graphic xlink:href="667240v3_eqn13a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here, <inline-formula id="inline-eqn-36"><inline-graphic xlink:href="667240v3_inline36.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for <italic>i</italic> = {1, 2} is controller <italic>i</italic>’s posterior estimate of the state (see Methods in the main manuscript). The feedback gains <italic>F</italic><sub>1</sub> and <italic>F</italic><sub>2</sub> constitute a Nash equilibrium solution to the linear quadratic game defined above (from Basar and Olsder Chapter 6, Corollary 6.1):<sup><xref ref-type="bibr" rid="sc1">S1</xref></sup>
<disp-formula id="eqn14a">
<graphic xlink:href="667240v3_eqn14a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn15a">
<graphic xlink:href="667240v3_eqn15a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Importantly, note that the solution <italic>F</italic><sub>1,<italic>k</italic></sub> contains the term <inline-formula id="inline-eqn-37"><inline-graphic xlink:href="667240v3_inline37.gif" mime-subtype="gif" mimetype="image"/></inline-formula> demonstrating knowledge of the partner’s feedback gains <italic>F</italic><sub>2,<italic>k</italic></sub>. In our modeling framework, this knowledge of the partner’s control policy reflects a partner representation.</p>
<p>Here, <italic>P</italic><sub><italic>i,k</italic></sub> is the solution to the set of coupled Riccati equations derived via dynamic programming for a linear quadratic game.
<disp-formula id="eqn16a">
<graphic xlink:href="667240v3_eqn16a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn17a">
<graphic xlink:href="667240v3_eqn17a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Note that <italic>P</italic><sub><italic>i,N</italic></sub> = <italic>Q</italic><sub><italic>i,N</italic></sub> where <italic>N</italic> is the final timestep, and <italic>P</italic><sub><italic>i,k</italic></sub> is recursively solved backwards in time.</p>
</sec>
<sec id="s8b2">
<title>State Estimation and Sensory Delays</title>
<p>Each controller receives delayed sensory feedback of its own hand position, velocity, and force, as well as the partner’s hand position and velocity. Further, each controller receives delayed sensory feedback of the center cursor position and target position. To incorporate sensory delays, we augmented the state vector with previous states.<sup><xref ref-type="bibr" rid="sc2">S2</xref>,<xref ref-type="bibr" rid="sc3">S3</xref></sup>
<disp-formula id="eqn18a">
<graphic xlink:href="667240v3_eqn18a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here <italic>δv</italic> = 110 ms to reflect the transmission delay associated with vision and aligned the model and experimental visuomotor response onset times. This value was converted into time steps in our program (<italic>n</italic><sub><italic>δv</italic></sub> = 11). To accommodate the augmented state vector, we also augmented <italic>A, B</italic><sub><italic>i</italic></sub>, and <italic>Q</italic><sub><italic>i</italic></sub>:
<disp-formula id="ueqn5">
<graphic xlink:href="667240v3_ueqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here <italic>n</italic><sub><italic>x</italic></sub> is the total number of states (16), <italic>n</italic><sub><italic>u</italic></sub> is the number of control states (2), and <italic>n</italic><sub><italic>xd</italic></sub> is the total number of time delayed states (<italic>n</italic><sub><italic>x</italic></sub> · <italic>n</italic><sub><italic>δv</italic></sub> = 176). <italic>I</italic><sub><italic>p</italic></sub> is the square identity matrix with <italic>n</italic><sub><italic>xd</italic></sub> rows and <italic>n</italic><sub><italic>xd</italic></sub> columns. 0 is a matrix with all zeros of the specified dimensions. <italic>B</italic><sup><italic>aug</italic></sup> is designed so the controllers only act on the current (non-delayed) state. <italic>Q</italic><sup><italic>aug</italic></sup> is designed so the controllers only incur a cost on the current (non-delayed) state.</p>
<p>The sensory states available to controller <italic>i</italic> is
<disp-formula id="eqn19a">
<graphic xlink:href="667240v3_eqn19a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn20a">
<graphic xlink:href="667240v3_eqn20a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>y</italic><sub>1</sub>, <italic>y</italic><sub>2</sub> are the vectors of delayed state observations for controller 1 and 2 respectively. <italic>ω</italic><sub>1,<italic>k</italic></sub> and <italic>ω</italic><sub>2,<italic>k</italic></sub> are each noise vectors whose elements are drawn from a Gaussian distribution with zero mean and covariance according to:
<disp-formula id="eqn21a">
<graphic xlink:href="667240v3_eqn21a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
See <xref rid="tbl1" ref-type="table">Table 1</xref> for the measurement noise values used in <italic>σ</italic>.</p>
<p><inline-formula id="inline-eqn-38"><inline-graphic xlink:href="667240v3_inline38.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is an observation matrix designed to selectively observe some of the delayed states. First we define <italic>C</italic><sub>1</sub> and <italic>C</italic><sub>2</sub>:
<disp-formula id="ueqn6">
<graphic xlink:href="667240v3_ueqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here controller <italic>i</italic> observes all states except their partner’s force production in both the x and y dimension. We augment both observation matrices to only observe the delayed state.
<disp-formula id="ueqn7">
<graphic xlink:href="667240v3_ueqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
To minimize extra notation, we drop the <italic>aug</italic> superscript moving forward. Like previous work, we used a linear Kalman filter to model participants sensory estimates of the state variables.<sup><xref ref-type="bibr" rid="sc4">S4</xref></sup> The posterior state estimate <inline-formula id="inline-eqn-39"><inline-graphic xlink:href="667240v3_inline39.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of controller 1 is obtained using an online filter of the form:
<disp-formula id="eqn22a">
<graphic xlink:href="667240v3_eqn22a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn23">
<graphic xlink:href="667240v3_eqn23.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here <inline-formula id="inline-eqn-40"><inline-graphic xlink:href="667240v3_inline40.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the prior prediction of the state corrupted by Gaussian noise <inline-formula id="inline-eqn-41"><inline-graphic xlink:href="667240v3_inline41.gif" mime-subtype="gif" mimetype="image"/></inline-formula> with zero mean and standard deviation of 1<italic>e</italic>-5. We assume the sensorimotor system obtains a noisy prior prediction of the states using an internal model of the state dynamics, which includes a prediction of the partner’s motor command. The prior prediction uses the previous posterior estimate <inline-formula id="inline-eqn-42"><inline-graphic xlink:href="667240v3_inline42.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the efference copy (<italic>u</italic><sub>1</sub>), and the prediction of the partner’s motor command (<italic>u</italic><sub>2</sub>)</p>
<p>The prior prediction of the state is updated using sensory measurements to obtain the posterior estimate <inline-formula id="inline-eqn-43"><inline-graphic xlink:href="667240v3_inline43.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref ref-type="disp-formula" rid="eqn22">Eq. 22</xref>). The sequence of Kalman gains <italic>K</italic><sub>1</sub> and <italic>K</italic><sub>2</sub> were updated recursively according to the classic algorithm:
<disp-formula id="eqn24">
<graphic xlink:href="667240v3_eqn24.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn25">
<graphic xlink:href="667240v3_eqn25.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn26">
<graphic xlink:href="667240v3_eqn26.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn27">
<graphic xlink:href="667240v3_eqn27.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
W is the measurement covariance matrix defined as:
<disp-formula id="ueqn8">
<graphic xlink:href="667240v3_ueqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Both controllers used the same measurement covariance matrix <italic>W</italic>. <italic>V</italic><sub><italic>i</italic></sub> is the process covariance matrix for controller <italic>i</italic> defined as:
<disp-formula id="ueqn9">
<graphic xlink:href="667240v3_ueqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="ueqn9a">
<graphic xlink:href="667240v3_ueqn9a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The process covariance is higher on the partner’s states to more heavily weigh sensory information about the partner’s states than the prediction of the partner’s states.</p>
</sec>
<sec id="s8b3">
<title>Simulating Perturbation and Probe Trials</title>
<p>We Euler integrated the state equation with a step size of <italic>h</italic> = 0.01. Based on the state cost, the controllers were required to move the center cursor into either a relevant or irrelevant target. They were required to stop the center cursor in the target at final time <italic>T</italic> = 0.8<italic>s</italic> (final timestep of <italic>N</italic> = 79). Just like the experiment, perturbations were implemented by shifting the center cursor −3cm or target +3 cm when the center cursor crossed 25% of the forward distance to the target. On probe trials, we simulated a force channel by setting the x-force element in the <italic>B</italic><sub>1</sub> and <italic>B</italic><sub>2</sub> matrices to 0. Thus, the controllers could only move the center cursor in the forward dimension. We were able to calculate the applied force for controller 1 in the lateral dimension using the original <italic>B</italic><sub>1</sub> matrix. During probe trials, the center cursor or target laterally shifted 3 cm when the center cursor crossed 25% of the distance to the target (6.25 cm), then shifted back to the original lateral position after 250 ms.</p>
</sec>
</sec>
</sec>
</app>
</app-group>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schmitz</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Vesper</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Sebanz</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Knoblich</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Co-Representation of Others’ Task Constraints in Joint Action</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>43</volume> (<issue>8</issue>), <fpage>1480</fpage>–<lpage>1493</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vesper</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Schmitz</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Knoblich</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Modulating Action Duration to Establish Nonconventional Communication</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>146</volume> (<issue>12</issue>), <fpage>1722</fpage>–<lpage>1737</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kourtis</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Knoblich</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Woźniak</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Sebanz</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Attention Allocation and Task Representation during Joint Action Planning</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>26</volume> (<issue>10</issue>), <fpage>2275</fpage>–<lpage>2286</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vesper</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>van der Wel</surname>, <given-names>R. P. R. D.</given-names></string-name>, <string-name><surname>Knoblich</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Sebanz</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Making Oneself Predictable: Reduced Temporal Variability Facilitates Joint Action Coordination</article-title>. <source>Experimental Brain Research</source>, <volume>211</volume> (<issue>3-4</issue>), <fpage>517</fpage>–<lpage>530</lpage>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Török</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Stanciu</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Sebanz</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Csibra</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Computing Joint Action Costs: Co-Actors Minimize the Aggregate Individual Costs in an Action Sequence</article-title>. <source>Open Mind</source>, <fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nashed</surname>, <given-names>J. Y.</given-names></string-name>, <string-name><surname>Crevecoeur</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Scott</surname>, <given-names>S. H.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Influence of the Behavioral Goal and Environmental Obstacles on Rapid Feedback Responses</article-title>. <source>Journal of Neurophysiology</source>, <volume>108</volume> (<issue>4</issue>), <fpage>999</fpage>–<lpage>1009</lpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sebanz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Knoblich</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Prinz</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Representing Others’ Actions: Just like One’s Own?</article-title> <source>Cognition</source>, <volume>88</volume> (<issue>3</issue>), <fpage>11</fpage>–<lpage>21</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vesper</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Schmitz</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Safra</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sebanz</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Knoblich</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2016</year>). <article-title>The Role of Shared Visual Information for Joint Action Coordination</article-title>. <source>Cognition</source>, <volume>153</volume>, <fpage>118</fpage>–<lpage>123</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leibfried</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Grau-Moya</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Braun</surname>, <given-names>D. A.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Signaling Equilibria in Sensorimotor Interactions</article-title>. <source>Cognition</source>, <volume>141</volume>, <fpage>73</fpage>–<lpage>86</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kourtis</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Woźniak</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sebanz</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Knoblich</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Evidence for We-Representations during Joint Action Planning</article-title>. <source>Neuropsychologia</source>, <volume>131</volume> (<month>April</month>), <fpage>73</fpage>–<lpage>83</lpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Todorov</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Jordan</surname>, <given-names>M. I.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Optimal Feedback Control as a Theory of Motor Coordination</article-title>. <source>Nature Neuroscience</source>, <volume>5</volume> (<issue>11</issue>), <fpage>1226</fpage>–<lpage>1235</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scott</surname>, <given-names>S. H.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Optimal Feedback Control and the Neural Basis of Volitional Motor Control</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>5</volume> (<issue>7</issue>), <fpage>532</fpage>–<lpage>544</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tanis</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Calalo</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Cashaback</surname>, <given-names>J. G. A.</given-names></string-name>, &amp; <string-name><surname>Kurtzer</surname>, <given-names>I. L.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Accuracy and Effort Costs Together Lead to Temporal Asynchrony of Multiple Motor Commands</article-title>. <source>Journal of Neurophysiology</source>, <volume>129</volume> (<issue>1</issue>), <fpage>1</fpage>–<lpage>6</lpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Knill</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Bondada</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Chhabra</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Flexible, Task-Dependent Use of Sensory Feedback to Control Hand Movements</article-title>. <source>The Journal of Neuroscience</source>, <volume>31</volume> (<issue>4</issue>), <fpage>1219</fpage>–<lpage>1237</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calalo</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Roth</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Lokesh</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sullivan</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Wong</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Semrau</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>Cashaback</surname>, <given-names>J. G. A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>The Sensorimotor System Modulates Muscular Co-Contraction Relative to Visuomotor Feedback Responses to Regulate Movement Variability</article-title>. <source>Journal of Neurophysiology</source>, <volume>129</volume> (<issue>4</issue>), <fpage>751</fpage>–<lpage>766</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Franklin</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name>, &amp; <string-name><surname>Franklin</surname>, <given-names>D. W.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Rapid Visuomotor Feedback Gains Are Tuned to the Task Dynamics</article-title>. <source>Journal of Neurophysiology</source>, <volume>118</volume> (<issue>5</issue>), <fpage>2711</fpage>–<lpage>2726</lpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Franklin</surname>, <given-names>D. W.</given-names></string-name>, &amp; <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Specificity of Reflex Adaptation for Task-Relevant Variability</article-title>. <source>The Journal of Neuroscience</source>, <volume>28</volume> (<issue>52</issue>), <fpage>14165</fpage>–<lpage>14175</lpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scott</surname>, <given-names>S. H.</given-names></string-name></person-group> (<year>2016</year>). <article-title>A Functional Taxonomy of Bottom-Up Sensory Feedback Processing for Motor Actions</article-title>. <source>Trends in Neurosciences</source>, <volume>39</volume> (<issue>8</issue>), <fpage>512</fpage>–<lpage>526</lpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smeets</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Brenner</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Fast Corrections of Movements with a Computer Mouse</article-title>. <source>Spatial Vision</source>, <volume>16</volume> (<issue>3</issue>), <fpage>365</fpage>–<lpage>376</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dimitriou</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name>, &amp; <string-name><surname>Franklin</surname>, <given-names>D. W.</given-names></string-name></person-group> (<year>2013</year>). <article-title>The Temporal Evolution of Feedback Gains Rapidly Update to Task Demands</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume> (<issue>26</issue>), <fpage>10898</fpage>–<lpage>10909</lpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Todorov</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Optimality Principles in Sensorimotor Control</article-title>. <source>Nature Neuro-science</source>, <volume>7</volume> (<issue>9</issue>), <fpage>907</fpage>–<lpage>915</lpage>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calalo</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Ngo</surname>, <given-names>T. T.</given-names></string-name>, <string-name><surname>Sullivan</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Strand</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Buggeln</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Lokesh</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Roth</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Carter</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Kurtzer</surname>, <given-names>I. L.</given-names></string-name>, &amp; <string-name><surname>Cashaback</surname>, <given-names>J. G.</given-names></string-name></person-group> (<year>2025</year>). <article-title>Online Movements Reflect Ongoing Deliberation</article-title>. <source>The Journal of Neuroscience</source>, <fpage>e1913242025</fpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lokesh</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sullivan</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>St. Germain</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Roth</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Calalo</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Buggeln</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ngo</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Marchhart</surname>, <given-names>V. R. F.</given-names></string-name>, <string-name><surname>Carter</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Cashaback</surname>, <given-names>J. G. A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Visual Accuracy Dominates over Haptic Speed for State Estimation of a Partner during Collaborative Sensorimotor Interactions</article-title>. <source>Journal of Neurophysiology</source>, <volume>130</volume> (<issue>1</issue>), <fpage>23</fpage>–<lpage>42</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Takagi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Usai</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Ganesh</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Sanguineti</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Burdet</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Haptic Communication between Humans Is Tuned by the Hard or Soft Mechanics of Interaction</article-title>. <source>PLoS Computational Biology</source>, <volume>14</volume> (<issue>3</issue>), <fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>Y</given-names></string-name>, <string-name><surname>Carboni</surname>, <given-names>G</given-names></string-name>, <string-name><surname>Gonzalez</surname>, <given-names>F</given-names></string-name>, <string-name><surname>Campolo</surname>, <given-names>D</given-names></string-name>, &amp; <string-name><surname>Burdet</surname>, <given-names>E</given-names></string-name></person-group> (<year>2019</year>). <source>Differential Game Theory for Versatile Physical Human–Robot Interaction</source>. <source>Nat Mach Intell</source> <volume>1</volume>, <fpage>36</fpage>–<lpage>43</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Vicariis</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Chackochan</surname>, <given-names>V. T.</given-names></string-name>, <string-name><surname>Bandini</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Ravaschio</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Sanguineti</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Computational Joint Action: Dynamical Models to Understand the Development of Joint Coordination</article-title>. <source>PLOS Computational Biology</source>, <volume>20</volume> (<issue>10</issue>):<fpage>e1011948</fpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chackochan</surname>, <given-names>V. T.</given-names></string-name>, &amp; <string-name><surname>Sanguineti</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Incomplete Information about the Partner Affects the Development of Collaborative Strategies in Joint Action</article-title>. <source>PLoS Computational Biology</source>, <volume>15</volume> (<issue>12</issue>), <fpage>1</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Gibson</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2014</year>). <source>The Theory of Affordances:(1979)</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Routledge</publisher-name>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cisek</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Cortical Mechanisms of Action Selection: The Affordance Competition Hypothesis</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>362</volume> (<issue>1485</issue>), <fpage>1585</fpage>–<lpage>1599</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kurtzer</surname>, <given-names>I. L.</given-names></string-name>, <string-name><surname>Pruszynski</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>Scott</surname>, <given-names>S. H.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Long-Latency Reflexes of the Human Arm Reflect an Internal Model of Limb Dynamics</article-title>. <source>Current Biology</source>, <volume>18</volume> (<issue>6</issue>), <fpage>449</fpage>–<lpage>453</lpage>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miall</surname>, <given-names>R. C.</given-names></string-name>, &amp; <string-name><surname>King</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2008</year>). <article-title>State Estimation in the Cerebellum</article-title>. <source>Cerebellum</source>, <volume>7</volume> (<issue>4</issue>), <fpage>572</fpage>–<lpage>576</lpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lillicrap</surname>, <given-names>T. P.</given-names></string-name>, &amp; <string-name><surname>Scott</surname>, <given-names>S. H.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Preference Distributions of Primary Motor Cortex Neurons Reflect Control Solutions Optimized for Limb Biomechanics</article-title>. <source>Neuron</source>, <volume>77</volume> (<issue>1</issue>), <fpage>168</fpage>–<lpage>179</lpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cothros</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Wong</surname>, <given-names>J. D.</given-names></string-name>, &amp; <string-name><surname>Gribble</surname>, <given-names>P. L.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Are There Distinct Neural Representations of Object and Limb Dynamics?</article-title> <source>Experimental Brain Research</source>, <volume>173</volume> (<issue>4</issue>), <fpage>689</fpage>–<lpage>697</lpage>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sebanz</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Knoblich</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Progress in Joint-Action Research</article-title>. <source>Current Directions in Psychological Science</source>, <volume>30</volume> (<issue>2</issue>), <fpage>138</fpage>–<lpage>143</lpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sebanz</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Knoblich</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Prediction in Joint Action: What, When, and Where</article-title>. <source>Topics in Cognitive Science</source>, <volume>1</volume> (<issue>2</issue>), <fpage>353</fpage>–<lpage>367</lpage>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ramnani</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Miall</surname>, <given-names>R. C.</given-names></string-name></person-group> (<year>2004</year>). <article-title>A System in the Human Brain for Predicting the Actions of Others</article-title>. <source>Nature Neuroscience</source>, <volume>7</volume> (<issue>1</issue>), <fpage>85</fpage>–<lpage>90</lpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xiang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Vélez</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Collaborative Decision Making Is Grounded in Representations of Other People’s Competence and Effort</article-title>. <source>J Exp Psychol Gen</source>. <volume>152</volume>:<fpage>1565</fpage>-<lpage>1579</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bicho</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Erlhagen</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Louro</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Costa E Silva</surname> <given-names>E.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Neuro-Cognitive Mechanisms of Decision Making in Joint Action: A Human–Robot Interaction Study</article-title>. <source>Human Movement Science</source>, <volume>30</volume> (<issue>5</issue>), <fpage>846</fpage>–<lpage>868</lpage>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Atmaca</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sebanz</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Knoblich</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2011</year>). <article-title>The Joint Flanker Effect: Sharing Tasks with Real and Imagined Co-Actors</article-title>. <source>Experimental Brain Research</source>, <volume>211</volume> (<issue>3</issue>), <fpage>371</fpage>–<lpage>385</lpage>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bahrami</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Olsen</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Latham</surname>, <given-names>P. E.</given-names></string-name>, <string-name><surname>Roepstorff</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rees</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Frith</surname>, <given-names>C. D.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Optimally Interacting Minds</article-title>. <source>Science</source>, <volume>329</volume> (<issue>5995</issue>), <fpage>1081</fpage>–<lpage>1085</lpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yoshida</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Saito</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Iriki</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Isoda</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Representation of Others’ Action by Neurons in Monkey Medial Frontal Cortex</article-title>. <source>Current Biology</source>, <volume>21</volume> (<issue>3</issue>), <fpage>249</fpage>–<lpage>253</lpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pruszynski</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Kurtzer</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Scott</surname>, <given-names>S. H.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Rapid Motor Responses Are Appropriately Tuned to the Metrics of a Visuospatial Task</article-title>. <source>Journal of Neurophysiology</source>, <volume>100</volume> (<issue>1</issue>), <fpage>224</fpage>–<lpage>238</lpage>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pruszynski</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Omrani</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Scott</surname>, <given-names>S. H.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Goal-Dependent Modulation of Fast Feedback Responses in Primary Motor Cortex</article-title>. <source>The Journal of Neuroscience</source>, <volume>34</volume> (<issue>13</issue>), <fpage>4608</fpage>–<lpage>4617</lpage>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pruszynski</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Kurtzer</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Nashed</surname>, <given-names>J. Y.</given-names></string-name>, <string-name><surname>Omrani</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brouwer</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Scott</surname>, <given-names>S. H.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Primary Motor Cortex Underlies Multi-Joint Integration for Fast Feedback Control</article-title>. <source>Nature</source>, <volume>478</volume> (<issue>7369</issue>), <fpage>387</fpage>–<lpage>390</lpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Day</surname>, <given-names>B. L.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Evidence for Subcortical Involvement in the Visual Control of Human Reaching</article-title>. <source>Brain</source>, <volume>124</volume> (<issue>9</issue>), <fpage>1832</fpage>–<lpage>1840</lpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Corneil</surname>, <given-names>B. D.</given-names></string-name>, &amp; <string-name><surname>Munoz</surname>, <given-names>D. P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Overt Responses during Covert Orienting</article-title>. <source>Neuron</source>, <volume>82</volume> (<issue>6</issue>), <fpage>1230</fpage>–<lpage>1243</lpage>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pruszynski</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Lillicrap</surname>, <given-names>T. P.</given-names></string-name>, &amp; <string-name><surname>Scott</surname>, <given-names>S. H.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Complex Spatiotemporal Tuning in Human Upper-Limb Muscles</article-title>. <source>Journal of Neurophysiology</source>, <volume>103</volume> (<issue>1</issue>), <fpage>564</fpage>–<lpage>572</lpage>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kozak</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name><surname>Corneil</surname>, <given-names>B. D.</given-names></string-name></person-group> (<year>2021</year>). <article-title>High-Contrast, Moving Targets in an Emerging Target Paradigm Promote Fast Visuomotor Responses during Visually Guided Reaching</article-title>. <source>Journal of Neurophysiology</source>, <volume>126</volume> (<issue>1</issue>), <fpage>68</fpage>–<lpage>81</lpage>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Contemori</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Loeb</surname>, <given-names>G. E.</given-names></string-name>, <string-name><surname>Corneil</surname>, <given-names>B. D.</given-names></string-name>, <string-name><surname>Wallis</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Carroll</surname>, <given-names>T. J.</given-names></string-name></person-group> (<year>2021</year>). <article-title>The Influence of Temporal Predictability on Express Visuomotor Responses</article-title>. <source>Journal of Neurophysiology</source>, <volume>125</volume> (<issue>3</issue>), <fpage>731</fpage>–<lpage>747</lpage>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kilner</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Lemon</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2013</year>). <article-title>What We Know Currently about Mirror Neurons</article-title>. <source>Current Biology</source>, <volume>23</volume> (<issue>23</issue>), <fpage>R1057</fpage>–<lpage>R1062</lpage>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cook</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Bird</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Catmur</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Press</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Heyes</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Mirror Neurons: From Origin to Function</article-title>. <source>Behavioral and Brain Sciences</source>, <volume>37</volume> (<issue>2</issue>), <fpage>177</fpage>–<lpage>192</lpage>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cattaneo</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Rizzolatti</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2009</year>). <article-title>The Mirror Neuron System</article-title>. <source>Archives of Neurology</source>, <volume>66</volume> (<issue>5</issue>),</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bruni</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gerbella</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bonini</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Borra</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Coudé</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Ferrari</surname>, <given-names>P. F.</given-names></string-name>, <string-name><surname>Fogassi</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Maranesi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rodà</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Simone</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Serventi</surname>, <given-names>F. U.</given-names></string-name>, &amp; <string-name><surname>Rozzi</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Cortical and Subcortical Connections of Parietal and Premotor Nodes of the Monkey Hand Mirror Neuron Network</article-title>. <source>Brain Structure and Function</source></mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rizzolatti</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Cattaneo</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Fabbri-Destro</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Rozzi</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Cortical Mechanisms Underlying the Organization of Goal-Directed Actions and Mirror Neuron-Based Action Understanding</article-title>. <source>Physiological Reviews</source>, <volume>94</volume> (<issue>2</issue>), <fpage>655</fpage>–<lpage>706</lpage>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Umiltà</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kohler</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Gallese</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Fogassi</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Fadiga</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Keysers</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Rizzolatti</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2001</year>). <article-title>I Know What You Are Doing</article-title>. <source>Neuron</source>, <volume>31</volume> (<issue>1</issue>), <fpage>155</fpage>–<lpage>165</lpage>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heyes</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Where Do Mirror Neurons Come From?</article-title> <source>Neuroscience &amp; Biobehavioral Reviews</source>, <volume>34</volume> (<issue>4</issue>), <fpage>575</fpage>–<lpage>583</lpage>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kawato</surname>, <given-names>M.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Internal Models for Motor Control and Trajectory Planning</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>9</volume> (<issue>6</issue>), <fpage>718</fpage>–<lpage>727</lpage>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Miall</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Kawato</surname>, <given-names>M.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Internal Models in the Cerebellum</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>2</volume> (<issue>9</issue>), <fpage>338</fpage>–<lpage>347</lpage>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ebner</surname>, <given-names>T. J.</given-names></string-name>, &amp; <string-name><surname>Pasalar</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Cerebellum Predicts the Future Motor State</article-title>. <source>The Cerebellum</source>, <volume>7</volume> (<issue>4</issue>), <fpage>583</fpage>–<lpage>588</lpage>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sokolov</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Miall</surname>, <given-names>R. C.</given-names></string-name>, &amp; <string-name><surname>Ivry</surname>, <given-names>R. B.</given-names></string-name></person-group> (<year>2017</year>). <article-title>The Cerebellum: Adaptive Prediction for Movement and Cognition</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>21</volume> (<issue>5</issue>), <fpage>313</fpage>–<lpage>332</lpage>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sokolov</surname>, <given-names>A. A.</given-names></string-name></person-group> (<year>2018</year>). <article-title>The Cerebellum in Social Cognition</article-title>. <source>Frontiers in Cellular Neuroscience</source>, <volume>12</volume>,</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miall</surname>, <given-names>R. C.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Connecting Mirror Neurons and Forward Models</article-title> <source>NeuroReport</source>, <volume>14</volume> (<issue>17</issue>), <fpage>2135</fpage>–<lpage>2137</lpage>.</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Doya</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Kawato</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2003</year>). <article-title>A Unifying Computational Framework for Motor Control and Social Interaction</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>358</volume> (<issue>1431</issue>), <fpage>593</fpage>–<lpage>602</lpage>.</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cisek</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Evolution of Behavioural Control from Chordates to Primates</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>377</volume> (<issue>1844</issue>),</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cisek</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Resynthesizing Behavior through Phylogenetic Refinement</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>, <volume>81</volume> (<issue>7</issue>), <fpage>2265</fpage>–<lpage>2287</lpage>.</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Vicariis</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Chackochan</surname>, <given-names>V. T.</given-names></string-name>, &amp; <string-name><surname>Sanguineti</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Game Theory and Partner Representation in Joint Action: Toward a Computational Theory of Joint Agency</article-title>. <source>Phenomenology and the Cognitive Sciences</source></mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Başar</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Olsder</surname>, <given-names>G. J.</given-names></string-name></person-group> (<year>1999</year>). <source>Dynamic Non-Cooperative Game Theory</source>. <edition>2nd</edition>. <publisher-name>SIAM’s Classics in Applied Mathematics</publisher-name>.</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crevecoeur</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Sepulchre</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Thonnard</surname>, <given-names>J.-L.</given-names></string-name>, &amp; <string-name><surname>Lefèvre</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Improving the State Estimation for Optimal Control of Stochastic Processes Subject to Multiplicative Noise</article-title>. <source>Automatica</source>, <volume>47</volume> (<issue>3</issue>), <fpage>591</fpage>–<lpage>596</lpage>.</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cashaback</surname>, <given-names>J. G. A.</given-names></string-name>, <string-name><surname>Lao</surname>, <given-names>C. K.</given-names></string-name>, <string-name><surname>Palidis</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Coltman</surname>, <given-names>S. K.</given-names></string-name>, <string-name><surname>McGregor</surname>, <given-names>H. R.</given-names></string-name>, &amp; <string-name><surname>Gribble</surname>, <given-names>P. L.</given-names></string-name></person-group> (<year>2019</year>). <article-title>The Gradient of the Reinforcement Landscape Influences Sensorimotor Learning</article-title>. <source>PLOS Computational Biology</source>, <volume>15</volume> (<issue>3</issue>):<fpage>e1006839</fpage>.</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lokesh</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sullivan</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Calalo</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Roth</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Swanik</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Carter</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Cashaback</surname>, <given-names>J. G. A.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Humans Utilize Sensory Evidence of Others’ Intended Action to Make Online Decisions</article-title>. <source>Scientific Reports</source>, <volume>12</volume> (<issue>1</issue>), <fpage>8806</fpage>.</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roth</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Lokesh</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Tang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Buggeln</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Calalo</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Sullivan</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Ngo</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Germain</surname>, <given-names>L. S.</given-names></string-name>, <string-name><surname>Carter</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Cashaback</surname>, <given-names>J. G.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Punishment Leads to Greater Sensorimotor Learning But Less Movement Variability Compared to Reward</article-title>. <source>Neuroscience</source>, <volume>540</volume>, <fpage>12</fpage>–<lpage>26</lpage>.</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roth</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Calalo</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Lokesh</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sullivan</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Grill</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Jeka</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Van Der Kooij</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Carter</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Cashaback</surname>, <given-names>J. G. A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Reinforcement-Based Processes Actively Regulate Motor Exploration along Redundant Solution Manifolds</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source>, <volume>290</volume> (<issue>2009</issue>), <fpage>20231475</fpage>.</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cashaback</surname>, <given-names>J. G. A.</given-names></string-name>, <string-name><surname>McGregor</surname>, <given-names>H. R.</given-names></string-name>, <string-name><surname>Mohatarem</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Gribble</surname>, <given-names>P. L.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Dissociating Error-Based and Reinforcement-Based Loss Functions during Sensorimotor Learning</article-title>. <source>PLOS Computational Biology</source>, <volume>13</volume> (<issue>7</issue>):<fpage>e1005623</fpage>.</mixed-citation></ref>
<ref id="dataref1"><mixed-citation publication-type="data" specific-use="generated"><person-group person-group-type="author"><string-name><surname>Seth R.Sullivan</surname></string-name>, <string-name><surname>John H.Buggeln</surname></string-name>, <string-name><surname>Jan A.Calalo</surname></string-name>, <string-name><surname>Truc T.Ngo</surname></string-name>, <string-name><surname>Jennifer A.Semrau</surname></string-name>, <string-name><surname>Michael J.Carter</surname></string-name>, <string-name><surname>Joshua G.A.Cashaback</surname></string-name></person-group> (<year iso-8601-date="2025">2025</year>) <article-title>Data - Involuntary visuomotor feedback responses reflect a representation of partner actions</article-title>. <source>figshare</source>. <pub-id pub-id-type="doi">10.6084/m9.figshare.30132088</pub-id></mixed-citation></ref>
<ref id="sc1"><label>S1.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Başar</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Olsder</surname>, <given-names>G. J.</given-names></string-name></person-group> (<year>1999</year>). <source>Dynamic Non-Cooperative Game Theory</source>. <edition>2nd</edition>. <publisher-name>SIAM’s Classics in Applied Mathematics</publisher-name>.</mixed-citation></ref>
<ref id="sc2"><label>S2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lokesh</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sullivan</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>St. Germain</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Roth</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Calalo</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Buggeln</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ngo</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Marchhart</surname>, <given-names>V. R. F.</given-names></string-name>, <string-name><surname>Carter</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Cashaback</surname>, <given-names>J. G. A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Visual Accuracy Dominates over Haptic Speed for State Estimation of a Partner during Collaborative Sensorimotor Interactions</article-title>. <source>Journal of Neurophysiology</source>, <volume>130</volume> (<issue>1</issue>), <fpage>23</fpage>–<lpage>42</lpage>.</mixed-citation></ref>
<ref id="sc3"><label>S3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crevecoeur</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Sepulchre</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Thonnard</surname>, <given-names>J.-L.</given-names></string-name>, &amp; <string-name><surname>Lefèvre</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Improving the State Estimation for Optimal Control of Stochastic Processes Subject to Multiplicative Noise</article-title>. <source>Automatica</source>, <volume>47</volume> (<issue>3</issue>), <fpage>591</fpage>–<lpage>596</lpage>.</mixed-citation></ref>
<ref id="sc4"><label>S4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Todorov</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Jordan</surname>, <given-names>M. I.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Optimal Feedback Control as a Theory of Motor Coordination</article-title>. <source>Nature Neuroscience</source>, <volume>5</volume> (<issue>11</issue>), <fpage>1226</fpage>–<lpage>1235</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109734.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Wei</surname>
<given-names>Kunlin</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study combines a two-person joint hand-reaching paradigm with game-theoretical modeling to examine whether, and how, one's reflexive visuomotor responses are modulated by a partner's control policy and cost structure. The study provides a <bold>solid</bold> and novel set of behavioral findings suggesting that involuntary visuomotor feedback is indeed modulated in the context of interpersonal coordination. The work will be of interest to cognitive scientists studying the motoric and social aspects of action control.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109734.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Sullivan and colleagues examined the modulation of reflexive visuomotor responses during collaboration between pairs of participants performing a joint reaching movement to a target. In their experiments, the players jointly controlled a cursor that they had to move towards narrow or wide targets. In each experimental block, each participant had a different type of target they had to move the joint cursor to. During the experiment, the authors used lateral perturbation of the cursor to test participants' fast feedback responses to the different target types. The authors suggest participants integrate the target type and related cost of their partner into their own movements, which suggests that visuomotor gains are affected by the partner's task.</p>
<p>Strengths:</p>
<p>The topic of the manuscript is very interesting, and the authors are using well-established methodology to test their hypothesis. They combine experimental studies with optimal control models to further support their work. Overall, the manuscript is very timely and shows important findings - that the feedback responses reflect both our and our partner's tasks.</p>
<p>Weaknesses:</p>
<p>However, in the current version of the manuscript, I believe the results could also be interpreted differently, which suggests that the authors should provide further support for their hypothesis and conclusions.</p>
<p>Major Comments:</p>
<p>(1) Results of the relevant conditions:</p>
<p>In addition to the authors' explanation regarding the results, it is also possible that the results represent a simple modulation of the reflexive response to a scaled version of cursor movement. That is, when the cursor is partially controlled by a partner, which also contributes to reducing movement error, it can also be interpreted by the sensorimotor system as a scaling of hand-to-cursor movement. In this case, the reflexes are modulated according to a scaling factor (how much do I need to move to bring the cursor to the target). I believe that a single-agent simulation of an OFC model with a scaling factor in the lateral direction can generate the same predictions as those presented by the authors in this study. In other words, maybe the controller has learned about the nature of the perturbation in each specific context, that in some conditions I need to control strongly, whereas in others I do not (without having any model of the partner). I suggest that the authors demonstrate how they can distinguish their interpretation of the results from other explanations.</p>
<p>(2) The effect of the partner target:</p>
<p>The authors presented both self and partner targets together. While the effect of each target type, presented separately, is known, it is unclear how presenting both simultaneously affects individual response. That is, does a small target with a background of the wide target affect the reflexive response in the case of a single participant moving? The results of Experiment 2, comparing the case of partner- and self-relevant targets versus partner-irrelevant and self-relevant targets, may suggest that the system acted based on the relevant target, regardless of the presence and instructions regarding the self-target.</p>
<p>(3) Experiment instructions:</p>
<p>It is unclear what the general instructions were for the participants and whether the instructions provided set the proposed weighted cost, which could be altered with different instructions.</p>
<p>(4) Some work has shown that the gain of visuomotor feedback responses reflects the time to target and that this is updated online after a perturbation (Cesonis &amp; Franklin, 2020, eNeuro; Cesonis and Franklin, 2021, NBDT; also related to Crevecoeur et al., 2013, J Neurophysiol). These models would predict different feedback gains depending on the distance remaining to the target for the participant and the time to correct for the jump, which is directly affected by the small or large targets. Could this time be used to target instead of explaining the results? I don't believe that this is the case, but the authors should try to rule out other interpretations. This is maybe a minor point, but perhaps more important is the location (&amp; time remaining) for each participant at the time of the jump. It appears from the figures that this might be affected by the condition (given the change in movement lengths - see Figure 3 B &amp; C). If this is the case, then could some of the feedback gain be related to these parameters and not the model of the partner, as suggested? Some evidence to rule this out would be a good addition to the paper - perhaps the distance of each partner at the time of the perturbation, for example. In addition, please analyze the synchrony of the two partners' movements.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109734.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Sullivan and colleagues studied the fast, involuntary, sensorimotor feedback control in interpersonal coordination. Using a cleverly designed joint-reaching experiment that separately manipulated the accuracy demands for a pair of participants, they demonstrated that the rapid visuomotor feedback response of a human participant to a sudden visual perturbation is modulated by his/her partner's control policy and cost. The behavioral results are well-matched with the predictions of the optimal feedback control framework implemented with the dynamic game theory model. Overall, the study provides an important and novel set of results on the fast, involuntary feedback response in human motor control, in the context of interpersonal coordination.</p>
<p>Review:</p>
<p>Sullivan and colleagues investigated whether fast, involuntary sensorimotor feedback control is modulated by the partner's state (e.g., cost and control policy) during interpersonal coordination. They asked a pair of participants to make a reaching movement to control a cursor and hit a target, where the cursor's position was a combination of each participant's hand position. To examine fast visuomotor feedback response, the authors applied a sudden shift in either the cursor (experiment 1) or the target (experiment 2) position in the middle of movement. To test the involvement of partner's information in the feedback response, they independently manipulated the accuracy demand for each participant by varying the lateral length of the target (i.e., a wider/narrower target has a lower/higher demand for correction when movement is perturbed). Because participants could also see their partner's target, they could theoretically take this information (e.g., whether their partner would correct, whether their correction would help their partner, etc.) into account when responding to the sudden visual shift. Computationally, the task structure can be handled using dynamic game theory, and the partner's feedback control policy and cost function are integrated into the optimal feedback control framework. As predicted by the model, the authors demonstrated that the rapid visuomotor feedback response to a sudden visual perturbation is modulated by the partner's control policy and cost. When their partner's target was narrow, they made rapid feedback corrections even when their own target was wide (no need for correction), suggesting integration of their partner's cost function. Similarly, they made corrections to a lesser degree when both targets were narrower than when the partner's target was wider, suggesting that the feedback correction takes the partner's correction (i.e., feedback control policy) into account.</p>
<p>The strength of the current paper lies in the combination of clever behavioral experiments that independently manipulate each participant's accuracy demand and a sophisticated computational approach that integrates optimal feedback control and dynamic game theory. Both the experimental design and data analysis sound good. While the main claim is well-supported by the results, the only current weakness is the lack of discussion of limitations and an alternative explanation. Adding these points will further strengthen the paper.</p>
</body>
</sub-article>
</article>