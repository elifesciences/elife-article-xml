<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">89947</article-id>
<article-id pub-id-type="doi">10.7554/eLife.89947</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89947.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Improved clinical data imputation via classical and quantum determinantal point processes</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Kazdaghli</surname>
<given-names>Skander</given-names>
</name>
    <email>skander.kazdaghli@gmail.com</email>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kerenidis</surname>
<given-names>Iordanis</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kieckbusch</surname>
<given-names>Jens</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Teare</surname>
<given-names>Philip</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<aff id="a1"><label>1</label><institution>QC Ware</institution>, <addr-line>Palo Alto</addr-line>, <country>USA</country> and <addr-line>Paris</addr-line>, <country>France</country></aff>
<aff id="a2"><label>2</label><institution>Université de Paris</institution>, <addr-line>CNRS, IRIF, 8 Place Aurélie Nemours, Paris 75013</addr-line>, <country>France</country></aff>
<aff id="a3"><label>3</label><institution>Emerging Innovations Unit, BioPharmaceuticals R&amp;D, AstraZeneca</institution>, <addr-line>Cambridge</addr-line>, <country>UK</country></aff>
<aff id="a4"><label>4</label><institution>Centre for AI, BioPharmaceuticals R&amp;D</institution>, <addr-line>AstraZeneca, Cambridge</addr-line>, <country>UK</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Graña</surname>
<given-names>Martin</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Institut Pasteur de Montevideo</institution>
</institution-wrap>
<city>Montevideo</city>
<country>Uruguay</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Walczak</surname>
<given-names>Aleksandra M</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>CNRS</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<pub-date pub-type="epub">
<day>31</day>
<month>03</month>
<year>2023</year>
</pub-date>
<pub-date date-type="original-publication" iso-8601-date="2023-08-23">
<day>23</day>
<month>08</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP89947</elocation-id>
<history><date date-type="sent-for-review" iso-8601-date="2023-06-09">
<day>09</day>
<month>06</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-03-31">
<day>31</day>
<month>03</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2303.17893"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Kazdaghli et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Kazdaghli et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-89947-v1.pdf"/>
<abstract>
<p>Imputing data is a critical issue for machine learning practitioners, including in the life sciences domain, where missing clinical data is a typical situation and the reliability of the imputation is of great importance. Currently, there is no canonical approach for imputation of clinical data and widely used algorithms introduce variance in the downstream classification. Here we propose novel imputation methods based on determinantal point processes that enhance popular techniques such as the Multivariate Imputation by Chained Equations (MICE) and MissForest. Their advantages are two-fold: improving the quality of the imputed data demonstrated by increased accuracy of the downstream classification; and providing deterministic and reliable imputations that remove the variance from the classification results. We experimentally demonstrate the advantages of our methods by performing extensive imputations on synthetic and real clinical data. We also develop quantum circuits for implementing determinantal point processes, since such quantum algorithms provide a computational advantage with respect to classical ones. We demonstrate competitive results with up to ten qubits for small-scale imputation tasks on a state-of-the-art IBM quantum processor. Our classical and quantum methods improve the effectiveness and robustness of clinical data prediction modeling by providing better and more reliable data imputations. These improvements can add significant value in settings where where high precision is critical, such as in pharmaceutical drug trials where our approach can provide higher confidence in the predictions made.</p>
</abstract>

</article-meta>
</front>
<body>
<p>Missing data is a recurring problem in machine learning and in particular for clinical datasets, where it is common that numerous feature values are not present for reasons including incomplete data collection, discrepancies in data formats and data corruption [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>]. Machine learning is routinely used in life science and clinical research for prediction tasks, such as diagnostics [<xref ref-type="bibr" rid="c22">22</xref>] and prognostics [<xref ref-type="bibr" rid="c2">2</xref>], as well as estimation tasks, such as biomarker proxies [<xref ref-type="bibr" rid="c28">28</xref>] and digital biomarkers [<xref ref-type="bibr" rid="c23">23</xref>]. Beyond the research setting, machine learning is becoming more commonplace as regulated Software as a Medical Device, where machine learning models are influencing - or making - clinical decisions that affect patient care.</p>
<p>Machine learning algorithms typically require complete data sets and missing values can significantly affect the quality of the machine learning models trained on such data. This is in large part due to the fact that there can be different underlying reasons for the missingness: for example, feature values can be missing completely at random (MCAR), missing at random (MAR) and missing not at random (MNAR), each one with their own characteristics.</p>
<p>Despite its importance for clinical trials, there is no canonical approach for dealing with missingness and finding appropriate, effective and reproducible methods remains a challenge. A common way to deal with missing clinical data is to exclude subjects that do not have the complete set of feature values present. A drawback of this approach is that excluding subjects can in fact introduce significant biases in the final model. For example, it can result in the model being trained to be more effective for the type of subjects that are likely to have complete data than for those that do not. Moreover, the effectiveness and reliability of clinical trials is reduced when subjects with missing feature values are excluded from the clinical trial.</p>
<p>Data imputation is an alternative to the complete dataset approach, where subjects with missing feature values are not excluded. Instead, missing values are imputed to create a complete dataset. There are different ways to achieve this, including “filling” missing values with zeros, or with the mean value of the feature across all subjects that have such a value present. These methods provide consistent imputation results, but there are important caveats for using such simple methods, since they ignore possible correlations between features and can make the dataset appear more homogeneous than it really is. More advanced data imputation methods have been proposed in the literature: iterative methods include the multivariate imputation by chained equations (MICE) [<xref ref-type="bibr" rid="c27">27</xref>] and MissForest [<xref ref-type="bibr" rid="c25">25</xref>] algorithms, and deep learning methods include GAIN (generative adversarial imputation nets) [<xref ref-type="bibr" rid="c29">29</xref>] and MIWAE (missing data importance-weighted autoencoder) [<xref ref-type="bibr" rid="c17">17</xref>]. Recent results [<xref ref-type="bibr" rid="c24">24</xref>] have shown that for clinical data two iterative imputation methods, MiceRanger, that uses predictive mean matching, and MissForest, that uses Random Forests to predict the missing values of each feature using the other features, provide the best results and have been used here as a baseline.</p>
<p>Several metrics are routinely used to quantify the quality of data imputation: point-wise discrepancy measures include root mean square error (RMSE), mean absolute error (MAE) and coefficient of determination (R<sup><xref ref-type="bibr" rid="c2">2</xref></sup>). Feature-wise discrepancy measures include Kullback-Leibler divergence, two-sample Kolmogorov-Smirnov statistic or (2-)Wasserstein distance. Ultimately, the quality and reliability of imputations can be measured by the performance of a downstream predictor, which is usually the AUC (area under the receiver operating curve) for a classification task. In practical terms, the performance of the downstream classifier is usually of highest importance for clinical data sets: for example, in one of our datasets, the classifier denominates a binary outcome of a critical care unit stay (e.g. survival) for each patient. Accordingly, we have used AUC for the classification task here to assess the performance of our novel methods.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>FIG. 1:</label>
<caption><p>Example of overall workflow for patient management through clinical data imputation and downstream classification</p></caption>
<graphic xlink:href="2303.17893v1_fig1.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<p>An inherent feature of standard Random Forest and determinantal sampling algorithms is randomness that produces data imputations that vary from one run of the algorithm to the next. This is often undesirable, since the downstream classification performance can also be affected. This motivated us to develop a deterministic version of determinantal sampling within the Random Forests of the imputation methods to provide robust and reliable imputation methods. Determinantal sampling, based on determinant point processes (DPP) [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c14">14</xref>], favors samples that are diverse and thus reduces the variance of the training of each decision tree, which in turn provides more accurate models. In essence, determinantal sampling picks subsets of data according to a distribution that gives more weight to subsets of data that contain diverse datapoints. More precisely, each subset of datapoints is picked according to the volume encapsulated by these datapoints. The determinantal distribution increases the attention given to uncommon or out-of-the-ordinary data points rather than biasing the learning process towards the more commonly found data, which can improve the overall prediction accuracy in particular for unbalanced datasets as is often the case for clinical data [<xref ref-type="bibr" rid="c6">6</xref>]. Determinantal sampling for regression and classification tasks with full data has been proposed previously for linear regressors [<xref ref-type="bibr" rid="c18">18</xref>] and for Random Forest training for a financial data classification use case where it outperformed the standard Random Forest model [<xref ref-type="bibr" rid="c26">26</xref>].</p>
<p>Through deterministic determinantal sampling we address two challenges in data imputation: first, we provide improved data imputation methods that can increase the performance of the downstream classifier; and second, we remove the variance of the common stochastic and multiple imputation methods, thus ensuring reproducibility, easier integration in machine learning workflows, and compliance with healthcare regulations. While these improvements are of particular relevance for clinical data, our algorithms can also be advantageous for other imputation tasks where improving downstream classification and removing variance is of importance.</p>
<p>In addition, we explore the potential of quantum computing to speed up these novel imputation methods: we demonstrate a quantum circuit implementation of the determinantal sampling algorithm that offers a computational advantage compared to its classical counterpart. The best classical algorithms for determinantal sampling take in practice cubic time in the number of features to provide a sample [<xref ref-type="bibr" rid="c6">6</xref>]. In contrast, the quantum algorithm we present here, based on theoretical analysis in [<xref ref-type="bibr" rid="c12">12</xref>], has running time only linear in the number of features, where running time is measured as the depth of the necessary quantum circuits. This suggests that with the advent of next generation quantum computers with more and better qubits, one could also expect a computational speedup in performing determinantal sampling using a quantum computer. Here, we demonstrate competitive results with up to ten qubits for small-scale imputation tasks on a state-of-the-art IBM quantum processor.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>FIG. 2:</label>
<caption><p>Imputation and downstream classification procedure to benchmark the imputation method’s performance.</p>
<p>First, the imputer is trained on the whole observed dataset X as shown in step (a). In step (b), the imputed data is split into 3 consecutive folds (holdout sets H1, H2 and H3) then a classifier is trained on each combination of 2 holdout sets (development sets D1, D2 and D3) and the AUC is calculated for each holdout set.</p></caption>
<graphic xlink:href="2303.17893v1_fig2.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<sec id="s1" sec-type="results">
<title>Results</title>
<p>We provide in Methods a detailed description of our four imputation methods, DPP-MICE, DPP-MissForest, detDPP-MICE and detDPP-MissForest. All of them are based on iterative imputation methods that use the observed values of every column to predict the missing values. The model used to fill missing values in each column is the Random Forest classifier. Our imputation methods replace the standard Random Forest used by the original miceRanger and MissForest imputers by the DPP-Random Forest model, for our first two imputers, and the detDPP-Random Forest for the latter two. The DPP-Random Forest model subsamples the data for each decision tree using determinantal sampling instead of uniform sampling, while the detDPP-Random Forest model deterministically picks for each decision tree the subset of data that has the maximum probability according to the determinantal distribution. We also describe a computationally advantageous way to perform the determinantal sampling using efficient quantum circuits.</p>
<p>In order to benchmark the different imputation methods, we used two types of datasets with a categorical outcome variable. First, a synthetic dataset, created using the scikit-learn method make classification. It consists of 2000 rows with 25 informative features. This is useful to study the imputation quality where features have equal importance. Second, the MIMIC-III dataset [<xref ref-type="bibr" rid="c11">11</xref>]: The Medical Information Mart for Intensive Care (MIMIC) dataset which is a freely available clinical database. It is comprised of data for patients who stayed in critical care units at the Beth Israel Deaconess Medical Center between 2001 and 2012. It contains the data of 7214 patients with 14 features.</p>
<p>We also applied two types of missingness on these datasets: MCAR (missing completely at random), where the missingness distribution is independent of any observed or unobserved variable; and MNAR (missing not at random), where the missingness distribution depends on the outcome variable.</p>
<p>We present the numerical results in terms of the AUC of the downstream classification task in <xref ref-type="table" rid="tbl1">Table I</xref> and provide graphs of the results in <xref ref-type="table" rid="tbl2">tables II</xref> and <xref ref-type="table" rid="tbl3">III</xref>. Each experiment was run ten times with different random seeds to get the variance of the results.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>TABLE I:</label>
<caption><p>AUC results for the SYNTH and MIMIC-III datasets, with MCAR and MNAR missingness, three holdout sets, and six different imputation methods. Values are expressed as mean ± SD (standard deviation) of 10 values for each experiment. DPP-MICE and detDPP-MICE are bold when outperforming MICE and the underlined one is the best of the three. DPP-MissForest and detDPP-MissForest are bold when outperforming MissForest and the underlined one is the best of the three.</p></caption>
<alternatives>
<graphic xlink:href="2303.17893v1_tbl1.jpg" mimetype="image" mime-subtype="jpeg"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="center" valign="top">Dataset</th>
<th align="center" valign="top">Missingness</th>
<th align="center" valign="top">Set</th>
<th align="center" valign="top">MICE</th>
<th align="center" valign="top">DPP-MICE</th>
<th align="center" valign="top">detDPP-MICE</th>
<th align="center" valign="top">MissForest</th>
<th align="center" valign="top">DPP-MissForest</th>
<th align="center" valign="top">detDPP-MissForest</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" valign="top" rowspan="6">SYNTH</td>
<td align="center" valign="top" rowspan="3">MCAR</td>
<td align="center" valign="top">H1</td>
<td align="center" valign="top">0.8318±0.0113</td>
<td align="center" valign="top"><bold>0.835±0.0083</bold></td>
<td align="center" valign="top"><underline><bold>0.8352</bold></underline></td>
<td align="center" valign="top">0.8525±0.0044</td>
<td align="center" valign="top"><bold>0.8552±0.0049</bold></td>
<td align="center" valign="top"><underline><bold>0.8582</bold></underline></td>
</tr>
<tr>
<td align="center" valign="top">H2</td>
<td align="center" valign="top">0.8316±0.008</td>
<td align="center" valign="top"><bold>0.8369±0.0128</bold></td>
<td align="center" valign="top"><underline><bold>0.84</bold></underline></td>
<td align="center" valign="top">0.8465±0.0057</td>
<td align="center" valign="top"><bold>0.849±0.003</bold></td>
<td align="center" valign="top"><underline><bold>0.8491</bold></underline></td>
</tr>
<tr>
<td align="center" valign="top">H3</td>
<td align="center" valign="top">0.8205±0.0127</td>
<td align="center" valign="top"><bold>0.8266±0.0096</bold></td>
<td align="center" valign="top"><underline><bold>0.8272</bold></underline></td>
<td align="center" valign="top">0.8436±0.0031</td>
<td align="center" valign="top"><bold>0.8452±0.0048</bold></td>
<td align="center" valign="top"><underline><bold>0.855</bold></underline></td>
</tr>
<tr>
<td align="center" valign="top" rowspan="3">MNAR</td>
<td align="center" valign="top">H1</td>
<td align="center" valign="top">0.8903±0.0046</td>
<td align="center" valign="top"><bold>0.8915±0.007</bold></td>
<td align="center" valign="top"><bold><underline>0.8934</underline></bold></td>
<td align="center" valign="top">0.7133±0.0063</td>
<td align="center" valign="top"><bold>0.7171±0.01</bold></td>
<td align="center" valign="top"><bold><underline>0.7185</underline></bold></td>
</tr>
<tr>
<td align="center" valign="top">H2</td>
<td align="center" valign="top">0.8755±0.01</td>
<td align="center" valign="top">0.8745±0.0072</td>
<td align="center" valign="top"><bold><underline>0.8955</underline></bold></td>
<td align="center" valign="top">0.7052±0.0036</td>
<td align="center" valign="top"><bold>0.7124±0.0078</bold></td>
<td align="center" valign="top"><bold><underline>0.7167</underline></bold></td>
</tr>
<tr>
<td align="center" valign="top">H3</td>
<td align="center" valign="top">0.9003±0.0059</td>
<td align="center" valign="top"><bold>0.9005±0.006</bold></td>
<td align="center" valign="top"><bold><underline>0.9041</underline></bold></td>
<td align="center" valign="top">0.769±0.0103</td>
<td align="center" valign="top"><bold>0.7773±0.0129</bold></td>
<td align="center" valign="top"><bold><underline>0.7905</underline></bold></td>
</tr>
<tr>
<td align="center" valign="top" rowspan="6">MIMIC</td>
<td align="center" valign="top" rowspan="3">MCAR</td>
<td align="center" valign="top">H1</td>
<td align="center" valign="top">0.7621±0.0046</td>
<td align="center" valign="top"><bold>0.7628±0.0049</bold></td>
<td align="center" valign="top"><bold><underline>0.7641</underline></bold></td>
<td align="center" valign="top">0.7687±0.0012</td>
<td align="center" valign="top"><bold>0.77±0.0013</bold></td>
<td align="center" valign="top"><bold><underline>0.771</underline></bold></td>
</tr>
<tr>
<td align="center" valign="top">H2</td>
<td align="center" valign="top">0.7541±0.0037</td>
<td align="center" valign="top">0.7532±0.0047</td>
<td align="center" valign="top"><bold><underline>0.7619</underline></bold></td>
<td align="center" valign="top">0.7649±0.0019</td>
<td align="center" valign="top"><bold>0.777<underline>±</underline>0.0019</bold></td>
<td align="center" valign="top"><bold><underline>0.7707</underline></bold></td>
</tr>
<tr>
<td align="center" valign="top">H3</td>
<td align="center" valign="top">0.7365±0.0055</td>
<td align="center" valign="top"><bold>0.7394±0.0052</bold></td>
<td align="center" valign="top"><bold><underline>0.7471</underline></bold></td>
<td align="center" valign="top">0.7485±0.001</td>
<td align="center" valign="top"><bold>0.7507±0.0017</bold></td>
<td align="center" valign="top"><bold><underline>0.7515</underline></bold></td>
</tr>
<tr>
<td align="center" valign="top" rowspan="3">MNAR</td>
<td align="center" valign="top">H1</td>
<td align="center" valign="top">0.77±0.0026</td>
<td align="center" valign="top"><bold>0.7717±0.0036</bold></td>
<td align="center" valign="top"><bold><underline>0.7722</underline></bold></td>
<td align="center" valign="top">0.6616±0.0065</td>
<td align="center" valign="top"><bold>0.6715±0.07</bold></td>
<td align="center" valign="top"><bold><underline>0.6760</underline></bold></td>
</tr>
<tr>
<td align="center" valign="top">H2</td>
<td align="center" valign="top">0.777±0.0064</td>
<td align="center" valign="top"><bold>0.7818<underline>±</underline>0.0029</bold></td>
<td align="center" valign="top"><bold><underline>0.7812</underline></bold></td>
<td align="center" valign="top">0.6748±0.0045</td>
<td align="center" valign="top"><bold>0.6778±0.0048</bold></td>
<td align="center" valign="top"><bold><underline>0.6798</underline></bold></td>
</tr>
<tr>
<td align="center" valign="top">H3</td>
<td align="center" valign="top">0.7324±0.0047</td>
<td align="center" valign="top"><bold>0.7363±0.0031</bold></td>
<td align="center" valign="top"><bold><underline>0.7403</underline></bold></td>
<td align="center" valign="top">0.6368±0.0034</td>
<td align="center" valign="top"><bold>0.64±0.004</bold></td>
<td align="center" valign="top"><bold><underline>0.6419</underline></bold></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>TABLE II:</label>
<caption>
<p>AUC results on the different holdout sets after imputation using MICE, DPP-MICE and detDPP-MICE.In the case of MICE and DPP-MICE, the boxplots correspond to 10 AUC values for 10 iterations of the same imputation and classification algorithms, depicting the lower and upper quartiles as well as the median of these 10 values. The AUC values are the same for every iteration of the detDPP-MICE algorithm.</p>
</caption>
<alternatives>
<graphic xlink:href="2303.17893v1_tbl2.jpg" mimetype="image" mime-subtype="jpeg"/>
<table frame="void" rules="all">
<thead>
<tr>
<th align="center" valign="top"/>
<th align="center" valign="top">MCAR</th>
<th align="center" valign="top">MNAR</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" valign="top">SYNTH</td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig1.jpg" mimetype="image" mime-subtype="jpeg"/></td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig2.jpg" mimetype="image" mime-subtype="jpeg"/></td>
</tr>
<tr>
<td align="center" valign="top">MIMIC</td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig3.jpg" mimetype="image" mime-subtype="jpeg"/></td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig4.jpg" mimetype="image" mime-subtype="jpeg"/></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>TABLE III</label>
<caption>
<p>AUC results on the different holdout sets after imputation using MissForest, DPP-MissForest and detDPP-MissForest. In the case of MissForest and DPP-MissForest, the boxplots correspond to 10 AUC values for 10 iterations of the same imputation and classification algorithm, depicting the lower and upper quartiles as well as the median of these 10 values. The AUC values are always the same for every iteration of the detDPP-MissForest algorithm.</p>
</caption>
<alternatives>
<graphic xlink:href="2303.17893v1_tbl3.jpg" mimetype="image" mime-subtype="jpeg"/>
<table frame="void" rules="all">
<thead>
<tr>
<th align="center" valign="top"/>
<th align="center" valign="top">MCAR</th>
<th align="center" valign="top">MNAR</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" valign="top">SYNTH</td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig5.jpg" mimetype="image" mime-subtype="jpeg"/></td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig6.jpg" mimetype="image" mime-subtype="jpeg"/></td>
</tr>
<tr>
<td align="center" valign="top">MIMIC</td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig7.jpg" mimetype="image" mime-subtype="jpeg"/></td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig8.jpg" mimetype="image" mime-subtype="jpeg"/></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Overall, DPP-MICE and DPP-MissForest provide improved results compared to their classical baseline MICE and MissForest. This is the case for both the synthetic and the MIMIC datasets and for both MCAR and MNAR missingness. Even more interestingly, the detDPP-MICE and detDPP-MissForest collapse the variance of the imputed data to 0 and moreover lead in most cases to even higher AUC than the expectation of the previous methods.</p>
<sec id="s1a">
<title>DPP-MICE, and detDPP-MICE outperform MICE</title>
<p>We present the performance results of MICE-based methods in terms of the AUC of the downstream classification task using an XGBoost classifier, which has been shown to be the strongest classifier for such datasets [<xref ref-type="bibr" rid="c24">24</xref>]. In each case, the original dataset with induced missing values is imputed using MICE, DPP-MICE or detDPP-MICE, then it is divided into 3 folds of Development/Holdout sets. The downstream classifier is then trained on each development set and its performance is measured by the AUC for the corresponding holdout set. The results appear in <xref ref-type="table" rid="tbl1">Table I</xref> and in the figures in <xref ref-type="table" rid="tbl2">Table II</xref>.</p>
<p>The imputation procedure is performed for a total of 10 iterations over the whole columns and for each column, a (DPP) Random Forest regressor is trained using 10 trees. For each Random Forest training, the dataset is divided into batches of 150 points each and DPPs are used to sample from every batch.</p>
<p>The results show that across the twelve in total dataset experiments DPP-MICE outperforms MICE on expectation in ten of them, while detDPP-MICE provides a single deterministic imputation which outperforms the expected result from MICE in all twelve datasets and from DPP-MICE eleven out of twelve times.</p>
</sec>
<sec id="s1b">
<title>DPP-MissForest and detDPP-MissForest outperform MissForest</title>
<p>Here we present the performance results of MissForest-based methods in terms of the AUC of the downstream classification task using again an XGBoost classifier. In each case, the original dataset with induced missing values is imputed using MissForest, DPP-MissForest or detDPP-MissForest, then it is divided into 3 folds of Development/Holdout sets. The downstream classifier is again then trained on each development set and its performance is measured by the AUC for the corresponding holdout set. The results appear in <xref ref-type="table" rid="tbl1">Table I</xref> and in the figures in <xref ref-type="table" rid="tbl3">Table III</xref>. The specifics of the Random Forest training are the same as in the case of MICE.</p>
<p>The results show that across all experiments, DPP-MissForest outperforms MissForest in all twelve of them, while detDPP-MissForest provides a single deterministic imputation which outperforms the expected result from MissForest in all twelve datasets and from DPP-MissForest in eleven out of twelve times.</p>
</sec>
<sec id="s1c">
<title>Quantum hardware implementation of DPP-MissForest results in competitive downstream classification</title>
<p>As we describe in Methods, quantum computers can in principle be used to offer a computational advantage in determinantal sampling. In order to better understand the state-of-the-art of current quantum hardware, we used a currently available quantum computer to perform determinantal sampling within a DPP-MissForest imputation method for scaled-down versions of the synthetic and MIMIC datasets.</p>
<list list-type="bullet">
<list-item><p>Reduced synthetic dataset: 100 points and 3 features, created using the sklearn method make classification.</p></list-item>
<list-item><p>Reduced MIMIC dataset: 200 points and 3 features. The three features were chosen from the original dataset features based on low degree of missingness and their predictiveness of the downstream classifier and they were: “Oxygen saturation std”, “Oxygen saturation mean” and “Diastolic blood pressure mean”.</p></list-item>
</list>
<p>For the purposes of our experiments, we used the “ibm hanoi” 27-qubit quantum processor shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. We implemented quantum circuits with up to 10 qubits. The decision trees of the DPP-Random Forests used by the imputation models are trained using batches of decreasing sizes (see <xref ref-type="table" rid="tbl4">Table IV</xref>). For example, for the algorithm with batch size equal to 10, the algorithm first samples two out of the ten data points to use for the first decision tree, then from the remaining eight datapoints it picks another two for the second tree, then two from the remaining six, and last two from the remaining four. In other words, we train four different trees, and each time we use a quantum circuit with number of qubits equal to 10, 8, 6, and 4, to perform the respective determinantal sampling.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>FIG. 3:</label>
<caption><p>IBM Hanoi 27-qubit quantum processor</p></caption>
<graphic xlink:href="2303.17893v1_fig3.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>TABLE IV:</label>
<caption>
<p>Data matrix sizes used by the quantum DPP circuits to train each tree. The number of rows corresponds to the number of data points and is equal to the number of qubits of every circuit.</p>
</caption>
<alternatives>
<graphic xlink:href="2303.17893v1_tbl4.jpg" mimetype="image" mime-subtype="jpeg"/>
<table frame="void" rules="all">
<thead>
<tr>
<th align="left" valign="top">Batch Size</th>
<th align="left" valign="top">Tree 1</th>
<th align="left" valign="top">Tree 2</th>
<th align="left" valign="top">Tree 3</th>
<th align="left" valign="top">Tree 4</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" valign="top">7</td>
<td align="center" valign="top">(7,2)</td>
<td align="center" valign="top">(5,2)</td>
<td align="center" valign="top">-</td>
<td align="center" valign="top">-</td>
</tr>
<tr>
<td align="center" valign="top">8</td>
<td align="center" valign="top">(8,2)</td>
<td align="center" valign="top">(6,2)</td>
<td align="center" valign="top">(4,2)</td>
<td align="center" valign="top">-</td>
</tr>
<tr>
<td align="center" valign="top">10</td>
<td align="center" valign="top">(10,2)</td>
<td align="center" valign="top">(8,2)</td>
<td align="center" valign="top">(6,2)</td>
<td align="center" valign="top">(4,2)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>In the figures of <xref ref-type="table" rid="tbl5">Table V</xref> and in <xref ref-type="table" rid="tbl6">Table VI</xref> we provide for the different dataset experiments the AUC for MissForest, the simulated results of the quantum version of DPP-MissForest, and the actual hardware experimental results of running the quantum version of DPP-MissForest. Even for these very small datasets, when simulating the quantum version of DPP-MissForest, we demonstrate an increase in the AUC compared to the MissForest algorithm. This further highlights the potential advantages of determinantal sampling within imputation methods. Of note, running our algorithms on current hardware introduces variance in the downstream classifier. Importantly, this variance is due to noise in the hardware rather than inherent to the algorithm.</p>
<table-wrap id="tbl5" orientation="portrait" position="float">
<label>TABLE V:</label>
<caption>
<p>Hardware results using the IBM quantum processor, depicting AUC results of the downstream classifier task after imputing missing values using DPP-MissForest. In the case of MissForest and the quantum hardware DPP-MissForest implementations, the boxplots correspond to 10 AUC values for 10 iterations of the same imputation and classification algorithm, depicting the lower and upper quartiles as well as the median of these 10 values. The AUC values are the same for every iteration of the quantum DPP-MissForest algorithm using the simulator.</p>
</caption>
<alternatives>
<graphic xlink:href="2303.17893v1_tbl5.jpg" mimetype="image" mime-subtype="jpeg"/>
<table frame="void" rules="all">
<thead>
<tr>
<th align="center" valign="top"/>
<th align="center" valign="top">Batch size: 7</th>
<th align="center" valign="top">Batch size: 8</th>
<th align="center" valign="top">Batch size: 10</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" valign="top">Number of trees: 2</td>
<td align="center" valign="top">Number of trees: 3</td>
<td align="center" valign="top">Number of trees: 4</td>
</tr>
<tr>
<td align="center" valign="top">MCAR SYNTH</td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig9.jpg" mimetype="image" mime-subtype="jpeg"/></td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig10.jpg" mimetype="image" mime-subtype="jpeg"/></td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig11.jpg" mimetype="image" mime-subtype="jpeg"/></td>
</tr>
<tr>
<td align="center" valign="top">MCAR MIMIC</td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig12.jpg" mimetype="image" mime-subtype="jpeg"/></td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig13.jpg" mimetype="image" mime-subtype="jpeg"/></td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig14.jpg" mimetype="image" mime-subtype="jpeg"/></td>
</tr>
<tr>
<td align="center" valign="top">MNAR SYNTH</td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig15.jpg" mimetype="image" mime-subtype="jpeg"/></td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig16.jpg" mimetype="image" mime-subtype="jpeg"/></td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig17.jpg" mimetype="image" mime-subtype="jpeg"/></td>
</tr>
<tr>
<td align="center" valign="top">MNAR MIMIC</td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig18.jpg" mimetype="image" mime-subtype="jpeg"/></td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig19.jpg" mimetype="image" mime-subtype="jpeg"/></td>
<td align="center" valign="top"><inline-graphic xlink:href="2303.17893v1_ig20.jpg" mimetype="image" mime-subtype="jpeg"/></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl6" orientation="portrait" position="float">
<label>TABLE VI:</label>
<caption>
<p>Numerical quantum hardware results showing the AUC results of the downstream classifier task on reduced datasets. Values are represented according to Mean±SD (standard deviation) format given 10 values for each experiment.</p>
</caption>
<alternatives>
<graphic xlink:href="2303.17893v1_tbl6.jpg" mimetype="image" mime-subtype="jpeg"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="center" valign="top">Dataset</th>
<th align="center" valign="top">Missingness</th>
<th align="center" valign="top">Batch size</th>
<th align="center" valign="top">Trees</th>
<th align="center" valign="top">MissForest</th>
<th align="center" valign="top">detDPP-MissForest (simulator)</th>
<th align="center" valign="top">detDPP-MissForest (hardware)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top" rowspan="6">SYNTH</td>
<td align="center" valign="top" rowspan="3">MCAR</td>
<td align="center" valign="top">7</td>
<td align="center" valign="top">2</td>
<td align="center" valign="top">0.868±0.0302</td>
<td align="center" valign="top">0.9026</td>
<td align="center" valign="top">0.8598±0.021</td>
</tr>
<tr>
<td align="center" valign="top">8</td>
<td align="center" valign="top">3</td>
<td align="center" valign="top">0.8667±0.0342</td>
<td align="center" valign="top">0.9256</td>
<td align="center" valign="top">0.8923±0.027</td>
</tr>
<tr>
<td align="center" valign="top">10</td>
<td align="center" valign="top">4</td>
<td align="center" valign="top">0.8725±0.0275</td>
<td align="center" valign="top">0.9028</td>
<td align="center" valign="top">0.8902±0.024</td>
</tr>
<tr>
<td align="center" valign="top" rowspan="3">MNAR</td>
<td align="center" valign="top">7</td>
<td align="center" valign="top">2</td>
<td align="center" valign="top">0.7122±0.0264</td>
<td align="center" valign="top">0.78</td>
<td align="center" valign="top">0.7149±0.02</td>
</tr>
<tr>
<td align="center" valign="top">8</td>
<td align="center" valign="top">3</td>
<td align="center" valign="top">0.7153±0.022</td>
<td align="center" valign="top">0.729</td>
<td align="center" valign="top">0.7036±0.0167</td>
</tr>
<tr>
<td align="center" valign="top">10</td>
<td align="center" valign="top">4</td>
<td align="center" valign="top">0.7258±0.0157</td>
<td align="center" valign="top">0.7868</td>
<td align="center" valign="top">0.7082±0.036</td>
</tr>
<tr>
<td align="left" valign="top" rowspan="6">MIMIC</td>
<td align="center" valign="top" rowspan="3">MCAR</td>
<td align="center" valign="top">7</td>
<td align="center" valign="top">2</td>
<td align="center" valign="top">0.7127±0.038</td>
<td align="center" valign="top">0.7522</td>
<td align="center" valign="top">0.7117±0.0315</td>
</tr>
<tr>
<td align="center" valign="top">8</td>
<td align="center" valign="top">3</td>
<td align="center" valign="top">0.7136±0.03</td>
<td align="center" valign="top">0.7728</td>
<td align="center" valign="top">0.7448±0.0258</td>
</tr>
<tr>
<td align="center" valign="top">10</td>
<td align="center" valign="top">4</td>
<td align="center" valign="top">0.6968±0.03</td>
<td align="center" valign="top">0.7327</td>
<td align="center" valign="top">0.7262±0.0299</td>
</tr>
<tr>
<td align="center" valign="top" rowspan="3">MNAR</td>
<td align="center" valign="top">7</td>
<td align="center" valign="top">2</td>
<td align="center" valign="top">0.7697±0.0133</td>
<td align="center" valign="top">0.7794</td>
<td align="center" valign="top">0.7742±0.0108</td>
</tr>
<tr>
<td align="center" valign="top">8</td>
<td align="center" valign="top">3</td>
<td align="center" valign="top">0.7713±0.0112</td>
<td align="center" valign="top">0.7943</td>
<td align="center" valign="top">0.767±0.0125</td>
</tr>
<tr>
<td align="center" valign="top">10</td>
<td align="center" valign="top">4</td>
<td align="center" valign="top">0.7712±0.0116</td>
<td align="center" valign="top">0.7922</td>
<td align="center" valign="top">0.7675±.01545</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Our quantum hardware results are competitive with standard methods and in many cases close to the values expected from the simulation. In some cases, we observed a clear deterioration of the AUC due to the noise and errors in the quantum hardware. The results are closer to the simulations when using MCAR missingness with larger batch sizes that use more trees both for synthetic and the MIMIC datasets. As explained above, even though the algorithm with batch size 10 means using a quantum circuit with 10 qubits, the fact that we use four trees overall with a decreasing number of datapoints each time, and thus a decreasing number of qubits (namely, 10, 8, 6, and 4), results in an overall more reliable imputation.</p>
</sec>
</sec>
<sec id="s2" sec-type="discussion">
<title>Discussion</title>
<p>Missing data is a critical issue for machine learning practitioners as complete data sets are usually required for training machine learning algorithms. To achieve complete data sets, missing values are usually imputed. In the case of clinical data, missing values and imputation can be a potential source of bias and can considerably influence the robustness and interpretability of results. Nevertheless, there is no canonical way to deal with missing data which makes improvements in data imputation methods an attractive and impactful approach to increase the effectiveness and reliability of clinical trials.</p>
<p>Determinantal point processing methods increase the diversity of the data picked to train the models, showcasing also that data gathering and pre-processing are important to remove biases related to over-representation of particular data types. This is more important when dealing with unbalanced datasets, as is the case often with clinical data. Determinantal sampling is an important tool not only for Random Forest models, but also for linear regression, where data diversity results in more robust and fair models [<xref ref-type="bibr" rid="c18">18</xref>]. Moreover, such sampling methods based on determinantal point processes are computationally intensive and quantum computers are expected to be useful in this case: quantum computers offer an asymptotic speedup for performing this sampling and it is expected that next generation quantum computers will provide a speedup in practice.</p>
<p>We show that, as expected, the quantum version of detDPP-MissForest does not introduce any variance in the downstream classifier when simulated in the absence of hardware noise. However, when implemented on quantum hardware, we observed variance that is caused by the noise in the hardware itself. More precisely, the output of the quantum circuit is not a sample from the precise determinantal distribution but from a noisy version of it, and this noise depends on the particular quantum circuit implemented and the quality of the hardware. Thus when attempting to compute the highest probability element using samples from the quantum circuit on current hardware, the result is not deterministic. Importantly, unlike for standard MissForest, this variance is not inherent in the algorithm and is expected to reduce considerably with the advent of better quality quantum computers. The quantum circuits needed to efficiently perform determinantal sampling require a number of qubits equal to the batch size used for each decision tree within the Random Forest training and the depth of the quantum circuit is roughly proportional to the number of features. As an example, if we would like to perform the quantum version of the determinantal imputation methods we used for MIMIC-III, then we would need a quantum computer with 150 qubits (the batch size) that can be reliably used to perform a quantum circuit of depth around 400 (the depth is given by 4<italic>d</italic>log <italic>n</italic>, where <italic>n</italic> = 150 is the batch size and <italic>d</italic> = 14 is the number of features [<xref ref-type="bibr" rid="c12">12</xref>]). While quantum hardware with a few hundred qubits that can perform computations of a few hundred steps are not available right now, it seems quite possible that they will be available in the not so far future. In the meantime further optimization could also help reduce the quantum resources needed for such imputation methods.</p>
<p>In summary, here we propose novel data imputation methods that: first, improve the widely-used iterative imputation methods –MiceRanger and MissForest– as measured by the AUC of a downstream classifier; second, remove the variance of the imputation methods, thus ensuring reproducibility and simpler integration into machine learning workflows; third, become even more efficient when run on quantum computers. Based on our results, we anticipate an impact of our algorithms on the reliability of models in high precision value settings, including in pharmaceutical drug trials where they can provide higher confidence in the predictions made by eradicating the stochastic variance due to multiple imputations. In addition, tasks that are currently overwhelmed by the challenges of missingness become more tractable through the approaches introduced here, which is a common problem with real-world-evidence investigations, where detDPP-MICE and detDPP-MissForest can yield improved performance in the face of missingness.</p>
</sec>
<sec id="s3" sec-type="methods">
<title>Methods</title>
<sec id="s3a">
<title>Determinantal Point Processes (DPPs)</title>
<p>Given a set of items <italic>y</italic> = {<italic>y</italic><sub>1</sub>, …, <italic>y</italic><sub>N</sub>}, a point process 𝓟 is a probability distribution over all subsets of the set <italic>Y</italic>. It is called a Determinantal Point Process (DPP) if, for a subset Y drawn from <italic>Y</italic> according to 𝓟, we have:
<disp-formula id="FD1">
<alternatives><mml:math display="block" id="M1"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>⊆</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>det</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2303.17893v1_eqn1.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives>
<label>(1)</label>
</disp-formula>
</p>
<p>where <italic>K</italic> is a real symmetric <italic>N</italic> × <italic>N</italic> matrix, and <italic>K</italic><sub><italic>T,T</italic></sub> is its submatrix whose rows and columns are indexed by <italic>T</italic>. The matrix <italic>K</italic> is called the marginal kernel of Y.</p>
<p>For an <italic>n</italic> × <italic>d</italic> data matrix <italic>A</italic> and <italic>L</italic> = <italic>AA</italic><sup><italic>T</italic></sup>, we define the L-ensemble DPP<sub><italic>L</italic></sub>(<bold>L</bold>) as the distribution where the probability of sampling <italic>T</italic> is:
<disp-formula id="FD2">
<alternatives><mml:math display="block" id="M2"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>{</mml:mo><mml:mi>T</mml:mi><mml:mo>}</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>det</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mrow><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>det</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>I</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>∝</mml:mo><mml:mi>V</mml:mi><mml:mi>o</mml:mi><mml:msup><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>T</mml:mi></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2303.17893v1_eqn2.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives>
<label>(2)</label>
</disp-formula>
</p>
<p>where <italic>Vol({a<sub>i</sub></italic> : <italic>i</italic> ∈ <italic>T</italic>}) is the volume spanned by the rows of <italic>A</italic> indexed by <italic>T</italic>.</p>
<p>According to this distribution, the probability of sampling points which are similar and form a smaller volume is thus reduced in favor of samples which are more diverse.</p>
<p>An L-ensemble is a Determinantal Point Process if <bold>K = L(I + L)</bold><sup>_1</sup>.</p>
<p><italic>Stochastic k-DPPs</italic>. The distribution <italic>k</italic> — DPP<sub>L</sub>(L) is defined as an L-ensemble which is constrained to subsets of size <italic>|T|</italic> = <italic>k</italic>.</p>
<p>Different algorithms have been proposed in the literature to sample from <italic>k</italic> — DPPs, namely [<xref ref-type="bibr" rid="c13">13</xref>] where sampling <italic>d</italic> rows from an <italic>N</italic> × <italic>d</italic> matrix takes <italic>0</italic>(<italic>Nd</italic><sup>2</sup>) time. There have been improvements over this initial proposal as in [<xref ref-type="bibr" rid="c19">19</xref>] where there is a preprocessing cost of <italic>0</italic>(<italic>Nd</italic><sup>2</sup>) and each DPP sample requires <italic>0</italic>(<italic>d</italic><sup>3</sup>) arithmetic operations.</p>
<p><italic>Deterministic k-DPPs</italic>. Stochastic DPP sampling may be efficient in practice, however deterministic algorithms are important for different use cases since they are more interpretable, are less prone to errors and have no failure probability, which is especially relevant for clinical data [<xref ref-type="bibr" rid="c7">7</xref>].</p>
<p>We use a deterministic version of DPP sampling as proposed in [<xref ref-type="bibr" rid="c10">10</xref>] (see Algorithm 1) which is a greedy maximum volume approach. For each deterministic <italic>k —</italic> DPP sample, elements with the highest probability are added iteratively.</p>
<fig position="float" fig-type="figure" id="box1"><label>Algorithm 1</label><caption><title>Deterministic k-DPP algorithm</title></caption><graphic xlink:href="2303.17893v1_algo1.tif" mimetype="image" mime-subtype="tiff"/></fig>

</sec>
<sec id="s3b">
<title>DPP-Random Forest and detDPP-Random Forest</title>
<p>The Random Forest is a widely-used ensemble learning model for classification and regression problems. It trains a number of decision trees on different samples from the dataset, and the final prediction of the Random Forest is the average of the decision trees for regression tasks or the class predicted by the most decision trees for classification tasks.</p>
<p>The samples used to train each tree are drawn uniformly with replacement from the original dataset (bootstrapping).</p>
<p>The DPP-Random Forest algorithm (see <xref ref-type="fig" rid="fig4">Figure 4</xref>) replaces the uniform sampling with DPP sampling without replacement.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>FIG. 4:</label>
<caption><p>The sampling and training procedure for the DPP-Random Forest algorithm: the dataset is divided into batches of similar size, the DPP sampling algorithm is then applied to every batch in parallel, the subsequent samples are then combined to form larger datasets used to train the decision trees</p></caption>
<graphic xlink:href="2303.17893v1_fig4.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<p>Determinantal sampling for regression and classification tasks with full data has been proposed previously for Linear Regressors [<xref ref-type="bibr" rid="c18">18</xref>] and for Random Forest training for a financial data classification use case where it outperformed the standard Random Forest model [<xref ref-type="bibr" rid="c26">26</xref>].</p>
<p>We can also use the deterministic version of DPP sampling for the Random Forest algorithm. This requires removing the sample used at each step (which is the one with the highest probability according to the determinantal distribution) in order to create a smaller dataset from which to sample for the next decision tree (see <xref ref-type="fig" rid="fig5">Figure 5</xref>). We call this new model detDPP-Random Forest.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>FIG. 5:</label>
<caption><p>Deterministic DPP sampling procedure for training decision trees. At each step, a decision tree is trained usingthe sample that corresponds to the highest determinantal probability, and which is then removed from the original batch before continuing to the next decision tree.</p></caption>
<graphic xlink:href="2303.17893v1_fig5.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<p>Let us note that the distributions of the in-bag DPP samples, which are biased towards diversity, and the out-of-bag (OOB) samples, which reflect the original dataset’s distribution, may be different. This could lead to an inaccuratecalculation of the OOB error that can be in fact overestimated [<xref ref-type="bibr" rid="c9">9</xref>]. In the DPP-Random Forest case, the batches are stratified and according to the output variable that follows the same distribution as the original dataset. Thus, sampling from different batches could bridge the gap between the in-bag and the out-of-bag distributions. We leave these considerations for future work.</p>
</sec>
<sec id="s3c">
<title>Quantum methods for DPPs</title>
<p>Quantum Machine Learning has been a rapidly developing field and many applications have been explored, including with biomedical data, both using quantum algorithms to speedup linear algebraic procedures and through quantum neural networks [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c15">15</xref>].</p>
<p>In [<xref ref-type="bibr" rid="c12">12</xref>], it was shown that there exist quantum algorithms for performing the determinantal sampling with better computational complexity than the best known classical methods. We describe below the quantum circuits that are needed for performing this quantum algorithm on quantum hardware with different connectivity characteristics and provide a resource analysis for the number of qubits, the number of gates and the depth of the quantum circuit.</p>
<p>First, we introduce an important component of the quantum DPP circuit which is the Clifford loader. Given an input state x ∈ R<sup>n</sup>, it performs the following operation:
<disp-formula id="FD3">
<alternatives><mml:math display="block" id="M3"><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mtext>​</mml:mtext></mml:msup></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math>
<graphic xlink:href="2303.17893v1_eqn3.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives>
</disp-formula>
</p>
<p>, in other words it encodes the vector x as a sum of the mutually anti-commuting operators generating the Clifford algebra.</p>
<p>For implementing this operation with an efficient quantum circuit, we use standard one- and two-qubit gates, such as the X, Z, CZ gates as well as a parameterized two-qubit gate called the Reconfigurable Beam Splitter gate (RBS), which do the following operations:
<disp-formula id="FD4">
<alternatives><mml:math display="block" id="M4"><mml:mrow><mml:mi>R</mml:mi><mml:mi>B</mml:mi><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>cos</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>sin</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>sin</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>cos</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
<graphic xlink:href="2303.17893v1_eqn4.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives>
<label>(3)</label>
</disp-formula>
</p>
<p>We provide in <xref ref-type="fig" rid="fig6">Fig. 6</xref> three different versions of the Clifford loader that take advantage of the specific connectivity of the quantum hardware, for example grid connectivity for superconducting qubits or all-to-all connectivity for trapped-ion qubits. These constructions are optimal (up to constant factor) on the number of two-qubit gates. We provide the exact resource analysis in <xref ref-type="table" rid="tbl7">Table VII</xref>.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>FIG. 6:</label>
<caption><p>Types of data loaders. Each line corresponds to a qubit. Each vertical line connecting two qubits corresponds to an RBS gate. We also use X, Z, C - Z gates. The depth of the first two loaders is linear and the last one is logarithmic on the number of qubits.</p></caption>
<graphic xlink:href="2303.17893v1_fig6.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<table-wrap id="tbl7" orientation="portrait" position="float">
<label>TABLE VII:</label>
<caption>
<p>Summary of the characteristics of the different quantum DPP circuits. NN = Nearest Neighbor connectivity</p></caption>
<alternatives>
<graphic xlink:href="2303.17893v1_tbl7.jpg" mimetype="image" mime-subtype="jpeg"/>
<table frame="void" rules="all">
<thead>
<tr>
<th align="left" valign="top">Clifford loader</th>
<th align="left" valign="top">Hardware connectivity</th>
<th align="left" valign="top">Depth</th>
<th align="left" valign="top"># of RBS gates</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" valign="top">Diagonal</td>
<td align="center" valign="top">NN</td>
<td align="center" valign="top">2<italic>nd</italic></td>
<td align="center" valign="top">2<italic>nd</italic></td>
</tr>
<tr>
<td align="center" valign="top">Semi-Diagonal</td>
<td align="center" valign="top">NN</td>
<td align="center" valign="top"><italic>nd</italic></td>
<td align="center" valign="top">2<italic>nd</italic></td>
</tr>
<tr>
<td align="center" valign="top">Parallel</td>
<td align="center" valign="top">All-to-all</td>
<td align="center" valign="top">4<italic>d</italic>log(<italic>n</italic>)</td>
<td align="center" valign="top">2<italic>nd</italic></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>We can now use the Clifford loaders described above to perform <italic>k — DPP</italic> sampling, as described [<xref ref-type="bibr" rid="c12">12</xref>].</p>
<p>Given an orthogonal matrix <italic>A</italic> = (<italic>a</italic><sup>1</sup>,…, <italic>a</italic>), we can apply the qDPP circuit shown in <xref ref-type="fig" rid="fig7">Fig. 7</xref>, which is just a sequential application of <italic>d</italic> Clifford loaders, one for each column of the matrix, to the |0<sup>n</sup>) state, and that leads to the following result:
<disp-formula id="FD5">
<alternatives><mml:math display="block" id="M5"><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="script">A</mml:mi><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋯</mml:mo><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mtext>​</mml:mtext></mml:msup></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:munder><mml:mi>det</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math>
<graphic xlink:href="2303.17893v1_eqn5.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives>
</disp-formula>
</p>
<fig id="fig7" position="float" fig-type="figure">
<label>FIG. 7:</label>
<caption><p>Quantum Determinant Sampling circuit for an orthogonal matrix <italic>A</italic> = (<italic>a</italic><sup>1</sup>,…, <italic>a</italic><sup><italic>d</italic></sup>). It uses the Clifford loader which is a unitary quantum operator: <inline-formula id="ID2">
<alternatives><mml:math display="inline" id="I2"><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext>  for  </mml:mtext><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math>
<inline-graphic xlink:href="2303.17893v1_ieq2.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives>
</inline-formula></p></caption>
<graphic xlink:href="2303.17893v1_fig7.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<p>Directly measuring at the end of the circuit provides a sample from the correct determinantal distribution.</p>
<p>Quantum versions of the imputation methods It is easy to define now a quantum version of the DPP-MICE and DPP-MissForest algorithms, where we use the quantum circuit described above to sample from the corresponding DPP. We can also define a variant of the deterministic algorithms, though here we need to pay attention to the fact that the quantum circuit enables to sample from the determinantal distribution but does not efficiently give us a classical description of the entire distribution. Hence one can instead sample many times from the quantum circuit and output the most frequent element. This provides a sample with less variance but it only becomes deterministic in the limit of infinite measurements.</p>
</sec>
</sec>
</body>
<back>
<ref-list>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jacob</given-names><surname>Biamonte</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Wittek</surname></string-name>, <string-name><given-names>Nicola</given-names> <surname>Pancotti</surname></string-name>, <string-name><given-names>Patrick</given-names> <surname>Rebentrost</surname></string-name>, <string-name><given-names>Nathan</given-names> <surname>Wiebe</surname></string-name>, and <string-name><given-names>Seth</given-names> <surname>Lloyd</surname></string-name></person-group>. <article-title>Quantum machine learning</article-title>. <source>Nature</source>, <volume>549</volume>(<issue>7671</issue>):<fpage>195</fpage>–<lpage>202</lpage>, <month>Sep</month> <year>2017</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Adam L</given-names> <surname>Booth</surname></string-name>, <string-name><given-names>Elizabeth</given-names> <surname>Abels</surname></string-name>, and <string-name><given-names>Peter</given-names> <surname>McCaffrey</surname></string-name></person-group>. <article-title>Development of a prognostic model for mortality in covid-19 infection using machine learning</article-title>. <source>Modern Pathology</source>, <volume>34</volume>(<issue>3</issue>):<fpage>522</fpage>–<lpage>531</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Cerezo</surname></string-name>, <string-name><given-names>Guillaume</given-names> <surname>Verdon</surname></string-name>, <string-name><given-names>Hsin-Yuan</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>Lukasz</given-names> <surname>Cincio</surname></string-name>, and <string-name><given-names>Patrick J.</given-names> <surname>Coles</surname></string-name></person-group>. <article-title>Challenges and opportunities in quantum machine learning</article-title>. <source>Nature Computational Science</source>, <volume>2</volume>(<issue>9</issue>):<fpage>567</fpage>–<lpage>576</lpage>, <month>Sep</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>El Amine</given-names> <surname>Cherrat</surname></string-name>, <string-name><given-names>Iordanis</given-names> <surname>Kerenidis</surname></string-name>, <string-name><given-names>Natansh</given-names> <surname>Mathur</surname></string-name>, <string-name><given-names>Jonas</given-names> <surname>Landman</surname></string-name>, <string-name><given-names>Martin</given-names> <surname>Strahm</surname></string-name>, and <string-name><given-names>Yun Yvonna</given-names> <surname>Li</surname></string-name></person-group>. <article-title>Quantum vision transformers</article-title>, <year>2022</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Michal</given-names> <surname>Derezinski</surname></string-name> and <string-name><given-names>Michael W</given-names> <surname>Mahoney</surname></string-name></person-group>. <article-title>Determinantal point processes in randomized numerical linear algebra</article-title>. <source>Notices of the American Mathematical Society</source>, <volume>68</volume>(<issue>1</issue>):<fpage>34</fpage>–<lpage>45</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Michal</given-names> <surname>Derezinsk</surname></string-name> and <string-name><given-names>Michael W.</given-names> <surname>Mahoney</surname></string-name></person-group>. <article-title>Determinantal point processes in randomized numerical linear algebra</article-title>. <source>Notices of the American Mathematical Society</source>, <volume>68</volume> (<issue>2021</issue>), pp. <fpage>34</fpage>–<lpage>45</lpage>,.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Radwa El</given-names> <surname>Shawi</surname></string-name>, <string-name><given-names>Youssef</given-names> <surname>Sherif</surname></string-name>, <string-name><surname>Mouaz</surname> <given-names>Al-Mallah</given-names></string-name>, and <string-name><given-names>Sherif</given-names> <surname>Sakr</surname></string-name></person-group>. <article-title>Interpretability in healthcare a comparative study of local machine learning interpretability techniques</article-title>. pages <fpage>275</fpage>–<lpage>280</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Tlamelo</given-names> <surname>Emmanuel</surname></string-name>, <string-name><given-names>Thabiso</given-names> <surname>Maupong</surname></string-name>, <string-name><given-names>Dimane</given-names> <surname>Mpoeleng</surname></string-name>, <string-name><given-names>Thabo</given-names> <surname>Semong</surname></string-name>, <string-name><given-names>Banyatsang</given-names> <surname>Mphago</surname></string-name>, and <string-name><given-names>Oteng</given-names> <surname>Tabona</surname></string-name></person-group>. <article-title>A survey on missing data in machine learning</article-title>. <source>Journal of Big Data</source>, <volume>8</volume>(<issue>1</issue>):<fpage>140</fpage>, <month>Oct</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Silke</given-names> <surname>Janitza</surname></string-name> and <string-name><given-names>Roman</given-names> <surname>Hornung</surname></string-name></person-group>. <article-title>On the overestimation of random forest’s out-of-bag error</article-title>. <source>PLOS ONE</source>, <volume>13</volume>(<issue>8</issue>):<fpage>1</fpage>–<lpage>31</lpage>, <day>08</day> <year>2018</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Michaël Fanuel Joachim</given-names> <surname>Schreurs</surname></string-name> and <string-name><given-names>Johan A.K.</given-names> <surname>Suykens</surname></string-name></person-group>. <article-title>Towards deterministic diverse subset sampling</article-title>. <source>Artificial Intelligence and Machine Learning</source>, page <fpage>137</fpage>–<lpage>151</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alistair E W</given-names> <surname>Johnson</surname></string-name>, <string-name><given-names>Tom J</given-names> <surname>Pollard</surname></string-name>, <string-name><given-names>Lu</given-names> <surname>Shen</surname></string-name>, <string-name><given-names>Li-wei H</given-names> <surname>Lehman</surname></string-name>, <string-name><given-names>Mengling</given-names> <surname>Feng</surname></string-name>, <string-name><given-names>Mohammad</given-names> <surname>Ghassemi</surname></string-name>, <string-name><given-names>Benjamin</given-names> <surname>Moody</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Szolovits</surname></string-name>, <string-name><given-names>Leo Anthony</given-names> <surname>Celi</surname></string-name>, and <string-name><given-names>Roger G</given-names> <surname>Mark</surname></string-name></person-group>. <article-title>Mimic-iii, a freely accessible critical care database</article-title>. <source>Scientific Data</source>, <volume>3</volume>(<issue>1</issue>):<fpage>10035</fpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Iordanis</given-names> <surname>Kerenidis</surname></string-name> and <string-name><given-names>Anupam</given-names> <surname>Prakash</surname></string-name></person-group>. <article-title>Quantum machine learning with subspace states</article-title>. <source>arXiv:2202.00054</source>, <year>2022</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alex</given-names> <surname>Kulesza</surname></string-name> and <string-name><given-names>Ben</given-names> <surname>Taskar</surname></string-name></person-group>. <article-title>Determinantal point processes for machine learning</article-title>. <source>Foundations and Trends® in Machine Learning</source>, <volume>5</volume>(<issue>2–3</issue>):<fpage>123</fpage>–<lpage>286</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Alex</given-names> <surname>Kulezsa</surname></string-name> and <string-name><given-names>Ben</given-names> <surname>Taskar</surname></string-name></person-group>. <article-title>k-dpps: fixed-size determinantal point processes</article-title>. <source>ICML’11: Proceedings of the 28th International Conference on International Conference on Machine Learning</source>, <year>2011</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jonas</given-names> <surname>Landman</surname></string-name>, <string-name><given-names>Natansh</given-names> <surname>Mathur</surname></string-name>, <string-name><given-names>Yun Yvonna</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Martin</given-names> <surname>Strahm</surname></string-name>, <string-name><given-names>Skander</given-names> <surname>Kazdaghli</surname></string-name>, <string-name><given-names>Anupam</given-names> <surname>Prakash</surname></string-name>, and <string-name><given-names>Iordanis</given-names> <surname>Kerenidis</surname></string-name></person-group>. <article-title>Quantum Methods for Neural Networks and Application to Medical Image Classification</article-title>. <source>Quantum</source>, <volume>6</volume>:<fpage>881</fpage>, <month>December</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yuan</given-names> <surname>Luo</surname></string-name></person-group>. <article-title>Evaluating the state of the art in missing data imputation for clinical data</article-title>. <source>Briefings in Bioinformatics</source>, <volume>23</volume>(<issue>1</issue>), <fpage>12</fpage> <year>2021</year>. <comment>bbab489</comment>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Pierre-Alexandre</given-names> <surname>Mattei</surname></string-name> and <string-name><given-names>Jes</given-names> <surname>Frellsen</surname></string-name></person-group>. <article-title>Miwae: Deep generative modelling and imputation of incomplete data sets</article-title>. <year>2019</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Manfred</surname> <given-names>K.</given-names></string-name> <string-name><given-names>Warmuth Michal</given-names> <surname>Dereziński</surname></string-name> and <string-name><given-names>Daniel</given-names> <surname>Hsu</surname></string-name></person-group>. <article-title>Leveraged volume sampling for linear regression</article-title>. <year>2018</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Michael W.</given-names> <surname>Mahoney Michal Derezinski</surname></string-name>, <string-name><given-names>Kenneth L.</given-names> <surname>Clarkson</surname></string-name> and <string-name><given-names>Manfred K.</given-names> <surname>Warmuth</surname></string-name></person-group>. <article-title>Minimax experimental design: Bridging the gap between statistical and worst-case approaches to least squares regression</article-title>. <source>Conference on Learning Theory (COLT). PMLR</source>, pages <fpage>1050</fpage>–<lpage>1069</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>William R.</given-names> <surname>Myers</surname></string-name></person-group>. <article-title>Handling missing data in clinical trials: An overview</article-title>. <source>Drug Information Journal</source>, <volume>34</volume>(<issue>2</issue>):<fpage>525</fpage>–<lpage>533</lpage>, <year>2000</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alma B</given-names> <surname>Pederse</surname></string-name>, <string-name><given-names>Ellen M</given-names> <surname>Mikkelsen</surname></string-name>, <string-name><surname>Deirdre</surname> <given-names>Cronin-Fenton</given-names></string-name>, <string-name><given-names>Nickolaj R</given-names> <surname>Kristensen</surname></string-name>, <string-name><given-names>Tra My</given-names> <surname>Pham</surname></string-name>, <string-name><given-names>Lars</given-names> <surname>Pedersen</surname></string-name>, and <string-name><given-names>Irene</given-names> <surname>Petersen</surname></string-name></person-group>. <article-title>Missing data and multiple imputation in clinical epidemiological research</article-title>. <source>Clinical Epidemiology</source>, <volume>9</volume>:<fpage>157</fpage>–<lpage>166</lpage>, <year>2017</year>. <comment>PMID: 28352203</comment>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jiongming</given-names> <surname>Qin</surname></string-name>, <string-name><given-names>Lin</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Yuhua</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Chuanjun</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Changhao</given-names> <surname>Feng</surname></string-name>, and <string-name><given-names>Bin</given-names> <surname>Chen</surname></string-name></person-group>. <article-title>A machine learning methodology for diagnosing chronic kidney disease</article-title>. <source>IEEE Access</source>, <volume>8</volume>:<fpage>20991</fpage>–<lpage>21002</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Michael C</given-names> <surname>Rendleman</surname></string-name>, <string-name><given-names>John M</given-names> <surname>Buatti</surname></string-name>, <string-name><given-names>Terry A</given-names> <surname>Braun</surname></string-name>, <string-name><given-names>Brian J</given-names> <surname>Smith</surname></string-name>, <string-name><given-names>Chibuzo</given-names> <surname>Nwakama</surname></string-name>, <string-name><given-names>Reinhard R</given-names> <surname>Beichel</surname></string-name>, <string-name><given-names>Bart</given-names> <surname>Brown</surname></string-name>, and <string-name><given-names>Thomas L</given-names> <surname>Casavant</surname></string-name></person-group>. <article-title>Machine learning with the tcga-hnsc dataset: improving usability by addressing inconsistency, sparsity, and high-dimensionality</article-title>. <source>BMC bioinformatics</source>, <volume>20</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>9</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T.</given-names> <surname>Shadbahr</surname></string-name></person-group>. <etal>et al.</etal> <article-title>Classification of datasets with imputed missing values: Does imputation quality matter?</article-title> <source>arXiv:2206.08478</source>, <year>2022</year>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Daniel J.</given-names> <surname>Stekhoven</surname></string-name> and <string-name><given-names>Peter</given-names> <surname>Bühlmann</surname></string-name></person-group>. <article-title>MissForest—non-parametric missing value imputation for mixed-type data</article-title>. <source>Bioinformatics</source>, <volume>28</volume>(<issue>1</issue>):<fpage>112</fpage>–<lpage>118</lpage>, <day>10</day> <year>2011</year>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Sohum</given-names> <surname>Thakkar</surname></string-name>, <string-name><given-names>Natansh</given-names> <surname>Mathur</surname></string-name>, <string-name><given-names>Skander</given-names> <surname>Kazdaghli</surname></string-name>, <string-name><given-names>Iordanis</given-names> <surname>Kerenidis</surname></string-name>, <string-name><given-names>Andre</given-names> <surname>Juan</surname></string-name> <string-name><given-names>Ferreira</given-names> <surname>Martins</surname></string-name>, and <string-name><given-names>Samurai Gomes Aguiar</given-names> <surname>Brito</surname></string-name></person-group>. <article-title>Quantum machine learning for predictive analytics in finance</article-title>. <source>under submission</source>, <year>2023</year>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Stef van</given-names> <surname>Buuren</surname></string-name> and <string-name><surname>Karin</surname> <given-names>Groothuis-Oudshoorn</given-names></string-name></person-group>. <article-title>mice: Multivariate imputation by chained equations in r</article-title>. <source>Journal of Statistical Software</source>, <volume>45</volume>(<issue>3</issue>):<fpage>1</fpage>–<lpage>67</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Zichen</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Li</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Benjamin S</given-names> <surname>Glicksberg</surname></string-name>, <string-name><given-names>Ariel</given-names> <surname>Israel</surname></string-name>, <string-name><given-names>Joel T</given-names> <surname>Dudley</surname></string-name>, and <string-name><given-names>Avi</given-names> <surname>Ma’ayan</surname></string-name></person-group>. <article-title>Predicting age by mining electronic medical records with deep learning characterizes differences between chronological and physiological age</article-title>. <source>Journal of biomedical informatics</source>, <volume>76</volume>:<fpage>59</fpage>–<lpage>68</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Jinsung</given-names> <surname>Yoon</surname></string-name>, <string-name><given-names>James</given-names> <surname>Jordon</surname></string-name>, and <string-name><given-names>Mihaela van der</given-names> <surname>Schaar</surname></string-name></person-group>. <article-title>Gain: Missing data imputation using generative adversarial nets</article-title>. <source>Proceedings of the 35th International Conference on Machine Learning</source>, <year>2018</year>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89947.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Graña</surname>
<given-names>Martin</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Institut Pasteur de Montevideo</institution>
</institution-wrap>
<city>Montevideo</city>
<country>Uruguay</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study presents a modification of earlier imputation methods for clinical data, effectively addressing missing values with a slight improvement over previous techniques. The methodology and results for the classical case are <bold>solid</bold>, although the evidence for the claim of a practical advantage in 'next generation' quantum computers is not validated. This work will be of value to scientists dealing with datasets involving imputation for classification tasks, particularly in clinical studies.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89947.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
The article written by Kazdaghli et al. proposes a modification of imputation methods, to better account for and exploit the variability of the data. The aim is to reduce the variability of the imputation results. The authors propose two methods, one that still includes some imputation variability, but accounts for the distribution of the data points to improve the imputation. The other one proposes a determinantal sampling, that presents no variation in the imputation data, at least no variation in the classification task. As these methods grow easily in computation requirements and time, they also propose an algorithm to run these methods in quantum processors.</p>
<p>Strengths:</p>
<p>
The sampling method for imputing missing values that accounts for the variability of the data seems to be accurate.</p>
<p>Weaknesses:</p>
<p>
While the proposed method seems accurate and should improve the imputation task, I think that the authors must explain a little better some parts of the algorithm that they are using. Although I think the authors could have evaluated the imputations directly, as they mention in the introduction, I understand that the final goal in the task is to have a better classification. The problem is that they do not explain what the classification is, or how is it trained. In a real situation, they would have data that would be used for training the algorithm, and then new data that needs to be imputed and classified. In this article, I do not see any train, plus test or validation data. I wonder if there could be some interaction between the imputation and the classification methods, that leads to overfitting the data; in particular when the deterministic DPP is used.</p>
<p>In its current state, I do not think this article brings not very much value to the community that could benefit from it. I did not find an implementation of the method available to other scientists, nor the data used to evaluate it (while one data set is public, the simulated data is not available). This not only hinders the use of the method by the scientific community, but also makes it impossible to reproduce the results or test the algorithm in similar databases.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89947.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this work, the authors address the problem of missing data imputation in the life sciences domain and propose several new algorithms which improve on the current state-of-the-art. In particular (i) they modify two existing Random Forest-based imputation methods -- MissForest and miceRanger -- to use either determinantal sampling or deterministic determinantal sampling, and show slightly improved classification performance on two datasets (one synthetic, one real); in addition, (ii) the authors present a quantum circuit for performing the determinantal sampling which scales asymptotically better than the best-known classical methods, and perform small scale experiments using both a (noiseless) quantum simulator as well as a 10 qubit IBM quantum computer to validate that the approach works in principle.</p>
<p>The problem of data imputation is important in practice, and results that improve on existing methods should be of interest to those in the field. The use of determinantal sampling for applications beyond data imputation should also be of broader interest, and the connection to quantum computing warrants further investigation and analysis.</p>
<p>The use of classification accuracy (as measured by AUC) as a measure of success is well-motivated, and the authors use both real and synthetic datasets to evaluate their methods, which consistently (if only marginally) outperform the existing state-of-the-art. The results obtained here motivate the further study of this approach to a wider class of datasets, and to areas beyond life sciences.</p>
<p>As it stands, in my opinion, two points need addressing.</p>
<p>1. Additional clarity is required on what is novel:</p>
<p>While the application of determinantal and deterministic determinantal sampling to the specific case of data imputation appears to be novel, the authors should make it more clear that both of these methods themselves are not new, and have been directly lifted from the literature. As it stands, the current wording in the main body of the paper gives the impression that the deterministic determinantal algorithm is novel, e.g. &quot;this motivated us to develop a deterministic version of determinantal sampling&quot; (p.2), and it is only in the methods section that a reference is made to the paper of Schreurs et al. which proposed the algorithm.</p>
<p>Similarly, in the abstract and main body of the text, the wording gives the impression that the quantum circuits presented here are new (e.g., &quot;We also develop quantum circuits for implementing determinantal point processes&quot;) whereas they have been previously proposed (although one of the authors of the current paper was also an author of the original paper proposing the quantum circuits for determinantal sampling).</p>
<p>2. Additional analysis is needed to support the claims of potential for quantum advantage:</p>
<p>The authors claim that the quantum algorithm for implementing determinantal point processes provides a computational advantage over classical ones, in that the quantum circuits scale linearly in the number of features compared with cubic scaling classically. While this may be true asymptotically, in my opinion, more discussion is required about the utility and feasibility of this method in practice, as well as the realistic prospects of this being a potential area of application for quantum computing.</p>
<p>For example, the authors mention that a quantum computer of 150 qubits capable of running circuits of depth 400 is needed to perform the determinantal sampling for the MIMIC-III dataset considered, and say &quot;while [such hardware is] not available right now, it seems quite possible that they will be available in the not so far future&quot;. The authors also state &quot;This suggests that with the advent of next-generation quantum computers... one could expect a computational speedup in performing determinantal sampling&quot; and &quot;it is expected that next-generation quantum computers will provide a speedup in practice&quot;. These are strong assertions (even if 'next generation' is not clearly defined), and in my opinion, are not sufficiently backed by evidence to date. Given that datasets of the size of MIMIC-III (and presumably much larger) can be handled efficiently classically, the authors should clarify whether one expects a quantum advantage by this approach in the &quot;NISQ&quot; (pre-error-corrected) era of quantum computing. This seems unlikely, and any argument that this is the case should include an analysis accounting for the absolute operation speeds and absolute times required to perform such computations, including any time required for inputting data, resetting quantum circuits etc. On the other hand, if by 'next generation' the authors mean quantum computers beyond the NISQ era (i.e., assuming fault-tolerant quantum computers and logical qubits), then the overhead costs of quantum error correction (both in terms of physical qubit numbers as well as computational time) should be analyzed, and the crossover regime (i.e., data size where a quantum computation takes less absolute time than classical) estimated in order to assess the prospects of a practical quantum advantage, especially in light of recent analyses e.g., [1,2] below.</p>
<p>[1] Hoefler, Haner, Troyer. Communicatios of the ACM, 66.5 (2023):82-87</p>
<p>
[2] Babbush et al., PRX Quantum 2.1 (2021):010103</p>
<p>Other comments and suggestions:</p>
<p>
The authors measure &quot;running time [as] the depth of the necessary quantum circuits.&quot; While circuit depth may indeed correspond to wall-clock time, quantum circuit size (i.e. number of gates) is the fairer complexity metric for comparison with classical running time. If depth is used, then a fair comparison to classical methods should be to compare with classical parallel processing time using N processors. However, if circuit size is used, then the quantum complexity is Nd, which contrasts with the classical value of Nd^2 (pre-processing) + d^3 (per sample). This yields a subquadratic quantum speedup over classical, as opposed to a qubic speedup.</p>
<p>The results (e.g Table 1) show that the new algorithms consistently outperform the original miceRanger and MissForest methods, although the degree of improvement is small, typically of order 1% or less. Some discussion is therefore warranted on the practical benefits of this method, and any tradeoff in terms of efficiency. In particular, while Table 1 compares the classification accuracy (as measured by AUC) of the newly proposed methods vs the existing state-of-the-art, a discussion on the scalability and efficiency would be welcome. The determinantal sampling takes time Nd^2, how does this compare with the original methods? For what dataset and feature sizes are the determinantal methods feasible (which will determine the scale at which other approaches, e.g. those based on quantum computing may be required).</p>
<p>A discussion (or at least mention) of the algorithmic complexity of the classical deterministic determinantal sampling (which seems to also be Nd^2) in the main body of the text would be welcome.</p>
<p>The final paragraph of the Methods section discusses sampling many times from the quantum circuits to estimate the most probable outcome, and hence perform the deterministic determinantal sampling. A more careful analysis of the number of samples needed (for bounded variance/error) and the impact on the running time (and whether one still expects an advantage over classical (although one must define some bounded error version of the deterministic algorithm to do so) or performance of the algorithm would be welcome.</p>
<p>A discussion on the absolute running time required for the quantum experiments performed (and how they compare to classical) would be interesting.</p>
<p>A mention of which quantum simulator was used would be welcome.</p>
<p>In the introduction, three kinds of data missingness (MCAR, MAR, MNAR) are mentioned, although experiments are only performed for MCAR and MNAR. Can some explanation for excluding MAR be given?</p>
<p>Reference 24 (Shadbar et al., the study that demonstrated the effectiveness of miceRanger and MissForest) used 4 datasets: MIMIC-III, Simulated, Breast Cancer, and NHSX COVID-19. Of these, MIMIC-III is used in the current paper, and Simulated appears similar (although with 1000 instead of 2000 rows) to the synthetic dataset of the current paper. An analysis of the determinantal sampling methods applied to the Breast Cancer and NHSX COVID-19 datasets (which have naturally occurring missingness), and a comparison to the results of Shadbar et al. would be interesting.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89947.1.sa3</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kazdaghli</surname>
<given-names>Skander</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kerenidis</surname>
<given-names>Iordanis</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kieckbusch</surname>
<given-names>Jens</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Teare</surname>
<given-names>Philip</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1</bold></p>
<p>More details about the classification and how it is trained</p>
</disp-quote>
<p>We included a sentence in the introduction to clarify which data we are using: &quot;In order to demonstrate this improvement, we apply our methods to two classification datasets: a synthetic dataset and a public clinical dataset where the predicted outcome is the survival of the patient&quot;</p>
<p>And about how the classifier is trained in the &quot;Results&quot; section: &quot;we used the default parameters of the classifier, since our focus is comparing the different imputation methods&quot;</p>
<disp-quote content-type="editor-comment">
<p>Availability of the code</p>
</disp-quote>
<p>Now the code is publicly available in a github repository <ext-link ext-link-type="uri" xlink:href="https://github.com/AstraZeneca/dpp_imp/">https://github.com/AstraZeneca/dpp_imp/</ext-link> (see Availability of Data and Code section)</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2</bold></p>
<p>Clarifying that Determinantal Point Processes and their deterministic version have been introduced before but are applied for the first time for data imputation in this work:</p>
</disp-quote>
<p>We added explanation in the 6th paragraph of the introduction that we use pre-existing DPP and deterministic-DPP algorithms for our imputation methods and include the references to avoid confusion</p>
<p>We also added a paragraph at the end of the introduction to summarize this work's contribution</p>
<disp-quote content-type="editor-comment">
<p>Explaining the claim about the computational advantage of using quantum determinantal point processes for the imputation methods:</p>
</disp-quote>
<p>In the fourth paragraph of the &quot;Discussion&quot; section (page 8),  we give an imputation example by numerically comparing the classical and quantum algorithms running time for DPP sampling, which shows the advantage of using the quantum algorithm.</p>
<disp-quote content-type="editor-comment">
<p>Regarding running time for classical DPP and quantum DPP sampling algorithms:</p>
</disp-quote>
<p>We included Table VIII (page 13) that compares the preprocessing and sampling complexities for both classical and quantum DPP algorithms, we consider the case where we sample d rows from an (n,d) matrix and n=O(d) which is usually the case for our DPP-Random Forest algorithm</p>
<p>We added some details regarding the quantum advantage in the first paragraph of page 12</p>
<disp-quote content-type="editor-comment">
<p>Regarding the comment about the modest improvement of the DPP methods and questions about their practical benefit:</p>
</disp-quote>
<p>As mentioned in the third paragraph of the &quot;Discussion&quot; section, we point out that the consistency of the improvement and the removal of variance as a result of using the DPP and deterministic DPP methods make our methods very beneficial to use on clinical data. Further exploration with different data sets can provide a more result in a more complete understanding of the practical advantages of the methods</p>
<disp-quote content-type="editor-comment">
<p>Algorithmic complexity of the deterministic DPP algorithm:</p>
</disp-quote>
<p>Detailed in the last sentence of the &quot;Determinantal Point Processes&quot; subsection of the &quot;Methods&quot; section: O(N^2 d) for the preprocessing step and O(Nd^3) for the sampling step</p>
<disp-quote content-type="editor-comment">
<p>Running time for the quantum deterministic DPP sampling and how it is done in practice:</p>
</disp-quote>
<p>While it is difficult to assess the real running time for the quantum detDPP algorithm for large circuits (100 or more qubits), due to the unavailability of such devices, we give more details about our practical implementation in the last paragraph of the &quot;Methods&quot; section. In our case (up to 10 qubits) we used 1000 shots to sample the highest probability elements.</p>
<disp-quote content-type="editor-comment">
<p>On which quantum simulator was used</p>
</disp-quote>
<p>We point out in the first paragraph of page 5 that we employ the qiskit noiseless simulator</p>
</body>
</sub-article>
</article>