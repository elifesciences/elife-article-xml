<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">95607</article-id>
<article-id pub-id-type="doi">10.7554/eLife.95607</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95607.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>ElectroPhysiomeGAN: Generation of Biophysical Neuron Model Parameters from Recorded Electrophysiological Responses</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5597-5142</contrib-id>
<name>
<surname>Kim</surname>
<given-names>Jimin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0419-3543</contrib-id>
<name>
<surname>Peng</surname>
<given-names>Minxian</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chen</surname>
<given-names>Shuqi</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9232-1420</contrib-id>
<name>
<surname>Liu</surname>
<given-names>Qiang</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<email>qiangliuemail@gmail.com</email>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3136-4531</contrib-id>
<name>
<surname>Shlizerman</surname>
<given-names>Eli</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>shlizee@uw.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>Department of Electrical and Computer Engineering, University of Washington</institution></institution-wrap>, <city>Seattle</city>, <country country="US">United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>Department of Applied Mathematics, University of Washington</institution></institution-wrap>, <city>Seattle</city>, <country country="US">United States</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03q8dnn23</institution-id><institution>Department of Neuroscience, City University of Hong Kong</institution></institution-wrap>, <city>Hong Kong</city>, <country country="HK">Hong Kong</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Bhalla</surname>
<given-names>Upinder S</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>National Centre for Biological Sciences</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing Interest Statement: The authors have declared no competing interest.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-06-13">
<day>13</day>
<month>06</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-02-13">
<day>13</day>
<month>02</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP95607</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-01-29">
<day>29</day>
<month>01</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-12-20">
<day>20</day>
<month>12</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.12.19.572452"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-06-13">
<day>13</day>
<month>06</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95607.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.95607.1.sa2">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95607.1.sa1">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95607.1.sa0">Reviewer #2 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Kim et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Kim et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-95607-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Recent advances in connectomics, biophysics, and neuronal electrophysiology warrant modeling of neurons with further details in both network interaction and cellular dynamics. Such models may be referred to as ElectroPhysiome, as they incorporate the connectome and individual neuron electrophysiology to simulate neuronal activities. The nervous system of <italic>C. elegans</italic> is considered a viable framework for such ElectroPhysiome studies due to advances in connectomics of its somatic nervous system and electrophysiological recordings of neuron responses. In order to achieve a simulated ElectroPhysiome, the set of parameters involved in modeling individual neurons need to be estimated from electrophysiological recordings. Here, we address this challenge by developing a deep generative estimation method called ElectroPhysiomeGAN (EP-GAN), which once trained, can instantly generate parameters associated with the Hodgkin-Huxley neuron model (HH-model) for multiple neurons with graded potential response. The method combines Generative Adversarial Network (GAN) architecture with Recurrent Neural Network (RNN) Encoder and can generate an extensive number of parameters (&gt;170) given the neuron’s membrane potential responses and steady-state current profiles. We validate our method by estimating HH-model parameters for 200 synthetic neurons with graded membrane potential followed by 9 experimentally recorded neurons (where 6 of them newly recorded) in the nervous system of <italic>C. elegans</italic>. Comparison of EP-GAN with existing estimation methods shows EP-GAN advantage in the accuracy of estimated parameters and in the inference speed. The advantage is especially significant when a large number of parameters is being inferred. In addition the architecture of EP-GAN permits inference of parameters even when partial membrane potential and steady-state currents profile are given as inputs. EP-GAN is designed to leverage the generative capability of GAN to align with the dynamical structure of HH-model, and thus able to achieve such performance.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>1. Author list updated
2. Abstract/Introduction
- We further emphasized EP-GAN strength in parameter inference of detailed neuron parameters vs specialized models with reduced parameters.
3. Results
- We further elaborated on the method of training EP-GAN on synthetic neurons and validating on both synthetic and experimental neurons.
- We added a new section Statistical Analysis and Loss Extension which includes:
-- Statistical evaluation of baseline EP-GAN and other methods on neurons with multi recording membrane potential responses/steady-state currents data: AWB, URX, HSN (Figure 5, Table 5)
-- Evaluation of EP-GAN with added resting potential loss + longer simulations to ensure stability of membrane potential (EP-GAN-E)
4. Methods
- We added a detailed explanation on &quot;inverse gradient process&quot;
- We added detailed current/voltage-clamp protocols for both synthetic and experimental validation and prediction scenarios (table 6)
5. Supplementary
- We added error distribution and representative samples for synthetic neuron validations (Fig S1)
- We added membrane potential response statistical analysis plots for existing methods for AWB, URX, HSN (Fig S6)
- We added steady-state currents statistical analysis plots on EP-GAN + existing methods for AWB, URX, HSN (Fig S7)
- We added mean membrane potential errors for AWB, URX, HSN normalized by empirical standard deviations for all methods (Table S4)
- Supplementary files updated to reflect the results from the extended method.
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Models of the nervous system aim to achieve biologically detailed simulations of large-scale neuronal activity through the incorporation of both structural connectomes (connectivity maps) and individual neural dynamics. The nervous system of <italic>Caenorhabditis elegans</italic> (<italic>C. elegans</italic>) is considered a framework for such a model as the connectome of its somatic nervous system for multiple types of interaction is mapped [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c3">3</xref>]. In addition to the connectome, advances in electrophysiological methodology allow the recording of whole-cell responses of individual neurons. These advances provide biophysically relevant details of individual neuro-dynamical properties and warrant a type of model for the <italic>C. elegans</italic> nervous system incorporating both the connectomes and individual biophysical processes of neurons. Such a model could be referred to as <italic>ElectroPhysiome</italic>, as it incorporates a layer of individual neural dynamics on top of the layer of inter-cellular interactions facilitated by the connectome.</p>
<p>The development of nervous system models that are further biophysically descriptive for each neuron, i.e., modeling neurons using the Hodgkin-Huxley type equations (HH-model), requires fitting a large number of parameters associated with ion channels found in the system. For a typical single neuron, these parameters could be tuned via local optimizations of individual ion channel parameters estimated separately to fit their respective <italic>in-vivo</italic> channel recordings such as activation/inactivation curves [<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>]. Such method requires multiple experiments to collect each channel data and when such experiments are infeasible, the parameters are often estimated through hand-tuning. In the context of developing the ElectroPhysiome of <italic>C. elegans</italic>, the method would have to model approximately 300 neurons each including an order of hundreds of parameters associated with up to 15 to 20 ionic current terms (with some of them having unknown ion channel composition), which would require large experimental studies [<xref ref-type="bibr" rid="c7">7</xref>]. Furthermore, the fitted model may not be the unique solution as different HH-parameters can produce similar neuron activity [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>]. As these limitations also apply for general neuron modeling tasks beyond <italic>C. elegans</italic> neurons, there has been an increasing search for alternative fitting methods requiring less experimental data and manual interventions.</p>
<p>A promising direction in associating model parameters with neurons has been the simultaneous estimation of all parameters of an individual neuron given only electrophysiological responses of cells, such as membrane potential responses and steady-state current profiles. Such an approach requires significantly less experimental data per neuron and offers more flexibility with respect to trainable parameters. The primary aim of this approach is to model macroscopic cell behaviors in an automated fashion. Indeed, several methods adopting the approach have been introduced. Buhry et al 2012 and Laredo et al 2022 utilized the Differential Evolution (DE) method to simultaneously estimate the parameters of a 3-channel HH-model given a whole-cell membrane potential responses recording [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>]. Naudin et al 2022 further developed the DE approach and introduced the Multi-objective Differential Evolution (DEMO) method to estimate 22 HH-parameters of 3 non-spiking neurons in <italic>C. elegans</italic> given their whole-cell membrane potential responses and steady-state current profiles [<xref ref-type="bibr" rid="c16">16</xref>]. The study was a significant step toward modeling whole-cell behaviors of <italic>C. elegans</italic> neurons in a systematic manner. From statistical standpoint, Wang et al 2022 used the Markov-Chain-Monte-Carlo method to obtain the posterior distribution of channel parameters for HH-models featuring 3 and 8 ion channels (2 and 9 parameters respectively) given the simulated membrane potential responses data [<xref ref-type="bibr" rid="c17">17</xref>]. From an analytic standpoint, Valle et al 2022 suggested an iterative gradient descent based method that directly manipulates HH-model to infer 3 conductance parameters and 3 exponents of activation functions given the measurements of membrane potential responses [<xref ref-type="bibr" rid="c18">18</xref>]. Recent advances in machine learning gave rise to deep learning based methods which infer steady-state activation functions and posterior distributions of 3-channel HH-model parameters inferred by an artificial neural network model given the membrane potential responses data [<xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c20">20</xref>].</p>
<p>While these methods suggest that simultaneous parameter estimation from macroscopic cell data is indeed possible through a variety of techniques, it is largely unclear whether they can be extended to fit more detailed HH-models featuring a large number of channels and parameters (e.g., <italic>C. elegans</italic> neurons) [<xref ref-type="bibr" rid="c7">7</xref>]. Furthermore, for most of the above methods, the algorithms require an independent (from scratch) optimization process for fitting each individual neuron, making them difficult to scale up the task toward a large number of neurons.</p>
<p>Here we propose a new machine learning approach that aims to address these aspects for the class of non-spiking neurons, which constitute the majority of neurons in <italic>C. elegans</italic> nervous system [<xref ref-type="bibr" rid="c21">21</xref>]. Specifically, we develop a deep generative neural network model (GAN) combined with a recurrent neural network (RNN) encoder called ElectroPhysiomeGAN (EP-GAN), which directly maps electrophysiological recordings of a neuron, e.g., membrane potential responses and steady-state current profiles, to HH-model parameters of arbitrary dimensions (<xref rid="fig1" ref-type="fig">Figure 1</xref>). EP-GAN can be trained with simulation data informed by a generic HH-model encompassing a large set of arbitrary ionic current terms and thus can generalize its modeling capability to multiple neurons. Unlike typical GAN architecture trained solely with adversarial losses, we propose to implement additional regression loss for reconstructing the given membrane potential responses and current profiles from generated parameters, thus improving the accuracy of the generative model. In addition, due to RNN component of EP-GAN, the approach supports input data with missing features such as incomplete membrane potential responses and current profiles.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Estimation of HH-model parameters from membrane potential and steady-state current profiles.</title>
<p>Given the membrane potential responses (V) and steady-state current profiles (IV) of a neuron, the task is to predict biophysical parameters of Hodgkin-Huxley type neuron model (Left). We use Encoder-Generator approach to predict the parameters (Right)</p></caption>
<graphic xlink:href="572452v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We validate our method to estimate HH-model parameters of 200 synthetic non-spiking neurons followed by applying it to three previously recorded non-spiking neurons of <italic>C. elegans</italic>, namely RIM, AFD, and AIY. Studies have shown that membrane potential responses of these neurons can be well modeled with typical HH-model formulations with 22 parameters [<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c22">22</xref>]. We show that when trained with a more detailed HH-model consisting of 15 ionic current terms resulting with 176 trainable parameters, EP-GAN can predict parameters reproducing their membrane potential responses with higher accuracy in reconstruction of membrane potential and significantly faster speed than existing algorithms such as Multi-Objective DE and Genetic Algorithms. Through ablation studies on input data, we show that EP-GAN retains its prediction capability when provided with incomplete membrane potential responses and current profiles. We also perform ablation studies on the model architecture components to elucidate each component’s contributions toward the accuracy of the predicted parameters. To further test EP-GAN, we estimate HH-model parameters for 6 newly recorded non-spiking <italic>C. elegans</italic> neurons: AWB, AWC, URX, RIS, DVC, and HSN, whose membrane potential responses were not previously modeled and perform additional statistical analysis of predicted membrane potential and current dynamics of neurons AWB, URX, HSN for which multiple recordings are available. We further demonstrate EP-GAN’s ability for extension by proposing an additional loss which optimization can supplement prediction capability.</p>
<p>Our results suggest that EP-GAN can learn a translation from electrophysiologically recorded responses and propose projections of them to parameter space. EP-GAN method is currently limited to non-spiking neurons in <italic>C. elegans</italic> as it was designed and trained with HH-model describing the ion channels of these neurons. EP-GAN applications can be potentially extended toward resolving neuron parameters in other organisms since non-spiking neurons are found within animals across different species [<xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>].</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>We evaluate EP-GAN with respect to 4 existing evolutionary algorithms introduced for general parameter estimation: NSGA2, DEMO, GDE3, and NSDE. Specifically, NSGA2 is a variant of the Genetic Algorithm utilizing a non-dominated sorting survival strategy, and is a commonly used bench-mark algorithm for multi-objective optimization problems including HH-model fitting [<xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c32">32</xref>, <xref ref-type="bibr" rid="c33">33</xref>]. DEMO, GDE3 and NSDE are variants of multi-objective differential evolution (DE) algorithms that combine DE mutation with pareto based ranking and crowding distance sorting applied in NSGA2’s survival strategy [<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c36">36</xref>]. These methods have been proposed as more effective methods than direct DE for estimation of HH-model parameters [<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c39">39</xref>]. In particular, DEMO has been successfully applied to estimate HH-model parameters for non-spiking neurons in <italic>C. elegans</italic> and we include additional comparison studies with EP-GAN in the sub-section <italic>Additional Comparisons with DEMO</italic> [<xref ref-type="bibr" rid="c16">16</xref>]. All 4 methods support multi-objective optimization over large parameter space allowing them to have similar setups as EP-GAN. All 4 methods were implemented in Python where DEMO uses the algorithm proposed in [<xref ref-type="bibr" rid="c16">16</xref>] whereas NSGA2, GDE3 and NSDE were implemented using Pymoo package [<xref ref-type="bibr" rid="c40">40</xref>].</p>
<p>For the HH-model to be estimated, we use the formulation introduced in [<xref ref-type="bibr" rid="c7">7</xref>]. The model features 15 ion channels that were found in <italic>C. elegans</italic> and other organisms expressing homologous channels and is considered the most detailed neuron model for the organism. The model has a total of <bold>216</bold> parameters, of which we identify 176 of them have approximate ranges with lower and upper bounds that can be inferred from the literature [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c41">41</xref>]. We thus target these 176 parameters as trainable parameters for all methods. For a detailed list of all 216 parameters included in the model and 176 parameters used for training, see [<xref ref-type="bibr" rid="c7">7</xref>] and the included table <italic>predicted parameters</italic> in supplementary materials.</p>
<sec id="s2a">
<title>Validation Scenarios (Synthetic)</title>
<p>We first validate EP-GAN by training and testing using synthetic neurons. Each synthetic neuron training sample consists of two inputs: 11 simulated membrane potential traces and 20 steady-state currents. For each neuron, the output is the set of 176 HH-parameters to be inferred. Each membrane potential trace is simulated for 7 seconds for a given stimulus according to current-clamp protocol where the stimulation is applied for 5 seconds at [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c6">6</xref>] seconds and no stimulation is applied at <italic>t</italic> = [0, 1) and <italic>t</italic> = (6, 7]. Similarly, steady-state currents are computed across 20 voltage states according to voltage-clamp protocol (see <xref rid="tbl6" ref-type="table">Table 6</xref> for detailed current/voltage clamp protocols used for synthetic neurons). The output HH-parameters are of the synthetic neurons chosen randomly from lower and upper bounds as previously described. For training EP-GAN, we simulate the total of 32,000 (32k) synthetic neurons where EP-GAN achieves both good predictive performance and training time. Specifically, 32k is a training data size in which membrane potential errors from the test set are within the mean RMSE recording error (∼6.79mV) obtained from experimental neurons with multiple membrane potential recording data. For more details on generating synthetic neuron training samples, see <italic>Generating Training Data</italic> under the Methods section.</p>
<p>To initially test EP-GAN performance, we evaluate the EP-GAN predicted parameters for 100 synthetic neurons from the training set (validation set) and 100 neurons outside of training set (test set). Since EP-GAN is based on a GAN architecture where training loss is not defined directly as output parameters accuracy but as adversarial loss, re-evaluating training samples with more commonly used error metric and comparing them with test data ensures that EP-GAN predictive performance generalizes to arbitrary neurons. For a given neuron being evaluated, EP-GAN predicted HH-parameters are obtained as follows: For each training epoch, EP-GAN generates a set of HH-parameters for the neuron and at the end of the training, the parameter set which achieved the lowest Root Mean Square Error (RMSE) error (detailed descriptions of its calculation provided in supplementary) of membrane potential responses during stimulation period w.r.t. ground truth is reported. In the case of multiple <italic>N</italic> -neurons being evaluated, the same procedure is followed except EP-GAN generates <italic>N</italic> -parameter sets in parallel at each epoch. Such multi-inference computation is possible due to EP-GAN being a neural network, where parallel processing of inputs can be done with minimal impact on inference speed. Using these procedures, EP-GAN predicted HH-parameters result in mean membrane potential RMSE error of <bold>5.84mV</bold> and <bold>5.81mV</bold> for validation and test set respectively (See Figure S1B for representative samples). These errors are within the mean recording RMSE error of 6.79mV obtained from experimental neurons and for both training and testing samples, the error distributions were skewed uni-modal type where the majority of the errors fall within 10mV (Figure S1A).</p>
</sec>
<sec id="s2b">
<title>Validation Scenarios (Experimental)</title>
<p>Following the validation of synthetic neurons parameters inference accuracy, we study HH-parameters for 3 experimentally recorded non-spiking neurons of <italic>C. elegans</italic> - RIM, AFD, and AIY, and compare the performance of EP-GAN with NSGA2, DEMO, GDE3, and NSDE. While these neurons have a single set of membrane potentials and steady-state currents recording data, they are publicly available and their modeling descriptions were elaborated by previous works, making them common benchmarks [<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c42">42</xref>]. We perform additional evaluations of algorithms for neurons with multiple sets of recording data, i.e. AWB, URX, HSN, and include in the sub-section <italic>Statistical Analysis and Loss Extensions</italic>. Unlike EP-GAN which can optimize multiple neurons in parallel, NSGA2, DEMO, GDE3 and NSDE are evolutionary methods where the optimization is done <bold>from scratch</bold> for each neuron. We therefore evaluate their respective performances by normalizing on the total amount of compute allocated during the entire optimization task. Specifically, for all methods, the maximum number of synthetic neuron simulations is set to approximately <italic>∼</italic>32,000. i.e., EP-GAN is trained with 32k synthetic neurons identical to synthetic validation scenario, and for NSGA2, DEMO, GDE3, and NSDE, each algorithm is allocated with up to 10.8k simulations (i.e., ∼11k synthetic neurons) during the search phase of HH-parameters for each neuron, thus adding up to total of ∼32k synthetic neurons for all 3 neurons. For each parameter search phase of NSGA2, DEMO, GDE3 and NSDE, the parameter set candidates are recorded at each iteration, and the candidate achieving the lowest membrane potential responses error during stimulation period is reported at the end of the search phase. For Multi-objective DE methods (DEMO, GDE3 and NSDE2), we follow the same configurations used in the literature to set their parameters and optimization scheme [<xref ref-type="bibr" rid="c16">16</xref>]. Specifically, we use the root-mean-square error normalized to the noise level for the objective functions for both membrane potential responses and steady-state current and set crossover parameter CR and scale factor F to 0.3 and 1.5 respectively. For all 4 methods, NP is set to 600 with total 18 iterations (i.e., <italic>∼</italic> 11<italic>k</italic> simulations).</p>
<p><xref rid="fig2" ref-type="fig">Figure 2</xref> shows comparison between reconstructed membrane potential responses of predicted parameters vs ground truth membrane potential responses for all 4 methods. We include predicted steady-state current profiles in supplement Figure S2. <xref rid="fig2" ref-type="fig">Figure 2</xref> illustrates that EP-GAN can reconstruct membrane potential responses close to ground truth responses. Indeed, when comparing the overall RMSE error (<xref rid="tbl1" ref-type="table">Table 1</xref>) between ground truth and reconstructed membrane potential responses, EP-GAN overall error (6.7mV) is 40% lower than that of NSGA2 (11.2mV) followed by NSDE (20.9mV), GDE3 (23.4mV) and DEMO (28.1mV). The overall error of 6.7mV is also in the similar ballpark of ∼ 5.8mV recorded during synthetic validation. Among the three neurons used for prediction, EP-GAN showed the best accuracy for AIY neuron (6.2mV), followed by AFD (6.4mV) and RIM (7.5mV).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>The lowest membrane potential responses errors achieved by each method for experimental validation scenarios.</title>
<p># of simulations represents the total number of synthetic neuron simulations each method used during optimization. For all scenarios, the error represents the average RMSE between ground truth and predicted membrane potential responses in each time point across all membrane potential responses traces.</p></caption>
<graphic xlink:href="572452v2_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Comparison of membrane potential responses reconstructed from predicted parameters (experimental validation scenarios).</title>
<p>Each row and column corresponds to predicted neuron and method respectively. For each neuron we show ground truth membrane potential responses (Black) against the reconstructed membrane potential responses (red) from the predicted parameters.</p></caption>
<graphic xlink:href="572452v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2c">
<title>Ablation Studies</title>
<p>To test the robustness of EP-GAN when incomplete input data is given, we provide the model with membrane potential responses and steady-state current profiles with missing data points. For each membrane potential responses and current profile, the data is reduced by 25%, 50%, and 75% each. For membrane potential responses data, the ablation is done on stimulus space where a 50% reduction corresponds to removing upper half of the membrane potential responses traces each associated with a stimulus. For steady-state current profile, we remove the first <italic>n</italic>-data points where they are instead extrapolated using linear interpolation with existing data points.</p>
<p>Our results show that the quality of the predicted parameters depends on membrane potential responses input (<xref rid="fig3" ref-type="fig">Figure 3A</xref>, <xref rid="tbl2" ref-type="table">Table 2</xref> Row 2, Figure S3A (predicted steady-state currents)). In particular, EP-GAN preserves baseline accuracy up to a 25% reduction in membrane potential responses with the overall error of 16.8mV (i.e. within 2 std from the mean error in modeling synthetic neurons) but becomes less accurate when it increases to 50%. With respect to steady-state current profiles, those have an effect on the accuracy (<xref rid="fig3" ref-type="fig">Figure 3B</xref>, <xref rid="tbl2" ref-type="table">Table 2</xref> Row 1, Figure S3B) but the effect is more minor than that of reducing membrane potential responses and the error doesn’t vary when scale is reduced. The results imply that the membrane potential responses play a major role in the performance of the model.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><title>Ablation studies.</title>
<p><bold>Top</bold>: membrane potential responses errors achieved for EP-GAN when provided with incomplete input data. <bold>Bottom</bold>: membrane potential responses errors achieved for EP-GAN upon using only adversarial loss (A) and using adversarial + current reconstruction loss (A, IV) and all three loss components (A, IV, V)</p></caption>
<graphic xlink:href="572452v2_tbl2.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3:</label>
<caption><title>The lowest membrane potential responses errors achieved by each method for prediction scenarios.</title>
<p># of simulations represents the total number of synthetic neuron simulations each method used during optimization.</p></caption>
<graphic xlink:href="572452v2_tbl3.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Input data ablation on EP-GAN.</title>
<p><bold>A</bold>: Reconstructed membrane potential responses when given with incomplete membrane potential responses data. Percentages in parenthesis represent the remaining portion of input membrane potential responses trajectories. <bold>B</bold>: Reconstructed membrane potential responses when given with incomplete current profile.</p></caption>
<graphic xlink:href="572452v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We also performed ablation studies on model architecture by removing each loss component of the Generator module, allowing us to evaluate the relative contribution of each loss to accuracy. From <xref rid="tbl2" ref-type="table">Table 2</xref> bottom, we see that removing the membrane potential loss term (V) results in a significant loss in performances for all three neurons as expected, where the reduction for AIY neuron is the most prominent. Upon removing the steady-state current reconstruction loss term (IV) in addition to membrane potential reconstruction loss, we see further reduction in overall performance. These results highlight the significance of the reconstruction losses in aligning the Generator to produce the desired outputs.</p>
</sec>
<sec id="s2d">
<title>Prediction Scenarios</title>
<p>To further test if EP-GAN can generalize to other testing scenarios, we applied EP-GAN on 6 additional neurons - AWB, AWC, URX, RIS, DVC, and HSN, which recording data is novel and these neurons were not previously modeled. Similar to modeling RIM, AFD, and AIY, EP-GAN with identical training set, size of 32k synthetic neurons, was used where membrane potential responses were rearranged for each training sample according to neuron specific current-clamp protocol (See <xref rid="tbl6" ref-type="table">Table 6</xref> for details). For NSGA2, DEMO, GDE3 and NSDE, each algorithm was allocated with 5.4k simulations for each neuron, adding up to 32k total simulations with 6 neurons. From <xref rid="fig4" ref-type="fig">Figure 4</xref>, Figure S4 (other methods) and Figure S5 (steady-state currents), we observe that EP-GAN is indeed able to predict parameters that closely recover ground truth membrane potential responses and steady-state currents for all 6 neurons. Upon inspecting the RMSE error and membrane potential responses, EP-GAN shows higher accuracy than other methods with 50% - 80% less error on average, and also records smaller overall error of 5.35mV than EP-GAN error for validation scenarios (∼ 6.7mV) and on synthetic data (∼ 5.8mV). These results indicate that the predictive capability of EP-GAN can generalize to a novel set of neurons with a relatively small training data size of 32k without need for retraining if same format of recordings is used.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Illustration of membrane potential responses reconstructed from predicted parameters (prediction scenarios).</title>
<p>Membrane potential responses trajectories reconstructed from EP-GAN predicted parameters (red) overlaid on top of the ground truth recording membrane potential responses (black).</p></caption>
<graphic xlink:href="572452v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2e">
<title>Parameter Inference Time</title>
<p>In addition to its capability to generalize over multiple neurons, we evaluated EP-GAN for scalability by assessing its overall inference time to other methods. Indeed, for estimation tasks involving many neurons, it is essential that the method is scalable so that the predictions are done within a reasonable time. In <xref rid="tbl4" ref-type="table">Table 4</xref> we list the computation times involved in each approach for validation scenarios. We show that while EP-GAN requires time for initial data generation and training, it is of orders of magnitude faster in the actual estimation process. As an example, given a hypothetical task of modeling all 279 somatic neurons in the <italic>C. elegans</italic> nervous system, it would take DEMO, GDE3, NSDE and NSGA2 more than 67 days (assuming 11k simulations per neuron) whereas, for EP-GAN, given that all neurons have identical recording protocol, the process would be done within a day under the similar training setup. For larger number of neurons, computational requirement of existing methods would grow linearly while EP-GAN would require constant time to complete the inference. Such scalability largely benefits from neural network being an inherently parallel architecture, allowing it to take multiple neuron profiles and output corresponding parameters in a single forward pass [<xref ref-type="bibr" rid="c43">43</xref>].</p>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4:</label>
<caption><title>Inference Time Comparison between methods for validation scenarios.</title>
<p>For EP-GAN, a training consists of 100 epochs.</p></caption>
<graphic xlink:href="572452v2_tbl4.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s2f">
<title>Additional Comparisons with DEMO</title>
<p>EP-GAN aim and primary application is similar to the algorithm introduced in [<xref ref-type="bibr" rid="c16">16</xref>] which utilizes DEMO (DE for Multi-objective Optimization). Both methods aim to estimate HH-model parameters for non-spiking neurons of <italic>C. elegans</italic>. DEMO is applied to 3 channels HH equations (<italic>I</italic><sub><italic>Ca,p</italic></sub> + <italic>I</italic><sub><italic>Kir</italic></sub> + <italic>I</italic><sub><italic>K,t</italic></sub> + <italic>I</italic><sub><italic>L</italic></sub>-model and <italic>I</italic><sub><italic>Ca,t</italic></sub> + <italic>I</italic><sub><italic>Kir</italic></sub> + <italic>I</italic><sub><italic>K,p</italic></sub> + <italic>I</italic><sub><italic>L</italic></sub>-model) consisting of 22 parameters to model RIM, AFD and AIY. This model is 12% in size compared to the dimension of the model (176 parameters) that we use previously. In addition to validating EP-GAN on more detailed model, we evaluate its performance on the same model for which DEMO was applied. We observe that for all simulation sizes for training (32k, 64k, 96k), EP-GAN achieves membrane potential response errors on par with those obtained with DEMO (Table S2). While the best overall error for EP-GAN (4.9mV) is higher than that of DEMO (3.2mV), EP-GAN requires significantly a smaller number of simulations (96k · 2 (EP-GAN) vs 12,000k · 3 (DEMO)). To further investigate the correlation between the number of simulations and DEMO performance, we re-evaluate DEMO with respect to the detailed HH-model of [<xref ref-type="bibr" rid="c7">7</xref>] when given with 64k simulations (∼ 107 iterations, <italic>NP</italic> = 600) for each neuron instead of 11k and 5.4k simulations considered in validation and prediction scenarios. From Table S3, we see that while DEMO overall error is improved, EP-GAN trained with a single training set of 32k samples maintains better results for both validation (6.7mV (EP-GAN) vs 26.8mV (DEMO)) and prediction scenarios (5.35mV (EP-GAN) vs 24.2mV (DEMO)). These results suggest that EP-GAN can be applied to different HH-models and efficiently generate parameters with consistent accuracy.</p>
</sec>
<sec id="s2g">
<title>Statistical Analysis and Loss Extensions</title>
<p>In addition to neurons with a single set of recording data, we evaluate EP-GAN and EP-GAN with extended loss along with existing methods on neurons AWB, URX, HSN for which multiple recording data sets are available (<italic>n</italic> = 3). These neurons allow statistical analysis on predicted membrane potential responses and steady-state currents w.r.t. experimental data. For EP-GAN with extended loss (EP-GAN-E), a supplementary generator loss is added to further optimize transient membrane potential traces during pre and post-activation.</p>
<p>EP-GAN-E is also supplemented with 5 seconds of stabilization period of zero stimulus prior to stimulation during simulations to ensure added loss terms result in stable membrane potential trajectories under longer timespan. For each neuron, we compute confidence bands of 2 standard deviations (<italic>p</italic> = 0.05, adjusted with sample size) for each membrane potential trace associated with a stimulus and calculate the percentage of predicted membrane potential trace that fall within the band (i.e. empirical range) (<xref rid="tbl5" ref-type="table">Table 5</xref>, <xref rid="fig5" ref-type="fig">Figure 5</xref>). For membrane potential traces plot for other methods w.r.t. confidence bands, see Figure S6. In addition to error evaluations using confidence bands, we compute the mean membrane potential error normalized by empirical standard deviations for each method as typically done in literature and summarize the results in Table S4.</p>
<table-wrap id="tbl5" orientation="portrait" position="float">
<label>Table 5:</label>
<caption><title>Percentages of predicted membrane potential trajectories within empirical range.</title>
<p>The predicted membrane potential at time <italic>t</italic> is considered acceptable if it falls within <italic>&lt;</italic> 2 standard deviations from experimental mean voltage at that time point. All methods use identical setups as Table 3.</p></caption>
<graphic xlink:href="572452v2_tbl5.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tbl6" orientation="portrait" position="float">
<label>Table 6:</label>
<caption><title>Simulation protocol for neurons.</title>
<p>Both neuron groups (training/testing) in synthetic neurons used identical protocol. Neurons modeled with EP-GAN-E are marked with (Ext) where simulation duration and stimulation periods are extended by 5 seconds to ensure stability of resting membrane potentials</p></caption>
<graphic xlink:href="572452v2_tbl6.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Predicted individual membrane potentials compared to ground truths with confidence bands for EP-GAN baseline (Top) vs EP-GAN-E (Bottom).</title>
<p>Each column represents an individual membrane potential trajectory associated with a current stimulus. Membrane potential responses trajectories reconstructed from predicted parameters (red) are overlaid on top of the ground truth recording membrane potential responses (black) where the grey shade represents the confidence band of 2 standard deviations obtained from ground truth experimental recordings (<italic>n</italic> = 3).</p></caption>
<graphic xlink:href="572452v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Our results show that the average percentage of predicted membrane potential traces within empirical range for AWB, URX and HSN with EP-GAN baseline are 51.7%, 92.9% and 97.7% respectively resulting in mean percentage of 80.8%. The percentage was higher than those of DEMO (57.9%), GDE3 (37.9%), NSDE (38.0%) and NSGA (60.2%) (<xref rid="tbl5" ref-type="table">Table 5</xref>). With extended generator loss supplemented with 5 seconds stabilization period (EP-GAN-E), the overall percentage has improved to 85% with significant improvement on AWB neuron (51.7% → 72.7%) but slight decrease in percentages for URX (92.9% → 90.5%) and HSN (97.7% → 91.8%). Evaluations of membrane potential errors normalized by empirical standard deviations also show similar results where EP-GAN baseline and EP-GAN-E have average error of 1.0 std and 0.7 std respectively, outperforming DEMO (1.7 std), GDE3 (2.0 std), NSDE (3.0 std) and NSGA (1.5 std).</p>
<p>In addition to the voltage traces, we also perform similar analysis on predicted steady-state currents for AWB, URX and HSN (Figure S7). Interestingly, we notice that the improvement in membrane potential prediction performance is not necessarily translated to steady-state currents. For instance, AWB steady-state currents generated from baseline EP-GAN had more current values falling within the empirical range than that of EP-GAN-E, but had lower accuracy in membrane potential prediction (<xref rid="fig2" ref-type="fig">Figure 2</xref>). Such competitive nature between membrane potential vs steady-state current optimizations has indeed reported in literature [<xref ref-type="bibr" rid="c16">16</xref>].</p>
</sec>
</sec>
<sec id="s3">
<title>Methods</title>
<p>We divide the methods section into two parts. In the first part we describe the detailed architecture of EP-GAN including its sub-modules with the protocol for training. In the second part, we describe the dataset and experimental protocol of novel neuron recording of AWB, AWC, URX, RIS, DVC, and HSN from <italic>C. elegans</italic> nervous system.</p>
<sec id="s3a">
<title>Architecture of EP-GAN</title>
<sec id="s3a1">
<title>Deep Generative Model for Parameter Prediction</title>
<p>EP-GAN receives neuron recording data such as membrane potential responses and steady-state current profiles and generates a parameter set that is associated with them in terms of simulating the inferred HH-model and comparing the simulated outcome with the inputs (<xref rid="fig1" ref-type="fig">Figure 1</xref>,<xref rid="fig6" ref-type="fig">6</xref>). We choose a deep generative model approach, specifically Generative Adversarial Network (GAN) as a base architecture of EP-GAN. The key advantage of GAN is that it is designed to generate artificial data that closely resembles real data. The generative nature of GAN is advantageous for addressing the multi-modal nature of our problem, where the parameter solution is not guaranteed to be unique for a given neuron recording. Indeed, several computational works attempting to solve inverse HH-model have pointed out the ill-posed nature of the parameter solutions [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c44">44</xref>, <xref ref-type="bibr" rid="c45">45</xref>]. Our approach is therefore leveraging GAN to learn a <italic>domain of parameter sets</italic> compatible with neuron recording instead of mapping onto a single solution. GAN consists of two separate networks, Generator and Discriminator. The goal of the Generator is to generate outputs that are indistinguishable from real data whereas the Discriminator’s goal is to distinguish outputs that are generated by the generator against real data. Throughout training, the Generator and the Discriminator compete with each other (zero-sum game) until they both converge to optimal states [<xref ref-type="bibr" rid="c46">46</xref>]. The particular architecture we use is Wasserstein GAN with gradient penalty (WGAN-GP), a variant of GAN architecture offering more stable training and faster convergence [<xref ref-type="bibr" rid="c47">47</xref>].</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Architecture of EP-GAN.</title>
<p>The architecture consists of an Encoder, Generator and Discriminator. Encoder compresses the membrane potential responses into a 1D vector (i.e., latent space) which is then concatenated with 1D steady-state current profile to be used as an input to both generator and discriminator. Generator translates the latent space vector into a vector of parameters <inline-formula><inline-graphic xlink:href="572452v2_inline25.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and the Discriminator outputs a scalar measuring the similarity between generated parameters <inline-formula><inline-graphic xlink:href="572452v2_inline26.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and ground truths <inline-formula><inline-graphic xlink:href="572452v2_inline27.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The Generator is trained with adversarial loss supplemented by reconstruction losses for both membrane potential responses and steady-state current profiles. The Discriminator is trained with discriminator adversarial loss only. Generator and Discriminator follow the architecture of Wasserstein GAN with gradient penalty (WGAN-GP) for more stable learning.</p></caption>
<graphic xlink:href="572452v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s3a2">
<title>Encoder Module</title>
<p>In addition to Generator and Discriminator, we implement Encoder module which pre-processes the input membrane potential responses for Generator and Discriminator. (<xref rid="fig6" ref-type="fig">Figure 6</xref> left). Specifically, Encoder serves two roles: i) Compression of membrane potential responses traces along the stimulus space, thus reducing its dimension from 2-dimensional to 1-dimensional, and ii) Translating membrane potential responses traces into a latent space encoding a meaningful internal representation for the Discriminator and Generator. The Encoder module uses Gated Recurrent Units (GRU) architecture, a variant of Recurrent Neural Network (RNN) to perform this task [<xref ref-type="bibr" rid="c48">48</xref>]. Each input sequence to a GRU cell at step <italic>t</italic> corresponds to the entire membrane potential response of length 500 (i.e., 500 time points) associated with a stimulus. The output of the Encoder is a latent space vector of length 500 encoding membrane potential responses information. The latent space vector is then concatenated with a 1D vector storing current profile information and fed to both Generator and Discriminator.</p>
</sec>
<sec id="s3a3">
<title>Discriminator Module</title>
<p>The goal of the Discriminator is given the input membrane potential responses and current profiles, to distinguish generated parameters from real ground truth parameters. The Discriminator receives as input the latent space vector from Encoder concatenated with generated or ground truth parameter vector and outputs a scalar representing the relative distance between two parameter sets (<xref rid="fig6" ref-type="fig">Figure 6</xref>, <xref ref-type="disp-formula" rid="eqn1">Eqn 1</xref>). Such a quantity is called Wasserstein distance or Wasserstein loss and differs from vanilla GAN Discriminator which only outputs 0 or 1. Wasserstein loss is known to remedy several common issues that arise from vanilla GAN such as vanishing gradient and mode collapse, leading to more stable training [<xref ref-type="bibr" rid="c47">47</xref>]. To further improve the training of WGAN architecture we supplement Wasserstein loss with a gradient penalty term, which ensures that the gradients of the Discriminator’s output with respect to the input has unit norms [<xref ref-type="bibr" rid="c49">49</xref>]. This condition is called Lipschitz continuity and prevents Discriminator outputs from having large variations when there is only small variations in the inputs [<xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c50">50</xref>]. Combined together, the Discriminator is trained with the following loss:
<disp-formula id="eqn1">
<graphic xlink:href="572452v2_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="572452v2_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="572452v2_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are the mean values of Discriminator outputs with respect to generated samples <inline-formula><inline-graphic xlink:href="572452v2_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and real samples <inline-formula><inline-graphic xlink:href="572452v2_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula> respectively and <inline-formula><inline-graphic xlink:href="572452v2_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the gradient penalty term modulated by <italic>λ</italic> where <inline-formula><inline-graphic xlink:href="572452v2_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the interpolation between generated and real samples with 0 <italic>≤ t ≤</italic> 1.</p>
</sec>
<sec id="s3a4">
<title>Generator Module</title>
<p>Being an adversary network of Discriminator, the goal of Generator is to fool the Discriminator by generating parameters that are indistinguishable from the real parameters. The Generator receives as input the concatenated vector from the Encoder and outputs a parameter vector (<xref rid="fig6" ref-type="fig">Figure 6</xref>). The module consists of 4 fully connected layers. Each parameter in the output vector is scaled between -1 and 1 which is then scaled back to the parameters’ original scales. The module is trained using 3 loss terms: i) Generator adversarial loss, ii) Membrane potential responses reconstruction loss and iii) Current reconstruction loss as follows:
<disp-formula id="eqn2">
<graphic xlink:href="572452v2_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn3">
<graphic xlink:href="572452v2_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn4">
<graphic xlink:href="572452v2_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="572452v2_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is Generator adversarial loss which is reciprocal of the mean Discriminator outputs with respect to generated samples and <inline-formula><inline-graphic xlink:href="572452v2_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="572452v2_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are <italic>L</italic><sub>1</sub> regression loss for reconstructed membrane potential responses and current profiles respectively. It’s important to note that <inline-formula><inline-graphic xlink:href="572452v2_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="572452v2_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are part of Generator’s computation graph and thus force Generator to optimize them on top of adversarial loss (<xref rid="fig7" ref-type="fig">Figure 7</xref>). The composite loss function of Generator makes EP-GAN a “model-informed” GAN as HH-model itself becomes part of the training process. Such networks have shown to be more data efficient during training as they don’t rely solely on training data to learn effective strategy [<xref ref-type="bibr" rid="c51">51</xref>, <xref ref-type="bibr" rid="c52">52</xref>]. The mathematical description of membrane potential responses and current reconstruction from generated parameter set <inline-formula><inline-graphic xlink:href="572452v2_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is as follows:</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Description of membrane potential responses and current reconstruction losses for the Generator.</title>
<p>Generated parameter vector <inline-formula><inline-graphic xlink:href="572452v2_inline28.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is used to evaluate membrane potential responses derivatives <italic>dV/dt</italic> at <italic>n</italic> time points sampled with fixed interval given the ground truth <inline-formula><inline-graphic xlink:href="572452v2_inline29.gif" mime-subtype="gif" mimetype="image"/></inline-formula> at those time points. The evaluated membrane potential responses derivatives are then used to reconstruct <inline-formula><inline-graphic xlink:href="572452v2_inline30.gif" mime-subtype="gif" mimetype="image"/></inline-formula> using the inverse gradient operation ∇<sup>−1</sup>. The reconstructed <inline-formula><inline-graphic xlink:href="572452v2_inline31.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is then compared with ground truth <inline-formula><inline-graphic xlink:href="572452v2_inline32.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to evaluate the membrane potential responses reconstruction loss <inline-formula><inline-graphic xlink:href="572452v2_inline33.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Current reconstruction is computed in a similar way via evaluating the currents at defined membrane potential responses steps <italic>V</italic> given generated parameters <inline-formula><inline-graphic xlink:href="572452v2_inline34.gif" mime-subtype="gif" mimetype="image"/></inline-formula> as inputs.</p></caption>
<graphic xlink:href="572452v2_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>
<disp-formula id="eqn5">
<graphic xlink:href="572452v2_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here <inline-formula><inline-graphic xlink:href="572452v2_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the right-hand-side function of HH-model which computes the membrane potential responses derivative at time <italic>t</italic> given the membrane potential responses <inline-formula><inline-graphic xlink:href="572452v2_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and parameter set <inline-formula><inline-graphic xlink:href="572452v2_inline15.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="572452v2_inline16.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the function that evaluates neuron’s current <italic>I</italic> given the membrane potential responses <inline-formula><inline-graphic xlink:href="572452v2_inline17.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and parameter set <inline-formula><inline-graphic xlink:href="572452v2_inline18.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Membrane potential responses are reconstructed by first evaluating their derivatives with respect to ground truth membrane potential responses and generated parameters <inline-formula><inline-graphic xlink:href="572452v2_inline19.gif" mime-subtype="gif" mimetype="image"/></inline-formula> at regularly sampled time points. This is followed by inverse derivative operation ∇<sup>−1</sup> which uses numerical method similar to Euler’s method to approximate <inline-formula><inline-graphic xlink:href="572452v2_inline20.gif" mime-subtype="gif" mimetype="image"/></inline-formula> at sampled time points given the initial condition <inline-formula><inline-graphic xlink:href="572452v2_inline21.gif" mime-subtype="gif" mimetype="image"/></inline-formula>:
<disp-formula id="eqn6">
<graphic xlink:href="572452v2_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>h</italic> is the time interval between sampled derivatives. <inline-formula><inline-graphic xlink:href="572452v2_inline22.gif" mime-subtype="gif" mimetype="image"/></inline-formula> can be selected at any time point within the ground truth membrane potential state (e.g., EP-GAN: mid-activation, EP-GAN-E: pre-activation, mid-activation, post-activation) to reconstruct different membrane potential features. Notably, all computation steps consisting of inverse gradient process are expected to be differentiable. This is necessary to incorporate inverse gradient process as part of the generator network that requires full differentiability and thus trainable via back-propagation algorithm [<xref ref-type="bibr" rid="c53">53</xref>]. Computationally, we achieve this by manually implementing the inverse gradient process with discrete array operations that support auto-differentiation and vectorization (e.g. PyTorch Tensors) instead of simulating the membrane potential with ODE solvers [<xref ref-type="bibr" rid="c54">54</xref>]. The variable <italic>h</italic> can also be adjusted to increase the accuracy of the membrane potential responses reconstruction in exchange for the increased computational cost. The current profile is reconstructed by directly evaluating <inline-formula><inline-graphic xlink:href="572452v2_inline23.gif" mime-subtype="gif" mimetype="image"/></inline-formula> which uses generated parameter set <inline-formula><inline-graphic xlink:href="572452v2_inline24.gif" mime-subtype="gif" mimetype="image"/></inline-formula> over the range of membrane potential responses values. We show in <xref rid="tbl2" ref-type="table">Table 2</xref> that reconstruction losses are essential for the accuracy of predicted parameters.</p>
</sec>
<sec id="s3a5">
<title>Generating Training Data</title>
<p>For a successful training of a neural network model, the training data must be of sufficient number of samples, denoised, and diverse. To ensure these conditions are met with a simulated dataset, we employ a two-step process for generating training data (<xref rid="fig8" ref-type="fig">Figure 8</xref>). In the first step, each parameter set is randomly generated by uniformly sampling of each parameter within a given range. The range is determined according to the biologically feasible ranges reported by the literature (See table <italic>predicted parameters</italic> in supplementary files for explicit range used for each parameter) [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c41">41</xref>]. In the second step, membrane potential responses and current traces are simulated for each sampled parameter set followed by imposing constraints on each of them (See supplementary for numerical simulation protocol). For the membrane potential response, the constraint is such that the variances between membrane potential responses trajectories are above a certain value. For the IV profile, an upper bound and lower bound on the y-axis (current axis) are set such that a certain proportion of data points must fall within two boundaries. Parameter sets that do not satisfy these constraints are then removed from the training set. The constraints serve two purposes: i) remove parameter sets that result in non-realistic membrane potential responses/current profiles from the training set and ii) serve as an initial data augmentation process for the model. The constraints can also be adjusted if deemed necessary for the improvement of the training process. Once constraints are applied, Gaussian noise is added to the membrane potential responses training data to mimic the measurement noises in experimental membrane potential responses recording data.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><title>Generating training data.</title>
<p>Each parameter is initially sampled from biologically plausible ranges using uniform sampling. A parameter set consists of 176 parameters spanning 15 known ion channels in <italic>C. elegans</italic>. Once parameter sets are generated, membrane potential responses and current profiles are evaluated for each set to ensure they satisfy the predefined constraints such as minimum offset between membrane potential responses traces and minimum and maximum bounds for steady-state current traces. Only parameter sets that meet the constraints are included in the training set.</p></caption>
<graphic xlink:href="572452v2_fig8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s3b">
<title>Experimental Protocol</title>
<sec id="s3b1">
<title><italic>C. elegans</italic> Culture and Strains</title>
<p>All animals used in this study were maintained at room temperature (22-23°C) on nematode growth medium (NGM) plates seeded with E. coli OP50 bacteria as a food source [<xref ref-type="bibr" rid="c55">55</xref>]. Strains used in this study were: CX7893 kyIs405 (AWB), CX3695 kyIs140 (AWC), ZG611 iaIs19 (URX), EG1285 lin-15B(n765);oxIs12 (RIS), UL2650 leEx2650 (DVC) and CX4857 kyIs179 (HSN).</p>
</sec>
<sec id="s3b2">
<title>Electrophysiology</title>
<p>Electrophysiological recordings were performed on young adult hermaphrodites (∼3-days old) at room temperature as previously described [<xref ref-type="bibr" rid="c8">8</xref>]. The gluing and dissection were performed under an Olympus SZX16 stereomicroscope equipped with a 1X Plan Apochromat objective and widefield 10X eyepieces. Briefly, an adult animal was immobilized on a Sylgard-coated (Sylgard 184, Dow Corning) glass coverslip in a small drop of DPBS (D8537; Sigma) by applying a cyanoacrylate adhesive (Vetbond tissue adhesive; 3M) along one side of the body. A puncture in the cuticle away from the incision site was made to relieve hydrostatic pressure. A small longitudinal incision was then made using a diamond dissecting blade (Type M-DL 72029 L; EMS) along the glue line adjacent to the neuron of interest. The cuticle flap was folded back and glued to the coverslip with GLUture Topical Adhesive (Abbott Laboratories), exposing the neuron to be recorded. The coverslip with the dissected preparation was then placed into a custom-made open recording chamber (∼1.5 ml volume) and treated with 1 mg/ml collagenase (type IV; Sigma) for ∼10 s by hand pipetting. The recording chamber was subsequently perfused with the standard extracellular solution using a custom-made gravity-feed perfusion system for <italic>∼</italic>10 ml.</p>
<p>All electrophysiological recordings were performed with the bath at room temperature under an upright microscope (Axio Examiner; Carl Zeiss, Inc) equipped with a 40X water immersion lens and 16X eyepieces. Neurons of interest were identified by fluorescent markers and their anatomical positions. Preparations were then switched to the differential interference contrast (DIC) setting for patch-clamp. Electrodes with resistance (RE) of 15-25 MΩ were made from borosilicate glass pipettes (BF100-58-10; Sutter Instruments) using a laser pipette puller (P-2000; Sutter Instruments) and fire-polished with a microforge (MF-830; Narishige). We used a motorized micromanipulator (PatchStar Micromanipulator; Scientifica) to control the electrodes back filled with standard intracellular solution. The standard pipette solution was (all concentrations in mM): [K-gluconate 115; KCl 15; KOH 10; MgCl2 5; CaCl2 0.1; Na2ATP 5; NaGTP 0.5; Na-cGMP 0.5; cAMP 0.5; BAPTA 1; Hepes 10; Sucrose 50], with pH adjusted with KOH to 7.2, osmolarity 320–330 mOsm. The standard extracellular solution was: [NaCl 140; NaOH 5; KCL 5; CaCl2 2; MgCl2 5; Sucrose 15; Hepes 15; Dextrose 25], with pH adjusted with NaOH to 7.3, osmolarity 330–340 mOsm. Liquid junction potentials were calculated and corrected before recording. Whole-cell current clamp and voltage-clamp experiments were conducted on an EPC-10 amplifier (EPC-10 USB; Heka) using PatchMaster software (Heka). Two-component capacitive compensation was optimized at rest, and series resistance was compensated to 50%. Analog data were filtered at 2 kHz and digitized at 10 kHz. Current-injection and voltage-clamp steps were applied through the recording electrode.</p>
</sec>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>In this work, we introduce a novel deep generative method and system called ElectroPhysiomeGAN (EP-GAN), for estimating Hodgkin-Huxley model (HH-model) parameters given the recordings of neurons with graded potential (non-spiking). The proposed system encompasses RNN Encoder layer to process the neural recordings information such as membrane potential responses and steady-state current profiles and Generator layer to generate large number of HH-model parameters in the order of 100s. The system can be trained entirely on simulation data informed by an arbitrary HH-model. When applied to neurons in <italic>C. elegans</italic>, EP-GAN generates parameters of HH-model which membrane potential responses is closer to test membrane potential responses than previous methods such as Differential Evolution and Genetic Algorithms. The advantage of EP-GAN is in the accuracy and inference speed achieved through less amount of compute than the previous methods for large parameter space and generic such that it doesn’t depend on the number of neurons for which inference is to be performed [<xref ref-type="bibr" rid="c16">16</xref>]. In addition, the method also preserves relative performance when provided with input data with partial information such as missing membrane potential responses (up to 25%) or steady-state current traces (up to 75%).</p>
<p>While EP-GAN is a step forward toward ElectroPhysiome model of <italic>C. elegans</italic>, its inability to support neurons with spiking membrane potential responses remains a limitation. The reason stems from the fact that neurons with spiking membrane potential responses are rare during the generation of training data of HH 15 ionic channels model without Na channels, making their translation strategies to parameter space difficult to learn. A similar limitation is present with bi-stable membrane potential responses, e.g., AWB and AWC, although at lesser extent. While the limitations for these profiles can be partially remedied through additional loss to the generator (e.g., resting potential loss), their relative sparseness in the training data tends to cause lower quality of predicted parameters. Indeed, previous studies of <italic>C. elegans</italic> nervous system found that the majority of neurons exhibit graded membrane potential response instead of spiking [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c21">21</xref>]. Furthermore, the limitation could also lie within the current architecture of EP-GAN as it currently processes data directly without a component that discerns and processes spiking membrane potential responses. Improving the sampling strategy for training data alongside enhancement of network architecture could address these limitations in the future.</p>
<p>As discussed in the Methods section, it’s worth noting that EP-GAN does not necessarily recover the ground truth parameters that are associated with the input membrane potential responses and steady-state current profiles. This is mainly due to the fact that there may exist multiple parameter regimes for the HH-model which support the given inputs [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c44">44</xref>, <xref ref-type="bibr" rid="c45">45</xref>]. The parameters generated by a single forward pass of EP-GAN (i.e., a single flow of information from the input to the output) could thus be interpreted as a one-time sampling from such a regime and a small perturbation to inputs may result in a different set of parameters. Such sensitivity to perturbation could be adjusted by supplementing the training samples or inputs with additional recording data (e.g., multiple recording data per neuron).</p>
<p>Beyond the loss extensions that were showcased in Results section, EP-GAN allows additional modifications to accommodate different configurations of the problem. For instance, update to HH-model would only require retraining of the network without changes to its architecture. Indeed, neuronal genome of <italic>C. elegans</italic> indicates additional voltage-gated channels that could be further incorporated to [<xref ref-type="bibr" rid="c7">7</xref>] to improve its modeling accuracy of membrane potential dynamics [<xref ref-type="bibr" rid="c56">56</xref>]. Extending the inputs to include additional data, e.g., channel activation profiles, can also be done in a straightforward manner by concatenating them to the input vectors of the encoder network.</p>
<p>Despite its primary focus on <italic>C. elegans</italic> neurons, we believe EP-GAN and its future extension could be viable for modeling a variety of neurons in other organisms. Indeed, there are increasing advances in resolving connectomes of more complex organisms and techniques for large-scale neural activities [<xref ref-type="bibr" rid="c57">57</xref>, <xref ref-type="bibr" rid="c58">58</xref>, <xref ref-type="bibr" rid="c59">59</xref>, <xref ref-type="bibr" rid="c60">60</xref>]. As neurons in these organisms can be described by a generic HH-model or similar differential equation model, EP-GAN is expected to be applicable and make contributions toward developing the biologically detailed nervous system models of neurons in these organisms.</p>
</sec>

</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>This work was supported in part by National Science Foundation grant CRCNS IIS-2113003 (JK,ES), Washington Research Fund (ES), CRCNS IIS-2113120 (QL), Kavli NSI Pilot Grant (QL), CityU New Research Initiatives/Infrastructure Support from Central APRC 9610587 (QL), the General Research Fund (GRF) and Early Career Scheme (ECS) Award from Hong Kong Research Grants Council RGC (CityU 21103522, CityU 11104123, CityU 11100524) (QL), and Chan Zuckerberg Initiative (to Cori Bargmann). Authors also acknowledge the partial support by the Departments of Electrical Computer Engineering (JK,ES), Applied Mathematics (ES), the Center of Computational Neuroscience (ES), and the eScience Institute (ES,JK) at the University of Washington. In addition, we thank Cori Bargmann and Ian Hope for <italic>C. elegans</italic> strains. Some strains were provided by the CGC, which is funded by NIH Office of Research Infrastructure Programs (P40 OD010440). We thank Saba Heravi for discussions regarding parameter inference for electrophysiological recordings.</p>
</ack>
<sec id="suppd1e1502" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e1493">
<label>supplementary</label>
<media xlink:href="supplements/572452_file02.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>John G</given-names> <surname>White</surname></string-name>, <string-name><given-names>Eileen</given-names> <surname>Southgate</surname></string-name>, <string-name><given-names>J</given-names> <surname>Nichol Thomson</surname></string-name>, <string-name><given-names>Sydney</given-names> <surname>Brenner</surname></string-name>, <etal>et al.</etal></person-group> <article-title>The structure of the nervous system of the nematode caenorhabditis elegans</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>, <volume>314</volume>(<issue>1165</issue>):<fpage>1</fpage>–<lpage>340</lpage>, <year>1986</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Lav R</given-names> <surname>Varshney</surname></string-name>, <string-name><given-names>Beth L</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Eric</given-names> <surname>Paniagua</surname></string-name>, <string-name><given-names>David H</given-names> <surname>Hall</surname></string-name>, and <string-name><given-names>Dmitri B</given-names> <surname>Chklovskii</surname></string-name></person-group>. <article-title>Structural properties of the caenorhabditis elegans neuronal network</article-title>. <source>PLoS computational biology</source>, <volume>7</volume>(<issue>2</issue>):<fpage>e1001066</fpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Steven J</given-names> <surname>Cook</surname></string-name>, <string-name><given-names>Travis A</given-names> <surname>Jarrell</surname></string-name>, <string-name><given-names>Christopher A</given-names> <surname>Brittin</surname></string-name>, <string-name><given-names>Yi</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Adam E</given-names> <surname>Bloniarz</surname></string-name>, <string-name><given-names>Maksim A</given-names> <surname>Yakovlev</surname></string-name>, <string-name><given-names>Ken CQ</given-names> <surname>Nguyen</surname></string-name>, <string-name><given-names>Leo T-H</given-names> <surname>Tang</surname></string-name>, <string-name><given-names>Emily A</given-names> <surname>Bayer</surname></string-name>, <string-name><given-names>Janet S</given-names> <surname>Duerr</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Whole-animal connectomes of both caenorhabditis elegans sexes</article-title>. <source>Nature</source>, <volume>571</volume>(<issue>7763</issue>):<fpage>63</fpage>–<lpage>71</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alan L</given-names> <surname>Hodgkin</surname></string-name> and <string-name><given-names>Andrew F</given-names> <surname>Huxley</surname></string-name></person-group>. <article-title>A quantitative description of membrane current and its application to conduction and excitation in nerve</article-title>. <source>The Journal of physiology</source>, <volume>117</volume>(<issue>4</issue>):<fpage>500</fpage>, <year>1952</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Allan R</given-names> <surname>Willms</surname></string-name></person-group>. <article-title>Neurofit: software for fitting hodgkin–huxley models to voltage-clamp data</article-title>. <source>Journal of neuroscience methods</source>, <volume>121</volume>(<issue>2</issue>):<fpage>139</fpage>–<lpage>150</lpage>, <year>2002</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Allan R</given-names> <surname>Willms</surname></string-name>, <string-name><given-names>Deborah J</given-names> <surname>Baro</surname></string-name>, <string-name><given-names>Ronald M</given-names> <surname>Harris-Warrick</surname></string-name>, and <string-name><given-names>John</given-names> <surname>Guckenheimer</surname></string-name></person-group>. <article-title>An improved parameter estimation method for hodgkin-huxley models</article-title>. <source>Journal of computational neuroscience</source>, <volume>6</volume>:<fpage>145</fpage>–<lpage>168</lpage>, <year>1999</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Martina</given-names> <surname>Nicoletti</surname></string-name>, <string-name><given-names>Alessandro</given-names> <surname>Loppini</surname></string-name>, <string-name><given-names>Letizia</given-names> <surname>Chiodo</surname></string-name>, <string-name><given-names>Viola</given-names> <surname>Folli</surname></string-name>, <string-name><given-names>Giancarlo</given-names> <surname>Ruocco</surname></string-name>, and <string-name><given-names>Simonetta</given-names> <surname>Filippi</surname></string-name></person-group>. <article-title>Biophysical modeling of c. elegans neurons: Single ion currents and whole-cell dynamics of awcon and rmd</article-title>. <source>PloS one</source>, <volume>14</volume>(<issue>7</issue>):<fpage>e0218738</fpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Qiang</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Philip B</given-names> <surname>Kidd</surname></string-name>, <string-name><given-names>May</given-names> <surname>Dobosiewicz</surname></string-name>, and <string-name><given-names>Cornelia I</given-names> <surname>Bargmann</surname></string-name></person-group>. <article-title>C. elegans awa olfactory neurons fire calcium-mediated all-or-none action potentials</article-title>. <source>Cell</source>, <volume>175</volume>(<issue>1</issue>):<fpage>57</fpage>–<lpage>70</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jingyuan</given-names> <surname>Jiang</surname></string-name>, <string-name><given-names>Yifan</given-names> <surname>Su</surname></string-name>, <string-name><given-names>Ruilin</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Haiwen</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Louis</given-names> <surname>Tao</surname></string-name>, and <string-name><given-names>Qiang</given-names> <surname>Liu</surname></string-name></person-group>. <article-title>C. elegans enteric motor neurons fire synchronized action potentials underlying the defecation motor program</article-title>. <source>Nature communications</source>, <volume>13</volume>(<issue>1</issue>):<fpage>2783</fpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Eve</given-names> <surname>Marder</surname></string-name> and <string-name><given-names>Jean-Marc</given-names> <surname>Goaillard</surname></string-name></person-group>. <article-title>Variability, compensation and homeostasis in neuron and network function</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>7</volume>(<issue>7</issue>):<fpage>563</fpage>–<lpage>574</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Eve</given-names> <surname>Marder</surname></string-name> and <string-name><given-names>Adam L</given-names> <surname>Taylor</surname></string-name></person-group>. <article-title>Multiple models to capture the variability in biological neurons and networks</article-title>. <source>Nature neuroscience</source>, <volume>14</volume>(<issue>2</issue>):<fpage>133</fpage>–<lpage>138</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Astrid A</given-names> <surname>Prinz</surname></string-name>, <string-name><given-names>Cyrus P</given-names> <surname>Billimoria</surname></string-name>, and <string-name><given-names>Eve</given-names> <surname>Marder</surname></string-name></person-group>. <article-title>Alternative to hand-tuning conductance-based models: construction and analysis of databases of model neurons</article-title>. <source>Journal of neurophysiology</source>, <year>2003</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Astrid A</given-names> <surname>Prinz</surname></string-name>, <string-name><given-names>Dirk</given-names> <surname>Bucher</surname></string-name>, and <string-name><given-names>Eve</given-names> <surname>Marder</surname></string-name></person-group>. <article-title>Similar network activity from disparate circuit parameters</article-title>. <source>Nature neuroscience</source>, <volume>7</volume>(<issue>12</issue>):<fpage>1345</fpage>–<lpage>1352</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Laure</given-names> <surname>Buhry</surname></string-name>, <string-name><given-names>Michele</given-names> <surname>Pace</surname></string-name>, and <string-name><given-names>Sylvain</given-names> <surname>Saïghi</surname></string-name></person-group>. <article-title>Global parameter estimation of an hodgkin–huxley formalism using membrane voltage recordings: Application to neuro-mimetic analog integrated circuits</article-title>. <source>Neurocomputing</source>, <volume>81</volume>:<fpage>75</fpage>–<lpage>85</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Juan</given-names> <surname>Luis</surname></string-name> <string-name><given-names>Jiménez</given-names> <surname>Laredo</surname></string-name>, <string-name><given-names>Loïs</given-names> <surname>Naudin</surname></string-name>, <string-name><given-names>Nathalie</given-names> <surname>Corson</surname></string-name>, and <string-name><given-names>Carlos M</given-names> <surname>Fernandes</surname></string-name></person-group>. <article-title>A methodology for determining ion channels from membrane potential neuronal recordings</article-title>. In <conf-name>Applications of Evolutionary Computation: 25th European Conference, EvoApplications 2022, Held as Part of EvoStar 2022, Madrid, Spain, April 20–22, 2022, Proceedings</conf-name>, pages <fpage>15</fpage>–<lpage>29</lpage>. <publisher-name>Springer</publisher-name>, <year>2022</year>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Loïs</given-names> <surname>Naudin</surname></string-name>, <string-name><given-names>Juan</given-names> <surname>Luis</surname></string-name> <string-name><given-names>Jiménez</given-names> <surname>Laredo</surname></string-name>, <string-name><given-names>Qiang</given-names> <surname>Liu</surname></string-name>, and <string-name><given-names>Nathalie</given-names> <surname>Corson</surname></string-name></person-group>. <article-title>Systematic generation of biophysically detailed models with generalization capability for non-spiking neurons</article-title>. <source>PloS one</source>, <volume>17</volume>(<issue>5</issue>):<fpage>e0268380</fpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Y</given-names> <surname>Curtis Wang</surname></string-name>, <string-name><given-names>Johann</given-names> <surname>Rudi</surname></string-name>, <string-name><given-names>James</given-names> <surname>Velasco</surname></string-name>, <string-name><given-names>Nirvik</given-names> <surname>Sinha</surname></string-name>, <string-name><given-names>Gideon</given-names> <surname>Idumah</surname></string-name>, <string-name><given-names>Randall K</given-names> <surname>Powers</surname></string-name>, <string-name><given-names>Charles J</given-names> <surname>Heckman</surname></string-name>, and <string-name><given-names>Matthieu K</given-names> <surname>Chardon</surname></string-name></person-group>. <article-title>Multimodal parameter spaces of a complex multi-channel neuron model</article-title>. <source>Front. Syst. Neurosci.</source> <year>2022</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jemy A Mandujano</given-names> <surname>Valle</surname></string-name> and <string-name><given-names>Alexandre L</given-names> <surname>Madureira</surname></string-name></person-group>. <article-title>Parameter identification problem in the hodgkin-huxley model</article-title>. <source>Neural Computation</source>, <volume>34</volume>(<issue>4</issue>):<fpage>939</fpage>–<lpage>970</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Pedro J</given-names> <surname>Gonçalves</surname></string-name>, <string-name><given-names>Jan-Matthis</given-names> <surname>Lueckmann</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Deistler</surname></string-name>, <string-name><given-names>Marcel</given-names> <surname>Nonnenmacher</surname></string-name>, <string-name><given-names>Kaan</given-names> <surname>Öcal</surname></string-name>, <string-name><given-names>Giacomo</given-names> <surname>Bassetto</surname></string-name>, <string-name><given-names>Chaitanya</given-names> <surname>Chintaluri</surname></string-name>, <string-name><given-names>William F</given-names> <surname>Podlaski</surname></string-name>, <string-name><given-names>Sara A</given-names> <surname>Haddad</surname></string-name>, <string-name><given-names>Tim P</given-names> <surname>Vogels</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Training deep neural density estimators to identify mechanistic models of neural dynamics</article-title>. <source>Elife</source>, <volume>9</volume>:<elocation-id>e56261</elocation-id>, <year>2020</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Lautaro</given-names> <surname>Estienne</surname></string-name></person-group>. <article-title>Towards an hybrid hodgkin-huxley action potential generation model</article-title>. In <conf-name>2021 XIX Workshop on Information Processing and Control (RPIC)</conf-name>, pages <fpage>1</fpage>–<lpage>6</lpage>. <publisher-name>IEEE</publisher-name>, <year>2021</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Miriam B</given-names> <surname>Goodman</surname></string-name>, <string-name><given-names>David H</given-names> <surname>Hall</surname></string-name>, <string-name><given-names>Leon</given-names> <surname>Avery</surname></string-name>, and <string-name><given-names>Shawn R</given-names> <surname>Lockery</surname></string-name></person-group>. <article-title>Active currents regulate sensitivity and dynamic range in c. elegans neurons</article-title>. <source>Neuron</source>, <volume>20</volume>(<issue>4</issue>):<fpage>763</fpage>–<lpage>772</lpage>, <year>1998</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Loïs</given-names> <surname>Naudin</surname></string-name>, <string-name><given-names>Nathalie</given-names> <surname>Corson</surname></string-name>, <string-name><given-names>MA</given-names> <surname>Aziz-Alaoui</surname></string-name>, <string-name><given-names>Juan Luis Jimenez</given-names> <surname>Laredo</surname></string-name>, and <string-name><given-names>Thibaut</given-names> <surname>Démare</surname></string-name></person-group>. <article-title>On the modeling of the three types of non-spiking neurons of the caenorhabditis elegans</article-title>. <source>International Journal of Neural Systems</source>, <volume>31</volume>(<issue>02</issue>):<fpage>2050063</fpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Kristin</given-names> <surname>Koch</surname></string-name>, <string-name><given-names>Judith</given-names> <surname>McLean</surname></string-name>, <string-name><given-names>Ronen</given-names> <surname>Segev</surname></string-name>, <string-name><given-names>Michael A</given-names> <surname>Freed</surname></string-name>, <string-name><given-names>Michael J</given-names> <surname>Berry</surname></string-name>, <string-name><given-names>Vijay</given-names> <surname>Balasubramanian</surname></string-name>, and <string-name><given-names>Peter</given-names> <surname>Sterling</surname></string-name></person-group>. <article-title>How much the eye tells the brain</article-title>. <source>Current biology</source>, <volume>16</volume>(<issue>14</issue>):<fpage>1428</fpage>–<lpage>1434</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Alan</given-names> <surname>Roberts</surname></string-name> and <string-name><given-names>Brian MH</given-names> <surname>Bush</surname></string-name></person-group>. <source>Neurones without impulses: their significance for vertebrate and invertebrate nervous systems</source>, volume <volume>6</volume>. <publisher-name>Cambridge University Press</publisher-name>, <year>1981</year>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>RE</given-names> <surname>Davis</surname></string-name> and <string-name><given-names>AO</given-names> <surname>Stretton</surname></string-name></person-group>. <article-title>Passive membrane properties of motorneurons and their role in long-distance signaling in the nematode ascaris</article-title>. <source>Journal of Neuroscience</source>, <volume>9</volume>(<issue>2</issue>):<fpage>403</fpage>–<lpage>414</lpage>, <year>1989</year>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ralph E</given-names> <surname>Davis</surname></string-name> and <string-name><given-names>AO</given-names> <surname>Stretton</surname></string-name></person-group>. <article-title>Signaling properties of ascaris motorneurons: graded active responses, graded synaptic transmission, and tonic transmitter release</article-title>. <source>Journal of Neuroscience</source>, <volume>9</volume>(<issue>2</issue>):<fpage>415</fpage>–<lpage>425</lpage>, <year>1989</year>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M</given-names> <surname>Burrows</surname></string-name>, <string-name><given-names>GJ</given-names> <surname>Laurent</surname></string-name>, and <string-name><given-names>LH</given-names> <surname>Field</surname></string-name></person-group>. <article-title>Proprioceptive inputs to nonspiking local interneurons contribute to local reflexes of a locust hindleg</article-title>. <source>Journal of Neuroscience</source>, <volume>8</volume>(<issue>8</issue>):<fpage>3085</fpage>–<lpage>3093</lpage>, <year>1988</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Gilles</given-names> <surname>Laurent</surname></string-name> and <string-name><given-names>Malcolm</given-names> <surname>Burrows</surname></string-name></person-group>. <article-title>Distribution of intersegmental inputs to nonspiking local interneurons and motor neurons in the locust</article-title>. <source>Journal of Neuroscience</source>, <volume>9</volume>(<issue>9</issue>):<fpage>3019</fpage>–<lpage>3029</lpage>, <year>1989</year>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Gilles</given-names> <surname>Laurent</surname></string-name> and <string-name><given-names>M</given-names> <surname>Burrows</surname></string-name></person-group>. <article-title>Intersegmental interneurons can control the gain of reflexes in adjacent segments of the locust by their action on nonspiking local interneurons</article-title>. <source>Journal of Neuroscience</source>, <volume>9</volume>(<issue>9</issue>):<fpage>3030</fpage>–<lpage>3039</lpage>, <year>1989</year>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Loïs</given-names> <surname>Naudin</surname></string-name></person-group>. <article-title>Biological emergent properties in non-spiking neural networks</article-title>. <source>AIMS Mathematics</source>, <volume>7</volume>(<issue>10</issue>):<fpage>19415</fpage>–<lpage>19439</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Kalyanmoy</given-names> <surname>Deb</surname></string-name>, <string-name><given-names>Samir</given-names> <surname>Agrawal</surname></string-name>, <string-name><given-names>Amrit</given-names> <surname>Pratap</surname></string-name>, and <string-name><given-names>Tanaka</given-names> <surname>Meyarivan</surname></string-name></person-group>. <article-title>A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: Nsga-ii</article-title>. In <conf-name>Parallel Problem Solving from Nature PPSN VI: 6th International Conference Paris, France, September 18–20, 2000 Proceedings</conf-name> <volume>6</volume>, pages <fpage>849</fpage>–<lpage>858</lpage>. <publisher-name>Springer</publisher-name>, <year>2000</year>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Etay</given-names> <surname>Hay</surname></string-name>, <string-name><given-names>Sean</given-names> <surname>Hill</surname></string-name>, <string-name><given-names>Felix</given-names> <surname>Schürmann</surname></string-name>, <string-name><given-names>Henry</given-names> <surname>Markram</surname></string-name>, and <string-name><given-names>Idan</given-names> <surname>Segev</surname></string-name></person-group>. <article-title>Models of neocortical layer 5b pyramidal cells capturing a wide range of dendritic and perisomatic active properties</article-title>. <source>PLoS computational biology</source>, <volume>7</volume>(<issue>7</issue>):<fpage>e1002107</fpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Werner</given-names> <surname>Van Geit</surname></string-name>, <string-name><given-names>Erik</given-names> <surname>De Schutter</surname></string-name>, and <string-name><given-names>Pablo</given-names> <surname>Achard</surname></string-name></person-group>. <article-title>Automated neuron model optimization techniques: a review</article-title>. <source>Biological cybernetics</source>, <volume>99</volume>:<fpage>241</fpage>–<lpage>251</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Tea</given-names> <surname>Robič</surname></string-name> and <string-name><given-names>Bogdan</given-names> <surname>Filipič</surname></string-name></person-group>. <article-title>Differential evolution for multiobjective optimization</article-title>. In <conf-name>Evolutionary Multi-Criterion Optimization: Third International Conference, EMO 2005, Guanajuato, Mexico, March 9-11, 2005. Proceedings</conf-name> <volume>3</volume>, pages <fpage>520</fpage>–<lpage>533</lpage>. <publisher-name>Springer</publisher-name>, <year>2005</year>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Saku</given-names> <surname>Kukkonen</surname></string-name> and <string-name><given-names>Jouni</given-names> <surname>Lampinen</surname></string-name></person-group>. <article-title>Gde3: The third evolution step of generalized differential evolution</article-title>. In <conf-name>2005 IEEE congress on evolutionary computation</conf-name>, volume <volume>1</volume>, pages <fpage>443</fpage>–<lpage>450</lpage>. <publisher-name>IEEE</publisher-name>, <year>2005</year>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Rakesh</given-names> <surname>Angira</surname></string-name> and <string-name><given-names>BV</given-names> <surname>Babu</surname></string-name></person-group>. <article-title>Non-dominated sorting differential evolution (nsde): An extension of differential evolution for multi-objective optimization</article-title>. In <conf-name>Iicai</conf-name>, pages <fpage>1428</fpage>–<lpage>1443</lpage>, <year>2005</year>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Timothy H</given-names> <surname>Rumbell</surname></string-name>, <string-name><given-names>Danel</given-names> <surname>Draguljić</surname></string-name>, <string-name><given-names>Aniruddha</given-names> <surname>Yadav</surname></string-name>, <string-name><given-names>Patrick R</given-names> <surname>Hof</surname></string-name>, <string-name><given-names>Jennifer I</given-names> <surname>Luebke</surname></string-name>, and <string-name><given-names>Christina M</given-names> <surname>Weaver</surname></string-name></person-group>. <article-title>Automated evolutionary optimization of ion channel conductances and kinetics in models of young and aged rhesus monkey pyramidal neurons</article-title>. <source>Journal of computational neuroscience</source>, <volume>41</volume>:<fpage>65</fpage>–<lpage>90</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author">J <string-name><given-names>Christopher</given-names> <surname>Octeau</surname></string-name>, <string-name><given-names>Mohitkumar R</given-names> <surname>Gangwani</surname></string-name>, <string-name><given-names>Sushmita L</given-names> <surname>Allam</surname></string-name>, <string-name><given-names>Duy</given-names> <surname>Tran</surname></string-name>, <string-name><given-names>Shuhan</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>Tuan M</given-names> <surname>Hoang-Trong</surname></string-name>, <string-name><given-names>Peyman</given-names> <surname>Golshani</surname></string-name>, <string-name><given-names>Timothy H</given-names> <surname>Rumbell</surname></string-name>, <string-name><given-names>James R</given-names> <surname>Kozloski</surname></string-name>, and <string-name><given-names>Baljit S</given-names> <surname>Khakh</surname></string-name></person-group>. <article-title>Transient, consequential increases in extracellular potassium ions accompany channelrhodopsin2 excitation</article-title>. <source>Cell reports</source>, <volume>27</volume>(<issue>8</issue>):<fpage>2249</fpage>–<lpage>2261</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Laure</given-names> <surname>Buhry</surname></string-name>, <string-name><given-names>Audrey</given-names> <surname>Giremus</surname></string-name>, <string-name><given-names>Eric</given-names> <surname>Grivel</surname></string-name>, <string-name><given-names>Sylvain</given-names> <surname>Saïghi</surname></string-name>, and <string-name><given-names>Sylvie</given-names> <surname>Renaud</surname></string-name></person-group>. <article-title>New variants of the differential evolution algorithm: application for neuroscientists</article-title>. In <conf-name>2009 17th European Signal Processing Conference</conf-name>, pages <fpage>2352</fpage>–<lpage>2356</lpage>. <publisher-name>IEEE</publisher-name>, <year>2009</year>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Julian</given-names> <surname>Blank</surname></string-name> and <string-name><given-names>Kalyanmoy</given-names> <surname>Deb</surname></string-name></person-group>. <article-title>Pymoo: Multi-objective optimization in python</article-title>. <source>IEEE Access</source>, <volume>8</volume>:<fpage>89497</fpage>–<lpage>89509</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Eugene M</given-names> <surname>Izhikevich</surname></string-name></person-group>. <source>Dynamical systems in neuroscience</source>. <publisher-name>MIT press</publisher-name>, <year>2007</year>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Loïs</given-names> <surname>Naudin</surname></string-name>, <string-name><given-names>Juan</given-names> <surname>Luis</surname></string-name> <string-name><given-names>Jiménez</given-names> <surname>Laredo</surname></string-name>, and <string-name><given-names>Nathalie</given-names> <surname>Corson</surname></string-name></person-group>. <article-title>A simple model of nonspiking neurons</article-title>. <source>Neural Computation</source>, <volume>34</volume>(<issue>10</issue>):<fpage>2075</fpage>–<lpage>2101</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jinming</given-names> <surname>Zou</surname></string-name>, <string-name><given-names>Yi</given-names> <surname>Han</surname></string-name>, and <string-name><given-names>Sung-Sau</given-names> <surname>So</surname></string-name></person-group>. <article-title>Overview of artificial neural networks</article-title>. <source>Artificial neural networks: methods and applications</source>, pages <fpage>14</fpage>–<lpage>22</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ashley E</given-names> <surname>Raba</surname></string-name>, <string-name><given-names>Jonathan M</given-names> <surname>Cordeiro</surname></string-name>, <string-name><given-names>Charles</given-names> <surname>Antzelevitch</surname></string-name>, and <string-name><given-names>Jacques</given-names> <surname>Beaumont</surname></string-name></person-group>. <article-title>Extending the conditions of application of an inversion of the hodgkin–huxley gating model</article-title>. <source>Bulletin of mathematical biology</source>, <volume>75</volume>:<fpage>752</fpage>–<lpage>773</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Loïs</given-names> <surname>Naudin</surname></string-name></person-group>. <article-title>Different parameter solutions of a conductance-based model that behave identically are not necessarily degenerate</article-title>. <source>Journal of Computational Neuroscience</source>, pages <fpage>1</fpage>–<lpage>6</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ian</given-names> <surname>Goodfellow</surname></string-name>, <string-name><given-names>Jean</given-names> <surname>Pouget-Abadie</surname></string-name>, <string-name><given-names>Mehdi</given-names> <surname>Mirza</surname></string-name>, <string-name><given-names>Bing</given-names> <surname>Xu</surname></string-name>, <string-name><given-names>David</given-names> <surname>Warde-Farley</surname></string-name>, <string-name><given-names>Sherjil</given-names> <surname>Ozair</surname></string-name>, <string-name><given-names>Aaron</given-names> <surname>Courville</surname></string-name>, and <string-name><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name></person-group>. <article-title>Generative adversarial networks</article-title>. <source>Communications of the ACM</source>, <volume>63</volume>(<issue>11</issue>):<fpage>139</fpage>–<lpage>144</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Martin</given-names> <surname>Arjovsky</surname></string-name>, <string-name><given-names>Soumith</given-names> <surname>Chintala</surname></string-name>, and <string-name><given-names>Léon</given-names> <surname>Bottou</surname></string-name></person-group>. <article-title>Wasserstein generative adversarial networks</article-title>. In <conf-name>International conference on machine learning</conf-name>, pages <fpage>214</fpage>–<lpage>223</lpage>. <publisher-name>PMLR</publisher-name>, <year>2017</year>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Kyunghyun</given-names> <surname>Cho</surname></string-name>, <string-name><given-names>Bart</given-names> <surname>Van Merriënboer</surname></string-name>, <string-name><given-names>Caglar</given-names> <surname>Gulcehre</surname></string-name>, <string-name><given-names>Dzmitry</given-names> <surname>Bahdanau</surname></string-name>, <string-name><given-names>Fethi</given-names> <surname>Bougares</surname></string-name>, <string-name><given-names>Holger</given-names> <surname>Schwenk</surname></string-name>, and <string-name><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name></person-group>. <article-title>Learning phrase representations using rnn encoder-decoder for statistical machine translation</article-title>. <source>arXiv</source> <pub-id pub-id-type="arxiv">1406.1078</pub-id>, <year>2014</year>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ishaan</given-names> <surname>Gulrajani</surname></string-name>, <string-name><given-names>Faruk</given-names> <surname>Ahmed</surname></string-name>, <string-name><given-names>Martin</given-names> <surname>Arjovsky</surname></string-name>, <string-name><given-names>Vincent</given-names> <surname>Dumoulin</surname></string-name>, and <string-name><given-names>Aaron C</given-names> <surname>Courville</surname></string-name></person-group>. <article-title>Improved training of wasserstein gans</article-title>. <source>Advances in neural information processing systems</source>, <volume>30</volume>, <year>2017</year>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Aladin</given-names> <surname>Virmaux</surname></string-name> and <string-name><given-names>Kevin</given-names> <surname>Scaman</surname></string-name></person-group>. <article-title>Lipschitz regularity of deep neural networks: analysis and efficient estimation</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>31</volume>, <year>2018</year>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>George Em</given-names> <surname>Karniadakis</surname></string-name>, <string-name><given-names>Ioannis G</given-names> <surname>Kevrekidis</surname></string-name>, <string-name><given-names>Lu</given-names> <surname>Lu</surname></string-name>, <string-name><given-names>Paris</given-names> <surname>Perdikaris</surname></string-name>, <string-name><given-names>Sifan</given-names> <surname>Wang</surname></string-name>, and <string-name><given-names>Liu</given-names> <surname>Yang</surname></string-name></person-group>. <article-title>Physics-informed machine learning</article-title>. <source>Nature Reviews Physics</source>, <volume>3</volume>(<issue>6</issue>):<fpage>422</fpage>–<lpage>440</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Maziar</given-names> <surname>Raissi</surname></string-name>, <string-name><given-names>Paris</given-names> <surname>Perdikaris</surname></string-name>, and <string-name><given-names>George E</given-names> <surname>Karniadakis</surname></string-name></person-group>. <article-title>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</article-title>. <source>Journal of Computational physics</source>, <volume>378</volume>:<fpage>686</fpage>–<lpage>707</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>David E</given-names> <surname>Rumelhart</surname></string-name>, <string-name><given-names>Geoffrey E</given-names> <surname>Hinton</surname></string-name>, and <string-name><given-names>Ronald J</given-names> <surname>Williams</surname></string-name></person-group>. <article-title>Learning representations by back-propagating errors</article-title>. <source>nature</source>, <volume>323</volume>(<issue>6088</issue>):<fpage>533</fpage>–<lpage>536</lpage>, <year>1986</year>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Adam</given-names> <surname>Paszke</surname></string-name>, <string-name><given-names>Sam</given-names> <surname>Gross</surname></string-name>, <string-name><given-names>Francisco</given-names> <surname>Massa</surname></string-name>, <string-name><given-names>Adam</given-names> <surname>Lerer</surname></string-name>, <string-name><given-names>James</given-names> <surname>Bradbury</surname></string-name>, <string-name><given-names>Gregory</given-names> <surname>Chanan</surname></string-name>, <string-name><given-names>Trevor</given-names> <surname>Killeen</surname></string-name>, <string-name><given-names>Zeming</given-names> <surname>Lin</surname></string-name>, <string-name><given-names>Natalia</given-names> <surname>Gimelshein</surname></string-name>, <string-name><given-names>Luca</given-names> <surname>Antiga</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>. <source>Advances in neural information processing systems</source>, <volume>32</volume>, <year>2019</year>.</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Sydney</given-names> <surname>Brenner</surname></string-name></person-group>. <article-title>The genetics of caenorhabditis elegans</article-title>. <source>Genetics</source>, <volume>77</volume>(<issue>1</issue>):<fpage>71</fpage>–<lpage>94</lpage>, <year>1974</year>.</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Oliver</given-names> <surname>Hobert</surname></string-name></person-group>. <chapter-title>The neuronal genome of caenorhabditis elegans</chapter-title>. <source>WormBook: The online review of C. elegans biology [Internet]</source>, <year>2018</year>.</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Paul</given-names> <surname>Brooks</surname></string-name></person-group>, <article-title>Andrew Champion, and Marta Costa. Mapping of the zebrafish brain takes shape</article-title>. <source>Nature Methods</source>, pages <fpage>1</fpage>–<lpage>2</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Michael</given-names> <surname>Winding</surname></string-name>, <string-name><given-names>Benjamin D</given-names> <surname>Pedigo</surname></string-name>, <string-name><given-names>Christopher L</given-names> <surname>Barnes</surname></string-name>, <string-name><given-names>Heather G</given-names> <surname>Patsolic</surname></string-name>, <string-name><given-names>Youngser</given-names> <surname>Park</surname></string-name>, <string-name><given-names>Tom</given-names> <surname>Kazimiers</surname></string-name>, <string-name><given-names>Akira</given-names> <surname>Fushiki</surname></string-name>, <string-name><given-names>Ingrid V</given-names> <surname>Andrade</surname></string-name>, <string-name><given-names>Avinash</given-names> <surname>Khandelwal</surname></string-name>, <string-name><given-names>Javier</given-names> <surname>Valdes-Aleman</surname></string-name>, <etal>et al.</etal></person-group> <article-title>The connectome of an insect brain</article-title>. <source>Science</source>, <volume>379</volume>(<issue>6636</issue>):<fpage>eadd9330</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c59"><label>[59]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Seung Wook</given-names> <surname>Oh</surname></string-name>, <string-name><given-names>Julie A</given-names> <surname>Harris</surname></string-name>, <string-name><given-names>Lydia</given-names> <surname>Ng</surname></string-name>, <string-name><given-names>Brent</given-names> <surname>Winslow</surname></string-name>, <string-name><given-names>Nicholas</given-names> <surname>Cain</surname></string-name>, <string-name><given-names>Stefan</given-names> <surname>Mihalas</surname></string-name>, <string-name><given-names>Quanxin</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Chris</given-names> <surname>Lau</surname></string-name>, <string-name><given-names>Leonard</given-names> <surname>Kuan</surname></string-name>, <string-name><given-names>Alex M</given-names> <surname>Henry</surname></string-name>, <etal>et al.</etal></person-group> <article-title>A mesoscale connectome of the mouse brain</article-title>. <source>Nature</source>, <volume>508</volume>(<issue>7495</issue>):<fpage>207</fpage>–<lpage>214</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c60"><label>[60]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nicholas James</given-names> <surname>Sofroniew</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Flickinger</surname></string-name>, <string-name><given-names>Jonathan</given-names> <surname>King</surname></string-name>, and <string-name><given-names>Karel</given-names> <surname>Svoboda</surname></string-name></person-group>. <article-title>A large field of view two-photon mesoscope with subcellular resolution for in vivo imaging</article-title>. <source>elife</source>, <volume>5</volume>:<elocation-id>e14472</elocation-id>, <year>2016</year>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95607.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bhalla</surname>
<given-names>Upinder S</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>National Centre for Biological Sciences</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study is a <bold>valuable</bold> contribution to the field of neuronal modeling by way of providing a method for rapidly obtaining neuronal physiology parameters from electrophysiological recordings. While the approach seems promising, in its current form it is <bold>incomplete</bold> since the generated models often diverge from the data and the comparison with existing methods has concerns.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95607.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The work by Kim et al. shows that a parameter generator for biophysical HH-like models can be trained through a GAN-based approach, to reproduce experimentally measured voltage responses and IV curves.</p>
<p>
A particularly interesting aspects of this generator is that, once it has been learned, it can be applied to new recordings to generate appropriate parameter sets at a low computational cost, a feature missing from more commonplace evolutionary approaches.</p>
<p>I appreciate the changes the authors have made to the manuscript. The authors have clarified their inverse gradient method. They also provide a better validation and a rich set of ablations. However, I still have major concerns that should be addressed.</p>
<p>Major concerns:</p>
<p>(1) The bad equilibria of the model still remain a concern, as well as other features like the transient overshoots that do not match with the data. I think they could achieve more accuracy here by assigning more weight to such specific features, through adding these as separate objectives for the generator explicitly. The traces contain a five-second current steps, and one second before and one second after the training step. This means that in the RMSE, the current step amplitude will dominate as a feature, as this is simply the state for which the data trace contains most time-points. Note that this is further exacerbated by using the IV curve as an auxiliary objective. I believe a better exploration of specific response features, incorporated as independently weighted loss terms for the generator, could improve the fit. E.g. an auxiliary term could be the equilibrium before and after the current step, another term could penalise response traces that do not converge back to their initial equilibrium, etc.</p>
<p>(2) The explanation of what the authors mean with 'inverse gradient operation' is clear now. However, this term is mathematically imprecise, as the inverse gradient does not exist because the gradient operator is not injective. The method is simply forward integration under the assumption that the derivate of the voltage is known at the grid time-points, and should be described as such.</p>
<p>(3) I appreciate that the authors' method provides parameters of models at a minimal computational cost compared to running an evolutionary optimization for every new recording. I also believe that with some tweaking of the objective, the method could improve in accuracy. However, I share reviewer 2's concerns that the evolutionary baseline methods are not sufficiently explored, as these methods have been used to successfully fit considerably more complex response patterns. One way out of the dilemma is to show that the EP-GAN estimated parameters provide an initial guess that considerably narrows the search space for the evolutionary algorithm. In this context, the authors should also discuss the recent gradient based methods such as Deistler et al. (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2024.08.21.608979">https://doi.org/10.1101/2024.08.21.608979</ext-link>) or Jones et al (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2407.04025">https://doi.org/10.48550/arXiv.2407.04025</ext-link>).</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95607.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Generating biophysically detailed computational models that capture the characteristic physiological properties of biological neurons for diverse cell types is an important and difficult problem in computational neuroscience. One major challenge lies in determining the large number of parameters of such models, which are notoriously difficult to fit to experimental data. Thereby, the computational and energy costs can be significant. The study 'ElectroPhysiomeGAN: Generation of Biophysical Neuron Model Parameters from Recorded Electrophysiological Responses' by Kim et al. describes a computationally efficient approach for predicting model parameters of Hodgkin-Huxley neuron models using Generative Adversarial Networks (GANs) trained on simulation data. The method is applied to generate models for 9 non-spiking neurons in C. elegans based on electrophysiological recordings. While the generated models capture the responses of these neurons to some degree, they generally show significant deviations from the empirically observed responses in important features. Although EP-GAN shows clear benefits under limited compute, the results do not yet demonstrate the quality needed to match other state-of-the-art methods. Future work examining extended training, larger datasets, or hybrid approaches would help clarify whether EP-GAN can generate models of high quality. If so, this would indeed be a major step forward; if not, the computationally more expensive methods will remain essential.</p>
<p>Strengths:</p>
<p>The authors work on an important and difficult problem. A noteworthy strength of their approach is that once trained, the GANs can generate models from new empirical data with very little computational effort. The generated models reproduce the response to current injections reasonably well.</p>
<p>Weaknesses:</p>
<p>Major 1: Models do not faithfully capture empirical responses. While the models generated with EP-GAN reproduce the average voltage during current injections reasonably well, the dynamics of the response are generally not well captured. For example, for the neuron labeled RIM (Figure 2), the most depolarized voltage traces show an initial 'overshoot' of depolarization, i.e. they depolarize strongly within the first few hundred milliseconds but then fall back to a less depolarized membrane potential. In contrast, the empirical recording shows no such overshoot. Similarly, for the neuron labeled AFD, all empirically recorded traces slowly ramp up over time. In contrast, the simulated traces are mostly flat. Furthermore, all empirical traces return to the pre-stimulus membrane potential, but many of the simulated voltage traces remain significantly depolarized, far outside of the ranges of empirically observed membrane potentials. The authors trained an additional GAN (EP-GAN Extended) to improve the fit to the resting membrane potential. Interestingly, for one neuron (AWB), this improved the response during stimulation, which now reproduced the slowly raising membrane potentials observed empirically, however, the neuron still does not reliably return to its resting membrane potential. For the other two neurons, the authors report a decrease in accuracy in comparison to EP-GAN. While such deviations may appear small in the Root mean Square Error (RMSE), they likely indicate a large mismatch between the model and the electrophysiological properties of the biological neuron. The authors added a second metric during the revision - percentages of predicted membrane potential trajectories within empirical range. I appreciate this additional analysis. As the empirical ranges across neurons are far larger than the magnitude of dynamical properties of the response ('slow ramps', etc.), this metric doesn't seem to be well suited to quantify to which degree these dynamical properties are captured by the models.</p>
<p>Major 2: Comparison with other approaches is potentially misleading. Throughout the manuscript, the authors claim that their approach outperforms the other approaches tested. But compare the responses of the models in the present manuscript (neurons RIM, AFD, AIY) to the ones provided for the same neurons in Naudin et al. 2022 (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal">https://doi.org/10.1371/journal</ext-link>. pone.0268380). Naudin et al. present models that seem to match empirical data far more accurately than any model presented in the current study. Naudin et al. achieved this using DEMO, an algorithm that in the present manuscript is consistently shown to be among the worst of all algorithms tested. I therefore strongly disagree with the authors claim that a &quot;Comparison of EP-GAN with existing estimation methods shows EP-GAN advantage in the accuracy of estimated parameters&quot;. This may be true in the context of the benchmark performed in the study (i.e., a condition of very limited compute resources - 18 generations with a population size of 600, compare that to 2000 generations recommended in Naudin et al.), but while EP-GAN wins under these specific conditions (and yes, here the authors convincingly show that their EP-GAN produces by far the best results!), other approaches seem to win with respect to the quality of the models they can ultimately generate.</p>
<p>Major 3: As long as the quality of the models generated by the EP-GAN cannot be significantly improved, I am doubtful that it indeed can contribute to the 'ElectroPhysiome', as it seems likely that dynamics that are currently poorly captured, like slow ramps, or the ability of the neuron to return to its resting membrane potential, will critically affect network computations. If the authors want to motivate their study based on this very ambitious goal, they should illustrate that single neuron model generation with their approach is robust enough to warrant well-constrained network dynamics. Based on the currently presented results, I find the framing of the manuscript far too bold.</p>
<p>Major 4: The conclusion of the ablation study 'In addition the architecture of EP-GAN permits inference of parameters even when partial membrane potential and steady-state currents profile are given as inputs' does not seem to be justified given the voltage traces shown in Figure 3. For example, for RIM, the resting membrane potential stays around 0 mV, but all empirical traces are around -40mV. For AFD, all simulated traces have a negative slope during the depolarizing stimuli, but a positive slope in all empirically observed traces. For AIY, the shape of hyperpolarized traces is off. While it may be that by their metric neurons in the 25% category are classified as 'preserving baseline accuracy', this doesn't seem justified given the voltage traces presented in the manuscript. It appears the metric is not strict enough.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95607.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kim</surname>
<given-names>Jimin</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5597-5142</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Peng</surname>
<given-names>Minxian</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0419-3543</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Chen</surname>
<given-names>Shuqi</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Liu</surname>
<given-names>Qiang</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9232-1420</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Shlizerman</surname>
<given-names>Eli</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3136-4531</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<p>We thank the reviewers for valuable feedback and comments. Based on the feedback we revised the manuscript and believe that we addressed most of the reviewers' raised points. Below we include a summary of key revisions and point-by-point responses to reviewers comments.</p>
<p>Abstract/Introduction</p>
<p>We further emphasized EP-GAN strength in parameter inference of detailed neuron parameters vs specialized models with reduced parameters.</p>
<p>Results</p>
<p>We further elaborated on the method of training EP-GAN on synthetic neurons and validating on both synthetic and experimental neurons.</p>
<p>We added a new section <italic>Statistical Analysis and Loss Extension</italic> which includes:</p>
<p>- Statistical evaluation of baseline EP-GAN and other methods on neurons with multi recording membrane potential responses/steady-state currents data: AWB, URX, HSN</p>
<p>- Evaluation of EP-GAN with added resting potential loss + longer simulations to ensure stability of membrane potential (EP-GAN-E)</p>
<p>Methods</p>
<p>We added a detailed explanation on &quot;inverse gradient process&quot;</p>
<p>We added detailed current/voltage-clamp protocols for both synthetic and experimental validation and prediction scenarios (table 6)</p>
<p>Supplementary</p>
<p>We added error distribution and representative samples for synthetic neuron validations (Fig S1)</p>
<p>We added membrane potential response statistical analysis plots for existing methods for AWB, URX, HSN (Fig S6)</p>
<p>We added steady-state currents statistical analysis plots on EP-GAN + existing methods for AWB, URX, HSN (Fig S7)</p>
<p>We added mean membrane potential errors for AWB, URX, HSN normalized by empirical standard deviations for all methods (Table S4)</p>
<p>Please see our point-by-point responses to specific feedback and comment below.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 1:</bold></p>
<p>First, at the methodological level, the authors should explain the inverse gradient operation in more detail, as the reconstructed voltage will not only depend on the evaluation of the right-hand side of the HH-equations, as they write but also on the initial state of the system. Why did the authors not simply simulate the responses?</p>
</disp-quote>
<p>We thank the reviewer for the feedback regarding the need for further explanation. We have revised the Methods section to provide a more detailed description of the inverse gradient process. The process uses a discrete integration method, similar to Euler’s formula, which takes systems’ initial conditions into account. For the EP-GAN baseline, the initial states were picked soon after the start of the stimulus to reconstruct the voltage during the stimulation period. For EP-GAN with extended loss (EP-GAN-E), introduced in this revision in sub-section <italic>Statistical Analysis and Loss Extension</italic>, initial states before/after stimulations were also taken into account to incorporate resting voltage states into target loss.</p>
<p>Since EP-GAN is a neural network and we want the inverse gradient process to be part of the training process (i.e., making EP-GAN a “model informed network”), the process is expected to be implemented as a differentiable function of generated parameter p. This enables the derivatives from reconstructed voltages to be traced back to all network components via back-propagation algorithm.</p>
<p>Computationally, this requires the implementation of the process as a combination of discrete array operations with “auto-differentiation”, which allows automatic computation of derivatives for each operation. While explicit simulation of the responses using ODE solvers provides more accurate solutions, the algorithms used by these solvers typically do not support such specialized arrays nor are they compatible with neural network training. We thus utilized PyTorch tensors [54], which support both auto-differentiation and vectorization to implement the process.</p>
<disp-quote content-type="editor-comment">
<p>The authors did not allow the models time to equilibrate before starting their reconstruction simulations, as testified by the large transients observed before stimulation onset in their plots. To get a sense of whether the models reproduce the equilibria of the measured responses to a reasonable degree, the authors should allow sufficient time for the models to equilibrate before starting their stimulation protocol.</p>
</disp-quote>
<p>In the added <italic>Statistical Analysis and Loss Extension</italic> under the Results section, we added results for EP-GAN-E where we simulate the voltage responses with 5 seconds of added stabilization period in the beginning of simulations. The added period mitigates voltage fluctuations observed during the initial simulation phase and we observe that simulated voltage responses indeed reach stable equilibrium for both prior stimulations and for the zero stimulus current-clamp protocol (Figure 5 bottom, Column 3).</p>
<disp-quote content-type="editor-comment">
<p>In fact, why did the authors not explicitly include the equilibrium voltage as a target loss in their set of loss functions? This would be an important quantity that determines the opening level of all the ion channels and therefore would influence the associated parameter values.</p>
</disp-quote>
<p>EP-GAN baseline does include equilibrium voltage as a target loss since all current-clamp protocols used in the study (both synthetic and experimental) include a membrane potential trace where the stimulus amplitude is zero throughout the entire recording duration (see added Table 6 for current clamp protocols), thus enforcing EP-GAN to optimize resting membrane potential alongside with other non-zero stimulus current-clamp scenarios.</p>
<p>To further study EP-GAN’s accuracy in resting potential, we evaluated EP-GAN with supplemental resting potential target loss and evaluated its performance in the sub-section <italic>Statistical Analysis and Loss Extension</italic>. The added loss, combined with 5 seconds of additional stabilization period, improved accuracy in predicting resting potentials by mitigating voltage fluctuations during the early simulation phase and made significant improvements to predicting AWB membrane potential responses where EP-GAN baseline resulted in overshoot of the resting potential.</p>
<disp-quote content-type="editor-comment">
<p>The authors should provide a more detailed evaluation of the models. They should explicitly provide the IV curves (this should be easy enough, as they compute them anyway), and clearly describe the time-point at which they compute them, as their current figures suggest there might be strong transient changes in them.</p>
</disp-quote>
<p>We included predicted IV-curve vs ground truth plots in addition to the voltages in the supplementary materials (Figure S2, S5) in the original submitted version of the manuscript. In this revision, we added additional IV-curve plots with statistical analysis for the neurons with multi-recording data (AWB, URX, HSN) in the supplementary materials (Figure S7).</p>
<p>For the evaluation of predicted membrane potential responses, we added further details in <italic>Validation Scenarios (Synthetic)</italic> under Results section such that it clearly explains on the current-clamp protocols used for both synthetic and experimental neurons and which time interval the RMSE evaluations were performed.</p>
<p>In the sub-section <italic>Statistical Analysis and Loss Extension</italic>, we introduced a new statistical metric in addition to RMSE, applied for neurons AWB, URX, HSN which evaluates the percentage of predicted voltages that fall within the empirical range (i.e., mean +- 2 std) and voltage error normalized by empirical standard deviations (Table S4).</p>
<disp-quote content-type="editor-comment">
<p>The authors should assess the stability of the models. Some of the models exhibit responses that look as if they might be unstable if simulated for sufficiently long periods of time. Therefore, the authors should investigate whether all obtained parameter sets lead to stable models.</p>
</disp-quote>
<p>In the sub-section <italic>Statistical Analysis and Loss Extension</italic>, we included individual voltage traces generated by both EP-GAN baseline and EP-GAN-E (extended) with longer simulation (+5 seconds) to ensure stability. EP-GAN-E is able to produce equilibrium voltages that are indeed stable and within empirical bounds throughout the simulations for the zero-stimulus current-clamp scenario (column 3) for the 3 tested neurons (AWB, URX, HSN).</p>
<disp-quote content-type="editor-comment">
<p>Minor:</p>
<p>The authors should provide a description of the model, and it's trainable parameters. At the moment, it is unclear which parameter of the ion channels are actually trained by the methodology.</p>
</disp-quote>
<p>The detailed description of the model and its ion channels can be found in [7]. Supplementary materials also include an excel table <italic>predicted parameters</italic> which lists all EP-GAN fitted parameters for 9 neurons (+3 new parameter sets for AWB, URX, HSN using EP-GAN-E) included in the study, the labels for trainability, and their respective lower/upper bounds used during training data generation. In the revised manuscript, we further elaborated on the above information in the second paragraph of the Results section.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 2:</bold></p>
<p>Major 1: While the models generated with EP-GAN reproduce the average voltage during current injections reasonably well, the dynamics of the response are not well captured. For example, for the neuron labeled RIM (Figure 2), the most depolarized voltage traces show an initial 'overshoot' of depolarization, i.e. they depolarize strongly within the first few hundred milliseconds but then fall back to a less depolarized membrane potential. In contrast, the empirical recording shows no such overshoot. Similarly, for the neuron labeled AFD, all empirically recorded traces slowly ramp up over time. In contrast, the simulated traces are mostly flat. Furthermore, all empirical traces return to the pre-stimulus membrane potential, but many of the simulated voltage traces remain significantly depolarized, far outside of the ranges of empirically observed membrane potentials. While these deviations may appear small in the Root mean Square Error (RMSE), the only metric used in the study to assess the quality of the models, they likely indicate a large mismatch between the model and the electrophysiological properties of the biological neuron.</p>
</disp-quote>
<p>EP-GAN main contribution is targeted towards parameter inference of <italic>detailed neuron model parameters,</italic> in a compute efficient manner. This is a difficult problem to address even with current state-of-the-art fitting algorithms. While EP-GAN is not perfect in capturing the dynamics of the responses and RMSE does not fully reflect the quality of predicted electrophysiological properties, it’s a generic error metric for time series that is easily interpretable and applicable for all methods. Using such a metric, our studies show that EP-GAN overall prediction quality exceeds those of existing methods when given identical optimization goals in a compute normalized setup.</p>
<p>In our revised manuscript, we included a new section <italic>Statistical Analysis and Loss Extension</italic> under Results section where we performed additional statistical evaluations (e.g., % of predicted responses within empirical range) of EP-GAN’s predictions for neurons with multi recording data. The results show that predicted voltage responses from EP-GAN baseline (introduced in original manuscript) are in general, within the empirical range with ~80% of its responses falling within +- 2 empirical standard deviations, which were higher than existing methods: DEMO (57.9%), GDE3 (37.9%), NSDE (38%), NSGA2 (60.2%).</p>
<disp-quote content-type="editor-comment">
<p>Major 2: Other metrics than the RMSE should be incorporated to validate simulated responses against electrophysiological data. A common approach is to extract multiple biologically meaningful features from the voltage traces before, during and after the stimulus, and compare the simulated responses to the experimentally observed distribution of these features. Typically, a model is only accepted if all features fall within the empirically observed ranges (see e.g. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002107">https://doi.org/10.1371/journal.pcbi.1002107</ext-link>). However, based on the deviations in resting membrane potential and the return to the resting membrane potential alone, most if not all the models shown in this study would not be accepted.</p>
</disp-quote>
<p>In our original manuscript, due to all of our neurons’ recordings having a single set of recording data, RMSE was chosen to be the most generic and interpretable error metric. We conducted additional electrophysiological recordings for 3 neurons in prediction scenarios (AWB, URX, HSN) and performed statistical analysis of generated models in the sub-section <italic>Statistical Analysis and Loss Extension</italic>. Specifically, we evaluated the percentage of predicted voltage responses that fall within the empirical range (empirical mean +- 2 std, p ~ 0.05) that encompass the responses before, during and after stimulus (Figure 5, Table 5) and mean membrane potential error normalized by empirical standard deviations (Table S4).</p>
<p>The results show that EP-GAN baseline achieves average of ~80% of its predicted responses falling within the empirical range, which is higher than the other methods: DEMO (57.9%), GDE3 (37.9%), NSDE (38%), NSGA2 (60.2%). Supplementing EP-GAN with additional resting potential loss (EPGAN-E) increased the percentage to ~85% with noticeable improvements in reproducing dynamical features for AWB (Figure 5). Evaluations of membrane potential errors normalized by empirical standard deviations also showed similar results where EP-GAN baseline and EP-GAN-E have average error of 1.0 std and 0.7 std respectively, outperforming DEMO (1.7 std), GDE3 (2.0 std), NSDE (3.0 std) and NSGA (1.5 std) (Table S4).</p>
<disp-quote content-type="editor-comment">
<p>Major 3: Abstract and introduction imply that the 'ElectroPhysiome' refers to models that incorporate both the connectome and individual neuron physiology. However, the work presented in this study does not make use of any connectomics data. To make the claim that ElectroPhysiomeGAN can jointly capture both 'network interaction and cellular dynamics', the generated models would need to be evaluated for network inputs, for example by exposing them to naturalistic stimuli of synaptic inputs. It seems likely that dynamics that are currently poorly captured, like slow ramps, or the ability of the neuron to return to its resting membrane potential, will critically affect network computations.</p>
</disp-quote>
<p>In the paper, EP-GAN is introduced as a parameter estimation method that can aid the development of ElectroPhysiome, which is a network model - these are two different method types and we do not claim EP-GAN is a model that can capture network dynamics. To avoid possible confusion, we made further clarifications in the abstract/introduction that EP-GAN is a machine learning approach for neuron HH-parameter estimation.</p>
<disp-quote content-type="editor-comment">
<p>I find it hard to believe that the methods EP-GAN is compared to could not perform any better. For example, multi-objective optimization algorithms are often successful in generating models that match empirical observations very well, but features used as target of the optimization need to be carefully selected for the optimization to succeed. Likely, each method requires extensive trial and error to achieve the best performance for a given problem. It is therefore hard to do a fair comparison. Given these complications, I would like to encourage the authors to rethink the framing of the story as a benchmark of EP-GAN vs. other methods. Also, the number of parameters does not seem that relevant to me, as long as the resulting models faithfully reproduce empirical data. What I find most interesting is that EP-GAN learns general relationships between electrophysiological responses and biophysical parameters, and likely could also be used to inspect the distribution of parameters that are consistent with a given empirical observation.</p>
</disp-quote>
<p>We thank the reviewer for providing this perspective. While it is indeed difficult to have a completely fair comparison between existing optimization methods vs EP-GAN due to the fundamental differences in their algorithms, we believe that the current comparisons with other methods are justified as they provide baseline performance metrics to test EP-GAN for its intended use cases.</p>
<p>The main strength of EP-GAN, as previously mentioned, is in its ability to efficiently navigate large detailed HH-models with many parameters so that it can aid in the development of nervous system models such as ElectroPhysiome, potentially fitting hundreds of neurons in a time efficient manner.</p>
<p>While EP-GAN’s ability to learn the general relationship between electrophysiological responses and parameter distribution are indeed interesting and warrant a more careful examination, this is not the main focus of the paper since in this work we focus on introducing EP-GAN as a methodology for parameter inference.</p>
<p>In this context, we believe the comparisons with other methods conducted in a compute normalized manner (i.e., each method is given the same # of simulations) and identical optimization targets provides an adequate framework for evaluating the aforementioned EP-GAN aim. Indeed, while EPGAN excels with larger HH-models, it performs slightly worse than DE for smaller models such as the one used by [16] despite it being more compute efficient (Table S2).</p>
<p>To emphasize the EP-GAN aim, we revised the main manuscript description to focus on its intended use in parameter inference of detailed neuron parameters vs specialized models with reduced parameters.</p>
<disp-quote content-type="editor-comment">
<p>I could not find important aspects of the methods. What are the 176 parameters that were targeted as trainable parameters? What are the parameter bounds? What are the remaining parameters that have been excluded? What are the Hodgkin-Huxley models used? Which channels do they represent? What are the stimulus protocols?</p>
</disp-quote>
<p>The detailed description and development of the HH-model that we use and its ion channel list can be found in [7]. Supplementary materials also include an excel table <italic>predicted parameters</italic> which lists all EP-GAN fitted parameters for 9 neurons (+3 new parameter sets for AWB, URX, HSN using EPGAN-E), the labels for trainability, and parameter bounds used for parameters during the generation of training data.</p>
<p>We also added a new Table which details the current/voltage clamp protocols used for 9 neurons including the ones used for evaluating EP-GAN-E, which was supplemented with longer simulation time to ensure voltage stability (please see Table 6).</p>
<disp-quote content-type="editor-comment">
<p>I could not assess the validation of the EP-GAN by modeling 200 synthetic neurons based on the data presented in the manuscript since the only reported metric is the RMSE (5.84mV and 5.81mV for neurons sampled from training data and testing data respectively) averaged over all 200 synthetic neurons. Please report the distribution of RMSEs, include other biologically more relevant metrics, and show representative examples. The responses should be carefully investigated for the types of mismatches that occur, and their biological relevance should be discussed. For example, is the EP-GAN biased to generate responses with certain characteristics, like the 'overshoot' discussed in Major 1? Is it generally poor at fitting the resting potential?</p>
</disp-quote>
<p>We thank the reviewer for the feedback regarding the need for additional supporting data for synthetic neuron validations. In the revised <italic>supplementary materials</italic> Figure S1, we included the distribution of RMSE errors for both groups of synthetic neuron validations (validation/test set) and representative samples for both EP-GAN baseline and EP-GAN-E. Notably, the inaccuracies observed during the experimental neuron predictions (e.g., resting potential, voltage overshoot) do not necessarily generalize to synthetic neurons, indicating that such mismatches could stem from the differences between synthetic neurons used for training and experimental neurons for predictions. While synthetic neurons are generated according to empirically determined parameter bounds, some experimental neuron types are rarer than the others and may also involve other channels that have not been recorded or modeled in [7], which can affect the quality of predicted parameters (see 2nd and 4th paragraphs of Discussions section for more detail). Also, properties such as recording error/noise that are often present in experimental neurons are not fully accounted for in synthetic neurons.</p>
<p>To further study how these mismatches can be mitigated, in the revision we added an extended version of EP-GAN where target loss was supplemented with additional resting potential and 5 seconds of stabilization period during simulations (EP-GAN-E described in <italic>Statistical Analysis and Loss Extension</italic>). With such extensions, EP-GAN-E was able to improve its accuracies on both resting potentials and dynamical features with the most notable improvements on AWB where predicted voltage responses closely match slowly rising voltage response during stimulation. EPGAN-E is an example of further extensions to loss function that account for additional experimental features.</p>
<disp-quote content-type="editor-comment">
<p>Furthermore, the conclusion of the ablation study ('EP-GAN preserves reasonable accuracy up to a 25% reduction in membrane potential responses') does not seem to be justified given the voltage traces shown in Figure 3. For example, for RIM, the resting membrane potential stays around 0 mV, but all empirical traces are around -40mV. For AFD, all simulated traces have a negative slope during the depolarizing stimuli, but a positive slope in all empirically observed traces. For AIY, the shape of hyperpolarized traces is off.</p>
</disp-quote>
<p>Since EP-GAN baseline optimizes voltage responses during the stimulation period, RMSE was also evaluated with respect to this period. From these errors, we evaluated whether the predicted voltage error for each ablation scenario fell within the 2 standard deviations from the mean error obtained from synthetic neuron test data (i.e. the baseline performance). We found that for input ablation for voltage responses, the error was within such range up to 25% reduction whereas for steady-state current input ablation, all 25%, 50% and 75% reductions resulted in errors within the range.</p>
<p>We extended the “Ablation Studies” sub-section so that the above reasoning is better communicated to the readers.</p>
<disp-quote content-type="editor-comment">
<p>Additionally, I found a number of minor issues:</p>
<p>Minor 1: Table 1 lists the number of HH simulations as '32k (11k · 3)'. Should it be 33k, since 11.000 times 3 is 33.000? Please specify the exact number of samples.</p>
<p>Minor 2: x- and y-ticks are missing in Fig 2, Fig 3, Fig S1, Fig S2, Fig S3 and Fig S4.</p>
<p>Minor 3: All files in the supplementary zip file should be listed and described.</p>
<p>Minor 4: Code for training the GAN, generation of training datasets and for reproducing the figures should be provided.</p>
<p>Minor 5: In the reference (Figure 3A, Table 1 Row 2): should this refer to Table 2?</p>
<p>Minor 6: 'the ablation is done on stimulus space where a 50% reduction corresponds to removing half of the membrane potential responses traces each associated with a stimulus.' - which half is removed?</p>
</disp-quote>
<p>We thank the reviewer for pointing out these errors in the original manuscript. The revised manuscript includes corrections for these items. We will publish the python code reproducing the results in the public repository in the near future.</p>
</body>
</sub-article>
</article>