<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">107301</article-id>
<article-id pub-id-type="doi">10.7554/eLife.107301</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107301.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Center-surround inhibition by expectation: a neuro-computational account</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Huang</surname>
<given-names>Ling</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
<xref ref-type="author-notes" rid="n2">†</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Shen</surname>
<given-names>Shiqi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Sun</surname>
<given-names>Yueling</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ou</surname>
<given-names>Shipei</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Ruyuan</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>de Lange</surname>
<given-names>Floris P</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0449-934X</contrib-id>
<name>
<surname>Zhang</surname>
<given-names>Xilin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>xlzhang@m.scnu.edu.cn</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kq0pv72</institution-id><institution>Key Laboratory of Brain, Cognition and Education Sciences, Ministry of Education, South China Normal University</institution></institution-wrap>, <city>Guangzhou</city>, <country country="CN">China</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kq0pv72</institution-id><institution>School of Psychology, Center for Studies of Psychological Application, and Guangdong Provincial Key Laboratory of Mental Health and Cognitive Science, South China Normal University</institution></institution-wrap>, <city>Guangzhou</city>, <country country="CN">China</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0220qvk04</institution-id><institution>Institute of Psychology and Behavioral Science, Antai College of Economics and Management, and Shanghai Mental Health Center, School of Medicine, Shanghai Jiao Tong University</institution></institution-wrap>, <city>Shanghai</city>, <country country="CN">China</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01bn89z48</institution-id><institution>Key Laboratory of Brain-Machine Intelligence for Information Behavior-Ministry of Education, Shanghai International Studies University</institution></institution-wrap>, <city>Shanghai</city>, <country country="CN">China</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Donders Institute for Brain, Cognition and Behaviour, Radboud University</institution></institution-wrap>, <city>Nijmegen</city>, <country country="NL">Netherlands</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Sui</surname>
<given-names>Jing</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>*</label><p>These authors contributed equally.</p></fn>
<fn id="n2" fn-type="present-address"><label>†</label><p>Present address: Department of Psychology, The Ohio State University, Columbus, OH 43201, USA.</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-07-28">
<day>28</day>
<month>07</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP107301</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-05-10">
<day>10</day>
<month>05</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-08-27">
<day>27</day>
<month>08</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.08.26.609781"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Huang et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Huang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-107301-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Expectation is beneficial for adaptive behavior through quickly deducing plausible interpretations of information. The profile and underlying neural computations of this process, however, remain unclear. When participants expected a grating with a specific orientation, we found a center-surround inhibition profile in orientation space, which was independent from attentional modulations by task-relevance. Using computational modeling, we showed that this center-surround inhibition could be reproduced by either a sharpening of tuning curves of expected orientation or a shift of tuning curves of unexpected orientations. Intriguingly, these two computations were further supported by orientation-adjustment and orientation-discrimination experiments. Finally, the ablation studies in convolutional neural networks revealed that predictive coding feedback played a critical role in the center-surround inhibition in expectation. Altogether, our study reveals for the first time that expectation results in both enhancement and suppression, optimizing plausible interpretations during perception by enhancing expected and attenuating similar but irrelevant and potentially interfering representations.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>expectation</kwd>
<kwd>center-surround inhibition</kwd>
<kwd>tuning sharpening</kwd>
<kwd>tuning shift</kwd>
<kwd>deep predictive coding neural network</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Human behavior is surprisingly efficient and adaptive. Although everyday environment is brimming with noisy and ambiguous information, our cognitive system can quickly and adeptly deduce plausible interpretations of this information by combining it with prior expectations, ultimately facilitating to flexible behavioral arises<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref></sup>. However, the structured manner (the profile, in other words) regarding how expectation demarcates the anticipated target from various distractors, and underlying neural computations remain largely unclear. This issue is particularly important since such profile is thought to closely reflect neural circuitry<sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c4">4</xref></sup>, and therefore, offers us a unique opportunity to give insight into neural circuit level computations of expectation, thereby not only furthering our understanding of how it facilitates perception and behavior to adapt to changing environment, but also addressing a long-standing debate about its underlying neural mechanisms<sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c6">6</xref></sup>.</p>
<p>One of the central questions to this debate is about the processing of unexpected stimuli that are sufficiently novel or surprising. The sharpening models (also referred to as Bayesian theories) propose that expectations preferentially suppress neurons tuned toward the unexpected stimuli, resulting in a sharper and more selective population responses<sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref></sup>. This sharpening account of expectation is similar to the notion of neuronal resonance<sup><xref ref-type="bibr" rid="c9">9</xref></sup> and has been supported by neurophysiological<sup><xref ref-type="bibr" rid="c10">10</xref>–<xref ref-type="bibr" rid="c14">14</xref></sup>, electro-/magneto-encephalogram<sup><xref ref-type="bibr" rid="c15">15</xref>–<xref ref-type="bibr" rid="c19">19</xref></sup>, and functional magnetic resonance imaging (fMRI)<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c20">20</xref>–<xref ref-type="bibr" rid="c22">22</xref></sup> studies. Conversely, the cancelation models (also referred to as dampening theories) propose a dampening of neural responses reduces redundancy in the sensory system, through suppressing neurons tuned toward the expected stimulus. By canceling the expected information, the brain could highlight the processing and cognitive resources of unexpected information<sup><xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c23">23</xref></sup>. This theory has also drawn wide support from neurophysiological<sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c24">24</xref></sup> and brain imaging<sup><xref ref-type="bibr" rid="c25">25</xref>–<xref ref-type="bibr" rid="c28">28</xref></sup> studies. Intriguingly, although these two models explaining how expectations render perception either veridical or informative are seemingly conflicting, both could be incorporated in the framework of predictive coding models<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c29">29</xref>–<xref ref-type="bibr" rid="c33">33</xref></sup>, which posits that the brain contains distinct neurons/units representing the best guess about the outside world (prediction units) and the discrepancy between these guesses and incoming sensory evidence (prediction error units). Several studies have proposed that the sharpening and cancelation accounts may occur in prediction and error neurons, respectively<sup><xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c30">30</xref></sup>. Within this framework, anticipating about what is possible or probable in the forthcoming sensory environment can be cast as a process of hierarchical Bayesian inference, in which the prediction units are more strongly weighted towards the expected rather than unexpected stimuli, while at the same time the prediction error units are selectively biased to surprising inputs. Increased gain on these surprising inputs would lead to high fidelity representations of unexpected stimuli across prediction units. Although, so far there has been no direct evidence for the existence of these two neuron types, and it is unclear how these two mechanisms are reconciled from different neural populations, the predictive coding framework may provide the underlying computational basis for various potential profiles of expectation.</p>
<p>Here, given these two mechanisms making opposite predictions about how expectation changes the neural responses of unexpected stimuli, thereby displaying different profiles of expectation, we speculated that if expectation operates by the sharpening model with suppressing unexpected information, we should observe an inhibitory zone surrounding the focus of expectation, and its profile then should display as a center-surround inhibition (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>, left). If, however, expectation operates as suggested by the cancelation model with highlighting unexpected information, the inhibitory zone surrounding the focus of expectation should be eliminated, and the profile should instead display a monotonic gradient (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>, right). To adjudicate between these theoretical possibilities, we manipulated the distance between the expected and unexpected stimuli in feature space to measure the profile of expectation in two psychophysical experiments (orientation was task-relevant or task-irrelevant on the orientation and spatial frequency discrimination tasks, respectively, <xref rid="fig1" ref-type="fig">Fig. 1b</xref>), both of which supported the sharpening account by showing a classical center-surround inhibition profile in orientation space, with enhanced neural responses to the expected orientation and suppressed neural responses to orientations similar to the expected orientation relative to orientations more distinct from it (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). Second, using computational modelling, we showed that the behaviorally observed center-surround inhibition in expectation could be reproduced by either a sharpening of tuning curves of expected orientation (Tuning sharpening account) or a shift of tuning curves of unexpected orientations (Tuning shift account) (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>). Third, these neural computations, consisting of both the tuning sharpening and tuning shift accounts, were further confirmed by orientation-adjustment (<xref rid="fig6" ref-type="fig">Fig. 6</xref>) and orientation-discrimination (<xref rid="fig7" ref-type="fig">Fig. 7</xref>) experiments. Finally, we found that a deep predictive coding neural network (DPCNN) exhibited a similar center-surround inhibition by expectation profile, both when it was trained to perform an orientation or a spatial frequency task. Most importantly, when we ablated predictive feedback, these center-surround inhibitions were eliminated in the DPCNN (<xref rid="fig8" ref-type="fig">Fig. 8</xref>), strongly supporting the framework of predictive coding models in expectation<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c29">29</xref>–<xref ref-type="bibr" rid="c33">33</xref></sup>. Altogether, our study reveals for the first time that expectation generates an orientation-specific enhancement and suppression profile that optimizes plausible interpretations during visual perception by boosting expected and attenuating interfering sensory representations.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 1 |</label>
<caption><title>Stimuli and protocols of the profile experiment.</title>
<p><bold>a,</bold> Left: the auditory cue, comprising either a low- or high-frequency tone, predicted the orientation of the first grating with equal validity in the baseline experiment. B20°: Baseline 20°; B70°: Baseline 70°. Right: in the main experiment, the low- or high-frequency tone predicted 20° or 70° (expected) orientation of the first grating with 75% validity. In the remaining 25% of trials, this orientation was chosen randomly and equally from four non-predicted orientations (30°, 40°, 50°, and 60°). There were two types of expected conditions: Expect 20° (E20°) and Expect 70° (E70°), and for both conditions, there were 5 possible distances in orientation space between the expected and test gratings, ranging from Δ0° through Δ40° with a step size of 10°. <bold>b,</bold> In both baseline and main experiments, each trial began with an auditory cue, followed by an 1800 ms fixation interval. Then, two consecutive gratings were each presented for 150-ms and separated by a 300-ms blank interval. Participants were first asked to make a 2AFC judgment of either the orientation (clockwise or anticlockwise) or the spatial frequency (lower or higher) of the second grating relative to the first on orientation discrimination (OD, purple) and spatial frequency discrimination (SFD, blue) tasks, respectively. Then, participants were asked to make another 2AFC judgment on the tone of auditory cue, either low or high. CW: clockwise; CCW: counterclockwise; HF: higher frequency; LF: lower frequency; HT: high tone; LT: low tone. <bold>c,</bold> Left: expectation operates by the sharpening model with suppressing unexpected information, under this configuration, the profile of expectation could display as a center-surround inhibition, with an inhibitory zone surrounding the focus of expectation. Right: expectation operates by the cancelation model with highlighting unexpected information, under this configuration, the profile of expectation could display as a monotonic gradient, without the inhibitory zone.</p></caption>
<graphic xlink:href="609781v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 2 |</label>
<caption><title>Results of the profile experiment.</title>
<p>The discrimination thresholds of OD (top) and SFD (bottom) tasks during baseline (<bold>a</bold>) and main (<bold>b</bold>) experiments. B20°: Baseline 20°; B70°: Baseline 70°; E20°: Expect 20°; E70°: Expect 70°. <bold>c,</bold> The averaged discrimination sensitivity (<italic>DS</italic>) of each distance on OD (top) and SFD (bottom) tasks, and the best fitting Gaussian and Mexican-hat functions to these DSs across distances. G, Gaussian model; M, Mexican-hat model. <bold>d,</bold> <italic>R<sup>2</sup></italic>of the best fitting Gaussian and Mexican-hat functions for individual participants in OD (top) and SFD (bottom) tasks. For both tasks, most dots located in the upper-left zone, demonstrating that the Mexican-hat model was favored over the Gaussian model. Open symbols represent the data from each participant and filled colored dots represented the mean across participants. Error bars indicate 1 SEM calculated across participants.</p></caption>
<graphic xlink:href="609781v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 3 |</label>
<caption><title>Results of computational modeling.</title>
<p><bold>a,</bold> Illustration of the Tuning sharpening model (left) and the Tuning shift model (right). The Tuning sharpening model postulates that expectation sharpens the tuning of individual neurons (thick curves) towards the expected orientation, which results in a center-surround population response profile (black curve) centered at the expected orientation. The Tuning shift model postulates that expectation attracts the tuning of individual neurons (thick curves) from unexpected orientation towards the expected orientation, which also results in a center-surround population response profile. <bold>b,</bold> The fitted discrimination thresholds on OD (left) and SFD (right) tasks in the baseline (top) and main (bottom) experiments. <bold>c,</bold> The averaged DSs using Tuning sharpening model on OD (left) and SFD (right) tasks. <bold>d,</bold> R<sup>2</sup> of the best fitting Gaussian and Mexican-hat functions for individual participants based on the fitted DSs using Tuning sharpening model on OD (left) and SFD (right) tasks. Open symbols represent the data from each participant and filled colored dots represented the mean across participants. <bold>e-g,</bold> The results from Tuning shift model, see caption for (<bold>b-d</bold>) for a description of each type of graph. The amplitude <italic>A</italic> (vertical stripes) and width σ (diagonal stripes) differences between the baseline and main experiments using Tuning sharpening model in Δ0° (<bold>h</bold>) and Δ10°-Δ40° (<bold>i</bold>) conditions, on OD (left) and SFD (right) tasks. The location <italic>x0</italic> differences between the baseline and main experiments using Tuning shift model in Δ0° (<bold>j</bold>) and Δ10°-Δ40° (<bold>k</bold>) conditions, on OD (left) and SFD (right) tasks. Open symbols represent the data from each participant and error bars indicate 1 SEM calculated across participants. B20°: Baseline 20°; B70°: Baseline 70°; E20°: Expect 20°; E70°: Expect 70° (*p &lt; 0.05; **p &lt; 0.005; ***p &lt; 0.001).</p></caption>
<graphic xlink:href="609781v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Profile experiment</title>
<p>The profile experiment consisted of a baseline and main experiment, with the baseline experiment always preceding the main experiment. The two experiments were the same, except for the probability relationship between the auditory cue and the orientation (20°, 30°, 40°, 50°, 60°, and 70°) of the first grating. For the baseline experiment, the auditory cue, comprising either a low- (240 Hz) or high- (540 Hz) frequency tone, predicted the orientation of the first grating with equal validity (16.67%, <xref rid="fig1" ref-type="fig">Fig. 1a</xref>, left). In the main experiment, this low- or high-frequency tone auditory cue predicted the orientation (20° or 70°) of the first grating with 75% validity. In the remaining 25% of trials, this orientation was chosen randomly and equally from four non-predicted orientations (30°, 40°, 50°, and 60°, <xref rid="fig1" ref-type="fig">Fig. 1a</xref>, right). Thus, for each participant, there were two types of expected conditions: Expect 20° and Expect 70°, and for both conditions, there were five possible distances in orientation space between the expected and test gratings, ranging from Δ0° through Δ40° with a step size of 10°. Note that, the matches between the tone (low- or high-frequency) of auditory cue and the expected orientation (20° or 70°) of the first grating were flipped across participants, and the order was also counterbalanced across participants. Moreover, for each participant, although the tone of auditory cue could not predict 20° or 70° orientation in the baseline experiment, the trials in the baseline experiment with the same tone that was matched with 20° or 70° orientation in the main experiment, were defined as Baseline 20° (i.e., the baseline of Expect 20°) and Baseline 70° (i.e., the baseline of Expect 70°) conditions, respectively.</p>
<p>Both the baseline and main experiments consisted of two tasks: the orientation discrimination (OD) task and spatial frequency discrimination (SFD) task; with the two tasks occurring on different days; the order of the two tasks was counterbalanced across participants. Differently, the baseline experiment consisted of 4 blocks (two for OD task and the other two for SFD task), and each block had 2 QUEST staircases<sup><xref ref-type="bibr" rid="c34">34</xref></sup> for each of six orientations (20°, 30°, 40°, 50°, 60°, and 70°). The main experiment consisted of 2 blocks (one for OD task and the other one for SFD task), and each block had 24 QUEST staircases for the expected orientations (20° and 70°) and 2 QUEST staircases for each of unexpected orientations (30°, 40°, 50°, and 60°). Each QUEST staircase comprised 40 trials and each trial began with an auditory cue, followed by a fixation interval. Then, two gratings were presented sequentially and participants were asked to make a two-alternative forced-choice (2AFC) judgment of either the orientation (clockwise or anticlockwise, where orientation was task-relevant) or the spatial frequency (lower or higher, where orientation was task-irrelevant) of the second grating relative to the first, on the OD and SFD tasks, respectively (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). The second grating differed trial by trial from the first in either orientation (Δθ<italic>°</italic>) or spatial frequency (Δλ cycles/°) on the OD and SFD tasks, respectively. The QUEST staircase was used to control the varied Δθ<italic>°</italic> or Δλ cycles/° adaptively for estimating participants’ discrimination thresholds (75% correct). At the end of each trial, participants needed to report the tone (either low or high) of the auditory cue. For either OD or SFD tasks, there was no significant difference in accuracy of this reporting across different conditions in either baseline or main experiments (<xref ref-type="supplementary-material" rid="supp1">Supplementary Fig. 1</xref>).</p>
<p>In the baseline experiment, participants’ mean discrimination thresholds in Baseline 20<italic>°</italic> and Baseline 70<italic>°</italic> conditions were submitted to a one-way repeated-measures ANOVA with orientation (20°, 30°, 40°, 50°, 60°, and 70°) as a within-participants factor. Results showed that the main effect of orientation was not significant in either OD (Baseline 20°: F(5,115) = 0.955, p = 0.431, η<sub>p</sub><sup>2</sup> = 0.040; Baseline 70°: F(5,115) = 1.314, p = 0.274, η<sub>p</sub><sup>2</sup> = 0.054) or SFD (Baseline 20°: F(5,115) = 1.163, p = 0.331, η<sub>p</sub><sup>2</sup> = 0.048; Baseline 70°: F(5,115) = 1.593, p = 0.184, η<sub>p</sub><sup>2</sup> = 0.065) tasks (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>), indicating that there was no significant difference in participant performance among six orientations. In other words, the tone of auditory cue in the baseline experiment was uninformative about the orientation of gratings. On both OD and SFD tasks and both two expected conditions (Expect 20° and Expect 70°), for each distance (Δ0°-Δ40°), we computed a discrimination sensitivity (<italic>DS</italic>) to quantify how much the discrimination threshold (<italic>DT</italic>) changed between baseline (<italic>DT<sub>baseline</sub></italic>) and main (<italic>DT<sub>main</sub></italic>) experiments: <italic>DS = DT<sub>baseline</sub> - DT<sub>main</sub></italic>. Because the <italic>DS</italic> from Expect 20° and Expect 70° showed as a similar pattern, they were pooled together for further analysis (unless otherwise stated, we present average data from two expected conditions). The averaged <italic>DSs</italic> were submitted to a one-way repeated-measures ANOVA with the distance (Δ0°-Δ40°) as a within-participants factor. Results showethat, the main effect of distance was significant in both OD (F(4,92) = 3.739, p = 0.010, η<sub>p</sub><sup>2</sup> = 0.140, <xref rid="fig2" ref-type="fig">Fig. 2c</xref>, top) and SFD (F(4,92) = 2.822, p = 0.042, η<sub>p</sub><sup>2</sup> = 0.109, <xref rid="fig2" ref-type="fig">Fig. 2c</xref>, bottom) tasks. To directly address the potential inhibitory zone surrounding the focus of expectation, we compared the <italic>DS</italic>s between Δ20° and Δ0°, and between Δ20° and Δ40° on each task. <italic>Post hoc</italic> paired <italic>t</italic> tests revealed that, for both tasks, the <italic>DSs</italic> of Δ20° were significantly lower than those of both Δ0° (OD task: t(23) = −4.263, p &lt; 0.001, Cohen’s <italic>d</italic> = 0.870; SFD task: t(23) = −4.679, p &lt; 0.001, Cohen’s <italic>d</italic> = 0.955) and Δ40° (OD task: t(23) = −2.214, p = 0.037, Cohen’s <italic>d</italic> = 0.452; SFD task: t(23) = −2.694, p = 0.013, Cohen’s <italic>d</italic> = 0.550), indicating a classical center-surround inhibition in expectation with the enhanced discriminability to the expected orientation (Δ0°) and decreased discriminability to orientations (Δ20°) similar to the expected orientation relative to orientations (Δ40°) more distinct from it. Intriguingly, this center-surround inhibition profile of expectation was independent of attentional modulations by task-relevance of the orientation.</p>
<p>Subsequently, to further assess the shape of this expectation pattern, we fitted a monotonic model and a non-monotonic model to the average <italic>DS</italic>s across distances (Δ0°-Δ40°) on both OD and SFD tasks. The monotonic and nonmonotonic models were implemented as the Gaussian and Mexican-hat functions, respectively<sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup>. To compare these two models to our data, we first computed the Akaike information criterion (AIC)<sup><xref ref-type="bibr" rid="c37">37</xref></sup> and Bayesian information criterion (BIC)<sup><xref ref-type="bibr" rid="c38">38</xref></sup> with the assumption of a normal error distribution. Then, we calculated the Likelihood ratio (LR) and Bayes factor (BF) of the Mexican-hat model over the Gaussian model based on AIC<sup><xref ref-type="bibr" rid="c39">39</xref></sup> and BIC<sup><xref ref-type="bibr" rid="c40">40</xref></sup> approximation, respectively. Results showed that, in both tasks, the LR/BFs were larger than 1 (OD task: LR/BF = 2.088×10<sup>5</sup>; SFD task: LR/BF = 1.288) and therefore strongly favored the Mexican-hat model over the Gaussian model (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>). Notably, we also conducted similar model comparisons for each participant’s data and found that the Mexican-hat model was favored over the Gaussian model in 23 and 17 of 24 participants, for OD and SFD tasks, respectively (<xref rid="fig2" ref-type="fig">Fig. 2d</xref>). Together, these results constituted strong evidence for the center-surround inhibition profile of expectation, and further indicated its independence of attentional modulations by task-relevance of the orientation.</p>
</sec>
<sec id="s2b">
<title>Computational models of the center-surround inhibition in expectation</title>
<p>Our results demonstrated the classical center-surround inhibition profile in expectation, yet it remains unclear what type of neural computations could account for this profile. We proposed that this profile could be explained by either Tuning sharpening (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>, left) or Tuning shift (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>, right) models. The Tuning sharpening model postulates that expectation sharpens the tuning of individual neurons (thick curves) of the expected orientation, which results in a center-surround population response profile (black curve) centered at the expected orientation. The Tuning shift model postulates that expectation attracts the tuning of individual neurons (thick curves) from unexpected orientations towards the expected orientation, which also results in a center-surround population response profile. Note that, in our study, the shift towards to 20° was (arbitrarily) considered to be the negative value, whereas the shift towards to 70° was thus the positive value, and unless otherwise stated, we present the average shift, i.e., <italic>mean shift = (shift towards to 70°-shift towards to 20°)/2</italic>, across conditions hereafter. For both OD and SFD tasks, to compare these two models, we fitted both the Tuning sharpening and Tuning shift models (sum of idealized channel tuning functions) to the population response profiles (the smooth negative values of discrimination thresholds) during baseline and main experiments, and measured their root mean squared deviation (RMSD) metric<sup><xref ref-type="bibr" rid="c41">41</xref></sup>. RMSD takes the number of model parameters into account, and a smaller RMSD indicates better model fitness. Our results showed that both models exhibited robust fits to our data (<xref rid="fig3" ref-type="fig">Fig. 3b</xref> and <xref rid="fig3" ref-type="fig">Fig. 3e</xref>), as indicated by high <italic>R<sup>2</sup></italic> values and comparably low RMSDs in both OD (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>) and SFD (<xref rid="fig4" ref-type="fig">Fig. 4b</xref>) tasks. Similarly, we computed a discrimination sensitivity (<italic>DS</italic>) to quantify how much the fitted discrimination threshold (<italic>FDT</italic>) changed between baseline (<italic>FDT<sub>baseline</sub></italic>) and main (<italic>FDT<sub>main</sub></italic>) experiments: <italic>DS</italic> = <italic>FDT<sub>baseline</sub></italic>- <italic>FDT<sub>main</sub></italic>. For both models, a similar center-surround inhibition profile of the <italic>DS</italic> was found on both OD (<xref rid="fig3" ref-type="fig">Fig. 3c</xref>) and SFD (<xref rid="fig3" ref-type="fig">Fig. 3f</xref>) tasks. Further model comparisons for each participant’s data confirmed that the Mexican-hat model was favored over the Gaussian model on both OD (22 and 19 of 24 participants for Tuning sharpening and Tuning shift models, respectively, <xref rid="fig3" ref-type="fig">Fig. 3d</xref>) and SFD (14 and 17 of 24 participants for Tuning sharpening and Tuning shift models, respectively, <xref rid="fig3" ref-type="fig">Fig. 3g</xref>) tasks. These results imply that the center-surround inhibition in expectation could be reproduced by either Tuning sharpening or Tuning shift models.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 4 |</label>
<caption><title>RMSDs of Tuning sharpening and Tuning shift models.</title>
<p>RMSDs of Tuning sharpening and Tuning shift models during the baseline (top) and main (bottom) experiments, on OD (<bold>a</bold>) and SFD (<bold>b</bold>) tasks. B20°: Baseline 20°; B70°: Baseline 70°; E20°: Expect 20°; E70°: Expect 70°. Open symbols represent the data from each participant and filled colored dots represented the mean across participants. Error bars indicate 1 SEM calculated across participants.</p></caption>
<graphic xlink:href="609781v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>For each model and each task, to directly compare the tuning curve changes of both the expected (Δ0°) and unexpected orientations (Δ10°-Δ40°) with and without expectation, we calculated the parameters changes of tuning functions (amplitude <italic>A</italic>, location <italic>x0</italic>, and width ơ) for hypothesized channels between baseline and main experiments (<xref ref-type="supplementary-material" rid="supp1">Supplementary Fig. 2</xref>). For the Tuning sharpening model, the tuning width of each channel’s tuning function is parameterized by 1., while all tuning functions are evenly distributed with 10° spacing on the <italic>x</italic>-axis and the areas under the curves (response energy) are identical. Conversely, for the Tuning shift model, the location of each channel’s tuning function is parameterized by <italic>x0</italic>, while they all share the same tuning amplitude and width. For both models, parameters were varied to obtain the minimal sum of squared errors between the population response profile and the model prediction, which is the sum of all channels’ tuning responses. For the expected orientation (Δ0°) of Tuning sharpening model, results showed that the amplitude change was significantly higher than zero on both OD (t(23) = 4.198, p &lt; 0.001, Cohen’s d = 0.857) and SFD (t(23) = 3.247, p = 0.004, Cohen’s d = 0.663) tasks (<xref rid="fig3" ref-type="fig">Fig. 3h</xref>, vertical stripes); the width change was significantly lower than zero on both OD (t(23) = −2.235, p = 0.035, Cohen’s d = 0.456) and SFD (t(23) = −3.313, p = 0.003, Cohen’s d = 0.676) tasks (<xref rid="fig3" ref-type="fig">Fig. 3h</xref>, diagonal stripes). For unexpected orientations (Δ10°-Δ40°), however, the amplitude and width changes were not significant with zero on either OD (amplitude change: t(23) = 1.948, p = 0.064, Cohen’s d = 0.397; width change: t(23) = −0.412, p = 0.684, Cohen’s d = 0.084) or SFD (amplitude change: t(23) = 1.708, p = 0.101, Cohen’s d = 0.349; width change: t(23) = 1.273, p = 0.216, Cohen’s d = 0.260) tasks (<xref rid="fig3" ref-type="fig">Fig. 3i</xref>). For the Tuning shift model, results showed that the location shift was significantly different than zero for unexpected orientations (Δ10°-Δ40°, OD task: t(23) = 2.547, p = 0.018, Cohen’s d = 0.520; SFD task: t(23) = 4.099, p &lt; 0.001, Cohen’s d = 0.837 (<xref rid="fig3" ref-type="fig">Fig. 3k</xref>), but not for the expected orientation (Δ0°, OD task: t(23) =0.993, p = 0.331, Cohen’s d = 0.203; SFD task: t(23) = 1.750, p = 0.093, Cohen’s d = 0.357 (<xref rid="fig3" ref-type="fig">Fig. 3j</xref>). These results further confirm the Tuning sharpening and Tuning shift computations for the center-surround inhibition in expectation.</p>
<p>In addition, across participants, we further used the non-parametric Wilcoxon signed-rank test to compare both the R<sup>2</sup> and RMSD between two models for Baseline 20°, Baseline 70°, Expect 20°, and Expect 70° conditions during each task. Results showed that there was no significant difference between two models in Baseline 20° (OD task: R<sup>2</sup>: z = 1.372, p = 0.170, effect size: r = 0.280; RMSD: z = 1.200, p = 0.230, effect size: r = 0.245; SFD task: R<sup>2</sup>: z = 0.857, p = 0.391, effect size: r = 0.175; RMSD: z = 0.829, p = 0.407, effect size: r = 0.169), Baseline 70° (OD task: R<sup>2</sup>: z = 0.371, p = 0.710, effect size: r = 0.076; RMSD: z = 0.029, p = 0.977, effect size: r = 0.006; SFD task: R<sup>2</sup>: z = 1.657, p = 0.097, effect size: r = 0.338; RMSD: z = 0.686, p = 0.493, effect size: r = 0.140), Expect 20° (OD task: R<sup>2</sup>: z = 0.686, p = 0.493, effect size: r = 0.140; RMSD: z = 1.600, p = 0.110, effect size: r = 0.327; SFD task: R<sup>2</sup>: z = 1.257, p = 0.209, effect size: r = 0.257; RMSD: z = 1.600, p = 0.110, effect size: r = 0.327), or Expect 70° (OD task: R<sup>2</sup>: z = 1.486, p = 0.137, effect size: r = 0.303; RMSD: z = 1.686, p = 0.092, effect size: r = 0.344; SFD task: R<sup>2</sup>: z = 0.514, p = 0.607, effect size: r = 0.105; RMSD: z = 0.143, p = 0.886, effect size: r = 0.029) conditions (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). These results further imply that Tuning sharpening and Tuning shift models may equally contribute to center-surround inhibition in expectation.</p>
</sec>
<sec id="s2c">
<title>Orientation adjustment experiment</title>
<p>To further explore the co-existence of both Tuning sharpening and Tuning shift computations in center-surround inhibition profile of expectation, participants were asked to perform a classic orientation adjustment experiment. The protocol of orientation adjustment experiment was similar with that of the profile experiment, except for two aspects. First, there were 4 possible (20°, 40°, 50°, and 70°) orientations for the first grating: 20°/70° (Δ0° deviated from the expected orientation) and 40°/50° (Δ20°/Δ30° deviated from the expected orientation). Second, in both baseline and main experiments, the second grating was set as a random orientation within the range of 0° to 90°, and participants were required to rotate the orientation of the second grating to match the first (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>). Similar to the profile experiment, no significant difference was found in tone report accuracies across distances (<xref ref-type="supplementary-material" rid="supp1">Supplementary Fig. 3</xref>). For both expected (Δ0°) and unexpected (Δ20°/Δ30°) orientations, we calculated the adjusted orientation difference between the baseline and main experiments. Results showed the adjusted difference was significantly higher than zero for unexpected orientations (0.735 ± 0.308: t(19) = 2.387, p = 0.028, Cohen’s d = 0.534, <xref rid="fig6" ref-type="fig">Fig. 6a</xref>, right), but not for the expected orientation (0.143 ± 0.523: t(19) = 0.274, p = 0.787, Cohen’s d = 0.061, <xref rid="fig6" ref-type="fig">Fig. 6a</xref>, left), suggesting a significant bias in the unexpected orientation representation towards the expected orientation.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 5 |</label>
<caption><title>Protocol and error distributions of the orientation adjustment experiment.</title>
<p><bold>a,</bold> The protocol of orientation adjustment experiment was similar with that of the profile experiment, except for two aspects. First, there were 4 possible (20°, 40°, 50°, and 70°) orientations for the first grating: 20°/70° (Δ0° deviated from the expected orientation) and 40°/50° (Δ20°/Δ30° deviated from the expected orientation). Second, in both baseline and main experiments, the second grating was set as a random orientation within the range of 0° to 90°, and participants were required to rotate the orientation of the second grating to match the first. HT: high tone; LT: low tone. <bold>b,</bold> Three-component mixture model to the adjusted errors from baseline (left) and main (middle) experiments. In current study, the shift towards to 20° was (arbitrarily) considered to be the negative value (‘-’), whereas the shift towards to 70° was thus the positive value (‘+’). The mean shift was calculated as: <italic>mean shift = (shift towards to 70°-shift towards to 20°)/2</italic>. The shaded error bars indicate 1 SEM calculated across participants. B20°: Baseline 20°; B70°: Baseline 70°; E20°: Expect 20°; E70°: Expect 70°.</p></caption>
<graphic xlink:href="609781v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 6 |</label>
<caption><title>Results of the orientation adjustment experiment.</title>
<p><bold>a,</bold> The adjusted orientation difference between the baseline and main experiments in both expected (20°/70°, i.e. Δ0°, middle) and unexpected (40°/50°, i.e. Δ20°/Δ30°, right) conditions. B20°: Baseline 20°; B70°: Baseline 70°; E20°: Expect 20°; E70°: Expect 70°. <bold>b-d,</bold> The parameter estimates difference between the baseline and main experiments in both expected (Δ0°, middle) and unexpected (Δ20°/Δ30°, right) orientations. The parameter estimates were obtained by fitting a three-component mixture model to adjusted errors in different conditions. <bold>b,</bold> <italic>mu</italic> reflects the response distribution shift away from the presented grating orientation. <bold>c,</bold> <italic>s.d.</italic> reflects precision of responses (with higher values indicating worse precision). <bold>d,</bold> <italic>g</italic> estimates the probability that the participant produced a random response (i.e., the guess). Open symbols represent the data from each participant and error bars indicate 1 SEM calculated across participants (*p &lt; 0.05; **p &lt; 0.005; ***p &lt; 0.001).</p></caption>
<graphic xlink:href="609781v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Subsequently, we employed a three-component mixture model<sup><xref ref-type="bibr" rid="c42">42</xref>,<xref ref-type="bibr" rid="c43">43</xref></sup> to the adjusted errors from both baseline and main experiments (<xref rid="fig5" ref-type="fig">Fig. 5b</xref>). This allowed us to estimate representation precision, including the mean shift (<italic>mu</italic>) and standard deviation (<italic>s.d.</italic>) of the von Mises distribution (positive values indicating rightward shift and higher values indicating lower precision, respectively), along with assessing the probability of stimulus guessing (<italic>g</italic>). Using the difference between the main and baseline experiments (<xref ref-type="supplementary-material" rid="supp1">Supplementary Fig. 4</xref>), we also found that the orientation representation significantly shifted for unexpected orientations (0.752 ± 0.303: t(19) = 2.481, p = 0.023, Cohen’s d = 0.555, <xref rid="fig6" ref-type="fig">Fig. 6b</xref>, right), but not for the expected orientation (0.214 ± 0.493: t(19) = 0.434, p = 0.669, Cohen’s d = 0.097, <xref rid="fig6" ref-type="fig">Fig. 6b</xref>, left). Conversely, participants exhibited higher orientation representation precision than baseline in the expected orientation (−0.973 ± 0.271: t(19) = −3.597, p = 0.002, Cohen’s d = 0.804, <xref rid="fig6" ref-type="fig">Fig. 6c</xref>, left), but not in unexpected orientations (−0.390 ± 0.212: t(19) = −1.837, p = 0.082, Cohen’s d = 0.411, <xref rid="fig6" ref-type="fig">Fig. 6c</xref>, right). Finally, we found no significant difference with zero in the rate of guessing in either expected (−0.0047 ± 0.0554: t(19) = −0.849, p = 0.406, Cohen’s d = 0.190, <xref rid="fig6" ref-type="fig">Fig. 6d</xref>, left) or unexpected (−0.0023 ± 0.0148: t(19) = −1.551, p = 0.138, Cohen’s d = 0.347, <xref rid="fig6" ref-type="fig">Fig. 6d</xref>, right) orientations. These results provide converging evidence supporting our hypothesis that both Tuning sharpening and Tuning shift contribute to the center-surround inhibition profile of expectation.</p>
</sec>
<sec id="s2d">
<title>Orientation discrimination experiment</title>
<p>Note that behavioral benefits in our orientation adjustment task could be due to improvements in either perceptual or decisional processes, as the expectation cue held information about both the most likely stimulus and the most likely correct response<sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup>. To remove this link between stimulus and response expectations and thereby avoid potential response biases induced by the cue we designed an additional orientation discrimination experiment. The protocol of this orientation discrimination experiment was very similar with that of the orientation adjustment experiment, except for two aspects (<xref rid="fig7" ref-type="fig">Fig. 7a</xref>). First, there were 3 possible (20°, 45°, and 70°) orientations for the first grating: 20°/70° (Δ0° deviated from the expected orientation) and 45° (Δ25° deviated from the expected orientation). Second, in both baseline and main experiments, the second grating were 1°, 3°, 5°, 7°, and 9° deviated from the first grating, either clockwise or counterclockwise. Participants were asked to make a 2AFC judgment of the orientation of the second grating relative to the first, either clockwise or anticlockwise. <xref rid="fig7" ref-type="fig">Fig. 7b</xref> and <xref rid="fig7" ref-type="fig">Fig. 7c</xref> shows the psychometric functions for each condition. We plotted the percentage of trials in which participants indicated the orientation of second grating that was anticlockwise or clockwise to the first for 20° (Baseline 20° and Expect 20°) and 70° (Baseline 70° and Expect 70°) conditions, respectively, as a function of the actual orientation difference between the two gratings. For each participant and each condition, the psychometric values at 10 orientation differences were fitted to a cumulative Gaussian function and we interpolated the data to find the slope (orientation uncertainty) and PSE (point of subjective equality, which is the shift here) as an index for Tuning sharpening and Tuning shift models, respectively.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 7 |</label>
<caption><title>Protocol and results of the orientation discrimination experiment.</title>
<p><bold>a,</bold> The protocol of orientation discrimination experiment was similar with that of the orientation adjustment experiment, except for two aspects. First, there were 3 possible (20°, 45°, and 70°) orientations for the first grating: 20°/70° (Δ0° deviated from the expected orientation) and 45° (Δ25° deviated from the expected orientation). Second, in both baseline and main experiments, the second grating were 1°, 3°, 5°, 7°, and 9° deviated from the first grating, either clockwise (CW) or counterclockwise (CCW). Participants were asked to make a 2AFC judgment of the orientation of the second grating relative to the first, either clockwise or anticlockwise. HT: high tone; LT: low tone. Psychometric functions showing orientation judgements in each condition for Δ0° (<bold>b</bold>) and Δ25° (<bold>c</bold>). Data points averaged across participants were fit using a cumulative normal function. The abscissa refers to ten orientation differences between the first and second gratings. The ordinate refers to the percentage of trials in which participants indicated the orientation of second grating that was anticlockwise or clockwise to the first for expected 20° (left) and 70° (right) conditions, respectively. The slope (an index for the Tuning sharpening model, <bold>d</bold>) and shift (an index for the Tuning shift model, <bold>e</bold>) differences between the baseline and main experiments for expected 20° and 70° conditions. Negative: shift to left; Positive: shift to right. Open symbols represent the data from each participant and error bars indicate 1 SEM calculated across participants. B20°: Baseline 20°; B70°: Baseline 70°; E20°: Expect 20°; E70°: Expect 70° (*p &lt; 0.05; **p &lt; 0.005).</p></caption>
<graphic xlink:href="609781v1_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Similar to the orientation adjustment experiment, no significant difference was found in tone report accuracies across distances (<xref ref-type="supplementary-material" rid="supp1">Supplementary Fig. 5</xref>). For both expected (Δ0°) and unexpected (Δ25°) orientations, we calculated the slope and shift difference between the baseline and main experiments. Results showed the slope difference was significantly higher than zero for expected orientations (0.0250 ± 0.0075: t(17) = 3.324, p = 0.004, Cohen’s d = 0.627, <xref rid="fig7" ref-type="fig">Fig. 7d</xref>, top), but not for the unexpected orientation (−0.0204 ± 0.0113: t(17) = −1.812, p = 0.088, Cohen’s d = 0.402, <xref rid="fig7" ref-type="fig">Fig. 7d</xref>, bottom). Conversely, the shift difference was significantly lower than zero for the unexpected orientation (−0.696 ± 0.287: t(17) = −2.423, p = 0.027, Cohen’s d = 0.507, <xref rid="fig7" ref-type="fig">Fig. 7e</xref>, bottom), but not for expected orientations (0.449 ± 0.509: t(17) = 0.881, p = 0.391, Cohen’s d = 0.452, <xref rid="fig7" ref-type="fig">Fig. 7e</xref>, top). These results indicated that the expectation not only sharpened the tuning curves of neurons for the expected orientation but also attracted the tuning curves of neurons for unexpected orientations, further confirming Tuning sharpening and Tuning shift models, respectively, in the center-surround inhibition of expectation.</p>
</sec>
<sec id="s2e">
<title>Artificial neural networks for the center-surround inhibition in expectation</title>
<p>Finally, we trained a deep predictive coding neural network (DPCNN), modified from Predify<sup><xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c45">45</xref></sup> to perform both the OD and SFD tasks. For both tasks, the DPCNN consisted of 6 feedforward encoding layers (<italic>e1-e6</italic>), 5 generative feedback decoding layers (<italic>d1-d5</italic>), and 3 fully connected (fc) layers (<xref rid="fig8" ref-type="fig">Fig. 8a</xref>). The reconstruction error (<italic>E1-E5</italic>) is computed and used for the proposed predictive coding updates<sup><xref ref-type="bibr" rid="c31">31</xref></sup>, denoting by <italic>P.C.</italic> loops. Note that the updating is only applied to <italic>e1-e5</italic> and for the last layer <italic>e6</italic>, there is no feedback. Before the layer <italic>e0,</italic> we obtained the pixel difference between the target and reference images, which was then superimposed on the channels of the reference image. This superimposed image was set as the input of the network and remained constant over timesteps. Besides, we used a feedforward encoding layer (i.e., <italic>e0</italic>) to match the number of the channels between superimposed feature maps and the pre-trained DPCNN. During the training, the last layer of network was trained to capture the difference between the target and reference and finally obtain the classification by softmax, to model decision making in our 2AFC paradigm (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>), in which participants were asked to make a 2AFC judgment of the orientation (either clockwise or anticlockwise) or the spatial frequency (either lower or higher) of the second grating (target) relative to the first (reference) in OD and SFD tasks, respectively. For both tasks, the DPCNN was independently and randomly trained 12 times, and for each distance (Δ0°-Δ40°), the training effect was defined as the accuracy difference (<italic>ACC<sub>difference</sub></italic>) between the pre- (ACC<sub>baseline</sub>) and post- (<italic>ACC<sub>trained</sub></italic>) training. Similar to our psychophysical results, on both tasks, the LR/BFs were much larger than 1 (OD task: LR/BF = 2.045×10<sup>5</sup>, <xref rid="fig8" ref-type="fig">Fig. 8d</xref>, left; SFD task: LR/BF = 5.5929, <xref rid="fig8" ref-type="fig">Fig. 8h</xref>, left) and therefore strongly favored the Mexican-hat model over the Gaussian model. The model comparison based on fitting individual data advocated that the Mexican-hat model was favored over the Gaussian model in 10 and 11 of 12 training data on OD (<xref rid="fig8" ref-type="fig">Fig. 8e</xref>, left) and SFD (<xref rid="fig8" ref-type="fig">Fig. 8i</xref>, left) tasks, respectively. Besides, across individual data, a non-parametric Wilcoxon signed-rank test was conducted to compare the <italic>R<sup>2</sup></italic> of two models, and results significantly advocated the Mexican-hat model over the Gaussian model on both OD (z = 2.197, p = 0.028, effect size: r = 0.634) and SFD (z = 2.981, p = 0.003, effect size: r = 0.861) tasks. These results suggest that our DPCNN can emerge the similar center-surround inhibition by expectation on both the orientation and spatial frequency trainings.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 8 |</label>
<caption><title>Results of artificial neural networks.</title>
<p><bold>a,</bold> Model structure and stimulus examples for deep predictive coding neural network (DPCNN) and standard feedforward CNN, on both OD (purple) and SFD (blue) tasks. DPCNN consisted of 6 feedforward encoding layers (<italic>e1-e6</italic>), 5 generative feedback decoding layers (<italic>d1-d5</italic>), and 3 fully connected (fc) layers. The reconstruction error (<italic>E1-E5</italic>) is computed and used for the proposed predictive coding updates, denoting by <italic>P.C.</italic> loops. The CNN is the same with DPCNN but removing feedback predictive coding iterations. The accuracy of each distance during the pre- (<bold>b</bold>) and post- (<bold>c</bold>) training for DPCNN (left) and CNN (right), on the OD task. <bold>d,</bold> The training effect (i.e., the ACC difference between pre- and post-training) of each distance in DPCNN (left) and CNN (right), and the best fitting Mexican-hat and Gaussian functions to these training effects across distances, on the OD task. M, Mexican-hat model; G, Gaussian model. <bold>e,</bold> <italic>R<sup>2</sup></italic> of the best fitting Mexican-hat and Gaussian functions from individual data in DPCNN (left) and CNN (right) on the OD task. Open symbols represent individual data and filled colored dots represented the mean across data. Error bars indicate 1 SEM calculated across data. <bold>f-i,</bold> The results from the SFD task, see caption for (<bold>b-e</bold>) for a description of each type of graph.</p></caption>
<graphic xlink:href="609781v1_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Additionally, to further determine the contribution of predictive (reconstructive) feedback to center-surround inhibition in expectation, we performed ablation studies, in which we trained the same network but removing feedback predictive coding iterations (a standard feedforward CNN, i.e., a modified network of AlexNet<sup><xref ref-type="bibr" rid="c46">46</xref></sup>). As expected, on both tasks, removing feedback leads to the disappearance of center-surround inhibition in expectation. Across individual data, there was no significant difference in the <italic>R<sup>2</sup></italic> between Mexican-hat and Gaussian models on either OD (non-parametric Wilcoxon signed-rank test: z = −0.314, p = 0.754, effect size: r = 0.091, <xref rid="fig8" ref-type="fig">Fig. 8e</xref>, right) or SFD (non-parametric Wilcoxon signed-rank test: z = −0.356, p = 0.722, effect size: r = 0.103, <xref rid="fig8" ref-type="fig">Fig. 8i</xref>, right) tasks. These results further confirm that the predictive coding feedback plays a critical role in producing the center-surround inhibition in expectation.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>The present results provide support for an attentional modulation-independent center-surround inhibition profile of expectation and further reveal its underlying neural computations. Specifically, on both OD and SFD tasks, the finest-grained discrimination performance, indexed by the lowest thresholds, of the expected orientation confirmed the previous notion that expectation had a facilitatory effect on various perceptions<sup><xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c47">47</xref>–<xref ref-type="bibr" rid="c52">52</xref></sup>. Whereas the coarser-grained discrimination performance, indexed by the higher thresholds, of orientations very similar to the expected orientation relative to orientations more distinct from the expected orientation, demonstrated a classical inhibitory zone surrounding the focus of expectation (i.e., the center-surround inhibition profile, <xref rid="fig2" ref-type="fig">Fig. 2</xref>). One could argue that this profile was derived from top-down attention rather than expectation. Compared to unexpected gratings with much low validity, the expected grating with very high validity in our study, presumably, had more degree of top-down attention that has been proven to display the similar center-surround inhibition profile in orientation space by previous studies<sup><xref ref-type="bibr" rid="c53">53</xref>–<xref ref-type="bibr" rid="c56">56</xref></sup> and computational models<sup><xref ref-type="bibr" rid="c57">57</xref>–<xref ref-type="bibr" rid="c59">59</xref></sup>. In other words, our study may not examine a center-surround inhibition profile of expectation, but instead of top-down attention. It is important to note that, in our study, for each grating, participants performed the same discrimination task at threshold, measured by the QUEST staircase procedure (75% correct)<sup><xref ref-type="bibr" rid="c34">34</xref></sup>, which could maximally (although not completely) control the difference in top-down attention among distances. More importantly, our observed center-surround inhibition profile of expectation in orientation space was independent of attentional modulations by the task-relevance of orientation (i.e., OD and SFD tasks, <xref rid="fig2" ref-type="fig">Fig. 2</xref>), consistent with previous findings<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c60">60</xref>–<xref ref-type="bibr" rid="c65">65</xref></sup>, showing an independency between attention and expectation. If the center-surround inhibition profile was derived from attention rather than expectation, then we should have not observed it on SFD task, in which the orientation was never task-relevant. Participants did not need to direct attention to this task-irrelevant feature, and therefore yielding none of profiles in orientation space.</p>
<p>The center-surround inhibition profile of expectation evident in our study is consistent with what has been observed for spatial attention<sup><xref ref-type="bibr" rid="c66">66</xref>–<xref ref-type="bibr" rid="c74">74</xref></sup>, feature-based attention<sup><xref ref-type="bibr" rid="c53">53</xref>–<xref ref-type="bibr" rid="c56">56</xref>,<xref ref-type="bibr" rid="c75">75</xref>–<xref ref-type="bibr" rid="c78">78</xref></sup>, working memory<sup><xref ref-type="bibr" rid="c79">79</xref>–<xref ref-type="bibr" rid="c81">81</xref></sup>, and visual perceptual learning<sup><xref ref-type="bibr" rid="c35">35</xref></sup>, in various feature spaces. This suggests that center-surround inhibition could be a unifying principle underlying a diversity of visual representations, as previously proposed by the selective tuning model<sup><xref ref-type="bibr" rid="c57">57</xref>–<xref ref-type="bibr" rid="c59">59</xref></sup>, however, the extent of the inhibitory zone varied largely across these domains and features. For example, within the orientation space, the inhibitory zone was about 20°, 45°, and 54° for expectation evident here, feature-based attention<sup><xref ref-type="bibr" rid="c21">21</xref></sup>, and visual perceptual learning<sup><xref ref-type="bibr" rid="c35">35</xref></sup>, respectively; within the feature-based attention, it was about 30° and 45° in color<sup><xref ref-type="bibr" rid="c77">77</xref></sup> and motion direction<sup><xref ref-type="bibr" rid="c53">53</xref></sup> spaces, respectively. Although speculative, these findings hint at the exciting possibility that the width of an inhibitory surround may vary flexibly with both cognitive and task demands, ultimately facilitating our perception and behavior to adapt to changing environment. Mechanistically, the center-surround inhibition profile can be optimal to locally resolve competition between inputs that overlap in their neural representations, specifically attenuating the interference from nearby irrelevant and confusable representations that would be presumably within the same cortical map, and therefore at the largest risk to confuse the current processing. Given the presence of a well-defined map-based organization of the cerebral cortex<sup><xref ref-type="bibr" rid="c82">82</xref>–<xref ref-type="bibr" rid="c87">87</xref></sup>, the center-surround inhibition would be beneficial across all features and therefore serves as a canonical neural computation that sharpens various cognitive processing across different domains.</p>
<p>Strikingly, we found that the center-surround inhibition profile of expectation observed behaviorally can be accounted by sharpening of tuning curves of neurons of the expected orientation, as revealed by the computational model (<xref rid="fig3" ref-type="fig">Fig. 3</xref>), orientation adjustment (<xref rid="fig5" ref-type="fig">Fig. 5</xref> and <xref rid="fig6" ref-type="fig">Fig. 6</xref>), and orientation discrimination (<xref rid="fig7" ref-type="fig">Fig. 7</xref>) experiments. These changes – sharpening of tuning curves – are not only in line with the sharpening hypothesis of expectation developed by previous neurophysiological<sup><xref ref-type="bibr" rid="c10">10</xref>–<xref ref-type="bibr" rid="c14">14</xref></sup>, electro-/magneto-encephalogram<sup><xref ref-type="bibr" rid="c15">15</xref>–<xref ref-type="bibr" rid="c19">19</xref></sup>, and fMRI<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c20">20</xref>–<xref ref-type="bibr" rid="c22">22</xref></sup> studies that has invoked the tuning sharpening as the neural basis of expectation-related effects (e.g., the sharpening of tuning curves facilitates fine orientation discrimination by increasing the activity difference between similar orientations), but also extend this hypothesis by identifying the same neural computation in its center-surround inhibition profile. More importantly, we further found that this profile of expectation can be accounted by the tuning shift computation that neurons of unexpected orientations shift their spectral tuning toward the expected orientation (<xref rid="fig3" ref-type="fig">Fig. 3</xref>, <xref rid="fig6" ref-type="fig">Fig. 6</xref>, and <xref rid="fig7" ref-type="fig">Fig. 7</xref>). In accordance with expectation, several other cognitive processing has also been shown to shift neuronal tuning curves or receptive fields toward the target, such as spatial<sup><xref ref-type="bibr" rid="c88">88</xref>–<xref ref-type="bibr" rid="c94">94</xref></sup> and feature-based<sup><xref ref-type="bibr" rid="c95">95</xref>–<xref ref-type="bibr" rid="c98">98</xref></sup> attention, as well as visual search<sup><xref ref-type="bibr" rid="c58">58</xref>,<xref ref-type="bibr" rid="c99">99</xref>–<xref ref-type="bibr" rid="c103">103</xref></sup> and perceptual learning<sup><xref ref-type="bibr" rid="c104">104</xref>–<xref ref-type="bibr" rid="c106">106</xref></sup>. Interestedly, several brain imaging studies have reported that expectation alters the baseline<sup><xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c107">107</xref>,<xref ref-type="bibr" rid="c108">108</xref></sup> or gain<sup><xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c109">109</xref>–<xref ref-type="bibr" rid="c111">111</xref></sup> of neurons in visual areas, consistent with a classical hypothesis, i.e., the labeled-line theories of visual information processing<sup><xref ref-type="bibr" rid="c112">112</xref>–<xref ref-type="bibr" rid="c116">116</xref></sup>, which posits that neurons in sensory cortex act as labeled lines with fixed tuning properties that encode input features consistently, regardless of task demands. However, this theory does not account for either tuning curve sharpening or tuning curve shifts of sensory neurons induced by expectation in our study. These changes we observed in the spectral tuning profiles of sensory neurons, conversely, are not only strongly supported by the matched filter hypothesis that neurons could act as matched filters and reshape or shift their tuning to match the target exactly<sup><xref ref-type="bibr" rid="c114">114</xref></sup>, but also compatible with both proposals from computational models<sup><xref ref-type="bibr" rid="c58">58</xref>,<xref ref-type="bibr" rid="c100">100</xref></sup> and Kalman filtering schemes for the signal detection<sup><xref ref-type="bibr" rid="c103">103</xref></sup>.</p>
<p>Although our study succeeded in linking the center-surround inhibition profile of expectation directly with the response of sensory neurons whose tuning properties make them optimal for demarcating the expected information from various unexpected information, we cannot deny a potential contribution from other cognitive processes, such as the decision making. Indeed, previous studies have indicated that expectations primarily influence decisions by modulating post-perceptual stages of information processing<sup><xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c117">117</xref>–<xref ref-type="bibr" rid="c119">119</xref></sup> or modulate interactions between lower sensory and higher decision areas<sup><xref ref-type="bibr" rid="c109">109</xref>,<xref ref-type="bibr" rid="c120">120</xref></sup>. In addition, these changes in the spectral tuning profiles of sensory neurons evident here derive mainly from psychophysics and computational models. To fully understand how changes in sensory responses contribute to both expectation and its center-surround inhibition profile, further work is needed using neurophysiological techniques or ultra-high field fMRI to explore the locus of events responsible for expectation-induced changes, the identity of neurons that undergo these changes, their patterns of connections, their interactions with higher decision processing, and underlying synaptic bases, especially for our observed shifts in unexpected orientation tunings.</p>
<p>In addition, the emerged center-surround inhibition of expectation in the pretrained DPCNN, is not only in line with previous studies and theories that interprets expectation within the predictive coding framework<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c29">29</xref>–<xref ref-type="bibr" rid="c33">33</xref></sup>, but also adds strong evidence supporting artificial neural networks’ potential to perform various human-like representations, such as visual perceptual learning<sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c121">121</xref>,<xref ref-type="bibr" rid="c122">122</xref></sup> and hierarchical coding<sup><xref ref-type="bibr" rid="c123">123</xref></sup>, face processing<sup><xref ref-type="bibr" rid="c124">124</xref></sup>, contour integration<sup><xref ref-type="bibr" rid="c125">125</xref></sup>, and the perception of illusory contours<sup><xref ref-type="bibr" rid="c45">45</xref></sup>. More importantly, our ablation studies further confirm a critical role of the predictive coding feedback in producing the center-surround inhibition in expectation (<xref rid="fig8" ref-type="fig">Fig. 8</xref>). Although our similarities between artificial neural networks and humans were mostly qualitative, the artificial neural network can provide new ways of studying expectation from behavior to physiology, serving as a test bed for various theories and assisting in generating predictions for physiological studies.</p>
<p>In sum, our study provides, to the best of our knowledge, the first evidence for a center-surround inhibition profile of expectation and how it is supported by not only changes in the tuning curves of sensory neurons but also the predictive coding framework, leading the way towards diversifying models or theories and takes a significant step in unraveling the neuronal computations underlying expectation, or, more generally, top-down processing.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>A total of 24 healthy human adults (16 females, 19–26 years old) were involved in the study. All of them participated in the profile experiments, 20 and 18 of them participated in the orientation adjustment and orientation discrimination experiments, respectively. All participants with normal or corrected-to-normal vision, gave written informed consent, were right-handed, and were naïve to the purpose of the experiments. The procedures and protocols were approved by the human participants review committee of School of Psychology at South China Normal University</p>
</sec>
<sec id="s4b">
<title>Apparatus</title>
<p>The experiments were conducted in a dark, acoustically shielded room. Visual stimuli were displayed on an IIYAMA color graphic monitor (model: HM204DT; refresh rate: 60 Hz; resolution: 1,280 × 1,024; size: 22 inches) at a viewing distance of 57 cm. Participants’ head position was stabilized using a chin rest.</p>
</sec>
<sec id="s4c">
<title>Experimental stimuli</title>
<p>Visual stimuli were two consecutive sinusoidal grating stimuli (1.0 contrast, random phase, radius 10°), which were generated using MATLAB (MathWorks, Natick, MA) in conjunction with the Psychophysics Toolbox<sup><xref ref-type="bibr" rid="c126">126</xref></sup>, and displayed centrally on the gray background (11.196 cd/m<sup>2</sup>). A white fixation point (radius 0.278°) was always presented at the center of the screen throughout the experiment. The auditory cue consisted of two pure tones (240 Hz and 540 Hz), presented over earphones.</p>
</sec>
<sec id="s4d">
<title>Experimental design and statistical analysis Profile experiment</title>
<sec id="s4d1">
<title>Experimental design</title>
<p>The profile experiment consisted of baseline and main experiments and the baseline experiment always preceded the main experiment. The two experiments were the same, except for the predicting probability relationship between the auditory cue and the orientation (20°, 30°, 40°, 50°, 60°, and 70°) of the first grating. For the baseline experiment, the auditory cue, comprising either a low- (240 Hz) or high- (540 Hz) frequency tone, predicted the orientation of the first grating with equal validity (16.67%, <xref rid="fig1" ref-type="fig">Fig. 1a</xref>, left). In the main experiment, this low- or high-frequency tone auditory cue predicted the orientation (20° or 70°) of the first grating with 75% validity. In the remaining 25% of trials, this orientation was chosen randomly and equally from four non-predicted orientations (30°, 40°, 50°, and 60°, <xref rid="fig1" ref-type="fig">Fig. 1a</xref>, right). Thus, for each participant, there were two types of expected conditions: Expect 20° and Expect 70°, and for both conditions, there were five possible distances in orientation space between the expected and test gratings, ranging from Δ0° through Δ40° with a step size of 10°. Note that, the matches between the tone (low- or high-frequency) of auditory cue and the expected orientation (20° or 70°) of the first grating were flipped across participants, and the order was also counterbalanced across participants. For each participant, although the tone of auditory cue could not predict 20° or 70° orientation in the baseline experiment, whose trials with the same tone that was matched with 20° or 70° orientation in the main experiment, were defined as Baseline 20° (i.e., the baseline of Expect 20°) and Baseline 70° (i.e., the baseline of Expect 70°) conditions, respectively.</p>
<p>Both the baseline and main experiments consisted of two tasks: the orientation discrimination (OD) task and spatial frequency discrimination (SFD) task; with the two tasks occurring on different days; the order of the two tasks was counterbalanced across participants. Differently, the baseline experiment consisted of 4 blocks (two for OD task and the other two for SFD task), and each block had 2 QUEST staircases<sup><xref ref-type="bibr" rid="c34">34</xref></sup> for each of six orientations (20°, 30°, 40°, 50°, 60°, and 70°). The main experiment consisted of 2 blocks (one for OD task and the other one for SFD task), and each block had 24 QUEST staircases for the expected orientations (20° and 70°) and 2 QUEST staircases for each of unexpected orientations (30°, 40°, 50°, and 60°). Each QUEST staircase comprised 40 trials, and on each trial, a low- (240 Hz) or a high- (540 Hz) frequency tone (i.e., the auditory cue) was randomly and equally presented for 200 ms, followed by an 1800 ms fixation interval. Then, two consecutive gratings were each presented for 150-ms and separated by a 300-ms blank interval (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). Participants were first asked to make a two-alternative forced-choice (2AFC) judgment of either the orientation (clockwise or anticlockwise, where orientation was task-relevant) or the spatial frequency (lower or higher, where orientation was task-irrelevant) of the second grating relative to the first, on the OD and SFD tasks, respectively. Then, participants were required to make another 2AFC judgment on tone of the auditory cue, either low or high. In the baseline experiment, for both OD and SFD tasks, the orientation of the first grating was chosen randomly and equally from 20°, 30°, 40°, 50°, 60°, and 70°, while its spatial frequency was fixed at 0.9 cycles/°. The second grating differed slightly from the first in terms of both orientation and spatial frequency. Differently, for OD task, its orientation difference (Δθ<italic>°</italic>, where orientation was task-relevant) varied trial by trial and was controlled by the QUEST staircase to estimate participants’ OD thresholds (75% correct), while its spatial frequency difference was set at 0.06 cycle/°; for SFD task, its spatial frequency difference (Δλ cycles/°, where orientation was task-irrelevant) varied trial by trial and was controlled by the QUEST staircase to estimate participants’ SFD thresholds (75% correct), while its orientation difference was set at 4.8° based on pretest data. Similarly, for each participant, the discrimination threshold obtained during the baseline experiment was used to set the undiscriminated feature difference (i.e., the spatial frequency and orientation for OD and SFD tasks, respectively) during the main experiment, to make the stimuli as similar as possible in both contexts.</p>
</sec>
<sec id="s4d2">
<title>Model fitting and comparison</title>
<p>In both OD and SFD tasks, for two expected conditions and each distance (i.e., Δ0° - Δ40°), we computed a discrimination sensitivity (<italic>DS</italic>) to quantify how much the discrimination threshold (<italic>DT</italic>) changed between baseline (<italic>DT<sub>baseline</sub></italic>) and main (<italic>DT<sub>main</sub></italic>) experiments: <italic>DS</italic> = <italic>DT<sub>baseline</sub></italic> - <italic>DT<sub>main</sub></italic>. Because the <italic>DS</italic> from two expected conditions (Expect 20° and Expect 70°) showed as a similar pattern, they were pooled together for further analysis (unless otherwise stated, we present average data from two expected conditions). During both tasks, for each participant, a monotonic model and a non-monotonic model to the averaged <italic>DS</italic> were fitted. The monotonic and non-monotonic models were implemented as the Gaussian and Mexican-hat (i.e., a negative second derivative of a Gaussian function) functions<sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup>, respectively, as follows:
<disp-formula>
<graphic xlink:href="609781v1_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>y</italic> is the measured <italic>DS</italic>, <italic>x</italic> is the distance; <italic>w</italic>, <italic>A</italic>, and <italic>y0</italic> are the three parameters controlling the shape of the Gaussian function; <italic>m</italic>, <italic>H</italic>, and <italic>y1</italic> are three parameters controlling the shape of the Mexican-hat function. To compare these two models to our data, we first computed the Akaike information criterion (<italic>AIC</italic>)<sup><xref ref-type="bibr" rid="c37">37</xref></sup> and Bayesian information criterion (<italic>BIC</italic>)<sup><xref ref-type="bibr" rid="c38">38</xref></sup>, with the assumption of a normal error distribution as follow:
<disp-formula>
<graphic xlink:href="609781v1_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>N</italic> is the number of observations, <italic>K</italic> is the number of free parameters, and <italic>RSS</italic> is residual sum of squares<sup><xref ref-type="bibr" rid="c127">127</xref></sup>. Then, we further calculated the likelihood ratio (<italic>LR</italic>) and Bayes factor (<italic>BF</italic>) of the non-monotonic models (Mexican-hat) over monotonic model (Gaussian) based on <italic>AIC</italic><sup><xref ref-type="bibr" rid="c39">39</xref></sup> and <italic>BIC</italic><sup><xref ref-type="bibr" rid="c40">40</xref></sup> approximation, respectively, as follows;
<disp-formula>
<graphic xlink:href="609781v1_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>AIC<sub>G</sub></italic>and <italic>BIC<sub>G</sub></italic> are for the Gaussian model and <italic>AIC<sub>M</sub></italic>and <italic>BIC<sub>M</sub></italic> are for Mexican-hat models.</p>
</sec>
</sec>
<sec id="s4e">
<title>Computational models of the center-surround inhibition in expectation</title>
<p>Prior to initiating model fitting, for both OD and SFD tasks, we first transformed the negative values of thresholds during baseline and main experiments into smooth population response profiles using linear interpolation, respectively. Subsequently, we fitted two candidate models, namely Tuning sharpening model and Tuning shift model (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>), to these population response profiles for each participant. In both models, the idealized tuning function for each channel was defined by the Gaussian functions:
<disp-formula>
<graphic xlink:href="609781v1_ueqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>x</italic> is the grating orientation, <italic>A</italic> is the amplitude of tuning function, <italic>x0</italic> is the location, and 1.2 is the width. Six and five tuning channels were hypothesized for data in baseline and main experiments, respectively. For the Tuning sharpening model, the tuning width of each channel’s tuning function is parameterized by 1.2, while all tuning functions are evenly distributed with 10° spacing on the x-axis and the areas under the curves (response energy) are identical. Conversely, for the Tuning shift model, the location of each channel’s tuning function is parameterized by <italic>x0</italic>, while they all share the same tuning amplitude and width. The parameter <italic>x0</italic> was constrained within ± 5° of the grating orientation limits, ranging from 15° to 75° during the baseline experiment, 15° to 65° and 25° to 75° for expected 20° and expected 70° conditions, respectively, during the main experiment. The parameters σ was set within the range of 0.01 to 200 to ensure the comparable goodness of fit. For both models, parameters were varied to obtain the minimal sum of squared errors between the population response profile and the model prediction, which is the sum of all channels’ tuning responses. To statistically compare the two models, for both orientation and SF discrimination tasks, we computed the root mean squared deviation (RMSD)<sup><xref ref-type="bibr" rid="c41">41</xref></sup> of the two fitted models for each participant during baseline and main experiments:
<disp-formula>
<graphic xlink:href="609781v1_ueqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>SSE</italic> is the sum of squared errors. <italic>N</italic> is the number of data points (i.e., 51 and 61), and <italic>K</italic> is the number of model parameters.</p>
</sec>
<sec id="s4f">
<title>Orientation adjustment experiment</title>
<sec id="s4f1">
<title>Experimental design</title>
<p>The protocol of orientation adjustment experiment was similar with that of the profile experiment, except for two aspects. First, there were 4 possible (20°, 40°, 50°, and 70°) orientations for the first grating: 20°/70° (Δ0° deviated from the expected orientation) and 40°/50° ( Δ 20°/Δ30° deviated from the expected orientation). Second, in both baseline and main experiments, the second grating was set as a random orientation within the range of 0° to 90°, and participants were required to rotate the orientation of the second grating to match the first (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>). Each participant completed 8 blocks of 48 trials in the baseline experiment, and 16 blocks of 48 trials in the main experiment.</p>
</sec>
<sec id="s4f2">
<title>Modeling response error</title>
<p>Response error was measured as the angular difference between the orientation of the first grating and the adjusted orientation of the second grating, such that errors ranged from 0° (a perfect response) to ± 90° (a maximally imprecise response). To evaluate performance, we categorized the response errors for each participant according to different conditions and modeled their distributions as a three-component mixture model<sup><xref ref-type="bibr" rid="c42">42</xref></sup>. This model comprised a von Mises distribution (0) corresponding to trials in which the grating orientation was encoded and a uniform distribution (<italic>p<sub>g</sub></italic>) accounting for the probability of random guessing without encoding<sup><xref ref-type="bibr" rid="c43">43</xref></sup>:
<disp-formula>
<graphic xlink:href="609781v1_ueqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where θ is the adjusted orientation value, Ø denotes the Von Mises distribution with μ and shape parameter <italic>k</italic>, and <italic>p<sub>g</sub></italic> represents a uniform distribution. Specifically, the von Mises probability density function for the angle x is given by:
<disp-formula>
<graphic xlink:href="609781v1_ueqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where I<sub>0</sub>(k) is the modified Bessel function of the first kind of order 0, with this scaling constant chosen so that the distribution suns to unity:
<disp-formula>
<graphic xlink:href="609781v1_ueqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here, we obtained maximum likelihood estimates for 3 parameters: (1) the systematic shift of von Mises distribution (<italic>mu</italic>), which reflects distribution shift away from the target grating orientation; (2) the dispersion of the von Mises distribution <inline-formula><inline-graphic xlink:href="609781v1_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which reflects response precision or resolution of representation; and (3) the height of the uniform distribution (<italic>g</italic>), which reflects the probability of guessing.</p>
</sec>
</sec>
<sec id="s4g">
<title>Orientation discrimination experiment</title>
<sec id="s4g1">
<title>Experimental design</title>
<p>The protocol of orientation discrimination experiment was very similar with that of the orientation adjustment experiment, except for two aspects (<xref rid="fig7" ref-type="fig">Fig. 7a</xref>). First, there were 3 possible (20°, 45°, and 70°) orientations for the first grating: 20°/70° (Δ0° deviated from the expected orientation) and 45° (Δ25° deviated from the expected orientation). Second, in both baseline and main experiments, the second grating were 1°, 3°, 5°, 7°, and 9° deviated from the first grating, either clockwise or counterclockwise. Participants were asked to make a 2AFC judgment of the orientation of the second grating relative to the first, either clockwise or anticlockwise. Each participant completed 10 blocks of 120 trials in the baseline experiment, and 20 blocks of 160 trials in the main experiment.</p>
</sec>
<sec id="s4g2">
<title>Data fitting and analysis</title>
<p>We first constructed a psychometric function for each condition shown in <xref rid="fig7" ref-type="fig">Fig. 7</xref>. We plotted the percentage of trials in which participants indicated the orientation of second grating that was anticlockwise or clockwise to the first for 20° (Baseline 20° and Expect 20°) and 70° (Baseline 70° and Expect 70°) conditions, respectively, as a function of the real orientation difference between two gratings. For each participant and each condition, the psychometric values at ten orientation differences were fitted to a cumulative Gaussian using Bayesian inference, implemented in the <italic>Psignifit toolbox</italic> for Matlab (Version 4)<sup><xref ref-type="bibr" rid="c128">128</xref></sup>, and we interpolated the data to find the slope (orientation uncertainty) and PSE (point of subjective equality, which is the shift here) as an index for Tuning sharpening and Tuning shift models, respectively.</p>
</sec>
</sec>
<sec id="s4h">
<title>Artificial neural networks for the center-surround inhibition in expectation</title>
<p>We trained a deep predictive coding neural network (DPCNN), modified from Predify<sup><xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c45">45</xref></sup> to perform both the OD and SFD tasks. Relative to the reference, on the OD task, DPCNN was trained to classify whether the target was tilted clockwise or counterclockwise; whereas on the SFD task, it was trained to classify whether the target had lower or higher spatial frequency. For both tasks, the DPCNN consisted of 6 feedforward encoding layers (<italic>e1-e6</italic>), 5 generative feedback decoding layers (<italic>d1-d5</italic>), and 3 fully connected (fc) layers (<xref rid="fig8" ref-type="fig">Fig. 8a</xref>). The reconstruction error (<italic>E1-E5</italic>) is computed and used for the proposed predictive coding updates<sup><xref ref-type="bibr" rid="c31">31</xref></sup>, denoting by <italic>P.C.</italic> loops. Note that the updating is only applied to <italic>e1-e5</italic> and for the last layer <italic>e6</italic>, there is no feedback. Before the layer <italic>e0,</italic> we obtained the pixel difference between the target and reference images, which was then superimposed on the channels of the reference image. This superimposed image was set as the input of the network and remained constant over timesteps. Besides, we used a feedforward encoding layer (i.e., <italic>e0</italic>) to match the number of the channels between superimposed feature maps and the pre-trained DPCNN. Additionally, to further determine the contribution of predictive coding framework to center-surround inhibition in expectation, we also trained the same network but removing feedback predictive coding iterations (a standard feedforward CNN, i.e., a modified network of AlexNet<sup><xref ref-type="bibr" rid="c46">46</xref></sup>). Note that all these architects were built to mimic our hypothesis of the visual pathway involved in expectation<sup><xref ref-type="bibr" rid="c5">5</xref>–<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c61">61</xref></sup> and could learn a lower-dimensional latent representation of a high-dimensional input space<sup><xref ref-type="bibr" rid="c129">129</xref></sup>. During the training, the last layer was trained to capture the difference between the target and reference and finally obtain the classification by softmax, to model decision making in our 2AFC paradigm (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>), in which participants were asked to make a 2AFC judgment of the orientation (either clockwise or anticlockwise) or the spatial frequency (either lower or higher) of the second grating (target) relative to the first (reference) in OD and SFD tasks, respectively. Moreover, for the OD task, the orientation difference between the target and reference in the network was set to 5°; for the SFD task, the spatial wavelength difference between them was set to 0.5.</p>
<p>For each task, both the DPCNN and CNN were independently and randomly trained 12 times. For each time, the trained orientation was chosen randomly from 0° to 180°; the 9 test gratings were 0°, ± 10°, ± 20°, ± 30°, and ± 40° deviated (clockwise and counterclockwise) from the trained orientation. All grating stimuli (phase: random) were centered on 227 × 227-pixels images with gray background. To improve the robustness of our model, we trained the network on all combinations of several parameters: contrast (0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, and 0.8), SD of the Gaussian additive noise (5, 25, and 45), and spatial wavelength (5, 10, 15, 20, 25, 30, 40, 50, 60, and 80 pixels) for the OD task; contrast ranging from 0.1 through 0.8 with a step size of 0.05 and SD of the Gaussian additive noise ranging from 3 through 60 with a step size of 3 for the SFD task. For each training, there were thus a total of 840 images; 600 images were the training set and the other 240 images were the test set. For both OD and SFD tasks, during the training set, there were 480 images for the expected orientation (Δ0°) and 30 images for each of unexpected orientations (Δ10°, Δ20°, Δ30°, and Δ40°); during the test set, there were 48 images for each of distances (Δ0°- Δ40°). For each distance, the training effect was defined as the accuracy difference (<italic>ACC<sub>difference</sub></italic>) between the pre- (ACC<sub>baseline</sub>) and post- (<italic>ACC<sub>trained</sub></italic>) training.</p>
</sec>
</sec>

</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We acknowledge the participants for their contribution to this study. This work was supported by National Natural Science Foundation of China (32271099), Research Center for Brain Cognition and Human Development of Guangdong Province, and Striving for the First-Class, Improving Weak Links and Highlighting Features (SIH) Key Discipline for Psychology in South China Normal University.</p>
</ack>
<sec id="d1e2026" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>Author contributions</title>
<p>Conceptualization, X.Z.; Methodology, L.H., S.S., Y.S., R.Z., F.L., and X.Z.; Formal Analysis, L.H., S.S., and Y.S.; Investigation, L.H., S.S., Y.S., and S.O.; Writing – Original Draft, X.Z.; Writing – Review and Editing, L.H., S.S., Y.S., F.L., and X.Z.; Supervision, X.Z.</p>
</sec>
</sec>
<sec id="suppd1e2026" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Supplemental information</label>
<media xlink:href="supplements/609781_file02.docx"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bar</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>Visual objects in context</article-title>. <source>Nat. Rev. Neurosci</source>. <volume>5</volume>, <fpage>617</fpage>–<lpage>629</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bar</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>The proactive brain: memory for predictions</article-title>. <source>Philos. Trans. R. Soc. Lond. B Biol. Sci</source>. <volume>364</volume>, <fpage>1235</fpage>–<lpage>1243</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teufel</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Fletcher</surname>, <given-names>P. C.</given-names></string-name></person-group> <article-title>Forms of prediction in the nervous system</article-title>. <source>Nat. Rev. Neurosci</source>. <volume>21</volume>, <fpage>231</fpage>–<lpage>242</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Watabe-Uchida</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Eshel</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Uchida</surname>, <given-names>N.</given-names></string-name></person-group> <article-title>Neural circuitry of reward prediction error. <italic>Ann</italic></article-title>. <source>Rev. Neurosci</source>. <volume>40</volume>, <fpage>373</fpage>–<lpage>394</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name>, <string-name><surname>Heilbron</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name></person-group> <article-title>How do expectations shape perception?</article-title> <source>Trends Cogn. Sci</source>. <volume>22</volume>, <fpage>764</fpage>–<lpage>779</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Press</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Yon</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>The perceptual prediction paradox</article-title>. <source>Trends Cogn. Sci</source>. <volume>24</volume>, <fpage>13</fpage>–<lpage>24</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Jehee</surname>, <given-names>J. F.</given-names></string-name> &amp; <string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> <article-title>Less is more: expectation sharpens representations in the primary visual cortex</article-title>. <source>Neuron</source> <volume>75</volume>, <fpage>265</fpage>–<lpage>270</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> <article-title>Expectation in perceptual decision making: neural and computational mechanisms</article-title>. <source>Nat. Rev. Neurosci</source>. <volume>15</volume>, <fpage>745</fpage>–<lpage>756</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname>, <given-names>T. S.</given-names></string-name> &amp; <string-name><surname>Mumford</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Hierarchical Bayesian inference in the visual cortex</article-title>. <source>J. Opt. Soc. Am. A Opt. Image Sci. Vis</source>. <volume>20</volume>, <fpage>1434</fpage>–<lpage>1448</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bell</surname>, <given-names>A. H.</given-names></string-name>, <string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Morin</surname>, <given-names>E. L.</given-names></string-name>, <string-name><surname>Malecek</surname>, <given-names>N. J.</given-names></string-name> &amp; <string-name><surname>Ungerleider</surname>, <given-names>L. G.</given-names></string-name></person-group> <article-title>Encoding of stimulus probability in macaque inferior temporal cortex</article-title>. <source>Curr. Biol</source>. <volume>26</volume>, <fpage>2280</fpage>–<lpage>2290</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fiser</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mahringer</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Oyibo</surname>, <given-names>H. K.</given-names></string-name>, <string-name><surname>Petersen</surname>, <given-names>A. V.</given-names></string-name>, <string-name><surname>Leinweber</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Keller</surname>, <given-names>G. B.</given-names></string-name></person-group> <article-title>Experience-dependent spatial expectations in mouse visual cortex</article-title>. <source>Nat. Neurosci</source>. <volume>19</volume>, <fpage>1658</fpage>–<lpage>1664</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kaposvari</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kumar</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Vogels</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>Statistical learning signals in macaque inferior temporal cortex</article-title>. <source>Cereb. Cortex</source> <volume>28</volume>, <fpage>250</fpage>–<lpage>266</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meyer</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Olson</surname>, <given-names>C. R.</given-names></string-name></person-group> <article-title>Statistical learning of visual transitions in monkey inferotemporal cortex</article-title>. <source>Proc. Natl Acad. Sci. USA</source> <volume>108</volume>, <fpage>19401</fpage>–<lpage>19406</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwiedrzik</surname>, <given-names>C. M.</given-names></string-name> &amp; <string-name><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name></person-group> <article-title>High-level prediction signals in a low-level area of the macaque face-processing hierarchy</article-title>. <source>Neuron</source> <volume>96</volume>, <fpage>89</fpage>–<lpage>97</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aitken</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name></person-group> <article-title>Prior expectations of motion direction modulate early sensory processing</article-title>. <source>J. Neurosci</source>. <volume>40</volume>, <fpage>6389</fpage>–<lpage>6397</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mostert</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> <article-title>Prior expectations induce prestimulus sensory templates</article-title>. <source>Proc. Natl Acad. Sci. USA</source> <volume>114</volume>, <fpage>10473</fpage>–<lpage>10478</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Todorovic</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>van Ede</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> <article-title>Prior expectation mediates neural adaptation to repeated sounds in the auditory cortex: an MEG study</article-title>. <source>J. Neurosci</source>. <volume>31</volume>, <fpage>9118</fpage>–<lpage>9123</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sedley</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Gander</surname>, <given-names>P. E.</given-names></string-name>, <string-name><surname>Kumar</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kovach</surname>, <given-names>C. K.</given-names></string-name>, <string-name><surname>Oya</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Kawasaki</surname>, <given-names>H.</given-names></string-name>, <etal>…</etal> <string-name><surname>Griffiths</surname>, <given-names>T. D.</given-names></string-name></person-group> <article-title>Neural signatures of perceptual inference</article-title>. <source>eLife</source> <volume>5</volume>, <elocation-id>e11476</elocation-id> (<year>2016</year>). <pub-id pub-id-type="doi">10.7554/eLife.11476</pub-id></mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wacongne</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Labyt</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Van Wassenhove</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Bekinschtein</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Naccache</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Evidence for a hierarchy of predictions and prediction errors in human cortex</article-title>. <source>Proc. Natl Acad. Sci. USA</source> <volume>108</volume>, <fpage>20754</fpage>–<lpage>20759</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alink</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schwiedrzik</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Kohler</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Singer</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Muckli</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>Stimulus predictability reduces responses in primary visual cortex</article-title>. <source>J. Neurosci</source>. <volume>30</volume>, <fpage>2960</fpage>–<lpage>2966</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Trittschuh</surname>, <given-names>E. H.</given-names></string-name>, <string-name><surname>Monti</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Mesulam</surname>, <given-names>M. M.</given-names></string-name> &amp; <string-name><surname>Egner</surname>, <given-names>T.</given-names></string-name></person-group> <article-title>Neural repetition suppression reflects fulfilled perceptual expectations</article-title>. <source>Nat. Neurosci</source>. <volume>11</volume>, <fpage>1004</fpage>–<lpage>1006</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yon</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Gilbert</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name> &amp; <string-name><surname>Press</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>Action sharpens sensory representations of expected outcomes</article-title>. <source>Nat. Commun</source>. <volume>9</volume>, <fpage>4288</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Richter</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Heilbron</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> <article-title>Dampened sensory representations for expected input across the ventral visual stream</article-title>. <source>Oxford Open Neurosci</source>. <volume>1</volume>, <fpage>kvac013</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kumar</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kaposvari</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Vogels</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>Encoding of predictable and unpredictable stimuli by inferior temporal cortical neurons</article-title>. <source>J. Cogn. Neurosci</source>. <volume>29</volume>, <fpage>1445</fpage>–<lpage>54</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blakemore</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name> &amp; <string-name><surname>Frith</surname>, <given-names>C. D.</given-names></string-name></person-group> <article-title>Central cancellation of self-produced tickle sensation</article-title>. <source>Nat. Neurosci</source>. <volume>1</volume>, <fpage>635</fpage>–<lpage>40</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blank</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Davis</surname>, <given-names>M. H.</given-names></string-name></person-group> <article-title>Prediction errors but not sharpened signals simulate multivoxel fMRI patterns during speech perception</article-title>. <source>PLoS Biol</source>. <volume>14</volume>, <fpage>1</fpage>–<lpage>32</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Han</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Mostert</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> <article-title>Predictable tones elicit stimulus-specific suppression of evoked activity in auditory cortex</article-title>. <source>NeuroImage</source> <volume>200</volume>, <fpage>242</fpage>–<lpage>9</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Richter</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ekman</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> <article-title>Suppressed sensory response to predictable object stimuli throughout the ventral visual stream</article-title>. <source>J. Neurosci</source>. <volume>38</volume>, <fpage>7452</fpage>–<lpage>7461</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feldman</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name></person-group> <article-title>Attention, uncertainty, and free-energy</article-title>. <source>Front. Hum. Neurosci</source>. <volume>4</volume>, <fpage>215</fpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name></person-group> <article-title>A theory of cortical responses. Philos</article-title>. <source>Trans. R. Soc. Lond. B Biol. Sci</source>. <volume>360</volume>, <fpage>815</fpage>–<lpage>836</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rao</surname>, <given-names>R. P. N.</given-names></string-name> &amp; <string-name><surname>Ballard</surname>, <given-names>D. H.</given-names></string-name></person-group> <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nat. Neurosci</source>. <volume>2</volume>, <fpage>79</fpage>–<lpage>87</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Koechlin</surname>, <given-names>E.</given-names></string-name></person-group> <article-title>A neural representation of prior information during perceptual inference</article-title>. <source>Neuron</source> <volume>59</volume>, <fpage>336</fpage>–<lpage>347</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yuille</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Kersten</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Vision as Bayesian inference: analysis by synthesis?</article-title> <source>Trends Cogn. Sci</source>. <volume>10</volume>, <fpage>301</fpage>–<lpage>308</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Watson</surname>, <given-names>A. B.</given-names></string-name> &amp; <string-name><surname>Pelli</surname>, <given-names>D. G.</given-names></string-name></person-group> <article-title>Quest: A Bayesian adaptive psychometric method</article-title>. <source>Percept. Psychophys</source>. <volume>33</volume>, <fpage>113</fpage>–<lpage>120</lpage> (<year>1983</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Mo</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Fang</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name></person-group> <article-title>Profiles of visual perceptual learning in feature space</article-title>. <source>iScience</source> <volume>27</volume>, <fpage>109128</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name></person-group> <article-title>Awareness-dependent normalization framework of visual bottom-up attention</article-title>. <source>J. Neurosci</source>. <volume>41</volume>, <fpage>9593</fpage>–<lpage>9607</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Akaike</surname>, <given-names>H.</given-names></string-name></person-group> <article-title>Maximum likelihood identification of Gaussian autoregressive moving average models</article-title>. <source>Biometrika</source> <volume>60</volume>, <fpage>255</fpage>–<lpage>265</lpage> (<year>1973</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwarz</surname>, <given-names>G.</given-names></string-name></person-group> <article-title>Estimating the dimension of a model</article-title>. <source>Ann. Stat</source>. <volume>6</volume>, <fpage>461</fpage>–<lpage>464</lpage> (<year>1978</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burnham</surname>, <given-names>K. P.</given-names></string-name> &amp; <string-name><surname>Anderson</surname>, <given-names>D. R.</given-names></string-name></person-group> <article-title>A practical information-theoretic approach</article-title>. <source>Model selection and multimodel inference</source> <volume>2</volume>, (<year>2002</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wagenmakers</surname>, <given-names>E. J.</given-names></string-name></person-group> <article-title>A practical solution to the pervasive problems of p values</article-title>. <source>Psychon. Bull. Rev</source>. <volume>14</volume>, <fpage>779</fpage>–<lpage>804</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pitt</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Myung</surname>, <given-names>I. J.</given-names></string-name> &amp; <string-name><surname>Zhang</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Toward a method of selecting among computational models of cognition</article-title>. <source>Psychol. Rev</source>. <volume>109</volume>, <fpage>472</fpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Suchow</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Brady</surname>, <given-names>T. F.</given-names></string-name>, <string-name><surname>Fougnie</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Alvarez</surname>, <given-names>G. A.</given-names></string-name></person-group> <article-title>Modeling visual working memory with the MemToolbox</article-title>. <source>J. Vision</source> <volume>13</volume>, <fpage>9</fpage>–<lpage>9</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Luck</surname>, <given-names>S. J.</given-names></string-name></person-group> <article-title>Discrete fixed-resolution representations in visual working memory</article-title>. <source>Nature</source> <volume>453</volume>, <fpage>233</fpage>–<lpage>235</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Choksi</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Mozafari</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Biggs O’May</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Ador</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Alamia</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>VanRullen</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>Predify: Augmenting deep neural networks with brain-inspired predictive coding dynamics</article-title>. <source>Adv. Neural Inf. Process. Syst</source>. <volume>34</volume>, <fpage>14069</fpage>–<lpage>14083</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pang</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>O’May</surname>, <given-names>C. B.</given-names></string-name>, <string-name><surname>Choksi</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>VanRullen</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>Predictive coding feedback results in perceived illusory contours in a recurrent neural network</article-title>. <source>Neural Networks</source> <volume>144</volume>, <fpage>164</fpage>–<lpage>175</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name></person-group> <article-title>Imagenet classification with deep convolutional neural networks</article-title>. <source>Adv. Neural Inf. Process. Syst</source>. <volume>25</volume>, <fpage>1097</fpage>–<lpage>1105</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cheadle</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Egner</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Wyart</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>Feature expectation heightens visual sensitivity during fine orientation discrimination</article-title>. <source>J. Vision</source> <volume>15</volume>, <fpage>14</fpage>–<lpage>14</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esterman</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Yantis</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Perceptual expectation evokes category-selective cortical activity</article-title>. <source>Cereb. Cortex</source> <volume>20</volume>, <fpage>1245</fpage>–<lpage>1253</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mareschal</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Calder</surname>, <given-names>A. J.</given-names></string-name> &amp; <string-name><surname>Clifford</surname>, <given-names>C. W.</given-names></string-name></person-group> <article-title>Humans have an expectation that gaze is directed toward them</article-title>. <source>Curr. Biol</source>. <volume>23</volume>, <fpage>717</fpage>–<lpage>721</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McAuley</surname>, <given-names>J. D.</given-names></string-name> &amp; <string-name><surname>Kidd</surname>, <given-names>G. R.</given-names></string-name></person-group> <article-title>Effect of deviations from temporal expectations on tempo discrimination of isochronous tone sequences</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>24</volume>, <fpage>1786</fpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stein</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name></person-group> <article-title>Content-specific expectations enhance stimulus detectability by increasing perceptual sensitivity</article-title>. <source>J. Exp. Psychol. Gen</source>. <volume>144</volume>, <fpage>1089</fpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stocker</surname>, <given-names>A. A.</given-names></string-name> &amp; <string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name></person-group> <article-title>Noise characteristics and prior expectations in human visual speed perception</article-title>. <source>Nat. Neurosci</source>. <volume>9</volume>, <fpage>578</fpage>–<lpage>585</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fang</surname>, <given-names>M. W.</given-names></string-name> &amp; <string-name><surname>Liu</surname>, <given-names>T.</given-names></string-name></person-group> <article-title>The profile of attentional modulation to visual features</article-title>. <source>J. Vis</source>. <volume>19</volume>, <fpage>13</fpage>–<lpage>13</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Fang</surname>, <given-names>M. W.</given-names></string-name> &amp; <string-name><surname>Saba-Sadiya</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Adaptive visual selection in feature space</article-title>. <source>Psychon. Bull. Rev</source>. <volume>30</volume>, <fpage>994</fpage>–<lpage>1003</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tombu</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Tsotsos</surname>, <given-names>J. K.</given-names></string-name></person-group> <article-title>Attending to orientation results in an inhibitory surround in orientation space</article-title>. <source>Percept. Psychophys</source>. <volume>70</volume>, <fpage>30</fpage>–<lpage>35</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yoo</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Tsotsos</surname>, <given-names>J. K.</given-names></string-name> &amp; <string-name><surname>Fallah</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>The attentional suppressive surround: eccentricity, location-based and feature-based effects and interactions</article-title>. <source>Front. Neurosci</source>. <volume>12</volume>, <fpage>710</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsotsos</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Culhane</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Cutzu</surname>, <given-names>F.</given-names></string-name></person-group> <article-title>From theoretical foundations to a hierarchical circuit for selective attention</article-title>. <source>Visual Attention and Cortical Circuits</source> <fpage>285</fpage>–<lpage>306</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsotsos</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Culhane</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Wai</surname>, <given-names>W. Y. K.</given-names></string-name>, <string-name><surname>Lai</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Davis</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Nuflo</surname>, <given-names>F.</given-names></string-name></person-group> <article-title>Modeling visual attention via selective tuning</article-title>. <source>Artif. Intell</source>. <volume>78</volume>, <fpage>507</fpage>–<lpage>545</lpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsotsos</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Rodriguez-Sanchez</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Rothenstein</surname>, <given-names>A. L.</given-names></string-name> &amp; <string-name><surname>Simine</surname>, <given-names>E.</given-names></string-name></person-group> <article-title>The different stages of visual recognition need different attentional binding strategies</article-title>. <source>Brain Res</source>. <volume>1225</volume>, <fpage>119</fpage>–<lpage>132</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rungratsameetaweemana</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Serences</surname>, <given-names>J. T.</given-names></string-name></person-group> <article-title>Dissociating the impact of attention and expectation on early sensory processing</article-title>. <source>Curr. Opin. Psychol</source>. <volume>29</volume>, <fpage>181</fpage>–<lpage>186</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Egner</surname>, <given-names>T.</given-names></string-name></person-group> <article-title>Expectation (and attention) in visual cognition</article-title>. <source>Trends Cogn. Sci</source>. <volume>13</volume>, <fpage>403</fpage>–<lpage>409</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gordon</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Tsuchiya</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Koenig-Robert</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Hohwy</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Expectation and attention increase the integration of top-down and bottom-up signals in perception through different pathways</article-title>. <source>PLoS Biol</source>. <volume>17</volume>, <fpage>e3000233</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tal-Perry</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Yuval-Greenberg</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>The spatiotemporal link of temporal expectations: contextual temporal expectation is independent of spatial attention</article-title>. <source>J. Neurosci</source>. <volume>42</volume>, <fpage>2516</fpage>–<lpage>2523</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilsch</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mercier</surname>, <given-names>M. R.</given-names></string-name>, <string-name><surname>Obleser</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Schroeder</surname>, <given-names>C. E.</given-names></string-name> &amp; <string-name><surname>Haegens</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Spatial attention and temporal expectation exert differential effects on visual and auditory discrimination</article-title>. <source>J. Cogn. Neurosci</source>. <volume>32</volume>, <fpage>1562</fpage>–<lpage>1576</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zuanazzi</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Noppeney</surname>, <given-names>U.</given-names></string-name></person-group> <article-title>Distinct neural mechanisms of spatial attention and expectation guide perceptual inference in a multisensory world</article-title>. <source>J. Neurosci</source>. <volume>39</volume>, <fpage>2301</fpage>–<lpage>2312</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schall</surname>, <given-names>J. D.</given-names></string-name> &amp; <string-name><surname>Hanes</surname>, <given-names>D. P.</given-names></string-name></person-group> <article-title>Neural basis of saccade target selection in frontal eye field during visual search</article-title>. <source>Nature</source> <volume>366</volume>, <fpage>467</fpage>–<lpage>469</lpage> (<year>1993</year>).</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hopf</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Boehler</surname>, <given-names>C. N.</given-names></string-name>, <string-name><surname>Luck</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Tsotsos</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Heinze</surname>, <given-names>H. J.</given-names></string-name> &amp; <string-name><surname>Schoenfeld</surname>, <given-names>M. A.</given-names></string-name></person-group> <article-title>Direct neurophysiological evidence for spatial suppression surrounding the focus of attention in vision</article-title>. <source>Proc. Natl Acad. Sci. USA</source> <volume>103</volume>, <fpage>1053</fpage>–<lpage>1058</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moran</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>Selective attention gates visual processing in the extrastriate cortex</article-title>. <source>Science</source> <volume>229</volume>, <fpage>782</fpage>–<lpage>784</lpage> (<year>1985</year>).</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schall</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Sato</surname>, <given-names>T. R.</given-names></string-name>, <string-name><surname>Thompson</surname>, <given-names>K. G.</given-names></string-name>, <string-name><surname>Vaughn</surname>, <given-names>A. A.</given-names></string-name> &amp; <string-name><surname>Juan</surname>, <given-names>C. H.</given-names></string-name></person-group> <article-title>Effects of search efficiency on surround suppression during visual selection in frontal eye field</article-title>. <source>J. Neurophysiol</source>. <volume>91</volume>, <fpage>2765</fpage>–<lpage>2769</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mounts</surname>, <given-names>J. R.</given-names></string-name></person-group> <article-title>Evidence for suppressive mechanisms in attentional selection: Feature singletons produce inhibitory surrounds</article-title>. <source>Percept. Psychophys</source>. <volume>62</volume>, <fpage>969</fpage>–<lpage>983</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Müller</surname>, <given-names>N. G.</given-names></string-name> &amp; <string-name><surname>Kleinschmidt</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>The attentional ‘spotlight’s’ penumbra: Center–surround modulation in striate cortex</article-title>. <source>Neuroreport</source> <volume>15</volume>, <fpage>977</fpage>–<lpage>980</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Müller</surname>, <given-names>N. G.</given-names></string-name>, <string-name><surname>Mollenhauer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rosler</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Kleinschmidt</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>The attentional field has a Mexican hat distribution</article-title>. <source>Vision Res</source>. <volume>45</volume>, <fpage>1129</fpage>–<lpage>1137</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boehler</surname>, <given-names>C. N.</given-names></string-name>, <string-name><surname>Tsotsos</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Schoenfeld</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Heinze</surname>, <given-names>H. J.</given-names></string-name> &amp; <string-name><surname>Hopf</surname>, <given-names>J. M.</given-names></string-name></person-group> <article-title>The center-surround profile of the focus of attention arises from recurrent processing in visual cortex</article-title>. <source>Cereb. Cortex</source> <volume>19</volume>, <fpage>982</fpage>–<lpage>991</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boehler</surname>, <given-names>C. N.</given-names></string-name>, <string-name><surname>Tsotsos</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Schoenfeld</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Heinze</surname>, <given-names>H. J.</given-names></string-name> &amp; <string-name><surname>Hopf</surname>, <given-names>J. M.</given-names></string-name></person-group> <article-title>Neural mechanisms of surround attenuation and distractor competition in visual search</article-title>. <source>J. Neurosci</source>. <volume>31</volume>, <fpage>5213</fpage>–<lpage>5224</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fang</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Becker</surname>, <given-names>M. W.</given-names></string-name> &amp; <string-name><surname>Liu</surname>, <given-names>T.</given-names></string-name></person-group> <article-title>Attention to colors induces surround suppression at category boundaries</article-title>. <source>Sci. Rep</source>. <volume>9</volume>, <fpage>1443</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bartsch</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>Loewe</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Merkel</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Heinze</surname>, <given-names>H. J.</given-names></string-name>, <string-name><surname>Schoenfeld</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Tsotsos</surname>, <given-names>J. K.</given-names></string-name> &amp; <string-name><surname>Hopf</surname>, <given-names>J. M.</given-names></string-name></person-group> <article-title>Attention to color sharpens neural population tuning via feedback processing in the human visual cortex hierarchy</article-title>. <source>J. Neurosci</source>. <volume>37</volume>, <fpage>10346</fpage>–<lpage>10357</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Störmer</surname>, <given-names>V. S.</given-names></string-name> &amp; <string-name><surname>Alvarez</surname>, <given-names>G. A.</given-names></string-name></person-group> <article-title>Feature-based attention elicits surround suppression in feature space</article-title>. <source>Curr. Biol</source>. <volume>24</volume>, <fpage>1985</fpage>–<lpage>1988</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Loach</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Frischen</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bruce</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Tsotsos</surname>, <given-names>J. K.</given-names></string-name></person-group> <article-title>An attentional mechanism for selecting appropriate actions afforded by graspable objects</article-title>. <source>Psychol. Sci</source>. <volume>19</volume>, <fpage>1253</fpage>–<lpage>1257</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kiyonaga</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Egner</surname>, <given-names>T.</given-names></string-name></person-group> <article-title>Center-surround inhibition in working memory</article-title>. <source>Curr. Biol</source>. <volume>26</volume>, <fpage>64</fpage>–<lpage>68</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shi</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Gao</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Zhang</surname>, <given-names>Q.</given-names></string-name></person-group> <article-title>The extent of center-surround inhibition for colored items in working memory</article-title>. <source>Mem. Cogn</source>. <volume>49</volume>, <fpage>733</fpage>–<lpage>746</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c81"><label>81.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shi</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Qi</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Gao</surname>, <given-names>H.</given-names></string-name></person-group> <article-title>The ERP correlates of color-based center-surround inhibition in working memory</article-title>. <source>Int. J. Psychophysiol</source>. <volume>181</volume>, <fpage>160</fpage>–<lpage>169</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c82"><label>82.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eickhoff</surname>, <given-names>S. B.</given-names></string-name>, <string-name><surname>Yeo</surname>, <given-names>B. T.</given-names></string-name> &amp; <string-name><surname>Genon</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Imaging-based parcellations of the human brain</article-title>. <source>Nat. Rev. Neurosci</source>. <volume>19</volume>, <fpage>672</fpage>–<lpage>686</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c83"><label>83.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Coalson</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>E. C.</given-names></string-name>, <string-name><surname>Hacker</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Harwell</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Yacoub</surname>, <given-names>E.</given-names></string-name>, <etal>…</etal> <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name></person-group> <article-title>A multi-modal parcellation of human cerebral cortex</article-title>. <source>Nature</source> <volume>536</volume>, <fpage>171</fpage>–<lpage>178</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c84"><label>84.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mountcastle</surname>, <given-names>V. B.</given-names></string-name></person-group> <article-title>The columnar organization of the neocortex</article-title>. <source>Brain</source> <volume>120</volume>, <fpage>701</fpage>–<lpage>722</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c85"><label>85.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tanaka</surname>, <given-names>K.</given-names></string-name></person-group> <article-title>Columns for complex visual object features in the inferotemporal cortex: clustering of cells with similar but slightly different stimulus selectivities</article-title>. <source>Cereb. Cortex</source> <volume>13</volume>, <fpage>90</fpage>–<lpage>99</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c86"><label>86.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name> &amp; <string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name></person-group> <article-title>Parcellating cerebral cortex: how invasive animal studies inform noninvasive mapmaking in humans</article-title>. <source>Neuron</source> <volume>99</volume>, <fpage>640</fpage>–<lpage>663</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c87"><label>87.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Dumoulin</surname>, <given-names>S. O.</given-names></string-name> &amp; <string-name><surname>Brewer</surname>, <given-names>A. A.</given-names></string-name></person-group> <article-title>Visual field maps in human cortex</article-title>. <source>Neuron</source> <volume>56</volume>, <fpage>366</fpage>–<lpage>383</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c88"><label>88.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Connor</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Preddie</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name> &amp; <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name></person-group> <article-title>Spatial attention effects in macaque area V4</article-title>. <source>J. Neurosci</source>. <volume>17</volume>, <fpage>3201</fpage>–<lpage>3214</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c89"><label>89.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fox</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Birman</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Gardner</surname>, <given-names>J. L.</given-names></string-name></person-group> <article-title>Gain, not concomitant changes in spatial receptive field properties, improves task performance in a neural network attention model</article-title>. <source>eLife</source> <volume>12</volume>, <elocation-id>e78392</elocation-id> (<year>2023</year>). <pub-id pub-id-type="doi">10.7554/eLife.78392</pub-id></mixed-citation></ref>
<ref id="c90"><label>90.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klein</surname>, <given-names>B. P.</given-names></string-name>, <string-name><surname>Harvey</surname>, <given-names>B. M.</given-names></string-name> &amp; <string-name><surname>Dumoulin</surname>, <given-names>S. O.</given-names></string-name></person-group> <article-title>Attraction of position preference by spatial attention throughout human visual cortex</article-title>. <source>Neuron</source> <volume>84</volume>, <fpage>227</fpage>–<lpage>237</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c91"><label>91.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sheremata</surname>, <given-names>S. L.</given-names></string-name> &amp; <string-name><surname>Silver</surname>, <given-names>M. A.</given-names></string-name></person-group> <article-title>Hemisphere-dependent attentional modulation of human parietal visual field representations</article-title>. <source>J. Neurosci</source>. <volume>35</volume>, <fpage>508</fpage>–<lpage>517</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c92"><label>92.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Moore</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Smirnakis</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Tehovnik</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Siapas</surname>, <given-names>A. G.</given-names></string-name> &amp; <string-name><surname>Schiller</surname>, <given-names>P. H.</given-names></string-name></person-group> <article-title>Eye movements modulate visual receptive fields of V4 neurons</article-title>. <source>Neuron</source> <volume>29</volume>, <fpage>757</fpage>–<lpage>767</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c93"><label>93.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vo</surname>, <given-names>V. A.</given-names></string-name>, <string-name><surname>Sprague</surname>, <given-names>T. C.</given-names></string-name> &amp; <string-name><surname>Serences</surname>, <given-names>J. T.</given-names></string-name></person-group> <article-title>Spatial tuning shifts increase the discriminability and fidelity of population codes in visual cortex</article-title>. <source>J. Neurosci</source>. <volume>37</volume>, <fpage>3386</fpage>–<lpage>3401</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c94"><label>94.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Womelsdorf</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Anton-Erxleben</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Pieper</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Treue</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Dynamic shifts of visual receptive fields in cortical area MT by spatial attention</article-title>. <source>Nat. Neurosci</source>. <volume>9</volume>, <fpage>1156</fpage>–<lpage>1160</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c95"><label>95.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cukur</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Nishimoto</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Huth</surname>, <given-names>A. G.</given-names></string-name> &amp; <string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name></person-group> <article-title>Attention during natural vision warps semantic representation across the human brain</article-title>. <source>Nat. Neurosci</source>. <volume>16</volume>, <fpage>763</fpage>–<lpage>770</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c96"><label>96.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>David</surname>, <given-names>S. V.</given-names></string-name>, <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name>, <string-name><surname>Mazer</surname>, <given-names>J. A.</given-names></string-name> &amp; <string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name></person-group> <article-title>Attention to stimulus features shifts spectral tuning of V4 neurons during natural vision</article-title>. <source>Neuron</source> <volume>59</volume>, <fpage>509</fpage>–<lpage>521</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c97"><label>97.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Motter</surname>, <given-names>B. C.</given-names></string-name></person-group> <article-title>Neural correlates of feature selective memory and pop-out in extrastriate area V4</article-title>. <source>J. Neurosci</source>. <volume>14</volume>, <fpage>2190</fpage>–<lpage>2199</lpage> (<year>1994</year>).</mixed-citation></ref>
<ref id="c98"><label>98.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Es</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Theeuwes</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Knapen</surname>, <given-names>T.</given-names></string-name></person-group> <article-title>Spatial sampling in human visual cortex is modulated by both spatial and feature-based attention</article-title>. <source>eLife</source> <volume>7</volume>, <elocation-id>e36928</elocation-id> (<year>2018</year>). <pub-id pub-id-type="doi">10.7554/eLife.36928</pub-id></mixed-citation></ref>
<ref id="c99"><label>99.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carrasco</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ling</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Read</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Attention alters appearance</article-title>. <source>Nat. Neurosci</source>. <volume>7</volume>, <fpage>308</fpage>–<lpage>313</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c100"><label>100.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Compte</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Wang</surname>, <given-names>X. J.</given-names></string-name></person-group> <article-title>Tuning curve shift by attention modulation in cortical neurons: a computational study of its mechanisms</article-title>. <source>Cereb. Cortex</source> <volume>16</volume>, <fpage>761</fpage>–<lpage>778</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c101"><label>101.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname>, <given-names>D. K.</given-names></string-name>, <string-name><surname>Itti</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Koch</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Braun</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Attention activates winner-take-all competition among visual filters</article-title>. <source>Nat. neurosci</source>. <volume>2</volume>, <fpage>375</fpage>–<lpage>381</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c102"><label>102.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>C. H.</given-names></string-name> &amp; <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name></person-group> <article-title>A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information</article-title>. <source>J. Neurosci</source>. <volume>13</volume>, <fpage>4700</fpage>–<lpage>4719</lpage> (<year>1993</year>).</mixed-citation></ref>
<ref id="c103"><label>103.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rao</surname>, <given-names>R. P.</given-names></string-name> &amp; <string-name><surname>Ballard</surname>, <given-names>D. H.</given-names></string-name></person-group> <article-title>Dynamic model of visual recognition predicts neural response properties in the visual cortex</article-title>. <source>Neural Comput</source>. <volume>9</volume>, <fpage>721</fpage>–<lpage>763</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c104"><label>104.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hanson</surname>, <given-names>H. M.</given-names></string-name></person-group> <article-title>Effects of discrimination training on stimulus generalization</article-title>. <source>J. Exp. Psychol</source>. <volume>58</volume>, <fpage>321</fpage> (<year>1959</year>).</mixed-citation></ref>
<ref id="c105"><label>105.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schumacher</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>McCann</surname>, <given-names>M. K.</given-names></string-name>, <string-name><surname>Maximov</surname>, <given-names>K. J.</given-names></string-name> &amp; <string-name><surname>Fitzpatrick</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Selective enhancement of neural coding in V1 underlies fine-discrimination learning in tree shrew</article-title>. <source>Curr. Biol</source>. <volume>32</volume>, <fpage>3245</fpage>–<lpage>3260</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c106"><label>106.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spence</surname>, <given-names>K. W.</given-names></string-name></person-group> <article-title>The differential response in animals to stimuli varying within a single dimension</article-title>. <source>Psychol. Rev</source>. <volume>44</volume>, <fpage>430</fpage> (<year>1937</year>).</mixed-citation></ref>
<ref id="c107"><label>107.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lucci</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Berchicci</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Perri</surname>, <given-names>R. L.</given-names></string-name>, <string-name><surname>Spinelli</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Di Russo</surname>, <given-names>F.</given-names></string-name></person-group> <article-title>Effect of target probability on pre-stimulus brain activity</article-title>. <source>Neuroscience</source> <volume>322</volume>, <fpage>121</fpage>–<lpage>128</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c108"><label>108.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Ede</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Jensen</surname>, <given-names>O.</given-names></string-name> &amp; <string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name></person-group> <article-title>Tactile expectation modulates pre-stimulus β-band oscillations in human sensorimotor cortex</article-title>. <source>Neuroimage</source> <volume>51</volume>, <fpage>867</fpage>–<lpage>876</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c109"><label>109.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Foley</surname>, <given-names>N. C.</given-names></string-name>, <string-name><surname>Kelly</surname>, <given-names>S. P.</given-names></string-name>, <string-name><surname>Mhatre</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Lopes</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Gottlieb</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Parietal neurons encode expected gains in instrumental information</article-title>. <source>Proc. Natl Acad. Sci. USA</source> <volume>114</volume>, <fpage>E3315</fpage>–<lpage>E3323</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c110"><label>110.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Van Lieshout</surname>, <given-names>L. L.</given-names></string-name> &amp; <string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> <article-title>Local expectation violations result in global activity gain in primary visual cortex</article-title>. <source>Sci. Rep</source>. <volume>6</volume>, <fpage>37706</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c111"><label>111.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Voss</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ingram</surname>, <given-names>J. N.</given-names></string-name>, <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name> &amp; <string-name><surname>Haggard</surname>, <given-names>P.</given-names></string-name></person-group> <article-title>Mere expectation to move causes attenuation of sensory signals</article-title>. <source>PLoS One</source> <volume>3</volume>, <fpage>e2866</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c112"><label>112.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adrian</surname>, <given-names>E. D.</given-names></string-name> &amp; <string-name><surname>Matthews</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>The action of light on the eye: Part I. The discharge of impulses in the optic nerve and its relation to the electric changes in the retina</article-title>. <source>J. Physiol</source>. <volume>63</volume>, <fpage>378</fpage> (<year>1927</year>).</mixed-citation></ref>
<ref id="c113"><label>113.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barlow</surname>, <given-names>H. B.</given-names></string-name></person-group> <article-title>Single units and sensation: a neuron doctrine for perceptual psychology?</article-title> <source>Perception</source> <volume>1</volume>, <fpage>371</fpage>–<lpage>394</lpage> (<year>1972</year>).</mixed-citation></ref>
<ref id="c114"><label>114.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>David</surname>, <given-names>S. V.</given-names></string-name>, <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name>, <string-name><surname>Mazer</surname>, <given-names>J. A.</given-names></string-name> &amp; <string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name></person-group> <article-title>Attention to stimulus features shifts spectral tuning of V4 neurons during natural vision</article-title>. <source>Neuron</source> <volume>59</volume>, <fpage>509</fpage>–<lpage>521</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c115"><label>115.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Doetsch</surname>, <given-names>G. S.</given-names></string-name></person-group> <article-title>Patterns in the brain: Neuronal population coding in the somatosensory system</article-title>. <source>Physiol. Behav</source>. <volume>69</volume>, <fpage>187</fpage>–<lpage>201</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c116"><label>116.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Marr</surname>, <given-names>D.</given-names></string-name></person-group> <source>Vision</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>W. H. Freeman Co</publisher-name> (<year>1982</year>).</mixed-citation></ref>
<ref id="c117"><label>117.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bang</surname>, <given-names>J. W.</given-names></string-name> &amp; <string-name><surname>Rahnev</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Stimulus expectation alters decision criterion but not sensory signal in perceptual decision making</article-title>. <source>Sci. Rep</source>. <volume>7</volume>, <fpage>17072</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c118"><label>118.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gold</surname>, <given-names>J. I.</given-names></string-name> &amp; <string-name><surname>Stocker</surname>, <given-names>A. A.</given-names></string-name></person-group> <article-title>Visual decision-making in an uncertain and dynamic world</article-title>. <source>Annu. Rev. Vis. Sci</source>. <volume>3</volume>, <fpage>227</fpage>–<lpage>250</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c119"><label>119.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rungratsameetaweemana</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Itthipuripat</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Salazar</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Serences</surname>, <given-names>J. T.</given-names></string-name></person-group> <article-title>Expectations do not alter early sensory processing during perceptual decision-making</article-title>. <source>J. Neurosci</source>. <volume>38</volume>, <fpage>5632</fpage>–<lpage>5648</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c120"><label>120.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rahnev</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lau</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> <article-title>Prior expectation modulates the interaction between sensory and prefrontal regions in the human brain</article-title>. <source>J. Neurosci</source>. <volume>31</volume>, <fpage>10741</fpage>–<lpage>10748</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c121"><label>121.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Manenti</surname>, <given-names>G. L.</given-names></string-name>, <string-name><surname>Dizaji</surname>, <given-names>A. S.</given-names></string-name> &amp; <string-name><surname>Schwiedrzik</surname>, <given-names>C. M.</given-names></string-name></person-group> <article-title>Variability in training unlocks generalization in visual perceptual learning through invariant representations</article-title>. <source>Curr. Biol</source>. <volume>33</volume>, <fpage>817</fpage>–<lpage>826</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c122"><label>122.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wenliang</surname>, <given-names>L. K.</given-names></string-name> &amp; <string-name><surname>Seitz</surname>, <given-names>A. R.</given-names></string-name></person-group> <article-title>Deep neural networks for modeling visual perceptual learning</article-title>. <source>J. Neurosci</source>. <volume>38</volume>, <fpage>6028</fpage>–<lpage>6044</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c123"><label>123.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bashivan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kar</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> <article-title>Neural population control via deep image synthesis</article-title>. <source>Science</source> <volume>364</volume>, <fpage>eaav9436</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c124"><label>124.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Meng</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Zhou</surname>, <given-names>K.</given-names></string-name></person-group> <article-title>Emerged human-like facial expression representation in a deep convolutional neural network</article-title>. <source>Sci. Adv</source>. <volume>8</volume>, <fpage>eabj4383</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c125"><label>125.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boutin</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Franciosini</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chavane</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Ruffier</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Perrinet</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>Sparse deep predictive coding captures contour integration capabilities of the early visual system</article-title>. <source>PLoS Comput. Biol</source>. <volume>17</volume>, <fpage>e1008629</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c126"><label>126.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brainard</surname>, <given-names>D. H.</given-names></string-name> &amp; <string-name><surname>Vision</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>The psychophysics toolbox</article-title>. <source>Spat. Vis</source>. <volume>10</volume>, <fpage>433</fpage>–<lpage>436</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c127"><label>127.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raftery</surname>, <given-names>A. E.</given-names></string-name></person-group> <article-title>Bayes factors and BIC: Comment on “A critique of the Bayesian information criterion for model selection”</article-title>. <source>Sociol. Methods Res</source>. <volume>27</volume>, <fpage>411</fpage>–<lpage>427</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c128"><label>128.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schütt</surname>, <given-names>H. H.</given-names></string-name>, <string-name><surname>Harmeling</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Macke</surname>, <given-names>J. H.</given-names></string-name> &amp; <string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name></person-group> <article-title>Painfree and accurate Bayesian estimation of psychometric functions for (potentially) overdispersed data</article-title>. <source>Vis. Res</source>. <volume>122</volume>, <fpage>105</fpage>–<lpage>123</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c129"><label>129.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Kingma</surname>, <given-names>D. P.</given-names></string-name> &amp; <string-name><surname>Welling</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>Auto-encoding variational bayes</article-title>. <source>arXiv</source> <pub-id pub-id-type="arxiv">1312.6114</pub-id> (<year>2013</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107301.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Sui</surname>
<given-names>Jing</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This is a methodologically rich manuscript that is <bold>important</bold> for elucidating the neural mechanisms of expectation in perception. The analyses are <bold>convincing</bold> in extending analogous findings in attention and working memory. With further clarification, the findings will be of broad interest to vision researchers.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107301.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors tested two competing mechanisms of expectation: (1) a sharpening model that suppresses unexpected information via center-surround inhibition; (2) a cancelation model that predicts a monotonic gradient response profile. Using two psychophysical experiments manipulating feature space distance between expected and unexpected stimuli, the results consistently supported the sharpening model. Computational modeling further showed that expectation effects were explained by either sharpened tuning curves or tuning shifts. Finally, convolutional neural network simulations revealed that feedback connections critically mediate the observed center-surround inhibition.</p>
<p>Strengths:</p>
<p>The manuscript provides compelling and convergent evidence from both psychophysical experiments and computational modeling to robustly support the sharpening model of expectation, demonstrating clear center-surround inhibition of unexpected information.</p>
<p>Weaknesses:</p>
<p>The manuscript could directly validate the experimental manipulations and address how these results reconcile with existing literature on expectation effects.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107301.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This is a compelling and methodologically rich manuscript. The authors used a variety of methods, including psychophysics, computational modeling, and artificial neural networks, to reveal a non-monotonic, center-surround &quot;Mexican-hat&quot; profile of expectation in orientation space. Their data convincingly extend analogous findings in attention and working memory, and the modeling nicely teases apart sharpening vs. shift mechanisms.</p>
<p>Strengths:</p>
<p>The findings are novel and important in elucidating the potential neural mechanisms by which expectation shapes perception. The authors conducted a series of well-designed psychophysical experiments to careful examination of the profile of expectation's modulation. Computational modeling also provides further insights, linking the neural mechanisms of expectation to behavioral results.</p>
<p>Weaknesses:</p>
<p>There are several aspects that could be strengthened or clarified.</p>
<p>(1) The sharpening model of expectation can predict surround suppression. The authors could further clarify how the cancellation model predicts a monotonic profile of expectation (Figure 1C) with the highest response at the expected orientation, while the cancellation model suggests a suppression of neurons tuned toward the expected stimulus.</p>
<p>(2) I'm a bit concerned about whether the profile solely arises from modulation of expectation. The two auditory cues are each associated with a fixed orientation, which may be confounded by other cognitive processes like visual working memory or attention (which I think the authors also discussed). Although the authors tried to use SFD task to render orientation task-irrelevant, luminance edges (i.e., orientation) and spatial frequency in gratings are highly intertwined and orientation of the gratings may help recall the first grating's SF (fixed at 0.9 c/{degree sign}), especially given the first and second grating's orientations are not very different (4.8{degree sign}).</p>
<p>(3) For each of the expected orientations (20{degree sign} or 70{degree sign}), the unexpected ones are linearly separable (i.e., all unexpected ones lie on one side of the expected angle). This might further encourage people to shift their attended or expected orientation, according to the optimal tuning hypothesis. Would this provide an alternative explanation to the tuning shift that the authors found?</p>
<p>(4) It is great that the authors conducted computational modeling to elucidate the potential neuronal mechanisms of expectation. But I think the sharpening hypothesis (e.g., reviewed in de Lange, Heilbron &amp; Kok, 2018) focuses on the neural population level, i.e., narrowing of population tuning profile, while the authors conducted the sharpening at the neuronal tuning level. However, the sharpening of population does not necessarily rely on the sharpening of individual neuronal tuning. For example, neuronal gain modulation can also account for such population sharpening. I think similar logic applies to the orientation adjustment experiment. The behavioral level shift does not necessarily suggest a similar shift at the neuronal level. I would recommend that the authors comment on this.</p>
<p>(5) If the orientation adjustment experiment suggests that both sharpening and shifting are present at the same time, have the authors tried combining both in their computational model?</p>
</body>
</sub-article>
</article>