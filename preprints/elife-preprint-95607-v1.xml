<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">95607</article-id>
<article-id pub-id-type="doi">10.7554/eLife.95607</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95607.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>ElectroPhysiomeGAN: Generation of Biophysical Neuron Model Parameters from Recorded Electrophysiological Responses</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5597-5142</contrib-id>
<name>
<surname>Kim</surname>
<given-names>Jimin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9232-1420</contrib-id>
<name>
<surname>Liu</surname>
<given-names>Qiang</given-names>
</name>
<email>qiangliuemail@gmail.com</email>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3136-4531</contrib-id>
<name>
<surname>Shlizerman</surname>
<given-names>Eli</given-names>
</name>
<email>shlizee@uw.edu</email>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Electrical and Computer Engineering, University of Washington</institution></aff>
<aff id="a2"><label>2</label><institution>Department of Applied Mathematics, University of Washington</institution></aff>
<aff id="a3"><label>3</label><institution>Department of Neuroscience, City University of Hong Kong</institution></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Bhalla</surname>
<given-names>Upinder S</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>National Centre for Biological Sciences</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="original-publication" iso-8601-date="2024-06-13">
<day>13</day>
<month>06</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP95607</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-01-29">
<day>29</day>
<month>01</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-12-20">
<day>20</day>
<month>12</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.12.19.572452"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Kim et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Kim et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-95607-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Recent advances in connectomics, biophysics, and neuronal electrophysiology warrant modeling of neurons with further details in both network interaction and cellular dynamics. Such models may be referred to as ElectroPhysiome, as they incorporate the connectome and individual neuron electrophysiology to simulate neuronal activities. The nervous system of <italic>C. elegans</italic> is considered a viable framework for such ElectroPhysiome studies due to advances in connectomics of its somatic nervous system and electrophysiological recordings of neuron responses. In order to achieve a simulated ElectroPhysiome, the set of parameters involved in modeling individual neurons need to be estimated from electrophysiological recordings. Here, we address this challenge by developing a novel deep generative method called ElectroPhysiomeGAN (EP-GAN), which once trained, can instantly generate parameters associated with the Hodgkin-Huxley neuron model (HH-model) for neurons with graded potential response. The method combines Generative Adversarial Network (GAN) architecture with Recurrent Neural Network (RNN) Encoder and can generate an extensive number of parameters (&gt;170) given the neuron’s membrane potential responses and steady-state current profiles. We validate our method by estimating HH-model parameters for 200 synthetic neurons with graded membrane potential followed by 9 experimentally recorded neurons (where 6 of them newly recorded) in the nervous system of <italic>C. elegans</italic>. Compared to other methods, EP-GAN is advantageous in both accuracy of generated parameters and inference speed. In addition, EP-GAN preserves performance when provided with incomplete membrane potential responses up to 25% and steady-state current profiles up to 75%. EP-GAN is designed to leverage the generative capability of GAN to align with the dynamical structure of HH-model, and thus able to achieve such performance.</p>
</abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Models of the nervous system aim to achieve biologically detailed simulations of large-scale neuronal activity through the incorporation of both structural connectomes (connectivity maps) and individual neural dynamics. The nervous system of <italic>Caenorhabditis elegans</italic> (<italic>C. elegans</italic>) is considered a framework for such a model as the connectome of its somatic nervous system for multiple types of interaction is mapped [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c3">3</xref>]. In addition to the connectome, advances in electrophysiological methodology allow the recording of whole-cell responses of individual neurons. These advances provide biophysically relevant details of individual neuro-dynamical properties and warrant a type of model for the <italic>C. elegans</italic> nervous system incorporating both the connectomes and individual biophysical processes of neurons. Such a model could be referred to as <italic>ElectroPhysiome</italic>, as it incorporates a layer of individual neural dynamics on top of the layer of inter-cellular interactions facilitated by the connectome.</p>
<p>The development of models that are further biophysically descriptive for each neuron, i.e., modeling neurons using the Hodgkin-Huxley type equations (HH-model) require fitting a large number of parameters. For a typical single neuron, these parameters could be tuned via local optimizations of individual ion channel parameters estimated separately to fit their respective <italic>in-vivo</italic> channel recordings such as activation/inactivation curves [<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>]. Such method requires multiple experiments to collect each channel data and when such experiments are infeasible, the parameters are often estimated through hand-tuning. In the context of developing the ElectroPhysiome of <italic>C. elegans</italic>, the method would have to model approximately 300 neurons each including an order of hundreds of parameters associated with up to 15 to 20 ionic current terms (with some of them having unknown ion channel composition), which would require large experimental studies [<xref ref-type="bibr" rid="c7">7</xref>]. Furthermore, the fitted model may not be the unique solution as different HH-parameters can produce similar neuron activity [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>]. As these limitations also apply for general neuron modeling tasks beyond <italic>C. elegans</italic> neurons, there has been an increasing search for alternative fitting methods requiring less experimental data and manual interventions.</p>
<p>A promising direction in associating model parameters with neurons has been the simultaneous estimation of all parameters of an individual neuron given only electrophysiological responses of cells, such as membrane potential responses and steady-state current profiles. Such an approach requires significantly less experimental data per neuron and offers more flexibility with respect to trainable parameters. The primary aim of this approach is to model macroscopic cell behaviors in an automated manner. Indeed, several methods adopting the approach have been introduced. Buhry et al 2012 and Laredo et al 2022 utilized the Differential Evolution (DE) method to simultaneously estimate the parameters of a 3-channel HH-model given a whole-cell membrane potential responses recording [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>]. Naudin et al 2022 further developed the DE approach and introduced the Multi-objective Differential Evolution (DEMO) method to estimate 22 HH-parameters of 3 non-spiking neurons in <italic>C. elegans</italic> given their whole-cell membrane potential responses and steady-state current profiles [<xref ref-type="bibr" rid="c16">16</xref>]. The study was a significant step toward modeling whole-cell behaviors of <italic>C. elegans</italic> neurons in a systematic manner. From statistical standpoint, Wang et al 2022 used the Markov-Chain-Monte-Carlo method to obtain the posterior distribution of channel parameters for HH-models featuring 3 and 8 ion channels (2 and 9 parameters respectively) given the simulated membrane potential responses data [<xref ref-type="bibr" rid="c17">17</xref>]. From an analytic standpoint, Valle et al 2022 suggested an iterative gradient descent based method that directly manipulates HH-model to infer 3 conductance parameters and 3 exponents of activation functions given the measurements of membrane potential responses [<xref ref-type="bibr" rid="c18">18</xref>]. Recent advances in machine learning gave rise to deep learning based methods which infer steady-state activation functions and posterior distributions of 3-channel HH-model parameters inferred by an artificial neural network model given the membrane potential responses data [<xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c20">20</xref>].</p>
<p>While these methods suggest that simultaneous parameter estimation from macroscopic cell data is indeed possible through a variety of techniques, it is largely unclear whether they can be extended to fit more detailed HH-models featuring a large number of parameters [<xref ref-type="bibr" rid="c7">7</xref>]. Furthermore, for most of the above methods, the algorithms require an independent (from scratch) optimization process for modeling each individual neuron, which leads to expensive computation when scaling them to a large number of neurons.</p>
<p>Here we propose a new data-driven model approach that aims to address these aspects for the class of non-spiking neurons, which constitute the majority of neurons in <italic>C. elegans</italic> nervous system [<xref ref-type="bibr" rid="c21">21</xref>]. Specifically, we develop a deep generative neural network model (GAN) combined with a recurrent neural network (RNN) encoder called ElectroPhysiomeGAN (EP-GAN), which directly maps electrophysiological recordings of a neuron, e.g., membrane potential responses and steady-state current profiles, to HH-model parameters of arbitrary dimensions (<xref rid="fig1" ref-type="fig">Figure 1</xref>). EP-GAN can be trained with simulation data informed by a generic HH-model encompassing a large set of arbitrary ionic current terms and thus can generalize its modeling capability to multiple neurons. Unlike typical GAN architecture trained solely with adversarial losses, we propose to implement additional supervised loss for reconstructing the given membrane potential responses and current profiles from generated parameters, thus improving the accuracy of the generative model. In addition, due to RNN component of EP-GAN, the approach supports input data with missing features such as incomplete membrane potential responses and current profiles.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Estimation of HH-model parameters from membrane potential and steady-state current profiles.</title>
<p>Given the membrane potential responses (V) and steady-state current profiles (IV) of a neuron, the task is to predict biophysical parameters of Hodgkin-Huxley type neuron model (Left). We use Encoder-Generator approach to predict the parameters (Right)</p></caption>
<graphic xlink:href="572452v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We validate our method to estimate HH-model parameters of 200 synthetic non-spiking neurons and apply to three previously recorded non-spiking neurons of <italic>C. elegans</italic>, namely RIM, AFD, and AIY. Studies have shown that membrane potential responses of these neurons can be well modeled with typical HH-model formulations with 22 parameters [<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c22">22</xref>]. We show that when trained with a more detailed HH-model consisting of 15 ionic current terms resulting with 176 trainable parameters, EP-GAN can produce parameters modeling their membrane potential responses with better accuracy in reconstruction of membrane potential and significantly faster speed than existing algorithms such as Multi-Objective DE and Genetic Algorithms. Through ablation studies on input data, we show that EP-GAN preserves its prediction capability even when provided with incomplete membrane potential responses and current profiles. We also perform ablation studies on the model architecture components to elucidate each component contributions toward the accuracy of the predicted parameters. To further test EP-GAN, we estimate HH-model parameters for 6 newly recorded non-spiking <italic>C. elegans</italic> neurons: AWB, AWC, URX, RIS, DVC, and HSN, whose membrane potential responses were not previously modeled.</p>
<p>Our results suggest that EP-GAN can learn a translation from electrophysiologically recorded responses and propose projections of them to parameter space. EP-GAN method is currently limited to non-spiking neurons in <italic>C. elegans</italic> as it was designed and trained with HH-model describing the ion channels of these neurons. EP-GAN applications can be potentially extended toward resolving neuron parameters in other organisms since non-spiking neurons are found within animals across different species [<xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>].</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<p>We evaluate EP-GAN with respect to 4 current evolutionary algorithms introduced for general parameter estimation: NSGA2, DEMO, GDE3, and NSDE. Specifically, NSGA2 is a variant of the Genetic Algorithm utilizing a non-dominated sorting survival strategy, and is a commonly used benchmark algorithm for multi-objective optimization problems including HH-model fitting [<xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c32">32</xref>, <xref ref-type="bibr" rid="c33">33</xref>]. DEMO, GDE3 and NSDE are variants of multi-objective differential evolution (DE) algorithms that combine DE mutation with pareto based ranking and crowding distance sorting applied in NSGA2’s survival strategy [<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c36">36</xref>]. These methods have been proposed as more effective methods than direct DE for estimation of HH-model parameters [<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c39">39</xref>]. In particular, DEMO has been successfully applied to estimate HH-model parameters for non-spiking neurons in <italic>C. elegans</italic> and we include additional comparison studies with EP-GAN in the sub-section <italic>Additional Comparisons with DEMO</italic> [<xref ref-type="bibr" rid="c16">16</xref>].</p>
<p>All 4 methods support multi-objective optimization over large parameter space allowing them to have similar training setups as EP-GAN. All 4 methods were implemented in Python where DEMO uses the algorithm proposed in [<xref ref-type="bibr" rid="c16">16</xref>] whereas NSGA2, GDE3 and NSDE were implemented using Pymoo package [<xref ref-type="bibr" rid="c40">40</xref>]. For the HH-model to be estimated, we use the formulation introduced in [<xref ref-type="bibr" rid="c7">7</xref>]. The model features 15 ion channels that were found in <italic>C. elegans</italic> and other organisms expressing homologous channels and is considered the most detailed neuron model for the organism. The model has a total of <bold>216</bold> parameters, of which 176 have approximate ranges with lower and upper bounds that can be inferred from the literature [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c41">41</xref>]. We thus target these 176 parameters as trainable parameters for all methods.</p>
<sec id="s2a">
<title>Validation Scenarios</title>
<p>We validate EP-GAN trained on 32,000 (32k) training samples (i.e., 32k HH simulated neurons) by modeling 200 synthetic neurons. These neurons are randomly simulated using HH-model and divided into two groups - 100 neurons sampled from training data and other 100 neurons from test data (i.e. not part of the training set). At each training epoch, EP-GAN generates parameters for the neurons being evaluated and for each neuron, the parameters which achieved the lowest RMSE error (detailed descriptions of its calculation provided in supplementary) of membrane potential responses w.r.t. ground truth are reported at the end of training. These results in EP-GAN scoring overall error of <bold>5</bold>.<bold>84mV</bold> and <bold>5</bold>.<bold>81mV</bold> for neurons sampled from training data and testing data respectively (Table S1).</p>
<p>We then model 3 experimentally recorded non-spiking neurons of <italic>C. elegans</italic> - RIM, AFD, and AIY, and compare the performance of EP-GAN with NSGA2, DEMO, GDE3, and NSDE. Recorded data for these neurons is publicly available and their modeling descriptions were elaborated by previous works, making them common benchmarks [<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c42">42</xref>]. For all methods, the total training data size of 32,000 (32k HH simulated neurons) is used. i.e., EP-GAN is given 32k training samples from synthetic training scenario, and for NSGA2, DEMO, GDE3, and NSDE, each algorithm is allocated with up to 11k HH simulations for each neuron during its search phase, thus adding up to total of 32k simulations for all three neurons. For each search phase of NSGA2, DEMO, GDE3 and NSDE, the parameter set candidates are recorded at each iteration, and the candidate achieving the lowest membrane potential responses error is reported at the end of the search phase. For Multi-objective DE methods (DEMO, GDE3 and NSDE2), we follow the same configurations used in the literature to set their parameters and optimization scheme [<xref ref-type="bibr" rid="c16">16</xref>]. Specifically, we use the root-mean-square error normalized to the noise level for the objective functions for both membrane potential responses and steady-state current and set crossover parameter CR and scale factor F to 0.3 and 1.5 respectively. For all 4 methods, NP is set to 600 with total 18 iterations (i.e., ∼ 11<italic>k</italic> HH simulations).</p>
<p><xref rid="fig2" ref-type="fig">Figure 2</xref> shows comparison between reconstructed membrane potential responses of predicted parameters vs ground truth membrane potential responses for all 4 methods. We include reconstructed steady-state current profiles in supplement Figure S1. <xref rid="fig2" ref-type="fig">Figure 2</xref> illustrates that EP-GAN can reconstruct membrane potential responses closer to ground truth responses than those of NSGA2, DEMO, GDE3 and NSDE. Indeed, when comparing the overall RMSE error (<xref rid="tbl1" ref-type="table">Table 1</xref>) between ground truth and reconstructed membrane potential responses, EP-GAN overall error (6.7mV) is 40% lower than that of NSGA2 (11.2mV) followed by NSDE (20.9mV), GDE3 (23.4mV) and DEMO (28.1mV). The overall error of 6.7mV is also in the similar ballpark of ∼ 5.8mV recorded during synthetic validation. Among the three neurons used for prediction, EP-GAN showed the best accuracy for AIY neuron (6.2mV), followed by AFD (6.4mV) and RIM (7.5mV).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>The lowest membrane potential responses errors achieved by each method for validation scenarios.</title>
<p>HH simulations represent the total number of simulations each method used for training. For all scenarios, the error represents the average RMSE between ground truth and predicted membrane potential responses in each time point across all membrane potential responses traces.</p></caption>
<graphic xlink:href="572452v1_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Comparison of membrane potential responses reconstructed from predicted parameters (validation scenarios).</title>
<p>Each row and column corresponds to predicted neuron and method respectively. For each neuron we show ground truth membrane potential responses (Black) against the reconstructed membrane potential responses (red) from the predicted parameters.</p></caption>
<graphic xlink:href="572452v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<title>Ablation Studies</title>
<p>To test the robustness of EP-GAN when incomplete input data is given, we provide the model with membrane potential responses and steady-state current profiles with missing data points. For each membrane potential responses and current profile, the data is reduced by 25%, 50%, and 75% each. For membrane potential responses data, the ablation is done on stimulus space where a 50% reduction corresponds to removing half of the membrane potential responses traces each associated with a stimulus. For steady-state current profile, we remove the first <italic>n</italic>-data points where they are instead extrapolated using linear interpolation with existing data points.</p>
<p>Our results show that the quality of the predicted parameters depends on membrane potential responses (<xref rid="fig3" ref-type="fig">Figure 3A</xref>, <xref rid="tbl1" ref-type="table">Table 1</xref> Row 2). In particular, EP-GAN preserves reasonable accuracy up to a 25% reduction in membrane potential responses but becomes less accurate when it increases to 50%. With respect to steady-state current profiles, those have an effect on the accuracy (<xref rid="fig3" ref-type="fig">Figure 3B</xref>, <xref rid="tbl2" ref-type="table">Table 2</xref> Row 1) but the effect is more minor than that of reducing membrane potential responses and the error doesn’t vary when scale is reduced. The results imply that the membrane potential responses play a major role in the performance of the model.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><title>Ablation studies.</title>
<p><bold>Top</bold>: membrane potential responses errors achieved for EP-GAN when provided with incomplete input data. <bold>Bottom</bold>: membrane potential responses errors achieved for EP-GAN upon using only adversarial loss (A) and using adversarial + current reconstruction loss (A, IV) and all three loss components (A, IV, V)</p></caption>
<graphic xlink:href="572452v1_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Input data ablation on EP-GAN.</title>
<p><bold>A</bold>: Reconstructed membrane potential responses when given with incomplete membrane potential responses data. Percentages in parenthesis represent the remaining portion of input membrane potential responses trajectories. <bold>B</bold>: Reconstructed membrane potential responses when given with incomplete current profile.</p></caption>
<graphic xlink:href="572452v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We also performed ablation studies on model architecture by removing each loss component of the Generator module, allowing us to evaluate the relative contribution of each loss to accuracy. From <xref rid="tbl2" ref-type="table">Table 2</xref> bottom, we see that removing the membrane potential loss term (V) results in a significant loss in performances for all three neurons as expected, where the reduction for AIY neuron is the most prominent. Upon removing the steady-state current reconstruction loss term (IV) in addition to membrane potential reconstruction loss, we see further reduction in overall performance. These results highlight the significance of the reconstruction losses in aligning the Generator to produce the desired outputs.</p>
</sec>
<sec id="s2c">
<title>Prediction Scenarios</title>
<p>To further test if EP-GAN can generalize to other testing scenarios, we applied EP-GAN on 6 additional neurons - AWB, AWC, URX, RIS, DVC, and HSN, which recording data is novel and these neurons were not previously modeled. Similar to modeling RIM, AFD, and AIY, EP-GAN with identical training data, size of 32k, was used where membrane potential responses were rearranged for each training sample according to neuron specific current-clamp protocol. For NSGA2, DEMO, GDE3 and NSDE, each algorithm was given 5.4k HH simulations for each neuron, adding up to 32k total simulations with 6 neurons. From <xref rid="fig4" ref-type="fig">Figure 4</xref>, we observe that EP-GAN is indeed able to predict parameters that closely recover ground truth membrane potential responses for all 6 neurons. Upon inspecting the RMSE error, EP-GAN significantly outperforms NSGA2, DEMO, GDE3 and NSDE with 50% - 80% less error on average, and also records smaller overall error of 5.35mV than EP-GAN error for validation scenarios ( ∼ 6.7mV) and on synthetic data ( ∼ 5.8mV). These results indicate that the predictive capability of EP-GAN can generalize to a novel set of neurons with a relatively small training data size of 32k without need for retraining if same format of recordings is used.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Illustration of membrane potential responses reconstructed from predicted parameters (prediction scenarios).</title>
<p>Membrane potential responses trajectories reconstructed from EP-GAN predicted parameters (red) overlaid on top of the ground truth recording membrane potential responsess (black).</p></caption>
<graphic xlink:href="572452v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2d">
<title>Parameter Inference Time</title>
<p>In addition to its capability to generalize over multiple neurons, we evaluated EP-GAN for scalability by comparing its overall inference time to other methods. Indeed, for estimation tasks involving many neurons, it is essential that the method is scalable so that the predictions are done within a reasonable time. In <xref rid="tbl4" ref-type="table">Table 4</xref> we list the computation times involved in each approach for validation scenarios. We show that while EP-GAN requires time for initial data generation and training, it is of orders of magnitude faster in the actual estimation process. As an example, given a hypothetical task of modeling all 279 somatic neurons in the <italic>C. elegans</italic> nervous system, it would take DEMO, GDE3, NSDE and NSGA2 more than 67 days (assuming 11k HH simulations per neuron) whereas, for EP-GAN, given that all neurons have identical recording protocol, the process would be done within a day under the similar training setup. For larger number of neurons, computational requirement of existing methods would grow linearly while EP-GAN would require constant time to complete the inference. This is due to EP-GAN being a neural network which supports parallel processing of inputs, allowing it to take multiple neuron profiles and output corresponding parameters in a single forward pass [<xref ref-type="bibr" rid="c43">43</xref>].</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3:</label>
<caption><title>The lowest membrane potential responses errors achieved by each method for prediction scenarios.</title>
<p>HH simulations represent the total number of simulations each method used for training.</p></caption>
<graphic xlink:href="572452v1_tbl3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4:</label>
<caption><title>Inference Time Comparison between methods for validation scenarios.</title>
<p>For EP-GAN, a training consists of 100 epochs.</p></caption>
<graphic xlink:href="572452v1_tbl4.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s2e">
<title>Additional Comparisons with DEMO</title>
<p>EP-GAN aim and primary application is similar to the algorithm introduced in [<xref ref-type="bibr" rid="c16">16</xref>] which utilizes DEMO (DE for Multi-objective Optimization). Both methods aim to estimate HH-model parameters for non-spiking neurons of <italic>C. elegans</italic>. DEMO is applied to 3 channels HH equations (<italic>I</italic><sub><italic>Ca,p</italic></sub> + <italic>I</italic><sub><italic>Kir</italic></sub> + <italic>I</italic><sub><italic>K,t</italic></sub> + <italic>I</italic><sub><italic>L</italic></sub>-model and <italic>I</italic><sub><italic>Ca,t</italic></sub> + <italic>I</italic><sub><italic>Kir</italic></sub> + <italic>I</italic><sub><italic>K,p</italic></sub> + <italic>I</italic><sub><italic>L</italic></sub>-model) consisting of 22 parameters to model RIM, AFD and AIY. This is much lower dimension model than we use previously. In addition to validating EP-GAN on more detailed model, we evaluate its performance on the same model for which DEMO was applied. We observe that for all HH simulation sizes used for training (32k, 64k, 96k), EP-GAN achieves membrane potential response errors on par with those obtained with DEMO (Table S2). While the best overall error for EP-GAN (4.9mV) is higher than that of DEMO (3.2mV), EP-GAN requires significantly less number of HH simulations (96k <italic>·</italic> 2 (EP-GAN) vs 12,000k <italic>·</italic> 3 (DEMO)). To further investigate the correlation between the number of HH simulations and DEMO performance, we re-evaluate DEMO with respect to the detailed HH-model of [<xref ref-type="bibr" rid="c7">7</xref>] when given with 64k HH simulations (∼ 107 iterations, <italic>NP</italic> = 600) for each neuron instead of 11k and 5.4k simulations conisdered in validation and prediction scenarios. From Table S3, we see that while DEMO overall error is improved, EP-GAN trained with a single training set of 32k HH simulations maintains better results for both validation (6.7mV (EP-GAN) vs 26.8mV (DEMO)) and prediction scenarios (5.35mV (EP-GAN) vs 24.2mV (DEMO)). These results suggest that EP-GAN can be applied to different HH-models and generate parameters with consistent accuracy.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Methods</title>
<p>We divide the method section into two parts. In the first part we describe the detailed architecture of EP-GAN including its sub-modules with the protocol for training. In the second part, we describe the dataset and experimental protocol of novel neuron recording of AWB, AWC, URX, RIS, DVC, and HSN from <italic>C. elegans</italic> nervous system.</p>
<sec id="s3a">
<label>3.1</label>
<title>Architecture of EP-GAN</title>
<sec id="s3a1">
<title>Deep Generative Model for Parameter Prediction</title>
<p>EP-GAN receives neuron recording data such as membrane potential responses and steady-state current profiles and generates a parameter set that is associated with them in terms of simulating the inferred HH-model and comparing the simulated outcome with the inputs (<xref rid="fig1" ref-type="fig">Figure 1</xref>,<xref rid="fig5" ref-type="fig">5</xref>). We choose a deep generative model approach, specifically Generative Adversarial Network (GAN) as a base architecture of EP-GAN. The key advantage of GAN is that it is designed to generate artificial data that closely resembles the real data. Such generative nature of GAN is advantageous for addressing the multi-modal nature of our problem, where the parameter solution is not guaranteed to be unique for a given neuron recording. Indeed, several computational works attempting to solve inverse HH-model have pointed out the ill-posed nature of the parameter solutions [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c44">44</xref>, <xref ref-type="bibr" rid="c45">45</xref>]. Our approach is therefore leveraging GAN to learn a <italic>domain of parameter sets</italic> compatible with a neuron recordings instead of mapping onto a single solution. GAN consists of two separate networks, Generator and Discriminator. The goal of the Generator is to generate outputs that are indistinguishable from real data whereas the Discriminator’s goal is to distinguish outputs that are generated by the generator against real data. Throughout training, the Generator and the Discriminator compete with each other (zero-sum game) until they both converge to optimal states [<xref ref-type="bibr" rid="c46">46</xref>]. The particular architecture we use is Wasserstein GAN with gradient penalty (WGAN-GP), a variant of GAN architecture offering more stable training and faster convergence [<xref ref-type="bibr" rid="c47">47</xref>].</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Architecture of EP-GAN.</title>
<p>The architecture consists of Encoder, Generator and Discriminator. Encoder compresses the membrane potential responses into a 1D vector (i.e., latent space) which is then concatenated with 1D steady-state current profile to be used as an input to both generator and discriminator. Generator translates the latent space vector into a vector of parameters <inline-formula><inline-graphic xlink:href="572452v1_inline20.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and the Discriminator outputs a scalar measuring the similarity between generated parameters <inline-formula><inline-graphic xlink:href="572452v1_inline21.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and ground truths <inline-formula><inline-graphic xlink:href="572452v1_inline22.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The Generator is trained with adversarial loss supplemented by reconstruction losses for both membrane potential responses and steady-state current profiles. The Discriminator is trained with discriminator adversarial loss only. Generator and Discriminator follow the architecture of Wasserstein GAN with gradient penalty (WGAN-GP) for more stable learning.</p></caption>
<graphic xlink:href="572452v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3a2">
<title>Encoder Module</title>
<p>In addition to Generator and Discriminator, we implement Encoder module which pre-processes the input membrane potential responses for Generator and Discriminator. (<xref rid="fig5" ref-type="fig">Figure 5</xref> left). Specifically, Encoder serves two roles: i) Compression of membrane potential responses traces along the stimulus space, thus reducing its dimension from 2-dimensional to 1-dimensional, and ii) Translating membrane potential responses traces into a latent space encoding a meaningful internal representation for the Discriminator and Generator. The Encoder module uses Gated Recurrent Units (GRU) architecture, a variant of Recurrent Neural Network (RNN) to perform this task [<xref ref-type="bibr" rid="c48">48</xref>]. Each input sequence to a GRU cell at step <italic>t</italic> corresponds to the entire membrane potential response of length 500 (i.e., 500 time points) associated with a stimulus. The output of the Encoder is a latent space vector of length 500 encoding membrane potential responses information. The latent space vector is then concatenated with a 1D vector storing current profile information and fed to both Generator and Discriminator.</p>
</sec>
<sec id="s3a3">
<title>Discriminator Module</title>
<p>The goal of the Discriminator is given the input membrane potential responses and current profiles, to distinguish generated parameters from real ground truth parameters. The Discriminator receives as input the latent space vector from Encoder concatenated with generated or ground truth parameter vector and outputs a scalar representing the relative distance between two parameter sets (<xref rid="fig5" ref-type="fig">Figure 5</xref>, <xref ref-type="disp-formula" rid="eqn3">Eqn 3</xref>.1). Such a quantity is called Wasserstein distance or Wasserstein loss and differs from vanilla GAN Discriminator which only outputs 0 or 1. Wasserstein loss is known to remedy several common issues that arise from vanilla GAN such as vanishing gradient and mode collapse, leading to more stable training [<xref ref-type="bibr" rid="c47">47</xref>]. To further improve the training of WGAN architecture we supplement Wasserstein loss with a gradient penalty term, which ensures that the gradients of the Discriminator’s output with respect to the input has unit norms [<xref ref-type="bibr" rid="c49">49</xref>]. This condition is called Lipschitz continuity and prevents Discriminator outputs from having large variations when there is only small variations in the inputs [<xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c50">50</xref>]. Combined together, the Discriminator is trained with the following loss:
<disp-formula id="eqn1">
<graphic xlink:href="572452v1_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="572452v1_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="572452v1_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are the mean values of Discriminator outputs with respect to generated samples <inline-formula><inline-graphic xlink:href="572452v1_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and real samples <inline-formula><inline-graphic xlink:href="572452v1_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> respectively and <inline-formula><inline-graphic xlink:href="572452v1_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the gradient penalty term modulated by <italic>λ</italic> where <inline-formula><inline-graphic xlink:href="572452v1_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the interpolation between generated and real samples with 0 ≤ <italic>t</italic> ≤ 1.</p>
</sec>
<sec id="s3a4">
<title>Generator Module</title>
<p>Being an adversary network of Discriminator, the goal of Generator is to fool the Discriminator by generating parameters that are indistinguishable from the real parameters. The Generator receives as input the concatenated vector from the Encoder and outputs a parameter vector (<xref rid="fig5" ref-type="fig">Figure 5</xref>). The module consists of 4 fully connected layers. Each parameter in the output vector is scaled between -1 and 1 which is then scaled back to the parameters’ original scales. The module is trained using 3 loss terms: i) Generator adversarial loss, ii) Membrane potential responses reconstruction loss and iii) Current reconstruction loss as follows:
<disp-formula id="eqn2">
<graphic xlink:href="572452v1_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn3">
<graphic xlink:href="572452v1_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn4">
<graphic xlink:href="572452v1_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="572452v1_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is Generator adversarial loss which is reciprocal of the mean Discriminator outputs with respect to generated samples and <inline-formula><inline-graphic xlink:href="572452v1_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="572452v1_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are <italic>L</italic><sub>1</sub> regression loss for reconstructed membrane potential responses and current profiles respectively. It’s important to note that <inline-formula><inline-graphic xlink:href="572452v1_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="572452v1_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are part of Generator’s computation graph and thus force Generator to optimize them on top of adversarial loss (<xref rid="fig6" ref-type="fig">Figure 6</xref>). The composite loss function of Generator makes EP-GAN a “physics-informed” neural network as HH-model itself becomes part of the training process. Such networks have shown to be more data efficient during training as they don’t rely solely on training data to learn effective strategy [<xref ref-type="bibr" rid="c51">51</xref>, <xref ref-type="bibr" rid="c52">52</xref>]. The mathematical description of membrane potential responses and current reconstruction from generated parameter set <inline-formula><inline-graphic xlink:href="572452v1_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is as follows:
<disp-formula id="eqn5">
<graphic xlink:href="572452v1_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula></p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Description of membrane potential responses and current reconstruction losses for the Generator.</title>
<p>Generated parameter vector <inline-formula><inline-graphic xlink:href="572452v1_inline23.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is used to evaluate membrane potential responses derivatives <italic>dV/dt</italic> at <italic>n</italic> time points sampled with fixed interval given the ground truth <inline-formula><inline-graphic xlink:href="572452v1_inline24.gif" mimetype="image" mime-subtype="gif"/></inline-formula> at those time points. The evaluated membrane potential responses derivatives are then used to reconstruct <inline-formula><inline-graphic xlink:href="572452v1_inline25.gif" mimetype="image" mime-subtype="gif"/></inline-formula> using the inverse gradient operation ∇<sup>−1</sup>. The reconstructed <inline-formula><inline-graphic xlink:href="572452v1_inline26.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is then compared with ground truth <inline-formula><inline-graphic xlink:href="572452v1_inline27.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to evaluate the membrane potential responses reconstruction loss <inline-formula><inline-graphic xlink:href="572452v1_inline28.gif" mimetype="image" mime-subtype="gif"/></inline-formula> . Current reconstruction is computed in a similar way via evaluating the currents at defined membrane potential responses steps <italic>V</italic> given generated parameters<inline-formula><inline-graphic xlink:href="572452v1_inline29.gif" mimetype="image" mime-subtype="gif"/></inline-formula> as inputs.</p></caption>
<graphic xlink:href="572452v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Here <inline-formula><inline-graphic xlink:href="572452v1_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the right-hand-side function of HH-model which computes the membrane potential responses derivative at time <italic>t</italic> given the membrane potential responses <italic>V</italic> and parameter set <inline-formula><inline-graphic xlink:href="572452v1_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="572452v1_inline15.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the function that evaluates neuron’s current <italic>I</italic> given the membrane potential responses and parameter set <inline-formula><inline-graphic xlink:href="572452v1_inline16.gif" mimetype="image" mime-subtype="gif"/></inline-formula> . Membrane potential responses are reconstructed by first evaluating their derivatives at regularly sampled time points which are followed by inverse derivative operation ∇<sup>−1</sup> which uses cumulative summation to approximate <inline-formula><inline-graphic xlink:href="572452v1_inline17.gif" mimetype="image" mime-subtype="gif"/></inline-formula> at sampled time points [<xref ref-type="bibr" rid="c53">53</xref>]. The total number of time points to be sampled can be adjusted to increase the accuracy of the membrane potential responses reconstruction in exchange for the increased computational cost. The current profile is reconstructed by directly evaluating <inline-formula><inline-graphic xlink:href="572452v1_inline18.gif" mimetype="image" mime-subtype="gif"/></inline-formula> which uses generated parameter set <inline-formula><inline-graphic xlink:href="572452v1_inline19.gif" mimetype="image" mime-subtype="gif"/></inline-formula> over the range of membrane potential responses values. We show in <xref rid="tbl2" ref-type="table">Table 2</xref> that reconstruction losses are essential for the accuracy of predicted parameters.</p>
</sec>
<sec id="s3a5">
<title>Generating Training Data</title>
<p>For a successful training of a neural network model, the training data must be of sufficient number of samples, denoised, and diverse. To ensure these conditions are met with a simulated dataset, we employ a two-step process for generating training data (<xref rid="fig7" ref-type="fig">Figure 7</xref>). In the first step, each parameter set is randomly generated by uniformly sampling of each parameter within a given range. The range is determined according to the biologically feasible ranges reported by the literature [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c41">41</xref>]. In the second step, membrane potential responses and current traces are simulated for each sampled parameter set followed by imposing constraints on each of them (See supplementary for numerical simulation protocol). For the membrane potential response, the constraint is such that the variances between membrane potential responses trajectories are above a certain value. For the IV profile, an upper bound and lower bound on the y-axis (current axis) are set such that a certain proportion of data points must fall within two boundaries. Parameter sets that do not satisfy these constraints are then removed from the training set. The constraints serve two purposes: i) remove parameter sets that result in non-realistic membrane potential responses/current profiles from the training set and ii) serve as an initial data augmentation process for the model. The constraints can also be adjusted if deemed necessary for the improvement of the training process. Once constraints are applied, Gaussian noise is added to the membrane potential responses training data to mimic the measurement noises in experimental membrane potential responses recording data.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7:</label>
<caption><title>Generating training data.</title>
<p>Each parameter is initially sampled from biologically plausible ranges using uniform sampling. A parameter set consists of 176 parameters spanning 15 known ion channels in <italic>C. elegans</italic>. Once parameter sets are generated, membrane potential responses and current profiles are evaluated for each set to ensure they satisfy the predefined constraints such as minimum offset between membrane potential responses traces and minimum and maximum bounds for steady-state current traces. Only parameter sets that meet the constraints are included in the training set.</p></caption>
<graphic xlink:href="572452v1_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3b">
<label>3.2</label>
<title>Experimental Protocol</title>
<sec id="s3b1">
<title><italic>C. elegans</italic> Culture and Strains</title>
<p>All animals used in this study were maintained at room temperature (22-23°C) on nematode growth medium (NGM) plates seeded with E. coli OP50 bacteria as a food source [<xref ref-type="bibr" rid="c54">54</xref>]. Strains used in this study were: CX7893 kyIs405 (AWB), CX3695 kyIs140 (AWC), ZG611 iaIs19 (URX), EG1285 lin-15B(n765);oxIs12 (RIS), UL2650 leEx2650 (DVC) and CX4857 kyIs179 (HSN).</p>
</sec>
<sec id="s3b2">
<title>Electrophysiology</title>
<p>Electrophysiological recordings were performed on young adult hermaphrodites (∼ 3-days old) at room temperature as previously described [<xref ref-type="bibr" rid="c8">8</xref>]. The gluing and dissection were performed under an Olympus SZX16 stereomicroscope equipped with a 1X Plan Apochromat objective and widefield 10X eyepieces. Briefly, an adult animal was immobilized on a Sylgard-coated (Sylgard 184, Dow Corning) glass coverslip in a small drop of DPBS (D8537; Sigma) by applying a cyanoacrylate adhesive (Vetbond tissue adhesive; 3M) along one side of the body. A puncture in the cuticle away from the incision site was made to relieve hydrostatic pressure. A small longitudinal incision was then made using a diamond dissecting blade (Type M-DL 72029 L; EMS) along the glue line adjacent to the neuron of interest. The cuticle flap was folded back and glued to the coverslip with GLUture Topical Adhesive (Abbott Laboratories), exposing the neuron to be recorded. The coverslip with the dissected preparation was then placed into a custom-made open recording chamber (∼ 1.5 ml volume) and treated with 1 mg/ml collagenase (type IV; Sigma) for 10 s by hand pipetting. The recording chamber was subsequently perfused with the standard extracellular solution using a custom-made gravity-feed perfusion system for ∼10 ml.</p>
<p>All electrophysiological recordings were performed with the bath at room temperature under an upright microscope (Axio Examiner; Carl Zeiss, Inc) equipped with a 40X water immersion lens and 16X eyepieces. Neurons of interest were identified by fluorescent markers and their anatomical positions. Preparations were then switched to the differential interference contrast (DIC) setting for patch-clamp. Electrodes with resistance (RE) of 15-25 MΩ were made from borosilicate glass pipettes (BF100-58-10; Sutter Instruments) using a laser pipette puller (P-2000; Sutter Instruments) and fire-polished with a microforge (MF-830; Narishige). We used a motorized micromanipulator (PatchStar Micromanipulator; Scientifica) to control the electrodes back filled with standard intracellular solution. The standard pipette solution was (all concentrations in mM): [K-gluconate 115; KCl 15; KOH 10; MgCl2 5; CaCl2 0.1; Na2ATP 5; NaGTP 0.5; Na-cGMP 0.5; cAMP 0.5; BAPTA 1; Hepes 10; Sucrose 50], with pH adjusted with KOH to 7.2, osmolarity 320–330 mOsm. The standard extracellular solution was: [NaCl 140; NaOH 5; KCL 5; CaCl2 2; MgCl2 5; Sucrose 15; Hepes 15; Dextrose 25], with pH adjusted with NaOH to 7.3, osmolarity 330–340 mOsm. Liquid junction potentials were calculated and corrected before recording. Whole-cell current clamp and voltage-clamp experiments were conducted on an EPC-10 amplifier (EPC-10 USB; Heka) using PatchMaster software (Heka). Two-component capacitive compensation was optimized at rest, and series resistance was compensated to 50%. Analog data were filtered at 2 kHz and digitized at 10 kHz. Current-injection and voltage-clamp steps were applied through the recording electrode.</p>
</sec>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Discussion</title>
<p>In this work, we introduce a novel deep generative method and system called ElectroPhysiomeGAN (EP-GAN), for estimating Hodgkin-Huxley model (HH-model) parameters given the recordings of neurons with graded potential (non-spiking). The proposed system encompasses RNN Encoder layer to process the neural recordings information such as membrane potential responses and steady-state current profiles and Generator layer to generate large number of HH-model parameters. The system can be trained entirely on simulation data informed by an arbitrary HH-model. When applied to neurons in <italic>C. elegans</italic>, EP-GAN generates parameters of HH-model which membrane potential responses is closer to test membrane potential responses than previous methods such as Differential Evolution and Genetic Algorithms. The advantage of EP-GAN is in both accuracy and inference speed achieved through less number of simulations than the previous methods and generic such that it doesn’t depend on the number of neurons for which inference is to be performed [<xref ref-type="bibr" rid="c16">16</xref>]. In addition, the method also preserves performance when provided with input data with partial information such as missing membrane potential responses (up to 25%) or steady-state current traces (up to 75%).</p>
<p>While EP-GAN is a step forward toward ElectroPhysiome model of <italic>C. elegans</italic>, its inability to support neurons with spiking membrane potential responses remains a limitation. The reason stems from the fact that neurons with spiking membrane potential responses are rare during the generation of training data of HH 15 ionic channels model without Na channels, making their translation strategies to parameter space difficult to learn. A similar limitation is present with bi-stable membrane potential responses, e.g., AWB and AWC, although at lesser extent. While these profiles are not as rare as spiking membrane potential responses, their relative sparseness in the training data tends to cause lower quality of predicted parameters. Indeed, previous studies of <italic>C. elegans</italic> nervous system found that the majority of neurons exhibit graded membrane potential response instead of spiking [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c21">21</xref>]. Furthermore, the limitation could also lie within the current architecture of EP-GAN as it currently processes data directly without a component that discerns and processes spiking membrane potential responses. Improving the sampling strategy for training data alongside enhancement of network architecture could address these limitations in the future.</p>
<p>As discussed in Methods section, it’s worth noting that EP-GAN does not necessarily recover the ground truth parameters that are associated with the input membrane potential responses and steady-state current profiles. This is mainly due to the fact that there may exist multiple parameter regimes for the HH-model which support the given inputs [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c44">44</xref>, <xref ref-type="bibr" rid="c45">45</xref>]. The parameters generated by a single forward pass of EP-GAN (i.e., a single flow of information from the input to the output) could thus be interpreted as a one time sampling from such a regime and a small perturbation to inputs may result in a different set of parameters. Such sensitivity to perturbation could be adjusted by supplementing the training samples or inputs with additional recording data (e.g., multiple recording data per neuron).</p>
<p>As a neural network architecture, EP-GAN allows extensions and modifications to accommodate different configurations of the problem. For instance, update to HH-model would only require retraining of the network without changes to its architecture. Extending the inputs to include additional data, e.g., channel activation profiles, can also be done in a straightforward manner by concatenating them to the input vectors of the encoder network.</p>
<p>Despite its primary focus on <italic>C. elegans</italic> neurons, we believe EP-GAN and its future extension could be viable for modeling a variety of neurons in other organisms. Indeed, there are increasing advances in resolving connectomes of more complex organisms and techniques for large-scale neural activities [<xref ref-type="bibr" rid="c55">55</xref>, <xref ref-type="bibr" rid="c56">56</xref>, <xref ref-type="bibr" rid="c57">57</xref>, <xref ref-type="bibr" rid="c58">58</xref>]. As neurons in these organisms can be described by a generic HH-model or similar differential equation model, EP-GAN is expected to be applicable and make contributions toward developing the biologically detailed nervous system models of neurons in these organisms.</p>
</sec>
<sec id="d1e1150" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1234">
<label>supplementary</label>
<media xlink:href="supplements/572452_file02.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>This work was supported in part by National Science Foundation grant CRCNS IIS-2113003 (JK,ES), Washington Research Fund (ES), CRCNS IIS-2113120 (QL), Kavli NSI Pilot Grant and Hong Kong Research Grants Council RGC/ECS grant (CityU 21103522) (QL), and Chan Zuckerberg Initiative (to Cori Bargmann). Authors also acknowledge the partial support by the Departments of Electrical Computer Engineering (JK,ES), Applied Mathematics (ES), the Center of Computational Neuroscience (ES), and the eScience Institute (ES,JK) at the University of Washington. In addition, we thank Cori Bargmann and Ian Hope for C. elegans strains. Some strains were provided by the CGC, which is funded by NIH Office of Research Infrastructure Programs (P40 OD010440).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><given-names>John G</given-names> <surname>White</surname></string-name>, <string-name><given-names>Eileen</given-names> <surname>Southgate</surname></string-name>, <string-name><given-names>J Nichol</given-names> <surname>Thomson</surname></string-name>, <string-name><given-names>Sydney</given-names> <surname>Brenner</surname></string-name>, <etal>et al.</etal> <article-title>The structure of the nervous system of the nematode caenorhabditis elegans</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>, <volume>314</volume>(<issue>1165</issue>):<fpage>1</fpage>–<lpage>340</lpage>, <year>1986</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><given-names>Lav R</given-names> <surname>Varshney</surname></string-name>, <string-name><given-names>Beth L</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Eric</given-names> <surname>Paniagua</surname></string-name>, <string-name><given-names>David H</given-names> <surname>Hall</surname></string-name>, and <string-name><given-names>Dmitri B</given-names> <surname>Chklovskii</surname></string-name>. <article-title>Structural properties of the caenorhabditis elegans neuronal network</article-title>. <source>PLoS computational biology</source>, <volume>7</volume>(<issue>2</issue>):<fpage>e1001066</fpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><given-names>Steven J</given-names> <surname>Cook</surname></string-name>, <string-name><given-names>Travis A</given-names> <surname>Jarrell</surname></string-name>, <string-name><given-names>Christopher A</given-names> <surname>Brittin</surname></string-name>, <string-name><given-names>Yi</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Adam E</given-names> <surname>Bloniarz</surname></string-name>, <string-name><given-names>Maksim A</given-names> <surname>Yakovlev</surname></string-name>, <string-name><given-names>Ken CQ</given-names> <surname>Nguyen</surname></string-name>, <string-name><given-names>Leo T-H</given-names> <surname>Tang</surname></string-name>, <string-name><given-names>Emily A</given-names> <surname>Bayer</surname></string-name>, <string-name><given-names>Janet S</given-names> <surname>Duerr</surname></string-name>, <etal>et al.</etal> <article-title>Whole-animal connectomes of both caenorhabditis elegans sexes</article-title>. <source>Nature</source>, <volume>571</volume>(<issue>7763</issue>):<fpage>63</fpage>–<lpage>71</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><given-names>Alan L</given-names> <surname>Hodgkin</surname></string-name> and <string-name><given-names>Andrew F</given-names> <surname>Huxley</surname></string-name>. <article-title>A quantitative description of membrane current and its application to conduction and excitation in nerve</article-title>. <source>The Journal of physiology</source>, <volume>117</volume>(<issue>4</issue>):<fpage>500</fpage>, <year>1952</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><given-names>Allan R</given-names> <surname>Willms</surname></string-name>. <article-title>Neurofit: software for fitting hodgkin–huxley models to voltage-clamp data</article-title>. <source>Journal of neuroscience methods</source>, <volume>121</volume>(<issue>2</issue>):<fpage>139</fpage>–<lpage>150</lpage>, <year>2002</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><given-names>Allan R</given-names> <surname>Willms</surname></string-name>, <string-name><given-names>Deborah J</given-names> <surname>Baro</surname></string-name>, <string-name><given-names>Ronald M</given-names> <surname>Harris-Warrick</surname></string-name>, and <string-name><given-names>John</given-names> <surname>Guckenheimer</surname></string-name>. <article-title>An improved parameter estimation method for hodgkin-huxley models</article-title>. <source>Journal of computational neuroscience</source>, <volume>6</volume>:<fpage>145</fpage>–<lpage>168</lpage>, <year>1999</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><string-name><given-names>Martina</given-names> <surname>Nicoletti</surname></string-name>, <string-name><given-names>Alessandro</given-names> <surname>Loppini</surname></string-name>, <string-name><given-names>Letizia</given-names> <surname>Chiodo</surname></string-name>, <string-name><given-names>Viola</given-names> <surname>Folli</surname></string-name>, <string-name><given-names>Giancarlo</given-names> <surname>Ruocco</surname></string-name>, and <string-name><given-names>Simonetta</given-names> <surname>Filippi</surname></string-name>. <article-title>Biophysical modeling of c. elegans neurons: Single ion currents and whole-cell dynamics of awcon and rmd</article-title>. <source>PloS one</source>, <volume>14</volume>(<issue>7</issue>):<fpage>e0218738</fpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><string-name><given-names>Qiang</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Philip B</given-names> <surname>Kidd</surname></string-name>, <string-name><given-names>May</given-names> <surname>Dobosiewicz</surname></string-name>, and <string-name><given-names>Cornelia I</given-names> <surname>Bargmann</surname></string-name>. <article-title>C. elegans awa olfactory neurons fire calcium-mediated all-or-none action potentials</article-title>. <source>Cell</source>, <volume>175</volume>(<issue>1</issue>):<fpage>57</fpage>–<lpage>70</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><given-names>Jingyuan</given-names> <surname>Jiang</surname></string-name>, <string-name><given-names>Yifan</given-names> <surname>Su</surname></string-name>, <string-name><given-names>Ruilin</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Haiwen</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Louis</given-names> <surname>Tao</surname></string-name>, and <string-name><given-names>Qiang</given-names> <surname>Liu</surname></string-name>. <article-title>C. elegans enteric motor neurons fire synchronized action potentials underlying the defecation motor program</article-title>. <source>Nature communications</source>, <volume>13</volume>(<issue>1</issue>):<fpage>2783</fpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><given-names>Eve</given-names> <surname>Marder</surname></string-name> and <string-name><given-names>Jean-Marc</given-names> <surname>Goaillard</surname></string-name>. <article-title>Variability, compensation and homeostasis in neuron and network function</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>7</volume>(<issue>7</issue>):<fpage>563</fpage>–<lpage>574</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><given-names>Eve</given-names> <surname>Marder</surname></string-name> and <string-name><given-names>Adam L</given-names> <surname>Taylor</surname></string-name>. <article-title>Multiple models to capture the variability in biological neurons and networks</article-title>. <source>Nature neuroscience</source>, <volume>14</volume>(<issue>2</issue>):<fpage>133</fpage>–<lpage>138</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><string-name><given-names>Astrid A</given-names> <surname>Prinz</surname></string-name>, <string-name><given-names>Cyrus P</given-names> <surname>Billimoria</surname></string-name>, and <string-name><given-names>Eve</given-names> <surname>Marder</surname></string-name>. <article-title>Alternative to hand-tuning conductance-based models: construction and analysis of databases of model neurons</article-title>. <source>Journal of neurophysiology</source>, <year>2003</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><given-names>Astrid A</given-names> <surname>Prinz</surname></string-name>, <string-name><given-names>Dirk</given-names> <surname>Bucher</surname></string-name>, and <string-name><given-names>Eve</given-names> <surname>Marder</surname></string-name>. <article-title>Similar network activity from disparate circuit parameters</article-title>. <source>Nature neuroscience</source>, <volume>7</volume>(<issue>12</issue>):<fpage>1345</fpage>–<lpage>1352</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><given-names>Laure</given-names> <surname>Buhry</surname></string-name>, <string-name><given-names>Michele</given-names> <surname>Pace</surname></string-name>, and Sylvain Saïghi. <article-title>Global parameter estimation of an hodgkin– huxley formalism using membrane voltage recordings: Application to neuro-mimetic analog integrated circuits</article-title>. <source>Neurocomputing</source>, <volume>81</volume>:<fpage>75</fpage>–<lpage>85</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="book"><string-name><given-names>Juan</given-names> <surname>Luis</surname></string-name> <string-name><given-names>Jiménez</given-names> <surname>Laredo</surname></string-name>, <string-name><given-names>Loïs</given-names> <surname>Naudin</surname></string-name>, <string-name><given-names>Nathalie</given-names> <surname>Corson</surname></string-name>, and <string-name><given-names>Carlos M</given-names> <surname>Fernandes</surname></string-name>. <chapter-title>A methodology for determining ion channels from membrane potential neuronal recordings</chapter-title>. <source>In Applications of Evolutionary Computation: 25th European Conference, EvoApplications 2022, Held as Part of EvoStar 2022, Madrid, Spain, April 20–22, 2022, Proceedings</source>, pages <fpage>15</fpage>–<lpage>29</lpage>. <publisher-name>Springer</publisher-name>, <year>2022</year>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><string-name><given-names>Loïs</given-names> <surname>Naudin</surname></string-name>, <string-name><given-names>Juan</given-names> <surname>Luis</surname></string-name> <string-name><given-names>Jiménez</given-names> <surname>Laredo</surname></string-name>, <string-name><given-names>Qiang</given-names> <surname>Liu</surname></string-name>, and <string-name><given-names>Nathalie</given-names> <surname>Corson</surname></string-name>. <article-title>Systematic generation of biophysically detailed models with generalization capability for non-spiking neurons</article-title>. <source>PloS one</source>, <volume>17</volume>(<issue>5</issue>):<fpage>e0268380</fpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><string-name><given-names>Y Curtis</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Johann</given-names> <surname>Rudi</surname></string-name>, <string-name><given-names>James</given-names> <surname>Velasco</surname></string-name>, <string-name><given-names>Nirvik</given-names> <surname>Sinha</surname></string-name>, <string-name><given-names>Gideon</given-names> <surname>Idumah</surname></string-name>, <string-name><given-names>Randall K</given-names> <surname>Powers</surname></string-name>, <string-name><given-names>Charles J</given-names> <surname>Heckman</surname></string-name>, and <string-name><given-names>Matthieu K</given-names> <surname>Chardon</surname></string-name>. <article-title>Multimodal parameter spaces of a complex multi-channel neuron model</article-title>. <source>Front. Syst. Neurosci.</source> <volume>16</volume> <year>2022</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><string-name><given-names>Jemy A Mandujano</given-names> <surname>Valle</surname></string-name> and <string-name><given-names>Alexandre L</given-names> <surname>Madureira</surname></string-name>. <article-title>Parameter identification problem in the hodgkin-huxley model</article-title>. <source>Neural Computation</source>, <volume>34</volume>(<issue>4</issue>):<fpage>939</fpage>–<lpage>970</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><string-name><given-names>Pedro J</given-names> <surname>Gonçalves</surname></string-name>, <string-name><given-names>Jan-Matthis</given-names> <surname>Lueckmann</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Deistler</surname></string-name>, <string-name><given-names>Marcel</given-names> <surname>Nonnenmacher</surname></string-name>, <string-name><given-names>Kaan</given-names> <surname>Öcal</surname></string-name>, <string-name><given-names>Giacomo</given-names> <surname>Bassetto</surname></string-name>, <string-name><given-names>Chaitanya</given-names> <surname>Chintaluri</surname></string-name>, <string-name><given-names>William F</given-names> <surname>Podlaski</surname></string-name>, <string-name><given-names>Sara A</given-names> <surname>Haddad</surname></string-name>, <string-name><given-names>Tim P</given-names> <surname>Vogels</surname></string-name>, <etal>et al.</etal> <article-title>Training deep neural density estimators to identify mechanistic models of neural dynamics</article-title>. <source>Elife</source>, <volume>9</volume>:<fpage>e56261</fpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="book"><string-name><given-names>Lautaro</given-names> <surname>Estienne</surname></string-name>. <chapter-title>Towards an hybrid hodgkin-huxley action potential generation model</chapter-title>. <source>In 2021 XIX Workshop on Information Processing and Control (RPIC)</source>, pages <fpage>1</fpage>–<lpage>6</lpage>. <publisher-name>IEEE</publisher-name>, <year>2021</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><given-names>Miriam B</given-names> <surname>Goodman</surname></string-name>, <string-name><given-names>David H</given-names> <surname>Hall</surname></string-name>, <string-name><given-names>Leon</given-names> <surname>Avery</surname></string-name>, and <string-name><given-names>Shawn R</given-names> <surname>Lockery</surname></string-name>. <article-title>Active currents regulate sensitivity and dynamic range in c. elegans neurons</article-title>. <source>Neuron</source>, <volume>20</volume>(<issue>4</issue>):<fpage>763</fpage>–<lpage>772</lpage>, <year>1998</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><given-names>Loïs</given-names> <surname>Naudin</surname></string-name>, <string-name><given-names>Nathalie</given-names> <surname>Corson</surname></string-name>, <string-name><given-names>MA</given-names> <surname>Aziz-Alaoui</surname></string-name>, <string-name><given-names>Juan Luis Jimenez</given-names> <surname>Laredo</surname></string-name>, and <string-name><given-names>Thibaut</given-names> <surname>Démare</surname></string-name>. <article-title>On the modeling of the three types of non-spiking neurons of the caenorhabditis elegans</article-title>. <source>International Journal of Neural Systems</source>, <volume>31</volume>(<issue>02</issue>):<fpage>2050063</fpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><given-names>Kristin</given-names> <surname>Koch</surname></string-name>, <string-name><given-names>Judith</given-names> <surname>McLean</surname></string-name>, <string-name><given-names>Ronen</given-names> <surname>Segev</surname></string-name>, <string-name><given-names>Michael A</given-names> <surname>Freed</surname></string-name>, <string-name><given-names>Michael J</given-names> <surname>Berry</surname></string-name>, <string-name><given-names>Vijay</given-names> <surname>Balasubramanian</surname></string-name>, and <string-name><given-names>Peter</given-names> <surname>Sterling</surname></string-name>. <article-title>How much the eye tells the brain</article-title>. <source>Current biology</source>, <volume>16</volume>(<issue>14</issue>):<fpage>1428</fpage>–<lpage>1434</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="book"><string-name><given-names>Alan</given-names> <surname>Roberts</surname></string-name> and <string-name><given-names>Brian MH</given-names> <surname>Bush</surname></string-name>. <source>Neurones without impulses: their significance for vertebrate and invertebrate nervous systems</source>, volume <volume>6</volume>. <publisher-name>Cambridge University Press</publisher-name>, <year>1981</year>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><string-name><given-names>RE</given-names> <surname>Davis</surname></string-name> and <string-name><given-names>AO</given-names> <surname>Stretton</surname></string-name>. <article-title>Passive membrane properties of motorneurons and their role in long-distance signaling in the nematode ascaris</article-title>. <source>Journal of Neuroscience</source>, <volume>9</volume>(<issue>2</issue>):<fpage>403</fpage>–<lpage>414</lpage>, <year>1989</year>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><string-name><given-names>Ralph E</given-names> <surname>Davis</surname></string-name> and <string-name><given-names>AO</given-names> <surname>Stretton</surname></string-name>. <article-title>Signaling properties of ascaris motorneurons: graded active responses, graded synaptic transmission, and tonic transmitter release</article-title>. <source>Journal of Neuroscience</source>, <volume>9</volume>(<issue>2</issue>):<fpage>415</fpage>–<lpage>425</lpage>, <year>1989</year>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Burrows</surname></string-name>, <string-name><given-names>GJ</given-names> <surname>Laurent</surname></string-name>, and <string-name><given-names>LH</given-names> <surname>Field</surname></string-name>. <article-title>Proprioceptive inputs to nonspiking local interneurons contribute to local reflexes of a locust hindleg</article-title>. <source>Journal of Neuroscience</source>, <volume>8</volume>(<issue>8</issue>):<fpage>3085</fpage>–<lpage>3093</lpage>, <year>1988</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><string-name><given-names>Gilles</given-names> <surname>Laurent</surname></string-name> and <string-name><given-names>Malcolm</given-names> <surname>Burrows</surname></string-name>. <article-title>Distribution of intersegmental inputs to nonspiking local interneurons and motor neurons in the locust</article-title>. <source>Journal of Neuroscience</source>, <volume>9</volume>(<issue>9</issue>):<fpage>3019</fpage>–<lpage>3029</lpage>, <year>1989</year>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><string-name><given-names>Gilles</given-names> <surname>Laurent</surname></string-name> and <string-name><given-names>M</given-names> <surname>Burrows</surname></string-name>. <article-title>Intersegmental interneurons can control the gain of reflexes in adjacent segments of the locust by their action on nonspiking local interneurons</article-title>. <source>Journal of Neuroscience</source>, <volume>9</volume>(<issue>9</issue>):<fpage>3030</fpage>–<lpage>3039</lpage>, <year>1989</year>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><string-name><given-names>Loïs</given-names> <surname>Naudin</surname></string-name>. <article-title>Biological emergent properties in non-spiking neural networks</article-title>. <source>AIMS Mathematics</source>, <volume>7</volume>(<issue>10</issue>):<fpage>19415</fpage>–<lpage>19439</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="book"><string-name><given-names>Kalyanmoy</given-names> <surname>Deb</surname></string-name>, <string-name><given-names>Samir</given-names> <surname>Agrawal</surname></string-name>, <string-name><given-names>Amrit</given-names> <surname>Pratap</surname></string-name>, and <string-name><given-names>Tanaka</given-names> <surname>Meyarivan</surname></string-name>. <chapter-title>A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: Nsga-ii</chapter-title>. <source>In Parallel Problem Solving from Nature PPSN VI: 6th International Conference Paris, France, September 18–20, 2000 Proceedings 6</source>, pages <fpage>849</fpage>–<lpage>858</lpage>. <publisher-name>Springer</publisher-name>, <year>2000</year>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><string-name><given-names>Etay</given-names> <surname>Hay</surname></string-name>, <string-name><given-names>Sean</given-names> <surname>Hill</surname></string-name>, <string-name><given-names>Felix</given-names> <surname>Schürmann</surname></string-name>, <string-name><given-names>Henry</given-names> <surname>Markram</surname></string-name>, and <string-name><given-names>Idan</given-names> <surname>Segev</surname></string-name>. <article-title>Models of neocortical layer 5b pyramidal cells capturing a wide range of dendritic and perisomatic active properties</article-title>. <source>PLoS computational biology</source>, <volume>7</volume>(<issue>7</issue>):<fpage>e1002107</fpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><given-names>Werner</given-names> <surname>Van Geit</surname></string-name>, <string-name><given-names>Erik</given-names> <surname>De Schutter</surname></string-name>, and <string-name><given-names>Pablo</given-names> <surname>Achard</surname></string-name>. <article-title>Automated neuron model optimization techniques: a review</article-title>. <source>Biological cybernetics</source>, <volume>99</volume>:<fpage>241</fpage>–<lpage>251</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="book"><string-name><given-names>Tea Robič and Bogdan</given-names> <surname>Filipič</surname></string-name>. <chapter-title>Differential evolution for multiobjective optimization</chapter-title>. <source>In Evolutionary Multi-Criterion Optimization: Third International Conference, EMO 2005, Guanajuato, Mexico, March 9-11, 2005. Proceedings 3</source>, pages <fpage>520</fpage>–<lpage>533</lpage>. <publisher-name>Springer</publisher-name>, <year>2005</year>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="book"><string-name><given-names>Saku</given-names> <surname>Kukkonen</surname></string-name> and <string-name><given-names>Jouni</given-names> <surname>Lampinen</surname></string-name>. <chapter-title>Gde3: The third evolution step of generalized differential evolution</chapter-title>. <source>In 2005 IEEE congress on evolutionary computation</source>, volume <volume>1</volume>, pages <fpage>443</fpage>–<lpage>450</lpage>. <publisher-name>IEEE</publisher-name>, <year>2005</year>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="confproc"><string-name><given-names>Rakesh</given-names> <surname>Angira</surname></string-name> and <string-name><given-names>BV</given-names> <surname>Babu</surname></string-name>. <article-title>Non-dominated sorting differential evolution (nsde): An extension of differential evolution for multi-objective optimization</article-title>. In <conf-name>IICAI</conf-name>, pages <fpage>1428</fpage>–<lpage>1443</lpage>, <year>2005</year>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><string-name><given-names>Timothy H</given-names> <surname>Rumbell</surname></string-name>, <string-name><given-names>Danel</given-names> <surname>Draguljić</surname></string-name>, <string-name><given-names>Aniruddha</given-names> <surname>Yadav</surname></string-name>, <string-name><given-names>Patrick R</given-names> <surname>Hof</surname></string-name>, <string-name><given-names>Jennifer I</given-names> <surname>Luebke</surname></string-name>, and <string-name><given-names>Christina M</given-names> <surname>Weaver</surname></string-name>. <article-title>Automated evolutionary optimization of ion channel conductances and kinetics in models of young and aged rhesus monkey pyramidal neurons</article-title>. <source>Journal of computational neuroscience</source>, <volume>41</volume>:<fpage>65</fpage>–<lpage>90</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><string-name><given-names>J Christopher</given-names> <surname>Octeau</surname></string-name>, <string-name><given-names>Mohitkumar R</given-names> <surname>Gangwani</surname></string-name>, <string-name><given-names>Sushmita L</given-names> <surname>Allam</surname></string-name>, <string-name><given-names>Duy</given-names> <surname>Tran</surname></string-name>, <string-name><given-names>Shuhan</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>Tuan M</given-names> <surname>Hoang-Trong</surname></string-name>, <string-name><given-names>Peyman</given-names> <surname>Golshani</surname></string-name>, <string-name><given-names>Timothy H</given-names> <surname>Rumbell</surname></string-name>, <string-name><given-names>James R</given-names> <surname>Kozloski</surname></string-name>, and <string-name><given-names>Baljit S</given-names> <surname>Khakh</surname></string-name>. <article-title>Transient, consequential increases in extracellular potassium ions accompany channelrhodopsin2 excitation</article-title>. <source>Cell reports</source>, <volume>27</volume>(<issue>8</issue>):<fpage>2249</fpage>–<lpage>2261</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="book"><string-name><given-names>Laure</given-names> <surname>Buhry</surname></string-name>, <string-name><given-names>Audrey</given-names> <surname>Giremus</surname></string-name>, <string-name><given-names>Eric</given-names> <surname>Grivel</surname></string-name>, <string-name><given-names>Sylvain</given-names> <surname>Saïghi</surname></string-name>, and <string-name><given-names>Sylvie</given-names> <surname>Renaud</surname></string-name>. <chapter-title>New variants of the differential evolution algorithm: application for neuroscientists</chapter-title>. <source>In 2009 17th European Signal Processing Conference</source>, pages <fpage>2352</fpage>–<lpage>2356</lpage>. <publisher-name>IEEE</publisher-name>, <year>2009</year>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><string-name><given-names>Julian</given-names> <surname>Blank</surname></string-name> and <string-name><given-names>Kalyanmoy</given-names> <surname>Deb</surname></string-name>. <article-title>Pymoo: Multi-objective optimization in python</article-title>. <source>IEEE Access</source>, <volume>8</volume>:<fpage>89497</fpage>–<lpage>89509</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="book"><string-name><given-names>Eugene M</given-names> <surname>Izhikevich</surname></string-name>. <source>Dynamical systems in neuroscience</source>. <publisher-name>MIT press</publisher-name>, <year>2007</year>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><string-name><given-names>Loïs</given-names> <surname>Naudin</surname></string-name>, <string-name><given-names>Juan</given-names> <surname>Luis</surname></string-name> <string-name><given-names>Jiménez</given-names> <surname>Laredo</surname></string-name>, and <string-name><given-names>Nathalie</given-names> <surname>Corson</surname></string-name>. <article-title>A simple model of nonspiking neurons</article-title>. <source>Neural Computation</source>, <volume>34</volume>(<issue>10</issue>):<fpage>2075</fpage>–<lpage>2101</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><string-name><given-names>Jinming</given-names> <surname>Zou</surname></string-name>, <string-name><given-names>Yi</given-names> <surname>Han</surname></string-name>, and <string-name><given-names>Sung-Sau</given-names> <surname>So</surname></string-name>. <article-title>Overview of artificial neural networks</article-title>. <source>Artificial neural networks: methods and applications</source>, pages <fpage>14</fpage>–<lpage>22</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><string-name><given-names>Ashley E</given-names> <surname>Raba</surname></string-name>, <string-name><given-names>Jonathan M</given-names> <surname>Cordeiro</surname></string-name>, <string-name><given-names>Charles</given-names> <surname>Antzelevitch</surname></string-name>, and <string-name><given-names>Jacques</given-names> <surname>Beaumont</surname></string-name>. <article-title>Extending the conditions of application of an inversion of the hodgkin–huxley gating model</article-title>. <source>Bulletin of mathematical biology</source>, <volume>75</volume>:<fpage>752</fpage>–<lpage>773</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><string-name><given-names>Loïs</given-names> <surname>Naudin</surname></string-name>. <article-title>Different parameter solutions of a conductance-based model that behave identically are not necessarily degenerate</article-title>. <source>Journal of Computational Neuroscience</source>, pages <fpage>1</fpage>–<lpage>6</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><string-name><given-names>Ian</given-names> <surname>Goodfellow</surname></string-name>, <string-name><given-names>Jean</given-names> <surname>Pouget-Abadie</surname></string-name>, <string-name><given-names>Mehdi</given-names> <surname>Mirza</surname></string-name>, <string-name><given-names>Bing</given-names> <surname>Xu</surname></string-name>, <string-name><given-names>David</given-names> <surname>Warde-Farley</surname></string-name>, <string-name><given-names>Sherjil</given-names> <surname>Ozair</surname></string-name>, <string-name><given-names>Aaron</given-names> <surname>Courville</surname></string-name>, and <string-name><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name>. <article-title>Generative adversarial networks</article-title>. <source>Communications of the ACM</source>, <volume>63</volume>(<issue>11</issue>):<fpage>139</fpage>–<lpage>144</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="book"><string-name><given-names>Martin</given-names> <surname>Arjovsky</surname></string-name>, <string-name><given-names>Soumith</given-names> <surname>Chintala</surname></string-name>, and <string-name><given-names>Léon</given-names> <surname>Bottou</surname></string-name>. <chapter-title>Wasserstein generative adversarial networks</chapter-title>. <source>In International conference on machine learning</source>, pages <fpage>214</fpage>–<lpage>223</lpage>. <publisher-name>PMLR</publisher-name>, <year>2017</year>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="preprint"><string-name><given-names>Kyunghyun</given-names> <surname>Cho</surname></string-name>, <string-name><given-names>Bart Van</given-names> <surname>Merriënboer</surname></string-name>, <string-name><given-names>Caglar</given-names> <surname>Gulcehre</surname></string-name>, <string-name><given-names>Dzmitry</given-names> <surname>Bahdanau</surname></string-name>, <string-name><given-names>Fethi</given-names> <surname>Bougares</surname></string-name>, <string-name><given-names>Holger</given-names> <surname>Schwenk</surname></string-name>, and <string-name><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name>. <article-title>Learning phrase representations using rnn encoder-decoder for statistical machine translation</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1406.1078</pub-id>, <year>2014</year>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><string-name><given-names>Ishaan</given-names> <surname>Gulrajani</surname></string-name>, <string-name><given-names>Faruk</given-names> <surname>Ahmed</surname></string-name>, <string-name><given-names>Martin</given-names> <surname>Arjovsky</surname></string-name>, <string-name><given-names>Vincent</given-names> <surname>Dumoulin</surname></string-name>, and <string-name><given-names>Aaron C</given-names> <surname>Courville</surname></string-name>. <article-title>Improved training of wasserstein gans</article-title>. <source>Advances in neural information processing systems</source>, <volume>30</volume>, <year>2017</year>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><string-name><given-names>Aladin</given-names> <surname>Virmaux</surname></string-name> and <string-name><given-names>Kevin</given-names> <surname>Scaman</surname></string-name>. <article-title>Lipschitz regularity of deep neural networks: analysis and efficient estimation</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>31</volume>, <year>2018</year>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><string-name><given-names>George Em</given-names> <surname>Karniadakis</surname></string-name>, <string-name><given-names>Ioannis G</given-names> <surname>Kevrekidis</surname></string-name>, <string-name><given-names>Lu</given-names> <surname>Lu</surname></string-name>, <string-name><given-names>Paris</given-names> <surname>Perdikaris</surname></string-name>, <string-name><given-names>Sifan</given-names> <surname>Wang</surname></string-name>, and <string-name><given-names>Liu</given-names> <surname>Yang</surname></string-name>. <article-title>Physics-informed machine learning</article-title>. <source>Nature Reviews Physics</source>, <volume>3</volume>(<issue>6</issue>):<fpage>422</fpage>–<lpage>440</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><string-name><given-names>Maziar</given-names> <surname>Raissi</surname></string-name>, <string-name><given-names>Paris</given-names> <surname>Perdikaris</surname></string-name>, and <string-name><given-names>George E</given-names> <surname>Karniadakis</surname></string-name>. <article-title>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</article-title>. <source>Journal of Computational physics</source>, <volume>378</volume>:<fpage>686</fpage>–<lpage>707</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="book"><string-name><given-names>James</given-names> <surname>Stewart</surname></string-name>, <string-name><given-names>Daniel K</given-names> <surname>Clegg</surname></string-name>, and <string-name><given-names>Saleem</given-names> <surname>Watson</surname></string-name>. <source>Calculus: early transcendentals</source>. <publisher-name>Cengage Learning</publisher-name>, <year>2020</year>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><string-name><given-names>Sydney</given-names> <surname>Brenner</surname></string-name>. <article-title>The genetics of caenorhabditis elegans</article-title>. <source>Genetics</source>, <volume>77</volume>(<issue>1</issue>):<fpage>71</fpage>–<lpage>94</lpage>, <year>1974</year>.</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><string-name><given-names>Paul</given-names> <surname>Brooks</surname></string-name>, <string-name><given-names>Andrew</given-names> <surname>Champion</surname></string-name>, and <string-name><given-names>Marta</given-names> <surname>Costa</surname></string-name>. <article-title>Mapping of the zebrafish brain takes shape</article-title>. <source>Nature Methods</source>, pages <fpage>1</fpage>–<lpage>2</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="journal"><string-name><given-names>Michael</given-names> <surname>Winding</surname></string-name>, <string-name><given-names>Benjamin D</given-names> <surname>Pedigo</surname></string-name>, <string-name><given-names>Christopher L</given-names> <surname>Barnes</surname></string-name>, <string-name><given-names>Heather G</given-names> <surname>Patsolic</surname></string-name>, <string-name><given-names>Youngser</given-names> <surname>Park</surname></string-name>, <string-name><given-names>Tom</given-names> <surname>Kazimiers</surname></string-name>, <string-name><given-names>Akira</given-names> <surname>Fushiki</surname></string-name>, <string-name><given-names>Ingrid V</given-names> <surname>Andrade</surname></string-name>, <string-name><given-names>Avinash</given-names> <surname>Khandelwal</surname></string-name>, <string-name><given-names>Javier</given-names> <surname>Valdes-Aleman</surname></string-name>, <etal>et al.</etal> <article-title>The connectome of an insect brain</article-title>. <source>Science</source>, <volume>379</volume>(<issue>6636</issue>):<fpage>eadd9330</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><string-name><given-names>Seung Wook</given-names> <surname>Oh</surname></string-name>, <string-name><given-names>Julie A</given-names> <surname>Harris</surname></string-name>, <string-name><given-names>Lydia</given-names> <surname>Ng</surname></string-name>, <string-name><given-names>Brent</given-names> <surname>Winslow</surname></string-name>, <string-name><given-names>Nicholas</given-names> <surname>Cain</surname></string-name>, <string-name><given-names>Stefan</given-names> <surname>Mihalas</surname></string-name>, <string-name><given-names>Quanxin</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Chris</given-names> <surname>Lau</surname></string-name>, <string-name><given-names>Leonard</given-names> <surname>Kuan</surname></string-name>, <string-name><given-names>Alex M</given-names> <surname>Henry</surname></string-name>, <etal>et al.</etal> <article-title>A mesoscale connectome of the mouse brain</article-title>. <source>Nature</source>, <volume>508</volume>(<issue>7495</issue>):<fpage>207</fpage>–<lpage>214</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="journal"><string-name><given-names>Nicholas James</given-names> <surname>Sofroniew</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Flickinger</surname></string-name>, <string-name><given-names>Jonathan</given-names> <surname>King</surname></string-name>, and <string-name><given-names>Karel</given-names> <surname>Svoboda</surname></string-name>. <article-title>A large field of view two-photon mesoscope with subcellular resolution for in vivo imaging</article-title>. <source>elife</source>, <volume>5</volume>:<fpage>e14472</fpage>, <year>2016</year>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95607.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bhalla</surname>
<given-names>Upinder S</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>National Centre for Biological Sciences</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>The study by Kim et al. is a <bold>valuable</bold> contribution to the topic of obtaining good channel conductance parameters from electrophysiological recordings. While promising in its ability to rapidly construct newly fitted models using generative adversarial networks, the approach is <bold>incompletely</bold> described and the generated models often substantially deviate from the dynamics observed empirically. The comparison with existing multi-objective optimization methods is also <bold>incomplete</bold>.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95607.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The manuscript describes a GAN-based approach that generates parameters for HH-like channels for multiple C. Elengans neurons. The network is trained on generated data to produce parameter sets that, on the one hand, reproduce voltage responses and IV curves, and on the other hand, are indistinguishable from the ground truth parameters, as tested by the discriminator. It is then shown that these generated parameter sets lead to reasonable reproductions of the recorded responses (but see the section &quot;weaknesses&quot; below for some reservations).</p>
<p>Strengths:</p>
<p>In itself, I find the methodology of high interest, particularly in that it can generate parameter sets to construct models of new recordings at a very low computational cost.</p>
<p>Weaknesses:</p>
<p>Nevertheless, I believe there are some weaknesses in the evaluation of the models that should be addressed before the quality of the methodology can be fully assessed. Firstly, at the methodological level, the authors should provide more clarity on the inverse gradient operation they use, as opposed to just simulating the models, as such an inversion depends not only on the parameters but also on the state of the model. How the state is obtained remains unclear here. Secondly, in the evaluation of their models, the authors could provided more information such as IV curves, as whether these would be accurate is difficult to visually infer from their figures. Thirdly, the authors do not address the question of whether all obtained parameter sets are stable when simulated over longer times, while their figures do include hints that this might not be the case for at least some of their models (e.g. voltage traces that do not converge back to the equilibrium after the stimulus, but rather seem to diverge).</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95607.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Generating biophysically detailed computational models that capture the characteristic physiological properties of biological neurons for diverse cell types is an important and difficult problem in computational neuroscience. One major challenge lies in determining the large number of parameters of such models, which are notoriously difficult to fit into experimental data. Thereby, the computational and energy costs can be significant. The study 'ElectroPhysiomeGAN: Generation of Biophysical Neuron Model Parameters from Recorded Electrophysiological Responses' by Kim et al. describes a computationally efficient approach for predicting model parameters of Hodgkin-Huxley neuron models using Generative Adversarial Networks (GANs) trained on simulation data. The method is applied to generate models for 9 non-spiking neurons in C. elegans based on electrophysiological recordings. While the generated models capture the responses of these neurons to some degree, they generally show significant deviations from the empirically observed responses in important features. While interesting, in its current form, the method has not been demonstrated to generate models that faithfully capture empirically observed responses.</p>
<p>Strengths:</p>
<p>The authors work on an important and difficult problem. A noteworthy strength of their approach is that once trained, the GANs can generate models from new empirical data with very little computational effort. The generated models reproduce the average voltage during current injections reasonably well.</p>
<p>Weaknesses:</p>
<p>Major 1: While the models generated with EP-GAN reproduce the average voltage during current injections reasonably well, the dynamics of the response are not well captured. For example, for the neuron labeled RIM (Figure 2), the most depolarized voltage traces show an initial 'overshoot' of depolarization, i.e. they depolarize strongly within the first few hundred milliseconds but then fall back to a less depolarized membrane potential. In contrast, the empirical recording shows no such overshoot. Similarly, for the neuron labeled AFD, all empirically recorded traces slowly ramp up over time. In contrast, the simulated traces are mostly flat. Furthermore, all empirical traces return to the pre-stimulus membrane potential, but many of the simulated voltage traces remain significantly depolarized, far outside of the ranges of empirically observed membrane potentials. While these deviations may appear small in the Root mean Square Error (RMSE), the only metric used in the study to assess the quality of the models, they likely indicate a large mismatch between the model and the electrophysiological properties of the biological neuron.</p>
<p>Major 2: Other metrics than the RMSE should be incorporated to validate simulated responses against electrophysiological data. A common approach is to extract multiple biologically meaningful features from the voltage traces before, during and after the stimulus, and compare the simulated responses to the experimentally observed distribution of these features. Typically, a model is only accepted if all features fall within the empirically observed ranges (see e.g. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002107">https://doi.org/10.1371/journal.pcbi.1002107</ext-link>). However, based on the deviations in resting membrane potential and the return to the resting membrane potential alone, most if not all the models shown in this study would not be accepted.</p>
<p>Major 3: Abstract and introduction imply that the 'ElectroPhysiome' refers to models that incorporate both the connectome and individual neuron physiology. However, the work presented in this study does not make use of any connectomics data. To make the claim that ElectroPhysiomeGAN can jointly capture both 'network interaction and cellular dynamics', the generated models would need to be evaluated for network inputs, for example by exposing them to naturalistic stimuli of synaptic inputs. It seems likely that dynamics that are currently poorly captured, like slow ramps, or the ability of the neuron to return to its resting membrane potential, will critically affect network computations.</p>
</body>
</sub-article>
</article>