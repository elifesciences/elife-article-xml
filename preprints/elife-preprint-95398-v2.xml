<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">95398</article-id>
<article-id pub-id-type="doi">10.7554/eLife.95398</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95398.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Multimodal mismatch responses in mouse auditory cortex</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2969-2963</contrib-id>
<name>
<surname>Solyga</surname>
<given-names>Magdalena</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1401-0117</contrib-id>
<name>
<surname>Keller</surname>
<given-names>Georg B</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>georg.keller@fmi.ch</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01bmjkv45</institution-id><institution>Friedrich Miescher Institute for Biomedical Research</institution></institution-wrap>, <city>Basel</city>, <country>Switzerland</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s6k3f65</institution-id><institution>Faculty of Science, University of Basel</institution></institution-wrap>, <city>Basel</city>, <country>Switzerland</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>King</surname>
<given-names>Andrew J</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>King</surname>
<given-names>Andrew J</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="original-publication" iso-8601-date="2024-03-22">
<day>22</day>
<month>03</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2024-11-18">
<day>18</day>
<month>11</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP95398</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-01-16">
<day>16</day>
<month>01</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-12-14">
<day>14</day>
<month>12</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.10.29.564593"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-03-22">
<day>22</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95398.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.95398.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95398.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95398.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95398.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Solyga &amp; Keller</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Solyga &amp; Keller</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-95398-v2.pdf"/>
<abstract>
<p>Our movements result in predictable sensory feedback that is often multimodal. Based on deviations between predictions and actual sensory input, primary sensory areas of cortex have been shown to compute sensorimotor prediction errors. How prediction errors in one sensory modality influence the computation of prediction errors in another modality is still unclear. To investigate multimodal prediction errors in mouse auditory cortex (ACx), we used a virtual environment to experimentally couple running to both self-generated auditory and visual feedback. Using two-photon microscopy, we first characterized responses of layer 2/3 (L2/3) neurons to sounds, visual stimuli, and running onsets and found responses to all three stimuli. Probing responses evoked by audiomotor mismatches, we found that they closely resemble visuomotor mismatch responses in visual cortex (V1). Finally, testing for cross modal influence on audiomotor mismatch responses by coupling both sound amplitude and visual flow speed to the speed of running, we found that audiomotor mismatch responses were amplified when paired with concurrent visuomotor mismatches. Our results demonstrate that multimodal and non-hierarchical interactions shape prediction error responses in cortical L2/3.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>1. Additional analysis of running onsets in closed and open loop conditions for audiomotor (Figure 2H) and visuomotor (Figure 3H) coupling. 2. Analysis of running speed and pupil dilation upon mismatch presentation (Figures S2A and S2B, S4A and S4B, and S5A and S5B). 3. Expanded discussion of the nature of differences between audiomotor and visuomotor mismatches.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Neuronal responses consistent with prediction errors have been described in a variety of different cortical areas (<xref ref-type="bibr" rid="c2">Audette and Schneider, 2023</xref>; <xref ref-type="bibr" rid="c4">Ayaz et al., 2019</xref>; <xref ref-type="bibr" rid="c12">Han and Helmchen, 2023</xref>; <xref ref-type="bibr" rid="c13">Heindorf et al., 2018</xref>; <xref ref-type="bibr" rid="c19">Keller et al., 2012</xref>; <xref ref-type="bibr" rid="c24">Liu and Kanold, 2022</xref>) and across different species (<xref ref-type="bibr" rid="c9">Eliades and Wang, 2008</xref>; <xref ref-type="bibr" rid="c20">Keller and Hahnloser, 2009</xref>; <xref ref-type="bibr" rid="c37">Stanley and Miall, 2007</xref>). In V1, these responses are thought to be learned with experience (<xref ref-type="bibr" rid="c1">Attinger et al., 2017</xref>) and depend on local plasticity in cortex (<xref ref-type="bibr" rid="c42">Widmer et al., 2022</xref>). Prediction errors signal the unanticipated appearance or absence of a sensory input, and are thought to be computed as a deviation between top-down predictions and bottom-up sensory inputs (<xref ref-type="bibr" rid="c31">Rao and Ballard, 1999</xref>). One type of top-down signals that conveys a prediction of sensory input are motor-related signals (<xref ref-type="bibr" rid="c22">Leinweber et al., 2017</xref>). In auditory cortex, motor-related signals can modulate responses to self-generated vocalizations (<xref ref-type="bibr" rid="c9">Eliades and Wang, 2008</xref>) or to sounds coupled to locomotion (<xref ref-type="bibr" rid="c35">Schneider et al., 2018</xref>). Auditory cortex is thought to use these motor-related signals to compute audiomotor prediction error responses (<xref ref-type="bibr" rid="c3">Audette et al., 2022</xref>; <xref ref-type="bibr" rid="c2">Audette and Schneider, 2023</xref>; <xref ref-type="bibr" rid="c9">Eliades and Wang, 2008</xref>; <xref ref-type="bibr" rid="c20">Keller and Hahnloser, 2009</xref>; <xref ref-type="bibr" rid="c24">Liu and Kanold, 2022</xref>). These audiomotor prediction errors can be described in a hierarchical variant of predictive processing, in which top-down motor-related signals function as predictions of bottom-up sensory input. While parts of both auditory and visual processing streams are well described by a hierarchy, the cortical network as a whole does not easily map onto a hierarchical architecture, anatomically (<xref ref-type="bibr" rid="c25">Markov et al., 2013</xref>) or functionally (<xref ref-type="bibr" rid="c38">St-Yves et al., 2023</xref>; <xref ref-type="bibr" rid="c39">Suzuki et al., 2023</xref>). One of the connections that does not neatly fit into a hierarchical model is the surprisingly dense reciprocal connection between ACx and V1 (<xref ref-type="bibr" rid="c7">Clavagnier et al., 2004</xref>; <xref ref-type="bibr" rid="c10">Falchier et al., 2002</xref>; <xref ref-type="bibr" rid="c16">Ibrahim et al., 2016</xref>; <xref ref-type="bibr" rid="c22">Leinweber et al., 2017</xref>; <xref ref-type="bibr" rid="c45">Zhao et al., 2022</xref>). From ACx to V1 this connection conveys a prediction of visual input given sound (<xref ref-type="bibr" rid="c11">Garner and Keller, 2022</xref>). What the reciprocal projection from V1 to ACx conveys, is still unclear. In proposals for hierarchical implementations of predictive processing there are no such lateral connections, and there is no reason to assume prediction error computations in different modalities should directly interact at the level of primary sensory areas. Thus, we argued that the lateral interaction between V1 and ACx is a good starting point to investigate how non-hierarchical interactions are involved in the computation of prediction errors, and how multimodal interactions shape sensorimotor prediction errors.</p>
<p>Based on this idea, we designed an experiment in which we could couple and transiently decouple running speed in a virtual environment to both self-generated auditory feedback and self-generated visual flow feedback. While doing this, we recorded activity in L2/3 neurons of ACx using two-photon calcium imaging. Using this approach, we first confirmed that a substantial subset of L2/3 neurons in ACx responds to either auditory, visual (Sharma et al., 2021), or motor-related inputs (<xref ref-type="bibr" rid="c15">Henschke et al., 2021</xref>; <xref ref-type="bibr" rid="c27">Morandell et al., 2023</xref>; <xref ref-type="bibr" rid="c41">Vivaldo et al., 2023</xref>). While we found that L2/3 neurons in ACx responded to audiomotor mismatches in a way that closely resembles visuomotor mismatch responses found in V1 (<xref ref-type="bibr" rid="c19">Keller et al., 2012</xref>), we found no evidence of responses to visuomotor mismatch in ACx. However, when coupling both visual flow and auditory feedback to running, we found that L2/3 neurons in ACx non-linearly combine information about visuomotor and audiomotor mismatches. Overall, our results demonstrate that prediction errors can be potentiated by multimodal interactions in primary sensory cortices.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Auditory, visual, and motor-related signals were intermixed in L2/3 of ACx</title>
<p>To investigate auditory, visual, and motor-related signals in mouse ACx, we combined an audiovisual virtual reality (VR) system with two-photon calcium imaging in L2/3 ACx neurons (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). We used an adeno-associated viral (AAV) vector to express a genetically encoded calcium indicator (AAV2/1-EF1α-GCaMP6f-WPRE) in ACx (<xref rid="fig1" ref-type="fig">Figures 1B-D</xref>). Following recovery from surgery, mice were habituated to the virtual reality setup (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). We first mapped the location of the primary auditory cortex (A1) and the anterior auditory field (AAF) using widefield calcium imaging (<xref rid="fig1" ref-type="fig">Figures 1C</xref> and <xref rid="fig1" ref-type="fig">1E</xref>). Based on these maps, we then chose recording locations for two-photon imaging of L2/3 neurons in either A1 or AAF. For the purposes of this work, we did not distinguish between A1 and AAF and will refer to these two areas here as ACx. To characterize basic sensory and motor-related responses, we recorded neuronal responses to pure tones, full-field moving gratings, and running onsets. Throughout all experiments mice were free to run, and auditory and visual responses were pooled across both sitting and running conditions (unless explicitly stated otherwise). We first assessed population responses of L2/3 neurons evoked by sounds (pure tones presented at 4 kHz, 8 kHz, 16 kHz, or 32 kHz at either 60 dB or 75 dB sound pressure level (SPL); <xref rid="fig1" ref-type="fig">Figure 1F</xref>) presented while the VR was off. While pure tones resulted in both increases or decreases in calcium activity in individual neurons (<xref rid="fig1" ref-type="fig">Figure 1G</xref>), the average population response exhibited a significant decrease in activity (<xref rid="fig1" ref-type="fig">Figure 1H</xref>). Next, we analyzed visual responses evoked by full-field drifting gratings (see Methods; <xref rid="fig1" ref-type="fig">Figure 1I</xref>). Visual stimulation resulted in a diverse response across the population of L2/3 neurons (<xref rid="fig1" ref-type="fig">Figure 1J</xref>) that was initially positive at the population level (<xref rid="fig1" ref-type="fig">Figure 1K</xref>). To quantify motor-related inputs, we analyzed activity during running onsets collected across all experimental conditions (<xref rid="fig1" ref-type="fig">Figure 1L</xref>). We found that the majority of neurons increased their activity during running onsets (<xref rid="fig1" ref-type="fig">Figure 1M</xref>), which was also reflected in a significant positive response on the population level (<xref rid="fig1" ref-type="fig">Figure 1N</xref>). Finally, we investigated how running modulates auditory and visual responses in ACx. In V1, running strongly increases responses to visual stimuli (<xref ref-type="bibr" rid="c29">Niell and Stryker, 2010</xref>), while in auditory cortex running has been shown to modulate auditory responses in a variety of different ways (<xref ref-type="bibr" rid="c3">Audette et al., 2022</xref>; <xref ref-type="bibr" rid="c5">Bigelow et al., 2019</xref>; <xref ref-type="bibr" rid="c15">Henschke et al., 2021</xref>; <xref ref-type="bibr" rid="c26">McGinley et al., 2015</xref>; <xref ref-type="bibr" rid="c27">Morandell et al., 2023</xref>; <xref ref-type="bibr" rid="c34">Schneider et al., 2014</xref>; <xref ref-type="bibr" rid="c41">Vivaldo et al., 2023</xref>; <xref ref-type="bibr" rid="c43">Yavorska and Wehr, 2021</xref>; <xref ref-type="bibr" rid="c46">Zhou et al., 2014</xref>). Separating auditory responses by running state, we found that sound evoked responses of ACx neurons were overall similar during sitting and running, but exhibited a smaller decrease in activity when the mouse was sitting (<xref ref-type="fig" rid="figs1">Figure S1A</xref>). Visual responses consisted of an increase of activity during running, and a decrease of activity during sitting (<xref ref-type="fig" rid="figs1">Figure S1B</xref>). This is reminiscent of the running modulation effect observed on visual responses in V1 (<xref ref-type="bibr" rid="c29">Niell and Stryker, 2010</xref>). Thus, running appears to moderately and differentially modulate auditory and visual responses in L2/3 ACx neurons. Consistent with previous work, these results demonstrate that auditory, visual, and motor-related signals are all present in ACx and that running modulation influences L2/3 ACx neurons differently than in V1.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Auditory, visual, and motor-related signals were present in L2/3 of ACx.</title><p>(A) Schematic of the virtual reality system. For imaging experiments, mice were head-fixed and free to run on an air-supported spherical treadmill. For all recordings, the microscope was tilted 45 degrees to the left to image left ACx.</p><p>(B) Strategy for two-photon imaging of L2/3 ACx neurons. We injected an AAV vector to express a genetically encoded calcium indicator in ACx.</p><p>(C) Timeline of the experiment. Starting 10 days after viral injection and window-implantation surgery mice were habituated to the virtual reality setup without any visual or auditory stimulation for 5 days. We mapped ACx with widefield imaging to be able to target two-photon (2P) imaging to ACx. In 1 to 6 recording sessions, 1 session per day, we recorded from 7637 neurons in 17 mice.</p><p>(D) Example two-photon image in L2/3 of ACx.</p><p>(E) Example widefield mapping of ACx. Response maps reflect regions with the strongest response for each tested sound frequency.</p><p>(F) The sound stimuli were 1 s long pure tones of 4 kHz, 8 kHz, 16 kHz, or 32 kHz played at 60 dB or 75 dB sound pressure level (SPL), presented with randomized inter-stimulus intervals.</p><p>(G) The average sound evoked response of all L2/3 ACx neurons across all tested frequencies and sound levels. Sound is presented from 0 s to 1 s. Red indicates an increase in activity, while blue indicates a decrease in activity. All responses are baseline subtracted. To avoid regression to the mean artifacts in plotting, the response heatmap is generated by splitting data in two halves by trials. The responses from the first half of trials are used to sort neurons by response strength and the average responses of the second half of trials are plotted for each neuron. To prevent graphical aliasing, the heatmaps are smoothed over 10 neurons for plotting.</p><p>(H) The average sound evoked population response of all ACx L2/3 neurons across all tested frequencies and sound levels (7637 neurons). Stimulus duration was 1 s (gray shading). Here and in subsequent panels, solid black lines represent mean and shading SEM. The horizontal bar above the plot marks time bins in which the response is statistically different from 0 (gray: not significant, black: p&lt;0.05; see Methods).</p><p>(I) The visual stimuli we used were full-field drifting gratings of 8 different directions, presented for 4 s to 8 s with randomized inter-stimulus intervals.</p><p>(J) As in <bold>G</bold>, but for gratings onsets responses averaged across all orientations.</p><p>(K) As in <bold>H</bold>, but for the population response to grating onsets averaged across all orientations.</p><p>(L) Motor-related activity was assessed based on responses upon running onsets.</p><p>(M) As in <bold>G</bold>, but for running onset responses.</p><p>(N) As in <bold>H</bold>, but for the average population response to running onsets. Only data from running onsets in which the mouse ran for at least 1 s (gray shading) were included.</p></caption>
<graphic xlink:href="564593v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2b">
<title>L2/3 neurons in ACx responded to audiomotor mismatch</title>
<p>To test whether auditory, visual, and motor-related signals are integrated in L2/3 neurons of ACx to compute prediction errors, we first probed for responses to audiomotor (AM) mismatches. A mismatch in this context, is the absence of a sensory input that the brain predicts to receive from the environment, and thus a specific type of negative prediction error. We experimentally generated a coupling between movement and sensory feedback and then used movement as a proxy for what the mouse predicts to receive as sensory feedback. To do this with an auditory stimulus, we coupled the sound amplitude of an 8 kHz pure tone to the running speed of the mouse on the spherical treadmill such that sound amplitude was proportional to locomotion speed (<xref rid="fig2" ref-type="fig">Figures 2A</xref> and <xref rid="fig2" ref-type="fig">2B</xref>). In this paradigm, a running speed of 0 corresponded to a sound amplitude of 0, while 30 cm/s running speed corresponded to a sound amplitude of 60 dB SPL. We refer to this type of session as closed loop. We then introduced AM mismatches by setting the sound amplitude to 0 for 1 s at random times (on average every 15 s). An alternative approach to introduce AM mismatches would have been to clamp the sound amplitude to a constant value. However, based on an assumed analogy between sound amplitude and visual flow speed in visuomotor (VM) mismatch paradigms, where we induce VM mismatch by setting visual flow speed to 0 (<xref ref-type="bibr" rid="c19">Keller et al., 2012</xref>), we chose the former. We found that AM mismatch resulted in a strong population response (<xref rid="fig2" ref-type="fig">Figures 2C</xref> and <xref rid="fig2" ref-type="fig">2D</xref>). AM mismatch responses likely cannot be attributed to changes in running speed and do not result in a pupil response. We found no evidence of AM mismatch induced changes in running speed (<xref ref-type="fig" rid="figs2">Figure S2A</xref>) or pupil dilation (<xref ref-type="fig" rid="figs2">Figure S2B</xref>). Interestingly, the AM mismatch response was already apparent in the first closed loop session with audiomotor coupling that the mice ever experienced, suggesting that this coupling is learned very rapidly (<xref ref-type="fig" rid="figs2">Figure S2C</xref>). To test whether AM mismatch responses can be explained by a sound offset response, we performed recordings in open loop sessions that consisted of a replay of the sound profile the mouse had self-generated in the preceding closed loop session. Mice were free to run during this session and did so at similar levels as during the closed loop session (<xref ref-type="fig" rid="figs2">Figure S2D</xref>). The average response to the playback of sound halt during the open loop session was significantly less strong than the average response to AM mismatch (<xref rid="fig2" ref-type="fig">Figure 2D</xref>), but in contrast to visual playback halt responses in V1 (<xref ref-type="bibr" rid="c40">Vasilevskaya et al., 2023</xref>), we found no evidence of a running modulation of the response to the playback halt (<xref ref-type="fig" rid="figs2">Figure S2E</xref>). Thus, L2/3 neurons in ACx respond to AM mismatch in a way similar to how the L2/3 neurons in V1 respond to visuomotor (VM) mismatch.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>L2/3 neurons of ACx responded to audiomotor mismatch events.</title><p>(A) Schematic of the virtual reality system used to study responses to audiomotor (AM) mismatches. The sound amplitude of an 8 kHz pure tone was coupled to the running speed of the mouse on a spherical treadmill. These experiments were performed with the VR switched off.</p><p>(B) In closed loop sessions, the running speed of the mouse was coupled to the sound amplitude. AM mismatches were introduced by briefly setting the sound amplitude to 0 for 1 s. Below, the calcium response of an example neuron to AM mismatch events.</p><p>(C) Responses of all L2/3 ACx neurons to audiomotor mismatches. The response heatmap is generated as described in <xref rid="fig1" ref-type="fig">Figure 1G</xref>.</p><p>(D) The average population response of all L2/3 neurons to AM mismatches and sound playback halts (4755 neurons). AM mismatch duration was 1 s (gray shading). The horizontal bar above the plot marks time bins in which the AM mismatch response is statistically different from the playback halt response (gray: not significant., black: p&lt;0.05; see Methods).</p><p>(E) The average population response of AM mismatch neurons (5% of strongest responders) to sound stimulation (black) and running onsets (green). Same data as in <xref rid="fig1" ref-type="fig">Figures 1H</xref> and <xref rid="fig1" ref-type="fig">1N</xref>, but subselected for AM MM neurons. Sound stimulation was 1 s (gray shading).</p><p>(F) Comparison of the response strength of AM mismatch (MM) neurons to sound stimulation (left) and running onsets (right) compared to those of the remainder of the neuron population. Error bars indicate SEM. Here and elsewhere, n.s.: not significant; *: p&lt;0.05; **: p&lt;0.01;</p><p>: p&lt;0.001. See <xref ref-type="table" rid="tbls1">Table S1</xref> for all statistical information.</p><p>(G) Scatter plot of the correlations of calcium activity with sound amplitude (x-axis) and running speed (y-axis) in open loop sessions, for all neurons. The color-code reflects the strength of responses to AM mismatch in the closed loop session. Note, AM mismatch-responsive neurons are enriched in the upper left quadrant.</p><p>(H) The average population response to running onsets in closed (red) and open (black) loop sessions (only data from neurons for which we had at least 2 running onsets in both closed and open loop sessions are included here, see <xref ref-type="table" rid="tbls1">Table S1</xref>).</p></caption>
<graphic xlink:href="564593v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Assuming that AM mismatch responses are computed as a difference between an excitatory motor-related prediction and an auditory stimulus driven inhibition, we would expect the neurons with high AM mismatch responses to exhibit opposing influence of motor-related and auditory input. To test this, we selected the 5% of neurons with the strongest responses to AM mismatch and quantified the responses of these neurons to sound stimulation and running onsets (as for the sound and running onset responses shown in <xref rid="fig1" ref-type="fig">Figure 1</xref>). Consistent with a model of a subtractive computation of prediction errors, we found that AM mismatch neurons exhibited a strong reduction in activity in response to sound stimulation and an increase of activity on running onsets (<xref rid="fig2" ref-type="fig">Figure 2E</xref>). Given that mismatch responses are likely enriched in the superficial part of L2/3 (<xref ref-type="bibr" rid="c30">O’Toole et al., 2023</xref>), and that in our two-photon imaging experiments we also preferentially recorded from more superficial neurons, we suspect that our population is enriched for mismatch neurons. Consistent with this interpretation, we observed a strong population response to AM mismatches (<xref rid="fig2" ref-type="fig">Figures 2C</xref> and <xref rid="fig2" ref-type="fig">2D</xref>) and a decrease in population activity in response to sound stimulation (<xref rid="fig1" ref-type="fig">Figure 1H</xref>). Nevertheless, sound evoked responses were significantly more negative in neurons strongly responsive to AM mismatch, than for the remainder of the L2/3 neuronal population (<xref rid="fig2" ref-type="fig">Figure 2F</xref>). This effect was similar when we used different thresholds for the selection of AM mismatch neurons (10% or 20% of neurons with the strongest response to AM mismatch; <xref ref-type="fig" rid="figs3">Figure S3</xref>). Consistent with a sound driven reduction of activity and running related increase of activity in AM mismatch neurons, the correlation of calcium activity of AM mismatch neurons was predominantly negative with sound amplitude and positive with running speed in open loop sessions (<xref rid="fig2" ref-type="fig">Figure 2G</xref>). This again resembles the properties of VM mismatch neurons in V1 (<xref ref-type="bibr" rid="c1">Attinger et al., 2017</xref>). If AM mismatch responses are computed as a difference between a locomotion driven excitation and a sound driven inhibition, we could also expect to find a correlation between the strength of mismatch response and the strength of sound playback halt responses. Even in the absence of locomotion driven excitation, a relief from sound driven inhibition could trigger an increase in calcium activity. When comparing AM mismatch responses with playback sound halt responses for all neurons, we do indeed find a positive correlation between the two (<xref ref-type="fig" rid="figs2">Figure S2F</xref>). Finally, again assuming a locomotion driven excitation and a sound driven inhibition, we should find systematic differences in closed and open loop running onsets. In closed loop running onsets in which the sound was coupled to movement, we indeed observed only a transient running onset response (<xref rid="fig2" ref-type="fig">Figure 2H</xref>). In contrast, in the open loop condition, running onset responses were sustained, possibly reflecting motor related input that is not cancelled out by sensory input. This analysis highlights the difference in input processing depending on whether the animal is in the coupled or uncoupled condition and is consistent with subtractive interactions between prediction and sensory signals. Overall, these results suggest that the implementation of sensorimotor prediction error computation generalizes beyond V1 to other primary cortices and might be a canonical cortical computation in L2/3.</p>
</sec>
<sec id="s2c">
<title>We found no evidence of visuomotor mismatch responses in L2/3 of ACx</title>
<p>Visuomotor mismatch responses are likely calculated in V1 (<xref ref-type="bibr" rid="c18">Jordan and Keller, 2020</xref>), and spread across dorsal cortex from there (<xref ref-type="bibr" rid="c14">Heindorf and Keller, 2023</xref>). To investigate multimodal mismatch responses, we first quantified the strength of these VM mismatch responses, which are independent of auditory input, in ACx. In these experiments, the running speed of the mouse was experimentally coupled to the visual flow speed in a virtual corridor, but not to sound feedback (<xref rid="fig3" ref-type="fig">Figures 3A</xref> and <xref rid="fig3" ref-type="fig">3B</xref>). Note, there are several remaining sources of naturally occurring sound that increase with increasing running speed of the mouse and are still coupled (sound of mouse footsteps, sound of the treadmill rotating, etc.). However, these cannot be easily experimentally manipulated. We introduced VM mismatches by halting visual flow for 1 s at random times while the mice were running, as previously described (<xref ref-type="bibr" rid="c19">Keller et al., 2012</xref>; <xref ref-type="bibr" rid="c47">Zmarz and Keller, 2016</xref>). To control for visual responses independent of visuomotor coupling, we used an open loop replay of the visual flow generated in the previous session (see Methods). We found that neither VM mismatches nor visual flow playback halts, which the mouse experienced in open loop sessions, resulted in a measurable population response in ACx (<xref rid="fig3" ref-type="fig">Figures 3C</xref> and <xref rid="fig3" ref-type="fig">3D</xref>). No significant changes in locomotion speed (<xref ref-type="fig" rid="figs4">Figure S4A</xref>) or pupil diameter (<xref ref-type="fig" rid="figs4">Figure S4B</xref>) were detected upon VM mismatch presentation. Selecting the 5% of neurons with the strongest responses to VM mismatches and quantifying their responses to grating presentations and running onsets, we found that these neurons exhibited positive responses to running onset and no significant response to grating stimuli (<xref rid="fig3" ref-type="fig">Figure 3E</xref>). These responses were not different from the population responses of the remainder of the neurons (<xref rid="fig3" ref-type="fig">Figure 3F</xref>). Quantifying the correlation of calcium activity with visual flow speed and running speed in the open loop session, we found that VM mismatch responsive neurons exhibited a distribution not different from chance (<xref rid="fig3" ref-type="fig">Figure 3G</xref>). Running onset responses were not different in closed and open loop conditions (<xref rid="fig3" ref-type="fig">Figure 3H</xref>). This suggests that motor-related predictions are not cancelled out by visual inputs in the auditory cortex. We also found no evidence of a correlation between VM mismatch responses and playback halt responses (<xref ref-type="fig" rid="figs4">Figure S4C</xref>). Thus, while there may be a small subset of VM mismatch responsive neurons in L2/3 of ACx, we find no evidence of a VM mismatch response at the level of the L2/3 population.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>We found no evidence of visuomotor mismatch responses in ACx.</title><p>(A) Schematic of the virtual reality system used to measure VM mismatch responses. The visual flow of the virtual corridor was coupled to the running speed of the mouse on a spherical treadmill. There was no sound stimulus present in these experiments.</p><p>(B) In closed loop sessions, the running speed of the mouse was coupled to the movement in a virtual corridor. VM mismatches were introduced by briefly setting visual flow speed to 0 for 1 s. Below, the calcium response of an example neuron to VM mismatch events.</p><p>(C) Responses of all L2/3 ACx neurons to VM mismatches. The response heatmap is generated as described in <xref rid="fig1" ref-type="fig">Figure 1G</xref>.</p><p>(D) The average population response of all L2/3 neurons to VM mismatches and visual flow playback halts (5688 neurons). Gray shading marks the duration of both stimuli. The horizontal bar above the plot marks time bins in which the VM mismatch response is statistically different from the playback halt response (gray: n.s., black: p&lt;0.05; see Methods).</p><p>(E) The average population response of VM mismatch neurons (5% of strongest responders) to grating stimulation (blue) and running onsets (green). Same data as in <xref rid="fig1" ref-type="fig">Figures 1H</xref> and <xref rid="fig1" ref-type="fig">1N</xref>, but subselected for VM MM neurons. Stimulus duration was 4 s to 8 s (gray shading).</p><p>(F) Comparison of the response strength of VM mismatch (MM) neurons to visual stimulation (left) and running onsets (right) compared to those of the remainder of the neuron population. Error bars indicate SEM. Here and elsewhere, n.s.: not significant; *: p&lt;0.05; **: p&lt;0.01;</p><p>: p&lt;0.001. See <xref ref-type="table" rid="tbls1">Table S1</xref> for all statistical information.</p><p>(G) Scatter plot of the correlation of calcium activity with visual flow speed (x-axis) and running speed (y-axis) in open loop sessions for all neurons. The color-code reflects the strength of responses to VM mismatch in the closed loop session. Note, VM mismatch responsive neurons are scattered randomly.</p><p>(H) The averaged population response to running onsets in closed (red) and open (black) loop session. (only data from neurons for which we had at least 2 running onsets in both closed and open loop sessions are included here, see <xref ref-type="table" rid="tbls1">Table S1</xref>).</p></caption>
<graphic xlink:href="564593v3_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2d">
<title>Mismatch responses were potentiated by multimodal interactions</title>
<p>Finally, we explored how multimodal coupling of both auditory and visual feedback to running speed influenced mismatch responses in L2/3 of ACx. To do this, we coupled both sound amplitude and visual flow speed to the running speed of the mouse in an audiovisual virtual environment (<xref rid="fig4" ref-type="fig">Figures 4A</xref> and <xref rid="fig4" ref-type="fig">4B</xref>). We then introduced mismatch events by halting both sound and visual flow for 1 s to trigger a concurrent audiomotor and visuomotor [AM + VM] mismatch (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). The nomenclature here is such that the first letter in the pair denotes the sensory input that is being predicted, while the second letter denotes the putative predictor – the square brackets are used to denote that the two events happen concurrently. By putative predictor, we mean an information source available to the mouse that would, in principle, allow it to predict another input, given the current experimental environment. Thus, in the case of a [AM + VM] mismatch both the movement related prediction of visual flow and sound amplitude are violated. The [AM + VM] mismatch resulted in a significant response on the population level (<xref rid="fig4" ref-type="fig">Figure 4C</xref>). As with [AM] and [VM] mismatch, we found no evidence of a change in running speed or pupil diameter following [AM + VM] mismatch (<xref rid="figs5" ref-type="fig">Figures S5A and S5B</xref>). The concurrent experience of mismatch between multiple modalities could simply be the result of a linear combination of the responses to the different mismatch stimuli or could be the result of a non-linear combination. To test whether we find evidence of a non-linear combination of mismatch responses, we compared the [AM+VM] mismatch to [AM] and [VM] mismatch events presented alone. We found that the presentation of a [AM+VM] mismatch led to a significantly larger response than either an [AM] or a [VM] mismatch in isolation (<xref rid="fig4" ref-type="fig">Figure 4D</xref>). To test whether the linear summation of [AM] + [VM] mismatch responses could explain the response to the concurrent presentation [AM+VM], we compared the two directly, and found that the concurrent presentation [AM+VM] elicited a significantly larger response than the linear sum of [AM] + [VM] mismatch responses (<xref rid="fig4" ref-type="fig">Figures 4E</xref> and <xref rid="figs5" ref-type="fig">S5C</xref>). Plotting the [AM+VM] mismatch responses against the linear sum of the [AM] + [VM] mismatch responses for each neuron, we found that while there is some correlation between the two, there is a subset of neurons (13.7%; red dots, <xref rid="fig4" ref-type="fig">Figure 4F</xref>) that selectively respond to the concurrent [AM+VM] mismatch, while a different subset of neurons (11.2%; yellow dots, <xref rid="fig4" ref-type="fig">Figure 4F</xref>) selectively responds to the mismatch responses in isolation. This demonstrates that mismatch responses in different modalities can interact non-linearly.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Mismatch responses were potentiated by multimodal interactions in L2/3 ACx neurons.</title><p>(A) Schematic of the virtual reality system used to measure multimodal mismatch responses. Both the sound amplitude of an 8 kHz pure tone and the movement of the virtual corridor were coupled to the running speed of the mouse on a spherical treadmill.</p><p>(B) Concurrent audiomotor and visuomotor [AM + VM] mismatches were introduced by simultaneously setting both sound amplitude and visual flow speed to 0 for 1 s.</p><p>(C) The average population response of all L2/3 ACx neurons to concurrent [AM + VM] mismatches (3289 neurons). Gray shading marks the duration of the mismatch stimulus. The horizontal bar above the plot marks time bins in which the [AM + VM] mismatch response is statistically different from 0 (gray: n.s., black: p&lt;0.05; see Methods).</p><p>(D) Average population response of all L2/3 neurons to concurrent [AM + VM] mismatch and responses evoked by [AM] and [VM] mismatches presented in isolation (same data as shown in <xref rid="fig2" ref-type="fig">Figures 2</xref> and <xref rid="fig3" ref-type="fig">3</xref> but sub-selected to match the neurons that were recorded in both unimodal and multimodal mismatch paradigms; 3289 neurons). The horizontal bars above the plot mark time bins in which the [AM + VM] mismatch response is larger than the [AM] or the [VM] mismatch response (gray: n.s., black: p&lt;0.05; see Methods). The two short horizontal color bars to the left of time 0 indicate which two responses are being compared. Gray shading indicates the duration of the stimulus.</p><p>(E) Average population response of all L2/3 neurons to a concurrent [AM + VM] mismatches compared to the linear sum of the responses evoked by [AM] and [VM] mismatches presented in isolation. Gray shading indicates the duration of the stimuli.</p><p>(F) Scatter plot of the responses of all neurons to the concurrent [AM + VM] mismatches against the linear sum of the responses evoked by [AM] and [VM] mismatches presented in isolation. In red, the subset of neurons (13.7%) that exhibited selective responses to the concurrent [AM+VM] mismatch, and in yellow, the subset of neurons (11.2%) in which the linear sum of the responses to [AM] + [VM] mismatches presented in isolation was significant, while response to their concurrent presentation was not. Neurons without a significant response, or that are responsive to both, are shown in gray.</p></caption>
<graphic xlink:href="564593v3_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Consistent with previous reports, we found that auditory, visual, and motor-related signals are intermixed in the population of L2/3 neurons in ACx (<xref rid="fig1" ref-type="fig">Figures 1H, 1K and 1N</xref>). Responses to both motor-related (<xref ref-type="bibr" rid="c26">McGinley et al., 2015</xref>; <xref ref-type="bibr" rid="c34">Schneider et al., 2014</xref>; <xref ref-type="bibr" rid="c41">Vivaldo et al., 2023</xref>; <xref ref-type="bibr" rid="c43">Yavorska and Wehr, 2021</xref>; <xref ref-type="bibr" rid="c46">Zhou et al., 2014</xref>) and visual signals (<xref ref-type="bibr" rid="c6">Bigelow et al., 2022</xref>; <xref ref-type="bibr" rid="c28">Morrill and Hasenstaub, 2018</xref>; Sharma et al., 2021) have been reported across layers in ACx, with the strongest running modulation effect found in L2/3 (<xref ref-type="bibr" rid="c34">Schneider et al., 2014</xref>). Also, consistent with previous reports, we found that a subset of L2/3 neurons in ACx respond to audiomotor prediction errors (<xref rid="fig2" ref-type="fig">Figures 2C</xref> and <xref rid="fig2" ref-type="fig">2D</xref>) (<xref ref-type="bibr" rid="c3">Audette et al., 2022</xref>; <xref ref-type="bibr" rid="c24">Liu and Kanold, 2022</xref>). In V1, it has been demonstrated that neurons signaling prediction errors exhibit opposing influence of bottom-up visual and top-down motor-related inputs. This has been speculated to be the consequence of a subtractive computation of prediction errors (<xref ref-type="bibr" rid="c18">Jordan and Keller, 2020</xref>; <xref ref-type="bibr" rid="c19">Keller et al., 2012</xref>; <xref ref-type="bibr" rid="c22">Leinweber et al., 2017</xref>). Our findings now reveal a similar pattern of opposing influence in prediction error neurons in primary ACx that exhibit a positive correlation with motor-related input and a negative correlation with auditory input (<xref rid="fig2" ref-type="fig">Figure 2G</xref>). This would be consistent with the idea that both visuomotor and audiomotor prediction errors are computed as a subtractive difference between bottom-up and top-down inputs. Based on this, it is conceivable that this type of computation extends also beyond primary sensory areas of cortex and may be a more general computational principle implemented in L2/3 of cortex.</p>
<p>While there are many similarities between the AM mismatch responses we describe here in auditory cortex and previously described visuomotor mismatch responses in visual cortex (<xref ref-type="bibr" rid="c1">Attinger et al., 2017</xref>; <xref ref-type="bibr" rid="c19">Keller et al., 2012</xref>; <xref ref-type="bibr" rid="c42">Widmer et al., 2022</xref>; <xref ref-type="bibr" rid="c47">Zmarz and Keller, 2016</xref>), there are several notable differences. Specifically, AM responses in auditory cortex (<xref rid="fig2" ref-type="fig">Figure 2D</xref>) appear to be more sustained than VM mismatch responses in visual cortex. We have no explanation for why this might be the case. Additionally, unlike the observed visual flow playback halt responses in V1 and their increase during running (<xref ref-type="bibr" rid="c40">Vasilevskaya et al., 2023</xref>), we found only very weak responses to sound playback halts and no evidence of an influence of running on these responses (<xref ref-type="fig" rid="figs2">Figure S2E</xref>). A decreased dependence of playback halt responses on running may be caused by the fact that the mouse generates additional sounds while running on an air-supported treadmill (e.g., treadmill rotation, changes in airflow, footsteps). These sounds correlate with running speed intensity and may reduce the relative salience of sound playback halts during running. Thus, differences in the experimental control of the sensorimotor coupling for a different sensory modalities could account for some of the observed differences between the AM and VM mismatch responses.</p>
<p>Finally, we found that concurrent prediction errors in multiple modalities result in an increase in prediction error response that exceeds a linear combination of the prediction error responses in single modalities (<xref rid="fig4" ref-type="fig">Figure 4E</xref>), with a subset of neurons selectively responding only to the combination of prediction error responses (<xref rid="fig4" ref-type="fig">Figure 4F</xref>). A similar non-linear relationship has been described between auditory and visual oddball responses in both ACx and V1 (<xref ref-type="bibr" rid="c36">Shiramatsu et al., 2021</xref>). At this point, it should be kept in mind that deviations from linearity in terms of spiking responses are difficult to assess using calcium imaging data. However, given that the difference between the concurrent presentation and the linear sum of the two individual mismatch responses was approximately a factor of two (<xref rid="fig4" ref-type="fig">Figure 4E</xref>), and the fact that we found a population of neurons that responds selectively to the concurrent presentation of both mismatches, we suspect that also the underlying spiking responses are non-linear. What are the mechanisms that could underlie this interaction? Neurons in ACx have access to information about VM mismatches from at least two sources. In widefield calcium imaging, VM mismatch responses are detectable across most of dorsal cortex (<xref ref-type="bibr" rid="c14">Heindorf and Keller, 2023</xref>). Thus, we speculate that VM mismatch responses are present in long-range cortico-cortical axons from V1, or possibly in L4, L5, or L6 neurons in ACx. Alternative sources of VM mismatch input are neuromodulatory signals. Locus coeruleus, for example, drives noradrenergic signals in response to VM mismatches across the entire dorsal cortex (<xref ref-type="bibr" rid="c17">Jordan and Keller, 2023</xref>). However, given that noradrenergic signals only weakly modulate responses in L2/3 neurons in V1 (<xref ref-type="bibr" rid="c17">Jordan and Keller, 2023</xref>), it is unclear if the broadcasted noradrenergic signals could non-linearly potentiate the AM mismatch responses of ACx neurons. We speculate that cholinergic signals are also unlikely to contribute to this effect. In V1 there are no cholinergic responses to VM mismatch (<xref ref-type="bibr" rid="c44">Yogesh and Keller, 2023</xref>). However, given that ACx and V1 receive cholinergic innervation from different sources (<xref ref-type="bibr" rid="c21">Kim et al., 2016</xref>), we cannot rule out the possibility that cholinergic signals in ACx respond to VM mismatch. In sum, given that the [AM+VM] mismatch responses do not simply appear to be an amplified variant of the [AM] mismatch responses (<xref rid="fig4" ref-type="fig">Figure 4F</xref>), we speculate that [AM+VM] mismatch responses are primarily driven by long-range cortico-cortical input from V1 that interacts with a local computation of [AM] mismatch responses in the L2/3 ACx circuit.</p>
<p>Lateral interactions in the computation of prediction errors between sensory streams are not accounted for by hierarchical variants of predictive processing. In these hierarchical variants, prediction errors are computed as a comparison between top-down and bottom-up inputs (<xref ref-type="bibr" rid="c31">Rao and Ballard, 1999</xref>). To explain the lateral interactions between prediction errors likely computed in ACx (AM mismatch responses) and prediction errors likely computed in V1 (VM mismatch responses) that we describe here, we will need new variants of predictive processing models that include lateral and non-hierarchical interactions. Thus, our results demonstrate that mismatch responses in different modalities interact non-linearly and can potentiate each other. The circuit mechanisms that underlie this form of multimodal integration of mismatch responses are still unclear and will require further investigation. However, we would argue that the relatively strong multimodal interaction demonstrates that unimodal and hierarchical variants of predictive processing are insufficient to explain cortical mismatch responses - if predictive processing aims to be a general theory of cortical function, we will need to explore non-hierarchical variants of predictive processing.</p>
</sec>
<sec id="s5">
<title>Methods</title>
<sec id="s5a">
<title>Mice and surgery</title>
<p>All animal procedures were approved by and carried out in accordance with guidelines of the Veterinary Department of the Canton Basel-Stadt, Switzerland. C57BL/6 female mice (Charles River), between the ages of 7 and 12 weeks were used in this study. For cranial window implantation, mice were anesthetized using a mixture of fentanyl (0.05 mg/kg), medetomidine (0.5 mg/kg), and midazolam (5 mg/kg). Analgesics were applied perioperatively. Lidocaine was injected subcutaneously into the scalp (10 mg/kg s.c.) prior to the surgery. Mice underwent a cranial window implantation surgery at an age of between 7 and 8 weeks. First, a custom-made titanium head-plate was attached to the skull (right hemisphere) with dental cement (Heraeus Kulzer). Next, a 3 mm craniotomy was made over left ACx (4.2 mm to 4.4 mm lateral from the midline and 2.6 mm to 2.8 mm posterior from bregma) followed by 4 to 6 injections of approximately 200 nl each of the AAV vector: AAV2/1-EF1<italic>α</italic>-GCaMP6f-WPRE (10<sup>13-</sup> <sup>14</sup> GC/ml). A circular glass cover slip was glued (Ultragel, Pattex) in place to seal the craniotomy. Metacam (5 mg/kg, s.c.) and buprenorphine (0.1 mg/kg s.c.) were injected intraperitoneally for 2 days after completion of the surgery. Mice were returned to their home cage and group housed for 10 days prior to the first experiments.</p>
</sec>
<sec id="s5b">
<title>Virtual reality environment</title>
<p>All recordings were done with mice head-fixed in a virtual reality system, as described previously (<xref ref-type="bibr" rid="c23">Leinweber et al., 2014</xref>). Mice were free to run on an air-supported polystyrene ball. Three types of closed loop conditions were used for the experiments. The rotation of the spherical treadmill was either coupled 1: to the sound amplitude of an 8 kHz pure tone (audiomotor coupling), while the animal was locomoting in near-darkness (low ambient light in the experimental rooms, primarily from computer screens), 2: to the movement in a virtual corridor (visuomotor coupling), or 3: to both the sound amplitude of an 8 kHz pure tone and the movement in a virtual corridor (audio-visuo-motor coupling). For audiomotor coupling, we used the running speed of the mouse to control the SPL of an 8 kHz pure tone presented to the mouse through a loudspeaker (see section auditory stimulation). This closed loop coupling was not instantaneous but exhibited a delay of 260 ms ± 60 ms (mean ± STD). For visuomotor coupling, the running speed of the mouse was coupled to the visual flow speed in the virtual environment projected onto a toroidal screen surrounding the mouse using a Samsung SP-F10M projector synchronized to the turnaround times of the resonant scanner of the two-photon microscope. The delay in the visuomotor closed loop coupling was 90 ms ± 10 ms (mean ± STD). From the point of view of the mouse, the screen covered a visual field of approximately 240 degrees horizontally and 100 degrees vertically.</p>
<p>The virtual environment presented on the screen was a corridor tunnel with walls consisting of vertical sinusoidal gratings. In auditory experiments, the mouse generated additional sounds while running on an air-supported treadmill (e.g., treadmill rotation, changes in airflow, footsteps), which correlated with running speed intensity. Prior to the recording experiments, mice were habituated to the setup, without any coupling, in 1 to 2-hour long sessions for up to 5 days, until they displayed regular locomotion. On the first recording day, the mice experienced all three types of closed loop conditions (audiomotor, visuomotor, or combined coupling) in a random order. Closed loop sessions were followed by open loop sessions, in which rotation of the spherical treadmill was decoupled from both the sound amplitude and the movement in the virtual corridor. During these open loop sessions, we replayed the amplitude modulated sound or the visual flow recorded in the previous closed loop session.</p>
</sec>
<sec id="s5c">
<title>Auditory stimulation</title>
<p>Sounds were generated with a 16-bit digital-to-analog converter (PCI6738, National Instruments) using custom scripts written in LabVIEW (LabVIEW 2020, National Instruments) at 160 kHz sampling rate, amplified (SA1, Tucker Davis Technologies, FL, USA) and played through an MF1 speaker (Tucker Davis Technologies. FL, USA) positioned 10 cm from the mouse’s right ear. Stimuli were calibrated with a wide-band ultrasonic acoustic sensor (Model 378C01, PCB Piezotronics, NY, USA). To study sound-evoked responses, we used 4 kHz, 8 kHz, 16 kHz, and 32 kHz pure tones played at 60 dB and 75 dB SPL (1 s duration, at a randomized inter-stimulus interval 4 s ± 1 s, 10 repetitions, 1 ms on and off-ramp, in a randomized order). For audiomotor coupling experiments, we used an 8 kHz pure tone with a sound amplitude that varied between 40 dB and 75 dB SPL.</p>
</sec>
<sec id="s5d">
<title>Visual stimulation</title>
<p>For visual stimulation, we used full-field sinusoidal drifting grating (0 degrees, 45 degrees, 90 degrees, 270 degrees, moving in either direction) in a pseudo-random sequence, each presented for a duration of 6 s ± 2 s, with between 2 and 7 repetitions, with a randomized inter-stimulus interval of 4.5 s ± 1.5 s during which a gray screen was displayed.</p>
</sec>
<sec id="s5e">
<title>Running onsets</title>
<p>Running onsets were defined as the running speed crossing a threshold of 3 cm/s, where the average speed in the previous 3 s was below 1.8 cm/s. To separate trials with AM mismatch, VM mismatch, auditory stimulus and grating stimulus based on locomotion state into those running and those while sitting, we used threshold of 0.3 cm/s in a 1 s window preceding the stimulus onset.</p>
</sec>
<sec id="s5f">
<title>Widefield calcium imaging</title>
<p>To establish a reference tonotopic map of A1 and AAF (<xref rid="fig1" ref-type="fig">Figure 1E</xref>), we performed widefield fluorescence imaging experiments on a custom-built microscope consisting of objectives mounted face-to-face (Nikon 85 mm/f1.8 sample side, Nikon 50 mm/f1.4 sensor side), as previously described (<xref ref-type="bibr" rid="c14">Heindorf and Keller, 2023</xref>). Blue illumination was provided by a light-emitting diode (470 nm, Thorlabs) and passed through an excitation filter (SP490, Thorlabs). Green fluorescence emission was filtered with a 525/50 bandpass filter. Images were acquired at a frame rate of 100 Hz on a sCMOS camera (PCO edge 4.2). The raw images were cropped on-sensor, and the resulting data was saved to disk with custom-written software in LabVIEW (National Instruments).</p>
</sec>
<sec id="s5g">
<title>Two-photon imaging</title>
<p>Calcium imaging of L2/3 neurons in A1 and AAF was performed using a modified Thorlabs Bergamo II microscope with a 16x, 0.8 NA objective (Nikon N16XLWD-PF), as previously described (<xref ref-type="bibr" rid="c23">Leinweber et al., 2014</xref>). To record in left ACx, the microscope was tilted 45 degrees to the left. The excitation light source was a tunable, femtosecond-pulsed laser (Insight, Spectra Physics or Chameleon, Coherent) tuned to 930 nm. The laser power was adjusted to 30 mW. A 12 kHz resonance scanner (Cambridge Technology) was used for line scanning, and we acquired 400 lines per frame. This resulted in a frame rate of 60 Hz at a resolution of 400 × 750 pixels. We used a piezo-electric linear actuator (Physik Instrumente, P-726) to record from imaging planes at four different cortical depths, separated by 15 μm. This reduced the effective frame rate per layer to 15 Hz. The emission light was bandpass filtered using a 525/50 nm filter (Semrock), and signals were detected with a photomultiplier (Hamamatsu, H7422), amplified (Femto, DHCPCA-100), digitized at 800 MHz (National Instruments, NI5772), and bandpass filtered at 80 MHz with a digital Fourier-transform filter on a field-programmable gate array (National Instruments, PXIe-7965). Recording locations were visually registered against the reference images acquired with widefield imaging previously using blood vessels patterns.</p>
</sec>
<sec id="s5h">
<title>Widefield image analysis</title>
<p>Off-line data processing and data analysis were done with custom-written MATLAB scripts. Slow drifts in the fluorescence signal were removed using 8<sup>th</sup> percentile filtering with a 62.5 s moving window, similar to what was used for two-photon imaging data (<xref ref-type="bibr" rid="c8">Dombeck et al., 2007</xref>). Activity was calculated as the ΔF/F<sub>0</sub>, where F<sub>0</sub> was the median fluorescence over the entire recording session. For stimulus responses, we use a response window of 0.2 s to 1.2 s following stimulus onset and a baseline window of -1 s to 0 s before stimulus onset. The pixels with the strongest response (top 3% - 5% of response distribution), were used to mark the tonotopic areas corresponding to the different stimuli.</p>
</sec>
<sec id="s5i">
<title>Two-photon image analysis</title>
<p>Calcium imaging data were processed as described previously. In brief, raw images were full-frame registered to correct for lateral brain motion. Neurons were selected manually based on mean and maximum fluorescence images. Average fluorescence per neuron over time was corrected for slow fluorescence drift using an 8<sup>th</sup> percentile filter and a 66 s (or 1000 frames) window (<xref ref-type="bibr" rid="c8">Dombeck et al., 2007</xref>; <xref ref-type="bibr" rid="c19">Keller et al., 2012</xref>; <xref ref-type="bibr" rid="c23">Leinweber et al., 2014</xref>) and divided by the median value over the entire trace to calculate ΔF/F<sub>0</sub>. All stimulus-response curves were baseline subtracted. The baseline subtraction window was -0.5 s to 0 s before stimulus onset. For quantification of responses during different onset types (auditory, visual, running, mismatch), ΔF/F was averaged over the response time window (0.5 s to 2.5 s after stimulus onset) and baseline subtracted (mean activity in a window preceding stimulus onset, -0.5 s to 0 s). Onsets which were not preceded by at least 2 s of baseline or not followed by at least 3 s of recording time, were excluded from the analysis. Sessions with less than two onsets were not included in the analysis. To quantify the difference in average calcium responses as a function of time, we used a hierarchical bootstrap test for every 5 frames of the calcium trace (333 ms) and marked comparisons where responses were different (p &lt; 0.05). Mismatch responsive neurons were selected based on the absolute response strength over the response time window (0.5 s to 2.5 s). To infer spikes from calcium signals (<xref ref-type="fig" rid="figs5">Figure S5C</xref>), we used CASCADE (<xref ref-type="bibr" rid="c32">Rupprecht et al., 2021</xref>).</p>
</sec>
<sec id="s5j">
<title>Statistical tests</title>
<p>All statistical information for the tests performed in this manuscript is provided in <xref ref-type="table" rid="tbls1">Table S1</xref>. We used hierarchical bootstrapping (<xref ref-type="bibr" rid="c33">Saravanan et al., 2020</xref>) for statistical testing to account for the nested structure of the data (multiple neurons from one imaging site). We first resampled the data with replacement at the level of imaging sites, followed by resampling at the level of neurons. We then computed the mean responses across the resampled population and repeated this process 10 000 times. The probability of one group being different from the other was calculated as a fraction of bootstrap sample means which violated the tested hypothesis.</p>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Table S1.</label>
<caption><title>Statistics</title><p>All information on statistical tests used in this manuscript are shown in <xref ref-type="table" rid="tbls1">Table S1</xref> below. We used hierarchical bootstrap (<xref ref-type="bibr" rid="c33">Saravanan et al., 2020</xref>) or a correlation coefficient for all comparisons.</p></caption>
<graphic xlink:href="564593v3_tbls1.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="564593v3_tbls1a.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="564593v3_tbls1b.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="564593v3_tbls1c.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s6">
<title>Key Resource Table</title>
<table-wrap id="utbl1" orientation="portrait" position="float">
<graphic xlink:href="564593v3_utbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank Tingjia Lu for the production of viral vectors and all the members of the Keller lab for discussion and support. This project has received funding from the Swiss National Science Foundation (GBK), the Novartis Research Foundation (GBK), and the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 865617) (GBK).</p>
</ack>
<sec id="s7">
<title>Additional information</title>
<sec id="s7a">
<title>Author contributions</title>
<p>MS designed and performed the experiments and analyzed the data. All authors wrote the manuscript.</p>
</sec>
<sec id="s8">
<title>Declaration of interests</title>
<p>The authors declare no competing financial interests.</p>
</sec>
</sec>
<sec id="s4">
<title>Supplementary figures</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1.</label>
<caption><title>Running exhibited differential effects on the responses to sound presentation and moving grating onsets. Related to <xref rid="fig1" ref-type="fig">Figure 1</xref>.</title><p>(A) The average population response of ACx L2/3 neurons to sound presentation (4390 neurons) during sitting (light gray) and running (dark gray). Stimulus duration was 1 s (gray shading). The horizontal bar above the plot marks time bins in which the responses are statistically different from each other (gray: n.s., black: p&lt;0.05; see Methods).</p><p>(B) As in <bold>A</bold>, but for responses to moving gratings (3701 neurons) during sitting (light blue) and running (dark blue). Stimulus duration was 4 s to 8 s (gray shading).</p></caption>
<graphic xlink:href="564593v3_figs1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2.</label>
<caption><title>Controls for audiomotor mismatch responses. Related to <xref rid="fig2" ref-type="fig">Figure 2</xref>.</title><p>(A) The average running speed of mice (38 sessions, 12 mice) upon AM mismatches (red) and sound playback halts (black). AM mismatch duration was 1 s (gray shading).</p><p>(B) Changes in pupil diameter upon the AM mismatches (red) and sound playback halts (black) (32 sessions, 11 mice).</p><p>(C) The average population response of L2/3 ACx neurons to AM mismatches as a function of experience with audiomotor coupling. AM mismatch duration was 1 s (gray shading). The data shown are from the first two audiomotor closed loop sessions. Each closed loop session lasted 5.5 minutes and mice experienced one such session per day (Day 1: 1271, Day 2: 904). Note, AM mismatch responses are already present in the first closed loop session.</p><p>(D) Comparison of running speeds during either closed or open loop audiomotor sessions. Dots are different recording sessions. Here and elsewhere, n.s.: not significant; *: p&lt;0.05; **: p&lt;0.01;</p><p>: p&lt;0.001. See <xref ref-type="table" rid="tbls1">Table S1</xref> for all statistical information.</p><p>(E) Comparison of the average population response of L2/3 neurons to sound playback halts while mice were running (dark gray, 4017 neurons) or sitting (light gray, 1878 neurons) in open loop session. AM mismatch duration was 1 s (gray shading).</p><p>(F) Scatter plot of the responses to AM mismatch and sound playback halt for all neurons. Neurons that exhibited significant (p &lt; 0.05) positive responses to AM mismatch are shown in red (13.7%). Black dashed line marks unity</p></caption>
<graphic xlink:href="564593v3_figs2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Figure S3.</label>
<caption><title>Opposing influence of sound and running on AM mismatch neurons. Related to <xref rid="fig2" ref-type="fig">Figure 2</xref>.</title><p>The same analysis presented as in <xref rid="fig2" ref-type="fig">Figure 2F</xref> but using the 10% of most AM mismatch responsive neurons (left), or the 20% of the most AM mismatch responsive neurons (right). Error bars indicate SEM. Here and elsewhere, n.s.: not significant; *: p&lt;0.05; **: p&lt;0.01;</p><p>: p&lt;0.001. See <xref ref-type="table" rid="tbls1">Table S1</xref> for all statistical information.</p></caption>
<graphic xlink:href="564593v3_figs3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Figure S4.</label>
<caption><title>Controls for visuomotor mismatch responses. Related to <xref rid="fig3" ref-type="fig">Figure 3</xref>.</title><p>(A) The average running speed of mice upon VM mismatches (red) and visual flow playback halts (black) (47 sessions, 15 mice). VM mismatch duration was 1 s (gray shading).</p><p>(B) Changes in pupil diameter upon the VM mismatches (red) and sound playback halts (black) presentation (32 sessions, 11 animals).</p><p>(C) Scatter plot of the responses to VM mismatch and visual flow playback halt for all neurons. The percentage of neurons (6.6%; in red) that exhibited significant (p &lt; 0.05) responses to VM mismatches, is only barely above chance.</p></caption>
<graphic xlink:href="564593v3_figs4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Figure S5.</label>
<caption><title>Controls for multimodal mismatch responses. Related to <xref rid="fig4" ref-type="fig">Figure 4</xref>.</title><p>(A) The average running speed of mice upon concurrent [AM + VM] mismatch (26 sessions, 10 mice). Gray shading marks the duration of the mismatch stimulus.</p><p>(B) Changes in pupil diameter upon the concurrent [AM + VM] mismatch (26 sessions, 10 mice).</p><p>(C) Non-linear combination of mismatch responses with spike estimation. As in <xref rid="fig4" ref-type="fig">Figure 4E</xref>, but using an estimate of firing rate calculated using CASCADE (<xref ref-type="bibr" rid="c32">Rupprecht et al., 2021</xref>). Gray shading marks the duration of the mismatch stimulus.</p></caption>
<graphic xlink:href="564593v3_figs5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Attinger</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B</given-names></string-name></person-group>., <year>2017</year>. <article-title>Visuomotor Coupling Shapes the Functional Development of Mouse Visual Cortex</article-title>. <source>Cell</source> <volume>169</volume>, <fpage>1291</fpage>–<lpage>1302.e14.</lpage> <pub-id pub-id-type="doi">10.1016/j.cell.2017.05.023</pub-id></mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Audette</surname>, <given-names>N.J.</given-names></string-name>, <string-name><surname>Schneider</surname>, <given-names>D.M</given-names></string-name></person-group>., <year>2023</year>. <article-title>Stimulus-specific prediction error neurons in mouse auditory cortex</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2023.01.06.523032</pub-id></mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Audette</surname>, <given-names>N.J.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>La Chioma</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schneider</surname>, <given-names>D.M.</given-names></string-name></person-group>, <year>2022</year>. <article-title>Precise movement-based predictions in the mouse auditory cortex</article-title>. <source>Curr Biol</source> <volume>32</volume>, <fpage>4925</fpage>–<lpage>4940.e6.</lpage> <pub-id pub-id-type="doi">10.1016/j.cub.2022.09.064</pub-id></mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ayaz</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Stäuble</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hamada</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wulf</surname>, <given-names>M.A.</given-names></string-name>, <string-name><surname>Saleem</surname>, <given-names>A.B.</given-names></string-name>, <string-name><surname>Helmchen</surname>, <given-names>F</given-names></string-name></person-group>., <year>2019</year>. <article-title>Layer-specific integration of locomotion and sensory information in mouse barrel cortex</article-title>. <source>Nature Communications</source> <volume>10</volume>, <fpage>1</fpage>–<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-10564-8</pub-id></mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bigelow</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Morrill</surname>, <given-names>R.J.</given-names></string-name>, <string-name><surname>Dekloe</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hasenstaub</surname>, <given-names>A.R</given-names></string-name></person-group>., <year>2019</year>. <article-title>Movement and VIP Interneuron Activation Differentially Modulate Encoding in Mouse Auditory Cortex</article-title>. <source>eNeuro</source> <volume>6</volume>. <pub-id pub-id-type="doi">10.1523/ENEURO.0164-19.2019</pub-id></mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bigelow</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Morrill</surname>, <given-names>R.J.</given-names></string-name>, <string-name><surname>Olsen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hasenstaub</surname>, <given-names>A.R</given-names></string-name></person-group>., <year>2022</year>. <article-title>Visual modulation of firing and spectrotemporal receptive fields in mouse auditory cortex</article-title>. <source>Current Research in Neurobiology</source> <volume>3</volume>, <fpage>100040</fpage>. <pub-id pub-id-type="doi">10.1016/j.crneur.2022.100040</pub-id></mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clavagnier</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Falchier</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kennedy</surname>, <given-names>H</given-names></string-name></person-group>., <year>2004</year>. <article-title>Long-distance feedback projections to area V1: Implications for multisensory integration, spatial awareness, and visual consciousness</article-title>. <source>Cognitive, Affective, &amp; Behavioral Neuroscience</source> <volume>4</volume>, <fpage>117</fpage>–<lpage>126</lpage>. <pub-id pub-id-type="doi">10.3758/CABN.4.2.117</pub-id></mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dombeck</surname>, <given-names>D.A.</given-names></string-name>, <string-name><surname>Khabbaz</surname>, <given-names>A.N.</given-names></string-name>, <string-name><surname>Collman</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Adelman</surname>, <given-names>T.L.</given-names></string-name>, <string-name><surname>Tank</surname>, <given-names>D.W</given-names></string-name></person-group>., <year>2007</year>. <article-title>Imaging large-scale neural activity with cellular resolution in awake, mobile mice</article-title>. <source>Neuron</source> <volume>56</volume>, <fpage>43</fpage>–<lpage>57</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2007.08.003</pub-id></mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eliades</surname>, <given-names>S.J.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>X</given-names></string-name></person-group>., <year>2008</year>. <article-title>Neural substrates of vocalization feedback monitoring in primate auditory cortex</article-title>. <source>Nature</source> <volume>453</volume>, <fpage>1102</fpage>–<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1038/nature06910</pub-id></mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Falchier</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Clavagnier</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Barone</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kennedy</surname>, <given-names>H</given-names></string-name></person-group>., <year>2002</year>. <article-title>Anatomical evidence of multimodal integration in primate striate cortex</article-title>. <source>J Neurosci</source> <volume>22</volume>, <fpage>5749</fpage>–<lpage>5759</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-13-05749.2002</pub-id></mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Garner</surname>, <given-names>A.R.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B</given-names></string-name></person-group>., <year>2022</year>. <article-title>A cortical circuit for audio-visual predictions</article-title>. <source>Nat Neurosci</source> <volume>25</volume>, <fpage>98</fpage>–<lpage>105</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-021-00974-7</pub-id></mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Han</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Helmchen</surname>, <given-names>F</given-names></string-name></person-group>., <year>2023</year>. <article-title>Behavior-relevant top-down cross-modal predictions in mouse neocortex</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2023.04.03.535389</pub-id></mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heindorf</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Arber</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B</given-names></string-name></person-group>., <year>2018</year>. <article-title>Mouse Motor Cortex Coordinates the Behavioral Response to Unpredicted Sensory Feedback</article-title>. <source>Neuron</source> <volume>99</volume>, <fpage>1040</fpage>–<lpage>1054.e5.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2018.07.046</pub-id></mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Heindorf</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B</given-names></string-name></person-group>., <year>2023</year>. <article-title>Antipsychotic drugs selectively decorrelate long-range interactions in deep cortical layers</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2022.01.31.478462</pub-id></mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Henschke</surname>, <given-names>J.U.</given-names></string-name>, <string-name><surname>Price</surname>, <given-names>A.T.</given-names></string-name>, <string-name><surname>Pakan</surname>, <given-names>J.M.P</given-names></string-name></person-group>., <year>2021</year>. <article-title>Enhanced modulation of cell-type specific neuronal responses in mouse dorsal auditory field during locomotion</article-title>. <source>Cell Calcium</source> <volume>96</volume>, <fpage>102390</fpage>. <pub-id pub-id-type="doi">10.1016/j.ceca.2021.102390</pub-id></mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ibrahim</surname>, <given-names>L.A.</given-names></string-name>, <string-name><surname>Mesik</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Ji</surname>, <given-names>X.-Y.</given-names></string-name>, <string-name><surname>Fang</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>H.-F.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Y.-T.</given-names></string-name>, <string-name><surname>Zingg</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>L.I.</given-names></string-name>, <string-name><surname>Tao</surname>, <given-names>H.W</given-names></string-name></person-group>., <year>2016</year>. <article-title>Cross-Modality Sharpening of Visual Cortical Processing through Layer-1-Mediated Inhibition and Disinhibition</article-title>. <source>Neuron</source> <volume>89</volume>, <fpage>1031</fpage>–<lpage>1045</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2016.01.027</pub-id></mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jordan</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B</given-names></string-name></person-group>., <year>2023</year>. <article-title>The locus coeruleus broadcasts prediction errors across the cortex to promote sensorimotor plasticity</article-title>. <source>eLife</source> <volume>12</volume>. <pub-id pub-id-type="doi">10.7554/eLife.85111</pub-id></mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jordan</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B</given-names></string-name></person-group>., <year>2020</year>. <article-title>Opposing Influence of Top-down and Bottom-up Input on Excitatory Layer 2/3 Neurons in Mouse Primary Visual Cortex</article-title>. <source>Neuron</source> <volume>108</volume>, <fpage>1194</fpage>–<lpage>1206.e5.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2020.09.024</pub-id></mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keller</surname>, <given-names>G.B.</given-names></string-name>, <string-name><surname>Bonhoeffer</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hübener</surname>, <given-names>M</given-names></string-name></person-group>., <year>2012</year>. <article-title>Sensorimotor Mismatch Signals in Primary Visual Cortex of the Behaving Mouse</article-title>. <source>Neuron</source> <volume>74</volume>, <fpage>809</fpage>–<lpage>815</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.040</pub-id></mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keller</surname>, <given-names>G.B.</given-names></string-name>, <string-name><surname>Hahnloser</surname>, <given-names>R.H.R</given-names></string-name></person-group>., <year>2009</year>. <article-title>Neural processing of auditory feedback during vocal practice in a songbird</article-title>. <source>Nature</source> <volume>457</volume>, <fpage>187</fpage>–<lpage>90</lpage>. <pub-id pub-id-type="doi">10.1038/nature07467</pub-id></mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>J.-H.</given-names></string-name>, <string-name><surname>Jung</surname>, <given-names>A.-H.</given-names></string-name>, <string-name><surname>Jeong</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Choi</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Shin</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>S.J.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>S.-H</given-names></string-name></person-group>., <year>2016</year>. <article-title>Selectivity of Neuromodulatory Projections from the Basal Forebrain and Locus Ceruleus to Primary Sensory Cortices</article-title>. <source>J. Neurosci</source>. <volume>36</volume>, <fpage>5314</fpage>–<lpage>5327</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4333-15.2016</pub-id></mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leinweber</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ward</surname>, <given-names>D.R.</given-names></string-name>, <string-name><surname>Sobczak</surname>, <given-names>J.M.</given-names></string-name>, <string-name><surname>Attinger</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B</given-names></string-name></person-group>., <year>2017</year>. <article-title>A Sensorimotor Circuit in Mouse Cortex for Visual Flow Predictions</article-title>. <source>Neuron</source> <volume>95</volume>, <fpage>1420</fpage>–<lpage>1432.e5.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2017.08.036</pub-id></mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leinweber</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Zmarz</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Buchmann</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Argast</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Hübener</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bonhoeffer</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B</given-names></string-name></person-group>., <year>2014</year>. <article-title>Two-photon calcium imaging in mice navigating a virtual reality environment</article-title>. <source>Journal of visualized experiments : JoVE</source> <elocation-id>e50885</elocation-id>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kanold</surname>, <given-names>P.O</given-names></string-name></person-group>., <year>2022</year>. <article-title>Interactive auditory task reveals complex sensory-action integration in mouse primary auditory cortex</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2022.12.12.520155</pub-id></mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Markov</surname>, <given-names>N.T.</given-names></string-name>, <string-name><surname>Ercsey-Ravasz</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Van Essen</surname>, <given-names>D.C.</given-names></string-name>, <string-name><surname>Knoblauch</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Toroczkai</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Kennedy</surname>, <given-names>H.</given-names></string-name></person-group>, <year>2013</year>. <article-title>Cortical high-density counterstream architectures</article-title>, <source>Science</source>. <pub-id pub-id-type="doi">10.1126/science.1238406</pub-id></mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McGinley</surname>, <given-names>M.J.</given-names></string-name>, <string-name><surname>David</surname>, <given-names>S.V.</given-names></string-name>, <string-name><surname>McCormick</surname>, <given-names>D.A</given-names></string-name></person-group>., <year>2015</year>. <article-title>Cortical Membrane Potential Signature of Optimal States for Sensory Signal Detection</article-title>. <source>Neuron</source> <volume>87</volume>, <fpage>179</fpage>–<lpage>192</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2015.05.038</pub-id></mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Morandell</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Yin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Del Rio</surname>, <given-names>R.T.</given-names></string-name>, <string-name><surname>Schneider</surname>, <given-names>D.M</given-names></string-name></person-group>., <year>2023</year>. <article-title>Movement-related modulation in mouse auditory cortex is widespread yet locally diverse</article-title>. <source>bioRxiv</source> <volume>2023</volume>.<fpage>07</fpage>.<lpage>03</lpage>.547560. <pub-id pub-id-type="doi">10.1101/2023.07.03.547560</pub-id></mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morrill</surname>, <given-names>R.J.</given-names></string-name>, <string-name><surname>Hasenstaub</surname>, <given-names>A.R</given-names></string-name></person-group>., <year>2018</year>. <article-title>Visual Information Present in Infragranular Layers of Mouse Auditory Cortex</article-title>. <source>J Neurosci</source> <volume>38</volume>, <fpage>2854</fpage>–<lpage>2862</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3102-17.2018</pub-id></mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niell</surname>, <given-names>C.M.</given-names></string-name>, <string-name><surname>Stryker</surname>, <given-names>M.P</given-names></string-name></person-group>., <year>2010</year>. <article-title>Modulation of visual responses by behavioral state in mouse visual cortex</article-title>. <source>Neuron</source> <volume>65</volume>, <fpage>472</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id></mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Toole</surname>, <given-names>S.M.</given-names></string-name>, <string-name><surname>Oyibo</surname>, <given-names>H.K.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B</given-names></string-name></person-group>., <year>2023</year>. <article-title>Molecularly targetable cell types in mouse visual cortex have distinguishable prediction error responses</article-title>. <source>Neuron</source> <volume>111</volume>, <fpage>2918</fpage>–<lpage>2928.e8.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2023.08.015</pub-id></mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rao</surname>, <given-names>R.P.N.</given-names></string-name>, <string-name><surname>Ballard</surname>, <given-names>D.H</given-names></string-name></person-group>., <year>1999</year>. <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nature Neuroscience</source> <volume>2</volume>, <fpage>79</fpage>–<lpage>87</lpage>. <pub-id pub-id-type="doi">10.1038/4580</pub-id></mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rupprecht</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Carta</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hoffmann</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Echizen</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Blot</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kwan</surname>, <given-names>A.C.</given-names></string-name>, <string-name><surname>Dan</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hofer</surname>, <given-names>S.B.</given-names></string-name>, <string-name><surname>Kitamura</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Helmchen</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Friedrich</surname>, <given-names>R.W</given-names></string-name></person-group>., <year>2021</year>. <article-title>A database and deep learning toolbox for noise-optimized, generalized spike inference from calcium imaging</article-title>. <source>Nat Neurosci</source> <volume>24</volume>, <fpage>1324</fpage>–<lpage>1337</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-021-00895-5</pub-id></mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Saravanan</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Berman</surname>, <given-names>G.J.</given-names></string-name>, <string-name><surname>Sober</surname>, <given-names>S.J.</given-names></string-name></person-group>, <year>2020</year>. <article-title>Application of the hierarchical bootstrap to multi-level data in neuroscience</article-title>. <source>Neuron Behav Data Anal Theory</source> <fpage>3</fpage>, <ext-link ext-link-type="uri" xlink:href="https://nbdt.scholasticahq.com/article/13927-application-of-the-hierarchical-bootstrap-to-multi-level-data-in-neuroscience">https://nbdt.scholasticahq.com/article/13927-application-of-the-hierarchical-bootstrap-to-multi-level-data-in-neuroscience</ext-link>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schneider</surname>, <given-names>D.M.</given-names></string-name>, <string-name><surname>Nelson</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mooney</surname>, <given-names>R</given-names></string-name></person-group>., <year>2014</year>. <article-title>A synaptic and circuit basis for corollary discharge in the auditory cortex</article-title>. <source>Nature</source> <volume>513</volume>, <fpage>189</fpage>–<lpage>194</lpage>. <pub-id pub-id-type="doi">10.1038/nature13724</pub-id></mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schneider</surname>, <given-names>D.M.</given-names></string-name>, <string-name><surname>Sundararajan</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Mooney</surname>, <given-names>R</given-names></string-name></person-group>., <year>2018</year>. <article-title>A cortical filter that learns to suppress the acoustic consequences of movement</article-title>. <source>Nature</source> <volume>561</volume>, <fpage>391</fpage>–<lpage>395</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-018-0520-5</pub-id></mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Sharma</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Srivastava</surname>, <given-names>H.K.</given-names></string-name>, <string-name><surname>Bandyopadhyay</surname>, <given-names>S.</given-names></string-name></person-group>, <year>2021</year>. <article-title>Modulation of auditory responses by visual inputs in the mouse auditory cortex</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2021.01.22.427870</pub-id></mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shiramatsu</surname>, <given-names>T.I.</given-names></string-name>, <string-name><surname>Mori</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ishizu</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Takahashi</surname>, <given-names>H</given-names></string-name></person-group>., <year>2021</year>. <article-title>Auditory, Visual, and Cross-Modal Mismatch Negativities in the Rat Auditory and Visual Cortices</article-title>. <source>Front Hum Neurosci</source> <volume>15</volume>, <fpage>721476</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2021.721476</pub-id></mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stanley</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Miall</surname>, <given-names>R.C</given-names></string-name></person-group>., <year>2007</year>. <article-title>Functional activation in parieto-premotor and visual areas dependent on congruency between hand movement and visual stimuli during motor-visual priming</article-title>. <source>NeuroImage</source> <volume>34</volume>, <fpage>290</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.08.043</pub-id></mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>St-Yves</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Allen</surname>, <given-names>E.J.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Naselaris</surname>, <given-names>T</given-names></string-name></person-group>., <year>2023</year>. <article-title>Brain-optimized deep neural network models of human visual areas learn non-hierarchical representations</article-title>. <source>Nat Commun</source> <volume>14</volume>, <fpage>3329</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-023-38674-4</pub-id></mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Suzuki</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pennartz</surname>, <given-names>C.M.A.</given-names></string-name>, <string-name><surname>Aru</surname>, <given-names>J</given-names></string-name></person-group>., <year>2023</year>. <article-title>How deep is the brain? The shallow brain hypothesis</article-title>. <source>Nat. Rev. Neurosci</source>. <fpage>1</fpage>–<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1038/s41583-023-00756-z</pub-id></mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vasilevskaya</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Widmer</surname>, <given-names>F.C.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B.</given-names></string-name>, <string-name><surname>Jordan</surname>, <given-names>R</given-names></string-name></person-group>., <year>2023</year>. <article-title>Locomotion-induced gain of visual responses cannot explain visuomotor mismatch responses in layer 2/3 of primary visual cortex</article-title>. <source>Cell Rep</source> <volume>42</volume>, <fpage>112096</fpage>. <pub-id pub-id-type="doi">10.1016/j.celrep.2023.112096</pub-id></mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vivaldo</surname>, <given-names>C.A.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Shorkey</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Keerthy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rothschild</surname>, <given-names>G</given-names></string-name></person-group>., <year>2023</year>. <article-title>Auditory cortex ensembles jointly encode sound and locomotion speed to support sound perception during movement</article-title>. <source>PLOS Biology</source> <volume>21</volume>, <fpage>e3002277</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3002277</pub-id></mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Widmer</surname>, <given-names>F.C.</given-names></string-name>, <string-name><surname>O’Toole</surname>, <given-names>S.M.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B</given-names></string-name></person-group>., <year>2022</year>. <article-title>NMDA receptors in visual cortex are necessary for normal visuomotor integration and skill learning</article-title>. <source>eLife</source> <volume>11</volume>, <elocation-id>e71476</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.71476</pub-id></mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yavorska</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Wehr</surname>, <given-names>M</given-names></string-name></person-group>., <year>2021</year>. <article-title>Effects of Locomotion in Auditory Cortex Are Not Mediated by the VIP Network</article-title>. <source>Front Neural Circuits</source> <volume>15</volume>, <fpage>618881</fpage>. <pub-id pub-id-type="doi">10.3389/fncir.2021.618881</pub-id></mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yogesh</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B</given-names></string-name></person-group>., <year>2023</year>. <article-title>Cholinergic input to mouse visual cortex signals a movement state and acutely enhances layer 5 responsiveness</article-title>. <source>eLife</source> <volume>12</volume>. <pub-id pub-id-type="doi">10.7554/eLife.89986</pub-id></mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhao</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Jiang</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Jia</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Luo</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Gong</surname>, <given-names>H</given-names></string-name></person-group>., <year>2022</year>. <article-title>Whole-Brain Direct Inputs to and Axonal Projections from Excitatory and Inhibitory Neurons in the Mouse Primary Auditory Area</article-title>. <source>Neurosci Bull</source> <volume>38</volume>, <fpage>576</fpage>–<lpage>590</lpage>. <pub-id pub-id-type="doi">10.1007/s12264-022-00838-5</pub-id></mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Liang</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Xiong</surname>, <given-names>X.R.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Xiao</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Tao</surname>, <given-names>H.W.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>L.I</given-names></string-name></person-group>., <year>2014</year>. <article-title>Scaling down of balanced excitation and inhibition by active behavioral states in auditory cortex</article-title>. <source>Nat Neurosci</source> <volume>17</volume>, <fpage>841</fpage>–<lpage>850</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3701</pub-id></mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zmarz</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B</given-names></string-name></person-group>., <year>2016</year>. <article-title>Mismatch Receptive Fields in Mouse Visual Cortex</article-title>. <source>Neuron</source> <volume>92</volume>, <fpage>766</fpage>–<lpage>772</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2016.09.057</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95398.2.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>King</surname>
<given-names>Andrew J</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study provides <bold>important</bold> findings on the modulation of cortical neuronal responses to sensory stimuli by motor-driven predictive signals. The study is methodologically sound and well-designed. <bold>Solid</bold> evidence is presented for the conclusion that audiomotor mismatch responses are observed in the auditory cortex and that these are strongly modulated by crossmodal signals, though further investigation of the effects of running speed on audiomotor coupling and of sound offset effects on the observed responses would strengthen the interpretation of the results.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95398.2.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The manuscript presents a short report investigating mismatch responses in the auditory cortex, following previous studies focused on visual cortex. By correlating mouse locomotion speed with acoustic feedback levels, the authors demonstrate excitatory responses in a subset of neurons to halts in expected acoustic feedback. They show a lack of responses to mismatch in he visual modality. A subset of neurons show enhanced mismatch responses when both auditory and visual modalities are coupled to the animal's locomotion.</p>
<p>While the study is well-designed and addresses a timely question, several concerns exist regarding the quantification of animal behavior, potential alternative explanations for recorded signals, correlation between excitatory responses and animal velocity, discrepancies in reported values, and clarity regarding the identity of certain neurons.</p>
<p>Strengths:</p>
<p>(1) Well-designed study addressing a timely question in the field.</p>
<p>
(2) Successful transition from previous work focused on visual cortex to auditory cortex, demonstrating generic principles in mismatch responses.</p>
<p>
(3) Correlation between mouse locomotion speed and acoustic feedback levels provides evidence for prediction signal in the auditory cortex.</p>
<p>
(4) Coupling of visual and auditory feedback show putative multimodal integration in auditory cortex.</p>
<p>Weaknesses:</p>
<p>(1) Lack of quantification of animal behavior upon mismatches, potentially leading to alternative interpretations of recorded signals.</p>
<p>
(2) Unclear correlation between excitatory responses and animal velocity during halts, particularly in closed-loop versus playback conditions.</p>
<p>
(3) Discrepancies in reported values in a few figure panels raise questions about data consistency and interpretation.</p>
<p>
(4) Ambiguity regarding the identity of the [AM+VM] MM neurons.</p>
<p>Comments on revisions:</p>
<p>I am satisfied with all clarifications and additional analyses performed by the authors.</p>
<p>
The only concern I have is about changes in running after [AM+VM] mismatches.</p>
<p>
The authors reported that they &quot;found no evidence of a change in running speed or pupil diameter following [AM + VM] mismatch (Figures S5A)&quot; (line 197).</p>
<p>
Nevertheless, it seems that there is a clear increase in running speed for the [AM+VM] condition (S5A). Could this be more specifically quantified? I am concerned that part of the [AM+VM] could stem from this change in running behavior. Could one factor out the running contribution?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95398.2.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this study, Solyga and Keller use multimodal closed-loop paradigms in conjunction with multiphoton imaging of cortical responses to assess whether and how sensorimotor prediction errors in one modality influence the computation of prediction errors in another modality. Their work addresses an important open question pertaining to the relevance of non-hierarchical (lateral cortico-cortical) interactions in predictive processing within the neocortex.</p>
<p>Specifically, they monitor GCaMP6f responses of layer 2/3 neurons in the auditory cortex of head-fixed mice engaged in VR paradigms where running is coupled to auditory, visual, or audio-visual sensory feedback. The authors find strong auditory and motor responses in the auditory cortex, as well as weak responses to visual stimuli. Further, in agreement with previous work, they find that the auditory cortex responds to audiomotor mismatches in a manner similar to that observed in visual cortex for visuomotor mismatches. Most importantly, while visuomotor mismatches by themselves do not trigger significant responses in the auditory cortex, simultaneous coupling of audio-visual inputs to movement non-linearly enhances mismatch responses in the auditory cortex.</p>
<p>Their results thus suggest that prediction errors within a given sensory modality are non-trivially influenced by prediction errors from another modality. These findings are novel, interesting, and important, especially in the context of understanding the role of lateral cortico-cortical interactions and in outlining predictive processing as a general theory of cortical function.</p>
<p>Comments on revisions:</p>
<p>The authors thoroughly addressed the concerns raised. In my opinion, this has substantially strengthened the manuscript, enabling much clearer interpretation of the results reported. I commend the authors for the response to review. Overall, I find the experiments elegantly designed, and the results robust, providing compelling evidence for non-hierarchical interactions across neocortical areas and more specifically for the exchange of sensorimotor prediction error signals across modalities.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95398.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study explores sensory prediction errors in sensory cortex. It focuses on the question of how these signals are shaped by non-hierarchical interactions, specifically multimodal signals arising from same level cortical areas. The authors used 2-photon imaging of mouse auditory cortex in head-fixed mice that were presented with sounds and/or visual stimuli while moving on a ball. First, responses to pure tones, visual stimuli and movement onset were characterized. Then, the authors made the running speed of the mouse predictive of sound intensity and/or visual flow (closed loop). Mismatches were created through the interruption of sound and/or visual flow for 1 second, disrupting the expected sensory signal. As a control, sensory stimuli recorded during the close loop phase were presented again decoupled from the movement (open loop). The authors suggest that auditory responses to the unpredicted interruption of the sound, which affected neither running speed nor pupil size, reflect mismatch responses. That these mismatch responses were enhanced when the visual flow was congruently interrupted, indicates cross-modal influence of prediction error signals.</p>
<p>This study's strengths are the relevance of the question and the design of the experiment. The authors are experts in the techniques used. The analysis explores neither the full power of the experimental design nor the population activity recorded with 2-photon, leaving open the question of to what extend what the authors call mismatch responses are not sensory responses to sound interruption (offset responses). The auditory system is sensitive to transitions and indeed responses to the interruption of the sound are similar in quality, if not quantity, in the predictive and the control situation.</p>
<p>Comments on revisions:</p>
<p>The incorporation of the analysis of the animal's running speed and the pupil size upon sound interruption improves the interpretation of the data. The authors can now conclude that responses to the mismatch are not due to behavioral effects.</p>
<p>
The issue of the relationship between mismatch responses and offset responses remains uncommented. The auditory system is sensitive to transitions, also to silence. See the work of the Linden or the Barkat labs (including the work of the first author of this manuscript) on offset responses, and also that of the Mesgarani lab (Khalighinejad et al., 2019) on responses to transitions 'to clean' (Figure 1c) in human auditory cortex. Offset responses, as the first author knows well, are modulated by intensity and stimulus length (after adaptation?). That responses to the interruption of the sound are similar in quality, if not quantity, in the closed and open loop conditions suggest that offset response might modulate the mismatch response. A mismatch response that reflects a break in predictability would presumably be less modulated by the exact details of the sensory input than an offset response. Therefore, what is the relationship between the mismatch response and the mean sound amplitude prior to the sound interruption (for example during the preceding 1 second)? And between the mismatch response and the mean firing rate over the same period?</p>
<p>
Finally, how do visual stimuli modulate sound responses in the absence of a mismatch? Is the multimodal response potentiation specific to a mismatch?</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95398.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Solyga</surname>
<given-names>Magdalena</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2969-2963</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Keller</surname>
<given-names>Georg B</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1401-0117</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<p>We thank you for the time you took to review our work and for your feedback! The main changes to the manuscript are:</p>
<p>(1) We have added additional analysis of running onsets in closed and open loop conditions for audiomotor (Figure 2H) and visuomotor (Figure 3H) coupling.</p>
<p>(2) We have also added analysis of running speed and pupil dilation upon mismatch presentation (Figures S2A and S2B, S4A and S4B, and S5A and S5B).</p>
<p>(3) We have expanded on the discussion of the nature of differences between audiomotor and visuomotor mismatches.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1:</bold></p>
<p>The manuscript presents a short report investigating mismatch responses in the auditory cortex, following previous studies focused on the visual cortex. By correlating the mouse locomotion speed with acoustic feedback levels, the authors demonstrate excitatory responses in a subset of neurons to halts in expected acoustic feedback. They show a lack of responses to mismatch in the visual modality. A subset of neurons show enhanced mismatch responses when both auditory and visual modalities are coupled to the animal's locomotion.</p>
<p>While the study is well-designed and addresses a timely question, several concerns exist regarding the quantification of animal behavior, potential alternative explanations for recorded signals, correlation between excitatory responses and animal velocity, discrepancies in reported values, and clarity regarding the identity of certain neurons.</p>
<p>Strengths:</p>
<p>(1) Well-designed study addressing a timely question in the field.</p>
<p>(2) Successful transition from previous work focused on the visual cortex to the auditory cortex, demonstrating generic principles in mismatch responses.</p>
<p>(3) The correlation between mouse locomotion speed and acoustic feedback levels provides evidence for a prediction signal in the auditory cortex.</p>
<p>(4) Coupling of visual and auditory feedback shows putative multimodal integration in the auditory cortex.</p>
<p>Weaknesses:</p>
<p>(1) Lack of quantification of animal behavior upon mismatches, potentially leading to alternative interpretations of recorded signals.</p>
<p>(2) Unclear correlation between excitatory responses and animal velocity during halts, particularly in closed-loop versus playback conditions.</p>
<p>(3) Discrepancies in reported values in a few figure panels raise questions about data consistency and interpretation.</p>
<p>(4) Ambiguity regarding the identity of the [AM+VM] MM neurons.</p>
<p>The manuscript is a short report following up on a series of papers focusing on mismatch responses between sensory inputs and predicted signals. While previous studies focused on the visual modality, here the authors moved to the auditory modality. By pairing mouse locomotion speed to the sound level of the acoustic feedback, they show that a subpopulation of neurons displays excitatory responses to halts in the (expected) acoustic feedback. These responses were lower in the open-loop state, when the feedback was uncorrelated to the animal locomotion.</p>
<p>Overall it is a well-designed study, with a timely and well-posed question. I have several concerns regarding the nature of the MM responses and their interpretations.</p>
<p>- One lacks quantification of the animal behavior upon mismatches. Behavioral responses may trigger responses in the mouse auditory cortex, and this would be an alternative explanation to the recorded signals.</p>
<p>What is the animal speed following closed-loop halts (we only have these data for the playback condition)?</p>
</disp-quote>
<p>We have quantified the running speed of the mouse following audiomotor and visuomotor mismatches. We found no evidence of a change in running speed. We have added this to Figures S2A and S4A, respectively.</p>
<disp-quote content-type="editor-comment">
<p>Is there any pupillometry to quantify possible changes in internal states upon halts (both closed-loop and playback)?</p>
</disp-quote>
<p>The term 'internal state' may be somewhat ambiguous in this context. We assume the reviewer is asking whether we have any evidence for possible neuromodulatory changes. We know that there are noradrenergic responses in visual cortex to visuomotor mismatches (Jordan and Keller, 2023), but no cholinergic responses (Yogesh and Keller, 2023). Pupillometry, however, is likely not always sensitive enough to pick up these responses. With very strong neuromodulatory responses (e.g. to air puffs, or other startling stimuli), pupil dilation is of course detected, but this effect is likely at best threshold linear. Looking at changes in pupil size following audiomotor and visuomotor mismatch responses, we found no evidence of a change. We have added this to Figures S2B and S4B, respectively. Note, we suspect this is also strongly experience-dependent. The first audio- or visuomotor mismatch the mouse encounters is likely a more salient stimulus (to the rest of the brain, not necessarily to auditory or visual cortex), than the following ones.</p>
<disp-quote content-type="editor-comment">
<p>These quantifications must be provided for the auditory mismatches but also for the VM or [AM+VM] mismatches.</p>
<p>During the presentation of multimodal mismatches [AM + VM], mice did not exhibit significant changes in running speed or pupil diameter. These data have been now added to Figures S5A and S5B.</p>
<p>- AM MM neurons supposedly receive a (excitatory) locomotion-driven prediction signal. Therefore the magnitude of the excitation should depend on the actual animal velocity. Does the halt-evoked response in a closed loop correlate with the animal speed during the halt? Is the correlation less in the playback condition?</p>
</disp-quote>
<p>This is indeed what one would expect. We fear, however, that we don’t have sufficient data to address this question properly. Moreover, there is an important experimental caveat that makes the interpretation of the results difficult. In addition to the sound we experimentally couple to the locomotion speed of the mouse, the mouse self-generates sound by running (the treadmill rotating, changes to the airflow of the air-supported treadmill, footsteps, etc.). These sources of sound all also correlate in intensity with running speed. Thus, it is not entirely clear how our increase in sound amplitude with increasing running speed relates to the increase in self-generated sounds on the treadmill. This is one of the key reasons we usually do this type of experiment in the visual system where experimental control of visual flow feedback (in a given retinotopic location) is straightforward.</p>
<p>Having said that, if we look at the how mismatch responses change as a function of locomotion speed across the entire population of neurons, there appears to be no systematic change with running speed (and the effects are highly dependent on speed bins we choose). However, just looking at the most audiomotor mismatch responsive neurons, we find a trend for increased responses with increasing running speed (Author response image 1). We analyzed the top 5% of cells that showed the strongest response to mismatch (MM) and divided the MM trials into three groups based on running speed: slow (10-20 cm/s), middle (20-30 cm/s), and fast (&gt;30 cm/s). Given the fact that we have on average 14 mismatch events in total per neuron, we don’t have sufficient data to analyze this.</p>
<fig id="sa4fig1">
<label>Author response image 1.</label>
<caption>
<title>The average response of strongest AM MM responders to AM mismatches as a function of running speed (data are from 51 cells, 11 fields of view, 6 mice).</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-95398-sa4-fig1.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>Values in Figure 2H are way higher than what can be observed in Figures 2C, and D. Could you explain the mismatch in values? Same for 3H and 4F.</p>
</disp-quote>
<p>In Figure 2H (now Figure S2F), we display responses from 4 755 individual neurons. Since most recorded neurons did not exhibit significant responses to mismatch presentations, their responses cluster around zero, significantly contributing to the final average shown in panel D. To clarify how individual neurons contribute to the overall population activity, we have added a histogram showing the distribution of neurons responding to audiomotor mismatch and sound playback halts. We hope this addition clarifies how individual neuron responses affect the final population activity.</p>
<disp-quote content-type="editor-comment">
<p>Furthermore, neurons exhibiting suppression upon closed-loop halts (Figure 2C) show changes in deltaF/F of the same order of magnitude as the AM MM neurons (with excitatory responses). I cannot picture where these neurons are found in the scatter plot of Figure 2H.</p>
</disp-quote>
<p>This is caused by a ceiling effect. While we could adjust the scale of the heat map to capture neurons with very high responses (e.g. [-50 50], Author response image 2), doing so would obscure the response dynamics of most neurons. Note that the number of neurons on the y-axis far exceeds the resolution of this figure and thus there are also aliasing issues that mask the strong responses.</p>
<fig id="sa4fig2">
<label>Author response image 2.</label>
<caption>
<title>Responses of all L2/3 ACx neurons to audiomotor mismatches.</title>
<p>Same as Figure 2C with different color scale [-50 50] which does not capture most of the neural activity.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-95398-sa4-fig2.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>- Are [AM+VM] MM neurons AM neurons?</p>
</disp-quote>
<p>Many of [AM + VM] and [AM] neurons overlap but it is not exactly the same population. This is partially visible in Figure 4F. There is a subset of neurons (13.7%; red dots, Figure 4F) that selectively responded to the concurrent [AM+VM] mismatch, while a different subset of neurons (11.2%; yellow dots, Figure 4F) selectively responded to the mismatch responses in isolation. The [VM] response contributes only little to the sum of the two responses [AM] + [VM].</p>
<disp-quote content-type="editor-comment">
<p>Please do not use orange in Figure 4F, it is perceptually too similar to red.</p>
</disp-quote>
<p>We have now changed it to yellow.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>In this study, Solyga and Keller use multimodal closed-loop paradigms in conjunction with multiphoton imaging of cortical responses to assess whether and how sensorimotor prediction errors in one modality influence the computation of prediction errors in another modality. Their work addresses an important open question pertaining to the relevance of non-hierarchical (lateral cortico-cortical) interactions in predictive processing within the neocortex.</p>
<p>Specifically, they monitor GCaMP6f responses of layer 2/3 neurons in the auditory cortex of head-fixed mice engaged in VR paradigms where running is coupled to auditory, visual, or audio-visual sensory feedback. The authors find strong auditory and motor responses in the auditory cortex, as well as weak responses to visual stimuli. Further, in agreement with previous work, they find that the auditory cortex responds to audiomotor mismatches in a manner similar to that observed in visual cortex for visuomotor mismatches. Most importantly, while visuomotor mismatches by themselves do not trigger significant responses in the auditory cortex, simultaneous coupling of audio-visual inputs to movement non-linearly enhances mismatch responses in the auditory cortex.</p>
<p>Their results thus suggest that prediction errors within a given sensory modality are non-trivially influenced by prediction errors from another modality. These findings are novel, interesting, and important, especially in the context of understanding the role of lateral cortico-cortical interactions and in outlining predictive processing as a general theory of cortical function.</p>
<p>In its current form, the manuscript lacks sufficient description of methodological details pertaining to the closed-loop training and the overall experimental design. In several scenarios, while the results per se are convincing and interesting, their exact interpretation is challenging given the uncertainty about the actual experimental protocols (more on this below). Second, the authors are laser-focused on sensorimotor errors (mismatch responses) and focus almost exclusively on what happens when stimuli deviate from the animal's expectations.</p>
<p>While the authors consistently report strong running-onset responses (during open-loop) in the auditory cortex in both auditory and visual versions of the task, they do not discuss their interpretation in the different task settings (see below), nor do they analyze how these responses change during closed-loop i.e. when predictions align with sensory evidence.</p>
<p>However, I believe all my concerns can be easily addressed by additional analyses and incorporation of methodological details in the text.</p>
<p>Major concerns:</p>
<p>(1) Insufficient analysis of audiomotor mismatches in the auditory cortex:</p>
<p>Lack of analysis of the dependence of audiomotor mismatches on the running speed: it would be helpful if the authors could clarify whether the observed audiomotor mismatch responses are just binary or scale with the degree of mismatch (i.e. running speed). Along the same lines, how should one interpret the lack of dependence of the playback halt responses on the running speed? Shouldn't we expect that during playback, the responses of mismatch neurons scale with the running speed?</p>
</disp-quote>
<p>Regarding the scaling of AM mismatch responses with running speed, please see our response to reviewer 1 above to the same question.</p>
<p>Regarding the playback halt response and dependence on running speed, we would not expect there to be a dependence. The playback halt response (by design) measures the strength of the sensory response to a cessation of a stimulus (think OFF response). These typically are less strong in cortex than the corresponding ON responses but need to be controlled for (else a mismatch response might just be an OFF response – the prediction error is quantified as the difference between AM mismatch response and playback halt response). Given that sound onset responses only have a small dependence on running state, we would similarly expect sound offset (playback halt) responses to exhibit only minimal dependence on running state.</p>
<disp-quote content-type="editor-comment">
<p>Slow temporal dynamics of audiomotor mismatches: despite the transient nature of the mismatches (1s), auditory mismatch responses last for several seconds. They appear significantly slower than previous reports for analogous visuomotor mismatches in V1 (by the same group, using the same methods) and even in comparison to the multimodal mismatches within this study (Figure 4C). What might explain this sustained activity? Is it due to a sustained change in the animal's running in response to the auditory mismatch?</p>
</disp-quote>
<p>This is correct, neither AM or AM+VM mismatch return to baseline in the 3 seconds following onset. VM mismatch response in visual cortex also do not return to baseline in that time window (see e.g.</p>
<p>Figure 1E in (Attinger et al., 2017), or Figure 1F in (Zmarz and Keller, 2016). What the origin or computation significance of this sustained calcium response is we do not know. In intracellular signals, we do not see this sustained response (Jordan and Keller, 2020). Also peculiar is indeed the fact that in the case of AM mismatch the sustained response is similar in strength to the initial response. But also here, why this would be the case, we do not know. It is conceivable that the initial and the sustained calcium response have different origins, if the sustained response amplitude is all or nothing, the fact that the AM mismatch response is the smallest of the three could explain why sustained and initial responses are closer than for [AM+VM] or VM (in visual cortex) mismatch responses. All sustained responses appear to be roughly 1% dF/F. There are no apparent changes in running speed or pupil dilation that would correlate with the sustained activity (new panel A in Figure S2).</p>
<disp-quote content-type="editor-comment">
<p>(2) Insufficient analysis and discussion of running onset responses during audiomotor sessions: The authors report strong running-onset responses during open-loop in identified mismatch neurons. They also highlight that these responses are in agreement with their model of subtractive prediction error, which relies on subtracting the bottom-up sensory evidence from top-down motor-related predictions. I agree, and, thus, assume that running-onset responses during the open loop in identified 'mismatch' neurons reflect the motor-related predictions of sensory input that the animal has learned to expect. If this is true, one would expect that such running-onset responses should dampen during closed-loop, when sensory evidence matches expectations and therefore cancels out this prediction. It would be nice if the authors test this explicitly by analyzing the running-related activity of the same neurons during closed-loop sessions.</p>
</disp-quote>
<p>Thank you for the suggestion. We now show running onset responses in both closed and open loop conditions for audiomotor and visuomotor coupling (new Figures 2H and 3H). In closed loop, we observe only a transient running onset response. In the open loop condition, running onset responses are sustained. For the visuomotor coupling, running onset responses are sustained in both closed and open loop conditions. This would be consistent with a slightly delayed cancellation of sound and motor related inputs in the audiomotor closed loop condition but not otherwise.</p>
<disp-quote content-type="editor-comment">
<p>(3) Ambiguity in the interpretation of responses in visuomotor sessions.</p>
<p>Unlike for auditory stimuli, the authors show that there are no obvious responses to visuomotor mismatches or playback halts in the auditory cortex. However, the interpretation of these results is somewhat complicated by the uncertainty related to the training history of these mice. Were these mice exclusively trained on the visuomotor version of the task or also on the auditory version? I could not find this info in the Methods. From the legend for Figure 4D, it appears that the same mice were trained on all versions of the task. Is this the case? If yes, what was the training sequence? Were the mice first trained on the auditory and then the visual version?</p>
<p>The training history of the animals is important to outline the nature of the predictions and mismatch responses that one should expect to observe in the auditory cortex during visuomotor sessions.</p>
<p>Depending on whether the mice in Figure 3 were trained on visual only or both visual and auditory tasks, the open-loop running onset responses may have different interpretations.</p>
<p>a) If the mice were trained only on the visual task, how should one interpret the strong running onset responses in the auditory cortex? Are these sensorimotor predictions (presumably of visual stimuli) that are conveyed to the auditory cortex? If so, what may be their role?</p>
<p>b) If the mice were also trained on the auditory version, then a potential explanation of the running-onset responses is that they are audiomotor predictions lingering from the previously learned sensorimotor coupling. In this case, one should expect that in the visual version of the task, these audiomotor predictions (within the auditory cortex) would not get canceled out even during the closedloop periods. In other words, mismatch neurons should constantly be in an error state (more active) in the closed-loop visuomotor task. Is this the case?</p>
<p>If so, how should one then interpret the lack of a 'visuomotor mismatch' aligned to the visual halts, over and above this background of continuous errors?</p>
<p>As such, the manuscript would benefit from clearly stating in the main text the experimental conditions such as training history, and from discussing the relevant possible interpretations of the responses.</p>
</disp-quote>
<p>Mice were not trained on either audiomotor or visuomotor coupling and were reared normally. Prior to the recording day, the mice were habituated to running on the air-supported treadmill without any coupling for up to 5 days. On the first recording day, the mice experienced all three types of sessions (audiomotor, visuomotor, or combined coupling) in a random order for the first time. We have clarified this in the methods.</p>
<p>Regarding the question of how one should interpret the strong running onset responses in the auditory cortex, this is complicated by the fact that – unless mice are raised visually or auditorily deprived – they always have life-long experience with visuomotor or audiomotor coupling. The visuomotor coupling they experience in VR is geometrically matched to what they would experience by moving in the real world, for the audiomotor coupling the exact relationship is less clear, but there are a diverse set of sound sources that scale in loudness with increasing running speed. Hence running onset responses reflect either such learned associations (as the reviewer also speculates), or spurious input. Rearing mice without coupling between movement and visual feedback does not abolish movement related responses in visual cortex (Attinger et al., 2017), to the contrary, it enhances them considerably. We suspect this reflects visual cortex being recruited for other functions in the absence of visual input. But given the data we have we cannot distinguish the different possible sources of running related responses. It is very likely that any “training” related effect we could achieve in a few hours pales in comparison to the life-long experience the mouse has in the world.</p>
<p>Regarding the lack of a 'visuomotor mismatch' aligned to the visual halts, we are not sure we understand. Our interpretation is that there are no (or only a very small - we speculate that any nonzero VM mismatch response is just inherited from visual cortex) VM mismatch responses in auditory cortex above chance. Our data are consistent with the interpretation that there is no opposition of bottom up visual and top down motor related input in auditory cortex, hence no VM mismatch responses (independent of how strong the top-down motor related input is). This is of course not surprising – this is more of a sanity check and becomes relevant in the context of interpreting AM+VM responses.</p>
<disp-quote content-type="editor-comment">
<p>(4) Ambiguity in the interpretation of responses in multimodal versus unimodal sessions.</p>
<p>The authors show that multimodal (auditory + visual) mismatches trigger stronger responses than unimodal mismatches presented in isolation (auditory only or visual only). Further, they find that even though visual mismatches by themselves do not evoke a significant response, co-presentation of visual and auditory stimuli non-linearly augments the mismatch responses suggesting the presence of nonhierarchical interactions between various predictive processing streams.</p>
<p>In my opinion, this is an important result, but its interpretation is nuanced given insufficient details about the experimental design. It appears that responses to unimodal mismatches are obtained from sessions in which only one stimulus is presented (unimodal closed-loop sessions). Is this actually the case? An alternative and perhaps cleaner experimental design would be to create unimodal mismatches within a multimodal closed-loop session while keeping the other stimulus still coupled to the movement.</p>
</disp-quote>
<p>This is correct, unimodal mismatches were acquired in unimodal coupling. Testing unimodal mismatch responses in multimodally coupled VR is an interesting idea we had initially even pursued. However, halting visual flow in a condition of coupling of both visual flow and sound amplitude to running speed has an additional complication. Introducing an audiomotor mismatch in this coupling inherently also creates an audiovisual (AV) mismatch, and the same applies to visuomotor mismatches, which cause a concurrent visuoaudio (VA) mismatch (Figure R3). This assumes that there are cross modal predictions from visual cortex to auditory cortex as there are from auditory cortex to visual cortex (Garner and Keller, 2022). There are interesting differences between the different types of mismatches, but with the all the necessary passive controls this quickly exceeded the amount of data we could reasonably acquire for this paper. This remains an interesting question for future research.</p>
<fig id="sa4fig3">
<label>Author response image 3.</label>
<caption>
<title>Rationale of unimodal mismatches introduced within multimodal paradigm.</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-95398-sa4-fig3.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>Given the current experiment design (if my assumption is correct), it is unclear if the multimodal potentiation of mismatch responses is a consequence of nonlinear interactions between prediction/error signals exchanged across visual and auditory modalities. Alternatively, could this result from providing visual stimuli (coupled or uncoupled to movement) on top of the auditory stimuli? If it is the latter, would the observed results still be evidence of non-hierarchical interactions between various predictive processing streams?</p>
</disp-quote>
<p>Mice are not in complete darkness during the AM mismatch experiments (the VR is off, but there is low ambient light in the experimental rooms primarily from computer screens), so we can rule out the possibility that the difference comes from having “no” visual input during AM mismatch responses. Addressing the question of whether it is this particular stimulus that cause the increase would require an experiment in which we couple sound amplitude but keep visual flow open loop. We did not do this, but also think this is highly unlikely. However, as described above, we did do an experiment in which we coupled both sound amplitude and visual flow to running, and then either halted visual flow, or sound amplitude, or both. Comparing the [AM+VM] and [AM+AV] mismatch responses, we find that [AM+VM] responses are larger than [AM+AV] responses as one would expect from an interaction between [AM] and [VM] responses (Author response image 4). Finally, either way the conclusion that there are nonhierarchical interactions of prediction error computations holds either way – if any visual stimulus (either visuomotor mismatch, or visual flow responses) influences audiomotor mismatch responses, this is evidence of non-hierarchical interactions.</p>
<fig id="sa4fig4">
<label>Author response image 4.</label>
<caption>
<title>Average population response of all L2/3 neurons to concurrent [AM + VM] or [AM+AV] mismatch.</title>
<p>Gray shading indicates the duration of the stimulus.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-95398-sa4-fig4.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>Along the same lines, it would be interesting to analyze how the coupling of visual as well as auditory stimuli to movement influences responses in the auditory cortex in close-loop in comparison to auditoryonly sessions. Also, do running onset responses change in open-loop in multimodal vs. unimodal playback sessions?</p>
</disp-quote>
<p>We agree, and why we started out doing the experiments described above. We stopped with this however, because it quickly became a combinatorial nightmare. We will leave addressing the question of how different types of coupling influences responses in auditory cortex to brave future neuroscientists.</p>
<p>Regarding the question of running onset responses, in both the multimodal and auditory only paradigms, running onset responses are transient; bottom-up sensory evidence is quickly subtracted from top-down motor-related prediction (Author response image 5). While there appears to be a small difference in the dynamics of running onset responses between these two paradigms, it was not significant. Note, we also have much less data than we would like here for this type of analysis.</p>
<fig id="sa4fig5">
<label>Author response image 5.</label>
<caption>
<title>Running onset responses recorded in unimodal and multimodal closed loop sessions (1903 neurons, 16 fields of view, 8 mice)</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-95398-sa4-fig5.jpg" mimetype="image"/>
</fig>
<p>We also compared running onsets in open loop sessions and did not find any significant differences between unimodal and multimodal sessions (Author response image 6). We found only six sessions in which animals performed at least two running onsets in each session type, therefore, we do not have enough data to include it in the manuscript.</p>
<fig id="sa4fig6">
<label>Author response image 6.</label>
<caption>
<title>Running onset responses recorded within unimodal and multimodal open loop sessions (659 cells, 6 field of view, 5 mice).</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-95398-sa4-fig6.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>Minor concerns and comments:</p>
<p>(1) Rapid learning of audiomotor mismatches: It is interesting that auditory mismatches are present even on day 1 and do not appear to get stronger with learning (same on day 2). The authors comment that this could be because the coupling is learned rapidly (line 110). How does this compare to the rate at which visuomotor coupling is learned? Is this rapid learning also observable in the animal's behavior i.e. is there a change in running speed in response to the mismatch?</p>
</disp-quote>
<p>In the visual system this is a bit more complicated. If you look at visuomotor mismatch responses in a normally reared mouse, responses are present from the first mismatch (as far as we can tell given the inherently small dataset with just one response pre mouse). However, this is of course confounded by the fact that a normally reared mouse has visuomotor coupling throughout life from eye-opening. Raising mice in complete darkness, we have shown that approximately 20 min of coupling are sufficient to establish visuomotor mismatch responses (Attinger et al., 2017).</p>
<p>Regarding the behavioral changes that correlate with learning, we are not sure what the reviewer would expect. We cannot detect a change in mismatch responses and hence would also not expect to see a change in behavior.</p>
<disp-quote content-type="editor-comment">
<p>(2) The authors should clarify whether the sound and running onset responses of the auditory mismatch neurons in Figure 2E were acquired during open-loop. This is most likely the case, but explicitly stating it would be helpful.</p>
</disp-quote>
<p>Both responses were measured in isolation (i.e. VR off, just sound and just running onset), not in an open-loop session. We have clarified in the figure legend that these are the same data as in Figure 1H and N.</p>
<disp-quote content-type="editor-comment">
<p>(3) In lines 87-88, the authors state 'Visual responses also appeared overall similar but with a small increase in strength during running ...'. This statement would benefit from clarification. From Figure S1 it appears that when the animal is sitting there are no visual responses in the auditory cortex. But when the animal is moving, small positive responses are present. Are these actually 'visual' responses - perhaps a visual prediction sent from the visual cortex to the auditory cortex that is gated by movement? If so, are they modulated by features of visual stimuli eg. contrast, intensity? Or, do these responses simply reflect motor-related activity (running)? Would they be present to the same extent in the same neurons even in the dark?</p>
</disp-quote>
<p>This was wrong indeed - we have rephrased the statement as suggested. Regarding the source of visual responses, we use the term “visual response” operationally here agnostic to what pathway might be driving it (i.e. it could be a prediction triggered by visual input).</p>
<p>We did not test if recorded visual responses are modulated by contrast or intensity. However, testing whether they are would not help us distinguish whether the responses are ‘visual’ or ‘visual predictions’. Finally, regarding the question about whether they are motor-related responses, this might be a misunderstanding. These are responses to visual stimuli while the mouse is already running (i.e. there is no running onset), hence we cannot test whether these responses are present in the dark (this would be the equivalent of looking at random triggers in the dark while the mouse is running).</p>
<disp-quote content-type="editor-comment">
<p>(4) The authors comment in the text (lines 106-107) about cessation of sound amplitude during audiomotor mismatches as being analogous to halting of visual flow in visuomotor mismatches. However, sound amplitude versus visual flow are quite different in nature. In the visuomotor paradigm, the amount of visual stimulation (photons per unit time) does not necessarily change systematically with running speed. Whereas, in the audiomotor paradigm, the SNR of the stimulus itself changes with running speed which may impact the accuracy of predictions. On a broader note, under natural settings, while the visual flow is coupled to movement, sound amplitude may vary more idiosyncratically with movement.</p>
</disp-quote>
<p>This is a question of coding space. The coding space of visual cortex of the mouse is probably visual flow (or change in image) not number of photons. This already starts in the retina. The demonstration of this is quite impressive. A completely static image on the retina will fade to zero response (even though the number of photons remains constant). This is also why most visual physiologists use dynamic stimuli – e.g. drifting gratings, not static gratings – to map visual responses in visual cortex. If responses were linear in number of photons, this would make less of a difference. The correspondence we make is between visual flow (which we assume is the main coding space of mouse V1 – this is not established fact, but probably implicitly the general consensus of the field) and sound amplitude. Responses in auditory cortex are probably more linear in sound amplitude than visual cortex responses are linear in number of photons, but whether that is the correct coding space is still unclear, and as far as we can tell there is no clear consensus in the field. We did consider coupling running speed to frequency, which may work as well, but given the possible equivalence (as argued above) and the fact that we could see similar responses with sound amplitude coupling we did not explore frequency coupling.</p>
<p>If visual speed is the coding space of V1, SNR should behave equivalently in both cases.</p>
<disp-quote content-type="editor-comment">
<p>Perhaps such differences might explain why unlike in the case of visual cortex experiments, running speed does not affect the strength of playback responses in the auditory cortex.</p>
</disp-quote>
<p>Possible, but the more straightforward framing of this point is that sensory responses are enhanced by running in visual cortex while they are not in auditory cortex. A playback halt response (by design) is just a sensory response. Why running does not generally increase sensory responses in auditory cortex (L2/3 neurons), but does so in visual cortex, would be the more general version of the same question.</p>
<p>We fear we have no intelligent answer to this question.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>This study explores sensory prediction errors in the sensory cortex. It focuses on the question of how these signals are shaped by non-hierarchical interactions, specifically multimodal signals arising from same-level cortical areas. The authors used 2-photon imaging of mouse auditory cortex in head-fixed mice that were presented with sounds and/or visual stimuli while moving on a ball. First, responses to pure tones, visual stimuli, and movement onset were characterized. Then, the authors made the running speed of the mouse predictive of sound intensity and/or visual flow. Mismatches were created through the interruption of sound and/or visual flow for 1 second while the animal moved, disrupting the expected sensory signal given the speed of movement. As a control, the same sensory stimuli triggered by the animal's movement were presented to the animal decoupled from its movement. The authors suggest that auditory responses to the unpredicted silence reflect mismatch responses. That these mismatch responses were enhanced when the visual flow was congruently interrupted, indicates the cross-modal influence of prediction error signals.</p>
<p>This study's strengths are the relevance of the question and the design of the experiment. The authors are experts in the techniques used. The analysis explores neither the full power of the experimental design nor the population activity recorded with 2-photon, leaving open the question of to what extent what the authors call mismatch responses are not sensory responses to sound interruption. The auditory system is sensitive to transitions and indeed responses to the interruption of the sound are similar in quality, if not quantity, in the predictive and the control situation.</p>
<p>This study's strengths are the relevance of the question and the design of the experiment. The authors are experts in the techniques used. The analysis explores neither the full power of the experimental design nor the population activity recorded with 2-photon, leaving open the question of to what extent what the authors call mismatch responses are not sensory responses to sound interruption. The auditory system is sensitive to transitions and indeed responses to the interruption of the sound are similar in quality, if not quantity, in the predictive and the control situation. The pattern they observe is different from the visuomotor mismatch responses the authors found in V1 (Keller et al., 2012), where the interruption of visual flow did not activate neuronal activity in the decoupled condition.</p>
</disp-quote>
<p>Just to add brief context to this. The reviewer is correct here, the (Keller et al., 2012) paper reports finding no responses to playback halt. However, this was likely a consequence of indicator sensitivity (these experiments were done with what now seems like a pre-historic version of GCaMP). Experiments performed with more modern indicators do find playback halt responses in visual cortex (see e.g. (Zmarz and Keller, 2016)).</p>
<disp-quote content-type="editor-comment">
<p>The auditory system is sensitive to transitions, also those to silence. See the work of the Linden or the Barkat labs on-off responses, and also that of the Mesgarani lab (Khalighinejad et al., 2019) on responses to transitions 'to clean' (Figure 1c) in the human auditory cortex. Since the responses described in the current work are modulated by movement and the relationship between movement and sound is more consistent during the coupled sessions, this could explain the difference in response size between coupled and uncoupled sessions. There is also the question of learning. Prediction signals develop over a period of several days and are frequency-specific (Schneider et al., 2018). From a different angle, in Keller et al. 2012, mismatch responses decrease over time as one might expect from repetition.</p>
</disp-quote>
<p>Also for brief context, this might be a misconception. We don’t find a decrease of mismatch responses in the (Keller et al., 2012) paper – we assume what the reviewer is referring to is the fact that mismatch responses decrease in open-loop conditions (they normally do not in closed-loop conditions). This is the behavior one would expect if the mouse learns that movement no longer predicts visual feedback.</p>
<disp-quote content-type="editor-comment">
<p>It would help to see the responses to varying sound intensity as a function of previous intensity, and to plot the interruption response as a function of both transition and movement in both conditions.</p>
<p>Given the large populations of neurons recorded and the diversity of the responses, from clearly negative to clearly positive, it would be interesting to understand better whether the diversity reflects the diversity of sounds used or a diversity of cell types, or both.</p>
<p>Comments and questions:</p>
<p>Does movement generate a sound and does this change with the speed of movement? It would be useful to have this in the methods.</p>
</disp-quote>
<p>There are three ways to interpret the question – below the answers to all three:</p>
<p>(1) Running speed is experimentally coupled to sound amplitude of a tone played through a loudspeaker. Tone amplitude is scaled with running speed of the mouse in a closed loop fashion. We assume this is not what the reviewer meant, as this is described in the methods (and the results section).</p>
<p>(2) Movements of the mouse naturally generate sounds (footsteps, legs moving against fur, etc.). Most of these sounds trivially scale with the frequency of leg movements – we assume this also not what the reviewer meant.</p>
<p>(3) Finally, there are experimental sounds related to the rotation speed of the air supported treadmill that increase with running speed of the mouse. We have added this to the methods as suggested.</p>
<disp-quote content-type="editor-comment">
<p>Figures 1a and 2a. The mouse is very hard to see. Focus on mouse, objective, and sensory stimuli? The figures are generally very clear though.</p>
</disp-quote>
<p>We have enlarged the mouse as suggested.</p>
<disp-quote content-type="editor-comment">
<p>1A-K was the animal running while these responses were measured?</p>
</disp-quote>
<p>We did not restrict this analysis to running or sitting and pooled responses over both conditions.  We have made this more explicit in the results section.</p>
<disp-quote content-type="editor-comment">
<p>Data in Figure 1: Since the modulation of sensory responses by movement is relevant for the mismatch responses, I would move this analysis from S1 to Figure 1 and analyze the responses more finely in terms of running speed relative to sound and gratings. I would include here a more thorough analysis of the responses to 8kHz at varying intensities, for example in the decoupled sessions. Does the response adapt? Does it follow the intensity?</p>
</disp-quote>
<p>We agree that these are interesting questions, but they do not directly pertain to our conclusions here. The key point Figure S1 addresses is whether auditory responses are generally enhanced by running (as they are e.g. in visual cortex) – the answer, on average, is no. We have tried emphasizing this more, but it changes the flow of the paper away from our main message, hence we have left the panels in the supplements.</p>
<p>Regarding the 8kHz modulation, there is a general increase of the suppression of activity with increasing sound amplitude (Author response image 7 and Author response image 8). But due to the continuously varying amplitude of the stimulus, we do not have sufficient data (or do not know how to with the data we have) to address questions of adaptation. We assume there is some form of adaptation. However, either way, we don’t see how this would change our conclusions.</p>
<fig id="sa4fig7">
<label>Author response image 7.</label>
<caption>
<title>Neural activity as a function of sound level in an AM open loop session.</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-95398-sa4-fig7.jpg" mimetype="image"/>
</fig>
<fig id="sa4fig8">
<label>Author response image 8.</label>
<caption>
<title>The average sound evoked population response of all ACx layer 2/3 neurons to 60 dB or 75 dB 8 kHz pure tones.</title>
<p>Stimulus duration was 1 s (gray shading).</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-95398-sa4-fig8.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>2C-D why not talk of motor modulation? Paralleling what happens in response to auditory and visual stimuli?</p>
</disp-quote>
<p>This is correct, a mismatch response (we use mismatch here to operationally describe the stimulus – not the interpretation) can be described either as a prediction error (this is the interpretation) or a stimulus specific motor modulation. Note, the key here is “stimulus specific”. It is stimulus specific as there is an approximately 3x change between mismatch and playback halt (the same sensory stimulus with and without locomotion), but basically no change for sound onsets (Figure S1). Having said that, one explanation (prediction error) has predictive power (and hence is testable – see e.g. (Vasilevskaya et al., 2023) for an extensive discussion on exactly this argument for mismatch responses in visual cortex), while the other does not (a “stimulus specific” motor modulation has no predictive value or computational theory behind it and is simply a description). Thus, we choose to interpret it as a prediction error. Note, this finding does not stand in isolation and many of the testable predictions of the predictive processing interpretation have turned out to be correct (see e.g. (Keller and Mrsic-Flogel, 2018) for a review).</p>
<p>Note, we try to only use the interpretation of “prediction error” when motivating why we do the experiments, and in the discussion, but not directly in the description of the results (e.g. in Figure 2).</p>
<disp-quote content-type="editor-comment">
<p>How does the mismatch affect the behavior of the mouse? Does it stop running? This could also influence the size of the response.</p>
</disp-quote>
<p>We quantified animal behavior during audiomotor mismatches and did not find any significant acceleration or slowing down upon mismatch events. Thus, neural responses recorded during AM mismatches are unlikely to be explained by changes in animal behavior. These data have been added in Figure S2A and Figure S4A.</p>
<disp-quote content-type="editor-comment">
<p>Figure 3. What about neurons that were positively modulated by both grating and movement? How do these neurons respond to the mismatch?</p>
</disp-quote>
<p>Neurons positively modulated by both grating and movement were slightly more responsive to MM than the rest of the population, though this difference was not significant (Author response image 9). This is also visible in Figure 3G – the high VM mismatch responsive neurons are randomly distributed in regard to correlation with running speed and visual flow speed.</p>
<fig id="sa4fig9">
<label>Author response image 9.</label>
<caption>
<title>Responses to visuomotor mismatches of neurons positively modulated by grating and movement and remaining of the population.</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-95398-sa4-fig9.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>Line 176. The authors say 'Thus, in the case of a [AM + VM] mismatch both the halted visual flow and the halted sound amplitude are predicted by running speed' but the mismatch (halted flow and amplitude) is not predicted by the speed, correct? Please rephrase.</p>
</disp-quote>
<p>Thank you for pointing this out – this was indeed phrased incorrectly. We have corrected this.</p>
<disp-quote content-type="editor-comment">
<p>How was the sound and/or visual flow interruption triggered? Did the animal have to run at a minimum speed in order for it to happen?</p>
</disp-quote>
<p>Sound and visual flow interruptions were triggered randomly, independent of the animal's running speed. However, for the analysis, only MM presentations during which animals were running at a speed of at least 0.3 cm/s were included. The 0.3 cm/s was simply the (arbitrary) threshold we used to determine if the mouse was running. In a completely stationary mouse a mismatch event will not have any effect (sound amplitude/visual flow speed are already at 0). This is described in the methods section.</p>
</body>
</sub-article>
</article>