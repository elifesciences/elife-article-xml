<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">104746</article-id>
<article-id pub-id-type="doi">10.7554/eLife.104746</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.104746.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Efficient coding explains neural response homeostasis and stimulus-specific adaptation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0884-3195</contrib-id>
<name>
<surname>Young</surname>
<given-names>Edward James</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>ey245@cam.ac.uk</email>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5942-0697</contrib-id>
<name>
<surname>Ahmadian</surname>
<given-names>Yashar</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>ya311@cam.ac.uk</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>Computational and Biological Learning Lab, Department of Engineering, University of Cambridge</institution></institution-wrap>, <city>Cambridge</city>, <country country="GB">United Kingdom</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Sharpee</surname>
<given-names>Tatyana O</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Salk Institute for Biological Studies</institution>
</institution-wrap>
<city>La Jolla</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-03-04">
<day>04</day>
<month>03</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP104746</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-11-20">
<day>20</day>
<month>11</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-10-16">
<day>16</day>
<month>10</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.10.29.564616"/>
</event>
</pub-history>
<permissions>
<copyright-statement>Â© 2025, Young &amp; Ahmadian</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Young &amp; Ahmadian</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-104746-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>In the absence of adaptation, the average firing rate of neurons would rise or drop when changes in the environment make their preferred stimuli more or less prevalent. However, by adjusting the responsiveness of neurons, adaptation can yield firing rate homeostasis and stabilise the average rates of neurons at fixed levels, despite changes in stimulus statistics. In sensory cortex, adaptation is typically also stimulus specific, in that neurons reduce their responsiveness to over-represented stimuli, but maintain or even increase their responsiveness to stimuli far from over-represented ones. Here, we present a normative explanation of firing rate homeostasis grounded in the efficient coding principle, showing that this homeostasis yields an optimal trade-off between coding fidelity and the metabolic cost of neural firing. Unlike previous efficient coding theories, we formulate the problem in a computation-agnostic manner, enabling our framework to apply far from the sensory periphery. We then apply this general framework to Distributed Distributional Codes, a specific computational theory of neural representations serving Bayesian inference. We demonstrate how homeostatic coding, combined with such Bayesian neural representations, provides a normative explanation for stimulus-specific adaptation, widely observed across the brain, and how this coding scheme can be accomplished by divisive normalisation with adaptive weights. Further, we develop a model within this combined framework, and by fitting it to previously published experimental data, quantitatively account for measures of stimulus-specific and homeostatic adaption in the primary visual cortex.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Minor additions to the Discussion section.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Neural population responses, and thus the computations on sensory inputs represented by them, are corrupted by noise. The extent of this corruption can, however, be modulated by changing neural gains. When the gain of a neuron increases, the strength of its response noise, or trial-to-trial variability, typically grows sublinearly and more slowly than its average response; for example, for Poisson-like firing, response noise grows as the square root of the mean response. The neuron can therefore increase its signal-to-noise ratio by increasing its gain or responsiveness. However, this increase in coding fidelity comes at the cost of an elevated average firing rate, and thus higher metabolic energy expenditure. From a normative perspective, coding fidelity and metabolic cost are thus two conflicting forces.</p>
<p>In this article, we ask what is the optimal way of adjusting neural gains in order to combat noise with minimal metabolic cost, <italic>irrespective</italic> of the computations represented by the neural population. As we will see, the answer depends on the prevailing stimulus statistics. We will thus address the following more specific question: given a neural population with <italic>arbitrary</italic> tuning curve shapes and noise distribution, how should neurons optimally adjust their gains depending on the stimulus statistics prevailing in the environment?</p>
<p>To address this question, we use efficient coding theory as our normative framework (<xref ref-type="bibr" rid="c2">Attneave, 1954</xref>; <xref ref-type="bibr" rid="c38">Nadal and Parga, 1999</xref>; <xref ref-type="bibr" rid="c27">Laughlin, 1981</xref>; <xref ref-type="bibr" rid="c3">Barlow, 2012</xref>; <xref ref-type="bibr" rid="c30">Linsker, 1988</xref>; <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="c58">Wei and Stocker, 2015</xref>; <xref ref-type="bibr" rid="c1">Atick and Redlich, 1990</xref>). Efficient coding theories formalise the problem of optimal coding subject to biological constraints and costs, and by finding the optimal solution make predictions about the behavior of nervous systems (which are postulated to have been approximately optimised, <italic>e</italic>.<italic>g</italic>., via natural selection). Concretely, the Infomax Principle (<xref ref-type="bibr" rid="c30">Linsker, 1988</xref>; <xref ref-type="bibr" rid="c1">Atick and Redlich, 1990</xref>; <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="c58">Wei and Stocker, 2015</xref>) states that sensory systems optimise the mutual information between a noisy neural representation and an external stimulus (or alternatively, the ideal noise-free result of computations on external stimuli), subject to metabolic constraints. In order to do so, the neural system should exploit the statistics of stimuli in its local environment (<xref ref-type="bibr" rid="c44">Simoncelli and Olshausen, 2001</xref>). Efficient coding theories can therefore predict how sensory systems should optimally <italic>adapt</italic> to changes in environmental stimulus statistics.</p>
<p>Suppose an environmental shift makes a stimulus feature more prevalent. Neurons sensitive to that feature will then see their preferred feature more often, and thus their average firing rate will initially increase. However, typically, the neurons do gradually adapt by reducing their responsiveness or gain (<xref ref-type="bibr" rid="c47">Solomon and Kohn, 2014</xref>; <xref ref-type="bibr" rid="c10">Clifford et al., 2007</xref>; <xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>). A special case of such adaptation is <italic>firing rate homeostasis</italic> (<xref ref-type="bibr" rid="c11">Desai, 2003</xref>; <xref ref-type="bibr" rid="c54">Turrigiano and Nelson, 2004</xref>; <xref ref-type="bibr" rid="c31">Maffei and Turrigiano, 2008</xref>; <xref ref-type="bibr" rid="c21">Hengen et al., 2013</xref>), in which adaptation brings the average firing rate back to the level prior to the environmental shift (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). Thus, under firing rate homeostasis, neuronal populations maintain a constant stimulus-averaged firing rate in spite of changes to the environment (<xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>; <xref ref-type="bibr" rid="c21">Hengen et al., 2013</xref>; <xref ref-type="bibr" rid="c31">Maffei and Turrigiano, 2008</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Homeostatic adaptation via gain modulation (cartoon example).</title>
<p>Each column of panels corresponds to an environment as defined by its stimulus distribution (bottom row). The top panels show the post-adaptation tuning curves of the same four neurons in these two environments. Neurons adapt only by independently adjusting their gains, <italic>i</italic>.<italic>e</italic>., by scaling their tuning curves up or down (correspondingly, tuning curves of the same color in the left and right columns, which belong to the same neuron, are scaled versions of each other and have identical shape). This adjustment is such that each neuron maintains the same stimulus-averaged firing rate in both environments. This is shown by the bar plots in the insets of the top row panels: the bar plot in the left column shows the average rates of the four neurons in the first environment, while the two bar plots in the right column show the average rates of these neurons before and after gain adaptation to the new statistics in the second environment. After adaptation the average rates return to their level before the environmental shift.</p></caption>
<graphic xlink:href="564616v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>There are multiple levels at which homeostatic adaptation can be observed. Firstly, there is population homeostasis, in which the stimulus-average firing rate of an entire population of neurons remains constant, without individual neurons necessarily holding their rates constant (<xref ref-type="bibr" rid="c45">Slomowitz et al., 2015</xref>; <xref ref-type="bibr" rid="c41">Sanzeni et al., 2023</xref>). Secondly, there is what we term <italic>cluster homeostasis</italic>. In this form of homeostasis, stimulus-average firing rate of groups or clusters of neurons with similar stimulus tuning remains stable under environmental shifts (<xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>), but the firing rate of individual neurons within a cluster can change. Lastly, in the strongest form, homeostasis can occur at the level of individual neurons, in which case the firing rate of each individual neuron is kept constant under changes in environment statistics (<xref ref-type="bibr" rid="c32">Marder and Prinz, 2003</xref>). Previous normative explanations for firing rate homeostasis often focus on the necessity of preventing the network from becoming hypo- or hyperactive (<xref ref-type="bibr" rid="c54">Turrigiano and Nelson, 2004</xref>; <xref ref-type="bibr" rid="c31">Maffei and Turrigiano, 2008</xref>; <xref ref-type="bibr" rid="c23">Keck et al., 2013</xref>; <xref ref-type="bibr" rid="c21">Hengen et al., 2013</xref>). Although this might explain homeostasis at the population level, it does not adequately explain homeostasis at a more fine-grained level. As we will show in this paper, the answer to the normative question that we posed above provides an explanation for firing rate homeostasis at such a finer level. Specifically, we will show that, for a wide variety of neural noise models (and under conditions that we argue do obtain in the visual cortex), optimal adjustment of neural gains to combat the effect of coding noise predicts firing rate homeostasis at the level of clusters of similarly-tuned neurons.</p>
<p>Having shown the optimality of homeostatic codes without making assumptions on the nature of the neural representation and computation, we apply our framework to <italic>Distributed Distributional Codes (DDC)</italic> (<xref ref-type="bibr" rid="c56">Vertes and Sahani, 2018</xref>), as a specific computational theory for neural implementation of Bayesian inference. Combining DDC theory (which determines tuning curve shapes) with our normative results (on optimal adjustments of neural gains) yields what we call <italic>homeostatic DDC</italic>. We will show that homeostatic DDC predict the typical finding that sensory adaptations are stimulus specific (<xref ref-type="bibr" rid="c25">Kohn, 2007</xref>; <xref ref-type="bibr" rid="c42">Schwartz et al., 2007</xref>). In stimulus specific adaptation (SSA), the suppression of response is greater for test stimuli that are closer to an over-represented adaptor stimulus than for stimuli further away (to which the neuron may even respond more strongly after adaptation). In particular, this causes a repulsion of tuning curves away from the adaptor (<xref ref-type="bibr" rid="c35">Movshon and Lennie, 1979</xref>; <xref ref-type="bibr" rid="c36">MÃ¼ller et al., 1999</xref>; <xref ref-type="bibr" rid="c14">Dragoi et al., 2000</xref>, <xref rid="c13" ref-type="bibr">2002</xref>; <xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>). We will show that homeostatic DDC are able to account for SSA effects which cannot be fully accounted for in previous efficient coding frameworks (<xref ref-type="bibr" rid="c58">Wei and Stocker, 2015</xref>; <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="c46">Snow et al., 2016</xref>), and we will use them to quantitatively model homeostatic SSA as observed in V1, by re-analysing previously published data (<xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>). This model relies on a special class of homeostatic DDC which we term <italic>Bayes-ratio coding</italic>. We show that Bayes-ratio coding has attractive computational properties: it can be propagated between populations without synaptic weight adjustments, and it can be achieved by divisive normalisation with adaptive weights (<xref ref-type="bibr" rid="c9">Carandini and Heeger, 2012</xref>; <xref ref-type="bibr" rid="c60">Westrick et al., 2016</xref>).</p>
<p>We start the next section by introducing our efficient coding framework for addressing the normative question posed above. We show numerically that our framework predicts that the firing rate of clusters of similarly tuned neurons should remain constant despite shifts in environmental stimulus statistics, with individual neurons free to shuffle their average rates. This result is shown to hold across a diverse set of noise models, demonstrating the robustness of the optimality of homeostasis to the precise nature of neural response noise. We additionally demonstrate that our framework can account for a wide distribution of stimulus-averaged single-neuron firing rates as observed in cortex. We then provide an analytic argument that demonstrates why, within a certain parameter regime, homeostasis arises from our optimisation problem, and show that cortical areas, in particular the primary visual cortex (V1), are likely to be within this parameter regime. We then numerically validate the quality of our analytic approximations to the optimal solution. Lastly, we apply our theory to DDC representational codes, showing how homeostatic DDC can account for stimulus specific adaptation effects observed experimentally.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<sec id="s2a">
<label>2.1</label>
<title>Theoretical framework</title>
<p>The central question we seek to address is how the presence of response noise affects optimal neural coding strategies. Specifically, we consider how a neural population should distribute activity in order to best mitigate the effect of neural noise on coding fidelity, subject to metabolic constraints, irrespective of the computations it performs on sensory inputs (which we do not optimise, but take as given). We consider a population of <italic>K</italic> neuronal units, responding to the (possibly high-dimensional) stimulus <bold><italic>s</italic></bold>. We assume that our population engages in rate coding using time bins of a fixed duration, and denote the vector of joint population spike counts in a coding interval by <bold><italic>n</italic></bold> = (<italic>n</italic><sub>1</sub>, â¦, <italic>n</italic><sub><italic>K</italic></sub>). Single-trial neural responses, <bold><italic>n</italic></bold>, are taken to be noisy emissions from neural tuning curves, <bold><italic>h</italic></bold>(<bold><italic>s</italic></bold>), according to a noise distributions <bold><italic>n</italic></bold> |<bold><italic>s</italic></bold> â¼<italic>P</italic><sub>noise</sub>(<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>), subject to ð¼[<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>] = <bold><italic>h</italic></bold>(<bold><italic>s</italic></bold>). We consider a number of different noise models, including correlated and power-law noise, showing that our main results are robust to specific assumptions about the nature of noise in neural systems. The sensory environment is characterised by a stimulus distribution <italic>P</italic> (<bold><italic>s</italic></bold>); accordingly, an âenvironmental shiftâ corresponds to a change in <italic>P</italic> (<bold><italic>s</italic></bold>) (making certain stimuli more or less prevalent).</p>
<p>We are interested in how changes in the stimulus distribution, <italic>P</italic>, affect the responsiveness of population neurons. We therefore adopt a shape-amplitude decomposition of the neural tuning curves. The tuning curve of the <italic>a</italic>-th unit, <italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>), is factorised into a <italic>representational curve</italic>, Î©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>), and a <italic>gain, g</italic><sub><italic>a</italic></sub>:
<disp-formula id="ueqn1">
<graphic xlink:href="564616v3_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<xref rid="fig2" ref-type="fig">Fig. 2A</xref> demonstrates the effect of changing the gain while keeping the representational curve constant. We take the Î©<sub><italic>a</italic></sub> to encode the computations represented by the neural population, and thus to be determined by those computational goals; we therefore treat them as given, and do not optimise them within our framework. Importantly, we do not make any assumptions on the shape of the representational curve: Î©<sub><italic>a</italic></sub> can be any complex (<italic>e</italic>.<italic>g</italic>., multi-modal, discontinuous) function of the possibly high-dimensional stimulus, and can thus represent <italic>any</italic> computation. In this respect, our treatment is more general than other efficient coding frameworks (<italic>e</italic>.<italic>g</italic>., <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref>), which place tight constraints on the shape and configuration of the tuning curves (see the Discussion for a more detailed comparison of our approach with those studies). In particular, this generality enables our theory to apply to populations located deep in the processing pathway, and not just to primary sensory neurons.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>The effect of changing the gain on tuning curves and different components of the objective function.</title>
<p>(A) As the gain, <italic>g</italic>, of a unit is increased, the shape of its tuning curve remains the same, but all firing rates are scaled up. (The cartoon shows a one-dimensional example, but our theory applies to general tuning curve shapes and joint configurations of population tuning curves, on high-dimensional stimulus spaces.) (B) Cartoon representation of the efficient coding objective function. Optimal neural gains maximise an objective function, â, which is the weighted difference between mutual information, <italic>I</italic>, capturing coding fidelity, and metabolic cost, <italic>E</italic>, given by average population firing rate.</p></caption>
<graphic xlink:href="564616v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Our framework, relating environmental stimuli to neural tuning curves and finally to noise responses, is summarised by the diagram in <xref rid="fig3" ref-type="fig">Fig. 3</xref>.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Diagrammatic representation of the generative model underlying the theoretical framework.</title>
<p>The environment gives rise to a stimulus distribution <italic>P</italic> from which the stimulus <bold><italic>s</italic></bold> is drawn. Conceptually, the brain performs some computation on <bold><italic>s</italic></bold>, the result of which are represented by neurons via their representational curves Î©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>). These are multiplied by adaptive gains, <italic>g</italic><sub><italic>a</italic></sub>, to yields the actual tuning curves <italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>). The single-trial neural responses are noisy emissions based on <italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>).</p></caption>
<graphic xlink:href="564616v3_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We theorise that units adapt their gains to maximise an objective function that trades off the metabolic cost of neural activity with the information conveyed by the responses (<xref ref-type="bibr" rid="c29">Levy and Baxter, 1996</xref>; <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli, 2014</xref>). Informally
<disp-formula id="eqn1">
<graphic xlink:href="564616v3_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
As is common in efficient coding frameworks (see <italic>e</italic>.<italic>g</italic>., <xref ref-type="bibr" rid="c1">Atick and Redlich (1990)</xref>; <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref>) we assume the main contributor to metabolic cost is the energy cost of emitting action potentials. Thus we take the metabolic cost term in the objective to be proportional to <inline-formula><inline-graphic xlink:href="564616v3_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the total average spike count fired by the population. On the other hand, a natural and common choice (up to approximations) for the coding fidelity is the mutual information between the stimulus and response, <italic>I</italic>(<bold><italic>n</italic></bold>; <bold><italic>s</italic></bold>). However, since in general the analytic maximisation of mutual information is intractable, in the tradition of efficient coding theory (<xref ref-type="bibr" rid="c7">Brunel and Nadal, 1998</xref>; <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="c30">Linsker, 1988</xref>), we will instead optimise an approximate surrogate for mutual information. We decompose the mutual information as <italic>I</italic>(<bold><italic>s</italic></bold>; <bold><italic>n</italic></bold>) = <italic>H</italic>[<bold><italic>n</italic></bold>] â<italic>H</italic>[<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>]. The marginal entropy term <italic>H</italic>[<bold><italic>n</italic></bold>] can be upper bounded by the entropy of a Gaussian with the same covariance,
<disp-formula id="eqn2">
<graphic xlink:href="564616v3_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Substituting the right hand side for the marginal entropy term in the mutual information, gives us an upper bound for the latter. Using this upper bound on mutual information as our quantification of âcoding fidelityâ, yields the objective function
<disp-formula id="eqn3">
<graphic xlink:href="564616v3_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the constant <italic>Âµ</italic> &gt; 0 controls the information-energy trade-off. This trade-off is illustrated in <xref rid="fig2" ref-type="fig">Fig. 2B</xref>. To summarise, we assume that the optimal gains maximize the objective <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>
<disp-formula id="eqn4">
<graphic xlink:href="564616v3_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Note that because the Î©<sub><italic>a</italic></sub> depend deterministically on the stimulus <bold><italic>s</italic></bold>, and in turn fully determine the statistics of <italic>n</italic><sub><italic>a</italic></sub>, we have the identity <italic>I</italic>(<bold><italic>n</italic></bold>; <bold><italic>s</italic></bold>) = <italic>I</italic>(<bold><italic>n</italic></bold>; <bold>Î©</bold>), which also holds for the upper bound on the mutual information that we are using. This means that the coding fidelity term in the objective function can equivalently be interpreted, without any reference to the low-level stimulus, as (an upper bound on) the mutual information between the populationâs noisy output and its ideal, noise-free outputs, <bold>Î©</bold>, as specified by the computational goals of the circuit.</p>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Optimal adaptation of gains leads to rate homeostasis across a variety of noise models</title>
<p>We now investigate the consequences of optimal gain adaptation, according to <xref ref-type="disp-formula" rid="eqn3">Eqs. (3)</xref>â<xref ref-type="disp-formula" rid="eqn4">(4)</xref>, on the statistics of population response. As our hypothesis is that gains adapt in order to optimally mitigate the effect of neural noise on coding fidelity (with minimal energy expenditure), ideally we would like our conclusions to be robust to the model of noise. We thus consider a broad family of noise models which allow for general noise correlation structure, as well as different (sub- or super-Poissonian) scalings of noise strength with mean response. Specifically, we assume that conditioned on the stimulus, the noisy population response, <bold><italic>n</italic></bold>, has a normal distribution, with mean <bold><italic>h</italic></bold>(<bold><italic>s</italic></bold>), and covariance matrix
<disp-formula id="eqn5">
<graphic xlink:href="564616v3_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>h</italic>(<bold><italic>s</italic></bold>) is the diagonal matrix with the components of <bold><italic>h</italic></bold>(<bold><italic>s</italic></bold>) on its diagonal (more generally, we adopt the convention that the non-bold and index-free version of a bold symbol, representing a vector, denotes a diagonal matrix made from that vector), <italic>Ï</italic> scales the overall strength of noise, and 0 &lt; <italic>Î±</italic> &lt; 1 is a parameter that controls the power-law scaling of noise power with mean response <bold><italic>h</italic></bold>(<bold><italic>s</italic></bold>). Finally, Î£(<bold><italic>s</italic></bold>) is the noise correlation matrix which can depend on the stimulus, but is independent of the gains. The noise model <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref> can equivalently be characterised by (1) the noise variance of spike count <italic>n</italic><sub><italic>a</italic></sub> is given by <italic>Ï</italic><sup>2</sup><italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>)<sup>2<italic>Î±</italic></sup>, and (2) the noise correlation coefficient between <italic>n</italic><sub><italic>a</italic></sub> and <italic>n</italic><sub><italic>b</italic></sub> is given by Î£<sub><italic>ab</italic></sub>(<bold><italic>s</italic></bold>), independently of gains. Thus the strength (standard deviation) of noise scales sublinearly as gain to the power <italic>Î±</italic>. Note that the case <italic>Ï</italic> = 2<italic>Î±</italic> = 1 corresponds to Poisson-like noise with variance of <italic>n</italic><sub><italic>a</italic></sub> equal to its mean; in App. B.2, we show that, under mild conditions, a true Poisson noise model (with no noise correlations) leads to the same results as we derive below for the Gaussian noise model, <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>, with <italic>Ï</italic> = 2<italic>Î±</italic> = 1 and Î£(<bold><italic>s</italic></bold>) = <italic>I</italic>.</p>
<p>In general, the objective â, <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>, depends tacitly on the stimulus distribution <italic>P</italic> (<bold><italic>s</italic></bold>) and the tuning curves <bold>Î©</bold>(<bold><italic>s</italic></bold>). As shown in App. B.1, for our class of noise models, <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>, â depends on <italic>P</italic> and <bold>Î©</bold> only through the following collection of parameters:</p>
<list list-type="order">
<list-item><p>The unitsâ pre-modulated average response (<italic>i</italic>.<italic>e</italic>., the average spike count before multiplication by the adaptive gain <italic>g</italic><sub><italic>a</italic></sub>)
<disp-formula id="eqn6">
<graphic xlink:href="564616v3_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
These are related to the actual average responses, <italic>r</italic><sub><italic>a</italic></sub>, via <italic>r</italic><sub><italic>a</italic></sub> = ð¼ [<italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>)] = <italic>g</italic><sub><italic>a</italic></sub><italic>Ï</italic><sub><italic>a</italic></sub>.</p></list-item>
<list-item><p>The coefficients of variation of Î©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>), which we denote by CV<sub><italic>a</italic></sub>.</p></list-item>
<list-item><p>The stimulus-averaged signal correlations, <italic>i</italic>.<italic>e</italic>., the Pearson correlation coefficients between Î©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>)âs. We denote the matrix of these signal correlations by <italic>Ï</italic>.</p></list-item>
<list-item><p>The matrix <italic>W</italic> with elements defined by
<disp-formula id="eqn7">
<graphic xlink:href="564616v3_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<italic>W</italic> can be viewed as a stimulus-averaged normalised noise covariance matrix (note that for <italic>Î±</italic> = 1<italic>/</italic>2 this matrix is a correlation matrix, in the sense that it is positive-definite and diagonal elements equal to 1.)</p></list-item>
</list>
<p>Note that these quantities depend on the stimulus distribution <italic>P</italic> and the functions Î©<sub><italic>a</italic></sub>. Note also that, except for <italic>Ï</italic><sub><italic>a</italic></sub>, the other three sets of quantities are invariant with respect to rescalings of the representational curves, and therefore can be equivalently defined in terms of the actual tuning curves, <italic>h</italic><sub><italic>a</italic></sub>, rather than Î©<sub><italic>a</italic></sub>. Thus, CV<sub><italic>a</italic></sub>âs are, equivalently, the coefficients of variation of the neuronsâ trial-averaged responses, and <italic>Ï</italic><sub><italic>ab</italic></sub>âs) are the correlation coefficients between pairs of trial-averaged responses (hence âsignal correlationâ).</p>
<p>As shown in App. B.1, in terms of the above quantities, the objective function, <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>, is given by
<disp-formula id="eqn8">
<graphic xlink:href="564616v3_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where, in line with the convention introduced above, CV, <italic>Ï</italic>, and <italic>g</italic> denote <italic>K</italic> Ã <italic>K</italic> diagonal matrices with CV<sub><italic>a</italic></sub>, <italic>Ï</italic><sub><italic>a</italic></sub>, and <italic>g</italic><sub><italic>a</italic></sub> on their diagonals, respectively. We have obtained the expression <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref> for the objective for the case of a Gaussian noise model, <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>. This noise model is very general, in the sense that it can capture any power-law relationship between the mean and variance of the response (given a stimulus) through <italic>Î±</italic>, and any noise strength and correlation structure through <italic>Ï</italic><sup>2</sup> and Î£(<bold><italic>s</italic></bold>). However, spike counts are discrete rather than continuous, a commonly used noise model for which is the Poisson distribution. In App. B.2, we demonstrate that, under appropriate conditions, for an independent Poisson noise model the objective <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref> is approximately given by <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref> with <italic>Î±</italic> = 1<italic>/</italic>2 and with <italic>Ï</italic><sup>2</sup><italic>W</italic> replaced by the identity matrix.</p>
<p>We now present the results of numerical optimisation of the objective <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref> for different noise distributions within the general family of noise models, <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>. To capture the notion of adaptation under environmental shift, we simulated a family of environment models smoothly parameterised by <italic>Î½</italic> â [0, 1]. An environment corresponds to a stimulus density <italic>P</italic> (<bold><italic>s</italic></bold>), which, as we have just seen, affects the objective only via <italic>Ï, Ï, W</italic>, and CV. We will therefore specify environments through these four quantities. For simplicity, we fixed <inline-formula><inline-graphic xlink:href="564616v3_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, <italic>Ï</italic><sup>2</sup>, and <italic>Âµ</italic> at 10, 1, and 5<italic>/</italic>(1 â â <italic>Î±</italic>), in all environments. (Below, in <xref ref-type="sec" rid="s2e">Sec. 2.5</xref>, we will discuss the biological basis for these parameter choices.) On the other hand, <italic>Ï</italic><sub><italic>a</italic></sub> and <italic>Ï</italic> depended smoothly on the environment parameter <italic>Î½</italic>, as follows. For each <italic>a</italic> = 1, â¦, <italic>K, Ï</italic><sub><italic>a</italic></sub>(<italic>Î½</italic> = 0) and <italic>Ï</italic><sub><italic>a</italic></sub>(<italic>Î½</italic> = 1) are drawn independently from a wide distribution (specifically the beta distribution Beta(6, 1)). The value of <italic>Ï</italic><sub><italic>a</italic></sub>(<italic>Î½</italic>) at intermediate values of <italic>Î½</italic> were then obtained by linear interpolation between the independently sampled boundary values. This method for sampling environments is illustrated in <xref rid="fig4" ref-type="fig">Fig. 4</xref>. Similarly, to construct the matrix <italic>Ï</italic>(<italic>Î½</italic>), we first constructed <italic>Ï</italic>(<italic>Î½</italic> = 0) and <italic>Ï</italic>(<italic>Î½</italic> = 1) with independent random structures and smoothly interpolated between them (see <xref ref-type="sec" rid="s4a">Sec. 4.1</xref> for the details). We did this in such a way that, for all <italic>Î½</italic>, the correlation matrices, <italic>Ï</italic>(<italic>Î½</italic>), were derived from covariance matrices with a 1<italic>/n</italic> power-law eigenspectrum (<italic>i</italic>.<italic>e</italic>., the ranked eigenvalues of the covariance matrix fall off inversely with their rank), in line with the findings of <xref ref-type="bibr" rid="c49">Stringer et al. (2019)</xref> in the primary visual cortex. Finally, the stimulus-averaged noise correlation matrix, <italic>W</italic>, is determined by our choice of the noise model subfamily used in each simulation. We now present simulation results for three 1-parameter subfamilies of the general family of noise models, <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>, in turn.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>The structure of pre-modulated average population responses in the simulated environments.</title>
<p>We simulated a one-parameter family of environments parameterised by <italic>Î½</italic> â [0, 1]. In the environments on the two extremes, for each unit, the pre-modulated responses <italic>Ï</italic><sub><italic>a</italic></sub>(<italic>Î½</italic> = 0) and <italic>Ï</italic><sub><italic>a</italic></sub>(<italic>Î½</italic> = 1) are randomly and independently sampled from a Beta(1, 6) distribution, with density shown on the left and right panels. For intermediate environments (<italic>i</italic>.<italic>e</italic>., for 0 &lt; <italic>Î½</italic> &lt; 1), <italic>Ï</italic><sub><italic>a</italic></sub>(<italic>Î½</italic>) is then obtained by linearly interpolating these values (middle panel). The middle panel shows 10000 such interpolations, with every 500th interpolation (ordered by initial value) coloured black.</p></caption>
<graphic xlink:href="564616v3_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<sec id="s2b1">
<title>Uncorrelated power-law noise</title>
<p>Consider first the case of no average noise correlations, in the sense that <italic>W</italic> = <italic>I</italic> (in terms of the objective function, <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>, this is mathematically equivalent to having zero stimulus-conditioned noise correlations, <italic>i</italic>.<italic>e</italic>., Î£(<bold><italic>s</italic></bold>) = 1, but with CVâs redefined not to denote the standard coefficient of variation, but rather <inline-formula><inline-graphic xlink:href="564616v3_inline106.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <inline-formula><inline-graphic xlink:href="564616v3_inline107.gif" mime-subtype="gif" mimetype="image"/></inline-formula>; for Poisson-like scaling, <italic>Î±</italic> = 1<italic>/</italic>2, this reduces to the standard coefficient of variation). We investigated the adaptation behavior of optimally gain-modulated population responses as a function of the noise scaling exponent <italic>Î±</italic>. <xref rid="fig5" ref-type="fig">Fig. 5A</xref> shows the scaling of spike count variance as a function of mean spike count for different values of <italic>Î±</italic>. By construction, in the absence of adaptation, an environmental shift (<italic>Î½</italic> = 0 â <italic>Î½</italic> = 1) results in a wide range of average firing rates (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>, middle panel). But following adaptation, the firing rates become tightly concentrated around a single value (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>, bottom panel), which matches the value they were concentrated around in the original environment (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>, top panel). Thus, adaptation results in (1) <italic>uniformisation</italic> of average firing rates across the population in a fixed environment, and (2) firing rate <italic>homeostasis</italic>, where following adaptation the mean firing rates of the units returns to their value before the environmental shift. These same uniformisation and homeostatic effects are seen for other values of <italic>Î±</italic> (<xref rid="fig5" ref-type="fig">Fig. 5B</xref> and Supp. Fig. 14). In <xref rid="fig5" ref-type="fig">Fig. 5B</xref> we quantify deviation from perfect homeostasis as the average % difference in the unitsâ mean firing rates between the initial environment (<italic>Î½</italic> = 0) and environments at increasing <italic>Î½</italic>. Even under super-poissonian noise (<italic>Î±</italic> = 0.75 in <xref rid="fig5" ref-type="fig">Fig. 5B</xref>), the average relative change in firing rates between environments never gets above 3%. <xref rid="fig5" ref-type="fig">Fig. 5B</xref> additionally shows that the slower spike count variance grows with mean spike count (<italic>i</italic>.<italic>e</italic>., the smaller <italic>Î±</italic> is) the more precise the firing rate homeostasis.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Optimal gains under uncorrelated power-law noise lead to homeostasis of rates.</title>
<p>(A) Illustration of power-law noise. The trial-to-trial variance of spike counts, as a function of the trial-averaged spike count, is shown for different values of the noise scaling parameter <italic>Î±</italic>. (C) Distribution of average firing rates before and after a discrete environmental shift from <italic>Î½</italic> = 0 to <italic>Î½</italic> = 1 for the case <italic>Î±</italic> = 1<italic>/</italic>2. Top: the histogram of firing rates adapted to the original environment (<italic>Î½</italic> = 0) before the shift, as determined by the optimal gains <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref> in this environment, <inline-formula><inline-graphic xlink:href="564616v3_inline102.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Middle: the histogram of firing rates immediately following the environmental shift to <italic>Î½</italic> = 1, but before gain adaptation; these are proportional to <inline-formula><inline-graphic xlink:href="564616v3_inline103.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Bottom: the histogram of firing rates after adaptation to the new environment, proportional to <inline-formula><inline-graphic xlink:href="564616v3_inline104.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We have normalised firing rates such that the pre-adaptation firing rate distribution has mean 1. (B) Deviation from firing rate homeostasis for different values of <italic>Î±</italic> as a function of environmental shift. For each environment (parametrised by <italic>Î½</italic>), we compute the optimal gains and find the adapted firing rate under these gains. The average relative deviation of the (post-adaptation) firing rates from their value before the environmental shift, <inline-formula><inline-graphic xlink:href="564616v3_inline105.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, is plotted as a function of <italic>Î½</italic>.</p></caption>
<graphic xlink:href="564616v3_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2b2">
<title>Uniform noise correlations</title>
<p>In the second sub-family of noise models we allow for noise correlations, but consider only Poisson-like scaling of noise power (<italic>i</italic>.<italic>e</italic>., <italic>Î±</italic> = 1<italic>/</italic>2). More precisely, we assume that the effective noise correlation coefficients between all pairs of units (corresponding to off-diagonal elements of the matrix <italic>W</italic>, <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>) are the same, equal to <italic>p</italic> (<xref rid="fig6" ref-type="fig">Fig. 6A</xref>) (the positivity of the noise correlation matrix, <italic>W</italic>, requires that <inline-formula><inline-graphic xlink:href="564616v3_inline108.gif" mime-subtype="gif" mimetype="image"/></inline-formula>; assuming large <italic>K</italic>, the right side of the inequality goes to zero; hence we only simulated cases with positive <italic>p</italic>). As shown in <xref rid="fig6" ref-type="fig">Fig. 6C</xref>, optimal gain adaptation leads to uniformisation and homeostasis of mean firing rates in this noise model as well. In fact, in the presence of uniform noise correlations, homeostasis is even stronger compared to the previous noise model with zero noise correlations; in this case, mean relative deviations in average firing rates never exceed 1% (<xref rid="fig6" ref-type="fig">Fig. 6B</xref>). Further, as noise correlations get stronger (<italic>i</italic>.<italic>e</italic>., as <italic>p</italic> increases) we see tighter homeostasis.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Firing rate homeostasis under Poissonian noise with uniform noise correlations.</title>
<p>(A) Illustration of an effective noise correlation matrix with uniform correlation coefficients (off-diagonal elements). Panels B and C have the same format as in <xref ref-type="fig" rid="fig5">Fig. 5</xref>.</p></caption>
<graphic xlink:href="564616v3_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2b3">
<title>Aligned correlated noise</title>
<p>In the final noise model we considered, we once again fixed <italic>Î±</italic> = 1<italic>/</italic>2 (Poisson-like scaling), but allowed for heterogeneous effective noise correlation coefficients. Specifically, we took the effective noise correlation matrix <italic>W</italic> to have approximately the same eigenbasis as <italic>Ï</italic>, the signal correlation matrix âhence âaligned noiseââ, but with a different eigenvalue spectrum. As described above, for all noise sub-families, <italic>Ï</italic>(<italic>Î½</italic>) was obtained by normalising a covariance matrix with 1<italic>/n</italic> spectrum (corresponding to the findings of <xref ref-type="bibr" rid="c49">Stringer et al. (2019)</xref> in V1); in the current case, we obtained <italic>W</italic> (<italic>Î½</italic>) by normalising a covariance matrix with the same eigenbasis but with a 1<italic>/n</italic><sup><italic>Î³</italic></sup> spectrum (<italic>i</italic>.<italic>e</italic>., with the <italic>n</italic>-th largest eigenvalue of <italic>W</italic> scaling as 1<italic>/n</italic><sup><italic>Î³</italic></sup>); see <xref rid="fig7" ref-type="fig">Fig. 7A</xref>, and <xref ref-type="sec" rid="s4a">Sec. 4.1</xref> for further details. <xref rid="fig7" ref-type="fig">Fig. 7C</xref> demonstrates that, as with the other noise sub-families, optimal gain adaptation in the presence of aligned correlated noise also leads to homeostasis and uniformisation of mean firing rates (see Supp. Fig. 16 for rate histograms for other values of <italic>Î³</italic>). Across different values of <italic>Î³</italic>, homeostasis is again tighter compared to the zero noise correlation case, with the average firing rate shift never exceeding 1% (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>). Further, with the exception of <italic>Î³</italic> = 1, we see more homeostasis for higher values of <italic>Î³</italic>, corresponding to lower dimensional noise. The special case of <italic>Î³</italic> = 1 corresponds to so-called information-limiting noise (<xref ref-type="bibr" rid="c33">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="c22">Kanitscheider et al., 2015</xref>), where noise and signal correlation structures are fully aligned: <italic>W</italic> = <italic>Ï</italic>. In this case, homeostasis is perfect, and the mean relative error in firing rates is 0. We provide an analytic proof that in this case we have perfect homeostasis and uniformisation in App. B.5, and discuss this situation further in <xref ref-type="sec" rid="s2e">Sec. 2.5</xref> and <xref ref-type="sec" rid="s2f">Sec. 2.6</xref>.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7:</label>
<caption><title>Firing rate homeostasis under Poissonian, aligned noise.</title>
<p>(A) Eigenspectra of noise covariance matrix for the aligned noise model for various values of <italic>Î³</italic>; in this model, the <italic>n</italic>-th eigenvalue is proportional to 1<italic>/n</italic><sup><italic>Î³</italic></sup>. Panels B and C have the same format as in <xref ref-type="fig" rid="fig5">Fig. 5</xref>.</p></caption>
<graphic xlink:href="564616v3_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We have shown that, according to our efficient coding framework, optimal adaptation of response gains (with the goal of combating neural noise at minimal metabolic cost) can robustly account for the firing rate homeostasis of individual units within a population, despite environmental shifts, under a diverse family of noise models. To shed light on these results, in <xref ref-type="sec" rid="s2d">Sec. 2.4</xref>, we analytically investigate sufficient conditions on the parameters, <xref ref-type="disp-formula" rid="eqn6">Eq. (6)</xref>â<xref ref-type="disp-formula" rid="eqn7">(7)</xref>, of our model under which we expect homeostasis to emerge. Following that, in <xref ref-type="sec" rid="s2e">Sec. 2.5</xref>, we will show that biological estimates for these parameters obtained in cortex (which are determined by natural stimulus statistics, as well as the stimulus dependence of high-dimensional population responses) are consistent with these conditions.</p>
<p>In addition to homeostasis, our framework also predicts uniformisation of average firing rates across the population, in any given environment. By contrast, cortical neurons display a wide range of average firing rates spanning a few orders of magnitude (<xref ref-type="bibr" rid="c8">BuzsÃ¡ki and Mizuseki, 2014</xref>). However, we can interpret the units in our preceding numerical experiments not as single neurons, but as clusters of similarly tuned neurons. In this interpretation, uniformisation occurs at the cluster level, with potentially a wide distribution of single-cell average firing rates within each cluster. In the next section, we show that our model is indeed consistent with such an interpretation. In particular, in the case of uncorrelated noise with Poisson-like mean-variance scaling, we show that a simple extension of our model gives rise to both diversity of firing rates as seen in cortex as well as homeostasis and uniformisation at the level of clusters of similarly tuned neurons.</p>
</sec>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>A clustered population accounts for the diversity of firing rates seen in cortex</title>
<p>A cortical neuron can have very similar response tuning to other neurons in the same area (particularly with nearby cells). We will call a collection of neurons which have similar (but not necessarily identical) representational curves a <italic>cluster</italic>. To gain intuition and allow for analytic solutions, we will henceforth adopt a toy model in which the <italic>N</italic> neurons in the population are sorted into <italic>K</italic> such clusters. In this toy model, neurons within a cluster have very similar stimulus tuning, but neurons in different clusters are tuned differently; the model thus exaggerates a more realistic situation in which there is a more gradual transition from similar to dissimilar tuning curves within the entire population. More concretely, we will assume that the representational curves of the neurons, indexed by <italic>i</italic>, belonging to cluster <italic>a</italic>, are small perturbations to the same representational curve characterising that cluster; <italic>i</italic>.<italic>e</italic>.,
<disp-formula id="eqn9">
<graphic xlink:href="564616v3_eqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>Ïµ</italic> is a small parameter controlling the deviation from perfect within-cluster similarity. Below, hatted symbols <inline-formula><inline-graphic xlink:href="564616v3_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denote variables defined analogously to <xref ref-type="disp-formula" rid="eqn6">Eq. (6)</xref>â<xref ref-type="disp-formula" rid="eqn7">(7)</xref> but for single neurons, with Î©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>) replaced by <inline-formula><inline-graphic xlink:href="564616v3_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p>We assume single-neuron gains adapt to optimise an objective â<sub>pop</sub>, defined analogously to the objective â, <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>, but for individual neurons rather than clusters. In the case of uncorrelated noise with Poisson-like scaling, this objective is given by
<disp-formula id="eqn10">
<graphic xlink:href="564616v3_eqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Where <inline-formula><inline-graphic xlink:href="564616v3_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are the mean firing rates of individual neurons, given by <inline-formula><inline-graphic xlink:href="564616v3_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula>; note that maximising â<sub>pop</sub> in terms of the single-neuron gains is equivalent to maximising it in terms of <inline-formula><inline-graphic xlink:href="564616v3_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. In App. B.3, we show that this objective can be expanded in <italic>Ïµ</italic> as
<disp-formula id="eqn11">
<graphic xlink:href="564616v3_eqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here, the leading order term, â (<bold><italic>r</italic></bold> = <italic>Ï</italic><bold><italic>g</italic></bold>), is a cluster-level objective and is gi(ven) by <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>, now viewed as a function of the cluster rates <italic>r</italic><sub><italic>a</italic></sub>, <italic>i</italic>.<italic>e</italic>., the totalLmean firing rate of neurons in a cluster; thus <inline-formula><inline-graphic xlink:href="564616v3_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>C</italic><sub><italic>a</italic></sub> is the set of neurons in cluster <italic>a</italic>. Note that this term is a function only of the cluster firing rates. Thus, for <italic>Ïµ</italic> = 0 (perfect similarity within clusters), the efficient coding objective function is blind to the precise distribution of single-cell firing rates, <inline-formula><inline-graphic xlink:href="564616v3_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, among the neurons of a cluster, as long as the total rate of the cluster is held fixed. This is because for clusters of identically-tuned and independently-firing neurons with Poisson-like noise, the cluster can be considered as a single coherent unit whose firing rate is given by the sum of the firing rates of its neurons. Thus at zeroth order, the distribution of individual neuron rates are free and undetermined by the optimisation, as long as they give rise to optimal cluster rates. At small but nonzero <italic>Ïµ</italic>, <inline-formula><inline-graphic xlink:href="564616v3_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is still dominated by â; we thus expect this term to play the dominant role in determining the cluster rates, with terms of order <italic>Ïµ</italic><sup>2</sup> and higher having negligible effect. However, the perturbation term <inline-formula><inline-graphic xlink:href="564616v3_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula> breaks the invariance of the loss with respect to changes in the distribution of single neuron gains (or rates) within clusters.</p>
<p>We will therefore approximate the maximisation of the total objective function â<sub>pop</sub> as follows. We first specify the cluster rates by maximising â (<bold><italic>r</italic></bold>), obtaining the optimal cluster rates <inline-formula><inline-graphic xlink:href="564616v3_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We then specify the rates of individual neurons within the population by maximising the leading correction term in the objective, <inline-formula><inline-graphic xlink:href="564616v3_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, subject to the constraint that the single-neuron rates in each cluster sum to that clusterâs optimal rate. We reinterpret the optimisation in the previous sections to correspond to the first stage of this two-stage optimisation, with population units there corresponding to clusters of similarly tuned neurons. As we have seen in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref>, optimisation of the cluster-level objective, â, can result in homeostasis and uniformisation across clusters.</p>
<p>As for the distribution of individual neuron rates, we show in App. B.3.4 (see the derivation of <xref ref-type="disp-formula" rid="eqn91">Eq. (91)</xref>) that, in the parameter regime (see <xref ref-type="sec" rid="s2d">Secs. 2.4</xref> and <xref ref-type="sec" rid="s2e">2.5</xref>) in which cluster-level homeostasis and uniformisation occurs, the perturbation objective â<sub>pert</sub> decomposes into a sum of objectives over different clusters. Thus, in the second stage of optimisation the problem decouples across clusters, and (as shown in App. B.3.4) the optimal single-neuron rates in cluster <italic>a</italic> maximise the objective
<disp-formula id="eqn12">
<graphic xlink:href="564616v3_eqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
subject to the constraints
<disp-formula id="eqn13">
<graphic xlink:href="564616v3_eqn13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here ÎÎ©<sub><italic>i</italic></sub> is defined to be the centered, zero-mean version of <italic>Î´</italic>Î©<sub><italic>i</italic></sub> (we can interpret ÎÎ©<sub><italic>i</italic></sub> as the direction of the perturbed tuning curve in excess of the cluster tuning curve Î©<sub><italic>a</italic></sub>):
<disp-formula id="eqn14">
<graphic xlink:href="564616v3_eqn14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Note that the optimisation problem, <xref ref-type="disp-formula" rid="eqn12">Eqs. (12)</xref>â<xref ref-type="disp-formula" rid="eqn13">(13)</xref>, is a quadratic program.</p>
<p>We solved the optimisation problem <xref ref-type="disp-formula" rid="eqn12">Eqs. (12)</xref>â<xref ref-type="disp-formula" rid="eqn13">(13)</xref> numerically for 10,000 randomly generated covariance matrices (note that different instances of the optimisation can be thought of as characterising different clusters within the same population). In general, the covariance matrix, <inline-formula><inline-graphic xlink:href="564616v3_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (which defines this optimization problem), depends on the stimulus distribution, the cluster tuning curve, and properties (<italic>e</italic>.<italic>g</italic>., degree of smoothness) of the intra-cluster variations in single-cell tuning curves. In our simulations, instead of making specific arbitrary choices for these different factors, we used a minimal toy model where the covariance matrix was generated as the matrix of inner products of <italic>k</italic> random vectors in a <italic>D</italic>-dimensional space (see <xref ref-type="sec" rid="s4b">Sec. 4.2</xref> for details). In App. B.3.5 we show that <italic>D</italic> can be interpreted as the dimensionality of the space of independent perturbations to the cluster-wide tuning curve Î©<sub><italic>a</italic></sub> which vary significantly on the portion of stimulus space to which the cluster tuning curve responds (below, we will refer to <italic>D</italic> as the effective tuning curve dimension).</p>
<p>In <xref rid="fig8" ref-type="fig">Fig. 8</xref> (see also Supp. Fig. 18) we show the resulting firing rate distributions for neurons aggregated across all clusters (corresponding to the different optimisations), for different values of the cluster size <italic>k</italic> and effective tuning curve dimension, <italic>D</italic>, of within-cluster tuning curve variations. Firstly, note that, depending on these parameters, a significant fraction of neurons can be silent, <italic>i</italic>.<italic>e</italic>., with zero firing rate in the optimal solution; the figure shows the rate distributions for the active neurons, as well as their percentage of all neurons. Our theory therefore predicts that a significant fraction of cortical neurons are silent in any given sensory environment, and that which neurons are silent can shuffle following shifts in environmental stimulus statistics (specifically shifts that result in significant changes in the signal correlation structure of the cluster neurons). Secondly, note that the distributions span multiple orders of magnitude, in agreement with empirical observations in the cortex (see <italic>e</italic>.<italic>g</italic>. <xref ref-type="bibr" rid="c45">Slomowitz et al. (2015)</xref>; <xref ref-type="bibr" rid="c21">Hengen et al. (2013)</xref>; <xref ref-type="bibr" rid="c31">Maffei and Turrigiano (2008)</xref>), and is approximately log-normal (albeit with a skewed tail towards small log-rates), also consistent with empirical findings (<xref ref-type="bibr" rid="c8">BuzsÃ¡ki and Mizuseki, 2014</xref>). Lastly, for fixed <italic>k</italic>, as the effective tuning curve dimension, <italic>D</italic>, increases, the fraction of silent neurons decreases (Supp. Fig. 18).</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8:</label>
<caption><title>The distribution of single neuron firing rates is consistent with empirical observations of approximately log-normal rates.</title>
<p>Each panel shows the distribution of log firing rates of the active single neurons for different choices of cluster size <italic>k</italic> and effective tuning curve dimension, <italic>D</italic>, of the space of within-cluster tuning curve variations (see the main text for the definition, and <xref ref-type="sec" rid="s4b">Sec. 4.2</xref> for further details). The distributions shown are for the active neurons with nonzero firing rate, with their percentage of the total population given in each panel (purple text). For all simulations we fixed the cluster firing rate to <italic>Âµ</italic> = <italic>k</italic> Ã 5 Hz, such that the mean rate of single neurons is 5 Hz on average. The plotted densities were obtained by averaging over 10, 000 optimisations of <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref> based on different random draws of Cov(Î<bold>Î©</bold>).</p></caption>
<graphic xlink:href="564616v3_fig8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To summarise, we have shown that the objective â considered in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref>, reinterpreted to correspond to the cluster level, can arise from optimisation of a similar objective, â<sub>pop</sub>, defined at the level of single neurons (<xref ref-type="disp-formula" rid="eqn10">Eqs. (10)</xref>â<xref ref-type="disp-formula" rid="eqn11">(11)</xref>). Furthermore, the correction term to the cluster-level objective, â<sub>pert</sub>, serves to break the symmetry within clusters, generating a diverse range of firing rates which matches the distribution of cortical single-neuron firing rates. In the rest of the paper, we will pursue the problem at the coarser level of clusters.</p>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Analytical insight on the optimality of homeostasis</title>
<p>The numerical results of <xref ref-type="sec" rid="s2b">Sec. 2.2</xref> showed that, across a range of neural noise models, optimal gain modulation for combating noise can give rise to firing rate homeostasis and uniformisation across neural clusters defined based on similarity of stimulus tuning. To shed light on these results, we analytically investigated how and in what parameter regimes these results arise.</p>
<p>We found that homeostasis and uniformisation emerge in the regime where the matrix
<disp-formula id="eqn15">
<graphic xlink:href="564616v3_eqn15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
is small, according to some notion of matrix norm; here <italic>Î²</italic> = 2 â 2<italic>Î±</italic>. The intuitive meaning of this matrix is that its inverse, Î<sup>â1</sup>, characterises the structure of signal-to-noise ratio (SNR) in the space of cluster responses (before gain modulation). Therefore the above condition can be roughly interpreted as the requirement that signal-to-noise ratio is strong. We will discuss this condition, and the constraints it imposes on various model parameters, further in <xref ref-type="sec" rid="s2e">Sec. 2.5</xref>, and will provide evidence that it is likely to hold in cortex. Here, we focus on its consequences.</p>
<p>We developed a perturbative expansion in Î, to obtain approximate analytical solutions for the cluster gains that maximise our objective â, <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref> (see App. B.4). In the first approximation, <italic>i</italic>.<italic>e</italic>., to zeroth order in Î, this expansion yields
<disp-formula id="eqn16">
<graphic xlink:href="564616v3_eqn16.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We denote this approximate solution by <inline-formula><inline-graphic xlink:href="564616v3_inline15.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. To see the significance of this result, note that the average response of cluster <italic>a, i</italic>.<italic>e</italic>., its stimulus-averaged total spike count is given by ð¼ [<italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>)] = <italic>g</italic><sub><italic>a</italic></sub><italic>Ï</italic><sub><italic>a</italic></sub>. Thus, in the leading approximation, the stimulus-average spike count of cluster <italic>a</italic> is given by the constant <inline-formula><inline-graphic xlink:href="564616v3_inline16.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Thus, since <italic>Î²Âµ</italic> is constant across both clusters and environments, the zeroth order solution yields homeostasis and uniformisation of cluster firing rates.</p>
<p>We also calculated the first order correction to this homeostatic solution. This is given by
<disp-formula id="eqn17">
<graphic xlink:href="564616v3_eqn17.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
which (assuming Î<sub><italic>aa</italic></sub> are small) yields approximate homeostasis, with small deviations depending on the SNR structure, as captured by Î<sub><italic>aa</italic></sub>, and how that structure shifts between environments.</p>
<p>Improvements to the zeroth-order homeostatic solution can also be obtained in a different manner: by finding the best configuration of cluster gains, <inline-formula><inline-graphic xlink:href="564616v3_inline17.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, that, by construction, are guaranteed to yield homeostasis and uniformisation. As we saw above, the latter arise if and only if <italic>g</italic><sub><italic>a</italic></sub><italic>Ï</italic><sub><italic>a</italic></sub> is constant (across clusters and environments). We denote this <italic>a priori</italic> unknown constant by <italic>Ï</italic>. In other words, we let
<disp-formula id="eqn18">
<graphic xlink:href="564616v3_eqn18.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
under which the average spike count of clusters in the coding interval is given by <italic>Ï</italic>. The variable <italic>Ï</italic> is then chosen to optimise the expectation of â, <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>, across a relevant family of environments (note that we need to optimise the average objective across environments, since otherwise the optimal <italic>Ï</italic> will in general depend on the environment, invalidating homeostasis). With such an optimal <italic>Ï</italic>, <bold><italic>g</italic></bold><sup>hom</sup> will, by definition, perform better than <bold><italic>g</italic></bold><sup>(0)</sup> in terms of our objective function. Further, it can be shown (see App. B.7) that <italic>Ï</italic> &lt; <italic>Î²Âµ, i</italic>.<italic>e</italic>., the mean cluster firing rate under the optimal homeostatic solution is strictly smaller than that predicted by <bold><italic>g</italic></bold><sup>(0)</sup>, and that the average spike-count of the entire population of clusters is approximately the same under this solution as it is under <italic>g</italic><sup>(1)</sup>. In <xref ref-type="sec" rid="s2f">Sec. 2.6</xref>, we numerically compare the performance of <bold><italic>g</italic></bold><sup>(0)</sup>, <bold><italic>g</italic></bold><sup>(1)</sup>, and <bold><italic>g</italic></bold><sup>hom</sup>, and find that in many cases, the homeostatic <bold><italic>g</italic></bold><sup>hom</sup> performs close to the (numerically computed) true optimal gains.</p>
<p>Finally, we note the following exact analytical result regarding homeostasis that holds even for non-small Î. It follows form the structure of the objective function, <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>, that, irrespective of how pre-modulation mean rates, <italic>Ï</italic><sub><italic>a</italic></sub>, change between two stimulus environments, as long as Î and <italic>W</italic>, the effective noise covariance, remain invariant across the environments, optimal gains imply exact homeostasis of each clusterâs average firing rate across the two environments. However, unless Î is small, in general the optimal solution does not result in uniformization of firing rates across clusters.</p>
</sec>
<sec id="s2e">
<label>2.5</label>
<title>Conditions for the validity of the homeostatic solution hold in cortex</title>
<p>In this section, we further examine the conditions under which the homeostatic solution, <xref ref-type="disp-formula" rid="eqn16">Eq. (16)</xref>, provides a good approximation to the optimal gains, and, based on empirical estimates of relevant quantities, discuss whether these conditions obtain in cortical populations. As noted above, the general mathematical condition is that the matrix Î, defined in <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref>, is in some sense small (recall that through its inverse, Î represents the structure of signal-to-noise ratio âbefore gain modulationâ in the coding space). We can make this statement more concrete as follows. The homeostatic solution, and the first order correction to it, <xref ref-type="disp-formula" rid="eqn17">Eq. (17)</xref>, arise from a perturbative asymptotic expansion in the small Î. When the leading order correction to the (zeroth-order) homeostatic solution becomes non-negligible, it indicates the breakdown of this expansion. From <xref ref-type="disp-formula" rid="eqn17">Eq. (17)</xref> we can thus reformulate the condition for the validity of the homeostatic approximation as
<disp-formula id="eqn19">
<graphic xlink:href="564616v3_eqn19.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
for all <italic>a</italic>. Before proceeding to empirical estimates, let us first delineate the dependence of Î (or its diagonal elements) on different model parameters. Firstly, note that Î scales linearly with <italic>Ï</italic><sup>2</sup>; thus, if noise is scaled up, so is Î. Next, consider increasing the noise scaling exponent <italic>Î±</italic>. This decreases (<italic>Î²Âµ</italic>)<sup><italic>Î²</italic></sup> provided <italic>Î±</italic> â¤1 â <italic>e/</italic>2<italic>Âµ</italic> (which holds in our simulations above). Moreover, increasing <italic>Î±</italic> increases the diagonal elements of <italic>W</italic> (see <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>). Intuitively, this increases the magnitude of <italic>W</italic>. Through these two mechanisms, increasing <italic>Î±</italic> will generally increase the size of Î. Next, note that Î has an inverse relationship with the coefficients of variation, CV, which increase as neurons become more stimulus selective; accordingly, the more selective the tuning curves, the smaller the Î. Lastly, Î has a complex dependency on the spectra of both CV<italic>Ï</italic>CV and <italic>W</italic> as well as the alignment of their eigenbases (which represent the geometric structure of signal and noise in the neural coding space, respectively). In <xref ref-type="sec" rid="s2f">Sec. 2.6</xref>, we will numerically investigate these dependencies, and their effects on the validity of the homeostatic approximation.</p>
<p>Translated to biological quantities, it follows from the above dependencies that the smallness of Î (or Î<sub><italic>aa</italic></sub>) requires that (1) the cluster firing rates are sufficiently high, (2) noise variance scales sufficiently slowly with mean response, (3) individual clusters are sufficiently selective in responding to stimuli, and (4) the neural representation has a high-dimensional geometry in the population response space. We now review the available biological data on these variables in cortical populations.</p>
<sec id="s2e1">
<title>High firing rate</title>
<p>We first estimate <italic>Î²Âµ</italic> which enters the prefactor in <xref ref-type="disp-formula" rid="eqn19">Eq. (19)</xref>. We see from the zeroth order solution <xref ref-type="disp-formula" rid="eqn16">Eq. (16)</xref> that <italic>Î²Âµ</italic> is approximately the average total spike count of a cluster over the rate-coding time interval. Condition <xref ref-type="disp-formula" rid="eqn19">Eq. (19)</xref> thus requires that the stimulus-averaged firing rate of all clusters are sufficiently high. A wide range of mean firing rates for individual neurons have been reported in cortex. Here we focus on firing rates in rodent V1 during free behaviour (which tends to be lower compared to rates in cat or monkey cortex). Reported values tend to range from 0.4 Hz (<xref ref-type="bibr" rid="c20">Greenberg et al., 2008</xref>) to 14 Hz (<xref ref-type="bibr" rid="c39">Parker et al., 2022</xref>), depending on the cortical layer or area or the brain state, with other values lying in a tighter range of 4-7 Hz (<xref ref-type="bibr" rid="c21">Hengen et al., 2013</xref>; <xref ref-type="bibr" rid="c50">Szuts et al., 2011</xref>; <xref ref-type="bibr" rid="c51">Torrado Pacheco et al., 2019</xref>). Therefore, for rodent V1, we take a mean firing rate of 5 Hz to be a reasonable rough estimate. Assuming a coding interval of 0.1 seconds, and a cluster size of <italic>k</italic> = 20 (as a guess for the number of similarly coding V1 neurons), we obtain <italic>Î²Âµ</italic> â¼ 10. (Note that for lower or higher single neuron firing rates, a similar value of <italic>Âµ</italic> could be achieved by correspondingly scaling cluster sizes up or down.)</p>
</sec>
<sec id="s2e2">
<title>Sufficiently slow scaling of noise variance</title>
<p>We can obtain an estimate for the prefactor <italic>Ï</italic><sup>2</sup><italic>/</italic>(<italic>Î²Âµ</italic>)<sup><italic>Î²</italic></sup>, by fitting the mean-variance relationship for cluster spike counts: variance = <italic>Ï</italic><sup>2</sup>mean<sup>2â<italic>Î²</italic></sup>. A number of papers have fit such a relationship (<xref ref-type="bibr" rid="c43">Shadlen and Newsome, 1998</xref>; <xref ref-type="bibr" rid="c16">Gershon et al., 1998</xref>; <xref ref-type="bibr" rid="c34">Moshitch and Nelken, 2014</xref>; <xref ref-type="bibr" rid="c26">Koyama, 2015</xref>). We adjusted the values reported by those papers for our chosen coding interval length and cluster size (see <xref ref-type="sec" rid="s4c">Sec. 4.3</xref>, the plot in <xref rid="fig13" ref-type="fig">Fig. 13</xref>, and the values of <italic>Î²</italic> and <italic>Ï</italic><sup>2</sup> thereby obtained). These (together with the estimate <italic>Î²Âµ</italic> â 10) yield values for <italic>Ï</italic><sup>2</sup><italic>/</italic>(<italic>Î²Âµ</italic>)<sup><italic>Î²</italic></sup> ranging from 0.05 (<xref ref-type="bibr" rid="c43">Shadlen and Newsome, 1998</xref>) to 0.31 (<xref ref-type="bibr" rid="c26">Koyama, 2015</xref>), with median value 0.14. Note that this is close to the value for Poissonian noise (<italic>Ï</italic><sup>2</sup> = 1, <italic>Î²</italic> = 1), which is 0.1.</p>
</sec>
<sec id="s2e3">
<title>Sufficiently selective responses</title>
<p>The coefficient of variation CV can be seen as a measure of neural selectivity or sparseness of neural responses. To see this, consider a toy model in which the cluster responds at a fixed level to a fraction <italic>p</italic> of stimuli and is silent otherwise. In this case, CV<sup>2</sup> = (1 â<italic>p</italic>)<italic>/p</italic> â 1<italic>/p</italic> for small <italic>p</italic>. Our condition therefore requires that neurons are sufficiently selective in their responses and respond only to a small fraction of stimuli. <xref ref-type="bibr" rid="c28">Lennie (2003)</xref> places the fraction of simultaneously active neurons (which we use as a proxy for the response probability of a single cluster) at under 5%. For our toy model, this yields the estimate CV<sup>2</sup> â 20. The Bernoulli distribution in the toy model is particularly sparse, and so we take CV<sup>2</sup> â 10 as a more conservative estimate.</p>
</sec>
<sec id="s2e4">
<title>High-dimensional signal geometry</title>
<p>Assuming, for simplicity, that the coefficients of variation are approximately the same for all clusters, we see that the expression for Î<sub><italic>aa</italic></sub> are proportional to the diagonal elements of <italic>Ï</italic><sup>â1</sup><italic>W</italic>. To estimate the latter, we will assume that these elements are all comparable in size (This is expected to be valid when the eigenbasis of <italic>Ï</italic><sup><italic>â</italic>1</sup><italic>W</italic> is not aligned with the standard (cluster) basis, which, in turn, corresponds to distributed coding), and will therefore estimate their average value, which is given by the normalised trace <inline-formula><inline-graphic xlink:href="564616v3_inline18.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. By the invariance of trace, this can equally be characterised as the mean eigenvalue of <italic>Ï</italic><sup>â1</sup><italic>W</italic>. We will estimate this trace for two sub-cases of interest.</p>
<p>Firstly, consider the case of zero noise correlations, <italic>i</italic>.<italic>e</italic>., <italic>W</italic> = <italic>I</italic>. In this case, the trace is the mean of [<italic>Ï</italic><sup>â1</sup>]<sub><italic>aa</italic></sub>. Recall that <italic>Ï</italic> is the signal correlation matrix of neuron clusters. For a correlation matrix, it can be shown that [<italic>Ï</italic><sup>â1</sup>]<sub><italic>aa</italic></sub> â¥ 1, with equality if and only if cluster <italic>a</italic> has zero signal correlation with every other cluster. [<italic>Ï</italic><sup>â1</sup>]<sub><italic>aa</italic></sub> can thus be seen as a measure of the extent to which cluster <italic>a</italic> shares its representation with other clusters, <italic>i</italic>.<italic>e</italic>., as a measure of representational redundancy. Many traditional efficient coding accounts predict zero signal correlation between neurons, at least in the low noise limit (<xref ref-type="bibr" rid="c3">Barlow, 2012</xref>; <xref ref-type="bibr" rid="c1">Atick and Redlich, 1990</xref>; <xref ref-type="bibr" rid="c37">Nadal and Parga, 1994</xref>, <xref ref-type="bibr" rid="c38">1999</xref>), providing an additional normative justification for low signal correlations. However, complete lack of signal correlations is not necessary for our condition to hold; we merely require a sufficiently slow decay of the eigenvalue spectrum of the signal correlation matrix. This condition is geometrically equivalent to neural responses forming a representation with high intrinsic dimensionality. Stringer <italic>et al</italic>. (<xref ref-type="bibr" rid="c49">Stringer et al., 2019</xref>) found that, the signal covariance matrix of mouse V1 neurons responding to natural stimuli possesses an approximately 1<italic>/n</italic> spectrum. In this case, for <italic>K</italic> large, we obtain the estimate [<italic>Ï</italic><sup>â1</sup>]<sub><italic>aa</italic></sub> â tr(<italic>Ï</italic><sup>â1</sup>)<italic>/K</italic> â ln(<italic>K</italic>)<italic>/</italic>2 (see App. B.6). Given the slow logarithmic increase with <italic>K</italic>, we expect that our condition can hold even for very large neural populations. For example, suppose we take the entire human V1 as our neural population, to obtain an arguably generous upper bound for <italic>K</italic>. This contains roughly 1.5 Ã 10<sup>8</sup> neurons (<xref ref-type="bibr" rid="c57">Wandell, 1995</xref>), leading to 7.5 Ã 10<sup>6</sup> clusters of 20 neurons, and therefore an average value of [<italic>Ï</italic><sup>â1</sup>]<sub><italic>aa</italic></sub> just under 8. This, together with our other estimates above, yields the following overall estimate for Î<sub><italic>aa</italic></sub>
<disp-formula id="eqn20">
<graphic xlink:href="564616v3_eqn20.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We now consider another sub-case of interest, that of so-called <italic>information-limiting noise</italic> where directions of strong signal variations align with those of strong noise variation (<xref ref-type="bibr" rid="c33">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="c22">Kanitscheider et al., 2015</xref>). Specifically, we consider the extreme case <italic>W</italic> = <italic>Ï, i</italic>.<italic>e</italic>., when noise and signal correlations are perfectly aligned. This case was considered numerically in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref>, <xref rid="fig7" ref-type="fig">Fig. 7</xref>, and we saw that in this case the optimal solution displayed perfect homeostasis and uniformisation of rates across clusters. Moreover, in this case, <italic>Ï</italic><sup>â1</sup><italic>W</italic> = <italic>I</italic>, and so the normalised trace tr(<italic>Ï</italic><sup>â1</sup><italic>W</italic>) does not scale with <italic>K</italic> at all. Note that this is an extreme of signal-noise alignment. In <xref ref-type="sec" rid="s2b">Sec. 2.2</xref> we also considered less extreme forms of alignment, in which <italic>W</italic> shares the eigen-bases with <italic>Ï</italic> but has a 1<italic>/n</italic><sup><italic>Î³</italic></sup> spectrum (as opposed to the 1<italic>/n</italic> spectrum for <italic>Ï</italic>). We show in App. B.6 that, for 0 &lt; <italic>Î³</italic> &lt; 1 (relatively high-dimensional and weakly correlated noise), we have <inline-formula><inline-graphic xlink:href="564616v3_inline19.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. On the other hand, for <italic>Î³</italic> &gt; 1 (relatively low-dimensional and strongly correlated noise), tr(<italic>Ï</italic><sup>â1</sup><italic>W</italic>) in fact decays like <italic>K</italic><sup>â min(<italic>Î³</italic>â1,1)</sup> as <italic>K</italic> grows. Thus, in this case, the estimate of Î<sub><italic>aa</italic></sub> is (potentially significantly) smaller than that of <xref ref-type="disp-formula" rid="eqn20">Eq. (20)</xref>. Accordingly, even partial signal-noise alignment (especially for correlated, relatively low-dimensional noise) can strongly reduce the size of Î, encouraging homeostatic coding.</p>
<p>The above analysis makes it clear when we should expect homeostasis in general â when cluster firing rates are <italic>high</italic>, the noise scaling of cluster responses is sufficiently <italic>slow</italic>, responses are highly <italic>selective</italic>, and signal correlation structure corresponds to a <italic>high-dimensional geometry</italic> (<xref ref-type="bibr" rid="c49">Stringer et al., 2019</xref>), possibly with <italic>information-limiting noise correlations</italic>. On the other hand, when these conditions are violated, the optimal gain configuration can potentially deviate strongly from the homeostatic solution. Moreover, given the above estimates, we thus see that in V1 and possibly other cortical areas, we can expect the leading corrections to the homeostatic solution (see <xref ref-type="disp-formula" rid="eqn17">Eq. (17)</xref> and <xref ref-type="disp-formula" rid="eqn20">Eq. (20)</xref>) to not exceed 10%, in relative terms. Indeed, as we will show in the <xref ref-type="sec" rid="s2f">Sec. 2.6</xref> using numerical validations, for values of the above parameters in their estimated biological range, the homeostatic solution provides a very good approximation to the optimal solution of <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>.</p>
</sec>
</sec>
<sec id="s2f">
<label>2.6</label>
<title>Numerical comparison of homeostatic gains to optimal gains</title>
<p>We now return to the numerical simulations performed in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref>, to compare the performance and accuracy of the different approximate solutions to optimally adapted gains, discussed in <xref ref-type="sec" rid="s2d">Sec. 2.4</xref>, with those of the optimal solution of <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>. For each of the different noise models considered in that section we compare the performance of the homeostatic gains <inline-formula><inline-graphic xlink:href="564616v3_inline20.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="564616v3_inline20a.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (where <italic>Ï</italic> is chosen as detailed in App. B.7), as well as the first-order correction solution <bold><italic>g</italic></bold><sup>(1)</sup> (see <xref ref-type="disp-formula" rid="eqn16">Eq. (16)</xref>â<xref ref-type="disp-formula" rid="eqn18">(18)</xref>) to the numerically optimised gains <bold><italic>g</italic></bold><sup>opt</sup>. To examine the adaptation properties of these gains, we once again work with the sequence of environments indexed by <italic>Î½</italic> â [0, 1], with the same specifications as in the numerical simulations of <xref ref-type="sec" rid="s2b">Sec. 2.2</xref>.</p>
<p>To measure the accuracy of the approximations to the optimal gains, for each environment <italic>Î½</italic>, we calculated the mean relative errors
<disp-formula id="eqn21">
<graphic xlink:href="564616v3_eqn21.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
for each of the approximate solutions, denoted by <bold><italic>g</italic></bold><sup>app</sup>(<italic>Î½</italic>), with <bold><italic>g</italic></bold><sup>opt</sup>(<italic>Î½</italic>), the numerically optimised solution in environment <italic>Î½</italic>. To measure the performance of an approximate solution, we use the following <italic>relative improvement</italic> measure, defined by
<disp-formula id="eqn22">
<graphic xlink:href="564616v3_eqn22.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
which can be interpreted as the improvement in the objective â (Â·; <italic>Î½</italic>) (the efficient coding objective <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref> according to the statistics of environment <italic>Î½</italic>) achieved by the adaptive gains <bold><italic>g</italic></bold><sup>app</sup>(<italic>Î½</italic>) over the unadapted gains from the original environment <bold><italic>g</italic></bold><sup>app</sup>(0), relative to the improvement obtained by the optimally adaptive gains <bold><italic>g</italic></bold><sup>opt</sup>(<italic>Î½</italic>). The better the approximation the closer <italic>C</italic><sup>app</sup>(<italic>Î½</italic>) will be to 1. Note that, in general, the relative improvement worsens when <italic>Î½</italic> goes to 0 (see <xref rid="fig9" ref-type="fig">Fig. 9B, D</xref> and <xref ref-type="fig" rid="fig9">F</xref>); this follows from the definition of <italic>C</italic>(<italic>Î½</italic>) and the fact that when environmental change is weak, using gains optimized in the original environment without adapting them in the new environment is superior to adaptive but only approximately optimal gains. We now summarise the results for the different noise models discussed in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref> (see that section for the detailed specification of the three noise models).</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9:</label>
<caption><title>Accuracy of homeostatic approximation to optimal gains.</title>
<p>(A) The relative error, <xref ref-type="disp-formula" rid="eqn21">Eq. (21)</xref>, of different approximate solutions for the optimal gains, averaged across environments (the standard deviations of these values across environments were negligible relative to the average), as a function of the noise scaling parameter <italic>Î±</italic>. To give a sense of the scale of variation of the optimal gains across environments, the black line shows the relative change in the optimal gains between the most extreme environments (a measure of effect size),<inline-formula><inline-graphic xlink:href="564616v3_inline109.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. (B) Relative improvement in the objective for the homeostatic approximation <bold><italic>g</italic></bold><sup>hom</sup> as measured by <xref ref-type="disp-formula" rid="eqn22">Eq. (22)</xref>. Panels C and E (panels F and D) are the same as panel A (panel B), but for the uniformly correlated noise and the aligned noise models, respectively.</p></caption>
<graphic xlink:href="564616v3_fig9.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<sec id="s2f1">
<title>Uncorrelated power-law noise</title>
<p>First, we consider the uncorrelated noise model with general power-law scaling of noise strength with mean responses. In this case, the homeostatic solution <bold><italic>g</italic></bold><sup>hom</sup>, and the first order correction, <bold><italic>g</italic></bold><sup>(1)</sup>, had very low (&lt; 3%) relative errors for all noise scaling exponents, <italic>Î±</italic>, while the relative error in <bold><italic>g</italic></bold><sup>(0)</sup> becomes large for significantly super-Poissonian noise scaling, <italic>i</italic>.<italic>e</italic>., <italic>Î±</italic> = 0.75 (<xref rid="fig9" ref-type="fig">Fig. 9A</xref>). (For the case of uncorrelated power-law noise model, as well as the aligned correlated noise model, we used an analytical approximation to the optimal <italic>Ï</italic> (see App. B.7) used in the homeostatic approximation <inline-formula><inline-graphic xlink:href="564616v3_inline110.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, <xref ref-type="disp-formula" rid="eqn18">Eq. (18)</xref>. This means that the performance measures in the plots of <xref rid="fig9" ref-type="fig">Fig. 9B</xref> and <xref ref-type="fig" rid="fig9">F</xref> are lower-bounds for the performance of <inline-formula><inline-graphic xlink:href="564616v3_inline111.gif" mime-subtype="gif" mimetype="image"/></inline-formula> with the truly optimal <italic>Ï</italic>.) The relative improvement measure <italic>C</italic><sup>hom</sup>(<italic>Î½</italic>) for the homeostatic solution is also close to one, the optimal value, for all <italic>Î±</italic>, except for 0.75 (<xref rid="fig9" ref-type="fig">Fig. 9B</xref>). For the other approximate solutions too (Supp. Fig. 17A,B), the relative improvement decreases with increasing noise scaling. This is consistent with our analysis, since increasing <italic>Î±</italic> increases the size of Î (see <xref ref-type="sec" rid="s2d">Sec. 2.4</xref>) and hence the influence of terms of higher order in Î.</p>
</sec>
<sec id="s2f2">
<title>Uniform noise correlation coefficients</title>
<p>We now examine the effect of noise correlations on the accuracy of the approximate homeostatic solutions, first within the noise family with uniform noise correlation coefficients, denoted by <italic>p</italic>, and Poisson-like scaling (<italic>Î±</italic> = 1<italic>/</italic>2). For all <italic>p</italic>, the relative deviation of the homeostatic solution <bold><italic>g</italic></bold><sup>hom</sup> from optimal gains was below 1% (<xref rid="fig9" ref-type="fig">Fig. 9C</xref>), while <bold><italic>g</italic></bold><sup>(0)</sup> and <bold><italic>g</italic></bold><sup>(1)</sup> were less accurate overall. Note that the accuracy of the homeostatic solutions <bold><italic>g</italic></bold><sup>hom</sup> and <bold><italic>g</italic></bold><sup>(0)</sup> increases with <italic>p</italic>, suggesting that positive noise correlations improve the optimality of homeostatic gains. The same trend can be seen in the relative improvement on the objective, <italic>C</italic><sup>hom</sup>, for homeostatic gain adaptation, which increases with <italic>p</italic> (<xref rid="fig9" ref-type="fig">Fig. 9D</xref>).</p>
</sec>
<sec id="s2f3">
<title>Aligned noise</title>
<p>Lastly, we considered the algined noise model, in which noise has Poisson-like scaling (<italic>i</italic>.<italic>e</italic>., <italic>Î±</italic> = 1<italic>/</italic>2), and is correlated across clusters with the noise and signal covariance matrices sharing the same eigenbasis. In this noise family, the parameter, <italic>Î³</italic>, controls the scaling of the eigenvalues of the noise covariance matrix with their rank, <italic>n</italic>, as 1<italic>/n</italic><sup><italic>Î³</italic></sup>. As in the previous cases, for all <italic>Î³</italic>, the homeostatic solution <bold><italic>g</italic></bold><sup>hom</sup> was very close to the optimal gains <bold><italic>g</italic></bold><sup>opt</sup> (<xref rid="fig9" ref-type="fig">Fig. 9E</xref>), and preformed nearly equally (<xref rid="fig9" ref-type="fig">Fig. 9F</xref>). The case <italic>Î³</italic> = 1, which corresponds to perfect alignment between signal and noise correlations, <italic>W</italic> = <italic>Ï</italic>, is particularly interesting. As noted in the previous section, this case corresponds to that of so-called information-limiting noise correlations (<xref ref-type="bibr" rid="c33">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="c22">Kanitscheider et al., 2015</xref>). As we show in App. B.5, in this case the optimal gains, without any approximation, lead to homeostasis and uniformisation of cluster firing rates. Thus, in this case <bold><italic>g</italic></bold><sup>opt</sup> = <bold><italic>g</italic></bold><sup>hom</sup> exactly, leading to zero relative error (<xref rid="fig9" ref-type="fig">Fig. 9E</xref>) for <bold><italic>g</italic></bold><sup>hom</sup>, and perfect relative improvement <italic>C</italic><sup>hom</sup> (<xref rid="fig9" ref-type="fig">Fig. 9F</xref>). For other cases, we can see that for increasing <italic>Î³</italic> (at least for the values we simulated), which corresponds to more correlated and lower-dimensional noise (as noise power falls off faster as a function of eigenvalue rank), the homeostatic gains <bold><italic>g</italic></bold><sup>hom</sup> tend to perform better.</p>
<p>Our numerical simulations across the three noise subfamilies (with varying noise power scaling, noise correlations, and noise spectrum) demonstrate that homeostatic strategies can - robustly and across a wide range of noise distributions - combat coding noise by effectively navigating the trade-off between energetic costs and coding fidelity. Finally, note that adaptation in <bold><italic>g</italic></bold><sup>hom</sup> uses only information local to each neuron, <italic>i</italic>.<italic>e</italic>., its firing rate. This implies that good performance does not require the use of complex regulation mechanisms which take into account how the encoding of each cluster relates to the population at large; homeostatic regulation, implemented by purely local mechanisms, is sufficient.</p>
</sec>
</sec>
<sec id="s2g">
<label>2.7</label>
<title>Synaptic normalisation allows for propagation of homeostasis between populations</title>
<p>So far, we have considered homeostatic coding from a purely normative perspective, showing with both theoretical arguments and numerical simulations that homeostasis efficiently trades-off between coding fidelity and energetic costs. In this section, we begin to address the question of biological plausibility; specifically, what biological mechanisms are necessary for the homeostatic coding regime to be propagated between neural populations. Specifically, We will show that for the case of a linear feedforward network implementing the representation curves, there is a connection between homeostatic coding and synaptic scaling (<xref ref-type="bibr" rid="c52">Turrigiano, 2008</xref>).</p>
<p>Consider two neural populations, with an upstream population providing feedforward input to a downstream population. We will suppose that the upstream population is engaged in homeostatic coding, and ask what is necessary for the downstream population to be also engaged in homeostatic coding. We will denote the tuning and representational curves of the upsteam population by <inline-formula><inline-graphic xlink:href="564616v3_inline21.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="564616v3_inline22.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, respectively, where <italic>i</italic> = 1 (<italic>i</italic> = 2) for the upstream (downstream) population. We drop corrections to the zeroth order solution for optimal gains, and work within the homeostatic coding regime; that is we assume the gains of the upstream population are given by <inline-formula><inline-graphic xlink:href="564616v3_inline23.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p>The downstream population of tuning curves will be given by <inline-formula><inline-graphic xlink:href="564616v3_inline24.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for <italic>m</italic> = 1, â¦, <italic>M</italic>. Let <italic>W</italic><sub><italic>ma</italic></sub> be the synaptic weight from neuron <italic>a</italic> in the upstream population to neuron <italic>m</italic> in the downstream population. Working in a linearized rate model, this gives us that the tuning curve of neuron <italic>m</italic> is
<disp-formula id="eqn23">
<graphic xlink:href="564616v3_eqn23.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Suppose that, as dictated by the computational goals of the circuit, the downstream population has representational curves <inline-formula><inline-graphic xlink:href="564616v3_inline25.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for <italic>m</italic> = 1, â¦, <italic>M</italic>. We similarly write each representational curve as a linear combination of the upstream cluster representational curves,
<disp-formula id="eqn24">
<graphic xlink:href="564616v3_eqn24.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We call these weights <italic>w</italic><sub><italic>ma</italic></sub> the <italic>computational weights</italic>, since they are determined by the computational goals of the downstream population. In line with our approach so far, we treat these computational goals, and hence also the computational weights, as given (<italic>i</italic>.<italic>e</italic>., set independently of optimal gain adaptation).</p>
<p>We asked how the synaptic weights <italic>W</italic><sub><italic>ma</italic></sub> should depend on the computational weights <italic>w</italic><sub><italic>ma</italic></sub>, and how they should adapt as stimulus statistics change in order for the downstream population to also engage in homeostatic coding. In App. B.8 we find that homeostatic coding in the downstream population is guaranteed if the synaptic weights are related to the computational weights via
<disp-formula id="eqn25">
<graphic xlink:href="564616v3_eqn25.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="564616v3_inline26.gif" mime-subtype="gif" mimetype="image"/></inline-formula> as before. Note that the total synaptic weight received by a downstream neuron is thus constant. In other words, this scheme requires the normalization of the total synaptic weights received by each downstream neuron to be constant, independently of adjustments of the gains of the pre-synaptic neurons or of possible changes in the computational weights (different normalisation factors are equivalent to different values of <italic>Ï</italic> for different populations; differences in the optimal choice of <italic>Ï</italic> may arise from <italic>e</italic>.<italic>g</italic>., different noise correlation statistics or different rate coding intervals between populations). Thus, homeostatic coding, applied sequentially to two populations, provides an additional normative interpretation of synaptic normalisation, in which synapses onto a neuron are jointly scaled to keep total input weights constant. This may be the computational reason for the synaptic scaling observed in certain studies of firing rate homeostasis (<xref ref-type="bibr" rid="c53">Turrigiano et al., 1998</xref>; <xref ref-type="bibr" rid="c52">Turrigiano, 2008</xref>) that occurs over relatively long timescales.</p>
</sec>
<sec id="s2h">
<label>2.8</label>
<title>Homeostatic noisy DDCs</title>
<p>Up to this point we have made no assumptions about the nature of cortical representations, beyond rate coding (and the condition described by <xref ref-type="disp-formula" rid="eqn19">Eq. (19)</xref>), and thus our framework has been independent of the computational goals of the circuit. We now apply our framework to a specific theory of neural representation and computation, namely the distributive distributional code (DDC) (<xref ref-type="bibr" rid="c56">Vertes and Sahani, 2018</xref>), introducing what we term a <italic>homeostatic DDC</italic>. We will show below, in <xref ref-type="sec" rid="s2j">Sec. 2.10</xref>, that specific forms of homeostatic DDC are able to account for stimulus-specific adaptation effects, observed in V1 (<xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>), which have been difficult to account for under other efficient coding frameworks. While there is ample behavioural and psychophysical evidence for the hypothesis that animal perception and decision making approximate optimal Bayesian probabilistic inference (<xref ref-type="bibr" rid="c24">Kersten et al., 2004</xref>; <xref ref-type="bibr" rid="c48">Stocker and Simoncelli, 2006</xref>; <xref ref-type="bibr" rid="c55">van Bergen et al., 2015</xref>; <xref ref-type="bibr" rid="c17">Geurts et al., 2022</xref>), our knowledge of the neural implementation of computations serving Bayesian inference (such as computations of posterior distributions and posterior expectations) are rudimentary at best. The DDC framework is one proposal for neural implementations of Bayesian computations underlying perception. In a DDC model, neural responses directly encode posterior expectations of the inferred latent causes of sensory inputs according to an internal generative model. This internal model mathematically relates the latent causes or latent variables, which we denote by <bold><italic>z</italic></bold>, to the sensory inputs or stimuli, <bold><italic>s</italic></bold>, via a family of conditional distributions, <italic>f</italic> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold>), as well as a prior distribution, <italic>Ï</italic>(<bold><italic>z</italic></bold>), over the latent variables. The sensory system is assumed to implement a so-called <italic>recognition model</italic> corresponding to this generative model. Namely, given a stimulus <bold><italic>s</italic></bold>, the task of the sensory system is to invert the generative process by calculating and representing the posterior distribution of latent variables given the current sensory input (see the schema in <xref rid="fig10" ref-type="fig">Fig. 10</xref>).</p>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10:</label>
<caption><title>Schema of the generative and recognition models underlying a DDC.</title>
<p>According to the internal generative model, sensory inputs, <bold><italic>s</italic></bold>, are caused by latent causes, <bold><italic>z</italic></bold>, that occur in the environment according to a prior distribution <italic>Ï</italic>(<bold><italic>z</italic></bold>). A conditional distribution <italic>f</italic> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold>), the so-called likelihood function, describes how the latent causes generate or give rise to the sensory inputs <bold><italic>s</italic></bold>. The task of the brain is to invert this generative process by inferring the latent causes based on the current sensory input, which is done by computing the posterior distribution Î (<bold><italic>z</italic></bold>|<bold><italic>s</italic></bold>).</p></caption>
<graphic xlink:href="564616v3_fig10.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In a DDC, neural responses are specifically postulated to directly represent the posterior expectations of the latent variables, <bold><italic>z</italic></bold>, or rather, the posterior expectations of a sufficiently rich (and fixed) class of so-called kernel functions of <bold><italic>z</italic></bold>. Thus, each neuron, say neuron <italic>a</italic>, is assigned to a so-called kernel function of the latent variables <italic>Ï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>), and its response represents the posterior expectation ð¼ [<italic>Ï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>) |<bold><italic>s</italic></bold>]. Here, we add two assumptions to this standard DDC scheme. First, we assume that the actual single-trial response, <italic>n</italic><sub><italic>a</italic></sub>, of the neuron is a noisy version of ð¼ [<italic>Ï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>) |<bold><italic>s</italic></bold>], but such that the trial-averaged tuning curve of the neuron is ð¼ [<italic>Ï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>)|<bold><italic>s</italic></bold>], up to a constant of proportionality. Second, we assume that these constants of proportionality are adaptive and are given by the optimal gains of our efficient coding framework, which we assume are the homeostatic gains given by <inline-formula><inline-graphic xlink:href="564616v3_inline27.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. In the language of previous sections, this is equivalent to saying that we assume the representational curve of a neuron with kernel function, <italic>Ï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>), is given by Î©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>) = ð¼ [<italic>Ï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>) |<bold><italic>s</italic></bold>]. Under optimal homeostatic gains, our noisy homeostatic DDC predicts the following relationship between the neural tuning curves and Bayesian posterior and prior expectations of <italic>Ï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>):
<disp-formula id="eqn26">
<graphic xlink:href="564616v3_eqn26.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Thus, this homeostatic DDC framework predicts that the neural tuning curves are the ratio of the posterior to the prior expectations of kernel functions. In deriving this result we have used the tower property to replace the trial-averaged posterior expectation ð¼ [ð¼ [<italic>Ï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>) |<bold><italic>s</italic></bold>]] with the prior expectation ð¼ [<italic>Ï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>)] under the internal model; strictly speaking, this is true under an ideal-observer internal model in which the generative internal model (as specified by <italic>f</italic> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold>) and <italic>Ï</italic>(<bold><italic>z</italic></bold>)) provides a bona fide, objective description of the statistics of sensory stimuli <bold><italic>s</italic></bold>. In <xref ref-type="sec" rid="s2j">Sec. 2.10</xref> below, we will relax this assumption and discuss the consequences of using a non-ideal internal model instead.</p>
</sec>
<sec id="s2i">
<label>2.9</label>
<title>Bayes-ratio coding</title>
<p>We now consider a special case of the homeostatic DDC in which the kernel functions are delta functions, <italic>Ï</italic><sub><italic>a</italic></sub>(<bold><italic>z</italic></bold>) = <italic>Î´</italic>(<bold><italic>z</italic></bold>â <bold><italic>z</italic></bold><sub><italic>a</italic></sub>). Here <bold><italic>z</italic></bold><sub><italic>a</italic></sub> is a point in latent variable space corresponding to neuron <italic>a</italic>âs preferred latent variable value. In this case, the homeostatic DDC becomes
<disp-formula id="eqn27">
<graphic xlink:href="564616v3_eqn27.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where in the last equality we used Bayesâ rule: Î (<bold><italic>z</italic></bold>|<bold><italic>s</italic></bold>) = <italic>f</italic> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold>)<italic>Ï</italic>(<bold><italic>z</italic></bold>)<italic>/P</italic> (<bold><italic>s</italic></bold>). Thus, in this case, the tuning curve of a neuron is the ratio of the posterior distribution to the prior distribution both evaluated at that neuronâs preferred latent variable value. Accordingly, we call this coding scheme <italic>Bayes-ratio coding</italic>.</p>
<p>Bayes-ratio coding can be achieved via divisive normalisation with adaptive weights. Divisive normalisation has been dubbed a canonical neural operation (<xref ref-type="bibr" rid="c9">Carandini and Heeger, 2012</xref>; <xref ref-type="bibr" rid="c60">Westrick et al., 2016</xref>), as it appears in multiple brain regions serving different computational goals. Given the feedforward inputs, <italic>F</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>), the divisive normalisation model computes the response (and thus the tuning curve) of neuron <italic>a</italic> as
<disp-formula id="eqn28">
<graphic xlink:href="564616v3_eqn28.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here, <italic>w</italic><sub><italic>b</italic></sub> are a collection of so-called normalisation weights, and <italic>Î±</italic> â¥ 0 and <italic>n</italic> â¥ 1 are constants. As we now show, Bayes-ratio coding can be naturally implemented by a divisive normalisation model in which <italic>n</italic> = 1. In the divisive normalisation implementation of Bayes-ratio coding, the feed-forward inputs, <italic>F</italic><sub><italic>b</italic></sub>(<bold><italic>s</italic></bold>), are given by the likelihood function, <italic>f</italic> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold><sub><italic>b</italic></sub>), of the DDCâs internal generative model, while the normalisation weights <italic>w</italic><sub><italic>b</italic></sub> encode the prior probabilities, <italic>w</italic><sub><italic>b</italic></sub> = <italic>Ï</italic>(<bold><italic>z</italic></bold><sub><italic>b</italic></sub>)<italic>Î´</italic><bold><italic>z</italic></bold><sub><italic>b</italic></sub> (the volume element, <italic>Î´</italic><bold><italic>z</italic></bold><sub><italic>b</italic></sub>, is chosen such that the latent variable space is the disjoint union of volumes of size <italic>Î´</italic><bold><italic>z</italic></bold><sub><italic>b</italic></sub> each containing their corresponding sample point <bold><italic>z</italic></bold><sub><italic>b</italic></sub>). We then obtain
<disp-formula id="ueqn2">
<graphic xlink:href="564616v3_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
which after substitution in <xref ref-type="disp-formula" rid="eqn28">Eq. (28)</xref> yields
<disp-formula id="eqn29">
<graphic xlink:href="564616v3_eqn29.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Taking the limit as <italic>Î±</italic> â 0, we obtain Bayes-ratio coding <xref ref-type="disp-formula" rid="eqn27">Eq. (27)</xref>. Therefore, provided <italic>Î±</italic> is small compared to <italic>P</italic> (<bold><italic>s</italic></bold>), divisive normalisation can be used to approximate Bayes-ratio coding. Not only does this result show that implementing Bayes-ratio coding is biologically plausible, it also provides a normative computational interpretation to both the feedforward inputs and the normalisation weights of the divisive normalization model, as the internal generative modelâs likelihoods and prior probabilities, respectively. Note also that the normalisation weights of this model are adaptive, and vary between environments with different latent variable marginal distributions, <italic>Ï</italic>(<bold><italic>z</italic></bold>).</p>
<p>Finally, we consider the propagation of Bayes-ratio coding between populations, by using the scheme discussed in <xref ref-type="sec" rid="s2g">Sec. 2.7</xref> for propagation of homeostatic codes. Suppose we now have a hierarchical generative model <bold><italic>z</italic></bold><sup>(2)</sup> â <bold><italic>z</italic></bold><sup>(1)</sup> â <bold><italic>s</italic></bold>, specified by a prior distribution <italic>Ï</italic><sup>(2)</sup>(<bold><italic>z</italic></bold><sup>(2)</sup>) and generative likelihood functions <italic>f</italic> <sup>(2)</sup>(<bold><italic>z</italic></bold><sup>(1)</sup> | <bold><italic>z</italic></bold><sup>(2)</sup>) and <italic>f</italic><sup>(1)</sup>(<bold><italic>s</italic></bold> | <bold><italic>z</italic></bold><sup>(1)</sup>). Then consider a recognition model corresponding to this generative model, comprised of a feedforward network with two populations or layers that implement a Bayes-ratio encoding of the posterior distributions of the hierarchical latent variables <bold><italic>z</italic></bold><sup>(1)</sup>, and <bold><italic>z</italic></bold><sup>(2)</sup>, respectively, conditional on the stimulus <bold><italic>s</italic></bold>. Let us assume that the first (upstream) layer is implementing Bayes-ratio coding for the lower-level variables <bold><italic>z</italic></bold><sup>(1)</sup> Then, as we show in App. B.9, a simple linear propagation of rates according to <xref ref-type="disp-formula" rid="eqn23">Eq. (23)</xref> with synaptic weights given by
<disp-formula id="eqn30">
<graphic xlink:href="564616v3_eqn30.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
would implement Bayes-ratio coding for <bold><italic>z</italic></bold><sup>(2)</sup> in the downstream layer. This result is significant for three reasons. Firstly, the synaptic weights make no reference to the prior distribution, <italic>Ï</italic><sup>(2)</sup>, over <bold><italic>z</italic></bold><sup>(2)</sup>. There is therefore no need to adapt the weights when the environment changes, if the change only affects the statistics of the higher-level causes <bold><italic>z</italic></bold><sup>(2)</sup> but not the generative process. Secondly, while the downstream population, is part of the <italic>recognition</italic> model and (effectively) represents the <italic>posterior</italic> distribution of <bold><italic>z</italic></bold><sup>(2)</sup>, the synaptic weights, <xref ref-type="disp-formula" rid="eqn30">Eq. (30)</xref>, needed to implement this representation only require knowledge of the <italic>generative</italic> probabilities (specifically those given by <italic>f</italic> <sup>(2)</sup>). Finally, we can see by induction that this scheme can be propagated through many layers, each representing posterior probabilities of higher-level latent variables further up a generative hierarchy.</p>
</sec>
<sec id="s2j">
<label>2.10</label>
<title>Homeostatic DDCs account for stimulus specific adaptation</title>
<p>We found that homeostatic DDCs can provide a normative account of stimulus specific adaptation (SSA). Here, we start by showing this in the special case of Bayes-ratio coding, which is mathematically simpler and provides good intuitions for a more general subset of homeostatic DDCs. We then discuss this more general case and build a homeostatic DDC model of empirically observed SSA in the primary visual cortex (V1) (<xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>).</p>
<p>According to <xref ref-type="disp-formula" rid="eqn27">Eq. (27)</xref>, for Bayes-ratio codes the tuning curve of neuron <italic>a</italic> is given by <italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>) = <italic>Ïf</italic> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold><sub><italic>a</italic></sub>)<italic>/P</italic> (<bold><italic>s</italic></bold>). Suppose now that the stimulus distribution in the environment, <italic>P</italic> (<bold><italic>s</italic></bold>), changes due to a change in the statistics of the latent causes, <italic>Ï</italic>(<bold><italic>z</italic></bold>), while the causal processes linking the latent causes and observed stimuli, as captured by <italic>f</italic> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold>), remain stable. In this case, by <xref ref-type="disp-formula" rid="eqn27">Eq. (27)</xref>, the tuning curve of <italic>all</italic> neurons are modified by a multiplicative factor given by the ratio of the <italic>P</italic> (<bold><italic>s</italic></bold>) in the old environment to that in the new environment. This will lead to a multiplicative suppression of the response of all neurons to stimuli that are over-represented in the new environment relative to the old one, in a way that is independent of the neuronsâ identity or preferred stimulus. Such a suppression thus constitutes a pure form of stimulus specific adaptation. In particular, neurons do not reduce their responsiveness to stimuli that are not over-represented in the new environment. Thus, tuning curves are suppressed only near over-represented stimuli (and may in fact be enhanced near under-represented stimuli), leading to a repulsion of tuning curve peak from over-represented stimuli. This repulsion is a typical manifestation of stimulus specific adaptation; in V1, for example, orientation tuning curves display a repulsion from an over-represented orientation (<xref ref-type="bibr" rid="c35">Movshon and Lennie, 1979</xref>; <xref ref-type="bibr" rid="c36">MÃ¼ller et al., 1999</xref>; <xref ref-type="bibr" rid="c14">Dragoi et al., 2000</xref>, <xref rid="c13" ref-type="bibr">2002</xref>; <xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>).</p>
<p>Heretofore we have implicitly made a so-called ideal observer assumption by assuming that the internal generative model underlying the DDC or Bayes-ratio code is a veridical model of the stimulus distribution in the environment. We now provide a significant generalization of the above results to the more general and realistic case of a non-ideal-observer internal model. As we will see, in this case, the effects of adaptation on tuning curves are captured by both a stimulus-specific factor as well as a neuron-specific factor. For a non-ideal observer model, there will be a discrepancy between the environmentâs true stimulus distribution, <italic>P</italic><sup><italic>E</italic></sup>(<bold><italic>s</italic></bold>), and the marginal stimulus distribution predicted by the internal generative model, <italic>i</italic>.<italic>e</italic>.,
<disp-formula id="eqn31">
<graphic xlink:href="564616v3_eqn31.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
In App. B.10, we demonstrate that the Bayes-ratio tuning curves in this case are given by
<disp-formula id="eqn32">
<graphic xlink:href="564616v3_eqn32.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
(instead of <xref ref-type="disp-formula" rid="eqn27">Eq. (27)</xref>) where
<disp-formula id="eqn33">
<graphic xlink:href="564616v3_eqn33.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Note that, for ideal-observer internal models, <italic>P</italic><sup><italic>E</italic></sup> = <italic>P</italic><sup><italic>I</italic></sup>, in which case the integral yields <italic>F</italic><sub><italic>a</italic></sub> = 1 due to the normalization of <italic>f</italic> (<bold><italic>s</italic></bold>| <bold><italic>z</italic></bold>), and we recover <xref ref-type="disp-formula" rid="eqn27">Eq. (27)</xref>. As we can see, the tuning curves neatly decompose into a âbase tuning curveâ <italic>Ïf</italic> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold><sub><italic>a</italic></sub>) multiplied by a stimulus-specific factor <italic>P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>)<sup>â1</sup> and a neuron-specific factor <inline-formula><inline-graphic xlink:href="564616v3_inline27a.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Consequently, if we consider a pair of environments indexed by <italic>Î½</italic> = 0, 1, and assume the likelihood function of the internal model does not change between environments, the transformation of the tuning curves due to adaptation will be given by <xref ref-type="disp-formula" rid="eqn34">Eq. (34)</xref>,
<disp-formula id="eqn34">
<graphic xlink:href="564616v3_eqn34.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Thus, adaptation from environment <italic>Î½</italic> = 0 to environment <italic>Î½</italic> = 1 causes the tuning curves to transform via multiplication by a <italic>stimulus specific factor P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>; <italic>Î½</italic> = 0)<italic>/P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>; <italic>Î½</italic> = 1) and a <italic>neuron specific factor F</italic><sub><italic>a</italic></sub>(<italic>Î½</italic> = 0)<italic>/F</italic><sub><italic>a</italic></sub>(<italic>Î½</italic> = 1). Now suppose an environmental change results in an increase in the prevalence of an adaptor stimulus. As long as the internal model is a sufficiently good (if not perfect) model of <italic>P</italic><sup><italic>E</italic></sup>(<bold><italic>s</italic></bold>), it should adapt its prior distribution, <italic>Ï</italic>(<bold><italic>z</italic></bold>), in a way that results in an increase of its predicted stimulus distribution, <italic>P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>), around the adaptor stimulus. This results in a stimulus specific factor that goes below 1 around the adaptor <bold><italic>s</italic></bold>, thus causing suppression and repulsion of tuning curves away from the adaptor. Now consider a neuron whose preferred stimulus (roughly represented by <bold><italic>z</italic></bold><sub><italic>a</italic></sub>) is close to the adaptor stimulus. We further make the assumption that the change in the internal model, in response to the environmental change, is sufficiently conservative such that it leads to a smaller increase in <italic>P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>) around the adaptor, compared with the increase in the veridical <italic>P</italic><sup><italic>E</italic></sup>(<bold><italic>s</italic></bold>) (this is a reasonable assumption especially for artificially extreme adaptation protocols used in experiments). In this case, we expect the ratio <italic>P</italic><sup><italic>E</italic></sup>(<bold><italic>s</italic></bold>)<italic>/P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>) to increase in the support of <italic>f</italic> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold><sub><italic>a</italic></sub>), which (by our assumption about the preferred stimulus <bold><italic>z</italic></bold><sub><italic>a</italic></sub>) is near the adaptor. According to <xref ref-type="disp-formula" rid="eqn33">Eq. (33)</xref> this will lead to an increase in <italic>F</italic><sub><italic>a</italic></sub>, and hence a suppressive neuron-specific factor <italic>F</italic><sub><italic>a</italic></sub>(<italic>v</italic> = 0)<italic>/F</italic><sub><italic>a</italic></sub>(<italic>v</italic> = 1) for neurons with preferred stimulus near the adaptor. In accordance with this picture, <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref> found that homeostatic and stimulus-specific adaptation in V1 can indeed be modelled via separable stimulus-specific and neuron-specific factors.</p>
<p>Note that the assumption of constancy of the likelihood across the two environments need not be framed as a veridical assumption about the objective environmental change, but rather as an assumption about the inductive biases of the internal generative model, according to which it tends to model changes in the observed stimulus distribution, <italic>P</italic><sup><italic>E</italic></sup>(<bold><italic>s</italic></bold>), as primarily resulting from changes in the statistics of the latent causes in the environment (which the model captures in <italic>Ï</italic>(<bold><italic>z</italic></bold>)), rather than in the causal process itself, as modelled by <italic>f</italic> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold>). As long as this is good enough an assumption to result in a <italic>P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>) which changes in similar ways to <italic>P</italic><sup><italic>E</italic></sup>(<bold><italic>s</italic></bold>) (and in particular exhibits an increase around adaptor stimuli), the conclusions reached above will hold.</p>
<p>In the case of DDC codes that use more general kernel functions, other than the delta functions underlying Bayes-ratio codes, we arrive at an analogous transformation, given by <xref ref-type="disp-formula" rid="eqn138">Eq. (138)</xref> in App. B.10. However, in this case, neuron-specific and stimulus-specific effects are mixed, and the wider the DDC kernel the stronger the mixing. Nevertheless, for kernels that are unimodal and sufficiently narrow, we expect an approximate factorisation of the effect of adaptation into a stimulus-specific and a neuron-specific suppression factor, as seen in <xref ref-type="disp-formula" rid="eqn34">Eq. (34)</xref>.</p>
<p>We now apply this result to the experiments performed by <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>. These experiments examined the effects of adaptation on orientation-tuned neurons in the cat primary visual cortex. Anaesthetised cats were shown a sequence of oriented gratings chosen randomly from a distribution that had an increased prevalence (3 to 5 times more likely) of one particular orientation (arbitrarily defined to be to 0<sup>Â°</sup>). A control group was exposed to a uniform distribution of gratings. After about 2 seconds (on the order of 50 stimulus presentations), V1 tuning curves had adapted, and exhibited both the suppressive and repulsive effects mentioned above.</p>
<p>To model these findings we constructed a homeostatic DDC model as follows, and fit it to the data from <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>. We took the modelâs stimulus and latent variable spaces to be the circular orientation space [â90, 90), and we took the modelâs generative likelihoods to be given by translation invariant functions <italic>f</italic> (<italic>s</italic>| <italic>z</italic>) = <italic>f</italic> (<italic>s</italic>â <italic>z</italic>), proportional to the Gaussian distribution with standard deviation <italic>Ï</italic><sub><italic>f</italic></sub>, normalised over the circle (more rigorously, we ought to use a wrapped (periodic) normal distribution; however, since the standard deviations are small compared to the length of the circle, the normalisation constant is approximately 1, and we can treat the density as a normal Gaussian). Likewise, we take the DDC kernel functions <italic>Ï</italic><sub><italic>a</italic></sub>(<italic>z</italic>) = <italic>Ï</italic> (<italic>z</italic>â <italic>z</italic><sub><italic>a</italic></sub>) proportional to the Gaussian distribution with standard deviation <italic>Ï</italic><sub><italic>Ï</italic></sub>.</p>
<p>The distributions of stimulus orientations used in the experiment is highly artificial, in the way they jump discontinuously near the adaptor orientation. The internal prior distribution employed by the brain is more likely to be smooth, reflecting the brainâs inductive bias adapted to natural environments. Thus in our model we chose the internal prior to be a smooth distribution. The orientation distribution used in the experiment can be understood as a mixture of a uniform distribution, and a spike concentrated at the adaptor (<xref rid="fig11" ref-type="fig">Fig. 11A</xref>, blue curve). To obtain the smooth internal prior in our model, we replace the spike component of the experimental orientation distribution with a Gaussian distribution centred at the adaptor with standard deviation <italic>Ï</italic><sub><italic>Ï</italic></sub> (<xref rid="fig11" ref-type="fig">Fig. 11A</xref>, red curve). Thus, our model has only three free parameters, <italic>Ï</italic><sub><italic>f</italic></sub>, <italic>Ï</italic><sub><italic>Ï</italic></sub> and <italic>Ï</italic><sub><italic>Ï</italic></sub>, <italic>i</italic>.<italic>e</italic>., the widths of the generative likelihood, kernel functions, and the internal orientation prior, respectively.</p>
<fig id="fig11" position="float" fig-type="figure">
<label>Figure 11:</label>
<caption><title>Components of the model for stimulus-specific adaptation in V1.</title>
<p>(A) The distribution of stimulus orientations used in the experiments (<xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>), for an adaptor probability of 30% is shown in blue. We assume V1âs internal model use continuous prior distributions and thus we used instead a mixture of the uniform distribution with a smooth Gaussian, with standard deviation <italic>Ï</italic><sub><italic>Ï</italic></sub>, centered at the adaptor orientation (red curve). (B) The baseline tuning curves, <italic>i</italic>.<italic>e</italic>., the tuning curves adapted to the uniform orientation distribution. The blue curve was obtained by averaging the experimentally measured tuning curves adapted to the uniform orientation distribution, after centering those curves at 0<sup>Â°</sup>. The modelâs baseline tuning curves (red) are given by the convolution of the Gaussian likelihood function, with width <italic>Ï</italic><sub><italic>f</italic></sub>, of the postulated internal generative model, and the Gaussian DDC kernel, with width <italic>Ï</italic><sub><italic>Ï</italic></sub>, and thus are themselves Gaussian with width <inline-formula><inline-graphic xlink:href="564616v3_inline112.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We fit this quantity to the experimentally observed baseline curves as described in <xref ref-type="sec" rid="s4d">Sec. 4.4</xref>.</p></caption>
<graphic xlink:href="564616v3_fig11.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Using the dataset from <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>, we found the baseline tuning curves, <italic>i</italic>.<italic>e</italic>., the tuning curves of the population adapted to the uniform orientation distribution, by assuming that neural response is a function only of the difference between the preferred orientation and stimulus orientation (see <xref ref-type="sec" rid="s4d">Sec. 4.4</xref>). We then calculate the changes in the neuronsâ preferred orientations (due to adaptation and the resulting tuning curve repulsion) in each of the experimental conditions (corresponding to different levels of over-representation of the adaptor stimulus). We then fit our modelâs three parameters to both the unadapted tuning curves (<xref rid="fig11" ref-type="fig">Fig. 11B</xref>) and the changes in preferred orientation (see <xref ref-type="sec" rid="s4d">Sec. 4.4</xref> for further details). The best fit values for the model parameters were found to be <italic>Ï</italic><sub><italic>Ï</italic></sub> = 18<sup>Â°</sup>, <italic>Ï</italic><sub><italic>f</italic></sub> = 14.2<sup>Â°</sup>, and <italic>Ï</italic><sub><italic>Ï</italic></sub> = 20<sup>Â°</sup>. The predictions of our model are compared to experimental data in <xref rid="fig12" ref-type="fig">Fig. 12</xref>. We found that the adaptive tuning curves within our model display suppression and repulsion away from the adaptor stimulus (<xref rid="fig12" ref-type="fig">Fig. 12A,D</xref>). In both the experimental data and our model, unadapted tuning curves exhibit a heightened average firing rate near the adaptor stimulus (<xref rid="fig12" ref-type="fig">Fig. 12B,E</xref>, blue curves). However, adaptation suppresses responses near the adaptor (<xref rid="fig12" ref-type="fig">Fig. 12A,D</xref>, red curves) to just the right degree to return the firing rates to the value before the environmental shift, leading to homeostasis of firing rates (as demonstrated by the uniformity of the red curves in <xref rid="fig12" ref-type="fig">Fig. 12B,E</xref>). Lastly, our model recapitulates the repulsion of preferred orientations found experimentally (<xref rid="fig12" ref-type="fig">Fig. 12C,F</xref>). Repulsion here is reflected by the change in preferred orientation having the same sign as the difference between the pre-adaptation preferred orientation and the adaptor orientation. As seen, in both the experiment and the model, repulsion is strongest for neurons with pre-adaptation preferred orientation about 30<sup>Â°</sup> from the adaptor.</p>
<fig id="fig12" position="float" fig-type="figure">
<label>Figure 12:</label>
<caption><title>A homeostatic DDC model accounts for the observations of stimulus-specific adaptation in V1 <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>.</title>
<p>Panels A-C (left column) and E-F (right column) correspond to the experimental results and the predictions of our model, respectively. (A, D) The tuning curves for the adapted (red) and unadapted (blue) populations, averaged across experimental conditions. (B, E) The trial-averaged firing rates of the adapted (red) and unadapted (blue) populations exposed to the non-uniform stimulus ensemble. The trial-averaged population responses to the uniform stimulus ensemble were normalised to 1. (C, F) The repulsion of preferred orientations, obtained from the average tuning curves in panel A, as a function of the deviation of the neuronsâ preferred orientation from the adaptor orientation.</p></caption>
<graphic xlink:href="564616v3_fig12.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In sum, the application of our homeostatic coding framework to a specific DDC model of Bayesian representations (which approximates what we have termed Bayes-ratio coding) explains quantitatively the empirical observations of both stimulus-specific and homeostatic adaptation in V1 (<xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>).</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<p>We developed a theory of optimal gain modulation for combating noise in neural representations serving arbitrary computations. We demonstrated that â when mean neural firing rates are not too small; neurons are sufficiently selective and sparsely activated; and their responses form a high-dimensional geometry â the trade-off between coding fidelity and metabolic cost is optimised by gains that react to shifts in environmental stimulus statistics to yield firing rate homeostasis. Specifically, our theory predicts the homeostasis of the mean firing rate for clusters of similarly tuned neurons, while, at the same time, it accounts for the optimality of a broad distribution of firing rates within such clusters. By examining empirical estimates of parameters characterising cortical representations, we argued that the conditions necessary for the homeostatic adaptation to be optimal are expected to hold in the visual cortex and potentially other cortical areas. We further validated our approximation by demonstrating numerically that it performs well compared to the calculated optimal gains. Having developed a normative theory of neural homeostasis divorced from the computational aims of the neural population, we next showed how our theory can account for stimulus specific adaptation when coupled with distributed distributional codes (DDC). In particular, we focused on a special case of homeostatic DDC codes which we termed Bayes-ratio coding and showed that this model can account for stimulus specific adaptation effects empirically observed in the primary visual cortex. In the following, we will situate our work within the broader context of efficient coding theory, Bayesian neural representations, and alternative theories of stimulus specific adaptation. We will also discuss the shortcomings of the present work and possible directions for extension.</p>
<p>Within efficient coding theory, our work is closely related to that of <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref>, but differs from it in important ways. <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref> considered a population of tuning curves, defined on a one-dimensional stimulus space, which was homogeneous and translationally invariant up to a warping (<italic>i</italic>.<italic>e</italic>., reparametrization) of the stimulus space and possible variations in gain. Their normative framework then optimised the neural gains as well as the stimulus warping (<italic>i</italic>.<italic>e</italic>., the reparametrization mapping from the original stimulus parameter to one over which the tuning curve population is homogeneous) to maximise a similar objective function to ours. Specifically, their objective employed the Fisher Information lower bound on the mutual information (<xref ref-type="bibr" rid="c7">Brunel and Nadal, 1998</xref>), as opposed to the Gaussian upper bound we used; our choice of the Gaussian upper bound was motivated by increased analytic tractability in the multidimensional stimulus case covered by our theory. In the case of unimodal (and one-dimensional) tuning curves, they showed that the optimal warping amounts to a concentration of tuning curves around stimulus values that are over-represented in the environment. On the other hand, in their setting, the optimal gains stay constant and do not adapt. Nevertheless, their solution also exhibits homeostasis of single-neuron firing rates.</p>
<p>Our framework extends and generalises that of <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref> in different ways. In their framework, the tuning curves are one-dimensional, and rigid constraints (most importantly, either unimodal or sigmoidal tuning curves, and homogeneity up to stimulus reparametrization) are placed on the shape and arrangement of the tuning curve population. In contrast, our framework is applicable to tuning curves with heterogeneous (in particular multi-modal) shapes and arbitrary arrangement over a high-dimensional stimulus space; our homeostatic solution only requires sufficiently high signal-to-noise ratio (which is also the condition for the tightness of the mutual information lower-bound employed in <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref>). On the other hand, we only optimised the neural gains, and not the shapes or arrangement of the tuning curves, as we assumed the latter are determined by the computational goals of the circuit and not by the aim of optimally combating coding noise; in this way, our framework is agnostic to the computational goals of the population. Additionally, <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref> only considered the case of uncorrelated Poissonian noise. In contrast, our framework can handle correlated noise with general (non-Poissonian) power-law scaling. Furthermore, we analytically calculated first-order corrections to the homeostatic solution that arises in the high signal-to-noise ratio limit.</p>
<p>As noted above, the theory of <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref> predicts that tuning curves optimised to provide an efficient code under a stimulus distribution with over-represented stimuli cluster around those stimuli. This has been used to explain asymmetries in neural selectivities, such as the over-abundance of V1 neurons with preferred orientations near cardinal orientations, which are more abundant in natural scenes; or more generally, the aggregation of tuning curves around stimuli that are more prevalent in the natural environment. Many of these asymmetries have likely resulted from adaptation at very long (<italic>e</italic>.<italic>g</italic>., ontogenetic, or possibly even evolutionary) timescales; however, long-term exposure on the order of few minutes to adaptor stimuli has also been found to result in the attraction of tuning curves towards adaptor stimuli (<xref ref-type="bibr" rid="c18">Ghisovan et al., 2009</xref>). This is in contrast to the repulsive effects seen in short-term adaptation (<xref ref-type="bibr" rid="c35">Movshon and Lennie, 1979</xref>; <xref ref-type="bibr" rid="c36">MÃ¼ller et al., 1999</xref>; <xref ref-type="bibr" rid="c14">Dragoi et al., 2000</xref>, <xref ref-type="bibr" rid="c13">2002</xref>; <xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>). It is likely that the effects of long-term exposure are the results of mechanisms which operate in parallel to those underlying short-term adaptation, with both types co-existing at different timescales. We can therefore interpret <xref ref-type="bibr" rid="c15">Ganguli and Simoncelli (2014)</xref> as a model of adaptation to natural stimulus statistics at long timescales. Our results obtained here are applicable to adaptation on shorter timescales (<xref ref-type="bibr" rid="c36">MÃ¼ller et al., 1999</xref>; <xref ref-type="bibr" rid="c14">Dragoi et al., 2000</xref>, <xref ref-type="bibr" rid="c13">2002</xref>; <xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>). In particular, when married with Bayesian theories of neural representation, our framework predicts repulsive effects around an adaptor stimulus, as shown in <xref ref-type="sec" rid="s2j">Sec. 2.10</xref>.</p>
<p><xref ref-type="bibr" rid="c60">Westrick et al. (2016)</xref> also propose a model of the experimental findings of <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>. Their model (which is not explicitly normative or based on efficient coding) uses divisive normalisation (<xref ref-type="bibr" rid="c9">Carandini and Heeger, 2012</xref>) with adaptive weights to achieve homeostasis and stimulus specific repulsion. As discussed above in <xref ref-type="sec" rid="s2i">Sec. 2.9</xref>, Bayes-ratio coding (a special case of homeostatic DDCs, which we showed can account for the findings of <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>) can be accomplished by such a divisive normalisation scheme. Our framework therefore yields a normative interpretation of the model of <xref ref-type="bibr" rid="c60">Westrick et al. (2016)</xref>, and links divisive normalisation with Bayesian representations and efficient coding theory. In particular, our theory provides an interpretation of the normalization weights of the divisive normalization model as the internal prior over latent causes of sensory inputs. This prior should naturally adapt as stimulus statistics change across environments. Additionally, the feedforward inputs to the divisive normalisation model are interpreted as the likelihood of stimuli given latent causes.</p>
<p><xref ref-type="bibr" rid="c46">Snow et al. (2016)</xref> developed two normative models of temporal adaptation effects in V1, based on statistics of dynamic natural visual scenes (natural movies). These models are based on generative models of natural movies in the class of mixtures of Gaussian scale mixture (MGSM) distributions. In MGSMs the outputs of linear oriented filters applied to video frames are assumed to be Gaussian variables, multiplied by positive scale variables. These scale variables can be shared within pools of filter outputs at different times (video frames) and orientations. The two models of <xref ref-type="bibr" rid="c46">Snow et al. (2016)</xref> differ according to which pools of filter outputs are able to share scale variables. In both models, V1 neural responses are assumed to represent the inferred Gaussian latent variables of the corresponding MGSM. While each model was able to account for some of the findings of <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>, neither one could account for all. In particular, each of their two models could only account for either stimulus-specific or neuron-specific adaptation factors found by <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref>. In particular, only the model that accounted for neuron-specific adaptation was able to produce firing rate homeostasis. As we showed in <xref ref-type="sec" rid="s2j">Sec. 2.10</xref>, our framework, which combines homeostatic gain modulation with the DDC theory of representation, robustly accounts both for stimulus-specific adaptation and firing rate homeostasis. Furthermore, when there is a discrepancy between the internal modelâs stimulus prior and the true environmental stimulus distribution, it additionally accounts for a neuron-specific adaptation factor as well (see <xref ref-type="disp-formula" rid="eqn34">Eq. (34)</xref>).</p>
<p>Our approach has relied on a number of assumptions and simplifications; relaxing or generalizing these assumptions provide opportunities for future research. Firstly, our toy model (see <xref ref-type="sec" rid="s2c">Sec. 2.3</xref>) imposed a cluster structure on the neural tuning curves, whereby neurons within a cluster are similarly tuned, while neurons in different clusters have distinct tuning. We found that total firings rates are equalized across clusters (<xref ref-type="sec" rid="s2b">Sec. 2.2</xref>), but the optimal rates of individual neurons within clusters span a broad range (<xref ref-type="sec" rid="s2c">Sec. 2.3</xref>). This results from the fact that the efficient coding objective function depends sharply on total cluster firing rates, but changes only slightly when the partition of the cluster firing rate among the clusterâs neurons changes. In reality, tuning similarity has a more graded distribution. As the distinction between clusters with different tuning (or similarity of tuning within clusters) weakens, higher order terms in our expansion of the objective <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref> cannot be neglected, and hence the justification for dividing the problem into two separate, cluster-level vs. within-cluster, problems breaks down. However, arguably, in a more realistic model with a more graded variation in neural signal correlations, the optimization landscape will still contain shallow directions (<italic>i</italic>.<italic>e</italic>., directions over which the objective changes slowly), corresponding to partitions of firing among similarly tuned neurons, and non-shallow directions corresponding to total firings of such groups of neurons; if so, we would expect a similar solution to emerge, whereby similarly tuned neurons display heterogeneous firing rates, with the total firing rate of such a group of neurons displaying approximate homeostasis.</p>
<p>Secondly, we showed analytically that, at the level of cluster responses, homeostasis emerges universally in the high signal-to-noise regime. In general, firing rate homeostasis is no longer optimal in the low signal-to-noise ratio limit, as correction terms to our solutions become large. However, as we argued in <xref ref-type="sec" rid="s2e">Sec. 2.5</xref>, based on empirical values we expect the signal-to-noise ratio within cortex to be sufficiently high for our results to hold. Generally, within efficient coding theory (except for simple cases such as linear Gaussian models), the regime of arbitrary signal-to-noise ratio is analytically intractable. Many approaches are therefore limited to the regime of high signal-to-noise ratio. For example, the Fisher Information Lower Bound used frequently within efficient coding theory breaks down outside of this regime (<xref ref-type="bibr" rid="c61">Yarrow et al., 2012</xref>; <xref ref-type="bibr" rid="c6">Bethge et al., 2002</xref>; <xref ref-type="bibr" rid="c59">Wei and Stocker, 2016</xref>). Thus, inevitably, exploring the low signal-to-noise ratio regime would require numerical simulation. However, such numerical simulations require concrete models of signal and noise structure. The manner in which optimal coding deviates from homeostatic coding will not be universal across these models, but depend on each modelâs specific details. This limits our ability to draw general conclusions from numerical simulations, which is why we have chosen not to pursue that strategy extensively here.</p>
<p>Thirdly, our analysis here considered a specific class of noise models. However, these noise models include those of particular biological relevance, such as information-limiting noise correlations (<xref ref-type="bibr" rid="c33">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="c22">Kanitscheider et al., 2015</xref>), signatures of which have been found in the cortex (<xref ref-type="bibr" rid="c40">Rumyantsev et al., 2020</xref>), and power-law variability (<xref ref-type="bibr" rid="c19">Goris et al., 2014</xref>) (see <xref ref-type="sec" rid="s2f">Sec. 2.6</xref>). We note that our homeostatic solution is particularly well suited to the case of information-limiting noise correlations, performing optimally in that case (see App. B.5). The core ideas of our framework and derivation could potentially be applied to an even wider class of noise models.</p>
<p>Finally, up to <xref ref-type="sec" rid="s2h">section 2.8</xref>, we make no assumptions about what the neural population is attempting to represent via the representational curves. We then apply our framework to a specific theory of Bayesian encoding (namely DDCs), and develop the new idea of Bayes-ratio coding. There is therefore an opportunity to apply our framework to other theories of neural representation, and in particular to alternative theories of Bayesian representations, such as Probabilistic Population Codes (PPCs) (<xref ref-type="bibr" rid="c4">Beck et al., 2007</xref>).</p>
<p>In summary, we showed that homeostatic coding can arise from optimal gain modulation for fighting corruption by noise in neural representations. Based on this coding scheme, we derived a novel implementation of probabilistic inference in neural populations, known as Bayes-ratio coding, which can be achieved by divisive normalisation with adaptive weights to invert generative probabilistic models in an adaptive manner. This coding scheme accounts for adaptation effects that are otherwise hard to explain exclusively based on efficient coding theory. These contributions provide important connections between Bayesian representation, efficient coding theory, and neural adaptation.</p>
</sec>
<sec id="s4">
<label>4</label>
<title>Methods</title>
<sec id="s4a">
<label>4.1</label>
<title>Specification of environments for simulations</title>
<p>To generate the <italic>K</italic>Ã <italic>K</italic> signal correlation matrix, <italic>Ï</italic>(<italic>Î½</italic>), for the different environments in our numerical simulations, in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref>, we first generate a covariance matrix Î£(<italic>Î½</italic>), and let <italic>Ï</italic>(<italic>Î½</italic>) be the corresponding correlation matrix. The covariance matrix Î£(<italic>Î½</italic>) is in turn constructed such that it has (1) the same eigenspectrum for all <italic>Î½</italic>, with eigenvalues that decrease inversely with rank, as observed in V1 (<xref ref-type="bibr" rid="c49">Stringer et al., 2019</xref>), and (2) an eigenbasis that varies smoothly with <italic>Î½</italic>. Thus we defined Î£(<italic>Î½</italic>) = <italic>U</italic> (<italic>Î½</italic>)Î<sub>1</sub><italic>U</italic> (<italic>Î½</italic>), where Î<sub>1</sub> = diag(1, 1<italic>/</italic>2, 1<italic>/</italic>3, â¦, 1<italic>/K</italic>) and <italic>U</italic> (<italic>Î½</italic>) is an orthogonal matrix that was generated as follows. We randomly and independently sample two <italic>K</italic> Ã <italic>K</italic> random iid Gaussian matrices <italic>R</italic><sub>0</sub>, <italic>R</italic><sub>1</sub> â¼ ð©N<sub><italic>K</italic>Ã<italic>K</italic></sub>(0, <italic>I</italic><sub><italic>K</italic>Ã<italic>K</italic></sub>) and obtain symmetric matrices <inline-formula><inline-graphic xlink:href="564616v3_inline28.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="564616v3_inline29.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. As is well known, the eigen-basis (represented by an orthogonal matrix) of such a random Gaussian matrix is distributed uniformly (<italic>i</italic>.<italic>e</italic>., according to the corresponding Haar measure) over the orthogonal group <italic>O</italic>(<italic>K</italic>). For the extreme environments, <italic>Î½</italic> = 0 and 1, we let <italic>U</italic> (<italic>Î½</italic>) be the matrix of eigenvectors of <italic>S</italic><sub>0</sub> and <italic>S</italic><sub>0</sub>, when ordered according to eigenvalue rank. For 0 &lt; <italic>Î½</italic> &lt; 1, we let <italic>U</italic> (<italic>Î½</italic>) be the matrix of the eigenvectors (again ordered according to eigenvalue rank) of the interpolated symmetric matrix <italic>S</italic>(<italic>Î½</italic>) = (1â<italic>Î½</italic>)<italic>S</italic><sub>0</sub> + <italic>Î½S</italic><sub>1</sub>. Thus, for the extreme environments the eigenbases of the covariance matrices Î£(0) and Î£(1) are sampled independently and uniformly (<italic>i</italic>.<italic>e</italic>., from the Haar measure), and the eigenbases of Î£(<italic>Î½</italic>) for intermediate environments (<italic>i</italic>.<italic>e</italic>., for 0 &lt; <italic>Î½</italic> &lt; 1) smoothly interpolate between these.</p>
<p>In the case of simulations for aligned noise, we also generate a noise correlation matrix <italic>W</italic> (<italic>Î½</italic>) which has an approximately 1<italic>/n</italic><sup><italic>Î³</italic></sup> spectrum. Ideally, <italic>W</italic> and <italic>Ï</italic> would have the same eigen-basis. However, this is impossible, since <italic>W</italic> and <italic>Ï</italic> are both correlation matrices. Instead, we generate <italic>W</italic> (<italic>Î½</italic>) by normalising the positive definite matrix <italic>U</italic> (<italic>Î½</italic>)Î<sub><italic>Î³</italic></sub><italic>U</italic> (<italic>Î½</italic>)<sup>T</sup> where Î<sub><italic>Î³</italic></sub> = diag(1, 1<italic>/</italic>2<sup><italic>Î³</italic></sup>, 1<italic>/</italic>3<sup><italic>Î³</italic></sup>, â¦, 1<italic>/K</italic><sup><italic>Î³</italic></sup>)</p>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Within-cluster optimisation problem</title>
<p>In <xref ref-type="sec" rid="s2c">Sec. 2.3</xref> we showed that in our clustered population model, the firing rates of individual neurons within a cluster optimise the objective <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref>. For a cluster of <italic>k</italic> neurons, this problem is defined by the <italic>k</italic>Ã <italic>k</italic> covariance matrix <inline-formula><inline-graphic xlink:href="564616v3_inline30.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. In our simulations, we generated this matrix as the the Gram matrix of <italic>k</italic> random <italic>D</italic>-dimensional vectors with isotropic Gaussians distributions, <italic>i</italic>.<italic>e</italic>., <inline-formula><inline-graphic xlink:href="564616v3_inline31.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>Î´</italic><bold><italic>w</italic></bold> â¼ ð© (0, <italic>I</italic><sub><italic>D</italic></sub>). (A justification for our construction of the covariance matrix, and an interpretation of the dimensionality <italic>D</italic>, can be found in App. B.3.5.) Additionally, we fixed the cluster rate to <italic>Âµ</italic> = <italic>k</italic> Ã 5Hz, such that the average firing rate per single neuron is 5Hz, and fixed <italic>Ï</italic><sup>2</sup> = 1 (corresponding to a Fano factor of 1). For various values of <italic>k</italic> and <italic>D</italic>, we used the above method to generate 10, 000 samples of the covariance matrix Cov(ÎÎ©). For each such sample, we then solved the problem (12â13) numerically using the cvxpy package in Python (<xref ref-type="bibr" rid="c12">Diamond and Boyd, 2016</xref>). Neurons with firing rates less than 10<sup>â4</sup>Hz had their firing rate set to 0Hz. We aggregated the set of nonzero rates obtained from each of these optimisations to create the histograms in <xref rid="fig8" ref-type="fig">Fig. 8</xref>.</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Estimating noise scaling parameters from biological data</title>
<p>In this section we discuss biological estimates for the parameters of the general form of mean-variance relationship of spike count cluster responses that we have assumed in our noise models,
<disp-formula id="ueqn3">
<graphic xlink:href="564616v3_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Four papers estimate this scaled power-law relationship: the findings of those papers are summarised in the first five columns of <xref rid="tbl1" ref-type="table">Table 1</xref>.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Table summarising empirical findings from four papers which estimate a scaled power-law relationship relationship between mean and variance of cortical spike count responses. To find the values in the last column we adjust the reported <inline-formula><inline-graphic xlink:href="564616v3_inline113.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to account for different coding intervals and cluster sizes used in each experiment (see <xref ref-type="disp-formula" rid="eqn35">Eq. (35)</xref>), and assume <italic>Î²Âµ</italic> = 10.</p></caption>
<graphic xlink:href="564616v3_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<p>Note that the time interval of these papers differs from the our assumed coding interval of 0.1<italic>s</italic>. We therefore must adjust the reported values to be for an interval of this size. We do this by assuming temporal homogeneity over the timescales of interest; specifically we assume that mean and variance are linear in the time interval. We must also adjust the data to account for the fact that our responses are for clusters of <italic>k</italic> = 20 similarly tuned neurons. We do this by assuming uncorrelated response noise within each cluster, making mean and variance both linear functions of cluster size. Our adjustments do not affect the value of <italic>Î²</italic>, but do change the value of the pre-factor <inline-formula><inline-graphic xlink:href="564616v3_inline32.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Specifically, we perform the transformation:
<disp-formula id="eqn35">
<graphic xlink:href="564616v3_eqn35.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>Î´t</italic> is the reported time interval, and Î<italic>t</italic> is our coding interval, Î<italic>t</italic> = 0.1s. <xref rid="fig13" ref-type="fig">Fig. 13</xref> shows the adjusted values on a scatter plot. Using the adjusted values, <italic>Ï</italic><sup>2</sup>, and assuming that <italic>ÂµÎ²</italic> = 10 (to maintain a firing rate of 5Hz per neuron), we can find the value of <italic>Ï</italic><sup>2</sup><italic>/</italic>(<italic>Î²Âµ</italic>)<sup><italic>Î²</italic></sup>. These are reported in the furthest right column of <xref rid="tbl1" ref-type="table">Table 1</xref>.</p>
<fig id="fig13" position="float" fig-type="figure">
<label>Figure 13:</label>
<caption><p>Empirically estimated values of the noise scaling parameter <italic>Î²</italic> and the base noise-level <italic>Ï</italic><sup>2</sup>, after adjusting for coding interval duration and cluster size using the formula <xref ref-type="disp-formula" rid="eqn35">Eq. (35)</xref>. The dashed lines show the values for Poissonian noise (<italic>Ï</italic><sup>2</sup> = 1, <italic>Î²</italic> = 1).</p></caption>
<graphic xlink:href="564616v3_fig13.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>Fitting a homeostatic DDC to <xref ref-type="bibr" rid="c5">Benucci <italic>et al</italic>. 2013</xref></title>
<p>The data we obtained from <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref> comprised 11 datasets. In each dataset, the neural population was clustered into 12 groups based on preferred orientation. These neural populations were then exposed (in different contexts) to a distribution of 6 to 12 oriented gratings. This distribution was either uniform or biased; in the latter case, one particular grating (arbitrarily defined as 0<sup>Â°</sup>) had higher prevalence, by either 30%, 35%, 40%, or 50%. We discarded all datasets with a 50% prevalence, since (<xref ref-type="bibr" rid="c5">Benucci et al., 2013</xref>) report that homeostasis was not obtained in this case, and they similarly discarded it from their main analysis. After exposure to the distribution of gratings, the responses (<italic>i</italic>.<italic>e</italic>., spike count) of each cluster to a test set of 20 oriented gratings were then measured. Following <xref ref-type="bibr" rid="c5">Benucci et al. (2013)</xref> (see that reference for details), we normalized responses via an affine transform so that after exposure to the uniform stimulus ensemble, the normalised response ranged from 0 to 1.</p>
<p>For each dataset, we obtained the average tuning curve in the uniform ensemble context (<xref rid="fig11" ref-type="fig">Fig. 11B</xref>, blue) we used a cubic spline to interpolate and up-sample all the population tuning curves in that context. These were then centered at 0<sup>Â°</sup> and averaged across populations with different preferred orientations, yielding a single average tuning curve. To fit the resultant curve with a Gaussian (<xref rid="fig11" ref-type="fig">Fig. 11B</xref>, red), the width of this curve at height <italic>e</italic><sup>â2</sup> was divided by 4 to obtain the standard deviation of the Gaussian. In subsequent fitting of the model, <inline-formula><inline-graphic xlink:href="564616v3_inline33.gif" mime-subtype="gif" mimetype="image"/></inline-formula> was constrained to equal this standard deviation. This reduces the number of free parameters to 2, which we choose to be <italic>Ï</italic><sub><italic>Ï</italic></sub> and <italic>Ï</italic><sub><italic>f</italic></sub>. We found the best value of these parameters by fitting the curves for repulsion of tuning curves (adaptation-induced change in preferred orientation vs. pre-adaptation preferred orientation â <xref rid="fig12" ref-type="fig">Fig. 12C</xref> and <xref ref-type="fig" rid="fig12">F</xref>) between the model and experimental data. Specifically, we performed a grid search over values of <italic>Ï</italic><sub><italic>Ï</italic></sub> and <italic>Ï</italic><sub><italic>f</italic></sub> and chose the pair of values which minimised the sum of the absolute value of the difference between the empirical and model curves.</p>
<p>We obtained the empirical repulsion curves (<xref rid="fig12" ref-type="fig">Fig. 12C</xref>) as follows. For each preferred orientation cluster and context (<italic>i</italic>.<italic>e</italic>., exposure to a uniform or biased ensemble), a smoothing kernel was applied before using cubic interpolation to generate up-sampled tuning curves. The smoothing kernel was applied to ensure that none of the tuning curves were multimodal (multimodality was a minor effect â reasonably attributable to finite-trial averaging noise â but could have nevertheless introduced relatively large noise in the estimated preferred orientations). The argmax of these tuning curves was found to give the preferred orientation. The preferred orientation was then compared across conditions (uniform ensemble vs biased ensemble) to give a change in preferred orientation. These were then averaged across contexts with the same adaptor probability to obtain the repulsion curve for each adaptor probability.</p>
</sec>
</sec>
</body>
<back>
<sec id="s5">
<title>Appendix A. Extended data figures</title>
<fig id="fig14" position="float" fig-type="figure">
<label>Figure 14:</label>
<caption><p>Distribution of average firing rates before and after a discrete environmental shift for the uncorrelated power-law noise subfamily. Each panel has the same format as in <xref ref-type="fig" rid="fig5">Fig. 5C</xref>, but for different values of the noise scaling parameter <italic>Î±</italic>. The case <italic>Î±</italic> = 0.5 is shown in <xref ref-type="fig" rid="fig5">Fig. 5C</xref>.</p></caption>
<graphic xlink:href="564616v3_fig14.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig15" position="float" fig-type="figure">
<label>Figure 15:</label>
<caption><p>Distribution of average firing rates before and after a discrete environmental shift for the Poissonian noise subfamily with uniform noise correlations. Each panel has the same format as in <xref ref-type="fig" rid="fig6">Fig. 6C</xref>, but for different values of the noise correlation coefficient <italic>p</italic>. The case <italic>p</italic> = 0.2 is shown in <xref ref-type="fig" rid="fig6">Fig. 6C</xref>.</p></caption>
<graphic xlink:href="564616v3_fig15.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig16" position="float" fig-type="figure">
<label>Figure 16:</label>
<caption><p>Distribution of average firing rates before and after a discrete environmental shift for the aligned Poissonian noise subfamily. Each panel has the same format as in <xref ref-type="fig" rid="fig7">Fig. 7C</xref>, but for different values of the noise spectrum decay parameter <italic>Î³</italic>. The case <italic>Î³</italic> = 1.5 is shown in <xref ref-type="fig" rid="fig7">Fig. 7C</xref>.</p></caption>
<graphic xlink:href="564616v3_fig16.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig17" position="float" fig-type="figure">
<label>Figure 17:</label>
<caption><title>Accuracy of homeostatic approximation to optimal gains.</title>
<p>The plots in the panels A, C, and E show the relative improvement in the efficient coding objective for the zeroth-order approximation for the optimal gains <bold><italic>g</italic></bold><sup>(0)</sup>, <xref ref-type="disp-formula" rid="eqn16">Eq. (16)</xref>, for the three noise sub-families; see the caption of <xref ref-type="fig" rid="fig9">Fig. 9</xref> for a description of panels B, D, and F there, respectively. Plots in panels B, D, and F here similarly show the relative improvement in the efficient coding objective for the first-order approximation <bold><italic>g</italic></bold><sup>(1)</sup>, <xref ref-type="disp-formula" rid="eqn17">Eq. (17)</xref>, for the three noise sub-families.</p></caption>
<graphic xlink:href="564616v3_fig17.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig18" position="float" fig-type="figure">
<label>Figure 18:</label>
<caption><title>Systematic exploration of how the firing rate distribution of individual neurons varies as the statistical properties of cluster responses change.</title>
<p>The panels here have the same description as those in <xref ref-type="fig" rid="fig8">Fig. 8</xref>, but show the single-neuron firing rate distributions for different choices of cluster size <italic>k</italic> and effective tuning curve dimension, <italic>D</italic>. (Note that the upper-right panels corresponding to cases in which <italic>D</italic> &lt; <italic>k</italic> are left empty; this is because in such cases the covariance matrix is singular, and therefore the quadratic program does not have a unique solution. Such cases are also less relevant biologically given realistic estimates for the size of the similarly tuned neurons vs. the effective dimensionality of tuning curves, noting that the nominal dimension of the tuning curve function space is infinite.)</p></caption>
<graphic xlink:href="564616v3_fig18.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s6">
<title>Appendix B. Supplementary Information: mathematical derivations</title>
<sec id="s6a">
<title>B.1 Analytic expression for L</title>
<p>In this appendix we derive an analytic expression for the upper bound objective â. We adopt the noise model <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>, repeated here for convenience:
<disp-formula id="eqn36">
<graphic xlink:href="564616v3_eqn36.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where Î£(<bold><italic>s</italic></bold>) is the stimulus-dependent noise correlation matrix, and 0 &lt; <italic>Î±</italic> &lt; 2 is a scaling parameter. Here we have made use of our convention that the non-bold version of a vector (without indices) denotes the diagonal matrix formed from that vector, so that, for example, <italic>h</italic>(<bold><italic>s</italic></bold>) is the diagonal matrix with <italic>aa</italic>-th entry equal to <italic>h</italic><sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>).</p>
<p>We decompose the mutual information between the spike counts and the stimulus as
<disp-formula id="ueqn4">
<graphic xlink:href="564616v3_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and then obtain â by replacing the term <inline-formula><inline-graphic xlink:href="564616v3_inline34.gif" mime-subtype="gif" mimetype="image"/></inline-formula> with the entropy of a Gaussian with equal covariance to <bold><italic>n</italic></bold>. Doing so gives us the expression
<disp-formula id="eqn37">
<graphic xlink:href="564616v3_eqn37.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We consider â as a function of the rates <italic>r</italic><sub><italic>a</italic></sub> = <italic>Ï</italic><sub><italic>a</italic></sub><italic>g</italic><sub><italic>a</italic></sub> in order to keep the derivation clean.</p>
<p>We start by deriving an expression for the noise entropy <italic>H</italic>[<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>]. We use â to denote equality up to additive pure constants or terms independent of <italic>r</italic> and <italic>g</italic>. We have
<disp-formula id="ueqn5">
<graphic xlink:href="564616v3_ueqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
(where in deriving the third line we used <italic>h</italic> = <italic>g</italic>Î© = <italic>rÏ</italic><sup>â1</sup>Î©), hence
<disp-formula id="eqn38">
<graphic xlink:href="564616v3_eqn38.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Next, we find an expression for Cov(<bold><italic>n</italic></bold>), using the orthogonal decomposition Cov(<bold><italic>n</italic></bold>) = ð¼ [Cov(<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>)] + Cov(ð¼ [<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>]). For the first term have
<disp-formula id="ueqn6">
<graphic xlink:href="564616v3_ueqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we used <italic>r</italic> = ð¼ [<italic>h</italic>(<bold><italic>s</italic></bold>)]. Thus
<disp-formula id="eqn39">
<graphic xlink:href="564616v3_eqn39.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we have defined (c.f. <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref> of main text) the normalized stimulus-averaged noise covariance matrix <italic>W</italic> as
<disp-formula id="ueqn7">
<graphic xlink:href="564616v3_ueqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Next, we find an expression for Cov(ð¼ [<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>]). Using <bold><italic>h</italic></bold>(<bold><italic>s</italic></bold>) = ð¼ [<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>], we find
<disp-formula id="ueqn8">
<graphic xlink:href="564616v3_ueqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
or
<disp-formula id="eqn40">
<graphic xlink:href="564616v3_eqn40.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we defined
<disp-formula id="eqn41">
<graphic xlink:href="564616v3_eqn41.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and introduced the coefficients of variation (CV) of trial-averaged responses, given by <italic>h</italic>(<bold><italic>s</italic></bold>) (or equivalently, the CVâs of the representational curves)
<disp-formula id="eqn42">
<graphic xlink:href="564616v3_eqn42.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and the Pearsonâs correlation matrix of <italic>h</italic> (or equivalently of the Î©âs), defined by
<disp-formula id="eqn43">
<graphic xlink:href="564616v3_eqn43.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Thus <italic>Ï</italic> is the matrix of signal correlations.</p>
<p>Putting <xref ref-type="disp-formula" rid="eqn39">Eq. (39)</xref> and <xref ref-type="disp-formula" rid="eqn40">Eq. (40)</xref> together, we obtain
<disp-formula id="ueqn9">
<graphic xlink:href="564616v3_ueqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Finally, plugging this result and <xref ref-type="disp-formula" rid="eqn38">Eq. (38)</xref> into <xref ref-type="disp-formula" rid="eqn37">Eq. (37)</xref>, up to an additive constant, we obtain
<disp-formula id="eqn44">
<graphic xlink:href="564616v3_eqn44.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
as required. Substituting back in <italic>r</italic><sub><italic>a</italic></sub> = <italic>Ï</italic><sub><italic>a</italic></sub><italic>g</italic><sub><italic>a</italic></sub> gets us <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>.</p>
</sec>
<sec id="s6b">
<title>B.2 Upper bound for Poissonian noise</title>
<p>In this appendix, we consider the following model for cluster spike counts.
<disp-formula id="eqn45">
<graphic xlink:href="564616v3_eqn45.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here we derive a Gaussian upper bound to the mutual information, and show that an approximation to it leads to the same expression <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref> for â derived for the Gaussian noise case, for <italic>Ï</italic><sup>2</sup><italic>W</italic> = <italic>I</italic> and <italic>Î±</italic> = 1<italic>/</italic>2 (<italic>i</italic>.<italic>e</italic>., uncorrelated unit Fano factor noise). We start with the objective
<disp-formula id="eqn46">
<graphic xlink:href="564616v3_eqn46.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We decompose <italic>I</italic>(<bold><italic>n</italic></bold>; <bold><italic>s</italic></bold>) = â <italic>H</italic>[<bold><italic>n</italic></bold>] <italic>H</italic>[<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>]. We will once again upper bound the marginal entropy <italic>H</italic>[<bold><italic>n</italic></bold>]. A difficulty arises from the fact that a Poisson random variable is discrete and the Gaussian upper bound we previously used is for a continuous random variables. We address this problem as follows: Consider a random variable <bold><italic>U</italic></bold> which is uniformly distributed on [0, 1)<sup><italic>K</italic></sup>, independent of <bold><italic>n</italic></bold>. Then <bold><italic>n</italic></bold> + <bold><italic>U</italic></bold> is a continuous random variable. We apply the Gaussian bound to this. Let <italic>p</italic> be the p.d.f. of <bold><italic>n</italic></bold> + <bold><italic>U</italic></bold>, P the p.m.f. of <bold><italic>n</italic></bold>, and <italic>u</italic> the p.d.f. of <bold><italic>U</italic></bold>.
<disp-formula id="ueqn10">
<graphic xlink:href="564616v3_ueqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Putting this together gives us the upper bound
<disp-formula id="eqn47">
<graphic xlink:href="564616v3_eqn47.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We now address the problem of the marginal entropy <italic>H</italic>[<bold><italic>n</italic></bold>|<bold><italic>s</italic></bold>]. By conditional independence, we have that
<disp-formula id="eqn48">
<graphic xlink:href="564616v3_eqn48.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We now make use of an additional assumption, namely that the representational curves Î©<sub><italic>a</italic></sub> have a baseline, and in particular
<disp-formula id="eqn49">
<graphic xlink:href="564616v3_eqn49.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
everywhere. Under this condition we can make the approximation that for fixed <bold><italic>s</italic></bold>,
<disp-formula id="eqn50">
<graphic xlink:href="564616v3_eqn50.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
This means we can obtain the approximate upper bound:
<disp-formula id="ueqn11">
<graphic xlink:href="564616v3_ueqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where â (<bold><italic>g</italic></bold>) is the same functional that we defined earlier for Gaussian random variables, in the case of uncorrelated, unit-fano factor noise, <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>. We have made an additional approximation by neglecting the <italic>I</italic><sub><italic>K</italic></sub><italic>/</italic>12 term, which relevant parameter conditions is negligible compared the covariance.</p>
</sec>
<sec id="s6c">
<title>B.3 Mathematical treatment of the clustered population</title>
<p>In this section we consider a clustered population in which the representational curves are a small perturbation to cluster-wide representational curves. Let <italic>C</italic><sub><italic>a</italic></sub> denote cluster <italic>a</italic> (or rather the set of indices of neurons belonging to that cluster). Then for <italic>i</italic> â <italic>C</italic><sub><italic>a</italic></sub>,
<disp-formula id="eqn51">
<graphic xlink:href="564616v3_eqn51.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We use hats to denote single-neuron quantities, and corresponding no-hat symbols for either cluster averages or zeroth-order values of tuning curves, etc, that are uniform across neurons in a cluster and hence only depend on cluster indices. In particular, we use <italic>P</italic> = CV<italic>Ï</italic>CV, defined in <xref ref-type="disp-formula" rid="eqn41">Eq. (41)</xref>, at cluster level (hence a <italic>K</italic>Ã <italic>K</italic> matrix), and use <inline-formula><inline-graphic xlink:href="564616v3_inline35.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to denote the equivalent matrix defined for individual neurons (an <italic>N</italic>Ã <italic>N</italic> matrix). As we noted in the main text <xref ref-type="sec" rid="s2c">Sec. 2.3</xref>, for this analysis, we limit ourselves to the case of uncorrelated noise, Î£(<bold><italic>s</italic></bold>) = <italic>I</italic>, with Poisson-like scaling <italic>Î±</italic> = 1<italic>/</italic>2. Mirroring <xref ref-type="disp-formula" rid="eqn44">Eq. (44)</xref>, the population level objective in this case can be written as
<disp-formula id="eqn52">
<graphic xlink:href="564616v3_eqn52.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
In the rest of this section we will set <italic>Ï</italic> to 1 by absorbing it into <inline-formula><inline-graphic xlink:href="564616v3_inline36.gif" mime-subtype="gif" mimetype="image"/></inline-formula>; at the end, we can replace all <inline-formula><inline-graphic xlink:href="564616v3_inline37.gif" mime-subtype="gif" mimetype="image"/></inline-formula> âor factors proportional to themâ by <inline-formula><inline-graphic xlink:href="564616v3_inline38.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to recover the general case).</p>
<p>To simplify the mathematical derivation, we will assume that clusters are the same size <italic>k</italic> = <italic>N/K</italic>, and that w.l.o.g. the population is sorted so that neurons in the same cluster appear adjacent to each other in the ordering. However, the assumption of equal size clusters is not essential, and our final results are valid for the case of clusters of variable size as well. To zeroth order in <italic>Ïµ</italic>, the elements of <inline-formula><inline-graphic xlink:href="564616v3_inline39.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are constant over blocks corresponding to clusters; in other words
<disp-formula id="eqn53">
<graphic xlink:href="564616v3_eqn53.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
for some deviation <inline-formula><inline-graphic xlink:href="564616v3_inline40.gif" mime-subtype="gif" mimetype="image"/></inline-formula> that is <italic>O</italic>(<italic>Ïµ</italic>) (here <bold>1</bold><sub><italic>k</italic></sub> denotes the <italic>k</italic>-dimensional vector with all components equal to 1). However, we will find that the first nonzero corrections to the loss (for <italic>Ïµ</italic> &gt; 0) arise from <italic>O</italic>(<italic>Ïµ</italic><sup>2</sup>) corrections to <inline-formula><inline-graphic xlink:href="564616v3_inline41.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (or equivalently to <inline-formula><inline-graphic xlink:href="564616v3_inline42.gif" mime-subtype="gif" mimetype="image"/></inline-formula> as defined by <xref ref-type="disp-formula" rid="eqn53">Eq. (53)</xref>). We will therefore (1) expand to second order in <inline-formula><inline-graphic xlink:href="564616v3_inline43.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, (2) (in the next section) expand <inline-formula><inline-graphic xlink:href="564616v3_inline44.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to second order in <italic>Ïµ</italic>, and (3) substitute <inline-formula><inline-graphic xlink:href="564616v3_inline45.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in the loss and eventually neglect terms that are <italic>o</italic>(<italic>Ïµ</italic><sup>2</sup>).</p>
<p>We start by expanding. Using <xref ref-type="disp-formula" rid="eqn60">Eq. (60)</xref>, and plugging <xref ref-type="disp-formula" rid="eqn53">Eq. (53)</xref> into <xref ref-type="disp-formula" rid="eqn52">Eq. (52)</xref> and expanding to second order in <inline-formula><inline-graphic xlink:href="564616v3_inline46.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, we obtain
<disp-formula id="eqn54">
<graphic xlink:href="564616v3_eqn54.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where
<disp-formula id="eqn55">
<graphic xlink:href="564616v3_eqn55.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn56">
<graphic xlink:href="564616v3_eqn56.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn57">
<graphic xlink:href="564616v3_eqn57.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and we defined the following matrices:
<disp-formula id="eqn58">
<graphic xlink:href="564616v3_eqn58.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn59">
<graphic xlink:href="564616v3_eqn59.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here and in the rest of this appendix we use <bold>1</bold>, instead of <bold>1</bold><sub><italic>k</italic></sub>, to denote the <italic>k</italic>-dimensional vector with all components equal to 1.</p>
<sec id="s6c1">
<title>B.3.1 Cluster level problem at the zeroth order</title>
<p>We now show that the zeroth order loss, <xref ref-type="disp-formula" rid="eqn55">Eq. (55)</xref>, depends only on the cluster rates <inline-formula><inline-graphic xlink:href="564616v3_inline47.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. This objective has two components: the information term and the energetic term. For the energetic term the claim is obvious, as we have
<disp-formula id="eqn60">
<graphic xlink:href="564616v3_eqn60.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>r</italic><sub><italic>a</italic></sub> denotes the total rate of cluster <italic>a</italic>. As for the information term, and will show that
<disp-formula id="ueqn12">
<graphic xlink:href="564616v3_ueqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
can be re-written (up to a constant) as
<disp-formula id="eqn61">
<graphic xlink:href="564616v3_eqn61.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
reducing the problem to a cluster-level one (the second determinant is of a <italic>K</italic> Ã <italic>K</italic> matrix, rather than <italic>N</italic> Ã <italic>N</italic>, and only depends on cluster rates <italic>r</italic><sub><italic>a</italic></sub> and the zeroth-order signal correlations encoded in <italic>P</italic>). Let us denote the Cholesky factor of (the positive definite) <italic>P</italic> by <italic>U</italic>, such that <italic>P</italic> = <italic>UU</italic>. We thus have
<disp-formula id="ueqn13">
<graphic xlink:href="564616v3_ueqn13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Further defining <inline-formula><inline-graphic xlink:href="564616v3_inline48.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to be the <italic>N</italic> Ã <italic>K</italic> matrix <inline-formula><inline-graphic xlink:href="564616v3_inline49.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, we find
<disp-formula id="eqn62">
<graphic xlink:href="564616v3_eqn62.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
By the matrix determinant lemma we then obtain
<disp-formula id="eqn63">
<graphic xlink:href="564616v3_eqn63.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Finally,
<disp-formula id="eqn64">
<graphic xlink:href="564616v3_eqn64.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Using this and making use of the matrix determinant lemma one more time, we see that the determinant on the right hand side of <xref ref-type="disp-formula" rid="eqn63">Eq. (63)</xref> can be written as <xref ref-type="disp-formula" rid="eqn61">Eq. (61)</xref>.</p>
<p>Putting together the information and energetic terms (and momentarily returning the <italic>Ï</italic><sup>2</sup>), we obtain that (up to an additive constant)
<disp-formula id="eqn65">
<graphic xlink:href="564616v3_eqn65.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s6c2">
<title>B.3.2 Expansion of <inline-formula><inline-graphic xlink:href="564616v3_inline50.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in <italic>ÏµÎ´</italic>Î©</title>
<p>We will denote <italic>ÏµÎ´</italic>Î©<sub><italic>i</italic></sub> above by <italic>V</italic><sub><italic>i</italic></sub> in this subsections (so <italic>V</italic> = <italic>O</italic>(<italic>Ïµ</italic>)), and will use <italic>Î´</italic>âs to denote deviation from expectation: <italic>Î´X</italic> = <italic>X</italic> â ð¼ [<italic>X</italic>]. We have
<disp-formula id="eqn66">
<graphic xlink:href="564616v3_eqn66.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn67">
<graphic xlink:href="564616v3_eqn67.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="564616v3_inline51.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We also have
<disp-formula id="eqn68">
<graphic xlink:href="564616v3_eqn68.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
or (keeping cluster indices explicit but hiding witihn-cluster ones)
<disp-formula id="eqn69">
<graphic xlink:href="564616v3_eqn69.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we defined the vector <bold><italic>c</italic></bold><sub><italic>ab</italic></sub> to have components
<disp-formula id="eqn70">
<graphic xlink:href="564616v3_eqn70.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
And
<disp-formula id="eqn71">
<graphic xlink:href="564616v3_eqn71.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Since
<disp-formula id="eqn72">
<graphic xlink:href="564616v3_eqn72.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
we also expand the diagonal factors to second order in <italic>V</italic> :
<disp-formula id="eqn73">
<graphic xlink:href="564616v3_eqn73.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we defined
<disp-formula id="eqn74">
<graphic xlink:href="564616v3_eqn74.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
(note that <italic>Ï</italic><sub><italic>a</italic></sub> are numbers, but <inline-formula><inline-graphic xlink:href="564616v3_inline52.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, <italic>v</italic><sub><italic>a</italic></sub> and <italic>u</italic><sub><italic>a</italic></sub> are <italic>k</italic> Ã <italic>k</italic> diagonal matrices).</p>
<p>First we consider the first order (in <italic>V</italic> or equivalently in <italic>Ïµ</italic>) contributions to <inline-formula><inline-graphic xlink:href="564616v3_inline52a.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Using <xref ref-type="disp-formula" rid="eqn69">Eq. (69)</xref> and <xref ref-type="disp-formula" rid="eqn73">Eq. (73)</xref> in <xref ref-type="disp-formula" rid="eqn72">Eq. (72)</xref> we have
<disp-formula id="eqn75">
<graphic xlink:href="564616v3_eqn75.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn76">
<graphic xlink:href="564616v3_eqn76.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we defined
<disp-formula id="eqn77">
<graphic xlink:href="564616v3_eqn77.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Next, we obtain the second order contributions to <inline-formula><inline-graphic xlink:href="564616v3_inline53.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Using <xref ref-type="disp-formula" rid="eqn69">Eq. (69)</xref> and <xref ref-type="disp-formula" rid="eqn73">Eq. (73)</xref> in <xref ref-type="disp-formula" rid="eqn72">Eq. (72)</xref>, we obtain
<disp-formula id="eqn78">
<graphic xlink:href="564616v3_eqn78.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and in particular
<disp-formula id="eqn79">
<graphic xlink:href="564616v3_eqn79.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s6c3">
<title>B.3.3 <italic>Î´</italic>Î©<sub><italic>i</italic></sub> corrections to the loss</title>
<p>We now consider the contributions of the first and second order corrections to <inline-formula><inline-graphic xlink:href="564616v3_inline54.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to the loss, via <xref ref-type="disp-formula" rid="eqn56">Eqs. (56)</xref> and (57): to calculate corrections to the loss to <italic>O</italic>(<italic>Ïµ</italic><sup>2</sup>), we need to plug <inline-formula><inline-graphic xlink:href="564616v3_inline55.gif" mime-subtype="gif" mimetype="image"/></inline-formula> into L<sup>(1)</sup>, and plug in <inline-formula><inline-graphic xlink:href="564616v3_inline56.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (only) in â <sup>(2)</sup>. Firstly, consider the matrix inverse that appears in expressions <xref ref-type="disp-formula" rid="eqn56">Eqs. (56)</xref>â<xref ref-type="disp-formula" rid="eqn57">(57)</xref> for â <sup>(1)</sup> and â <sup>(2)</sup>. Using <inline-formula><inline-graphic xlink:href="564616v3_inline57.gif" mime-subtype="gif" mimetype="image"/></inline-formula> from <xref ref-type="disp-formula" rid="eqn62">Eq. (62)</xref> and the Woodbury matrix identity, we can write
<disp-formula id="ueqn14">
<graphic xlink:href="564616v3_ueqn14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Using <inline-formula><inline-graphic xlink:href="564616v3_inline58.gif" mime-subtype="gif" mimetype="image"/></inline-formula> see <xref ref-type="disp-formula" rid="eqn64">Eq. (64)</xref>â we find
<disp-formula id="ueqn15">
<graphic xlink:href="564616v3_ueqn15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Assuming <italic>P</italic> is full-rank (the generic case), we have <italic>U</italic> [<italic>I</italic><sup><italic>K</italic></sup> + <italic>U</italic><sup>T</sup> <italic>rU</italic>]<sup>â1</sup><italic>U</italic><sup>T</sup> = (<italic>U</italic> <sup>â1</sup>)<sup>T</sup> <italic>U</italic> <sup>â1</sup> + <italic>r)</italic><sup>â1</sup> = (<italic>P</italic> <sup>â1</sup> + <italic>r</italic>)<sup>â1</sup>,</p>
<p>Yielding
<disp-formula id="eqn80">
<graphic xlink:href="564616v3_eqn80.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Up until this point, all expressions we have obtained have been exact (apart from the <italic>Ïµ</italic>-expansion itself). We now make an approximation to the full perturbation objective which is valid in the high signal-to-noise ratio regime as outlined in <xref ref-type="sec" rid="s2d">Sec. 2.4</xref>. Specifically, we will take the matrix
<disp-formula id="ueqn15a">
<graphic xlink:href="564616v3_ueqn15a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
to be small, and expand to zeroth order in this matrix. Î represents the high-dimensional structure of signal-to-noise ratio. As we show in App. B.4, to zeroth order in this matrix, <italic>r</italic> = <italic>ÂµI</italic><sub><italic>K</italic></sub>. Using the approximation
<disp-formula id="eqn81">
<graphic xlink:href="564616v3_eqn81.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
valid to zeroth-order in Î, in <xref ref-type="disp-formula" rid="eqn80">Eq. (80)</xref> we obtain
<disp-formula id="eqn82">
<graphic xlink:href="564616v3_eqn82.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we defined
<disp-formula id="eqn83">
<graphic xlink:href="564616v3_eqn83.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Note that in writing <xref ref-type="disp-formula" rid="eqn81">Eq. (81)</xref> we have assumed that cluster rates are <italic>O</italic>(<italic>Âµ</italic>), so that the second term in the parenthesis is <italic>O</italic>(1) and dominates Î; this will be justified <italic>a posteriori</italic>, by the homeostasis result for the inter-cluster problem (as the approximate optimiser of <xref ref-type="disp-formula" rid="eqn65">Eq. (65)</xref>, in the small Î regime).</p>
<p>We will show that C and hence <italic>I</italic> â C are projection operators and we will characterise the latterâs kernel (<italic>i</italic>.<italic>e</italic>., the vectors annihilated by it). First C is clearly a symmetric matrix. So we just need to show that C<sup>2</sup> = C. To prove this (and other statements), we note that products of <italic>N</italic> Ã <italic>N</italic> matrices (or their products with <italic>N</italic> -dimensional vectors) can be written in terms of products of their blocks (corresponding to the clustering of neurons) as follows: <inline-formula><inline-graphic xlink:href="564616v3_inline59.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where the subscripts index blocks, with <italic>a, b</italic> â {1, â¦ <italic>K</italic>}, and <italic>A</italic><sub><italic>ac</italic></sub> and <italic>B</italic><sub><italic>cb</italic></sub> are multiplying as matrices. As we will see our results do not truly rely on the tensor product structure we have assumed for various matrices (here A and C); but only on their constancy within blocks defined by clusters. <italic>Thus our results generalise to the case where clusters contain different number of neurons</italic>. For simplicity, however, we will stick to the tensor structure, corresponding to the same number of neurons in different clusters. Using the above observation, we can write (note that below <italic>r</italic><sub><italic>a</italic></sub> and <italic>Î´</italic><sub><italic>ab</italic></sub> are scalars, while <inline-formula><inline-graphic xlink:href="564616v3_inline60.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denotes the <italic>a</italic>-th diagonal block of <inline-formula><inline-graphic xlink:href="564616v3_inline61.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and is a diagonal matrix)
<disp-formula id="ueqn16">
<graphic xlink:href="564616v3_ueqn16.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we used <inline-formula><inline-graphic xlink:href="564616v3_inline62.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p>Similarly for vectors of the form <inline-formula><inline-graphic xlink:href="564616v3_inline63.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (namely vectors with components that are constant over each cluster), we have <inline-formula><inline-graphic xlink:href="564616v3_inline64.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, as
<disp-formula id="ueqn17">
<graphic xlink:href="564616v3_ueqn17.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
(note that <italic>w</italic><sub><italic>a</italic></sub> are scalars). Thus if <inline-formula><inline-graphic xlink:href="564616v3_inline65.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is any such vector, then <inline-formula><inline-graphic xlink:href="564616v3_inline66.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is left invariant by <inline-formula><inline-graphic xlink:href="564616v3_inline67.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Finally, this means that <inline-formula><inline-graphic xlink:href="564616v3_inline68.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is in the kernel of <italic>I</italic> â C = (<italic>I</italic> + A)<sup>â1</sup> and is annihilated by it. Moreover, since ð is symmetric, row vectors of the form <bold><italic>w</italic></bold><sup>T</sup> <bold>1</bold><sup>T</sup> are annihilated when multiplied by <inline-formula><inline-graphic xlink:href="564616v3_inline69.gif" mime-subtype="gif" mimetype="image"/></inline-formula> on the right. Finally, matrices with cluster blocks that have uniform rows (columns) are annihilated when multiplied by <inline-formula><inline-graphic xlink:href="564616v3_inline70.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (by <inline-formula><inline-graphic xlink:href="564616v3_inline70a.gif" mime-subtype="gif" mimetype="image"/></inline-formula>) on the left (right).</p>
<p>It follows immediately that, given the structure of <xref ref-type="disp-formula" rid="eqn56">Eqs. (56)</xref>â<xref ref-type="disp-formula" rid="eqn58">(58)</xref>, any of the terms in the expressions <xref ref-type="disp-formula" rid="eqn76">Eqs. (76)</xref>â<xref ref-type="disp-formula" rid="eqn89">(89)</xref> for blocks of <inline-formula><inline-graphic xlink:href="564616v3_inline71.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="564616v3_inline72.gif" mime-subtype="gif" mimetype="image"/></inline-formula> that have <bold>1</bold> as an outer product factor contribute nothing to <italic>Î´â</italic>. In particular, the <italic>O</italic>(<italic>Ïµ</italic>) part, <inline-formula><inline-graphic xlink:href="564616v3_inline73.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, makes no contribution to the loss. Since we are interested in leading order corrections, it thus suffices to only consider the contribution of<inline-formula><inline-graphic xlink:href="564616v3_inline74.gif" mime-subtype="gif" mimetype="image"/></inline-formula>âafter dropping terms involving <bold>1</bold> in <xref ref-type="disp-formula" rid="eqn89">Eq. (89)</xref>â as it enters â <sup>(1)</sup> (in the Î âª 1 limit). Denoting this correction by <italic>Î´</italic>L, from <xref ref-type="disp-formula" rid="eqn56">Eqs. (56)</xref>â<xref ref-type="disp-formula" rid="eqn58">(58)</xref> and <xref ref-type="disp-formula" rid="eqn82">Eq. (82)</xref> we find
<disp-formula id="eqn84">
<graphic xlink:href="564616v3_eqn84.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn85">
<graphic xlink:href="564616v3_eqn85.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Traces of <italic>N</italic> Ã <italic>N</italic> matrix products can be written in terms of traces of products of their blocks as Tr(<italic>AB</italic>) = Î£<sup><italic>ab</italic></sup> tr(<italic>A</italic><sub><italic>ab</italic></sub><italic>B</italic><sub><italic>ab</italic></sub>) where tr denotes trace over <italic>k</italic> Ã <italic>k</italic> blocks. We get a further simplification because <inline-formula><inline-graphic xlink:href="564616v3_inline75.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is diagonal and <inline-formula><inline-graphic xlink:href="564616v3_inline76.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is block-diagonal (due to the diagonality of <inline-formula><inline-graphic xlink:href="564616v3_inline77.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <italic>r</italic>), with the diagonal blocks given by <inline-formula><inline-graphic xlink:href="564616v3_inline78.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. But if <italic>D</italic> is a block-diagonal matrix, then Tr(<italic>DB</italic>) = tr(<italic>D</italic><sub><italic>aa</italic></sub><italic>B</italic><sub><italic>aa</italic></sub>). Thus
<disp-formula id="eqn86">
<graphic xlink:href="564616v3_eqn86.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn87">
<graphic xlink:href="564616v3_eqn87.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn88">
<graphic xlink:href="564616v3_eqn88.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
In other words, the correction to the loss is a sum of terms, each of which depends only on the rates in one of the cluster only. As a result, given the total cluster rates <italic>r</italic><sub><italic>a</italic></sub>, the optimisation of rates of single neurons decouples across clusters. Which means we can optimise the rates in cluster <italic>a</italic> under the constraint that they sum up to <italic>r</italic><sub><italic>a</italic></sub>, to maximise <xref ref-type="disp-formula" rid="eqn88">Eq. (88)</xref>. Using <xref ref-type="disp-formula" rid="eqn89">Eq. (89)</xref> we now express <italic>Î´â</italic><sub><italic>a</italic></sub> explicitly.</p>
</sec>
<sec id="s6c4">
<title>B.3.4 Within-cluster loss to leading order in <italic>Ïµ</italic></title>
<p>From <xref ref-type="disp-formula" rid="eqn79">Eq. (79)</xref>, after dropping the terms with factors of <bold>1</bold>, we have
<disp-formula id="eqn89">
<graphic xlink:href="564616v3_eqn89.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We further ignored the factor <inline-formula><inline-graphic xlink:href="564616v3_inline79.gif" mime-subtype="gif" mimetype="image"/></inline-formula> which multiplies both terms in the loss and is thus irrelevant to the optimisation; we will similarly ignore a factor of <italic>Âµ</italic> in the loss below. The right hand side of the above expression is the covariance of the zero-mean vector (of random variables)
<disp-formula id="eqn90">
<graphic xlink:href="564616v3_eqn90.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
(as can be easily checked: <inline-formula><inline-graphic xlink:href="564616v3_inline80.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p>Since the problem has decoupled across clusters we will now drop the cluster index <italic>a</italic>; in particular we will denote the unperturbed Î©<sub><italic>a</italic></sub> by Î©<sub>0</sub>, and we will also denote the given/fixed total cluster rate, <italic>r</italic><sub><italic>a</italic></sub>, by <italic>R</italic><sub>0</sub>. Substituting the above in <xref ref-type="disp-formula" rid="eqn88">Eq. (88)</xref> and ignoring irrelevant overall prefactors, we find that the single-neuron rates in a cluster with total rate <italic>R</italic><sub>0</sub> are solutions of the following quadratic programming problem:
<disp-formula id="eqn91">
<graphic xlink:href="564616v3_eqn91.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Where
<disp-formula id="eqn92">
<graphic xlink:href="564616v3_eqn92.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
This is equivalent (up to replacement of <italic>R</italic><sub>0</sub> with <italic>Âµ</italic>) to the optimisation problem, <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref>, of the main text. Finally, note the scaling property of this quadratic programming problem: if we scale the cluster rate, <italic>R</italic><sub>0</sub>, by some <italic>Î±</italic>, the solution <inline-formula><inline-graphic xlink:href="564616v3_inline81.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and thus the rates of individual neurons in the cluster, scale by the same factor. Note also that the scaling of ÎÎ©<sup><italic>i</italic></sup> by a constant (within a cluster) factor does not make a difference, and we can make it âdimensionlessâ by dividing it by <italic>Ï</italic><sub>0</sub> if helpful (in fact this amounts to returning the prefactor <inline-formula><inline-graphic xlink:href="564616v3_inline82.gif" mime-subtype="gif" mimetype="image"/></inline-formula> that we dropped after <xref ref-type="disp-formula" rid="eqn89">Eq. (89)</xref>).</p>
</sec>
<sec id="s6c5">
<title>B.3.5 A toy model for a clusterâs tuning curves</title>
<p>Here we will develop a toy model for Cov(Î<bold>Î©</bold>), <italic>i</italic>.<italic>e</italic>., for the statistics of deviations of representational curves of neurons in the same cluster from the unperturbed curve Î©<sub>0</sub>. Assuming a degree of smoothness for Î©<sup><italic>i</italic></sup>(<bold><italic>s</italic></bold>) (as a function on the stimulus space), we will adopt a basis of smooth functions <italic>b</italic><sub><italic>Âµ</italic></sub>(<bold><italic>s</italic></bold>) for 1 â¤<italic>Âµ</italic>â¤ <italic>N</italic><sub><italic>b</italic></sub>, and assume that the log tuning curves are in the span of this set of functions. In other words, we assume
<disp-formula id="eqn93">
<graphic xlink:href="564616v3_eqn93.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
for some vector of coefficients <inline-formula><inline-graphic xlink:href="564616v3_inline83.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Similar to <xref ref-type="disp-formula" rid="eqn51">Eq. (51)</xref> we assume <inline-formula><inline-graphic xlink:href="564616v3_inline84.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (we absorb <italic>Ïµ</italic> into <italic>Î´</italic><bold><italic>w</italic></bold><sup><italic>i</italic></sup>). Expanding the above equation we get
<disp-formula id="ueqn18">
<graphic xlink:href="564616v3_ueqn18.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
or equivalently
<disp-formula id="eqn94">
<graphic xlink:href="564616v3_eqn94.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
It follows that <italic>v</italic><sup><italic>i</italic></sup> = ð¼ [Î©<sub>0</sub><bold><italic>b</italic></bold>] Â· <italic>Î´</italic><bold><italic>w</italic></bold><sup><italic>i</italic></sup>, and thus (from <xref ref-type="disp-formula" rid="eqn92">Eq. (92)</xref>)
<disp-formula id="eqn95">
<graphic xlink:href="564616v3_eqn95.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we defined
<disp-formula id="eqn96">
<graphic xlink:href="564616v3_eqn96.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We thus obtain:
<disp-formula id="eqn97">
<graphic xlink:href="564616v3_eqn97.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn98">
<graphic xlink:href="564616v3_eqn98.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We think of <italic>G</italic> as a metric defining the inner product of the vectors <italic>Î´</italic><bold><italic>w</italic></bold><sub><italic>i</italic></sub>. Note also that given the final comment in the previous subsection, we can replace Î©<sub>0</sub>(<bold><italic>s</italic></bold>) inside the integral expression for <italic>G</italic> with Î©<sub>0</sub>(<bold><italic>s</italic></bold>)<italic>/Ï</italic><sub>0</sub>; this is nice as it means that if it happens that the cluster tuning curve Î©<sub>0</sub> is mostly supported on regions with small stimulus frequency, the above metric does not go to zero. Alternatively, since the scaling of Cov(Î<bold>Î©</bold>) is irrelevant to the within-cluster rate optimisation, we can assume that the expectation in the definition of <italic>G</italic> is taken, not with respect to <italic>P</italic> (<bold><italic>s</italic></bold>), but with respect to the <italic>normalised</italic> measure with density proportional to <inline-formula><inline-graphic xlink:href="564616v3_inline85.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. If we denote expectation under this âtiltedâ measure by <inline-formula><inline-graphic xlink:href="564616v3_inline86.gif" mime-subtype="gif" mimetype="image"/></inline-formula> we can also write:
<disp-formula id="eqn99">
<graphic xlink:href="564616v3_eqn99.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Similarly we could have written
<disp-formula id="eqn100">
<graphic xlink:href="564616v3_eqn100.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
(note that this last expectation is tilted by Î©<sub>0</sub> and not by <inline-formula><inline-graphic xlink:href="564616v3_inline87.gif" mime-subtype="gif" mimetype="image"/></inline-formula>). To summarise, we have found that the quadratic form for our quadratic programming optimisation is given by the Gram matrix of the neuronsâ coefficients <italic>Î´</italic><bold><italic>w</italic></bold><sub><italic>i</italic></sub> with respect to the inner product defined by <italic>G</italic>.</p>
<p>For our minimal toy model, we will assume that the perturbations <italic>Î´</italic><bold><italic>w</italic></bold><sub><italic>i</italic></sub>are isotropic, mean-zero Gaussians, <italic>i</italic>.<italic>e</italic>., <inline-formula><inline-graphic xlink:href="564616v3_inline88.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We will additionally assume that the positive-definite matrix <italic>G</italic> has <italic>D</italic> eigenvalues which are roughly equal, and all significantly larger than the other eigenvalues. In this case, (up to scaling) we can approximate <italic>G</italic> as a projection matrix onto <italic>D</italic>-dimensions. This allows us to generate <inline-formula><inline-graphic xlink:href="564616v3_inline89.gif" mime-subtype="gif" mimetype="image"/></inline-formula> simply as <inline-formula><inline-graphic xlink:href="564616v3_inline90.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where the vectors <inline-formula><inline-graphic xlink:href="564616v3_inline91.gif" mime-subtype="gif" mimetype="image"/></inline-formula> were drawn independently from a <italic>D</italic>-dimensional isotropic Gaussian, <italic>Î´</italic><bold><italic>w</italic></bold><sub><italic>i</italic></sub> â¼ â¼ ð© (0, <italic>I</italic><sub><italic>D</italic></sub>).</p>
</sec>
</sec>
<sec id="s6d">
<title>B.4 First order maximisation of â</title>
<p>In this section we compute the maximiser of â, to first order in the parameter Î, defined by <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref>:
<disp-formula id="ueqn19">
<graphic xlink:href="564616v3_ueqn19.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
To simplify the derivation we will work with â as a function of the rates <italic>r</italic><sub><italic>a</italic></sub> = <italic>g</italic><sub><italic>a</italic></sub><italic>Ï</italic><sub><italic>a</italic></sub>, rather than the gains. Writing <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref> in terms of <italic>r</italic><sub><italic>a</italic></sub>, we have
<disp-formula id="ueqn20">
<graphic xlink:href="564616v3_ueqn20.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We will also introduce the matrix so that
<disp-formula id="eqn101">
<graphic xlink:href="564616v3_eqn101.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
so that
<disp-formula id="eqn102">
<graphic xlink:href="564616v3_eqn102.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Note that
<disp-formula id="eqn103">
<graphic xlink:href="564616v3_eqn103.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>E</italic><sub><italic>a</italic></sub> is the diagonal matrix which is zero everywhere except having 1 as the <italic>a</italic>-th element along the diagonal.</p>
<p>Using <xref ref-type="disp-formula" rid="eqn102">Eq. (102)</xref> and <xref ref-type="disp-formula" rid="eqn103">Eq. (103)</xref>, we obtain that
<disp-formula id="eqn104">
<graphic xlink:href="564616v3_eqn104.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn105">
<graphic xlink:href="564616v3_eqn105.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn106">
<graphic xlink:href="564616v3_eqn106.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn107">
<graphic xlink:href="564616v3_eqn107.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we used the cyclicity of trace and the symmetry of <italic>M</italic> and <italic>W</italic>. Setting the derivative in <xref ref-type="disp-formula" rid="eqn104">Eq. (104)</xref> to zero and rearranging, and defining <italic>Î²</italic> = 2(1 â <italic>Î±</italic>), we obtain
<disp-formula id="eqn108">
<graphic xlink:href="564616v3_eqn108.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We now apply the ansatz
<disp-formula id="eqn109">
<graphic xlink:href="564616v3_eqn109.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Plugging this into <xref ref-type="disp-formula" rid="eqn101">Eq. (101)</xref> we have that
<disp-formula id="ueqn21">
<graphic xlink:href="564616v3_ueqn21.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
which after substitution in the right hand side of <xref ref-type="disp-formula" rid="eqn108">Eq. (108)</xref> yields
<disp-formula id="ueqn22">
<graphic xlink:href="564616v3_ueqn22.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where on the penultimate line we have used the Neumann expansion of [<italic>I</italic> + Î + ðª (Î<sup>2</sup>)]<sup>â1</sup>.</p>
</sec>
<sec id="s6e">
<title>B.5 Aligned noise leads to perfect homeostasis and uniformisation</title>
<p>In this appendix we demonstrate that (for constant coefficient of variation) if <italic>Ï</italic> and <italic>W</italic> are perfectly aligned <italic>i</italic>.<italic>e</italic>.,, <italic>Ï</italic> = <italic>W</italic>, then the optimal rates are all equal and invariant of the environment, <italic>i</italic>.<italic>e</italic>., we have perfect homeostasis and uniformisation.</p>
<p>In App. B.4 we derived an equation for the maximiser of the objective function â (<bold><italic>r</italic></bold>), <xref ref-type="disp-formula" rid="eqn108">Eq. (108)</xref>,
<disp-formula id="eqn110">
<graphic xlink:href="564616v3_eqn110.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We now show that, when <italic>W</italic> = <italic>Ï</italic> and CV is constant, this equation is satisfied by constant <bold><italic>r</italic></bold> = <italic>Ï</italic><bold>1</bold><sub><italic>k</italic></sub> for some <italic>Ï</italic>, and find an equation for <italic>Ï</italic>.
<disp-formula id="ueqn23">
<graphic xlink:href="564616v3_ueqn23.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We will now show that this equation has a solution, provided certain conditions on the parameters hold. We divide into two cases. If <italic>Î²</italic> â¤ 1 then <italic>f</italic> (<italic>Ï</italic>) is strictly increasing and limits to 0 as <italic>Ï</italic> â 0 and infinity as <italic>Ï</italic> â â. By continuity we must have <italic>f</italic> (<italic>Ï</italic>) = <italic>ÂµÎ²</italic> at some point.</p>
<p>If <italic>Ï</italic> &gt; 1 then we still have <italic>f</italic> (<italic>Ï</italic>) â â as <italic>Ï</italic> â â. We now find the minimum of <italic>f</italic>. Taking derivatives, we see that this occurs at
<disp-formula id="eqn111">
<graphic xlink:href="564616v3_eqn111.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
At this point, we have
<disp-formula id="eqn112">
<graphic xlink:href="564616v3_eqn112.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
This is less than <italic>ÂµÎ²</italic> provided that
<disp-formula id="eqn113">
<graphic xlink:href="564616v3_eqn113.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
So provided <xref ref-type="disp-formula" rid="eqn113">Eq. (113)</xref> holds, there is a solution by continuity. Recall that <italic>ÂµÎ²</italic> is approximately the average firing rate of a cluster. Typically we are interested in holding the product of these parameters constant at a value <italic>r</italic>. If we do so, we can take the derivative of the right hand side of <xref ref-type="disp-formula" rid="eqn113">Eq. (113)</xref> in <italic>Î²</italic>, obtaining that this expression is minimised when <italic>Î²</italic> = <italic>r/</italic>(<italic>r</italic> â 1), where it attains a value of <italic>r</italic>â 1. Putting this together, we can say that there exists a uniform, homeostatic solution provided
<disp-formula id="eqn114">
<graphic xlink:href="564616v3_eqn114.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>r</italic> is the product <italic>Î²Âµ</italic>.</p>
</sec>
<sec id="s6f">
<title>B.6 Estimates for <inline-formula><inline-graphic xlink:href="564616v3_inline92.gif" mime-subtype="gif" mimetype="image"/></inline-formula></title>
<p>We define <italic>A</italic><sub><italic>Î³</italic></sub> to be the normalisation constant of a correlation matrix with an eigenspectrum proportional to 1<italic>/n</italic><sup><italic>Î³</italic></sup>. Since a <italic>K</italic> Ã <italic>K</italic> correlation matrix has trace <italic>K</italic>, we have
<disp-formula id="eqn115">
<graphic xlink:href="564616v3_eqn115.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We start with the case <italic>Î³</italic>â  1. Then we have that
<disp-formula id="ueqn24">
<graphic xlink:href="564616v3_ueqn24.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Now, taking the limit as <italic>Î³</italic> â 1, we obtain that
<disp-formula id="eqn116">
<graphic xlink:href="564616v3_eqn116.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Now when <italic>Î³</italic> &lt; 1 and <italic>K</italic> is large, we obtain
<disp-formula id="eqn117">
<graphic xlink:href="564616v3_eqn117.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Lastly, when <italic>Î³</italic> &gt; 1 and <italic>K</italic> is large, we obtain
<disp-formula id="eqn118">
<graphic xlink:href="564616v3_eqn118.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
These approximations allow us to arrive at the following estimates. Firstly, consider tr(<italic>Ï</italic><sup>â1</sup>)<italic>/K</italic>.
<disp-formula id="ueqn25">
<graphic xlink:href="564616v3_ueqn25.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Next, consider the case of aligned noise, when <italic>W</italic> has a <italic>A</italic><sub><italic>Î³</italic></sub><italic>/n</italic><sup><italic>Î³</italic></sup> eigenspectrum and aligned eigenbases with <italic>Ï</italic>. In this case,
<disp-formula id="ueqn26">
<graphic xlink:href="564616v3_ueqn26.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We can then derive the following approximations for tr(<italic>Ï</italic><sup>â1</sup><italic>W</italic>)<italic>/K</italic> using <xref ref-type="disp-formula" rid="eqn116">Eqs. (116)</xref>â<xref ref-type="disp-formula" rid="eqn118">(118)</xref>.
<disp-formula id="ueqn27">
<graphic xlink:href="564616v3_ueqn27.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s6g">
<title>B.7 Optimal homeostatic gains</title>
<p>We now consider the case where we enforce homeostasis on the gains. Prior to taking expectations across environments, our objective, considered as a function of <italic>r</italic><sub><italic>a</italic></sub> = <italic>g</italic><sub><italic>a</italic></sub><italic>Ï</italic><sub><italic>a</italic></sub> is (up to additive constants)
<disp-formula id="ueqn28">
<graphic xlink:href="564616v3_ueqn28.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>P</italic> = CV<italic>Ï</italic>CV. Enforcing homeostasis at the cluster level means setting <inline-formula><inline-graphic xlink:href="564616v3_inline93.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for all clusters. Substituting this in, we obtain the function
<disp-formula id="ueqn29">
<graphic xlink:href="564616v3_ueqn29.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <italic>Î»</italic><sub><italic>n</italic></sub> are the eigenvalues of <italic>Ï</italic><sup>â2</sup><italic>W</italic> <sup>â1</sup><italic>P</italic>. Note that this is only a function of the spectrum, and not of the eigenbasis. Therefore, under the assumption that the spectrum remains fixed across environments, we can drop the need to take expectations, and work with â as a function only of the spectrum. We work under this assumption going forward.</p>
<p>The optimal <italic>Ï</italic>, within this family of approximate solutions, obeys
<disp-formula id="eqn119">
<graphic xlink:href="564616v3_eqn119.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We now demonstrate that, under appropriate conditions, <inline-formula><inline-graphic xlink:href="564616v3_inline94.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.
<disp-formula id="ueqn30">
<graphic xlink:href="564616v3_ueqn30.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Now, under the assumption that (<italic>Î²Âµ</italic>)<sup><italic>Î²</italic></sup><italic>Î»</italic><sub><italic>n</italic></sub> â«1 for each <italic>n</italic>, we can say that <italic>Ï</italic>â <italic>ÂµÎ²</italic> and (<italic>Î²Âµ</italic>)<sup><italic>Î²</italic></sup><italic>Î»</italic><sub><italic>n</italic></sub> â <italic>Ï</italic><sup><italic>Î²</italic></sup><italic>Î»</italic><sub><italic>n</italic></sub> + 1. Substituting this in gives us the required result. Note that this condition is equivalent to the high signal-to-noise ratio condition we have used throughout.</p>
<p>We now consider the special cases discussed in <xref ref-type="sec" rid="s2b">Sec. 2.2</xref> in which more precise solutions can be obtained. Note that in all three cases the analytics correspond to idealisations of the numerical simulations we actually perform in <xref ref-type="sec" rid="s2e">Sec. 2.6</xref>.</p>
<sec id="s6g1">
<title>Uncorrelated power-law noise</title>
<p>In the first special case under consideration, <italic>W</italic> = <italic>I</italic><sub><italic>K</italic></sub>, CV<sup>2</sup> is constant, and <italic>Ï</italic> has approximately a <italic>A</italic><sub>1</sub><italic>/n</italic> eigenspectrum, where <italic>A</italic><sub>1</sub> is chosen to normalise the trace of <italic>Ï</italic> to be equal to <italic>K, i</italic>.<italic>e</italic>.,
<disp-formula id="ueqn31">
<graphic xlink:href="564616v3_ueqn31.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The spectrum of <italic>Ï</italic><sup>â2</sup><italic>W</italic> <sup>â1</sup><italic>P</italic> is therefore <italic>Î»</italic><sub><italic>n</italic></sub> = <italic>b/n</italic> where
<disp-formula id="eqn120">
<graphic xlink:href="564616v3_eqn120.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
In this case, (defining <italic>u</italic> = <italic>n/K</italic>) we can make the following approximation to the right hand side of <xref ref-type="disp-formula" rid="eqn119">Eq. (119)</xref>:
<disp-formula id="ueqn32">
<graphic xlink:href="564616v3_ueqn32.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
This gives the new equation
<disp-formula id="eqn121">
<graphic xlink:href="564616v3_eqn121.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Approximating <italic>A</italic><sub>1</sub> â <italic>K/</italic> ln(<italic>K</italic>) (see App. B.6) and using <xref ref-type="disp-formula" rid="eqn120">Eq. (120)</xref>, we obtain that
<disp-formula id="eqn122">
<graphic xlink:href="564616v3_eqn122.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Substituting <xref ref-type="disp-formula" rid="eqn122">Eq. (122)</xref> into <xref ref-type="disp-formula" rid="eqn121">Eq. (121)</xref> gives us
<disp-formula id="eqn123">
<graphic xlink:href="564616v3_eqn123.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s6g2">
<title>Aligned noise</title>
<p>In the aligned noise case, we approximate <italic>Ï</italic> and <italic>W</italic> as having the same eigenbasis. <italic>Ï</italic> still has an eigenspectrum of <italic>A</italic><sub>1</sub><italic>/n</italic>, and we take <italic>W</italic> to have an eigenspectrum of <italic>A</italic><sub><italic>Î³</italic></sub><italic>/n</italic><sup><italic>Î³</italic></sup> where <italic>A</italic><sub><italic>Î³</italic></sub> normalises the trace of <italic>W</italic> to be equal to <italic>K</italic>,
<disp-formula id="ueqn33">
<graphic xlink:href="564616v3_ueqn33.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The matrix <italic>Ï</italic><sup>2</sup><italic>W</italic> <sup>â1</sup><italic>P</italic> therefore has eigenspectrum <italic>bn</italic><sup><italic>Î³</italic>â1</sup> where <italic>b</italic> = <italic>Ï</italic><sup>â2</sup><italic>A</italic><sub>1</sub>CV<sup>2</sup><italic>/A</italic><sub><italic>Î³</italic></sub>. Inserting this into <xref ref-type="disp-formula" rid="eqn119">Eq. (119)</xref> gives us
<disp-formula id="eqn124">
<graphic xlink:href="564616v3_eqn124.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
See App. B.6 for estimates of <italic>A</italic><sub><italic>Î³</italic></sub>.</p>
</sec>
<sec id="s6g3">
<title>Constant correlation noise</title>
<p>The next special case occurs when <italic>Î²</italic> = 1, and <italic>W</italic> = (1â<italic>p</italic>)<italic>I</italic><sup><italic>K</italic></sup> + <italic>p</italic><bold>11</bold><sup>T</sup>. Using the Sherman-Morrison formula, we obtain
<disp-formula id="ueqn34">
<graphic xlink:href="564616v3_ueqn34.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Since <inline-formula><inline-graphic xlink:href="564616v3_inline95.gif" mime-subtype="gif" mimetype="image"/></inline-formula> we neglect this term and approximate <inline-formula><inline-graphic xlink:href="564616v3_inline96.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. This therefore has the same effect as making the replacement <italic>Ï</italic><sup>2</sup> 1â (1 â <italic>p</italic>)<italic>Ï</italic><sup>2</sup>. Substituting this into <xref ref-type="disp-formula" rid="eqn123">Eq. (123)</xref>, and using <italic>Î²</italic> = 1, we obtain
<disp-formula id="eqn125">
<graphic xlink:href="564616v3_eqn125.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We define
<disp-formula id="ueqn35">
<graphic xlink:href="564616v3_ueqn35.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and rearrange to get
<disp-formula id="ueqn36">
<graphic xlink:href="564616v3_ueqn36.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
</sec>
<sec id="s6h">
<title>B.8 Homeostatic propagation</title>
<p>In this appendix, we derive how the weights between neural populations must change in order for homeostasis to be propagated between them. We work in a linear rate model.</p>
<p>We start by considering a downstream population of tuning curves <inline-formula><inline-graphic xlink:href="564616v3_inline97.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for <italic>m</italic> = 1, â¦, <italic>M</italic>, with <italic>W</italic><sub><italic>ma</italic></sub> the synaptic weight from neuron <italic>a</italic> to neuron <italic>m</italic> in the downstream population. The tuning curve of neuron <italic>m</italic> is
<disp-formula id="eqn126">
<graphic xlink:href="564616v3_eqn126.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We assume the downstream population has representational curves <inline-formula><inline-graphic xlink:href="564616v3_inline98.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for <italic>m</italic> = 1, â¦, <italic>M</italic> given by
<disp-formula id="eqn127">
<graphic xlink:href="564616v3_eqn127.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
If the downstream population is also implementing homeostatic coding, we know that
<disp-formula id="ueqn37">
<graphic xlink:href="564616v3_ueqn37.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="564616v3_inline99.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. But then
<disp-formula id="ueqn38">
<graphic xlink:href="564616v3_ueqn38.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Substituting this in, we get
<disp-formula id="eqn128">
<graphic xlink:href="564616v3_eqn128.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Comparing coefficients, we can see that
<disp-formula id="ueqn39">
<graphic xlink:href="564616v3_ueqn39.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
as claimed.</p>
</sec>
<sec id="s6i">
<title>B.9 Hierarchical Bayes-ratio coding</title>
<p>In this appendix, we calculate synaptic weights for propagation of Bayes-ratio coding between populations. We start with a generative model <bold><italic>z</italic></bold><sup>(2)</sup> â <bold><italic>z</italic></bold><sup>(1)</sup> â <bold><italic>s</italic></bold>. The downstream representational curves are
<disp-formula id="ueqn40">
<graphic xlink:href="564616v3_ueqn40.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We recall that the synaptic weights <italic>W</italic><sub><italic>ma</italic></sub> are given by the <xref ref-type="disp-formula" rid="eqn25">Eq. (25)</xref>, repeated here:
<disp-formula id="ueqn41">
<graphic xlink:href="564616v3_ueqn41.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>w</italic><sub><italic>ma</italic></sub> are the coefficients of the expansion <inline-formula><inline-graphic xlink:href="564616v3_inline100.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We start by calculating <italic>w</italic><sub><italic>ma</italic></sub>.
<disp-formula id="ueqn42">
<graphic xlink:href="564616v3_ueqn42.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We plug this into our formula, and use additionally that <inline-formula><inline-graphic xlink:href="564616v3_inline101.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. This gives us
<disp-formula id="ueqn43">
<graphic xlink:href="564616v3_ueqn43.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
which gives the result as required.</p>
</sec>
<sec id="s6j">
<title>B.10 Stimulus specific adaptation for non-ideal-observer models</title>
<p>In this appendix, we show how discrepancies between the internal model and the external environment lead to simultaneous stimulus specific and neuron specific adaptation effects in a homeostatic DDC code. We begin with the special case of a Bayes-ratio code, which is a DDC in which the kernel functions are delta functions (as discussed in <xref ref-type="sec" rid="s2i">Sec. 2.9</xref>). In a Bayes-ratio code, representational curves are given by
<disp-formula id="eqn129">
<graphic xlink:href="564616v3_eqn129.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where Î  is the posterior distribution under the internal generative model. We consider the case of a non-ideal-observer generative model, in which case there will be a discrepancy between the marginal stimulus distribution predicted by the model, <italic>P</italic><sup><italic>I</italic></sup> (<bold><italic>s</italic></bold>) and the external environment stimulus marginal <italic>P</italic><sup><italic>E</italic></sup>(<bold><italic>s</italic></bold>). We reason as follows:
<disp-formula id="eqn130">
<graphic xlink:href="564616v3_eqn130.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Where
<disp-formula id="eqn131">
<graphic xlink:href="564616v3_eqn131.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
According to homeostatic coding, the tuning curve is given by
<disp-formula id="eqn132">
<graphic xlink:href="564616v3_eqn132.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>Ï</italic><sub><italic>a</italic></sub> is the temporal average of the representational curve, or equivalently, the expectation of Î©<sub><italic>a</italic></sub>(<bold><italic>s</italic></bold>) under the <italic>true</italic> environmental stimulus distribution. Thus
<disp-formula id="eqn133">
<graphic xlink:href="564616v3_eqn133.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we defined
<disp-formula id="eqn134">
<graphic xlink:href="564616v3_eqn134.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Notice that when <italic>P</italic><sup><italic>E</italic></sup> = <italic>P</italic><sup><italic>I</italic></sup>, as would be the case for an ideal-observer, we will have <italic>F</italic><sub><italic>a</italic></sub> = 1, due to the normalization of <italic>f</italic> (<bold><italic>s</italic></bold>|<bold><italic>z</italic></bold>). Substituting <xref ref-type="disp-formula" rid="eqn130">Eq. (130)</xref> and <xref ref-type="disp-formula" rid="eqn133">Eq. (133)</xref> into <xref ref-type="disp-formula" rid="eqn132">Eq. (132)</xref> we obtain
<disp-formula id="eqn135">
<graphic xlink:href="564616v3_eqn135.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
as claimed in <xref ref-type="disp-formula" rid="eqn32">Eq. (32)</xref>.</p>
<p>We now consider the more general case of a DDC code. We define
<disp-formula id="eqn136">
<graphic xlink:href="564616v3_eqn136.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
so that <italic>F</italic><sub><italic>a</italic></sub> in the above is equal to <italic>F</italic> (<bold><italic>z</italic></bold><sub><italic>a</italic></sub>). An analogous derivation yields:
<disp-formula id="eqn137">
<graphic xlink:href="564616v3_eqn137.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We can see that in this case there is no clean separation of neuron specific and stimulus specific factors. In particular, the generalization of <xref ref-type="disp-formula" rid="eqn34">Eq. (34)</xref> takes the form:
<disp-formula id="eqn138">
<graphic xlink:href="564616v3_eqn138.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
In the limit as the width of the kernels becomes small, the integrals involving kernels collapse to sampling at a single point, as occurs with the Bayes-ratio code. When this happens, we get full separation to stimulus and neuron specific effects; otherwise these effects are mixed together, with more mixing occurring the larger the width of the kernels. However, when the kernels are unimodal and sufficiently narrow, we would expect an approximate factorization of the effect of adaptation into a stimulus-specific and a neuron-specific suppression factor.</p>
</sec>
</sec>
<ack>
<label>5</label>
<title>Acknowledgements</title>
<p>We thank MÃ¡tÃ© Lengyel for helpful discussions and comments on the manuscript. EY was supported by the UKRI Engineering and Physical Sciences Research Council Doctoral Training Program grant EP/T517847/1. YA was supported by UKRI Biotechnology and Biological Sciences Research Council research grant BB/X013235/1.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Atick</surname>, <given-names>J. J.</given-names></string-name> and <string-name><surname>Redlich</surname>, <given-names>A. N.</given-names></string-name></person-group> (<year>1990</year>). <article-title>Towards a Theory of Early Visual Processing</article-title>. <source>Neural Computation</source>, <volume>2</volume>(<issue>3</issue>):<fpage>308</fpage>â<lpage>320</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Attneave</surname>, <given-names>F.</given-names></string-name></person-group> (<year>1954</year>). <article-title>Some informational aspects of visual perception</article-title>. <source>Psychological Review</source>, <volume>61</volume>(<issue>3</issue>):<fpage>183</fpage>â<lpage>193</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Barlow</surname>, <given-names>H. B.</given-names></string-name></person-group> (<year>2012</year>). <chapter-title>Possible Principles Underlying the Transformations of Sensory Messages</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Rosenblith</surname>, <given-names>W. A.</given-names></string-name></person-group>, editor, <source>Sensory Communication</source>, pages <fpage>216</fpage>â<lpage>234</lpage>. <publisher-name>The MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Beck</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ma</surname>, <given-names>W. J.</given-names></string-name>, <string-name><surname>Latham</surname>, <given-names>P. E.</given-names></string-name>, and <string-name><surname>Pouget</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2007</year>). <chapter-title>Probabilistic population codes and the exponential family of distributions</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Cisek</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Drew</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Kalaska</surname>, <given-names>J. F.</given-names></string-name></person-group>, editors, <source>Progress in Brain Research, volume 165 of Computational Neuroscience: Theoretical Insights into Brain Function</source>, pages <fpage>509</fpage>â<lpage>519</lpage>. <publisher-name>Elsevier</publisher-name>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benucci</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Saleem</surname>, <given-names>A. B.</given-names></string-name>, and <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Adaptation maintains population homeostasis in primary visual cortex</article-title>. <source>Nature Neuroscience</source>, <volume>16</volume>(<issue>6</issue>):<fpage>724</fpage>â<lpage>729</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rotermund</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Pawelzik</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Optimal short-term population coding: when Fisher information fails</article-title>. <source>Neural Computation</source>, <volume>14</volume>(<issue>10</issue>):<fpage>2317</fpage>â<lpage>2351</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brunel</surname>, <given-names>N.</given-names></string-name> and <string-name><surname>Nadal</surname>, <given-names>J.-P.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Mutual Information, Fisher Information, and Population Coding</article-title>. <source>Neural Computation</source>, <volume>10</volume>(<issue>7</issue>):<fpage>1731</fpage>â<lpage>1757</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>BuzsÃ¡ki</surname>, <given-names>G.</given-names></string-name> and <string-name><surname>Mizuseki</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2014</year>). <article-title>The log-dynamic brain: how skewed distributions affect network operations</article-title>. <source>Nature Reviews. Neuroscience</source>, <volume>15</volume>(<issue>4</issue>):<fpage>264</fpage>â<lpage>278</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Heeger</surname>, <given-names>D. J.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Normalization as a canonical neural computation</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>13</volume>(<issue>1</issue>):<fpage>51</fpage>â<lpage>62</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clifford</surname>, <given-names>C. W. G.</given-names></string-name>, <string-name><surname>Webster</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Stanley</surname>, <given-names>G. B.</given-names></string-name>, <string-name><surname>Stocker</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Kohn</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sharpee</surname>, <given-names>T. O.</given-names></string-name>, and <string-name><surname>Schwartz</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Visual adaptation: Neural, psychological and computational aspects</article-title>. <source>Vision Research</source>, <volume>47</volume>(<issue>25</issue>):<fpage>3125</fpage>â<lpage>3131</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desai</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Homeostatic plasticity in the CNS: Synaptic and intrinsic forms</article-title>. <source>Journal of physiology, Paris</source>, <volume>97</volume>:<fpage>391</fpage>â<lpage>402</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Diamond</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Boyd</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2016</year>). <article-title>CVXPY: A Python-embedded modeling language for convex optimization</article-title>. <source>Journal of Machine Learning Research</source>, <volume>17</volume>(<issue>83</issue>):<fpage>1</fpage>â<lpage>5</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dragoi</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Sharma</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>E. K.</given-names></string-name>, and <string-name><surname>Sur</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Dynamics of neuronal sensitivity in visual cortex and local feature discrimination</article-title>. <source>Nature Neuroscience</source>, <volume>5</volume>(<issue>9</issue>):<fpage>883</fpage>â<lpage>891</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dragoi</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Sharma</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Sur</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Adaptation-induced plasticity of orientation tuning in adult visual cortex</article-title>. <source>Neuron</source>, <volume>28</volume>(<issue>1</issue>):<fpage>287</fpage>â<lpage>298</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ganguli</surname>, <given-names>D.</given-names></string-name> and <string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Efficient Sensory Encoding and Bayesian Inference with Heterogeneous Neural Populations</article-title>. <source>Neural Computation</source>, <volume>26</volume>(<issue>10</issue>):<fpage>2103</fpage>â<lpage>2134</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gershon</surname>, <given-names>E. D.</given-names></string-name>, <string-name><surname>Wiener</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Latham</surname>, <given-names>P. E.</given-names></string-name>, and <string-name><surname>Richmond</surname>, <given-names>B. J.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Coding Strategies in Monkey V1 and Inferior Temporal Cortices</article-title>. <source>Journal of Neurophysiology</source>, <volume>79</volume>(<issue>3</issue>):<fpage>1135</fpage>â<lpage>1144</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Geurts</surname>, <given-names>L. S.</given-names></string-name>, <string-name><surname>Cooke</surname>, <given-names>J. R. H.</given-names></string-name>, <string-name><surname>van Bergen</surname>, <given-names>R. S.</given-names></string-name>, and <string-name><surname>Jehee</surname>, <given-names>J. F. M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Subjective confidence reflects representation of Bayesian probability in cortex</article-title>. <source>Nature Human Behaviour</source>, <volume>6</volume>(<issue>2</issue>):<fpage>294</fpage>â<lpage>305</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ghisovan</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Nemri</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Shumikhina</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Molotchnikoff</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Long adaptation reveals mostly attractive shifts of orientation tuning in cat primary visual cortex</article-title>. <source>Neuroscience</source>, <volume>164</volume>(<issue>3</issue>):<fpage>1274</fpage>â<lpage>1283</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goris</surname>, <given-names>R. L. T.</given-names></string-name>, <string-name><surname>Movshon</surname>, <given-names>J. A.</given-names></string-name>, and <string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Partitioning neuronal variability</article-title>. <source>Nature Neuroscience</source>, <volume>17</volume>(<issue>6</issue>):<fpage>858</fpage>â<lpage>865</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greenberg</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Houweling</surname>, <given-names>A. R.</given-names></string-name>, and <string-name><surname>Kerr</surname>, <given-names>J. N. D.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Population imaging of ongoing neuronal activity in the visual cortex of awake rats</article-title>. <source>Nature Neuroscience</source>, <volume>11</volume>(<issue>7</issue>):<fpage>749</fpage>â<lpage>751</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hengen</surname>, <given-names>K. B.</given-names></string-name>, <string-name><surname>Lambo</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Van Hooser</surname>, <given-names>S. D.</given-names></string-name>, <string-name><surname>Katz</surname>, <given-names>D. B.</given-names></string-name>, and <string-name><surname>Turrigiano</surname>, <given-names>G. G.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Firing Rate Homeostasis in Visual Cortex of Freely Behaving Rodents</article-title>. <source>Neuron</source>, <volume>80</volume>(<issue>2</issue>):<fpage>335</fpage>â<lpage>342</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kanitscheider</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Coen-Cagli</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Pouget</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Origin of information-limiting noise correlations</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>112</volume>(<issue>50</issue>):<fpage>E6973</fpage>â<lpage>6982</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keck</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G. B.</given-names></string-name>, <string-name><surname>Jacobsen</surname>, <given-names>R. I.</given-names></string-name>, <string-name><surname>Eysel</surname>, <given-names>U. T.</given-names></string-name>, <string-name><surname>Bonhoeffer</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>HÃ¼bener</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Synaptic Scaling and Homeostatic Plasticity in the Mouse Visual Cortex In Vivo</article-title>. <source>Neuron</source>, <volume>80</volume>(<issue>2</issue>):<fpage>327</fpage>â<lpage>334</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kersten</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Mamassian</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Yuille</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Object perception as Bayesian inference</article-title>. <source>Annual Review of Psychology</source>, <volume>5</volume>:<fpage>271</fpage>â<lpage>304</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kohn</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Visual adaptation: physiology, mechanisms, and functional benefits</article-title>. <source>Journal of Neurophysiology</source>, <volume>97</volume>(<issue>5</issue>):<fpage>3155</fpage>â<lpage>3164</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koyama</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2015</year>). <article-title>On the Spike Train Variability Characterized by Variance-to-Mean Power Relationship</article-title>. <source>Neural Computation</source>, <volume>27</volume>(<issue>7</issue>):<fpage>1530</fpage>â<lpage>1548</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laughlin</surname>, <given-names>S.</given-names></string-name></person-group> (<year>1981</year>). <article-title>A Simple Coding Procedure Enhances a Neuronâs Information Capacity</article-title>. <source>Zeitschrift fÃ¼r Naturforschung C</source>, <volume>36</volume>(<issue>9-10</issue>):<fpage>910</fpage>â<lpage>912</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lennie</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2003</year>). <article-title>The Cost of Cortical Computation</article-title>. <source>Current Biology</source>, <volume>13</volume>(<issue>6</issue>):<fpage>493</fpage>â<lpage>497</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Levy</surname>, <given-names>W.</given-names></string-name> and <string-name><surname>Baxter</surname>, <given-names>R.</given-names></string-name></person-group> (<year>1996</year>). <article-title>Energy Efficient Neural Codes</article-title>. <source>Neural computation</source>, <volume>8</volume>:<fpage>531</fpage>â<lpage>43</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Linsker</surname>, <given-names>R.</given-names></string-name></person-group> (<year>1988</year>). <article-title>Self-organization in a perceptual network</article-title>. <source>Computer</source>, <volume>21</volume>(<issue>3</issue>):<fpage>105</fpage>â<lpage>117</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maffei</surname>, <given-names>A.</given-names></string-name> and <string-name><surname>Turrigiano</surname>, <given-names>G. G.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Multiple Modes of Network Homeostasis in Visual Cortical Layer 2/3</article-title>. <source>The Journal of Neuroscience</source>, <volume>28</volume>(<issue>17</issue>):<fpage>4377</fpage>â<lpage>4384</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marder</surname>, <given-names>E.</given-names></string-name> and <string-name><surname>Prinz</surname>, <given-names>A. A.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Current compensation in neuronal homeostasis</article-title>. <source>Neuron</source>, <volume>37</volume>(<issue>1</issue>):<fpage>2</fpage>â<lpage>4</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moreno-Bote</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Beck</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kanitscheider</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Pitkow</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Latham</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Pouget</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Information-limiting correlations</article-title>. <source>Nature Neuroscience</source>, <volume>17</volume>(<issue>10</issue>):<fpage>1410</fpage>â<lpage>1417</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moshitch</surname>, <given-names>D.</given-names></string-name> and <string-name><surname>Nelken</surname>, <given-names>I.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Using Tweedie distributions for fitting spike count data</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>225</volume>:<fpage>13</fpage>â<lpage>28</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Movshon</surname>, <given-names>J. A.</given-names></string-name> and <string-name><surname>Lennie</surname>, <given-names>P.</given-names></string-name></person-group> (<year>1979</year>). <article-title>Pattern-selective adaptation in visual cortical neurones</article-title>. <source>Nature</source>, <volume>278</volume>(<issue>5707</issue>):<fpage>850</fpage>â<lpage>852</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>MÃ¼ller</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Metha</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>Krauskopf</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Lennie</surname>, <given-names>P.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Rapid adaptation in visual cortex to the structure of images</article-title>. <source>Science</source>, <volume>285</volume>(<issue>5432</issue>):<fpage>1405</fpage>â<lpage>1408</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nadal</surname>, <given-names>J.-P.</given-names></string-name> and <string-name><surname>Parga</surname>, <given-names>N.</given-names></string-name></person-group> (<year>1994</year>). <article-title>Nonlinear neurons in the low-noise limit: a factorial code maximizes information transfer</article-title>. <source>Network: Computation in Neural Systems</source>, <volume>5</volume>(<issue>4</issue>):<fpage>565</fpage>â<lpage>581</lpage>. <pub-id pub-id-type="doi">10.1088/0954-898X_5_4_008</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Nadal</surname>, <given-names>J.-P.</given-names></string-name> and <string-name><surname>Parga</surname>, <given-names>N.</given-names></string-name></person-group> (<year>1999</year>). <chapter-title>Sensory coding: information maximization and redundancy reduction</chapter-title>. In <source>Neuronal Information Processing, Volume 7 of Series in Mathematical Biology and Medicine</source>, pages <fpage>164</fpage>â<lpage>171</lpage>. <publisher-name>WORLD SCIENTIFIC</publisher-name>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parker</surname>, <given-names>P. R. L.</given-names></string-name>, <string-name><surname>Abe</surname>, <given-names>E. T. T.</given-names></string-name>, <string-name><surname>Leonard</surname>, <given-names>E. S. P.</given-names></string-name>, <string-name><surname>Martins</surname>, <given-names>D. M.</given-names></string-name>, and <string-name><surname>Niell</surname>, <given-names>C. M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Joint coding of visual input and eye/head position in V1 of freely moving mice</article-title>. <source>Neuron</source>, <volume>110</volume>(<issue>23</issue>):<fpage>3897</fpage>â<lpage>3906.</lpage> </mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rumyantsev</surname>, <given-names>O. I.</given-names></string-name>, <string-name><surname>Lecoq</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Hernandez</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Savall</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chrapkiewicz</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zeng</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ganguli</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Schnitzer</surname>, <given-names>M. J.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Fundamental bounds on the fidelity of sensory cortical coding</article-title>. <source>Nature</source>, <volume>580</volume>(<issue>7801</issue>):<fpage>100</fpage>â<lpage>105</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sanzeni</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Palmigiano</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>T. H.</given-names></string-name>, <string-name><surname>Luo</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Nassi</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Reynolds</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Histed</surname>, <given-names>M. H.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>K. D.</given-names></string-name>, and <string-name><surname>Brunel</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Mechanisms underlying reshuffling of visual responses by optogenetic stimulation in mice and monkeys</article-title>. <source>Neuron</source>, <volume>111</volume>(<issue>24</issue>):<fpage>4102</fpage>â<lpage>4115.</lpage> </mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwartz</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Hsu</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Space and time in visual context</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>8</volume>(<issue>7</issue>):<fpage>522</fpage>â<lpage>535</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shadlen</surname>, <given-names>M. N.</given-names></string-name> and <string-name><surname>Newsome</surname>, <given-names>W. T.</given-names></string-name></person-group> (<year>1998</year>). <article-title>The Variable Discharge of Cortical Neurons: Implications for Connectivity, Computation, and Information Coding</article-title>. <source>The Journal of Neuroscience</source>, <volume>18</volume>(<issue>10</issue>):<fpage>3870</fpage>â<lpage>3896</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name> and <string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Natural Image Statistics and Neural Representation</article-title>. <source>Annual Review of Neuroscience</source>, <volume>24</volume>(<issue>1</issue>):<fpage>1193</fpage>â<lpage>1216</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Slomowitz</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Styr</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Vertkin</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Milshtein-Parush</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Nelken</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Slutsky</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Slutsky</surname>, <given-names>I.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Interplay between population firing stability and single neuron dynamics in hippocampal networks</article-title>. <source>eLife</source>, <volume>4</volume>:<elocation-id>e04378</elocation-id>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Snow</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Coen-Cagli</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Schwartz</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Specificity and timescales of cortical adaptation as inferences about natural movie statistics</article-title>. <source>Journal of Vision</source>, <volume>16</volume>(<issue>13</issue>):<fpage>1</fpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Solomon</surname>, <given-names>S. G.</given-names></string-name> and <string-name><surname>Kohn</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Moving sensory adaptation beyond suppressive effects in single neurons</article-title>. <source>Current biology: CB</source>, <volume>24</volume>(<issue>20</issue>):<fpage>R1012</fpage>â<lpage>1022</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stocker</surname>, <given-names>A. A.</given-names></string-name> and <string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Noise characteristics and prior expectations in human visual speed perception</article-title>. <source>Nature Neuroscience</source>, <volume>9</volume>(<issue>4</issue>):<fpage>578</fpage>â<lpage>585</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stringer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name></person-group> (<year>2019</year>). <article-title>High-dimensional geometry of population responses in visual cortex</article-title>. <source>Nature</source>, <volume>571</volume>(<issue>7765</issue>):<fpage>361</fpage>â<lpage>365</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Szuts</surname>, <given-names>T. A.</given-names></string-name>, <string-name><surname>Fadeyev</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Kachiguine</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sher</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Grivich</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>AgrochÃ£o</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hottowy</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Dabrowski</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Lubenov</surname>, <given-names>E. V.</given-names></string-name>, <string-name><surname>Siapas</surname>, <given-names>A. G.</given-names></string-name>, <string-name><surname>Uchida</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Litke</surname>, <given-names>A. M.</given-names></string-name>, and <string-name><surname>Meister</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2011</year>). <article-title>A wireless multi-channel neural amplifier for freely moving animals</article-title>. <source>Nature Neuroscience</source>, <volume>14</volume>(<issue>2</issue>):<fpage>263</fpage>â<lpage>269</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Torrado Pacheco</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Tilden</surname>, <given-names>E. I.</given-names></string-name>, <string-name><surname>Grutzner</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Lane</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hengen</surname>, <given-names>K. B.</given-names></string-name>, <string-name><surname>Gjorgjieva</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Turrigiano</surname>, <given-names>G. G.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Rapid and active stabilization of visual cortical firing rates across lightâdark transitions</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>116</volume>(<issue>36</issue>):<fpage>18068</fpage>â<lpage>18077</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turrigiano</surname>, <given-names>G. G.</given-names></string-name></person-group> (<year>2008</year>). <article-title>The self-tuning neuron: synaptic scaling of excitatory synapses</article-title>. <source>Cell</source>, <volume>135</volume>(<issue>3</issue>):<fpage>422</fpage>â<lpage>435</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turrigiano</surname>, <given-names>G. G.</given-names></string-name>, <string-name><surname>Leslie</surname>, <given-names>K. R.</given-names></string-name>, <string-name><surname>Desai</surname>, <given-names>N. S.</given-names></string-name>, <string-name><surname>Rutherford</surname>, <given-names>L. C.</given-names></string-name>, and <string-name><surname>Nelson</surname>, <given-names>S. B.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Activity-dependent scaling of quantal amplitude in neocortical neurons</article-title>. <source>Nature</source>, <volume>391</volume>(<issue>6670</issue>):<fpage>892</fpage>â<lpage>896</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turrigiano</surname>, <given-names>G. G.</given-names></string-name> and <string-name><surname>Nelson</surname>, <given-names>S. B.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Homeostatic plasticity in the developing nervous system</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>5</volume>(<issue>2</issue>):<fpage>97</fpage>â<lpage>107</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Bergen</surname>, <given-names>R. S.</given-names></string-name>, <string-name><surname>Ji Ma</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Pratte</surname>, <given-names>M. S.</given-names></string-name>, and <string-name><surname>Jehee</surname>, <given-names>J. F. M.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Sensory uncertainty decoded from visual cortex predicts behavior</article-title>. <source>Nature Neuroscience</source>, <volume>18</volume>(<issue>12</issue>):<fpage>1728</fpage>â<lpage>1730</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Vertes</surname>, <given-names>E.</given-names></string-name> and <string-name><surname>Sahani</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Flexible and accurate inference and learning for deep generative models</article-title>. <source>arXiv</source>:<pub-id pub-id-type="doi">10.48550/arXiv.1805.11051</pub-id>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name></person-group> (<year>1995</year>). <source>Foundations of vision</source>. <publisher-name>Sinauer Associates</publisher-name>, <publisher-loc>Sunderland, MA, US</publisher-loc>. Pages: xvi, <fpage>476</fpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname>, <given-names>X.-X.</given-names></string-name> and <string-name><surname>Stocker</surname>, <given-names>A. A.</given-names></string-name></person-group> (<year>2015</year>). <article-title>A Bayesian observer model constrained by efficient coding can explain âanti-Bayesianâ percepts</article-title>. <source>Nature Neuroscience</source>, <volume>18</volume>(<issue>10</issue>):<fpage>1509</fpage>â<lpage>1517</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname>, <given-names>X.-X.</given-names></string-name> and <string-name><surname>Stocker</surname>, <given-names>A. A.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Mutual Information, Fisher Information, and Efficient Coding</article-title>. <source>Neural Computation</source>, <volume>28</volume>(<issue>2</issue>):<fpage>305</fpage>â<lpage>326</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Westrick</surname>, <given-names>Z. M.</given-names></string-name>, <string-name><surname>Heeger</surname>, <given-names>D. J.</given-names></string-name>, and <string-name><surname>Landy</surname>, <given-names>M. S.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Pattern Adaptation and Normalization Reweighting</article-title>. <source>Journal of Neuroscience</source>, <volume>36</volume>(<issue>38</issue>):<fpage>9805</fpage>â<lpage>9816</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yarrow</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Challis</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>SeriÃ¨s</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Fisher and Shannon Information in Finite Neural Populations</article-title>. <source>Neural Computation</source>, <volume>24</volume>(<issue>7</issue>):<fpage>1740</fpage>â<lpage>1780</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104746.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Sharpee</surname>
<given-names>Tatyana O</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Salk Institute for Biological Studies</institution>
</institution-wrap>
<city>La Jolla</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This work derives a <bold>valuable</bold> general theory unifying theories of efficient information transmission in the brain with population homeostasis. The general theory provides an explanation for firing rate homeostasis at the level of neural clusters with firing rate heterogeneity within clusters. Applying this theory to the primary visual cortex, the authors present <bold>solid</bold> evidence that accounts for stimulus-specific and neuron-specific adaptation.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104746.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This work derives a general theory of optimal gain modulation in neural populations. It demonstrates that population homeostasis is a consequence of optimal modulation for information maximization with noisy neurons. The developed theory is then applied to the distributed distributional code (DDC) model of the primary visual cortex to demonstrate that homeostatic DDCs can account for stimulus-specific adaptation.</p>
<p>What I consider to be the most important contribution of this work is the unification of efficient information transmission in neural populations with population homeostasis. The former is an established theoretical framework, and the latter is a well-known empirical phenomenon - the relationship between them has never been fully clarified. I consider this work to be an interesting and relevant step in that direction.</p>
<p>The theory proposed in the paper is rigorous and the analysis is thorough. The manuscript begins with a general mathematical setting to identify normative solutions to the problem of information maximization. It then gradually builds towards questions about approximate solutions, neural implementation and plausibility of these solutions, applications of the theory to specific models of neural computation (DDC), and finally comparisons to experimental data in V1. Such a connection of different levels of abstraction is an obvious strength of this work.</p>
<p>Overall I find this contribution interesting and assess it positively. At the same time, I have three major points of criticism, which I believe the authors should address. I list them below, followed by a number of more specific comments and feedback.</p>
<p>Major comments:</p>
<p>(1) Interpretation of key results and relationship between different parts of the manuscript. The manuscript begins with an information-transmission ansatz which is described as &quot;independent of the computational goal&quot; (e.g. p. 17). While information theory indeed is not concerned with what quantity is being encoded (e.g. whether it is sensory periphery or hippocampus), the goal of the studied system is to *transmit* the largest amount of bits about the input in the presence of noise. In my view, this does not make the proposed framework &quot;independent of the computational goal&quot;. Furthermore, the derived theory is then applied to a DDC model which proposes a very specific solution to inference problems. The relationship between information transmission and inference is deep and nuanced. Because the writing is very dense, it is quite hard to understand how the information transmission framework developed in the first part applies to the inference problem. How does the neural coding diagram in Figure 3 map onto the inference diagram in Figure 10? How does the problem of information transmission under constraints from the first part of the manuscript become an inference problem with DDCs? I am certain that authors have good answers to these questions - but they should be explained much better.</p>
<p>(2) Clarity of writing for an interdisciplinary audience. I do not believe that in its current form, the manuscript is accessible to a broader, interdisciplinary audience such as eLife readers. The writing is very dense and technical, which I believe unnecessarily obscures the key results of this study.</p>
<p>(3) Positioning within the context of the field and relationship to prior work. While the proposed theory is interesting and timely, the manuscript omits multiple closely related results which in my view should be discussed in relationship to the current work. In particular:</p>
<p>A number of recent studies propose normative criteria for gain modulation in populations:</p>
<p>- Duong, L., Simoncelli, E., Chklovskii, D. and Lipshutz, D., 2024. Adaptive whitening with fast gain modulation and slow synaptic plasticity. Advances in Neural Information Processing Systems</p>
<p>
- Tring, E., Dipoppa, M. and Ringach, D.L., 2023. A power law describes the magnitude of adaptation in neural populations of primary visual cortex. Nature Communications, 14(1), p.8366.</p>
<p>
- MÅynarski, W. and TkaÄik, G., 2022. Efficient coding theory of dynamic attentional modulation. PLoS Biology</p>
<p>
- Haimerl, C., Ruff, D.A., Cohen, M.R., Savin, C. and Simoncelli, E.P., 2023. Targeted V1 co-modulation supports task-adaptive sensory decisions. Nature Communications</p>
<p>
- The Ganguli and Simoncelli framework has been extended to a multivariate case and analyzed for a generalized class of error measures:</p>
<p>
- Yerxa, T.E., Kee, E., DeWeese, M.R. and Cooper, E.A., 2020. Efficient sensory coding of multidimensional stimuli. PLoS Computational Biology</p>
<p>
- Wang, Z., Stocker, A.A. and Lee, D.D., 2016. Efficient neural codes that minimize LP reconstruction error. Neural Computation, 28(12),</p>
<p>More detailed comments and feedback:</p>
<p>(1) I believe that this work offers the possibility to address an important question about novelty responses in the cortex (e.g. Homann et al, 2021 PNAS). Are they encoding novelty per-se, or are they inefficient responses of a not-yet-adapted population? Perhaps it's worth speculating about.</p>
<p>(2) Clustering in populations - typically in efficient coding studies, tuning curve distributions are a consequence of input statistics, constraints, and optimality criteria. Here the authors introduce randomly perturbed curves for each cluster - how to interpret that in light of the efficient coding theory? This links to a more general aspect of this work - it does not specify how to find optimal tuning curves, just how to modulate them (already addressed in the discussion).</p>
<p>(3) Figure 8 - where do Hz come from as physical units? As I understand there are no physical units in simulations.</p>
<p>(4) Inference with DDCs in changing environments. To perform efficient inference in a dynamically changing environment (as considered here), an ideal observer needs some form of posterior-prior updating. Where does that enter here?</p>
<p>(5) Page 6 - &quot;We did this in such a way that, for all Î½, the correlation matrices, Ï(Î½), were derived from covariance matrices with a 1/n power-law eigenspectrum (i.e., the ranked eigenvalues of the covariance matrix fall off inversely with their rank), in line with the findings of Stringer et al. (2019) in the primary visual cortex.&quot; This is a very specific assumption, taken from a study of a specific brain region - how does it relate to the generality of the approach?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104746.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Using the theory of efficient coding, the authors study how neural gains may be adjusted to optimize coding by noisy neural populations while minimizing metabolic costs. The manuscript first presents mathematical results for the general case where the computational goals of the neural population are not specified (the computation is implicit in the assumed tuning curves) and then develops the theory for a specific probabilistic coding scheme. The general theory provides an explanation for firing rate homeostasis at the level of neural clusters with firing rate heterogeneity within clusters, and the specific application further captures stimulus-specific and neuron-specific adaptation in the visual cortex.</p>
<p>The mathematical derivations, simulations, and application to visual cortex data are solid as far as I can tell.</p>
<p>In the current format, the significance is difficult to assess fully: the manuscript is a bit sprawling, in the first half the general theory is lengthy and technical, and then in the second half a few phenomena are addressed without a clear relation between them (rate homeostasis, rate heterogeneity, synaptic homeostasis, V1 adaptation, divisive normalization), requiring several ad-hoc choices and assumptions.</p>
<p>Strengths:</p>
<p>The problem of efficient coding is a long-standing and important one. This manuscript contributes to that field by proposing a theory of efficient coding through gain adjustments, independent of the computational goals of the system. The main result is a normative explanation for firing rate homeostasis at the level of neural clusters (groups of neurons that perform a similar computation) with firing rate heterogeneity within each cluster. Both phenomena are widely observed, and reconciling them under one theory is important.</p>
<p>The mathematical derivations are thorough as far as I can tell. Although the model of neural activity is artificial, the authors make sure to include many aspects of cortical physiology, while also keeping the models quite general.</p>
<p>Section 2.5 derives the conditions in which homeostasis would be near-optimal in the cortex, which appear to be consistent with many empirical observations in V1. This indicates that homeostasis in V1 might be indeed close to the optimal solution to code efficiently in the face of noise.</p>
<p>The application to the data of Benucci et al 2013 is the first to offer a normative explanation of stimulus-specific and neuron-specific adaptation in V1.</p>
<p>Weaknesses:</p>
<p>The novelty and significance of the work are not presented clearly. The relation to other theoretical work, particularly Ganguli and Simoncelli and other efficient coding theories, is explained in the Discussion but perhaps would be better placed in the Introduction, to motivate some of the many choices of the mathematical models used here.</p>
<p>The manuscript is very hard to read as is, it almost feels like this could be two different papers. The first half seems like a standalone document, detailing the general theory with interesting results on homeostasis and optimal coding. The second half, from Section 2.7 on, presents a series of specific applications that appear somewhat disconnected, are not very clearly motivated nor pursued in-depth, and require ad-hoc assumptions.</p>
<p>For instance, it is unclear if the main significant finding is the role of homeostasis in the general theory or the demonstration that homeostatic DDC with Bayes Ratio coding captures V1 adaptation phenomena. It would be helpful to clarify if this is being proposed as a new/better computational model of V1 compared to other existing models.</p>
<p>Early on in the manuscript (Section 2.1), the theory is presented as general in terms of the stimulus dimensionality and brain area, but then it is only demonstrated for orientation coding in V1.</p>
<p>The manuscript relies on a specific response noise model, with arbitrary tuning curves. Using a population model with arbitrary tuning curves and noise covariance matrix, as the basis for a study of coding optimality, is problematic because not all combinations of tuning curves and covariances are achievable by neural circuits (e.g. <ext-link ext-link-type="uri" xlink:href="https://pubmed.ncbi.nlm.nih.gov/27145916/">https://pubmed.ncbi.nlm.nih.gov/27145916/</ext-link> )</p>
<p>The paper Benucci et al 2013 shows that homeostasis holds for some stimulus distributions, but not others i.e. when the 'adapter' is present too often. This manuscript, like the Benucci paper, discards those datasets. But from a theoretical standpoint, it seems important to consider why that would be the case, and if it can be predicted by the theory proposed here.</p>
</body>
</sub-article>
</article>