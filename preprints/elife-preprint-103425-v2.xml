<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">103425</article-id>
<article-id pub-id-type="doi">10.7554/eLife.103425</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.103425.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Dual-format attentional template during preparation in human visual cortex</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Chen</surname>
<given-names>Yilin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Liu</surname>
<given-names>Taosheng</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Jia</surname>
<given-names>Ke</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
<email>kjia@zju.edu.cn</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Theeuwes</surname>
<given-names>Jan</given-names>
</name>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0333-4957</contrib-id>
<name>
<surname>Gong</surname>
<given-names>Mengyuan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>gongmy426@zju.edu.cn</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>Department of Psychology and Behavioral Sciences, Zhejiang University</institution></institution-wrap>, <city>Hangzhou</city>, <country country="CN">China</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05hs6h993</institution-id><institution>Department of Psychology, Michigan State University</institution></institution-wrap>, <city>East Lansing</city>, <country country="US">United States</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>Liangzhu Laboratory, MOE Frontier Science Center for Brain Science and Brain-machine Integration, State Key Laboratory of Brain-machine Intelligence, Zhejiang University</institution></institution-wrap>, <city>Hangzhou</city>, <country country="CN">China</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0310dsa24</institution-id><institution>Department of Neurobiology, Affiliated Mental Health Center &amp; Hangzhou Seventh People’s Hospital, Zhejiang University School of Medicine</institution></institution-wrap>, <city>Hangzhou</city>, <country country="CN">China</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>NHC and CAMS Key Laboratory of Medical Neurobiology, Zhejiang University</institution></institution-wrap>, <city>Hangzhou</city>, <country country="CN">China</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008xxew50</institution-id><institution>Department of Experimental and Applied Psychology, Vrije Universiteit Amsterdam</institution></institution-wrap>, <city>Amsterdam</city>, <country country="NL">Netherlands</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Zhang</surname>
<given-names>Xilin</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>South China Normal University</institution>
</institution-wrap>
<city>Guangzhou</city>
<country country="CN">China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Luo</surname>
<given-names>Huan</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-12-03">
<day>03</day>
<month>12</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-06-25">
<day>25</day>
<month>06</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP103425</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-10-03">
<day>03</day>
<month>10</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-10-03">
<day>03</day>
<month>10</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.12.602176"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-12-03">
<day>03</day>
<month>12</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103425.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.103425.1.sa3">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.103425.1.sa2">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.103425.1.sa1">Reviewer #2 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.103425.1.sa0">Reviewer #3 (Public review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Chen et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Chen et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-103425-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Goal-directed attention relies on forming internal templates of key information relevant for guiding behavior, particularly when preparing for upcoming sensory inputs. However, evidence on how these attentional templates is represented during preparation remains controversial. Here, we combine functional magnetic resonance imaging (fMRI) with an orientation cueing task to isolate preparatory activity from stimulus-evoked responses. Using multivariate pattern analysis, we found decodable information of the to-be-attended orientation during preparation; yet preparatory activity patterns were different from those evoked when actual orientations were perceived. When perturbing the neural activity by means of a visual impulse (‘pinging’ technique), the preparatory activity patterns in visual cortex resembled those associated with perceiving these orientations. The observed differential patterns with and without the impulse perturbation suggest a predominantly non-sensory format and a latent, sensory-like format of representation during preparation. Furthermore, the emergence of the sensory-like template coincided with enhanced information connectivity between V1 and frontoparietal areas and was associated with improved behavioral performance. By engaging this dual-format mechanism during preparation, the brain is able to encode both abstract, non-sensory information and more detailed, sensory information, potentially providing advantages for adaptive attentional control. For example, consistent with recent theories of visual search, a predominantly non-sensory template can support the initial guidance and a latent sensory-like format can support prospective stimulus processing.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Attentional selection</kwd>
<kwd>representational format</kwd>
<kwd>fMRI</kwd>
<kwd>decoding</kwd>
<kwd>visual cortex</kwd>
<kwd>impulse perturbation</kwd>
</kwd-group>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution>National Science and Technology Innovation 2030</institution>
</institution-wrap>
</funding-source>
<award-id>Major Project 2021ZD0200409</award-id>
</award-group>
<award-group id="funding-2">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01h0zpd94</institution-id><institution>National Natural Science Foundation of China</institution>
</institution-wrap>
</funding-source>
<award-id>32371087</award-id>
</award-group>
<award-group id="funding-3">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01h0zpd94</institution-id><institution>National Natural Science Foundation of China</institution>
</institution-wrap>
</funding-source>
<award-id>32300855</award-id>
</award-group>
<award-group id="funding-4">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01h0zpd94</institution-id><institution>National Natural Science Foundation of China</institution>
</institution-wrap>
</funding-source>
<award-id>3200784</award-id>
</award-group>
<award-group id="funding-5">
<funding-source>
<institution-wrap>
<institution>Fundamental Research Funds for the Central University</institution>
</institution-wrap>
</funding-source>
<award-id>226-2024-00118</award-id>
</award-group>
<award-group id="funding-6">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02drdmm93</institution-id>
<institution>Non-profit Central Research Institute Fund of Chinese Academy of Medical Sciences</institution>
</institution-wrap>
</funding-source>
<award-id>2023-PT310-01</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>(1) Additional analyses in the main texts and Supplementary Information (2) Discussions</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>To address the challenge of processing the overwhelming amounts of sensory inputs from the external environment, the brain must allocate attentional resources to prioritize the processing of task-relevant information. Importantly, humans can proactively prepare for stimulus selection before the arrival of sensory inputs (<xref ref-type="bibr" rid="c59">Summerfield &amp; De Lange, 2014</xref>). For example, when preparing to hail a taxi on the road, we tend to form a mental representation of the defining features of a taxi (e.g., yellow with a car-like shape). This ability relies on the formation of attentional templates — mental representations of the target — to accelerate stimulus selection and resolve perceptual competition by enhancing task-relevant information and suppressing irrelevant information (<xref ref-type="bibr" rid="c8">Desimone &amp; Duncan, 1995</xref>; <xref ref-type="bibr" rid="c29">Kastner et al., 1999</xref>). While most attentional models posit that attentional templates during stimulus processing reflect veridical representations of the target (<xref ref-type="bibr" rid="c28">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c39">Malcolm &amp; Henderson, 2009</xref>), the nature of the template during preparation remains less understood.</p>
<p>A classical view suggests that attentional template during preparation may reflect veridical target features, analogous to the representational format during stimulus selection. However, evidence supporting this account has been mixed. For example, while some previous fMRI studies have demonstrated that preparatory activity contains target information similar to the sensory responses to the corresponding targets (<xref ref-type="bibr" rid="c32">Kok et al., 2014</xref>; <xref ref-type="bibr" rid="c35">Lewis-Peacock et al., 2015</xref>; <xref ref-type="bibr" rid="c58">Stokes et al., 2009</xref>), more recent electrophysiological studies suggest that, if anything, this template is engaged only shortly before the expected arrival of sensory input rather than being continuously active (<xref ref-type="bibr" rid="c18">Grubert &amp; Eimer, 2018</xref>; <xref ref-type="bibr" rid="c41">Myers et al., 2015</xref>). Notably, in some cases the template is even largely undetectable during preparation (<xref ref-type="bibr" rid="c65">Wen et al., 2019</xref>). Alternatively, an emerging view suggests a non-veridical template suffices for guiding attention during preparation, where precise processing of stimuli may be unnecessary at this stage. Support for this notion comes from the identification of attentional signals during preparation that differ from neural signals observed during perceptual target processing (<xref ref-type="bibr" rid="c14">Gong et al., 2022</xref>). Recent theories of visual search also propose a non-veridical, “good-enough” template for early attentional guidance (<xref ref-type="bibr" rid="c66">Wolfe, 2021</xref>; <xref ref-type="bibr" rid="c72">Yu et al., 2023</xref>). However, it remains unclear whether a “good-enough” template for search also applies to preparatory attention.</p>
<p>The notion that there may be a sensory and non-sensory attentional template might not be as far-fetched as it seems. Indeed, it is feasible that during preparation, following stimulus presentation, attentional signals undergo a transformation from a non-sensory to a sensory-like template. Previous behavioral (<xref ref-type="bibr" rid="c20">Hamblin-Frohman &amp; Becker, 2021</xref>; <xref ref-type="bibr" rid="c71">Yu et al., 2022</xref>) and neural studies (<xref ref-type="bibr" rid="c14">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="c28">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c65">Wen et al., 2019</xref>) are generally consistent with this idea of coarse-to-fine transitions, suggesting that during preparation, a sensory-like template may not be initially necessary but only becomes relevant when the stimulus needs to be identified. However, if and in what way the brain coordinates these non-sensory and sensory-like templates remains unclear. Here, we propose that during preparation, a sensory-like template may be stored in a latent (e.g., activity-silent) state concurrently with a non-sensory template. This idea parallels recent findings from working memory studies, which suggest that information intended for proactive use is kept in activity-silent traces to support future behavior (<xref ref-type="bibr" rid="c56">Stokes, 2015</xref>; <xref ref-type="bibr" rid="c68">Wolff et al., 2015</xref>, <xref ref-type="bibr" rid="c69">2017</xref>). The present study seeks to determine the possibility of the latent, sensory-like template during the preparation for discriminating an upcoming stimulus.</p>
<p>To test these hypotheses, participants engaged in a cuing task in which they prepared during an extended period of time for the presentation of a compound stimulus grating containing the cued orientation and a distractor orientation. In addition, in order to be able to construct the sensory-format representations (leftward and rightward orientation), single orientations were presented during the perception task. Critically, we used a “pinging” technique combined with multivariate decoding methods, which has been shown to be effective in retrieving information from latent brain states (<xref ref-type="bibr" rid="c10">Duncan et al., 2023</xref>; <xref ref-type="bibr" rid="c68">Wolff et al., 2015</xref>, <xref ref-type="bibr" rid="c69">2017</xref>; <xref ref-type="bibr" rid="c73">Zhang &amp; Luo, 2023</xref>). In the standard condition (<italic>No-Ping session</italic>), the preparation period was devoid of visual impulses. During preparation, the neural activity patterns in visual and frontoparietal areas could discriminate between the orientations that participants were preparing for. Yet, neural activity patterns evoked by the perception of orientations were distinct from those evoked during preparation for upcoming orientations, suggesting a predominantly non-sensory template during preparation. By contrast, when we presented a high-contrast, task-irrelevant impulse stimulus during preparation (<italic>Ping session</italic>), neural activity patterns evoked by the perception of orientations were similar to those activated by the preparation for orientations in the visual cortex, suggesting the existence of a latent, sensory-like format of representation during preparatory attention. Furthermore, the emergence of sensory-like template coincided with enhanced information connectivity between V1 and frontoparietal areas and was associated with improved behavioral performance. Our findings provide evidence for the co-existence of two formats of attentional templates (non-sensory vs. sensory-like) during preparation, as well as a novel neural mechanism for their maintenance in different functional states (active vs. latent). We propose that this dual-format representation may serve to increase flexibility of attentional control.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Behavioral performance during the attention tasks</title>
<p>In an orientation cueing task, participants were shown a color cue indicating the reference orientation (45° or 135°) to attend to during preparation period (a delay of 5.5 s or 7.5 s) with or without the impulse perturbation (<xref rid="fig1" ref-type="fig">Fig 1A</xref>). This was followed by the presentation of a compound stimulus consisting of two oriented gratings. During the stimulus selection period (after the gratings appeared), participants were tasked with discriminating a small angular offset of the cued grating from the cued reference orientation. The angular offset was individually thresholded before the scanning sessions (mean offset = 2.50° in the No-Ping session and 2.52° in the Ping session) without significant difference between two sessions (independent t-test: t(19) = 0.085, <italic>p</italic> = 0.932, Cohen’s d = −0.027). Participants’ discrimination performance showed no significant difference between two attended orientations in either the No-Ping (paired t-test: t(19) = 1.439, p = 0.166, Cohen’s d = −0.321) or the Ping session (paired t-test: t(19) = 0.494, p = 0.627, Cohen’s d = 0.122; <xref rid="fig1" ref-type="fig">Fig 1C</xref>). A two-way mixed ANOVA (attended orientation × session) revealed neither significant main effects (attended orientation: F(1,38) = 0.392, <italic>p</italic> = 0.535, η<sub>p</sub><sup>2</sup> = 0.01; session: F(1,38) = 0.001, <italic>p</italic> = 0.970, η<sub>p</sub><sup>2</sup> &lt; 0.001) nor interaction effect (F(1,38) = 1.811, <italic>p</italic> = 0.186, η<sub>p</sub><sup>2</sup> = 0.045). Bayesian analyses provided moderate evidence to support the null hypothesis (BF<sub>excl</sub> &gt; 3.633), suggesting comparable performance levels between two sessions and two attended orientations.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig 1.</label>
<caption><title>Experiment procedure and behavioral performance.</title>
<p>(A) Attention tasks in the No-Ping and Ping sessions. Note that only long-delay trials are shown. A small proportion of short-delay trials (20%, with a delay of 1.5 s or 3.5 s) were included to create temporal uncertainty and encourage consistent active preparation during the delay. Both component gratings were flickering at 10 Hz between white and black, so that luminance could not confound either the task strategy (e.g., attending to luminance) or neural measures. The inset shows two sets of color-orientation mapping, which were reversed halfway through the experiment to minimize the impact of cue-induced sensory difference on neural activity. A high-contrast impulse was presented during preparation period in the Ping session. (B) Perception task. Similar to the attention task, the single-orientation grating also flickered at 10 Hz between white and black. (C) Behavioral accuracy in the attention tasks in the No-Ping and Ping sessions. Each dot represents one subject’s data. Error bars denote standard error of the means (SEM).</p></caption>
<graphic xlink:href="602176v3_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<title>A default, non-sensory representation of attentional template during preparation</title>
<p>The first aim of this study was to determine whether attentional signals during preparation are encoded in a sensory-like or non-sensory format. To address this, we first examined whether, in the attention task during the No-Ping condition, the distributed neural pattern contained feature-specific information. We trained and tested separate classifiers to predict the attended orientation during the preparation and stimulus selection periods (<xref rid="fig2" ref-type="fig">Fig 2A</xref>, “Attention decoding”; see Materials and Methods for details). This analysis was performed for each of the four regions along the visual hierarchy, including primary visual cortex (V1), extrastriate visual cortex (EVC), intraparietal sulcus (IPS) and prefrontal cortex (PFC). The average decoding accuracies for both preparation and stimulus selection periods were significantly above chance level in each region (Permutation analyses: <italic>ps</italic> &lt; 0.004 across regions, <xref rid="fig2" ref-type="fig">Fig 2B</xref>), indicating that the brain maintained reliable information about the attended feature both before and after the onset of the compound grating. Next, we examined whether the preparatory activity reflected a sensory-like format of attentional template (<xref rid="fig2" ref-type="fig">Fig 2A</xref>, “Cross-task generalization”, see Materials and Methods). We trained a classifier using data from the perception task (leftward vs. rightward orientation; <xref rid="fig1" ref-type="fig">Fig 1B</xref>) and tested its performance on data from the preparation period in the attention task (attend leftward vs. attend rightward). However, this cross-task generalization analysis yielded no significant effects (<italic>ps</italic> &gt; 0.132 across the regions). In contrast, we observed above-chance generalization from the perception task to the stimulus selection period (<italic>ps</italic> &lt; 0.001 across regions, <xref rid="fig2" ref-type="fig">Fig 2C</xref>), confirming previous findings of the sensory-like attentional template following stimulus presentation (<xref ref-type="bibr" rid="c14">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="c28">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c65">Wen et al., 2019</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig 2.</label>
<caption><title>MVPA for the No-Ping session and Ping session.</title>
<p>(A) Schematic illustration of the decoding of attended orientation (attend leftward vs. attend rightward) in the attention task (left panel) and the cross-task generalization analysis from perception task to the attention task (right panel). The four regions are shown on a representative right hemisphere as colored areas: V1 is marked in red, EVC in yellow, IPS in cyan and PFC in purple. (B) Decoding accuracy during preparation and stimulus selection periods across regions in the No-Ping and (D) Ping session. (C) Cross-task generalization performance from the perception task to the preparatory periods and the stimulus selection periods across regions in the No-Ping and (E) Ping session. The dashed lines represent the theoretical chance level (0.5). Each dot represents one subject’s data. Error bars denote SEM.</p></caption>
<graphic xlink:href="602176v3_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Before drawing conclusions based on the lack of generalization from the perception task to preparatory attention, we considered two alternative explanations to rule out potential confounds. First, the robust attention decoding during preparation ruled out the possibility that participants were not actively engaged in the task during preparation (<xref rid="fig2" ref-type="fig">Fig 2B</xref>, unfilled bars). Second, the generalizable effect from the perception task to the stimulus selection period across regions (<xref rid="fig2" ref-type="fig">Fig 2C</xref>, filled bars) argues against the possibility of low statistical power. Overall, these findings suggest that the preparatory attention and sensory processing of features have distinct formats, presumably reflecting a non-sensory format of representation during the preparation. These results replicate those of a previous fMRI study using motion stimuli with a similar design (<xref ref-type="bibr" rid="c14">Gong et al., 2022</xref>). Furthermore, consistent with previous studies (<xref ref-type="bibr" rid="c14">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="c28">Jigo et al., 2018</xref>), univariate analysis did not reveal any reliable difference in overall BOLD responses between attention orientations (Supplementary Information and S1 Fig).</p>
</sec>
<sec id="s2c">
<title>A latent, sensory-like attentional template during preparation revealed by visual impulse</title>
<p>The second aim of our study was to examine whether a latent, sensory-like template exists during preparation. While this precise template may not be necessary for preparation, it is relevant for subsequent target selection and discrimination (i.e., select the cued grating from the compound stimulus and discriminate a small angular offset between the cued grating and the reference orientation). To test this hypothesis, we perturbed the neural activity by means of a visual impulse during the preparation period in the Ping session (<xref rid="fig1" ref-type="fig">Fig 1A</xref>, right panel). Using the same analyses as those performed in the No-Ping session, robust attentional signals were observed during both preparation and stimulus selection periods (Permutation analyses: <italic>ps</italic> &lt; 0.001 across regions; <xref rid="fig2" ref-type="fig">Fig 2D</xref>). Importantly, the cross-task generalization analyses indicated that the visual impulse led to above-chance generalization from the perception task to preparation period (<xref rid="fig2" ref-type="fig">Fig 2E</xref>, unfilled bars) in V1 and EVC (<italic>ps</italic> &lt; 0.001), but not in IPS and PFC (<italic>ps</italic> &gt; 0.584), along with generalizable effects from the perception task to the stimulus selection periods (<italic>ps</italic> &lt; 0.036 across regions; <xref rid="fig2" ref-type="fig">Fig 2E</xref>; filled bars). These results suggest that different brain areas are involved in coding for sensory-like templates. To further evaluate whether the cross-task generalization from the perception task to the preparation period was statistically different with and without visual impulse, we conducted a two-way mixed ANOVA (session × region) on the generalization performance. The analysis revealed main effects of region (F(3,114) = 5.220, <italic>p</italic> = 0.002, η<sub>p</sub><sup>2</sup> = 0.121), session (F(1,38) = 7.321, <italic>p</italic> = 0.010, η<sub>p</sub><sup>2</sup> = 0.162), and importantly, a significant interaction effect (F(3,114) = 3.964, <italic>p</italic> = 0.010, η<sub>p</sub><sup>2</sup> = 0.094), supporting the observation that the visual impulse led to significantly increased decoding accuracy in V1 (independent t-test: t(38) = 3.145<italic>, p</italic> = 0.003, Cohen’s d = 0.995) and EVC (independent t-test: t(38) = 2.153, <italic>p</italic> = 0.038, Cohen’s d = 0.681), but not in the frontoparietal regions (<italic>ps</italic> &gt; 0.374). This dissociable result between the two sessions further supports the activation of a latent, sensory-like template by the visual impulse during preparatory attention.</p>
<p>To further solidify this conclusion, following analyses were used to examine several alternative possibilities. First, we examined whether the impulse-driven generalization resulted from stronger feature information in the Ping compared to No-Ping session during the perception task. This was not the case, as evidenced by comparable levels of decodable orientation information between the Ping and No-Ping sessions (see Supplementary Information and S2 Fig). Next, we asked whether the increased generalization was due to generally stronger attentional signals in the Ping session during the attention tasks – for example, if visual impulses simply refocused attention during long delays. This was not the case, as the two-way mixed ANOVAs (session × region) on attention decoding accuracy revealed neither a significant main effect of session nor an interaction effect during both the preparation (<italic>ps</italic> &gt; 0.519; BF<sub>excl</sub> &gt; 3.247) and stimulus selection periods (<italic>ps</italic> &gt; 0.336; BF<sub>excl</sub> &gt; 3.297), suggesting comparable amount of attentional information between the two sessions. Therefore, the findings of impulse-driven sensory-like template in the visual cortex during preparation cannot be explained by general differences between two sessions.</p>
</sec>
<sec id="s2d">
<title>Matching preparatory attention to sensory template: impact on neural representation and behavior</title>
<p>The reported decoding accuracy from the cross-task generalization analysis quantifies the degree to which differences in neural activity pattern between two conditions are shared across attention and perception tasks. However, it does not directly measure how similar the neural patterns are when attending to an orientation compared to perceiving that orientation. Unlike decoding accuracies, Mahalanobis distance provides a continuous measure for characterizing representational geometries between different conditions (<xref ref-type="bibr" rid="c38">Mahalanobis, 1936</xref>). To further corroborate our findings of the impulse-driven sensory-like template, we calculated the Mahalanobis distance between each attention condition during preparation and each perception condition (see Materials and Methods). If the patterns of activity reflect a sensory-like template, we would expect greater pattern similarity (smaller distance) between “attend leftward” and “perceive leftward” than between “attend leftward” and “perceive rightward”, and vice versa for the “attend rightward” conditions (see <xref rid="fig3" ref-type="fig">Fig 3A</xref> for a schematic of the four pair-wise distance measures), leading to an interaction between attended and perceived orientation conditions.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig 3.</label>
<caption><title>Orientation-selective attentional modulations on neural pattern distances during preparation.</title>
<p>(A) Schematic illustration of the representational distance (mean Mahalanobis distances) between each of the attention conditions and each of the perception conditions. Colored arrows indicate measures of the pair-wise Mahalanobis distance. The right panel shows two attention trials (red indicates attend-to-leftward and green indicates attend-to-rightward) to the distribution of each perception condition (shown in a cloud of light-colored dots). (B) Mahalanobis distance between preparatory attention condition and perceived orientation condition in the No-Ping and (C) Ping sessions. Error bars denote SEM. (D) Correlations between attentional modulation index (AMI) and reaction time (RT) in the No-Ping and (E) Ping sessions. Each dot represents one subject’s data. The shaded area represents the confidence intervals of the regressed lines. * <italic>p</italic> &lt; 0.05, ** <italic>p</italic> &lt; 0.01.</p></caption>
<graphic xlink:href="602176v3_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We used a two-way repeated-measures ANOVA (attended orientation × perceived orientation) on the Mahalanobis distance, separately for each session and each region. During the preparatory period in the No-Ping session, no significant interaction effects were observed across regions (<italic>ps</italic> &gt; 0.443; <xref rid="fig3" ref-type="fig">Fig 3B</xref>). In contrast, the same analyses applied to the Ping session revealed significant interaction effects in visual areas (V1: F(1,19) = 9.335, <italic>p</italic> = 0.007, η<sub>p</sub><sup>2</sup> = 0.329; EVC: F(1,19) = 8.563, <italic>p</italic> = 0.009, η<sub>p</sub><sup>2</sup> = 0.311; <xref rid="fig3" ref-type="fig">Fig 3C</xref>; also see Supplementary Table 1), but not for frontoparietal regions (<italic>ps</italic> &gt; 0.213). This cross-region difference is consistent with the function of sensory areas in encoding precise neural representations for basic visual features. To directly compare whether the attentional modulation on Mahalanobis distance was statistically different with and without the visual impulse, we used a three-way mixed ANOVA (session × attended orientation × perceived orientation). The analysis revealed no main effects (<italic>ps</italic> &gt; 0.300 across comparisons) but a significant three-way interaction in V1 (F(1,38) = 5.00, <italic>p</italic> = 0.031, η<sub>p</sub><sup>2</sup> = 0.116; see Supplementary Information, Table 1), suggesting that by “pinging” the brain, the attentional template during preparation became more similar to the perception of corresponding orientation. We also calculated the Mahalanobis distance between neural patterns evoked by the superimposed gratings during the stimulus selection period and each condition in the perception task, finding similar results (see Supplementary Information and S4 Fig). This result was expected, as feature-based attention is known to selectively enhance the representation of task-relevant features while filtering out task-irrelevant ones.</p>
<p>The continuous nature of the Mahalanobis distance also made it possible to further investigate potential neural-behavioral correlations. We examined whether activating a sensory-like template during preparation would benefit subsequent orientation processing. In particular, we calculated attentional modulation indices (AMIs) based on trialwise Mahalanobis distance in V1. The index was calculated as follows: AMI = (D<sub>different</sub> – D<sub>same</sub>)/(D<sub>different</sub> + D<sub>same</sub>), where D<sub>same</sub> and D<sub>different</sub> are the measured distance (D) in the Same (e.g., attend and perceive the same orientation) and Different (e.g., attend and perceive different orientations) orientation condition, respectively (see Methods and Materials). Then, we calculated the correlation between AMI and both reaction time (RT) and accuracy across participants, separately for each session. In the No-Ping session, we observed no significant correlation between AMI in V1 and RT (<italic>r</italic> = - 0.366, <italic>p</italic> = 0.113; <xref rid="fig3" ref-type="fig">Fig. 3D</xref>). By contrast, the same analysis in the Ping condition revealed a significantly negative correlation (<italic>r</italic> = −0.518, <italic>p</italic> = 0.019; <xref rid="fig3" ref-type="fig">Fig. 3E</xref>). These results indicate that the attentional modulations evoked by visual impulse was associated with faster RTs. These effects were not observed for accuracy (<italic>ps</italic> &gt; 0.550). Furthermore, we also performed within-subject analysis by sorting trials as “strong modulation” and “weak modulation” trials based on each individual’s AMI values, facilitated RTs were observed in “strong modulation” trials during the Ping session (Supplementary Information and S3 Fig). These results suggest that the impulse-driven sensory-like template in primary visual cortex is functionally relevant to subsequent attentional selection, providing evidence for the prospective use of sensory-like template in this task. In addition, we did not observe such behavioral differences in analogous analyses using data from the stimulus selection period in either session (<italic>ps</italic> &gt; 0.230), which might due to the potential dilution by strong stimulus-evoked responses during the stimulus selection period.</p>
</sec>
<sec id="s2e">
<title>Activating sensory-like template strengthens the informational connectivity between sensory and frontoparietal areas</title>
<p>Selective attention is generally believed to rely on coordinated network activity (<xref ref-type="bibr" rid="c7">Corbetta &amp; Shulman, 2002</xref>). In particular, studies have shown that functional connectivity between sensory and frontoparietal areas was modulated by attentional control (<xref ref-type="bibr" rid="c6">Bressler et al., 2008</xref>; <xref ref-type="bibr" rid="c48">Rosenberg et al., 2020</xref>). Given that the impulse-driven sensory-like template facilitated behavior, we reasoned that it may also enhance network communication. Thus, we examined informational connectivity measures to explore how the impulse altered network function during the attention task.</p>
<p>We used a method that allows inference based on multivoxel pattern information rather than univariate BOLD response (<xref ref-type="bibr" rid="c26">Jia et al., 2020</xref>; <xref ref-type="bibr" rid="c42">Ng et al., 2021</xref>). For each ROI, we calculated the cross-validated Mahalanobis distance from each attention trial (from one left-out run) to the distribution of each attended orientation (all trials from remaining runs) during preparation (see Methods and Materials). To quantify the degree of attentional modulation during preparation, we calculated the AMI based on trial-wise Mahalanobis distance and generated a time course of AMI values across trials. Pearson correlation was used to estimate the covariation between each pair of ROIs, the resulting correlation coefficients were transformed using Fisher’s z-transform for statistical inference (<xref rid="fig4" ref-type="fig">Fig 4A</xref>). The analysis revealed numerically higher levels of connectivity in Ping than in No-Ping session, this impulse-driven increase in connectivity reached statistical significance in two pairs (<xref rid="fig4" ref-type="fig">Fig 4B</xref>): V1-IPS (independent t-test: t(38) = 2.566<italic>, p</italic> = 0.014; Cohen’s d = 0.812) and V1-PFC (independent t-tests: t(38) = 3.158<italic>, p</italic> = 0.003; Cohen’s d = 0.999). The enhanced functional connectivity between V1 and frontoparietal areas driven by the impulse may potentially facilitate information flow among areas to improve attentional control, as implicated by a trend of ping-enhanced correlations between V1-PFC and RTs (Supplementary Information and S5 Fig). Additionally, the same analysis of AMI based on cross-validated Mahalanobis distance during the stimulus selection period showed no significant differences in information connectivity between No-Ping and Ping sessions (<italic>ps</italic> &gt; 0.224; Supplementary Information and S6A Fig). The lack of changes in long-range connectivity during stimulus selection period may be attributed to a general rise in connectivity caused by strong sensory inputs in this period, which could have attenuated any potential impacts of visual impulses. Furthermore, connectivity analysis based on mean BOLD response over time did not reveal any significant changes in inter-cortical connections between the two sessions (<italic>ps</italic> &gt; 0.136; S6B Fig), suggesting that the impulse-driven increased information connectivity between V1 and higher-order areas was unlikely contributed by the overall changes of BOLD response.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig 4.</label>
<caption><title>Information connectivity analysis.</title>
<p>(A) Schematic illustration of the procedure for the information connectivity analysis in the space of two hypothetical voxels. For each region, we calculated the Mahalanobis distance of the attention trial (from one left-out run) from two attention distributions (all trials from remaining runs). Red and green dots indicate activity patterns from two trials (right panel). The brain image shows an example pair of intercortical information connectivity between V1 and PFC. The time series (lower-left panel) consisted of attentional modulation index (AMI) based on the Mahalanobis distance. (B) Between-region information connectivity in the No-Ping and Ping sessions. (C) The differences in connectivity between the Ping and No-Ping sessions. * <italic>p</italic> &lt; 0.05, ** <italic>p</italic> &lt; 0.01.</p></caption>
<graphic xlink:href="602176v3_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>While there is ample evidence that the brain can maintain an attentional template of an upcoming target before sensory information is presented (<xref ref-type="bibr" rid="c8">Desimone &amp; Duncan, 1995</xref>; <xref ref-type="bibr" rid="c29">Kastner et al., 1999</xref>; <xref ref-type="bibr" rid="c59">Summerfield &amp; De Lange, 2014</xref>), its representational format remains unclear. To address this, we used an orientation cueing paradigm with separated preparation and stimulus selection periods and applied MVPA to decode neural activity patterns associated with feature-specific attentional information of the upcoming target. The analyses showed robust attentional information both before and after the presentation of the compound grating, indicating a sustained maintenance of attentional templates throughout a trial. Importantly, while the decoders trained on the perception of single orientations could not generalize to preparation until the stimulus selection period (<italic>No-Ping session</italic>), perturbing the brain with a visual impulse resulted in generalizable activity patterns during preparation in V1 and EVC (<italic>Ping session</italic>). These results suggest a predominantly non-sensory format of representation, with a sensory-like template in a latent state during feature-based preparation in the visual cortices. Furthermore, impulse-driven sensory-like template was accompanied by enhanced information connectivity between V1 and frontoparietal areas, as well as enhanced orientation-specific neural modulations of neural distances in the visual areas that predicted levels of behavioral performance. We observed similar response profiles in V1 and EVC, with V1 exhibiting more robust ping-evoked changes compared to EVC, consistent with its primary role in orientation processing (<xref ref-type="bibr" rid="c44">Priebe, 2016</xref>). The differences between the Ping and No-Ping sessions could not be attributed to differences in sensory information from the perception task, overall strength of preparatory attention, or differences in eye position (S7 Fig). Therefore, our findings suggest a dual-format neural representation scheme (non-sensory vs. sensory-like) operating in different functional states (active vs. latent). This mechanism may give rise to flexible attentional control, allowing effective transition from coarse to fine attentional templates at various processing stages (initial guidance vs. precise stimulus discrimination).</p>
<p>Recent advances in theories of visual search differentiated between the “guiding template” and “target template” based on measures of behavioral performance and eye movements (<xref ref-type="bibr" rid="c66">Wolfe, 2021</xref>; <xref ref-type="bibr" rid="c72">Yu et al., 2023</xref>). According to these theories, early attentional guidance typically depends on a non-veridical codes that represent only the most diagnostic information, whereas later, target-match processes utilize more precise codes to optimize decision accuracy (<xref ref-type="bibr" rid="c30">Kerzel, 2019</xref>; <xref ref-type="bibr" rid="c52">Scolari et al., 2012</xref>; <xref ref-type="bibr" rid="c71">Yu et al., 2022</xref>). Our study reveals a parallel coding mechanism in the context of feature-based attention, expanding upon these theoretical notions in three key aspects.</p>
<p>First, we provide neural evidence for a default, predominantly non-sensory template during preparation, indicating that the concept of a “guiding template”, as proposed by visual search theories (<xref ref-type="bibr" rid="c66">Wolfe, 2021</xref>; <xref ref-type="bibr" rid="c72">Yu et al., 2023</xref>), also applies to preparatory attention in a non-search context. This highlights a shared functional role of a non-veridical attentional template in early guidance across different scenarios. Second, despite the theoretical notion that the brain maintains a more veridical template with detailed target information than is typically utilized to form the guiding template (<xref ref-type="bibr" rid="c66">Wolfe, 2021</xref>; <xref ref-type="bibr" rid="c72">Yu et al., 2023</xref>), neural evidence supporting this hypothesis is currently lacking. We provide evidence for this notion and propose a plausible neural implementation for preserving a more veridical, sensory-like template in the latent state. A natural question is why the sensory-like template remains latent during preparation. We note that our task requires both coarse and fine featural information. During preparation, a coarse, non-veridical guiding template suffices for target-distractor discrimination, while during stimulus selection, a precise template is needed for the fine discrimination task (reporting the tilted direction of a small angular offset). Maintaining a latent sensory-like template during preparation is thus efficient, as it facilitates future sensory processing while conserving resources. Finally, information connectivity between visual and higher-order frontoparietal regions was enhanced by visual impulse during preparation, which correlated with improved behavioral performance in feature selection. This result suggests that improved information flow across the relevant areas leads to enhanced attentional control, which in turn contributes to refined sensory representations of target in early visual cortex, facilitating transitions from a non-sensory to sensory-like template. Future studies may adopt layer-specific fMRI to infer the direction of this improved information flow (<xref ref-type="bibr" rid="c25">Jia et al., 2023</xref>; <xref ref-type="bibr" rid="c27">Jia et al., 2024</xref>) and explore the relationship between long-range connections and the utilization of different formats of target templates.</p>
<p>It could be argued that preparatory attention relies on the same mechanisms as working memory maintenance. While these functions are intuitively similar and likely overlap, there is also evidence indicating that they can be dissociated (<xref ref-type="bibr" rid="c4">Battistoni et al., 2017</xref>). In particular, we note that in our task, attention is guided by symbolic cues (color-orientation associations), while working memory tasks typically present the actual visual stimulus as the memorandum. A central finding in working memory studies is that neural signals during WM maintenance are sensory in nature, as demonstrated by generalizable neural activity patterns from stimulus encoding to maintenance in visual cortex (<xref ref-type="bibr" rid="c21">Harrison &amp; Tong, 2009</xref>; <xref ref-type="bibr" rid="c53">Serences et al., 2009</xref>; <xref ref-type="bibr" rid="c47">Rademaker et al., 2019</xref>). However, in our task, neural signals during preparation were non-sensory, as demonstrated by a lack of such generalization in the No-Ping condition (see also <xref ref-type="bibr" rid="c14">Gong et al., 2022</xref>). We believe that the differences in cue format and task demand in these studies may account for such differences. In addition to the difference in the sensory nature of the preparatory versus delay-period activity, our ping-related results also exhibited divergence from working memory studies (<xref ref-type="bibr" rid="c68">Wolff et al., 2015</xref>, <xref ref-type="bibr" rid="c69">2017</xref>). While these studies used the visual impulse to differentiate active and latent representations of <italic>different items</italic> (e.g., attended vs. unattended memory item), our study demonstrated the active and latent representations of <italic>a single item in different formats</italic> (i.e., non-sensory vs. sensory-like). Moreover, unlike our study, the impulse did not evoke sensory-like neural patterns during memory retention (<xref ref-type="bibr" rid="c69">Wolff et al., 2017</xref>). These observations suggest that the cognitive and neural processes underlying preparatory attention and working memory maintenance could very well diverge. Future studies are necessary to delineate the relationship between these functions both at the behavioral and neural level.</p>
<p>While we found that the ping allowed us to detect a sensory-like template during preparation, the underlying neural mechanism of such effects remains unclear. One possibility, as informed by theoretical studies of working memory, is that the sensory-like template could be maintained via an “activity-silent” mechanism through short-term changes in synaptic weights (<xref ref-type="bibr" rid="c40">Mongillo et al., 2008</xref>). In this framework, a visual impulse may function as nonspecific inputs that momentarily convert latent traces into detectable activity patterns (<xref ref-type="bibr" rid="c46">Rademaker &amp; Serences, 2017</xref>). Related to our findings, it is unlikely that the orientation-specific templates observed during the Ping session emerged <italic>de novo</italic> from purely non-sensory representations and were entirely induced by an exogenous ping, which was devoid of any orientation signal. Instead, the more parsimonious explanation is that visual impulse reactivated pre-existing latent sensory signals, consistent with the models of “activity-silent” working memory. However, the detailed circuit-level mechanism of such reactivation is still unclear, as well as whether this effect is modality-specific. Prior work shows that only visual, but not auditory, impulses reactivate latent visual working memory (<xref ref-type="bibr" rid="c70">Wolff et al., 2020</xref>), suggesting some degree of modality-specificity. However, this finding warrants direct investigation in future studies. Furthermore, we acknowledge that whether pinging identifies an activity-silent mechanism is currently debated (<xref ref-type="bibr" rid="c3">Barbosa et al., 2021</xref>; <xref ref-type="bibr" rid="c51">Schneegans &amp; Bays, 2017</xref>). An alternative possibility is that the visual impulse amplified a subtle but active representation of the sensory template during preparation. Distinguishing between these alternatives likely requires future studies with more detailed neurophysiological measurements. Regardless of the precise neural mechanism for the observed latent, sensory representation, our results suggest that both sensory and non-sensory templates likely co-exist.</p>
<p>The non-generalizable activity patterns from perception to preparatory attention, in the absence of visual impulse, suggests a default, predominantly non-sensory template during preparation. This finding is largely consistent with electrophysiological studies (<xref ref-type="bibr" rid="c41">Myers et al., 2015</xref>; <xref ref-type="bibr" rid="c65">Wen et al., 2019</xref>) and our prior fMRI work on preparatory attention to motion directions (<xref ref-type="bibr" rid="c14">Gong et al., 2022</xref>), but differs from some previous neuroimaging studies that demonstrated sensory-like templates during preparation (<xref ref-type="bibr" rid="c32">Kok et al., 2014</xref>; <xref ref-type="bibr" rid="c43">Peelen &amp; Kastner, 2011</xref>; <xref ref-type="bibr" rid="c58">Stokes et al., 2009</xref>). One potential account for these discrepancies is that those studies used cue-only trials where the target was expected but not actually presented, in contrast to our task where the target was shown on every trial with temporally separated preparation and stimulus selection periods. This seemingly subtle difference may significantly impact the formats of the neural representations. Because cue-only trials increased the likelihood of target appearance at the subsequent time point, sensory template may be activated due to modulations of temporal expectations (<xref ref-type="bibr" rid="c18">Grubert &amp; Eimer, 2018</xref>). This explanation is consistent with theories suggesting differential influences of expectation and attention on neural activity: expectation reflects visual interpretations of stimuli due to sensory uncertainty, whereas attention is guided based on the task relevance of sensory information (<xref ref-type="bibr" rid="c49">Rungratsameetaweemana &amp; Serences, 2019</xref>; <xref ref-type="bibr" rid="c60">Summerfield &amp; Egner, 2009</xref>, <xref ref-type="bibr" rid="c61">2016</xref>). Our finding of a predominantly non-sensory format may indicate an optimized coding strategy employed by the brain to effectively and robustly represent information for future use. This aligns with the proposed role of attention in modulating sensory representations to encode only currently relevant information at a minimal cost (<xref ref-type="bibr" rid="c72">Yu et al., 2023</xref>).</p>
<p>While our findings cannot pinpoint the exact format of this non-sensory template, we consider categorical coding a plausible candidate based on previous findings. For instance, visual search studies demonstrate that categorical attributes (e.g., steep vs. shallow; left-tilted vs. right-tilted) efficiently guide attention for simple features, such as an orientation or a color (<xref ref-type="bibr" rid="c34">Kong et al., 2017</xref>; <xref ref-type="bibr" rid="c67">Wolfe et al., 1992</xref>), particularly when features are consistent and predictable (<xref ref-type="bibr" rid="c23">Hout et al., 2017</xref>). In our task, the angular relations between the target and distractor orientation were defined by categorical attributes (e.g., left-tilted vs. right-tilted) and remained consistent across trials, making a categorical template feasible during preparatory attention. Furthermore, categorical template allows for greater tolerance of stimulus variability, which is also useful given the trial-by-trial variations in target orientation around the reference orientation in our task. Future studies are needed to address the nature of the non-sensory template during preparation as well as task parameters that might modulate them.</p>
<p>In summary, the current study suggests that there are two formats of attentional templates each having a distinct functional state: a default, non-sensory format and a latent, sensory-like format. This dual-format representation aligns with theories on the dual-function of attentional template for different task goals (<xref ref-type="bibr" rid="c22">Hout &amp; Goldinger, 2015</xref>; <xref ref-type="bibr" rid="c72">Yu et al., 2023</xref>). The current findings provide a plausible neural implementation for these theories by demonstrating different formats in different functional states. This mechanism likely reflects an optimized coding scheme that effectively balances processing efforts and demands, particularly well-suited for flexible control and transitions from coarse to fine task demands in visually guided behavior.</p>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>Twenty individuals participated in the No-Ping session (11 females, mean age = 22.9) and twenty individuals participated in the Ping session (14 females, mean age = 23.7). Among them, 14 participants took part in both sessions, while 12 of them took part in only one session. The sample size was comparable to previous studies using similar attention tasks (<xref ref-type="bibr" rid="c2">Baldauf &amp; Desimone, 2014</xref>; <xref ref-type="bibr" rid="c15">Gong &amp; Liu, 2020a</xref>; <xref ref-type="bibr" rid="c16">Gong et al., 2020b</xref>; <xref ref-type="bibr" rid="c19">Guo et al., 2012</xref>; <xref ref-type="bibr" rid="c28">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c37">Liu &amp; Hou, 2013</xref>). Because our primary interest is the generalization from the perception task to attention task, we used the minimal effect size of decoding accuracy across regions (one-sample t-tests: <italic>d</italic> = 0.868) from our previous study with a similar design (<xref ref-type="bibr" rid="c14">Gong et al., 2022</xref>), and used G*Power (Version 3.1) (<xref ref-type="bibr" rid="c12">Faul et al., 2007</xref>) to confirm that this sample size is sufficient to detect a cross-task generalization effect with a power greater than 95% (a = 0.05). All participants were right-handed and had a normal or corrected-to-normal vision. Participants provided written informed consent according to the study protocol approved by the Institutional Review Board at Zhejiang University (2020-06-001). They were paid ¥200 (∼$27.4) for their participation in each session.</p>
</sec>
<sec id="s4b">
<title>Stimuli and Apparatus</title>
<p>Stimuli were generated using Psychtoolbox (<xref ref-type="bibr" rid="c5">Brainard, 1997</xref>; <xref ref-type="bibr" rid="c31">Kleiner et al., 2007</xref>) implemented in MATLAB. The stimuli were presented on an LCD monitor (resolution: 1920 × 1080, refresh rate: 60 Hz) during behavioral training, at a viewing distance of 90 cm in a dark room. During the fMRI scans, stimuli were projected to a screen via a MR-compatible LCD projector (PT-011, Jiexin Technology Co., Ltd., Shenzhen, China) with the same resolution and refresh rate as the LCD monitor during behavioral training. Participants viewed the screen via an angled mirror attached to the head coil at a viewing distance of 115 cm. Angular stimulus size was the same across behavioral and fMRI sessions.</p>
<p>The orientation stimuli were square-wave gratings (1.3 cycles per deg, duty cycle: 10%) in a circular aperture (inner radius: 1.5°; outer radius: 6°). The gratings flashed on a gray background at 10 Hz, alternating between black and white. There were two types of stimuli: two overlapping gratings orientated leftward (∼135°) and rightward (∼45°), or a single grating with one of the two orientations (∼135° or ∼45°). Here, we refer to the 45° and 135° orientations as the reference orientations. The impulse stimulus was a high-contrast, white (at the maximum projector output level) circular disk that covered the same area as the orientation stimulus (radius: 6°).</p>
</sec>
<sec id="s4c">
<title>Experimental Procedures and Tasks</title>
<p>Each participant completed at least two fMRI sessions on different days. One session was used for defining ROIs (see Definition of Regions of Interests), while the remaining sessions were used for the main experiment (see Attention task and Perception task). Before the scanning sessions, participants were trained to familiarize with the tasks in a separate behavioral session. The procedures and tasks were similar to our previous work (<xref ref-type="bibr" rid="c28">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c14">Gong et al., 2022</xref>).</p>
<sec id="s4c1">
<title>Attention task</title>
<p>We used a cueing paradigm (<xref rid="fig1" ref-type="fig">Fig 1A</xref>). Each trial began with a color cue (red or green) for 0.5 s to indicate the reference orientation of the upcoming target (leftward vs. rightward orientation). In the No-Ping session, the cue was followed by a blank display during the preparation period; in the Ping session, a task-irrelevant, high-luminance visual impulse (“ping”, 0.1 s) occurred at either 0.5 s (for short delays of 1.5 s and 3.5 s) or 2.5 s (for long delays of 5.5 s and 7.5 s) after the onset of the cue display. The orders of these sessions were counterbalanced across participants who completed both. Following the preparatory period, two superimposed gratings were then shown for 1 s. The target grating was shown with a small angular offset with respect to the cued reference orientation, whereas the distractor grating was shown in the uncued reference orientation (e.g., if rightward orientation was cued, the rightward grating was shown in 45°± d and the leftward grating was shown in 135°). Note that the angular offset was determined individually based on the threshold obtained during the training session (at least 3 blocks, 30 trials/block), using a staircase procedure (Best Parameter Estimation by Sequential Testing, Best PEST), as implemented in the Palamedes Toolbox (<xref ref-type="bibr" rid="c45">Prins &amp; Kingdom, 2009</xref>). Participants used a keypad to report whether the attended orientation was more leftward or rightward relative to the reference orientation. Each trial was separated by an inter-trial interval of 3 s to 7 s (2 s per step). Trial-by-trial feedback (“correct” or “incorrect”) was provided in the training session but not during scanning. Instead, the percentage of correct responses was provided at the end of each run in the scanning session to avoid the impact of trial-level feedback on neural activity.</p>
<p>Given the need to maximize the number of trials for fMRI-based MVPA, we could not accomodate additional conditions (e.g., neutral cue) to measure the behavioral effects of attention. However, our prior work using similar feature cueing paradigms (<xref ref-type="bibr" rid="c36">Liu et al., 2007</xref>; <xref ref-type="bibr" rid="c28">Jigo et al., 2018</xref>) found that attentional cueing improved behavioral performance relative to a neutral condition. Thus, it is highly likely that our well-trained participants used the cue to direct their attention in the fMRI experiment. Furthermore, our neural measures of attentional signals revealed feature-specific attentional modulations, further validating our approach. To prevent the cue-related sensory difference from contributing to neural activity, we reversed the mapping between colors and orientations halfway through the experiment (e.g., red indicated “attend leftward orientation” and green indicated “attend rightward orientation” in the first half of the runs, and vice versa for the second half of the runs), with the order counterbalanced across subjects. The mapping of colors and orientations was reversed only once in the middle of the experiment to prevent misremembering of the color-orientation associations. To reduce temporal expectancy over a fixed period, the preparatory period (i.e., cue-to-stimulus interval) varied from 1.5 s to 7.5 s with different probabilities (10% for 1.5 s or 3.5 s each, 40% for 5.5 s or 7.5 s each). The long-delay trials (5.5 s or 7.5 s) were selected for subsequent analyses, as they allow the separation of the preparatory activity from the grating-evoked response during fMRI scanning. The short-delay trials were included to encourage a sustained maintenance of attention throughout the entire preparation period.</p>
</sec>
<sec id="s4c2">
<title>Perception task</title>
<p>On each trial of the perception task (<xref rid="fig1" ref-type="fig">Fig 1B</xref>), a single grating was shown for 1 s, followed by an inter-trial interval between 3 s to 7 s. To equate the sensory inputs between attention and perception tasks, the orientation was shifted away from the reference orientation by the same angular offset as that used in the attention task with each individual participant’s own threshold. Participants performed the same orientation discrimination task by comparing the single orientation to the reference orientation. We provided the percentage of correct responses at the end of each run as feedback.</p>
</sec>
</sec>
<sec id="s4d">
<title>Eye Tracking and Analysis</title>
<p>To evaluate the stability of visual fixation, we used Eyelink Portable Duo system (SR Research, Ontario, Canada) to monitor each observer’s eye position during the training session at a sampling rate of 500 Hz. One participant’s data was not used due to the unstable recording of the eye. The data were then analyzed using custom Matlab code.</p>
<p>To examine whether participants adopted a space-based strategy during the preparatory period in the attention task, such as directing their gaze leftward in attend leftward trials, and vice versa for the attend rightward trials, we analyzed the participants’ eye positions recorded during the training session. Horizontal and vertical eye positions were analyzed separately. Paired t-tests were performed to compare horizontal and vertical eye positions between two attention conditions. A two-way mixed ANOVA (2 sessions × 2 attended orientations) was applied to the horizontal and vertical positions, respectively.</p>
</sec>
<sec id="s4e">
<title>fMRI Data Acquisition</title>
<p>Imaging was performed on a Siemens 3T scanner (MAGNETOM Prisma, Siemens Healthcare, Erlangen, Germany) using a 20-channel coil at Zhejiang University (Hangzhou, China). For each participant, we acquired high-resolution T1-weighted anatomical images (field of view, 240 × 240 mm, 208 sagittal slices; 0.9 mm<sup>3</sup> resolution), T2*-weighted echo-planar functional images consisting of 46 slices (TR, 2 s; TE, 34 ms; flip angle, 50°; matrix size, 80 × 80; in-plane resolution, 3 × 3 mm; slice thickness, 3 mm, interleaved, no gap) and a 2D T1-weighted anatomical image (0.8 × 0.8 × 3 mm) for aligning functional data to high-resolution anatomical data.</p>
</sec>
<sec id="s4f">
<title>fMRI Data Preprocessing</title>
<p>Data analyses were performed using mrTools (<xref ref-type="bibr" rid="c13">Gardner et al., 2018</xref>) and custom code in Matlab. For each run, functional data were preprocessed with head motion correction, linear detrending and temporal high pass filtering at 0.01 Hz. Data were converted to percentage signal change by dividing the time course of each voxel by its mean signals in each run. We concatenated the 6 runs of the attention task and the 3 runs of the perception task separately for further analysis. One of the attention runs in one subject was excluded due to low accurate performance (&lt;50%).</p>
</sec>
<sec id="s4g">
<title>Definition of Regions of Interests (ROI)</title>
<sec id="s4g1">
<title>Visual and parietal ROIs</title>
<p>Following previous work (<xref ref-type="bibr" rid="c28">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c16">Gong et al., 2020b</xref>; <xref ref-type="bibr" rid="c14">Gong et al., 2022</xref>), for each observer, we ran a separate retinotopic mapping session to obtain ROIs in occipital and parietal areas. Observers viewed four runs of rotating wedges (i.e., clockwise and counterclockwise) and two runs of rings (i.e., expanding and contracting) to map the polar angle and radial components, respectively (<xref ref-type="bibr" rid="c9">DeYoe et al., 1996</xref>; <xref ref-type="bibr" rid="c11">Engel, 1997</xref>; <xref ref-type="bibr" rid="c54">Sereno et al., 1995</xref>). Borders between areas were defined as the phase reversals in a polar angle map of the visual field. Phase maps were visualized on computationally flattened representations of the cortical surface, which were generated from the high-resolution anatomical image using FreeSurfer (<ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu">http://surfer.nmr.mgh.harvard.edu</ext-link>) and custom Matlab code.</p>
<p>To help identify the topographic areas in parietal areas, we ran 2 runs of memory-guided saccade task modeled after previous studies (<xref ref-type="bibr" rid="c33">Konen &amp; Kastner, 2008</xref>; <xref ref-type="bibr" rid="c50">Schluppeck et al., 2006</xref>; <xref ref-type="bibr" rid="c55">Sereno et al., 2001</xref>). Observers fixated at the screen center while a peripheral (∼10° radius) target dot was flashed for 500 ms. The flashed target was quickly masked by a ring of 100 distractor dots randomly positioned within an annulus (8.5° – 10.5°). The mask remained on screen for 3 s, after which participants were instructed to make a saccade to the memorized target position, then immediately saccade back to the central fixation. The position of the peripheral target shifted around the annulus from trial to trial in either a clockwise or counterclockwise order. Data from the memory-guided saccade task were analyzed using the same phase encoding method as the wedge and ring data. Therefore, the following regions of interest (ROIs) in each hemisphere were identified after the completion of this session: V1, V2, V3, V3A/B, V4, V7/IPS0, IPS1 to IPS4.</p>
</sec>
<sec id="s4g2">
<title>Frontal ROIs</title>
<p>Following previous work (<xref ref-type="bibr" rid="c28">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c16">Gong et al., 2020b</xref>; <xref ref-type="bibr" rid="c14">Gong et al., 2022</xref>), we used a deconvolution approach by fitting each voxel’s time series from the attention task with a general linear model (GLM) to determine the event-related activations in the brain (see Supplementary Materials: Deconvolution). For each voxel, we computed the goodness of fit measure (r<sup>2</sup> value), which indicates the amount of variance explained by the deconvolution model (<xref ref-type="bibr" rid="c13">Gardner et al., 2018</xref>). The r<sup>2</sup> value represents the degree to which the voxel’s time series is correlated with the task events, regardless of any differential responses among conditions. Based on the task-related activation (as indexed by r<sup>2</sup> value) and anatomical criteria, we defined two frontal areas in each hemisphere that were active during the attention task: one is located superior to the precentral sulcus and near the superior frontal sulcus (FEF) and the other is located towards the inferior precentral sulcus, close to the junction with the inferior frontal sulcus (IFJ).</p>
</sec>
<sec id="s4g3">
<title>Groups of Region</title>
<p>To characterize the patterns of neural response across cortical hierarchy and streamline data presentation, we grouped results from the nine areas into four groups based on functional and anatomical considerations: primary visual cortex (V1); extrastriate visual cortex (EVC), consisting of V2, V3, V3ab and V4; IPS, consisting of IPS0 to IPS4; prefrontal cortex (PFC), consisting of FEF and IFJ. Individual areas within each group exhibited qualitatively similar results.</p>
<p>Note that we analyzed V1 separately for two reasons. First, previous studies consistently identify V1 as the main locus of sensory-like templates during feature-specific preparatory attention (<xref ref-type="bibr" rid="c32">Kok et al., 2014</xref>; <xref ref-type="bibr" rid="c1">Aitken et al., 2020</xref>). Second, V1 shows the strongest orientation selectivity within the visual hierarchy (<xref ref-type="bibr" rid="c44">Priebe, 2016</xref>). In contrast, the extrastriate visual cortex (EVC; comprising V2, V2, V3AB and V4) demonstrates broader selectivity for complex features (<xref ref-type="bibr" rid="c17">Grill-Spector &amp; Malach, 2004</xref>). Therefore, it would be particularly informative to analyze V1 separately for our orientation-based attention paradigm.</p>
</sec>
</sec>
<sec id="s4h">
<title>Multivoxel Pattern Analysis (MVPA)</title>
<sec id="s4h1">
<title>Decoding of attended orientation</title>
<p>To test if multivariate patterns of activity represent information of the attended orientation, separate MVPA analyses were applied on the activity patterns for the preparation and stimulus selection periods. Following previous work (<xref ref-type="bibr" rid="c28">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c16">Gong et al., 2020b</xref>; <xref ref-type="bibr" rid="c14">Gong et al., 2022</xref>), for this analysis, we extracted fMRI signals from raw time series in the long delay trials with correct behavioral responses (∼72 trials per attention condition); short delay trials were excluded as they could not provide enough data points to measure preparatory activity. We then obtained averaged BOLD response in a 2 s window for each voxel and each trial in a given ROI, separately for preparatory activity (4 to 6 s after the onset of the cue) and stimulus-evoked activity (4 to 6 s after the onset of the gratings). The response amplitudes across two attention conditions in each ROI were further z-normalized, separately for the preparation and stimulus-related activity. These normalized single-trial BOLD responses were used for the MVPA. We trained a classifier using the Fisher linear discriminant (FLD) analysis to discriminate between two attended orientations (leftward vs. rightward) and tested its performance with a leave-one-run-out cross-validation scheme. This process was repeated until each run was tested once and the decoding accuracy (i.e., the proportion of correctly classified trials) was averaged across the cross-validation folds. The statistical significance of decoding accuracy was evaluated by comparing it to the chance level obtained from a permutation test (see Permutation test). To assess if the decoding accuracy differed between No-Ping and Ping experiments, we performed two-way mixed ANOVAs (2 sessions × 4 regions) on the decoding accuracy.</p>
</sec>
<sec id="s4h2">
<title>Cross-task generalization from the perception task to attention task</title>
<p>Following previous work (<xref ref-type="bibr" rid="c28">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c14">Gong et al., 2022</xref>), to test whether the neural patterns in the preparatory and stimulus selection periods from the attention task reflected sensory processing of isolated features, we trained an FLD classifier using the normalized BOLD responses from the perception task (4 to 6 s after the trial onset) to discriminate leftward vs. rightward orientation. Then, we tested this classifier on the normalized response from the independent runs of attention task to discriminate between attend leftward vs. attend rightward orientations, separately for preparation and stimulus selection periods. The significance of decoding accuracy was compared to the chance level obtained from a permutation test (see Permutation test). To assess if the generalization performance differed between No-Ping and Ping sessions, we performed two-way mixed ANOVAs (2 sessions × 4 regions) on the decoding accuracy.</p>
</sec>
<sec id="s4h3">
<title>Neural distance between attended and perceived orientations</title>
<p>The decoding accuracy from the cross-task generalization test reflects a discretized readout of the pattern similarity between different conditions. However, employing continuous similarity measures, such as Mahalanobis distance (<xref ref-type="bibr" rid="c38">Mahalanobis, 1936</xref>), could be more reliable compared to decoding accuracy (<xref ref-type="bibr" rid="c64">Walther et al., 2016</xref>). Therefore, we calculated the Mahalanobis distance to quantify the pattern similarity between two attended orientations and two perceived orientations. For each participant and each ROI, we have M points (i.e., M trials for each attended orientation) in the N-dimensional space (N = 100, number of voxels). For each data point in the attended orientation condition, we computed its distance to each of the orientation distributions (from the perception task). Averaged distance values were then calculated for each combination of attended orientation and perceived orientation pairs. A sensory-like hypothesis would predict smaller distance between the distribution of the attended orientation (e.g., attend leftward) and the distribution of corresponding orientation (e.g., perceive leftward) compared to the alternative orientation (e.g., perceive rightward). Two-way repeated-measures ANOVA (2 attended orientations × 2 perceived orientations) was applied on the Mahalanobis distance, separately for each region and each session.</p>
</sec>
<sec id="s4h4">
<title>Neural-behavioral relationships</title>
<p>We tested if the representation format during preparatory attention was associated with subsequent behavior. For each trial, we calculated the Mahalanobis distance between the attention conditions (attend leftward and attend rightward) and the perceived orientations (leftward and rightward orientation). We estimated the attentional modulation index (AMI) based on these distance values. This index measures how much attention modulated the pattern similarity for the Same orientation condition (e.g., attend and perceive the same orientation) relative to the Different orientation condition (e.g., attend and perceive different orientations). The index was calculated as follows: AMI = (D<sub>different</sub> – D<sub>same</sub>)/(D<sub>different</sub> + D<sub>same</sub>), where D<sub>same</sub> and D<sub>different</sub> are the measured distance (D) in the Same and Different orientation condition, respectively. Next, we tested the behavioral relevance of AMI in two ways: (1) Inter-subject analysis: correlated AMI with RT and accuracy across participants, separately for each session; (2) Within-subject analysis: for each participant, we sorted the single-trial AMI values in descending order and selected top-ranked 25% trials and bottom-ranked 25% trials to represent “strong modulation” and “weak modulation” trials, respectively. We then extracted behavioral responses on these selected trials and calculated RT and accuracy for each trial type. Paired t-tests were used to compare between “strong modulation” and “weak modulation” trials in each session.</p>
</sec>
<sec id="s4h5">
<title>Informational connectivity analysis</title>
<p>We used Informational Connectivity (IC) to examine shared changes in pattern discriminability over time, a method that allows inference based on multivoxel pattern information rather than overall BOLD response (<xref ref-type="bibr" rid="c26">Jia et al., 2020</xref>; <xref ref-type="bibr" rid="c42">Ng et al., 2021</xref>). To track the flow of multivariate information across time (i.e., across trials), we measured the fluctuations (covariance) in pattern-based discriminability by calculating the Mahalanobis distance of each trial to the two attended orientations, using a leave-one-run-out cross-validation scheme. For each ROI, we calculated the Mahalanobis distance between the pattern of activity for each attention trial from one left-out run and the distribution of each attended orientation of the remaining runs. To quantify the degree of attentional modulation, we calculated the AMI using the same formula as mentioned above, where D<sub>same</sub> and D<sub>different</sub> are the measured distance (D) in the Same and Different condition. This index measures how much the pattern similarity increased for the same attention condition (e.g., attend leftward to attend leftward) relative to the different attention condition (e.g., attend leftward to attend rightward). A positive AMI indicates relative proximity to the same attention condition, whereas a negative AMI indicates relative proximity to the different attention condition. A time course of AMI values was generated across runs and pairwise correlated between ROIs using Pearson correlation analysis and Fisher z-transformed. Independent t-tests were used to compare the connectivity between No-Ping and Ping sessions. To assess the relationship between ICs and behavior, we correlated ICs with RT and accuracy across participants, separately for each session.</p>
</sec>
</sec>
<sec id="s4i">
<title>Permutation test to evaluate classifier performance</title>
<p>Following previous work (<xref ref-type="bibr" rid="c28">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c14">Gong et al., 2022</xref>), for each brain area, we evaluated the statistical significance of the observed decoding accuracy using a permutation test scheme. We first shuffled the trial labels in the training data and trained the same FLD classifier on the shuffled data. We then tested the classifier on the (unshuffled) test data to obtain decoding accuracy. For each ROI and each participant, we repeated this procedure 1000 times to compute a null distribution of decoding accuracy. To compute the group-level significance, we averaged the 20 null distributions to obtain a single null distribution of 1000 values for each ROI. To determine if the observed decoding accuracy significantly exceeds the chance level, we compared the observed value to the 95 percentiles of this group-level distribution (corresponding to <italic>p</italic> = 0.05). Note that these ROIs were pre-defined with strong priors as their activation in attention tasks has been consistently reported in the literature. Nevertheless, for those analyses where multiple comparisons were performed across regions, we applied a Bonferroni-correction to adjust the <italic>p</italic>-values.</p>
</sec>
<sec id="s4j">
<title>Bayesian analysis</title>
<p>To evaluate the strength of evidence for the null hypothesis, we conducted Bayesian analyses (<xref ref-type="bibr" rid="c63">Wagenmakers, 2007</xref>) using standard priors as implemented in JASP Version 0.17.1 (JASP <xref ref-type="bibr" rid="c24">Team, 2023</xref>). We performed Bayesian t-tests and computed Bayes factor (BF<sub>01</sub>) to compare between two attention conditions (attend leftward vs. attend rightward). Additionally, we used Bayesian repeated-measures ANOVA and computed the exclusion Bayes factors (BF<sub>excl</sub>) to assess the evidence for excluding specific effects across all models. A Bayes factor (BF) greater than 1 provides support for the null hypothesis. Specifically, a BF between 1 and 3 indicates weak evidence, a BF between 3 and 10 indicates moderate evidence, and a BF greater than 10 indicates strong evidence (<xref ref-type="bibr" rid="c62">Van Doorn et al., 2021</xref>).</p>
</sec>
<sec id="s4k">
<title>Approach to handle partially overlapped samples</title>
<p>Our study used partially overlapping samples, with 14 out of 20 participants completing both No-Ping and Ping sessions, while the remainder completed one of the two sessions. The most important analyses entailed assessing whether decoding accuracy was above chance, for which we used the permutation-based method (see above) within each session. Thus, these analyses were unaffected by the partially overlapping samples. In a few analyses where we compared across sessions, we used statistical tests treating “session” as a between-subject factor. We believe this is a reasonable approach, as a between-subject test is more conservative than a within-subject test, such that any significant effect emerged should be a genuine effect. To be certain, we also conducted additional analyses with “session” as a within-subject factor on the subset of data from the 14 participants who completed both sessions in a counterbalanced order. The results were highly similar to those reported in the main text.</p>
</sec>
</sec>

</body>
<back>
<sec sec-type="data-availability" id="d1e4271">
<title>Data Availability</title>
<p>All data, analyses, and task codes have been made publicly available via the Open Science Framework at <ext-link ext-link-type="uri" xlink:href="https://osf.io/ghaxv/">https://osf.io/ghaxv/</ext-link></p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by National Science and Technology Innovation 2030—Major Project 2021ZD0200409, National Natural Science Foundation of China (32371087, 32300855, 3200784), Fundamental Research Funds for the Central University (226-2024-00118), a grant from the MOE Frontiers Science Center for Brain Science &amp; Brain-Machine Integration at Zhejiang University and Non-profit Central Research Institute Fund of Chinese Academy of Medical Sciences 2023-PT310-01.</p>
</ack>
<sec id="d1e1537" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>Author contributions</title>
<p>Original idea by K.J. and M.G. Experiment programmed by Y.C. Data collected by Y.C. Analysis programmed by Y.C., K.J. and M.G. Manuscript drafted by Y.C., T.L., and M.G. Manuscript edited and revised by T.L., J. T., K.J., and M.G.</p>
</sec>
</sec>
<sec id="suppd1e1537" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e1528">
<label>Supplementary Information</label>
<media xlink:href="supplements/602176_file03.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aitken</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Menelaou</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Warrington</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Koolschijn</surname>, <given-names>R. S.</given-names></string-name>, <string-name><surname>Corbin</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Callaghan</surname>, <given-names>M. F.</given-names></string-name>, &amp; <string-name><surname>Kok</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Prior expectations evoke stimulus-specific activity in the deep layers of the primary visual cortex</article-title>. <source>PLOS Biology</source>, <volume>18</volume>(<issue>12</issue>), <fpage>e3001023</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3001023</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baldauf</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Desimone</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Neural mechanisms of object-based attention</article-title>. <source>Science</source>, <volume>344</volume>(<issue>6182</issue>), <fpage>424</fpage>–<lpage>427</lpage>. <pub-id pub-id-type="doi">10.1126/science.1247003</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barbosa</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lozano-Soldevilla</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Compte</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Pinging the brain with visual impulses reveals electrically active, not activity-silent, working memories</article-title>. <source>PLOS Biology</source>, <volume>19</volume>(<issue>10</issue>), <fpage>e3001436</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3001436</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Battistoni</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Stein</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Peelen</surname>, <given-names>M. V</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Preparatory attention in visual cortex</article-title>. <source>Annals of the New York Academy of Sciences</source>, <volume>1396</volume>(<issue>1</issue>), <fpage>92</fpage>–<lpage>107</lpage>. <pub-id pub-id-type="doi">10.1111/nyas.13320</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brainard</surname>, <given-names>D. H</given-names></string-name></person-group>. (<year>1997</year>). <article-title>The psychophysics toolbox</article-title>. <source>Spatial Vision</source>, <volume>10</volume>(<issue>4</issue>), <fpage>433</fpage>–<lpage>436</lpage>. <pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bressler</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Tang</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Sylvester</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Shulman</surname>, <given-names>G. L.</given-names></string-name>, &amp; <string-name><surname>Corbetta</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Top-down control of human visual cortex by frontal and parietal cortex in anticipatory visual spatial attention</article-title>. <source>The Journal of Neuroscience</source>, <volume>28</volume>(<issue>40</issue>), <fpage>10056</fpage>–<lpage>10061</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1776-08.2008</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Corbetta</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Shulman</surname>, <given-names>G. L</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Control of goal-directed and stimulus-driven attention in the brain</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>3</volume>(<issue>3</issue>), <fpage>201</fpage>–<lpage>215</lpage>. <pub-id pub-id-type="doi">10.1038/nrn755</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Duncan</surname>, <given-names>J</given-names></string-name></person-group>. (<year>1995</year>). <article-title>Neural mechanisms of selective visual attention</article-title>. <source>Annual Review of Neuroscience</source>, <volume>18</volume>(<issue>1</issue>), <fpage>193</fpage>–<lpage>222</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.001205</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DeYoe</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Carman</surname>, <given-names>G. J.</given-names></string-name>, <string-name><surname>Bandettini</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Glickman</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Wieser</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Cox</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Neitz</surname>, <given-names>J</given-names></string-name></person-group>. (<year>1996</year>). <article-title>Mapping striate and extrastriate visual areas in human cerebral cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>93</volume>(<issue>6</issue>), <fpage>2382</fpage>–<lpage>2386</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.93.6.2382</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duncan</surname>, <given-names>D. H.</given-names></string-name>, <string-name><surname>van Moorselaar</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Theeuwes</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Pinging the brain to reveal the hidden attentional priority map using encephalography</article-title>. <source>Nature Communications</source>, <volume>14</volume>(<issue>1</issue>), <fpage>4749</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-023-40405-8</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engel</surname>, <given-names>S</given-names></string-name></person-group>. (<year>1997</year>). <article-title>Retinotopic organization in human visual cortex and the spatial precision of functional MRI</article-title>. <source>Cerebral Cortex</source>, <volume>7</volume>(<issue>2</issue>), <fpage>181</fpage>–<lpage>192</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/7.2.181</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Faul</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Erdfelder</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Lang</surname>, <given-names>A. G.</given-names></string-name>, &amp; <string-name><surname>Buchner</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2007</year>). <article-title>G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences</article-title>. <source>Behavior Research Methods</source>, <volume>39</volume>(<issue>2</issue>), <fpage>175</fpage>–<lpage>191</lpage>. <pub-id pub-id-type="doi">10.3758/BF03193146</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gardner</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Merriam</surname>, <given-names>E. P.</given-names></string-name>, <string-name><surname>Schluppeck</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Besle</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Heeger</surname>, <given-names>D. J</given-names></string-name></person-group>. (<year>2018</year>). <article-title>mrTools: Analysis and visualization package for functional magnetic resonance imaging data (Version 4.7) [Computer software]</article-title>. <source>Zenodo</source>. <pub-id pub-id-type="doi">10.5281/ZENODO.1299483</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gong</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Preparatory attention to visual features primarily relies on non-sensory representation</article-title>. <source>Scientific Reports</source>, <volume>12</volume>(<issue>1</issue>), <fpage>21726</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-022-26104-2</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gong</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2020a</year>). <article-title>Biased neural representation of feature-based attention in the human frontoparietal network</article-title>. <source>Journal of Neuroscience</source>, <volume>40</volume>(<issue>43</issue>), <fpage>8386</fpage>–<lpage>8395</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0690-20.2020</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gong</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2020b</year>). <article-title>Continuous and discrete representations of feature-based attentional priority in human frontoparietal network</article-title>. <source>Cognitive Neuroscience</source>, <volume>11</volume>(<issue>1–2</issue>), <fpage>47</fpage>–<lpage>59</lpage>. <pub-id pub-id-type="doi">10.1080/17588928.2019.1601074</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Malach</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2004</year>). <article-title>The human visual cortex</article-title>. <source>Annual Review of Neuroscience</source>, <volume>27</volume>(<issue>1</issue>), <fpage>649</fpage>-<lpage>677</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144220</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grubert</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Eimer</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2018</year>). <article-title>The time course of target template activation processes during preparation for visual search</article-title>. <source>The Journal of Neuroscience</source>, <volume>38</volume>(<issue>44</issue>), <fpage>9527</fpage>–<lpage>9538</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0409-18.2018</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guo</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Preston</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Das</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Giesbrecht</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Eckstein</surname>, <given-names>M. P</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Feature-independent neural coding of target detection during search of natural scenes</article-title>. <source>Journal of Neuroscience</source>, <volume>32</volume>(<issue>28</issue>), <fpage>9499</fpage>– <lpage>9510</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5876-11.2012</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hamblin-Frohman</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Becker</surname>, <given-names>S. I</given-names></string-name></person-group>. (<year>2021</year>). <article-title>The attentional template in high and low similarity search: Optimal tuning or tuning to relations?</article-title> <source>Cognition</source>, <volume>212</volume>, <fpage>104732</fpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2021.104732</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harrison</surname>, <given-names>S. A.</given-names></string-name>, &amp; <string-name><surname>Tong</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title>. <source>Nature</source>, <volume>458</volume>(<issue>7238</issue>), <fpage>632</fpage>–<lpage>635</lpage>. <pub-id pub-id-type="doi">10.1038/nature07832</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hout</surname>, <given-names>M. C.</given-names></string-name>, &amp; <string-name><surname>Goldinger</surname>, <given-names>S. D</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Target templates: The precision of mental representations affects attentional guidance and decision-making in visual search</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>, <volume>77</volume>(<issue>1</issue>), <fpage>128</fpage>–<lpage>149</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-014-0764-6</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hout</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Robbins</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Godwin</surname>, <given-names>H. J.</given-names></string-name>, <string-name><surname>Fitzsimmons</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Scarince</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Categorical templates are more useful when features are consistent: Evidence from eye movements during search for societally important vehicles</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>, <volume>79</volume>(<issue>6</issue>), <fpage>1578</fpage>–<lpage>1592</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-017-1354-1</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><given-names>JASP</given-names> <surname>Team</surname></string-name></person-group>. (<year>2023</year>). <article-title>JASP (Version 0.17.1) [Computer software]</article-title>. <ext-link ext-link-type="uri" xlink:href="https://jasp-stats.org/">https://jasp-stats.org/</ext-link></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jia</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Goebel</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Kourtzi</surname>, <given-names>Z</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Ultra-high field imaging of human visual cognition</article-title>. <source>Annual Review of Vision Science</source>, <volume>9</volume>(<issue>1</issue>), <fpage>479</fpage>–<lpage>500</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-vision-111022-123830</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jia</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zamboni</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Kemper</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Rua</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Goncalves</surname>, <given-names>N. R.</given-names></string-name>, <string-name><surname>Ng</surname>, <given-names>A. K. T.</given-names></string-name>, <string-name><surname>Rodgers</surname>, <given-names>C. T.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Goebel</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Kourtzi</surname>, <given-names>Z</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Recurrent processing drives perceptual plasticity</article-title>. <source>Current Biology</source>, <volume>30</volume>(<issue>21</issue>), <fpage>4177</fpage>–<lpage>4187.e4</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2020.08.016</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jia</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Steinwurzel</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Ziminski</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Xi</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Emir</surname>, <given-names>U.</given-names></string-name>, &amp; <string-name><surname>Kourtzi</surname>, <given-names>Z</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Recurrent inhibition refines mental templates to optimize perceptual decisions</article-title>. <source>Science Advances</source>, <volume>10</volume>(<issue>31</issue>), <fpage>eado7378</fpage>. <pub-id pub-id-type="doi">10.1126/sciadv.ado7378</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jigo</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gong</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Neural determinants of task performance during feature-based attention in human cortex</article-title>. <source>eNeuro</source>, <volume>5</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1523/ENEURO.0375-17.2018</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kastner</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pinsk</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>De Weerd</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Ungerleider</surname>, <given-names>L. G.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Increased activity in human visual cortex during directed attention in the absence of visual stimulation</article-title>. <source>Neuron</source>, <volume>22</volume>(<issue>4</issue>), <fpage>751</fpage>–<lpage>761</lpage>. <pub-id pub-id-type="doi">10.1016/S0896-6273(00)80734-5</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kerzel</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2019</year>). <article-title>The precision of attentional selection is far worse than the precision of the underlying memory representation</article-title>. <source>Cognition</source>, <volume>186</volume>, <fpage>20</fpage>–<lpage>31</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2019.02.001</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kleiner</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brainard</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Pelli</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2007</year>). <article-title>What’s new in Psychtoolbox-3?</article-title> <source>Perception</source>, <volume>36</volume>, <fpage>1</fpage>–<lpage>16</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Failing</surname>, <given-names>M. F.</given-names></string-name>, &amp; <string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Prior expectations evoke stimulus templates in the primary visual cortex</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>26</volume>(<issue>7</issue>), <fpage>1546</fpage>–<lpage>1554</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_00562</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Konen</surname>, <given-names>C. S.</given-names></string-name>, &amp; <string-name><surname>Kastner</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Representation of eye movements and stimulus motion in topographically organized areas of human posterior parietal cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>28</volume>(<issue>33</issue>), <fpage>8361</fpage>–<lpage>8375</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1930-08.2008</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kong</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Alais</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Van Der Burg</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Orientation categories used in guidance of attention in visual search can differ in strength</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>, <volume>79</volume>(<issue>8</issue>), <fpage>2246</fpage>–<lpage>2256</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-017-1387-5</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lewis-Peacock</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Drysdale</surname>, <given-names>A. T.</given-names></string-name>, &amp; <string-name><surname>Postle</surname>, <given-names>B. R</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Neural evidence for the flexible control of mental representations</article-title>. <source>Cerebral Cortex</source>, <volume>25</volume>(<issue>10</issue>), <fpage>3303</fpage>–<lpage>3313</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhu130</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Larsson</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Carrasco</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Feature-based attention modulates orientation-selective responses in human visual cortex</article-title>. <source>Neuron</source>, <volume>55</volume>(<issue>2</issue>), <fpage>313</fpage>–<lpage>323</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.030</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Hou</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2013</year>). <article-title>A hierarchy of attentional priority signals in human frontoparietal cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume>(<issue>42</issue>), <fpage>16606</fpage>–<lpage>16616</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1780-13.2013</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mahalanobis</surname>, <given-names>P. C</given-names></string-name></person-group>. (<year>1936</year>). <article-title>On the generalised distance in statistics</article-title>. <source>Proceedings of the National Institute of Sciences of India</source>, <volume>2</volume>, <fpage>49</fpage>–<lpage>55</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Malcolm</surname>, <given-names>G. L.</given-names></string-name>, &amp; <string-name><surname>Henderson</surname>, <given-names>J. M</given-names></string-name></person-group>. (<year>2009</year>). <article-title>The effects of target template specificity on visual search in real-world scenes: Evidence from eye movements</article-title>. <source>Journal of Vision</source>, <volume>9</volume>(<issue>11</issue>), <fpage>8</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1167/9.11.8</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mongillo</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Barak</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Tsodyks</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Synaptic theory of working memory</article-title>. <source>Science</source>, <volume>319</volume>(<issue>5869</issue>), <fpage>1543</fpage>–<lpage>1546</lpage>. <pub-id pub-id-type="doi">10.1126/science.1150769</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Myers</surname>, <given-names>N. E.</given-names></string-name>, <string-name><surname>Walther</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Wallis</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Stokes</surname>, <given-names>M. G.</given-names></string-name>, &amp; <string-name><surname>Nobre</surname>, <given-names>A. C</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Temporal dynamics of attention during encoding versus maintenance of working memory: complementary views from event-related potentials and alpha-band oscillations</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>27</volume>(<issue>3</issue>), <fpage>492</fpage>–<lpage>508</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_00727</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ng</surname>, <given-names>A. K. T.</given-names></string-name>, <string-name><surname>Jia</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Goncalves</surname>, <given-names>N. R.</given-names></string-name>, <string-name><surname>Zamboni</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Kemper</surname>, <given-names>V. G.</given-names></string-name>, <string-name><surname>Goebel</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Welchman</surname>, <given-names>A. E.</given-names></string-name>, &amp; <string-name><surname>Kourtzi</surname>, <given-names>Z.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Ultra-High-Field neuroimaging reveals fine-scale processing for 3D perception</article-title>. <source>The Journal of Neuroscience</source>, <volume>41</volume>(<issue>40</issue>), <fpage>8362</fpage>–<lpage>8374</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0065-21.2021</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name>, &amp; <string-name><surname>Kastner</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2011</year>). <article-title>A neural basis for real-world visual search in human occipitotemporal cortex</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>108</volume>(<issue>29</issue>), <fpage>12125</fpage>–<lpage>12130</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1101042108</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Priebe</surname>, <given-names>N. J</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Mechanisms of orientation selectivity in the primary visual cortex</article-title>. <source>Annual Review of Vision Science</source>, <volume>2</volume>(<issue>1</issue>), <fpage>85</fpage>–<lpage>107</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-vision-111815-114456</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Prins</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Kingdom</surname>, <given-names>F. A. A.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Palamedes: Matlab routines for analyzing psychophysical data</article-title>. <ext-link ext-link-type="uri" xlink:href="http://www.palamedestoolbox.org">http://www.palamedestoolbox.org</ext-link></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rademaker</surname>, <given-names>R. L.</given-names></string-name>, &amp; <string-name><surname>Serences</surname>, <given-names>J. T</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Pinging the brain to reveal hidden memories</article-title>. <source>Nature Neuroscience</source>, <volume>20</volume>(<issue>6</issue>), <fpage>767</fpage>–<lpage>769</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4560</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rademaker</surname>, <given-names>R. L.</given-names></string-name>, <string-name><surname>Chunharas</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Serences</surname>, <given-names>J. T</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Coexisting representations of sensory and mnemonic information in human visual cortex</article-title>. <source>Nature Neuroscience</source>, <volume>22</volume>(<issue>8</issue>), <fpage>1336</fpage>–<lpage>1344</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-019-0428-x</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rosenberg</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Scheinost</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Greene</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Avery</surname>, <given-names>E. W.</given-names></string-name>, <string-name><surname>Kwon</surname>, <given-names>Y. H.</given-names></string-name>, <string-name><surname>Finn</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Ramani</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Qiu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Constable</surname>, <given-names>R. T.</given-names></string-name>, &amp; <string-name><surname>Chun</surname>, <given-names>M. M</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Functional connectivity predicts changes in attention observed across minutes, days, and months</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>117</volume>(<issue>7</issue>), <fpage>3797</fpage>–<lpage>3807</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1912226117</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rungratsameetaweemana</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Serences</surname>, <given-names>J. T</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Dissociating the impact of attention and expectation on early sensory processing</article-title>. <source>Current Opinion in Psychology</source>, <volume>29</volume>, <fpage>181</fpage>–<lpage>186</lpage>. <pub-id pub-id-type="doi">10.1016/j.copsyc.2019.03.014</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schluppeck</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Curtis</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Glimcher</surname>, <given-names>P. W.</given-names></string-name>, &amp; <string-name><surname>Heeger</surname>, <given-names>D. J</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Sustained activity in topographic areas of human posterior parietal cortex during memory-guided saccades</article-title>. <source>The Journal of Neuroscience</source>, <volume>26</volume>(<issue>19</issue>), <fpage>5098</fpage>–<lpage>5108</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5330-05.2006</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schneegans</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Bays</surname>, <given-names>P. M</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Restoration of fMRI decodability does not imply latent working memory states</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>29</volume>(<issue>12</issue>), <fpage>1977</fpage>–<lpage>1994</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_01180</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scolari</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Byers</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Serences</surname>, <given-names>J. T</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Optimal deployment of attentional gain during fine discriminations</article-title>. <source>The Journal of Neuroscience</source>, <volume>32</volume>(<issue>22</issue>), <fpage>7723</fpage>–<lpage>7733</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5558-11.2012</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Serences</surname>, <given-names>J. T.</given-names></string-name>, <string-name><surname>Ester</surname>, <given-names>E. F.</given-names></string-name>, <string-name><surname>Vogel</surname>, <given-names>E. K.</given-names></string-name>, &amp; <string-name><surname>Awh</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Stimulus-specific delay activity in human primary visual cortex</article-title>. <source>Psychological Science</source>, <volume>20</volume>(<issue>2</issue>), <fpage>207</fpage>–<lpage>214</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02276.x</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sereno</surname>, <given-names>M. I.</given-names></string-name>, <string-name><surname>Dale</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Reppas</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Kwong</surname>, <given-names>K. K.</given-names></string-name>, <string-name><surname>Belliveau</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Brady</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Rosen</surname>, <given-names>B. R.</given-names></string-name>, &amp; <string-name><surname>Tootell</surname>, <given-names>R. B. H</given-names></string-name></person-group>. (<year>1995</year>). <article-title>Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title>. <source>Science</source>, <volume>268</volume>(<issue>5212</issue>), <fpage>889</fpage>–<lpage>893</lpage>. <pub-id pub-id-type="doi">10.1126/science.7754376</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sereno</surname>, <given-names>M. I.</given-names></string-name>, <string-name><surname>Pitzalis</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Martinez</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2001</year>). <article-title>Mapping of contralateral space in retinotopic coordinates by a parietal cortical area in humans</article-title>. <source>Science</source>, <volume>294</volume>(<issue>5545</issue>), <fpage>1350</fpage>–<lpage>1354</lpage>. <pub-id pub-id-type="doi">10.1126/science.1063695</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname>, <given-names>M. G</given-names></string-name></person-group>. (<year>2015</year>). <article-title>‘Activity-silent’ working memory in prefrontal cortex: A dynamic coding framework</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>19</volume>(<issue>7</issue>), <fpage>394</fpage>–<lpage>405</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2015.05.004</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Muhle-Karbe</surname>, <given-names>P. S.</given-names></string-name>, &amp; <string-name><surname>Myers</surname>, <given-names>N. E</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Theoretical distinction between functional states in working memory and their corresponding neural states</article-title>. <source>Visual Cognition</source>, <volume>28</volume>(<issue>5–8</issue>), <fpage>420</fpage>–<lpage>432</lpage>. <pub-id pub-id-type="doi">10.1080/13506285.2020.1825141</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Thompson</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name>, &amp; <string-name><surname>Duncan</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Shape-specific preparatory activity mediates attention to targets in human visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>106</volume>(<issue>46</issue>), <fpage>19569</fpage>–<lpage>19574</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0905306106</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Expectation in perceptual decision making: Neural and computational mechanisms</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>15</volume>(<issue>11</issue>), <fpage>745</fpage>–<lpage>756</lpage>. <pub-id pub-id-type="doi">10.1038/nrn3838</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Egner</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Expectation (and attention) in visual cognition</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>13</volume>(<issue>9</issue>), <fpage>403</fpage>–<lpage>409</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2009.06.003</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Egner</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Feature-based attention and feature-based expectation</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>20</volume>(<issue>6</issue>), <fpage>401</fpage>–<lpage>404</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2016.03.008</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Doorn</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Van Den Bergh</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Böhm</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Dablander</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Derks</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Draws</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Etz</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Evans</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Gronau</surname>, <given-names>Q. F.</given-names></string-name>, <string-name><surname>Haaf</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Hinne</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kucharský</surname>, <given-names>Š.</given-names></string-name>, <string-name><surname>Ly</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Marsman</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Matzke</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Gupta</surname>, <given-names>A. R. K. N.</given-names></string-name>, <string-name><surname>Sarafoglou</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Stefan</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Voelkel</surname>, <given-names>J. G.</given-names></string-name>, &amp; <string-name><surname>Wagenmakers</surname>, <given-names>E. J.</given-names></string-name></person-group> (<year>2021</year>). <article-title>The JASP guidelines for conducting and reporting a Bayesian analysis</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>28</volume>(<issue>3</issue>), <fpage>813</fpage>–<lpage>826</lpage>. <pub-id pub-id-type="doi">10.3758/s13423-020-01798-5</pub-id></mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wagenmakers</surname>, <given-names>E. J</given-names></string-name></person-group>. (<year>2007</year>). <article-title>A practical solution to the pervasive problems ofp values</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>14</volume>(<issue>5</issue>), <fpage>779</fpage>–<lpage>804</lpage>. <pub-id pub-id-type="doi">10.3758/BF03194105</pub-id></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Walther</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Nili</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ejaz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Alink</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Diedrichsen</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Reliability of dissimilarity measures for multi-voxel pattern analysis</article-title>. <source>NeuroImage</source>, <volume>137</volume>, <fpage>188</fpage>–<lpage>200</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.012</pub-id></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Duncan</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Mitchell</surname>, <given-names>D. J</given-names></string-name></person-group>. (<year>2019</year>). <article-title>The time-course of component processes of selective attention</article-title>. <source>NeuroImage</source>, <volume>199</volume>, <fpage>396</fpage>–<lpage>407</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.05.067</pub-id></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolfe</surname>, <given-names>J. M</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Guided Search 6.0: An updated model of visual search</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>28</volume>(<issue>4</issue>), <fpage>1060</fpage>–<lpage>1092</lpage>. <pub-id pub-id-type="doi">10.3758/s13423-020-01859-9</pub-id></mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolfe</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Friedman-Hill</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Stewart</surname>, <given-names>M. I.</given-names></string-name>, &amp; <string-name><surname>O’Connell</surname>, <given-names>K. M</given-names></string-name></person-group>. (<year>1992</year>). <article-title>The role of categorization in visual search for orientation</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>18</volume>(<issue>1</issue>), <fpage>34</fpage>–<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1037/0096-1523.18.1.34</pub-id></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolff</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Myers</surname>, <given-names>N. E.</given-names></string-name>, &amp; <string-name><surname>Stokes</surname>, <given-names>M. G</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Revealing hidden states in visual working memory using electroencephalography</article-title>. <source>Frontiers in Systems Neuroscience</source>, <volume>9</volume>, <fpage>123</fpage>. <pub-id pub-id-type="doi">10.3389/fnsys.2015.00123</pub-id></mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolff</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Jochim</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Akyürek</surname>, <given-names>E. G.</given-names></string-name>, &amp; <string-name><surname>Stokes</surname>, <given-names>M. G</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Dynamic hidden states underlying working-memory-guided behavior</article-title>. <source>Nature Neuroscience</source>, <volume>20</volume>(<issue>6</issue>), <fpage>864</fpage>–<lpage>871</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4546</pub-id></mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolff</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Kandemir</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Stokes</surname>, <given-names>M. G.</given-names></string-name>, &amp; <string-name><surname>Akyürek</surname>, <given-names>E. G</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Unimodal and bimodal access to sensory working memories by auditory and visual impulses</article-title>. <source>Journal of Neuroscience</source>, <volume>40</volume>(<issue>3</issue>), <fpage>671</fpage>–<lpage>681</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1194-19.2019</pub-id></mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Hanks</surname>, <given-names>T. D.</given-names></string-name>, &amp; <string-name><surname>Geng</surname>, <given-names>J. J</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Attentional guidance and match decisions rely on different template information during visual search</article-title>. <source>Psychological Science</source>, <volume>33</volume>(<issue>1</issue>), <fpage>105</fpage>–<lpage>120</lpage>. <pub-id pub-id-type="doi">10.1177/09567976211032225</pub-id></mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Becker</surname>, <given-names>S. I.</given-names></string-name>, <string-name><surname>Boettcher</surname>, <given-names>S. E. P.</given-names></string-name>, &amp; <string-name><surname>Geng</surname>, <given-names>J. J</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Good-enough attentional guidance</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>27</volume>(<issue>4</issue>), <fpage>391</fpage>–<lpage>403</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2023.01.007</pub-id></mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Luo</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Feature-specific reactivations of past information shift current neural encoding thereby mediating serial bias behaviors</article-title>. <source>PLOS Biology</source>, <volume>21</volume>(<issue>3</issue>), <fpage>e3002056</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3002056</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103425.2.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Xilin</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>South China Normal University</institution>
</institution-wrap>
<city>Guangzhou</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>By combining the 'pinging' technique with fMRI-based multivariate pattern analysis, this <bold>important</bold> study provides <bold>convincing</bold> evidence for a dual-format of attentional representation during preparatory period. The result reconciles the competing views between the sensory-like versus non-sensory accounts of attentional template and advances our understanding of how the brain flexibly utilizes different versions of template to guide attention. This work will be of interest to researchers in psychology, vision science, and cognitive science.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103425.2.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The aim of the experiment reported in this paper is to examine the nature of the representation of a template of an upcoming target. To this end, participants were presented with compound gratings (consisting of tilted to the right and tilted to the left lines) and were cued to a particular orientation - red left tilt or blue right tilt (counterbalanced across participants). There are two directly compared conditions: (i) no ping: where there was a cue, that was followed by a 5.5-7.5s delay, then followed by a target grating in which the cued orientation deviated from the standard 45 degrees; and (ii) ping condition in which all aspects were the same with the only difference that a ping (visual impulse presented for 100ms) was presented after the 2.5 seconds following the cue. There was also a perception task in which only the 45 degrees to the right or to the left lines were presented. It was observed that during the delay, only in the ping condition, were the authors able to decode the orientation of the to-be-reported target using the cross-task generalization. Attention decoding, on the other hand, was decoded in both ping and non-ping conditions. It is concluded that the visual system has two different functional states associated with a template during preparation: a predominantly non-sensory representation for guidance and a latent sensory-like for prospective stimulus processing.</p>
<p>Strengths:</p>
<p>There is so much to be impressed with in this report. The writing of the manuscript is incredibly clear. The experimental design is clever and innovative. The analysis is sophisticated and also innovative - the cross-task decoding, the use of Mahalanobis distance as a function of representational similarity, the fact that the question is theoretically interesting, and the excellent figures.</p>
<p>Weaknesses:</p>
<p>While I think that this is an interesting study that addresses an important theoretical question, I have several concerns about the experimental paradigm and the subsequent conclusions that can be drawn.</p>
<p>(1) Why was V1 separated from the rest of the visual cortex, and why the rest of the areas were simply lumped into an EVC ROI? It would be helpful to understand the separation into ROIs.</p>
<p>(2) It would have been helpful to have a behavioral measure of the &quot;attended&quot; orientation to show that participants in fact attended to a particular orientation and were faster in the cued condition. The cue here was 100% valid, so no such behavioral measure of attention is available here.</p>
<p>(3) As I was reading the manuscript I kept thinking that the word attention in this manuscript can be easily replaced with visual working memory. Have the authors considered what it is about their task or cognitive demand that makes this investigation about attention or working memory?</p>
<p>(4) If I understand correctly, the only ROI that showed a significant difference for the cross-task generalization is V1. Was it predicted that only V1 would have two functional states? It should also be made clear that the only difference where the two states differ is V1.</p>
<p>(5) My primary concern about the interpretation of the finding is that the result, differences in cross-task decoding within V1 between the ping and no-ping condition might simply be explained by the fact that the ping condition refocuses attention during the long delay thus &quot;resharpening&quot; the template. In the no-ping condition during the 5.5 to 7.5 seconds long delay, attention for orientation might start getting less &quot;crisp.&quot; In the ping condition, however, the ping itself might simply serve to refocus attention. So, the result is not showing the difference between the latent and non-latent stages, rather it is the difference between a decaying template representation and a representation during the refocused attentional state. It is important to address this point. Would a simple tone during the delay do the same? If so, the interpretation of the results will be different.</p>
<p>(6) The neural pattern distances measured using Mahalanobis values are really great! Have the authors tried to use all of the data, rather than the high AMI and low AMI to possibly show a linear relationship between response times and AMI?</p>
<p>(7) After reading the whole manuscript I still don't understand what the authors think the ping is actually doing, mechanistically. I would have liked a more thorough discussion, rather than referencing previous papers (all by the co-author).</p>
<p>Comments on revisions:</p>
<p>I am impressed with the thoroughness with which the authors addressed my concerns. I don't have any further concerns and think that this paper makes an interesting and significant contribution to our understanding of VWM. I would only suggest adding citations to the newly added paragraph where the authors state &quot;It could be argued that preparatory attention relies on the same mechanisms as working memory maintenance.&quot; They could cite work by Bettencourt and Xu, 2016; and Sheremata, Somers, and Shomstein (2018).</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103425.2.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In the present study, the authors investigated the nature of attentional templates during preparatory period of goal-directed attention. By combing the use of 'pinging' the neural activity with a visual impulse and fMRI-based multivariate decoding, the authors found that the nature of the neural representations of the prospective feature target during preparatory period was contingent on the presence of the 'pinging' impulse. While the preparatory representations contained highly similar information content as the perceptual representations when the pinging impulse was introduced, they fundamentally differed from perceptual representations in the absence of the pinging impulse. Based on these findings, the authors proposed a dual-format mechanism in which both a &quot;non-sensory&quot; template and a latent &quot;sensory&quot; template coexisted during attentional preparation. The former actively guides activity in the preparatory state, and the latter is utilized for future stimulus processing.</p>
<p>Strengths:</p>
<p>Overall, I think that the authors' revision has addressed most, if not all, of my major concerns noted in my previous comments.</p>
<p>Weaknesses:</p>
<p>The results appear convincing and I do not have additional comments.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103425.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This paper discusses how non-sensory and latent, sensory-like attentional templates are represented during attentional preparation. Using multivariate pattern analysis, they found that visual impulses can enhance the decoding generalization from perception to attention tasks in the preparatory stage in the visual cortex. Furthermore, the emergence of the sensory-like template coincided with enhanced information connectivity between V1 and frontoparietal areas and was associated with improved behavioral performance. It is an interesting paper with supporting evidence for the latent, sensory-like attentional template.</p>
<p>(1) The authors addressed most of my previous concerns and provided additional data analysis. They conducted further analyses to demonstrate that the observed changes in network communication are associated with behavioral RTs, supporting the idea that the impulse-driven sensory-like template enhances informational connectivity between sensory and frontoparietal areas, and relates to behavior.</p>
<p>(2) I would like to further clarify my previous points regarding the definition of the two types of templates and the evidence for their coexistence. The authors stated that the sensory-like template likely existed in a latent state and was reactivated by visual pings, proposing that sensory and non-sensory templates coexist. However, it remains unclear whether this reflects a dynamic switch between formats or true coexistence. If the templates are non-sensory in nature, what exactly do they represent? Are they meant to be abstract or conceptual representations, or, put simply, just &quot;top-down attentional information&quot;? If so, why did the generalization analyses-training classifiers on activity during the stimulus selection period and testing on preparatory activity-fail to yield significant results? While the stimulus selection period necessarily encodes both target and distractor information, it should still contain attentional information. I would appreciate more discussion from this perspective.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103425.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Chen</surname>
<given-names>Yilin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Liu</surname>
<given-names>Taosheng</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jia</surname>
<given-names>Ke</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Theeuwes</surname>
<given-names>Jan</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Gong</surname>
<given-names>Mengyuan</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0333-4957</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>(1) Why was V1 separated from the rest of the visual cortex, and why the rest of the areas were simply lumped into an EVC ROI? It would be helpful to understand the separation into ROIs.</p>
</disp-quote>
<p>We thank the reviewer for raising the concerns regarding the definition of ROI. Our approach to analyze V1 separately was based on two key considerations. First, previous studies consistently identify V1 as the main locus of sensory-like templates during featurespecific preparatory attention (Kok et al., 2014; Aitken et al., 2020). Second, V1 shows the strongest orientation selectivity within the visual hierarchy (Priebe, 2016). In contrast, the extrastriate visual cortex (EVC; comprising V2, V2, V3AB and V4) demonstrates broader selectivity, such as complex features like contour and texture (Grill-Spector &amp; Malach, 2004). Thus, we think it would be particularly informative to analyze V1 data separately as our experiment examines orientation-based attention. We should also note that we conducted MVPA separately for each visual ROIs (V2, V3, V3AB and V4). After observing similar patterns of results across these regions, we averaged the decoding accuracies into a single value and labeled it as EVC. This approach allowed us to simplify data presentation while preserving the overall data pattern in decoding performance. We now added the related explanations on the ROI definition in the revised texts (Page 26; Line 576-581).</p>
<disp-quote content-type="editor-comment">
<p>(2) It would have been helpful to have a behavioral measure of the &quot;attended&quot; orientation to show that participants in fact attended to a particular orientation and were faster in the cued condition. The cue here was 100% valid, so no such behavioral measure of attention is available here.</p>
</disp-quote>
<p>We thank the reviewer for the comments. We agree that including valid and neutral cue trials would have provided valuable behavioral measures of attention; Yet, our current design was aimed at maximizing the number of trials for decoding analysis due to fMRI time constraints. Thus, we could not fit additional conditions to measure the behavioral effects of attention. However, we note that in our previous studies using a similar feature cueing paradigm, we observed benefits of attentional cueing on behavioral performance when comparing valid and neutral conditions (Liu et al., 2007; Jigo et al., 2018). Furthermore, our neural data indeed demonstrated attention-related modulation (as indicated by MVPA results, Fig. 2 in the main texts) so we are confident that on average participants followed the instruction and deployed their attention accordingly. We now added the related explanations on this point in the revised texts (Page 23; Line 492-498).</p>
<disp-quote content-type="editor-comment">
<p>(3) As I was reading the manuscript I kept thinking that the word attention in this manuscript can be easily replaced with visual working memory. Have the authors considered what it is about their task or cognitive demand that makes this investigation about attention or working memory?</p>
</disp-quote>
<p>We thank the reviewer for this comment. We added the following extensive discussion on this point in the revised texts (Page 18; Line 363-381).</p>
<p>“It could be argued that preparatory attention relies on the same mechanisms as working memory maintenance. While these functions are intuitively similar and likely overlap, there is also evidence indicating that they can be dissociated (Battistoni et al., 2017). In particular, we note that in our task, attention is guided by symbolic cues (color-orientation associations), while working memory tasks typically present the actual visual stimulus as the memorandum. A central finding in working memory studies is that neural signals during WM maintenance are sensory in nature, as demonstrated by generalizable neural activity patterns from stimulus encoding to maintenance in visual cortex (Harrison &amp; Tong, 2009; Serences et al., 2009; Rademaker et al., 2019). However, in our task, neural signals during preparation were nonsensory, as demonstrated by a lack of such generalization in the No-Ping session (see also Gong et al., 2022). We believe that the differences in cue format and task demand in these studies may account for such differences. In addition to the difference in the sensory nature of the preparatory versus delay-period activity, our ping-related results also exhibited divergence from working memory studies (Wolff et al., 2017; 2020). While these studies used the visual impulse to differentiate active and latent representations of different items (e.g., attended vs. unattended memory item), our study demonstrated the active and latent representations of a single item in different formats (i.e., non-sensory vs. sensory-like). Moreover, unlike our study, the impulse did not evoke sensory-like neural patterns during memory retention (Wolff et al., 2017). These observations suggest that the cognitive and neural processes underlying preparatory attention and working memory maintenance could very well diverge. Future studies are necessary to delineate the relationship between these functions both at the behavioral and neural level.”</p>
<disp-quote content-type="editor-comment">
<p>(4) If I understand correctly, the only ROI that showed a significant difference for the crosstask generalization is V1. Was it predicted that only V1 would have two functional states? It should also be made clear that the only difference where the two states differ is V1.</p>
</disp-quote>
<p>We thank the reviewer for this comment. We would like to clarify that our analyses revealed similar patterns of preparatory attentional representations in V1 and EVC. During the Ping session, the cross-task generalization analyses revealed decodable information in both V1 and EVC (<italic>ps</italic> &lt; 0.001), significantly higher than that in the No-Ping session for V1 (independent t-test: t(38) = 3.145, p = 0.003; Cohen’s d = 0.995) and EVC (independent t-test: t(38) = 2.153, p = 0.038, Cohen’s d = 0.681) (Page 10; Line 194-196). While both areas maintained similar representations, additional measures (Mahalanobis distance, neural-behavior relationship and connectivity changes) showed more robust ping-evoked changes in V1 compared to EVC. This differential pattern likely reflects the primary role of V1 in orientation processing, with EVC showing a similar but weaker response profile. We have revised the text to clarity this point (Page 16; Line 327-329).</p>
<disp-quote content-type="editor-comment">
<p>(5) My primary concern about the interpretation of the finding is that the result, differences in cross-task decoding within V1 between the ping and no-ping condition might simply be explained by the fact that the ping condition refocuses attention during the long delay thus &quot;resharpening&quot; the template. In the no-ping condition during the 5.5 to 7.5 seconds long delay, attention for orientation might start getting less &quot;crisp.&quot; In the ping condition, however, the ping itself might simply serve to refocus attention. So, the result is not showing the difference between the latent and non-latent stages, rather it is the difference between a decaying template representation and a representation during the refocused attentional state. It is important to address this point. Would a simple tone during the delay do the same? If so, the interpretation of the results will be different.</p>
</disp-quote>
<p>We thank the reviewer for this comment. The reviewer proposed an alternative account suggesting that visual pings may function to refocus attention, rather than reactivate latent information during the preparatory period. If this account holds (i.e., attention became weaker in the no-ping condition and it was strengthened by the ping due to re-focusing), we would expect to observe a general enhancement of attentional decoding during the preparatory period. However, our data reveal no significant differences in overall attention decoding between two conditions during this period (<italic>ps</italic> &gt; 0.519; BF<sub>excl</sub> &gt; 3.247), arguing against such a possibility.</p>
<p>The reviewer also raised an interesting question about whether an auditory tone during preparation could produce effects similar to those observed with visual pings. Although our study did not directly test this possibility, existing literature provides some relevant evidence. In particular, prior studies have shown that latent visual working memory contents are selectively reactivated by visual impulses, but not by auditory stimuli (Wolff et al., 2020). This finding supports the modality-specificity for visually encoded contents, suggesting that sensory impulses must match the representational domain to effectively access latent visual information, which also argues against the refocusing hypothesis above. However, we do think that this is an important question that merits direct investigation in future studies. We now added the related discussion on this point in the revised texts (Page 10, Line 202-203; Page 19, Line 392395).</p>
<disp-quote content-type="editor-comment">
<p>(6) The neural pattern distances measured using Mahalanobis values are really great! Have the authors tried to use all of the data, rather than the high AMI and low AMI to possibly show a linear relationship between response times and AMI?</p>
</disp-quote>
<p>We thank the reviewer for this comment. We took the reviewer’s suggestion to explore the relationship between attentional modulation index (AMI) and RTs across participants for each session (see Figure 3). In the No-Ping session, we observed no significant correlation between AMI and RT (<italic>r</italic> = -0.366, <italic>p</italic> = 0.113). By contrast, the same analysis in the Ping condition revealed a significantly negative correlation (<italic>r</italic> = -0.518, <italic>p</italic> = 0.019). These results indicate that the attentional modulations evoked by visual impulse was associated with faster RTs, supporting the functional relevance of activating sensory-like representations during preparation. We have now included these inter-subject correlations in the main texts (Page 13, Line 258-264; Fig 3D and 3E) along with within-subject correlations in the Supplementary Information (Page 6, Line, 85-98; S3 Fig).</p>
<disp-quote content-type="editor-comment">
<p>(7) After reading the whole manuscript I still don't understand what the authors think the ping is actually doing, mechanistically. I would have liked a more thorough discussion, rather than referencing previous papers (all by the co-author).</p>
</disp-quote>
<p>We thank the reviewer for this comment regarding the mechanistic basis of visual pings. We agree that this warrants deeper discussion. One possibility, as informed by theoretical studies of working memory, is that the sensory-like template could be maintained via an “activity-silent” mechanism through short-term changes in synaptic weights (Mongillo et al., 2008). In this framework, a visual impulse may function as nonspecific inputs that momentarily convert latent traces into detectable activity patterns (Rademaker &amp; Serences, 2017). Related to our findings, it is unlikely that the orientation-specific templates observed during the Ping session emerged from purely non-sensory representations and were entirely induced by an exogenous ping, which was devoid of any orientation signal. Instead, the more parsimonious explanation is that visual impulse reactivated pre-existing latent sensory signals. To our knowledge, the detailed circuit-level mechanism of such reactivation is still unclear; existing evidence only suggests a relationship between ping-evoked inputs and the neural output (Wolff et al., 2017; Fan et al., 2021; Duncan et al., 2023). We now included the discussion on this point in the main texts (Page 19, Line 383-401).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>(1) The origin of the latent sensory-like representation. By 'pinging' the neural activity with a high-contrast, task-irrelevant visual stimulus during the preparation period, the authors identified the representation of the attentional feature target that contains the same information as perceptual representations. The authors interpreted this finding as a 'sensory-like' template is inherently hosted in a latent form in the visual system, which is revealed by the pinging impulse. However, I am not sure whether such a sensory-like template is essentially created, rather than revealed, by the pinging impulses. First, unlike the classical employment of the pinging technique in working memory studies, the (latent) representation of the memoranda during the maintenance period is undisputed because participants could not have performed well in the subsequent memory test otherwise. However, this appears not to be the case in the present study. As shown in Figure 1C, there was no significant difference in behavioral performance between the ping and the no-ping sessions (see also lines 110-125, pg. 5-6). In other words, it seems to me that the subsequent attentional task performance does not necessarily rely on the generation of such sensory-like representations in the preparatory period and that the emergence of such sensory-like representations does not facilitate subsequent attentional performance either. In such a case, one might wonder whether such sensory-like templates are really created, hosted, and eventually utilized during the attentional process. Second, because the reference orientations (i.e. 45 degrees and 135 degrees) have remained unchanged throughout the experiment, it is highly possible that participants implicitly memorized these two orientations as they completed more and more trials. In such a case, one might wonder whether the 'sensory-like' templates are essentially latent working memory representations activated by the pinging as was reported in Wolff et al. (2017), rather than a functional signature of the attentional process.</p>
</disp-quote>
<p>We thank the reviewer for this comment. We agree that the question of whether the sensory-like template is created or merely revealed by visual pinging is crucial for the understanding our findings. First, we acknowledge that our task may not be optimized for detecting changes in accuracy, as the task difficulty was controlled using individually adjusted thresholds (i.e., angular difference). Nevertheless, we observed some evidence supporting the neural-behavioral relationships. In particular, the impulse-driven sensory-like template in V1 contributed to facilitated faster RTs during stimulus selection (Page 12, Fig. 3D and 3E in the main texts; also see our response to R1, Point 6).</p>
<p>Second, the reviewer raised an important concern about whether the attended feature might be stored in the memory system due to the trial-by-trial repetition of attention conditions (attend 45º or attend 135º). Although this is plausible, we don’t think it is likely. We note that neuroimaging evidence shows that attended working memory contents maintain sensory-like representations in visual cortex (Harrison &amp; Tong, 2009; Serences et al., 2009; Rademaker et al., 2019), with generalizable neural activity patterns from perception to working memory delay-period, whereas unattended items in multi-item working memory tasks are stored in a latent state for prospective use (Wolff et al., 2017). Importantly, our task only required maintaining a single attentional template at a time. Thus, there was no need to store it via latent representations, if participants simply used a working memory mechanism for preparatory attention. Had they done so, we should expect to find evidence for a sensory template, i.e., generalizable neural pattern between perception and preparation in the No-Ping condition, which was not what we found. We have mentioned this point in the main texts (Page 18, Line 367-372).</p>
<disp-quote content-type="editor-comment">
<p>(2) The coexistence of the two types of attentional templates. The authors interpreted their findings as the outcome of a dual-format mechanism in which 'a non-sensory template' and a latent 'sensory-like' template coexist (e.g. lines 103-106, pg. 5). While I find this interpretation interesting and conceptually elegant, I am not sure whether it is appropriate to term it 'coexistence'. First, it is theoretically possible that there is only one representation in either session (i.e. a non-sensory template in the no-ping session and a sensory-like template in the ping session) in any of the brain regions considered. Second, it seems that there is no direct evidence concerning the temporal relationship between these two types of templates, provided that they commonly emerge in both sessions. Besides, due to the sluggish nature of fMRI data, it is difficult to tell whether the two types of templates temporally overlap.</p>
</disp-quote>
<p>We thank the reviewer for the comment regarding our interpretation of the ‘coexistence’ of non-sensory and sensory-like attentional template. While we acknowledge the limitations of fMRI in resolving temporal relationships between these two types of templates, several aspects of our data support a dual-format interpretation.</p>
<p>First, our key findings remained consistent for the subset of participants (N=14) who completed both No-Ping and Ping sessions in counterbalanced order. It thus seems improbable that participants systematically switched cognitive strategies (e.g., using non-sensory templates in the No-Ping session versus sensory-like templates in the Ping session) in response to the task-irrelevant, uninformative visual impulse. Second, while we agree with the reviewer that the temporal dynamics between these two templates remain unclear, it is difficult to imagine that orientation-specific templates observed during the Ping session emerged <italic>de novo</italic> from a purely non-sensory templates and an exogenous ping. In other words, if there is no orientation information at all to begin with, how does it come into being from an orientation-less external ping? It seems to us that the more parsimonious explanation is that there was already some orientation signal in a latent format, and it was activated by the ping, in line with the models of “activity-silent” working memory. To address these concerns, we have added the related discussion of these alternative interpretations in the main texts (Page 19, Line 387-391)</p>
<disp-quote content-type="editor-comment">
<p>(3) The representational distance. The authors used Mahalanobis distance to quantify the similarity of neural representation between different conditions. According to the authors' hypothesis, one would expect greater pattern similarity between 'attend leftward' and 'perceived leftward' in the ping session in comparison to the no-ping session. However, this appears not to be the case. As shown in Figures 3B and C, there was no major difference in Mahalanobis distance between the two sessions in either ROI and the authors did not report a significant main effect of the session in any of the ANOVAs. Besides, in all the ANOVAs, the authors reported only the statistic term corresponding to the interaction effect without showing the descriptive statistics related to the interaction effect. It is strongly advised that these descriptive statistics related to the interaction effect should be included to facilitate a more effective and intuitive understanding of their data.</p>
</disp-quote>
<p>We thank the reviewer for this comment. We expected greater pattern similarity between 'attend leftward' and 'perceived leftward' in the Ping session in comparison to the Noping session. This prediction was supported by a significant three-way interaction effect between session × attended orientation × perceived orientation (<italic>F</italic>(1,38) = 5.00, <italic>p</italic> = 0.031, <italic>η<sub>p</sub><sup>2</sup></italic> = 0.116). In particular, there was a significant interaction between attended orientation × perceived orientation (F(1,19) = 9.335, <italic>p</italic> = 0.007, <italic>η<sub>p</sub><sup>2</sup></italic> = 0.329) in the Ping session, but not in the No-Ping session (F(1,19) = 0.017, <italic>p</italic> = 0.898, <italic>η<sub>p</sub><sup>2</sup></italic> = 0.001). These above-mentioned statistical results were reported in the original texts. In addition, this three-way mixed ANOVA (session × attended orientation × perceived orientation) on Mahalanobis distance in V1 revealed no significant main effects (session: F(1,38) = 0.009, <italic>p</italic> = 0.923, <italic>η<sub>p</sub><sup>2</sup></italic> &lt; 0.001; attended orientation: F(1,38) = 0.116, <italic>p</italic> = 0.735, <italic>η<sub>p</sub><sup>2</sup></italic> = 0.003; perceived orientation: (F(1,38) = 1.106, <italic>p</italic> = 0.300, <italic>η<sub>p</sub><sup>2</sup></italic> = 0.028). We agree with the reviewer that a complete reporting of analyses enhances understanding of the data. Therefore, we have now included the main effects in the main texts (Page 11, Line 233).</p>
<p>We thank the reviewer for the suggestion regarding the inclusion of descriptive statistics for interaction effects. However, since the data were already visualized in Fig. 3B and 3C in the main texts, to maintain conciseness and consistency with the reporting style of other analyses in the texts, we have opted to include these statistics in the Supplementary Information (Page 5, Table 1).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public review):</bold></p>
<p>(1) The title is &quot;Dual-format Attentional Template,&quot; yet the supporting evidence for the nonsensory format and its guiding function is quite weak. The author could consider conducting further generalization analysis from stimulus selection to preparation stages to explore whether additional information emerges.</p>
</disp-quote>
<p>We thank the reviewer for this comment. Our approach to investigate whether preparatory attention is encoded in sensory or non-sensory format - by training classifier using separate runs of perception task – closely followed methods from previous studies (Stokes et al., 2009; Peelen et al., 2011; Kok et al., 2017). Following the reviewer’s suggestion, we performed generalization analyses by training classifiers on activity during the stimulus selection period and testing them preparatory activity. However, we observed no significant generalization effects in either No-Ping and Ping sessions (<italic>ps</italic> &gt; 0.780). This null result may stem from a key difference in the neural representations: classifiers trained on neural activity from stimulus selection period necessarily encode both target and distractor information, thus relying on somewhat different information than classifier trained exclusively on isolated target information in the perception task.</p>
<disp-quote content-type="editor-comment">
<p>(2) In Figure 2, the author did not find any decodable sensory-like coding in IPS and PFC, even during the impulse-driven session, indicating that these regions do not represent sensory-like information. However, in the final section, the author claimed that the impulse-driven sensorylike template strengthens informational connectivity between sensory and frontoparietal areas. This raises a question: how can we reconcile the lack of decodable coding in these frontoparietal regions with the reported enhancement in network communication? It would be helpful if the author provided a clearer explanation or additional evidence to bridge this gap.</p>
</disp-quote>
<p>We thank the reviewer for this comment. We would like to clarity that although we did not observe sensory-like coding during preparation in frontoparietal areas, we did observe attentional signals in these regions, as evidenced by the above-chance within-task attention decoding performance (Fig. 2 in the main texts). This could reflect different neural codes in different areas, and suggests that inter-regional communication does not necessarily require identical representational formats. It seems plausible that the representation of a non-sensory attentional template in frontoparietal areas supports top-down attentional control, consistent with theories suggesting increasing abstraction as the cortical hierarchy ascends (Badre, 2008; Brincat et al., 2018), and their interaction with the sensory representation in the visual areas is enhanced by the visual impulse.</p>
<disp-quote content-type="editor-comment">
<p>(3) Given that the impulse-driven sensory-like template facilitated behavior, the author proposed that it might also enhance network communication. Indeed, they observed changes in informational connectivity. However, it remains unclear whether these changes in network communication have a direct and robust relationship with behavioral improvements.</p>
</disp-quote>
<p>We thank the reviewer for the suggestion. To examine how network communication relates to behavior, we performed a correlation analysis between information connectivity (IC) and RTs across participants (see Figure S5). We observed a trend of correlations between V1-PFC connectivity and RTs in the Ping session (<italic>r</italic> = -0.394, <italic>p</italic> = 0.086), but not in the NoPing session (<italic>r</italic> = -0.046, &lt;i.p = 0.846). No significant correlations were found between V1-IPS and RTs (<italic>ps</italic> &gt; 0.400) or between ICs and accuracy (<italic>ps</italic> &gt; 0.399). These results suggests that ping-enhanced connectivity might contributed to facilitated responses. Although we may not have sufficient statistical power to warrant a strong conclusion, we think this result is still highly suggestive, so we now added the texts in the Supplementary Information (Page 8, Line 116121; S5 Fig) and mentioned this result in the main texts (Page 14, Line 292-293).</p>
<disp-quote content-type="editor-comment">
<p>(4) I'm uncertain about the definition of the sensory-like template in this paper. Is it referring to the Ping impulse-driven condition or the decodable performance in the early visual cortex? If it is the former, even in working memory, whether pinging identifies an activity-silent mechanism is currently debated. If it's the latter, the authors should consider whether a causal relationship - such as &quot;activating the sensory-like template strengthens the informational connectivity between sensory and frontoparietal areas&quot; - is reasonable.</p>
</disp-quote>
<p>We apologize for the confusions. The sensory-like template by itself does not directly refer to representations under Ping session or the attentional decoding in early visual cortex. Instead, it pertains to the representational format of attentional signals during preparation. Specifically, its existence is inferred from cross-task generalization, where neural patterns from a perception task (perceive 45º or perceive 135º) generalize to an attention task (attend 45 º or attend 135º). We think this is a reasonable and accepted operational definition of the representational format. Our findings suggest that the sensory-like template likely existed in a latent state and was reactivated by visual pings, aligning more closely with the first account raised by the reviewer.</p>
<p>We agree with the reviewer that whether ping identifies an activity-silent mechanism is currently debated (Schneegans &amp; Bays, 2017; Barbosa et al., 2021). It is possible that visual impulse amplified a subtle but active representation of the sensory template during attentional preparation and resulted in decodable performance in visual cortex. Distinguishing between these two accounts likely requires neurophysiological measurements, which are beyond the scope of the current study. We have explicitly addressed this limitation in our Discussion (Page 19, Line 395-399).</p>
<p>Nevertheless, the latent sensory-like template account remains plausible for three reasons. First, our interpretation aligns with theoretical framework proposing that the brain maintains more veridical, detailed target templates than those typically utilized for guiding attention (Wolfe, 2021; Yu et al., 2023). Second, this explanation is consistent with the proposed utility of latent working memory for prospective use, as maintaining a latent sensory-like template during preparation would be useful for subsequent stimulus selection. The latter point was further supported by the reviewer’s suggestion about whether “activating the sensory-like template strengthens the informational connectivity between sensory and frontoparietal areas is reasonable”. Our additional analyses (also refer to our response to Reviewer 3, Point 3) suggested that impulse-enhanced V1-PFC connectivity was associated with a trend of faster behavioral responses (r = -0.394, <italic>p</italic> = 0.086; see Supplementary Information, Page 8, Line 116-121; S5 Fig). Considering these findings in totality, we think it is reasonable to suggest that visual impulse may strengthen information flow among areas to enhance attentional control.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendation for the Authors:</bold></p>
<p><bold>Reviewer #1 (Recommendation for the authors):</bold></p>
<p>I hate to suggest another fMRI experiment, but in order to make strong claims about two states, I would want to see the methodological and interpretation confounds addressed. Ping condition - would a tone lead to the same result of sharpening the template? If so, then why? Can a ping be manipulated in its effectiveness? That would be an excellent manipulation condition.</p>
</disp-quote>
<p>We thank the reviewer for the comments. Please refer to our reply to Reviewer 1, Point 5 for detailed explanation.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendation for the authors):</bold></p>
<p>It is strongly advised that these descriptive statistics related to the interaction effect should be included to facilitate a more effective understanding of their data.</p>
</disp-quote>
<p>We thank the reviewer for the comments. We now included the relevant descriptive statistics in the Supplementary Information, Table 1.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendation for the authors):</bold></p>
<p>In addition to p-values, I see many instances of 'ps'. Does this indicate the plural form of p?</p>
</disp-quote>
<p>We used ‘<italic>ps</italic>’ to denote the minimal p-value across multiple statistical analyses, such as when applying identical tests to different region groups.</p>
<p>References</p>
<p>Aitken, F., Menelaou, G., Warrington, O., Koolschijn, R. S., Corbin, N., Callaghan, M. F., &amp; Kok, P. (2020). Prior expectations evoke stimulus-specific activity in the deep layers of the primary visual cortex. PLoS Biology, 18(12), e3001023.</p>
<p>Badre, D. (2008). Cognitive control, hierarchy, and the rostro–caudal organization of the frontal lobes. Trends in Cognitive Sciences, 12(5), 193-200.</p>
<p>Barbosa, J., Lozano-Soldevilla, D., &amp; Compte, A. (2021). Pinging the brain with visual impulses reveals electrically active, not activity-silent, working memories. PLoS Biology, 19(10), e3001436.</p>
<p>Battistoni, E., Stein, T., &amp; Peelen, M. V. (2017). Preparatory attention in visual cortex. Annals of the New York Academy of Sciences, 1396(1), 92-107.</p>
<p>Brincat, S. L., Siegel, M., von Nicolai, C., &amp; Miller, E. K. (2018). Gradual progression from sensory to task-related processing in cerebral cortex. Proceedings of the National Academy of Sciences, 115(30), E7202-E7211.</p>
<p>Duncan, D. H., van Moorselaar, D., &amp; Theeuwes, J. (2023). Pinging the brain to reveal the hidden attentional priority map using encephalography. Nature Communications, 14(1), 4749.</p>
<p>Grill-Spector, K., &amp; Malach, R. (2004). The human visual cortex. Annual Review of Neuroscience, 27(1),  649-677.</p>
<p>Gong, M., Chen, Y., &amp; Liu, T. (2022). Preparatory attention to visual features primarily relies on nonsensory representation. Scientific Reports, 12(1), 21726.</p>
<p>Fan, Y., Han, Q., Guo, S., &amp; Luo, H. (2021). Distinct Neural Representations of Content and Ordinal Structure in Auditory Sequence Memory. Journal of Neuroscience, 41(29), 6290–6303.</p>
<p>Harrison, S. A., &amp; Tong, F. (2009). Decoding reveals the contents of visual working memory in early visual areas. Nature, 458(7238), 632-635.</p>
<p>Jigo, M., Gong, M., &amp; Liu, T. (2018). Neural determinants of task performance during feature-based attention in human cortex. eNeuro, 5(1).</p>
<p>Kok, P., Failing, M. F., &amp; de Lange, F. P. (2014). Prior expectations evoke stimulus templates in the primary visual cortex. Journal of Cognitive Neuroscience, 26(7), 1546-1554.</p>
<p>Kok, P., Mostert, P., &amp; De Lange, F. P. (2017). Prior expectations induce prestimulus sensory templates. Proceedings of the National Academy of Sciences, 114(39), 10473-10478.</p>
<p>Liu, T., Stevens, S. T., &amp; Carrasco, M. (2007). Comparing the time course and efficacy of spatial and feature-based attention. Vision Research, 47(1), 108-113.</p>
<p>Mongillo, G., Barak, O., &amp; Tsodyks, M. (2008). Synaptic theory of working memory. Science, 319(5869), 1543-1546.</p>
<p>Peelen, M. V., &amp; Kastner, S. (2011). A neural basis for real-world visual search in human occipitotemporal cortex. Proceedings of the National Academy of Sciences, 108(29), 12125-12130. Priebe, N. J. (2016). Mechanisms of orientation selectivity in the primary visual cortex. Annual Review  of Vision Science, 2(1), 85-107.</p>
<p>Rademaker, R. L., &amp; Serences, J. T. (2017). Pinging the brain to reveal hidden memories. Nature Neuroscience, 20(6), 767-769.</p>
<p>Rademaker, R. L., Chunharas, C., &amp; Serences, J. T. (2019). Coexisting representations of sensory and mnemonic information in human visual cortex. Nature Neuroscience, 22(8), 1336-1344.</p>
<p>Serences, J. T., Ester, E. F., Vogel, E. K., &amp; Awh, E. (2009). Stimulus-specific delay activity in human primary visual cortex. Psychological Science, 20(2), 207-214.</p>
<p>Schneegans, S., &amp; Bays, P. M. (2017). Restoration of fMRI decodability does not imply latent working memory states. Journal of Cognitive Neuroscience, 29(12), 1977-1994.</p>
<p>Stokes, M., Thompson, R., Nobre, A. C., &amp; Duncan, J. (2009). Shape-specific preparatory activity mediates attention to targets in human visual cortex. Proceedings of the National Academy of Sciences, 106(46), 19569-19574.</p>
<p>Wolfe, J. M. (2021). Guided Search 6.0: An updated model of visual search. Psychonomic Bulletin &amp; Review, 28(4), 1060-1092.</p>
<p>Wolff, M. J., Jochim, J., Akyürek, E. G., &amp; Stokes, M. G. (2017). Dynamic hidden states underlying working-memory-guided behavior. Nature Neuroscience, 20(6), 864 – 871.</p>
<p>Wolff, M. J., Kandemir, G., Stokes, M. G., &amp; Akyürek, E. G. (2020). Unimodal and bimodal access to sensory working memories by auditory and visual impulses. Journal of Neuroscience, 40(3), 671-681.</p>
<p>Yu, X., Zhou, Z., Becker, S. I., Boettcher, S. E., &amp; Geng, J. J. (2023). Good-enough attentional guidance. Trends in Cognitive Sciences, 27(4), 391-403.</p>
</body>
</sub-article>
</article>