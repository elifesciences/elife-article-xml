<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">100056</article-id>
<article-id pub-id-type="doi">10.7554/eLife.100056</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.100056.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Multi-talker speech comprehension at different temporal scales in listeners with normal and impaired hearing</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5210-6224</contrib-id>
<name>
<surname>Li</surname>
<given-names>Jixing</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
<email>jixingli@cityu.edu.hk</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Wang</surname>
<given-names>Qixuan</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhou</surname>
<given-names>Qian</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Yang</surname>
<given-names>Lu</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Shen</surname>
<given-names>Yutong</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Huang</surname>
<given-names>Shujian</given-names>
</name>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wang</surname>
<given-names>Shaonan</given-names>
</name>
<xref ref-type="aff" rid="a7">7</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Pylkkänen</surname>
<given-names>Liina</given-names>
</name>
<xref ref-type="aff" rid="a8">8</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Huang</surname>
<given-names>Zhiwu</given-names>
</name>
<xref ref-type="aff" rid="a9">9</xref>
<email>huangzw86@126.com</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03q8dnn23</institution-id><institution>Department of Linguistics and Translation, City University of Hong Kong</institution></institution-wrap>, <city>Hong Kong</city>, <country country="HK">Hong Kong</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02wc1yz29</institution-id><institution>Department of Facial Plastic and Reconstructive Surgery and ENT institute, Eye &amp; ENT Hospital, Fudan University</institution></institution-wrap>, <city>Shanghai</city>, <country country="CN">China</country>.</aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/010826a91</institution-id><institution>Department of Otolaryngology-Head and Neck Surgery, Shanghai Ninth People’s Hospital, Shanghai Jiao Tong University School of Medicine</institution></institution-wrap>, <city>Shanghai</city>, <country country="CN">China</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/010826a91</institution-id><institution>Department of Otolaryngology-Head and Neck Surgery, Shanghai Ninth People’s Hospital, Shanghai Jiao Tong University School of Medicine</institution></institution-wrap>, <city>Shanghai</city>, <country country="CN">China</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01rxvg760</institution-id><institution>Department of Computer Science and Technology, Nanjing University</institution></institution-wrap>, <city>Nanjing</city>, <country country="CN">China</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01rxvg760</institution-id><institution>Department of Computer Science and Technology, Nanjing University</institution></institution-wrap>, <city>Nanjing</city>, <country country="CN">China</country></aff>
<aff id="a7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/034t30j35</institution-id><institution>Institute of Automation, Chinese Academy of Sciences</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
<aff id="a8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Department of Linguistics, Department of Psychology, New York University</institution></institution-wrap>, <city>New York</city>, <country country="US">United States</country></aff>
<aff id="a9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/010826a91</institution-id><institution>Department of Otolaryngology-Head and Neck Surgery, Shanghai Ninth People’s Hospital, Shanghai Jiao Tong University School of Medicine; College of Health Science and Technology, Shanghai Jiao Tong University School of Medicine.</institution></institution-wrap>, <city>Shanghai</city>, <country country="CN">China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ding</surname>
<given-names>Nai</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Zhejiang University</institution>
</institution-wrap>
<city>Hangzhou</city>
<country>China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>†</label><p>co-first authors</p></fn>
<fn id="n4" fn-type="con"><p><bold>Author contributions:</bold> Jixing Li designed the research, analyzed data, and wrote the paper; Qixuan Wang and Zhiwu Huang designed the research; Qian Zhou and Lu Yang collected data; Yutong Shen and Shujian Huang implemented the model; Shaonan Wang provided speech stimuli and helped write the paper; Liina Pylkkänen helped write the paper.</p></fn>
<fn id="n5" fn-type="supported-by"><p><bold>Funding:</bold> This work was supported by the CityU Start-up Grant 7020086 and CityU Strategic Research Grant 7200747 (Li, J.) the National Natural Science Foundation of China 82201273 (Wang Q.), Shanghai Science and Technology Commission Grant 22Y11902000 (Huang Z.) and award G1001 from NYUAD Institute, New York University Abu Dhabi (Pylkkänen, L.).</p></fn>
<fn id="n6" fn-type="coi-statement"><p><bold>Conflict of interest:</bold> The authors declare no competing interests.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-09-02">
<day>02</day>
<month>09</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-04-24">
<day>24</day>
<month>04</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP100056</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-06-17">
<day>17</day>
<month>06</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-06-26">
<day>26</day>
<month>06</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.06.20.599315"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-09-02">
<day>02</day>
<month>09</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.100056.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.100056.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.100056.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.100056.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.100056.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Li et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Li et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-100056-v2.pdf"/>
<abstract>
<title>Abstract</title><p>Comprehending speech requires deciphering a range of linguistic representations, from phonemes to narratives. Prior research suggests that in single-talker scenarios, the neural encoding of linguistic units follows a hierarchy of increasing temporal receptive windows. Shorter temporal units like phonemes and syllables are encoded by lower-level sensory brain regions, whereas longer units such as sentences and paragraphs are processed by higher-level perceptual and cognitive areas. However, the brain’s representation of these linguistic units under challenging listening conditions, such as a cocktail party situation, remains unclear. In this study, we recorded electroencephalogram (EEG) responses from both normal-hearing and hearing-impaired participants as they listened to individual and dual speakers narrating different parts of a story. The inclusion of hearing-impaired listeners allowed us to examine how hierarchically organized linguistic units in competing speech streams affect comprehension abilities. We leveraged a hierarchical language model to extract linguistic information at multiple levels—phoneme, syllable, word, phrase, and sentence—and aligned these model activations with the EEG data. Our findings showed distinct neural responses to dual-speaker speech between the two groups. Specifically, compared to normal-hearing listeners, hearing-impaired listeners exhibited poorer model fits at the acoustic, phoneme, and syllable levels as well as the sentence levels, but not at the word and phrase levels. These results suggest that hearing-impaired listeners experience disruptions at both shorter and longer temporal scales, while their processing at medium temporal scales remains unaffected.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>cocktail party</kwd>
<kwd>speech comprehension</kwd>
<kwd>hierarchical language model</kwd>
<kwd>hearing loss</kwd>
<kwd>EEG</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Revised paper according to reviewers' comments</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Human speech encompasses elements at different levels, from phonemes to syllables, words, phrases, sentences and paragraphs. These elements manifest over distinct timescales: Phonemes occur over tens of milliseconds, and paragraphs span a few minutes. Understanding how these units at different time scales are encoded in the brain during speech comprehension remains a challenge. In the visual system, it has been well-established that there is a hierarchical organization such that neurons in the early visual areas have smaller receptive fields, while neurons in the higher-level visual areas receive inputs from lower-level neurons and have larger receptive fields (<xref ref-type="bibr" rid="c24">Hubel &amp; Wiesel, 1962</xref>; 1965). This organizing principle is theorized to be mirrored in the auditory system, where a hierarchy of temporal receptive windows (TRW) extends from primary sensory regions to advanced perceptual and cognitive areas (<xref ref-type="bibr" rid="c22">Hasson et al., 2008</xref>; <xref ref-type="bibr" rid="c23">Honey et al., 2012</xref>; <xref ref-type="bibr" rid="c25">Lerner et al., 2011</xref>; <xref ref-type="bibr" rid="c34">Murray et al., 2014</xref>). Under this assumption, neurons in the lower-level sensory regions, such as the core auditory cortex, support rapid processing of the ever-changing auditory and phonemic information, whereas neurons in the higher cognitive regions, with their extended temporal receptive windows, process information at the sentence or discourse level.</p>
<p>Recent functional magnetic resonance imaging (fMRI) studies have shown some evidence that different levels of linguistic units are encoded at different cortical regions (<xref ref-type="bibr" rid="c4">Blank &amp; Fedorenko, 2020</xref>; <xref ref-type="bibr" rid="c8">Chang et al., 2022</xref>; <xref ref-type="bibr" rid="c22">Hasson et al., 2008</xref>; <xref ref-type="bibr" rid="c25">Lerner et al., 2011</xref>; <xref ref-type="bibr" rid="c40">Schmitt et al., 2021</xref>). For example, <xref ref-type="bibr" rid="c40">Schmitt et al. (2021)</xref> used artificial neural networks to predict the next word in a story across five stacked time scales. By correlating model predictions with brain activity while listening to a story in an fMRI scanner, they discerned a hierarchical progression along the temporoparietal pathway. This pathway identifies the role of the bilateral primary auditory cortex in processing words over shorter durations and the involvement of the inferior parietal cortex in processing paragraph-length units over extended periods. Studies using electroencephalogram (EEG), magnetoencephalography (MEG) and electrocorticography (ECoG) have also revealed synchronous neural responses to different linguistics units at different time scales (e.g., <xref ref-type="bibr" rid="c12">Ding et al., 2015</xref>; <xref ref-type="bibr" rid="c13">Ding &amp; Simon, 2012</xref>; <xref ref-type="bibr" rid="c23">Honey et al., 2012</xref>; <xref ref-type="bibr" rid="c28">Luo &amp; Poeppel, 2007</xref>). Notably, <xref ref-type="bibr" rid="c12">Ding et al. (2015)</xref> showed that the MEG-derived cortical response spectrum concurrently tracked the timecourses of abstract linguistic structures at the word, phrase and sentence levels.</p>
<p>Although there is a growing consensus on the hierarchical encoding of linguistic units in the brain, the neural representations of these units in a multi-talker setting remain less explored. In the classic “cocktail party” situation in which multiple speakers talk simultaneously (<xref ref-type="bibr" rid="c9">Cherry, 1953</xref>), listeners must separate a speech signal from a cacophony of other sounds (<xref ref-type="bibr" rid="c30">McDermott, 2009</xref>). Studies have shown that listeners with normal hearing can selectively attend to a chosen speaker in the presence of two competing speakers (<xref ref-type="bibr" rid="c6">Brungart, 2001</xref>; <xref ref-type="bibr" rid="c42">Shinn-Cunningham, 2008</xref>), resulting in enhanced neural responses to the attended speech stream (<xref ref-type="bibr" rid="c5">Brodbeck et al., 2018</xref>; <xref ref-type="bibr" rid="c13">Ding &amp; Simon, 2012</xref>; <xref ref-type="bibr" rid="c31">Mesgarani &amp; Chang, 2012</xref>; <xref ref-type="bibr" rid="c36">O’Sullivan et al., 2015</xref>; <xref ref-type="bibr" rid="c46">Zion Golumbic et al., 2013</xref>). For example, <xref ref-type="bibr" rid="c13">Ding and Simon (2012)</xref> showed that neural responses were selectively phase-locked to the broadband envelope of the attended speech stream in the posterior auditory cortex. Furthermore, when the intensity of the attended and unattended speakers is separately varied, the neural representation of the attended speech stream adapts only to the intensity of the attended speaker. <xref ref-type="bibr" rid="c46">Zion Golumbic et al. (2013)</xref> further suggested that the neural representation appears to be more “selective” in higher perceptual and cognitive brain regions such that there is no detectable tracking of ignored speech.</p>
<p>This selective entrainment to attended speech has primarily focused on the low-level acoustic properties of the speech, while largely ignoring the higher-level linguistic units beyond the phonemic levels. <xref ref-type="bibr" rid="c5">Brodbeck et al. (2018)</xref> were the first study to simultaneously compare the neural responses to the acoustic envelopes as well as the phonemes and words in two competing speech streams. They found that although the acoustic envelopes of both the attended and unattended speech could be decoded from brain activity in the temporal cortex, only phonemes and words of the attended speech showed significant responses. However, as their study only examined two linguistic units, a complete model of how the brain tracks linguistic units, from phonemes to sentences, in two competing speech streams is still lacking.</p>
<p>In this study, we investigate the neural underpinnings of processing diverse linguistic units in the context of competing speech streams among listeners with both normal and impaired hearing. We included hearing-impaired listeners to examine how hierarchically organized linguistic units in competing speech streams impact comprehension abilities. The experiment design consisted of a multi-talker condition and a single-talker condition. In the multi-talker condition, participants listened to mixed speech from female and male speakers narrating simultaneously. Before each trial, instructions on the screen indicated which speaker to focus on. In the single-talker condition, the speeches from male and female speakers were presented separately (refer to <xref rid="fig1" ref-type="fig">Figure 1A</xref> for the experimental procedure). We employed a hierarchical multiscale Long Short-Term Memory network (HM-LSTM; <xref ref-type="bibr" rid="c10">Chung et al., 2017</xref>) to dissect the linguistic information of the stimuli across phoneme, syllable, word, phrase, and sentence levels (detailed model architecture is described in the “Hierarchical multiscale LSTM model” section in Materials and Methods). We then performed ridge regressions using these linguistic units as regressors, time-locked to the offset of each sentence at nine latencies (see <xref rid="fig1" ref-type="fig">Figure 1B</xref> for the analysis pipeline). This model-brain alignment method has been commonly employed in the literature (e.g., <xref ref-type="bibr" rid="c7">Caucheteux &amp; King, 2022</xref>; <xref ref-type="bibr" rid="c19">Goldstein et al., 2022</xref>; <xref ref-type="bibr" rid="c40">Schmitt et al., 2021</xref>; <xref ref-type="bibr" rid="c41">Schrimpf et al., 2021</xref>). By examining how model alignments with brain activity vary across different linguistic levels between listeners with normal and impaired hearing, our goal is to identify specific levels of linguistic processing that pose comprehension challenges for hearing-impaired individuals.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
    <caption><title>Methods and behavioral results.</title>
    <p><bold>A.</bold> Experimental procedure. The experimental task consisted of a multi-talker condition followed by a single-talker condition. In the multi-talker condition, the mixed speech was presented twice with the female and male speakers narrating simultaneously. Before each trial, instructions appeared in the center of the screen indicating which of the talkers to attend to (e.g., ‘‘Attend female’’). In the single-talker condition, the male and female speeches were presented sequentially. <bold>B.</bold> Analyses pipeline. Hidden-layer activity of the HM-LSTM model, which represents each level of linguistic units for each sentence, was extracted and aligned with EEG data, time-locked to the offset of each sentence at nine different latencies.</p></caption>
<graphic xlink:href="599315v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Behavioral results</title>
<p>A total of 41 participants (21 females, mean age=24.1 years, SD=2.1 years) with extended high frequency (EHF) hearing loss and 33 participants (13 females, mean age=22.94 years, SD=2.36 years) with normal hearing were included in the study. EHF hearing loss refers to hearing loss at frequencies above 8 kHz. It is considered a major cause of hidden hearing loss, which cannot be detected by audiometry (<xref ref-type="bibr" rid="c3">Bharadwaj et al., 2019</xref>). Although the phonetic information required for speech perception in quiet conditions is below 6 kHz, ample evidence suggests that salient information in the higher-frequency regions may also affect speech intelligibility (e.g., <xref ref-type="bibr" rid="c1">Apoux &amp; Bacon, 2004</xref>; <xref ref-type="bibr" rid="c2">Badri et al., 2011</xref>; <xref ref-type="bibr" rid="c11">Collins et al., 1981</xref>; <xref ref-type="bibr" rid="c26">Levy et al., 2015</xref>). Unlike age-related hearing loss, EHF hearing loss is commonly found in young adults who frequently use earbuds and headphones for prolonged periods and are exposed to high noise levels during recreational activities (<xref ref-type="bibr" rid="c33">Motlagh Zadeh et al., 2019</xref>). Consequently, this demographic is a suitable comparison group for their age-matched peers with normal hearing. EHF hearing loss was diagnosed using the pure tone audiometry (PTA) test, thresholded at frequencies greater than 8 kHz. As shown in <xref rid="fig2" ref-type="fig">Figure 2A</xref>, starting at 10 kHz, participants with EHF hearing loss exhibit significantly higher hearing thresholds (M=6.42 dB, SD=7 dB) compared to those with normal hearing (M=3.3 dB, SD=4.9 dB), as confirmed by an independent two-sample one-tailed t-test (t(72)=2, p=0.02).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Behavioral results.</title>
<p><bold>A.</bold> PTA results for participants with normal hearing and EHF hearing loss. Starting at 10 kHz, participants with EHF hearing loss have significantly higher hearing thresholds (M=6.42 dB, SD=7 dB) compared to normal-hearing participants (M=3.3 dB, SD=4.9 dB; t=2, p=0.02). <bold>B.</bold> Distribution of self-rated intelligibility scores for mixed and single-talker speech across the two listener groups. * indicates p &lt; .05, ** indicates p &lt;. 01 and *** indicates p &lt; .001.</p></caption>
<graphic xlink:href="599315v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p><xref rid="fig2" ref-type="fig">Figure 2B</xref> illustrates the distribution of intelligibility ratings for both mixed and single-talker speech across the two listener groups. The average intelligibility ratings for both mixed and single-talker speech were significantly higher for normal-hearing participants (mixed: M=3.89, SD=0.83; single-talker: M=4.64, SD=0.56) compared to hearing-impaired participants (mixed: M=3.38, SD=1.04; single-talker: M=4.09, SD=0.89), as shown by independent two-sample one-tailed t-tests (mixed: t(72)=2.11, p=0.02; single: t(72)=2.81, p=0.003). Additionally, paired two-sample one-tailed t-tests indicated significantly higher intelligibility scores for single-talker speech compared to mixed speech within both listener groups (normal-hearing: t(32)=4.58, p&lt;0.0001; hearing-impaired: t(40)=4.28, p=0.0001). These behavioral results confirm that mixed speech presents greater comprehension challenges for both groups, with hearing-impaired participants experiencing more difficulty in understanding both types of speech compared to those with normal hearing.</p>
</sec>
<sec id="s2b">
<title>HM-LSTM model performance</title>
<p>To simultaneously estimate linguistic content at the phoneme, syllable, word, phrase and sentence levels, we adopted the HM-LSTM model originally developed by <xref ref-type="bibr" rid="c10">Chung et al. (2017)</xref>. The original model consists of only two levels: the word level and the phrase level. We expanded its architecture to include five levels: phoneme, syllable, word, phrase, and sentence. Since our input consists of phoneme embeddings, we cannot directly apply their model, so we trained our model on the WenetSpeech corpus (<xref ref-type="bibr" rid="c45">Zhang et al., 2021</xref>), which provides phoneme-level transcripts. The inputs to the model were the vector representations of the phonemes in two sentences and the output of the model was the classification result of whether the second sentence follows the first sentence (see <xref rid="fig3" ref-type="fig">Figure 3A</xref> and the “Hierarchical multiscale LSTM model” section in “Materials and Methods” for the detailed model architecture). Unlike the Transformer-based language models that can predict only word- or sentence- and paragraph-level information, our HM-LSTM model can disentangle the impact of different informational content associated with phonemic and syllabic levels. After 130 epochs, the model achieved an accuracy of 0.87 on the training data and 0.83 on our speech stimuli, which comprise 570 sentence pairs. We subsequently extracted activity from the trained model’s four hidden layers for each sentence in our stimuli to represent information at the phoneme, syllable, word, sentence, and paragraph levels, with the sentence-level information represented by the last unit of the fourth layer. We computed the correlations among the activations at the five levels of the HM-LSTM model (see “Correlations among LSTM model layers” section in “Materials and Methods” for the detailed analysis procedure). We did not observe very high correlations (all below 0.22) compared to prior model-brain alignment studies which report correlation coefficients above 0.5 for linguistic regressors (e.g., <xref ref-type="bibr" rid="c16">Gao et al., 2024</xref>; <xref ref-type="bibr" rid="c43">Sugimoto et al., 2024</xref>). In Chinese, a single syllable can also function as a word, potentially leading to higher correlations between regressors for syllables and words. However, we refrained from overinterpreting the results to suggest a higher correlation between syllable and sentence compared to syllable and word. A paired t-test of the syllable-word coefficients versus syllable-sentence coefficients across the 284 sentences revealed no significant difference (t(28399)=-3.96, p=1). This suggests that different layers of the model captured distinct patterns in the stimuli (see <xref rid="fig3" ref-type="fig">Figure 3B</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
    <caption><title>The HM-LSTM model architecture and hidden layer activity for the stimuli sentences and the 4-word Chinese sentences with same vowels.</title>
    <p><bold>A.</bold> The HM-LSTM model architecture. The model includes four hidden layers, corresponding to the phoneme-, syllable-, word- and phrase-level information. Sentence-level information was represented by the last unit of the 4<sup>th</sup> layer. The inputs to the model were the vector representations of the phonemes in two sentences and the output of the model was the classification result of whether the second sentence follows the first sentence. <bold>B.</bold> Correlation matrix for the HM-LSTM model’s hidden layer activity for the sentences in the experimental stimuli. <bold>C.</bold> Scatter plot of hidden-layer activity at the five linguistic levels for each of the 20 4-syllable sentences after MDS.</p></caption>
<graphic xlink:href="599315v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To verify that the hidden-layer activity indeed reflects information at the corresponding linguistic levels, we constructed a test dataset comprising 20 four-syllable sentences where all syllables contain the same vowels, such as “mā ma mà mǎ” (mother scolds horse), “shū shu shŭ shù” (uncle counts numbers). <xref rid="tbls1" ref-type="table">Table S1</xref> in the Supplementary lists all four-syllable sentences with the same vowels. We hypothesized that the activity in the phoneme and syllable layer would be more similar than other layers for same-vowel sentences. The results confirmed our hypothesis: Hidden-layer activity for same-vowel sentences exhibited much more similar distributions at the phoneme and syllable levels compared to those at the word, phrase and sentence levels. <xref rid="fig3" ref-type="fig">Figure 3C</xref> displays the scatter plot of the model activity at the five linguistic levels for each of the 20 4-syllable sentences, post dimension reduction using multidimensional scaling (MDS). We used color-coding to represent the activity of five hidden layers after dimensionality reduction. Each dot on the plot corresponds to one test sentence. Only phonemes are labeled because each syllable in our test sentences contains the same vowels (see <xref rid="tbls1" ref-type="table">Table S1</xref>).The plot reveals that model representations at the phoneme and syllable levels are more dispersed for each sentence, while representations at the higher linguistic levels—word, phrase, and sentence—are more centralized. Additionally, similar phonemes tend to cluster together across the phoneme and syllable layers, indicating that the model captures a greater amount of information at these levels when the phonemes within the sentences are similar.</p>
</sec>
<sec id="s2c">
<title>Regression results for single-talker speech versus attended speech</title>
<p>To examine the differences in neural activity between single- and dual-talker speech, we first compared the model fit of acoustic and linguistic features for the single-talker speech against the attended speech in the context of mixed speech across both listener groups. (see “Ridge regression at different time latencies” and “Spatiotemporal clustering analysis” in “Materials and Methods” for analysis details). We have also analyzed the epoched EEG data by decomposing it into different frequency bands (see “EEG Recording and Preprocessing” in “Materials and Methods”). We specifically examined the delta and theta bands, which are conventionally used in the literature for speech analysis. However, the results from these bands were very similar to those obtained using data from all frequency bands (see <xref rid="figs2" ref-type="fig">Supplementary Figures S2</xref> and S3). Therefore, we opted to use the epoched EEG data from all frequency bands for our analyses. <xref rid="fig4" ref-type="fig">Figure 4</xref> shows the sensors and time window where acoustic and linguistic features significantly better predicted EEG data in the left temporal region during single-talker speech as compared to the attended speech. It can be seen that for hearing-impaired participants, acoustic features showed a better model fit in single-talker settings as opposed to mixed speech conditions from -100 to 100 ms around sentence offsets (t=1.4, Cohen’s d=1.5, p=0.002). However, no significant differences in the model fit between the single-talker and the attended speeches were observed for normal-hearing participants. Group comparisons revealed a significant difference in the model fit for the two conditions from -100 to 50 ms around sentence offsets (t=1.43, Cohen’s d=1.28, p=0.011).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
    <caption><title>Significant sensor and time window for the model fit to the EEG data for the acoustic and linguistic features extracted from the HM-LSTM model between single-talker and attended speech across the two listener groups.</title>
    <p><bold>A.</bold> Significant sensors showing higher model fit for single-talker speech compared to the attended speech at the acoustic, phoneme, and syllable levels for the two listener groups and their contrast. <bold>B.</bold> Timecourses of mean model fit in the significant clusters where normal-hearing participants showed higher model fit at the acoustic, phoneme, and syllable levels than hearing-impaired participants. The coefficient of determination (R<sup>2</sup>) were z-transformed. Shaded regions indicate significant time windows. * denotes p&lt;0.05, ** denotes p&lt;0.01 and *** denotes p&lt;0.001.</p></caption>
<graphic xlink:href="599315v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>For the linguistic features, both the phoneme and syllable layers from the HM-LSTM model were more predictive of EEG data in single-talker speech compared to attended speech among hearing-impaired participants in the left temporal regions (phoneme: t=1.9, Cohen’s d=0.49, p=0.004; syllable: t=1.9, Cohen’s d=0.37, p=0.002). The significant effect occurred from approximately 0-100 ms for phonemes and 50-150 ms for syllables after sentence offsets. No significant differences in model fit were observed between the two conditions for participants with normal hearing. Comparisons between groups revealed significant differences in the contrast maps from 0-100 ms after sentence offsets for phonemes (t=2.39, Cohen’s d=0.72, p=0.004) and from 50-150 ms after the sentence offsets for syllables (t=2.11, Cohen’s d=0.78, p=0.001). The model fit to the EEG data for higher-level linguistic features—words, phrases, and sentences—does not show any significant differences between single-talker and attended speech across the two listener groups. This suggests that both normal-hearing and hearing-impaired participants are able to extract information at the word, phrase, and sentence levels from the attended speech in dual-speaker scenarios, similar to conditions involving only a single talker.</p>
</sec>
<sec id="s2d">
<title>Regression results for single-talker versus unattended speech</title>
<p>We also compared the model fit for single-talker speech and the unattended speech under the mixed speech condition. As shown in <xref rid="fig5" ref-type="fig">Figure 5</xref>, the acoustic features showed a better model fit in single-talker settings as opposed to mixed speech conditions from -100 to 50 ms around sentence offsets for hearing-impaired listeners (t=2.05, Cohen’s d=1.1, p=&lt;0.001) and from -100 to 50 ms for normal-hearing listeners (t=2.61, Cohen’s d=0.23, p=&lt;0.001). No group difference was observed with regard to the contrast of the model fit for the two conditions.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
    <caption><title>Significant sensor and time window for the model fit to the EEG data for the acoustic and linguistic features between the single-talker and unattended speech in the mixed speech condition across the two listener groups.</title>
    <p><bold>A.</bold> Significant sensors showing higher model fit for the single-talker speech compared to the unattended speech at the acoustic and linguistic levels for the two listener groups and their contrast. <bold>B.</bold> Timecourses of mean model fit in the significant clusters. The significant time windows for within-group comparisons. The coefficient of determination (R<sup>2</sup>) were z-transformed. Shaded regions indicate significant time windows. * denotes p&lt;0.05, ** denotes p&lt;0.01 and *** denotes p&lt;0.001.</p></caption>
<graphic xlink:href="599315v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>All the five linguistic features were more predictive of EEG data in single-talker speech compared to the unattended speech for both hearing-impaired participants (phoneme: t=1.72, Cohen’s d=0.79, p&lt;0.001; syllable: t=1.94, Cohen’s d=0.9, p&lt;0.001; word: t=2.91, Cohen’s d=1.08, p&lt;0.001; phrase: t=1.4, Cohen’s d=0.61, p=0.041; sentence: t=1.67, Cohen’s d=1.01, p=0.023) and normal-hearing participants (phoneme: t=1.99, Cohen’s d=0.31, p=0.02; syllable: t=1.78, Cohen’s d=0.8, p&lt;0.001; word: t=2.85, Cohen’s d=1.55, p=0.001; phrase: t=1.74, Cohen’s d=1.4, p&lt;0.001; sentence: t=1.86, Cohen’s d=0.81, p=0.046). The significant effects occurred progressively later from phoneme to sentence level for both hearing-impaired participants (phoneme: -100-100 ms; syllable: 0-200 ms; word: 0-250 ms; phrase: 200-300 ms; sentence: 200-300 ms) and normal-hearing participants (phoneme: -50-100 ms; syllable:0-200 ms; word: 50-250 ms; phrase: 100-300 ms; sentence: 200-300 ms.). No significant group differences in the model fit were observed between the two conditions for all the linguistic levels.</p>
</sec>
<sec id="s2e">
<title>Regression results for attended versus unattended speech</title>
<p><xref rid="fig6" ref-type="fig">Figure 6</xref> depicts the model fit of acoustic and linguistic predictors against EEG data for both attended and unattended speech while two speakers narrated simultaneously. It can be seen that for normal-hearing participants, acoustic features demonstrated a better model fit for attended speech compared to unattended speech from -100 ms to sentence offsets (t=3.21, Cohen’s d=1.34, p=0.02). However, for hearing-impaired participants, no significant differences were observed in this measure. The difference between attended and unattended speech in normal-hearing and hearing-impaired participants was confirmed to be significant in the left temporal region from -100 ms to -50 ms before sentence offsets (t=2.24, Cohen’s d=1.01, p=0.02) by a permutation two-sample t-test.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
    <caption><title>Significant sensor and time window for the model fit to the EEG data for the acoustic and linguistic features between the attended and unattended speech in the mixed speech condition across the two listener groups.</title>
    <p><bold>A.</bold> Significant sensors showing higher model fit for the attended speech compared to the unattended speech at the acoustic and linguistic levels for the two listener groups and their contrast. <bold>B.</bold> Timecourses of mean model fit in the significant clusters where normal-hearing participants showed higher model fit than hearing-impaired participants.. The significant time windows for within-group comparisons. The coefficient of determination (R<sup>2</sup>) were z-transformed. Shaded regions indicate significant time windows. * denotes p&lt;0.05, ** denotes p&lt;0.01 and *** denotes p&lt;0.001.</p></caption>
<graphic xlink:href="599315v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Both phoneme and syllable features significantly better predicted attended speech compared to unattended speech among normal-hearing participants (phoneme: t=1.58, Cohen’s d=0.46, p=0.0006; syllable: t=1.05, Cohen’s d=1.02, p=0.0001). The significant time window for phonemes was from -100 to 250 ms after sentence offsets, earlier than that for syllables, which was from 0 to 250 ms. No significant differences were observed in the hearing-impaired group. The contrast maps were significantly different across the two groups during the 0-100 ms window for phonemes (t=2.28, Cohen’s d=1.32, p=0.026) and the 0-150 ms window for syllables (t=2.64, Cohen’s d=1.04, p=0.022).</p>
<p>The word- and phrase-level features were significantly more effective at predicting EEG responses for attended speech than for unattended speech in both normal-hearing (word: t=2.59, Cohen’s d=1.14, p=0.002; phrase: t=1.77, Cohen’s d=0.68, p=0.027) and hearing-impaired listeners (word: t=3.61, Cohen’s d=1.59, p=0.001; phrase: t=1.87, Cohen’s d=0.71, p=0.004). The significant time windows for word processing were from 150-250 ms for hearing-impaired listeners and 150-200 ms for normal-hearing listeners. For phrase processing, significant time windows were from 150-300 ms for hearing-impaired listeners and 250-300 ms for normal-hearing listeners. No significant discrepancies were observed between the two groups regarding the model fit of words and phrases to the EEG data for attended versus unattended speeches. Surprisingly, we found a significantly better model fit for sentence-level features in attended speech for normal-hearing participants (t=1.52, Cohen’s d=0.98, p=0.003) but not for hearing-impaired participants, and the contrast between the two groups was significant (t=1.7, Cohen’s d=1.27, p&lt;0.001), suggesting that hearing-impaired participants also struggle more with tracking information at longer temporal scales in multi-talker scenarios.</p>
<p>To validate the linguistic features from our HM-LSTM models, we also examined baseline models for the linguistic features using multivariate TRF (mTRF) analysis. Our predictors include phoneme, syllable, word, phrase, and sentence (i.e., marking a value of 1 at each unit boundary offset). Our EEG data spans the entire 10-minute duration for each condition, sampled at 10-ms intervals. The TRF results for our main comparison—attended versus unattended conditions—showed similar patterns to those observed using features from our HM-LSTM model (see <xref rid="figs2" ref-type="fig">Figure S2</xref>). At the phoneme and syllable levels, normal-hearing listeners showed marginally significantly higher TRF weights for attended speech compared to unattended speech at approximately -80 to 150 ms after phoneme offsets (t=2.75, Cohen’s d=0.87, p=0.057), and 120 to 210 ms after syllable offsets (t=3.96, Cohen’s d=0.73d = 0.73, p=0.083). At the word and phrase levels, normal-hearing listeners exhibited significantly higher TRF weights for attended speech compared to unattended speech at 190 to 290 ms after word offsets (t=4, Cohen’s d=1.13, p=0.049), and around 120 to 290 ms after phrase offsets (t=5.27, Cohen’s d=1.09, p=0.045). For hearing-impaired listeners, marginally significant effects were observed at 190 to 290 ms after word offsets (t=1.54, Cohen’s d=0.6, p=0.059), and 180 to 290 ms after phrase offsets (t=3.63, Cohen’s d=0.89, p=0.09).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Speech comprehension in a multi-talker environment is especially challenging for listeners with impaired hearing (<xref ref-type="bibr" rid="c15">Fuglsang et al., 2020</xref>). Consequently, exploring the neural underpinnings of multi-talker speech comprehension in hearing-impaired listeners could yield valuable insights into the challenges faced by both normal-hearing and hearing-impaired individuals in this scenario. Studies have reported abnormally enhanced responses to fluctuations in acoustic envelope in the central auditory system of older listeners (e.g., <xref ref-type="bibr" rid="c21">Goossens et al., 2016</xref>; <xref ref-type="bibr" rid="c37">Parthasarathy et al., 2019</xref>; <xref ref-type="bibr" rid="c39">Presacco et al., 2016</xref>) and listeners with peripheral hearing loss (<xref ref-type="bibr" rid="c20">Goossens et al., 2018</xref>; <xref ref-type="bibr" rid="c32">Millman et al., 2017</xref>). As older listeners also suffer from suppression of task-irrelevant sensory information due to reduced cortical inhibitory control functions (<xref ref-type="bibr" rid="c18">Gazzaley et al., 2005</xref>, <xref ref-type="bibr" rid="c17">2008</xref>), it is possible that impaired speech comprehension in a cocktail party situation arises from an attentional deficit linked to aging (<xref ref-type="bibr" rid="c14">Du et al., 2016</xref>; <xref ref-type="bibr" rid="c39">Presacco et al., 2016</xref>). However, younger listeners with EHF hearing loss have also reported difficulty understanding speech in a multi-talker environment (<xref ref-type="bibr" rid="c33">Motlagh Zadeh et al., 2019</xref>). It remains unknown what information is lost during multi-talker speech perception, and how the hierarchically organized linguistic units in competing speech streams affect the comprehension ability of people with impaired hearing.</p>
<p>In this study, we show that for normal-hearing listeners, the acoustic and linguistic features extracted from an HM-LSTM model can significantly predict EEG responses during both single-talker and attended speech in the context of two speakers talking simultaneously. Interestingly, their intelligibility scores for mixed speech are lower compared to single-talker speech, suggesting that normal-hearing listeners are still capable of tracking linguistic information at these levels in a cocktail party scenario, although with potentially greater effort. The model fit of the EEG data for attended speech is significantly higher than that for unattended speech across all levels. This aligns with previous research on “selective auditory attention,” which demonstrates that individuals can focus on specific auditory stimuli for processing, while effectively filtering out background noise (<xref ref-type="bibr" rid="c5">Brodbeck et al., 2018</xref>; <xref ref-type="bibr" rid="c6">Brungart, 2001</xref>; <xref ref-type="bibr" rid="c13">Ding &amp; Simon, 2012</xref>; <xref ref-type="bibr" rid="c31">Mesgarani &amp; Chang, 2012</xref>; <xref ref-type="bibr" rid="c36">O’Sullivan et al., 2015</xref>; <xref ref-type="bibr" rid="c42">Shinn-Cunningham, 2008</xref>; <xref ref-type="bibr" rid="c46">Zion Golumbic et al., 2013</xref>). Expanding on prior research which suggested that phonemes and words of attended speech could be decoded from the left temporal cortex of normal-hearing participants (<xref ref-type="bibr" rid="c5">Brodbeck et al., 2018</xref>), our results demonstrate that linguistic units across all hierarchical levels can be tracked in the neural signals.</p>
<p>For listeners with hearing impairments, the model fit for attended speech is significantly poorer at the acoustic, phoneme, and syllable levels compared to that for single-talker speech. Additionally, there is no significant difference in model fit at the acoustic, phoneme, and syllable levels between attended and unattended speech when two speakers are talking simultaneously. However, the model fit for the word and phrase features do not differ between single-talker and attended speech, and is significantly higher for that of the unattended speech. These findings suggest that hearing-impaired listeners may encounter difficulties in processing information at shorter temporal scales, including the dynamic amplitude envelope and spectrotemporal details of speech, as well as phoneme and syllable-level content. This is expected as our EHF hearing loss participants all exhibit higher hearing thresholds at frequencies above 8 kHz. Although these frequencies exceed those necessary for phonetic information in quiet environments, which are below 6 kHz, they may still impact the ability to process auditory information at faster temporal scales more than at slower speeds. Surprisingly, hearing-impaired listeners did not demonstrate an improved model fit for sentence features of the attended speech compared to the unattended speech, indicating that their ability to process information at longer temporal scales is also compromised. One limitation to consider is the absence of a behavioral task, such as comprehension questions at the end of the listening sections. This raises the possibility that the reduced cortical encoding of attended versus unattended speech across multiple linguistic levels in hearing-impaired listeners could stem from a different attentional strategy. For instance, they may focus on “getting the gist” of the story or intermittently disengage from the task, tuning back in only for selected keywords or word combinations. However, we would like to emphasize that our hearing-impaired participants have EHF hearing loss, with impairment limited to frequencies above 8 kHz. This condition is unlikely to be severe enough to induce a fundamentally different attentional strategy for this task. Furthermore, normal-hearing listeners may also employ varied attentional strategies, yet the comparison still revealed significant differences. Based on these findings, we hypothesize that hearing-impaired listeners may struggle to extract low-level information from competing speech streams. Such a disruption in bottom-up processing could impede their ability to discern sentence boundaries effectively, which in turn hampers their ability to benefit from top-down information processing.</p>
<p>The hierarchical temporal receptive window (TRW) hypothesis proposed that linguistic units at shorter temporal scales, such as phonemes, are encoded in the core auditory cortex, while information of longer duration is processed in higher perceptual and cognitive regions, such as the anterior temporal or posterior temporal and parietal regions (<xref ref-type="bibr" rid="c22">Hasson et al., 2008</xref>; <xref ref-type="bibr" rid="c23">Honey et al., 2012</xref>; <xref ref-type="bibr" rid="c25">Lerner et al., 2011</xref>; <xref ref-type="bibr" rid="c34">Murray et al., 2014</xref>). With the limited spatial resolution of EEG, we could not directly compare the spatial localization of these units at different temporal scales, however, we did observe an increasing latency in the significant model fit across different linguistic levels. Specifically, the significant time window for acoustic and phoneme features occurred around -100 to 100 ms relative to sentence offsets; syllables and words around 0-200 ms; and phrases and sentences around 200-300 ms. These progressively later effects from lower to higher linguistic levels suggest that these units may indeed be represented in brain regions with increasingly longer TRWs.</p>
<p>Our hierarchical linguistic contents were extracted using the HM-LSTM model adapted from (<xref ref-type="bibr" rid="c10">Chung et al., 2017</xref>). This model has been adopted by <xref ref-type="bibr" rid="c40">Schmitt et al. (2021)</xref> to show that a “surprisal hierarchy” based on the hidden layer activity correlated with fMRI blood oxygen level-dependent (BOLD) signals along the temporal-parietal pathway during naturalistic listening. Although their research question is different from ours, their results suggested that the model has effectively captured information at different linguistic levels. Our testing results further confirmed that the model representations at the phoneme and syllable levels are different from model representations at the higher linguistic levels when the phonemes within the sentences are similar. Compared to the increasingly popular “model-brain alignment” studies that typically use transformer architectures (e.g., (<xref ref-type="bibr" rid="c7">Caucheteux &amp; King, 2022</xref>; <xref ref-type="bibr" rid="c19">Goldstein et al., 2022</xref>; <xref ref-type="bibr" rid="c41">Schrimpf et al., 2021</xref>), our HM-LSTM model is considerably smaller in parameter size and does not match the capabilities of state-of-the-art large language models (LLMs) in downstream natural language processing (NLP) tasks such as question-answering, text summarization, translation, etc. However, our model incorporates phonemic and syllabic level representations, which are absent in LLMs that operate at the sub-word level. This feature could provide unique insights into how the entire hierarchy of linguistic units is processed in the brain.</p>
<p>It’s important to note that we do not assert any similarity between the model’s internal mechanisms and the brain’s mechanisms for processing linguistic units at different levels. Instead, we use the model to disentangle linguistic contents associated with these levels. This approach has proven successful in elucidating language processing in the brain, despite the notable dissimilarities in model architectures compared to the neural architecture of the brain. For example, <xref ref-type="bibr" rid="c35">Nelson et al. (2017)</xref> correlated syntactic processing under different parsing strategies with the intracranial electrophysiological signals and found that the left-corner and bottom-up strategies fit the left temporal data better than the most eager top-down strategy; <xref ref-type="bibr" rid="c19">Goldstein et al. (2022)</xref> and <xref ref-type="bibr" rid="c7">Caucheteux &amp; King (2022)</xref> also showed that the human brain and the deep learning language models share the computational principles as they process the same natural narrative.</p>
<p>In summary, our findings show that linguistic units extracted from a hierarchical language model better explain the EEG responses of normal-hearing listeners for attended speech, as opposed to unattended speech, when two speakers are talking simultaneously. However, hearing-impaired listeners exhibited poorer model fits at the acoustic, phoneme, syllable, and sentence levels, although their model fits at the word and phrase levels were not significantly affected. These results suggest that processing information at both shorter and longer temporal scales is especially challenging for hearing-impaired listeners when attending to a chosen speaker in a cocktail party situation. As such, these findings connect basic research on speech comprehension with clinical studies on hearing loss, especially hidden hearing loss, a global issue that is increasingly common among young adults.</p>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>A total of 51 participants (26 females, mean age=24 years, SD=2.12 years) with EHF hearing loss and 51 normal-hearing participants (26 females, mean age=22.92 years, SD=2.14 years) took part in the experiment. 28 participants (18 females, mean age=23.55, SD=2.18) were removed from the analyses due to excessive motion, drowsiness or inability to complete the experiment, resulting in a total of 41 participants (21 females, mean age=24.1, SD=2.1) with EHF hearing loss and 33 participants (13 females, mean age=22.94, SD=2.36) with normal-hearing. All participants were right-handed native Mandarin speakers currently studying in Shanghai for their undergraduate or graduate degree, with no self-reported neurological disorders. EHF hearing loss was diagnosed using the PTA test, thresholded at frequencies above 8 kHz. The PTA was performed by experienced audiological technicians using an audiometer (Madsen Astera, GN Otometrics, Denmark) with headphones (HDA-300, Sennheiser, Germany) in a soundproof booth with background noise below 25 dB(A), as described previously (<xref ref-type="bibr" rid="c44">Wang et al., 2021</xref>). Air-conduction audiometric thresholds for both ears at frequencies of 0.5, 1, 2, 3, 4, 6, 8, 10, 12.5, 14 and 16 kHz were measured in 5-dB steps in accordance with the regulations of ISO 8253-1:2010.</p>
</sec>
<sec id="s4b">
<title>Stimuli</title>
<p>Our experimental stimuli were two excerpts from the Chinese translation of “The Little Prince” (available at <ext-link ext-link-type="uri" xlink:href="http://www.xiaowangzi.org/">http://www.xiaowangzi.org/</ext-link>), previously used in fMRI studies where participants with normal hearing listened to the book in its entirety (<xref ref-type="bibr" rid="c27">Li et al., 2022</xref>). This material has been enriched with detailed linguistic predictions, from lexical to syntactic and discourse levels, using advanced natural language processing tools. Such rich annotation is critical for modeling hierarchical linguistic structures in our study. The two excerpts were narrated by one male and one female computer-synthesized voice, developed by the Institute of Automation, Chinese Academy of Sciences. The synthesized speech (available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/fjv5n/">https://osf.io/fjv5n/</ext-link>) is comparable to human narration, as confirmed by participants’ post-experiment assessment of its naturalness. Additionally, using computer-synthesized voice instead of human-narrated speech alleviates the potential issue of imbalanced voice intensity and speaking rate that can arise between female and male narrators. The two sections were matched in length (approximately 10 minutes) and mean amplitude (approximately 65 dB), and were mixed digitally in a single channel to prevent any biases in hearing ability between the left and right ears.</p>
</sec>
<sec id="s4c">
<title>Experimental procedure</title>
<p>The experimental task consisted of a multi-talker condition and a single-talker condition. In the multi-talker condition, the mixed speech was presented twice with the female and male speakers narrating simultaneously. Before each trial, instructions appeared in the center of the screen indicating which of the talkers to attend to (e.g., ‘‘Attend Female’’). In the single-talker condition, the male and female speeches were presented separately (see <xref rid="fig1" ref-type="fig">Figure 1A</xref> for the experiment procedure). The presentation order of the 4 conditions was randomized, and breaks were given between each trial. Stimuli were presented using insert earphones (ER-3C, Etymotic Research, United States) at a comfortable volume level of approximately 65 dB SPL. Participants were instructed to maintain visual fixation for the duration of each trial on a crosshair centered on the computer screen, and to minimize eye blinking and all other motor activities for the duration of each section. The whole experiment lasted for about 65 minutes, and participants rated the intelligibility of the multi-talker and the single-talker speeches on a 5-point Likert scale after the experiment. The experiment was conducted at the Department of Otolaryngology-Head and Neck Surgery, Shanghai Ninth People’s Hospital affiliated with the School of Medicine at Shanghai Jiao Tong University. The experimental procedures were approved by the Ethics Committee of the Ninth People’s Hospital affiliated with Shanghai Jiao Tong University School of Medicine (SH9H-2019-T33-2). All participants provided written informed consent prior to the experiment and were paid for their participation.</p>
</sec>
<sec id="s4d">
<title>Acoustic features of the speech stimuli</title>
<p>The acoustic features included the broadband envelopes and the log-mel spectrograms of the two single-talker speech streams. The amplitude envelope of the speech signal was extracted using the Hilbert transform. The 129-dimension spectrogram and 1-dimension envelope were concatenated to form a 130-dimension acoustic feature at every 10 ms of the speech stimuli.</p>
</sec>
<sec id="s4e">
<title>Hierarchical multiscale LSTM model</title>
<p>We extended the original HM-LSTM model developed by <xref ref-type="bibr" rid="c10">Chung et al. (2017)</xref> to include not just the word and phrasal levels but also the sub-lexical phoneme and syllable levels. The model inputs were individual phonemes from two sentences, each transformed into a 1024-dimensional vector using a simple lookup table. This lookup table stores embeddings for a fixed dictionary of all unique phonemes in Chinese. This approach is a foundational technique in many advanced NLP models, enabling the representation of discrete input symbols in a continuous vector space. The output of the model was the classification result of whether the second sentence follows the first sentence. At each layer, the model implements a COPY or UPDATE operation at each time step <italic>t</italic>. The COPY operation maintains the current cell state of without any changes until it receives a summarized input from the lower layer. The UPDATE operation occurs when a linguistic boundary is detected in the layer below, but no boundary was detected at the previous time step <italic>t-1</italic>. In this case, the cell updates its summary representation, similar to standard RNNs. We trained our model on 10,000 sentence pairs from the WenetSpeech corpus (<xref ref-type="bibr" rid="c45">Zhang et al., 2021</xref>), a collection that features over 10,000 hours of labeled Mandarin Chinese speech sourced from YouTube and podcasts. We used 1024 units for the input embedding and 2048 units for each HM-LSTM layer.</p>
</sec>
<sec id="s4f">
<title>Correlations among LSTM model layers</title>
<p>All the regressors are represented as 2048-dimensional vectors derived from the hidden layers of the trained HM-LSTM model. We applied the trained model to all 284 sentences in our stimulus text, generating a set of 284 × 2048-dimensional vectors. Next, we performed Principal Component Analysis (PCA) on the 2048 dimensions and extracted the first 100 principal components (PCs), resulting in 284 × 100-dimensional vectors for each regressor. These 284 × 100 matrices were then flattened into 28,400-dimensional vectors. Subsequently, we computed the correlation matrix for the z-transformed 28,400-dimensional vectors of our five linguistic regressors.</p>
</sec>
<sec id="s4g">
<title>EEG recording and preprocessing</title>
<p>EEG was recorded using a standard 64-channel actiCAP mounted according to the international 10-20 system against a nose reference (Brain Vision Recorder, Brain Products). The ground electrode was set at the forehead. EEG signals were registered between 0.016 and 80 Hz with a sampling rate of 500 Hz. The impedances were kept below 20 kΩ. The EEG recordings were band-pass filtered between 0.1 Hz and 45 Hz using a linear-phase finite impulse response (FIR) filter. Independent component analysis (ICA) was then applied to remove eye blink artifacts. The EEG data were then segmented into epochs spanning 500 ms pre-stimulus onset to 10 minutes post-stimulus onset and were subsequently downsampled to 100 Hz. We further decomposed the epoched EEG time series for each section into six classic frequency bands components (delta 1–3 Hz, theta 4–7 Hz, alpha 8–12 Hz, beta 12–20 Hz, gamma 30–45 Hz) by convolving the data with complex Morlet wavelets as implemented in MNE-Python (version 0.24.0). The number of cycles in the Morlet wavelets was set to frequency/4 for each frequency bin. The power values for each time point and frequency bin were obtained by taking the square root of the resulting time-frequency coefficients. These power values were normalized to reflect relative changes (expressed in dB) with respect to the 500 ms pre-stimulus baseline. This yielded a power value for each time point and frequency bin for each section.</p>
</sec>
<sec id="s4h">
<title>Ridge regression at different time latencies</title>
<p>For each subject, we modeled the EEG responses at each sensor from the single-talker and mixed-talker conditions with our acoustic and linguistic features using ridge regression with a default regularization coefficient of 1 (see <xref rid="fig1" ref-type="fig">Figure 1B</xref>). For each sentence in the speech stimuli, we extracted the 2048-dimensional hidden layer activity from the HM-LSTM model to represent features at the phoneme, syllable, word, phrase and sentence levels. We then employed Principal Component Analysis (PCA) to reduce the 2048-dimensional vectors to the first 150 principal components. The 150-dimensional vectors for the 5 linguistic levels were then fit to the EEG signals time-locked to the offset of each sentence in the stimuli using ridge regression. The regressors are aligned to sentence offsets because all our regressors are taken from the hidden layer of our HM-LSTM model, which generates vector representations corresponding to the five linguistic levels of the entire sentence. To assess the temporal progression of the regression outcomes, we conducted the analysis at nine sequential time points, ranging from 100 milliseconds before to 300 milliseconds after the sentence offset, with a 50-millisecond interval. We chose this time window as lexical or phrasal processing typically occurs 200 ms after stimulus offsets (Bemis &amp; Pylkkanen, 2011; <xref ref-type="bibr" rid="c19">Goldstein et al., 2022</xref>; Li et al., 2024; Li &amp; Pylkkänen, 2021). Additionally, we included the -100 to 200 ms time period in our analysis to examine phoneme and syllable level processing (e.g., Gwilliams et al., 2022). Using the entire sentence duration was not feasible, as the sentences in the stimuli vary in length, making statistical analysis challenging. Additionally, since the stimuli consist of continuous speech, extending the time window would risk including linguistic units from subsequent sentences. This would introduce ambiguity as to whether the EEG responses correspond to the current or the following sentence. The same regression procedure was applied to the 130-dimensional acoustic features. Both the EEG data and the regressors were z-scored before regression. The ridge regression was performed using customary python codes, making heavy use of the scikit-learn (1.2.2) package.</p>
</sec>
<sec id="s4i">
<title>Spatiotemporal clustering analysis</title>
<p>The timecourses of the z-transformed coefficient of determination (R<sup>2</sup>) from the regression results at the nine time points for each sensor, corresponding to the linguistic and acoustic regressor for each subject, underwent spatiotemporal cluster permutation test (<xref ref-type="bibr" rid="c29">Maris &amp; Oostenveld, 2007</xref>) to determine their statistical significance at the group level. For instance, to assess whether words from the attended stimuli better predict EEG signals during the mixed speech compared to words from the unattended stimuli, we used the 150-dimensional vectors corresponding to the word layer from our LSTM model for the attended and unattended stimuli as regressors. We then fit these regressors to the EEG signals at 9 time points (spanning -100 ms to 300 ms around the sentence offsets, with 50 ms intervals). We then conducted one-tailed two-sample t-tests to determine whether the differences in the contrasts of the z-transformed R² timecourses were statistically significant. We repeated these procedures 10,000 times, replacing the observed t-values with shuffled t-values for each participant to generate a null distribution of t-values for each sensor. Sensors whose t-values were in the top 5th percentile of the null distribution were deemed significant (sensor-wise significance p &lt; 0.05). The same method was applied to analyze the contrasts between attended and unattended speech during mixed speech conditions, both within and between groups. All our analyses were performed using custom python codes, making heavy use of the mne (v.1.6.1), torch (v2.2.0), scipy (v1.12.0) and scikit-learn (1.2.2) packages.</p>
</sec>
</sec>
</body>
<back>
<sec id="s6">
<title>Supplementary</title>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Table S1.</label>
<caption><title>All four-syllable Chinese sentences with same vowels.</title></caption>
<graphic xlink:href="599315v2_tbls1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1.</label>
<caption><title>Correlation matrices of regression outcomes for the five linguistic predictors between the EEG data from delta, theta and all frequency bands.</title></caption>
<graphic xlink:href="599315v2_figs1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2.</label>
<caption><title>Contrast of TRF weights to the EEG data of attended and unattended speech for the five linguistic predictors.</title></caption>
<graphic xlink:href="599315v2_figs2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s5" sec-type="data-availability">
<title>Data and code availability</title>
<p>All data and codes are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/fjv5n/">https://osf.io/fjv5n/</ext-link>.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Apoux</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Bacon</surname>, <given-names>S. P</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Relative importance of temporal information in various frequency regions for consonant identification in quiet and in noise</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>116</volume>(<issue>3</issue>), <fpage>1671</fpage>–<lpage>1680</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Badri</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Siegel</surname>, <given-names>J. H.</given-names></string-name>, &amp; <string-name><surname>Wright</surname>, <given-names>B. A</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Auditory filter shapes and high-frequency hearing in adults who have impaired speech in noise performance despite clinically normal audiograms</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>129</volume>(<issue>2</issue>), <fpage>852</fpage>–<lpage>863</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bharadwaj</surname>, <given-names>H. M.</given-names></string-name>, <string-name><surname>Mai</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Simpson</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Choi</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Heinz</surname>, <given-names>M. G.</given-names></string-name>, &amp; <string-name><surname>Shinn-Cunningham</surname>, <given-names>B. G</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Non-invasive assays of cochlear synaptopathy – Candidates and considerations</article-title>. <source>Neuroscience</source>, <volume>407</volume>, <fpage>53</fpage>–<lpage>66</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blank</surname>, <given-names>I. A.</given-names></string-name>, &amp; <string-name><surname>Fedorenko</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2020</year>). <article-title>No evidence for differences among language regions in their temporal receptive windows</article-title>. <source>NeuroImage</source>, <volume>219</volume>, <fpage>116925</fpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brodbeck</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Hong</surname>, <given-names>L. E.</given-names></string-name>, &amp; <string-name><surname>Simon</surname>, <given-names>J. Z</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Rapid transformation from auditory to linguistic representations of continuous speech</article-title>. <source>Current Biology</source>, <volume>28</volume>(<issue>24</issue>), <fpage>3976</fpage>–<lpage>3983.</lpage> </mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brungart</surname>, <given-names>D. S</given-names></string-name></person-group>. (<year>2001</year>). <article-title>Informational and energetic masking effects in the perception of two simultaneous talkers</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>109</volume>(<issue>3</issue>), <fpage>1101</fpage>–<lpage>1109</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Caucheteux</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>King</surname>, <given-names>J.-R</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Brains and algorithms partially converge in natural language processing</article-title>. <source>Communications Biology</source>, <volume>5</volume>(<issue>1</issue>), <fpage>134</fpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname>, <given-names>C. H. C.</given-names></string-name>, <string-name><surname>Nastase</surname>, <given-names>S. A.</given-names></string-name>, &amp; <string-name><surname>Hasson</surname>, <given-names>U</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Information flow across the cortical timescale hierarchy during narrative construction</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>119</volume>(<issue>51</issue>), <fpage>e2209307119</fpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cherry</surname>, <given-names>E. C</given-names></string-name></person-group>. (<year>1953</year>). <article-title>Some experiments on the recognition of speech, with one and with two ears</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>25</volume>(<issue>5</issue>), <fpage>975</fpage>–<lpage>979</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Chung</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ahn</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Bengio</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Hierarchical multiscale recurrent neural networks</article-title>. <source>arXiv</source>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Collins</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Cullen</surname>, <given-names>J. K.</given-names></string-name>, &amp; <string-name><surname>Berlin</surname>, <given-names>C. I</given-names></string-name></person-group>. (<year>1981</year>). <article-title>Auditory signal processing in a hearing-impaired subject with residual ultra-audiometric hearing</article-title>. <source>Audiology: Official Organ of the International Society of Audiology</source>, <volume>20</volume>(<issue>4</issue>), <fpage>347</fpage>–<lpage>361</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ding</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mellon</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Tian</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Poeppel</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Cortical tracking of hierarchical linguistic structures in connected speech</article-title>. <source>Nature Neuroscience</source>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ding</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Simon</surname>, <given-names>J. Z</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Emergence of neural encoding of auditory objects while listening to competing speakers</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>109</volume>(<issue>29</issue>), <fpage>11854</fpage>–<lpage>11859</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Du</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Buchsbaum</surname>, <given-names>B. R.</given-names></string-name>, <string-name><surname>Grady</surname>, <given-names>C. L.</given-names></string-name>, &amp; <string-name><surname>Alain</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Increased activity in frontal motor cortex compensates impaired speech perception in older adults</article-title>. <source>Nature Communications</source>, <volume>7</volume>(<issue>1</issue>), <fpage>12241</fpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fuglsang</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Märcher-Rørsted</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Dau</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Hjortkjær</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Effects of sensorineural hearing loss on cortical synchronization to competing speech during selective attention</article-title>. <source>Journal of Neuroscience</source>, <volume>40</volume>(<issue>12</issue>), <fpage>2562</fpage>–<lpage>2572</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Gao</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Huang</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Measuring meaning composition in the human brain with composition scores from large language models</article-title>. <conf-name>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</conf-name> (pp. <fpage>11295</fpage>–<lpage>11308</lpage>). <publisher-name>Association for Computational Linguistics</publisher-name>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gazzaley</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Clapp</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Kelley</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>McEvoy</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Knight</surname>, <given-names>R. T.</given-names></string-name>, &amp; <string-name><surname>D’Esposito</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Age-related top-down suppression deficit in the early stages of cortical visual memory processing</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>105</volume>(<issue>35</issue>), <fpage>13122</fpage>–<lpage>13126</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gazzaley</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cooney</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Rissman</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>D’Esposito</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Top-down suppression deficit underlies working memory impairment in normal aging</article-title>. <source>Nature Neuroscience</source>, <volume>8</volume>(<issue>10</issue>), <fpage>1298</fpage>–<lpage>1300</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goldstein</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zada</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Buchnik</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Schain</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Price</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Aubrey</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Nastase</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Feder</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Emanuel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Cohen</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jansen</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gazula</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Choe</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Rao</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Casto</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Fanda</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Doyle</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Friedman</surname>, <given-names>D.</given-names></string-name>, <etal>…</etal> <string-name><surname>Hasson</surname>, <given-names>U</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Shared computational principles for language processing in humans and deep language models</article-title>. <source>Nature Neuroscience</source>, <volume>25</volume>(<issue>3</issue>), <fpage>Article 3</fpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goossens</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Vercammen</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Wouters</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>van Wieringen</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Neural envelope encoding predicts speech perception performance for normal-hearing and hearing-impaired adults</article-title>. <source>Hearing Research</source>, <volume>370</volume>, <fpage>189</fpage>–<lpage>200</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goossens</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Vercammen</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Wouters</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Wieringen</surname>, <given-names>A. van.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Aging affects neural synchronization to speech-related acoustic modulations</article-title>. <source>Frontiers in Aging Neuroscience</source>, <volume>8</volume>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hasson</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Vallines</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Heeger</surname>, <given-names>D. J.</given-names></string-name>, &amp; <string-name><surname>Rubin</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2008</year>). <article-title>A hierarchy of temporal receptive windows in human cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>28</volume>(<issue>10</issue>), <fpage>2539</fpage>–<lpage>2550</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Honey</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Thesen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Donner</surname>, <given-names>T. H.</given-names></string-name>, <string-name><surname>Silbert</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Carlson</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Devinsky</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Doyle</surname>, <given-names>W. K.</given-names></string-name>, <string-name><surname>Rubin</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Heeger</surname>, <given-names>D. J.</given-names></string-name>, &amp; <string-name><surname>Hasson</surname>, <given-names>U</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Slow cortical dynamics and the accumulation of information over long timescales</article-title>. <source>Neuron</source>, <volume>76</volume>(<issue>2</issue>), <fpage>423</fpage>–<lpage>434</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hubel</surname>, <given-names>D. H.</given-names></string-name>, &amp; <string-name><surname>Wiesel</surname>, <given-names>T. N</given-names></string-name></person-group>. (<year>1962</year>). <article-title>Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex</article-title>. <source>The Journal of Physiology</source>, <volume>160</volume>(<issue>1</issue>), <fpage>106</fpage>–<lpage>154</lpage>.2.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lerner</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Honey</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Silbert</surname>, <given-names>L. J.</given-names></string-name>, &amp; <string-name><surname>Hasson</surname>, <given-names>U</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Topographic mapping of a hierarchy of temporal receptive windows using a narrated story</article-title>. <source>Journal of Neuroscience</source>, <volume>31</volume>(<issue>8</issue>), <fpage>2906</fpage>–<lpage>2915</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Levy</surname>, <given-names>S. C.</given-names></string-name>, <string-name><surname>Freed</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Nilsson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Moore</surname>, <given-names>B. C. J.</given-names></string-name>, &amp; <string-name><surname>Puria</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Extended high-frequency bandwidth improves speech reception in the presence of spatially separated masking speech</article-title>. <source>Ear and Hearing</source>, <volume>36</volume>(<issue>5</issue>), <fpage>e214</fpage>–<lpage>224</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bhattasali</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Franzluebbers</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Luh</surname>, <given-names>W.-M.</given-names></string-name>, <string-name><surname>Spreng</surname>, <given-names>R. N.</given-names></string-name>, <string-name><surname>Brennan</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Pallier</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Hale</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Le Petit Prince multilingual naturalistic fMRI corpus</article-title>. <source>Scientific Data</source>, <volume>9</volume>(<issue>1</issue>), <fpage>Article 1</fpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luo</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Poeppel</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex</article-title>. <source>Neuron</source>, <volume>54</volume>(<issue>6</issue>), <fpage>1001</fpage>–<lpage>1010</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Oostenveld</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>164</volume>(<issue>1</issue>), <fpage>177</fpage>–<lpage>190</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDermott</surname>, <given-names>J. H</given-names></string-name></person-group>. (<year>2009</year>). <article-title>The cocktail party problem</article-title>. <source>Current Biology</source>, <volume>19</volume>(<issue>22</issue>), <fpage>R1024</fpage>–<lpage>R1027</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mesgarani</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Chang</surname>, <given-names>E. F</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Selective cortical representation of attended speaker in multi-talker speech perception</article-title>. <source>Nature</source>, <volume>485</volume>(<issue>7397</issue>), <fpage>233</fpage>–<lpage>236</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Millman</surname>, <given-names>R. E.</given-names></string-name>, <string-name><surname>Mattys</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Gouws</surname>, <given-names>A. D.</given-names></string-name>, &amp; <string-name><surname>Prendergast</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Magnified neural envelope coding predicts deficits in speech perception in noise</article-title>. <source>The Journal of Neuroscience</source>, <volume>37</volume>(<issue>32</issue>), <fpage>7727</fpage>–<lpage>7736</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Motlagh Zadeh</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Silbert</surname>, <given-names>N. H.</given-names></string-name>, <string-name><surname>Sternasty</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Swanepoel</surname>, <given-names>D. W.</given-names></string-name>, <string-name><surname>Hunter</surname>, <given-names>L. L.</given-names></string-name>, &amp; <string-name><surname>Moore</surname>, <given-names>D. R.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Extended high-frequency hearing enhances speech perception in noise</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>116</volume>(<issue>47</issue>), <fpage>23753</fpage>–<lpage>23759</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murray</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Bernacchia</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Freedman</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Romo</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Wallis</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Cai</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Padoa-Schioppa</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Pasternak</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Seo</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Wang</surname>, <given-names>X.-J</given-names></string-name></person-group>. (<year>2014</year>). <article-title>A hierarchy of intrinsic timescales across primate cortex</article-title>. <source>Nature Neuroscience</source>, <volume>17</volume>(<issue>12</issue>), <fpage>1661</fpage>–<lpage>1663</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nelson</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Karoui</surname>, <given-names>I. E.</given-names></string-name>, <string-name><surname>Giber</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Cohen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Koopman</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Cash</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Naccache</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Hale</surname>, <given-names>J. T.</given-names></string-name>, <string-name><surname>Pallier</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Dehaene</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Neurophysiological dynamics of phrase-structure building during sentence processing</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>114</volume>(<issue>18</issue>), <fpage>E3669</fpage>–<lpage>E3678</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Sullivan</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Power</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Mesgarani</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Rajaram</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Foxe</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Shinn-Cunningham</surname>, <given-names>B. G.</given-names></string-name>, <string-name><surname>Slaney</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Shamma</surname>, <given-names>S. A.</given-names></string-name>, &amp; <string-name><surname>Lalor</surname>, <given-names>E. C</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Attentional selection in a cocktail party environment can be decoded from single-trial EEG</article-title>. <source>Cerebral Cortex</source>, <volume>25</volume>(<issue>7</issue>), <fpage>1697</fpage>–<lpage>1706</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parthasarathy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Herrmann</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Bartlett</surname>, <given-names>E. L</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Aging alters envelope representations of speech-like sounds in the inferior colliculus</article-title>. <source>Neurobiology of Aging</source>, <volume>73</volume>, <fpage>30</fpage>–<lpage>40</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peelle</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Chandrasekaran</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Powers</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>E. E.</given-names></string-name>, &amp; <string-name><surname>Grossman</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Age-related vulnerability in the neural systems supporting semantic processing</article-title>. <source>Frontiers in Aging Neuroscience</source>, <volume>5</volume>, <fpage>46</fpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Presacco</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name>, &amp; <string-name><surname>Anderson</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Evidence of degraded representation of speech in noise, in the aging midbrain and cortex</article-title>. <source>Journal of Neurophysiology</source>, <volume>116</volume>(<issue>5</issue>), <fpage>2346</fpage>–<lpage>2355</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schmitt</surname>, <given-names>L.-M.</given-names></string-name>, <string-name><surname>Erb</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Tune</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Rysop</surname>, <given-names>A. U.</given-names></string-name>, <string-name><surname>Hartwigsen</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Obleser</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Predicting speech from a cortical hierarchy of event-based time scales</article-title>. <source>Science Advances</source>, <volume>7</volume>(<issue>49</issue>), <fpage>eabi6070</fpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schrimpf</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Blank</surname>, <given-names>I. A.</given-names></string-name>, <string-name><surname>Tuckute</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Kauf</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Hosseini</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name>, &amp; <string-name><surname>Fedorenko</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2021</year>). <article-title>The neural architecture of language: Integrative modeling converges on predictive processing</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>118</volume>(<issue>45</issue>), <fpage>e2105646118</fpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shinn-Cunningham</surname>, <given-names>B. G</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Object-based auditory and visual attention</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>12</volume>(<issue>5</issue>), <fpage>182</fpage>–<lpage>186</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sugimoto</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Yoshida</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Jeong</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Koizumi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brennan</surname>, <given-names>J. R.</given-names></string-name>, &amp; <string-name><surname>Oseki</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Localizing Syntactic Composition with Left-Corner Recurrent Neural Network Grammars</article-title>. <source>Neurobiology of Language</source>, <volume>5</volume>(<issue>1</issue>), <fpage>201</fpage>–<lpage>224</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Qian</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hong</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Wu</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Acute recreational noise-induced cochlear synaptic dysfunction in humans with normal hearing: A prospective cohort study</article-title>. <source>Frontiers in Neuroscience</source>, <volume>15</volume>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Lv</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Guo</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Shao</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Xie</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Bu</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Zeng</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Peng</surname>, <given-names>Z.</given-names></string-name></person-group> (<year>2021</year>). <article-title>WenetSpeech: A 10000+ hours multi-domain mandarin corpus for speech recognition</article-title>. <source>arXiv</source>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zion Golumbic</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Bickel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Schevon</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>McKhann</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>Goodman</surname>, <given-names>R. R.</given-names></string-name>, <string-name><surname>Emerson</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Mehta</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name>, <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Schroeder</surname>, <given-names>C. E.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Mechanisms underlying selective neuronal tracking of attended speech at a “cocktail party.”</article-title> <source>Neuron</source>, <volume>77</volume>(<issue>5</issue>), <fpage>980</fpage>–<lpage>991</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100056.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ding</surname>
<given-names>Nai</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Zhejiang University</institution>
</institution-wrap>
<city>Hangzhou</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study combines a computational language model, i.e., HM-LSTM, and temporal response function (TRF) modeling to quantify the neural encoding of hierarchical linguistic information in speech, and addresses how hearing impairment affects neural encoding of speech. The analysis has been significantly improved during the revision but remain somewhat <bold>incomplete</bold> - The TRF analysis should be more clearly described and controlled. The study is of potential interest to audiologists and researchers who are interested in the neural encoding of speech.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100056.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>About R squared in the plots:</p>
<p>
The authors have used a z-scored R squared in the main ridge regression plots. While this may be interpretable, it seems non-standard and overly complicated. The authors could use a simple Pearson r to be most direct and informative (and in line with similar work, including Goldstein et al. 2022 which they mentioned). This way the sign of the relationships is preserved.</p>
<p>About the new TRF analysis:</p>
<p>
The new TRF analysis is a necessary addition and much appreciated. However, it is missing the results for the acoustic regressors, which should be there analogous to the HM-LSTM ridge analysis. The authors should also specify which software they have utilized to conduct the new TRF analysis. It also seems that the linguistic predictors/regressors have been newly constructed in a way more consistent with previous literature (instead of using the HM-LSTM features); these specifics should also be included in the manuscript (did it come from Montreal Forced Aligner, etc.?). Now that the original HM-LSTM can be compared to a more standard TRF analysis, it is apparent that the results are similar.</p>
<p>The authors' wording about this suggests that these new regressors have a nonzero sample at each linguistic event's offset, not onset. This should also be clarified. As the authors know, the onset would be more standard, and using the offset has implications for understanding the timing of the TRFs, as a phoneme has a different duration than a word, which has a different duration from a sentence, etc.</p>
<p>About offsets:</p>
<p>
TRFs can still be interpretable using the offset timings though; however, the main original analysis seems to be utilizing the offset times in a different, more confusing way. The authors still seem to be saying that only the peri-offset time of the EEG was analyzed at all, meaning the vast majority of the EEG trial durations do not factor into the main HM-LSTM response results whatsoever. The way the authors describe this does not seem to be present in any other literature, including the papers that they cite. Therefore, much more clarification on this issue is needed. If the authors mean that the regressors are simply time-locked to the EEG by aligning their offsets (rather than their onsets, because they have varying onsets or some such experimental design complexity), then this would be fine. But it does not seem to be what the authors want to say. This may be a miscommunication about the methods, or the authors may have actually only analyzed a small portion of the data. Either way, this should be clarified to be able to be interpretable.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100056.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study presents a valuable finding on the neural encoding of speech in listeners with normal hearing and hearing impairment, uncovering marked differences in how attention to different levels of speech information is allocated, especially when having to selectively attend to one speaker while ignoring an irrelevant speaker. The results overall support the claims of the authors, although a more explicit behavioural task to demonstrate successful attention allocation would have strengthened the study. Importantly, the use of more &quot;temporally continuous&quot; analysis frameworks could have provided a better methodology to assess the entire time course of neural activity during speech listening. Despite these limitations, this interesting work will be useful to the hearing impairment and speech processing research community.</p>
<p>The study compares speech-in-quiet vs. multi-talker scenarios, allowing to assess within-participant the impact that the addition of a competing talker has on the neural tracking of speech. Moreover, the inclusion of a population with hearing loss is useful to disentangle the effects of attention orienting and hearing ability. The diagnosis of high-frequency hearing loss was done as part of the experimental procedure by professional audiologists, leading to a high control of the main contrast of interest for the experiment. Sample size was big, allowing to draw meaningful comparisons between the two populations.</p>
<p>An HM-LSTM model was employed to jointly extract speech features spanning from the stimulus acoustics to word-level and phrase-level information, represented by embeddings extracted at successive layers of the model. The model was specifically expanded to include lower level acoustic and phonetic information, reaching a good representation of all intermediate levels of speech.</p>
<p>Despite conveniently extracting all features jointly, the HM-LSTM model processes linguistic input sentence-by-sentence, and therefore only allows to assess the corresponding EEG data at sentence offset. If I understood correctly, while the sentence information extracted with the HM-LSTM reflects the entire sentence - in terms of its acoustic, phonetic and more abstract linguistic features - it only gives a condensed final representation of the sentence. As such, feature extraction with the HM-LSTM is not compatible with a continuous temporal mapping on the EEG signal, and this is the main reason behind the authors' decision to fit a regression at nine separate time points surrounding sentence offsets.</p>
<p>While valid and previously used in the literature, this methodology, in the particular context of this experiment, might be obscuring important attentional effects impacted by hearing-loss. By fitting a regression only around sentence-final speech representations, the method might be overlooking the more &quot;online&quot; speech processing dynamics, and only assessing the permanence of information at different speech levels at sentence offset. In other words, the acoustic attentional bias between Attended and Unattended speech might exist even in hearing-impaired participants but, due to a lower encoding or permanence of acoustic information in this population, it might only emerge when using methodologies with a higher temporal resolution, such as Temporal Response Functions (TRFs). If a univariate TRF fit simply on the continuous speech envelope did not show any attentional bias (different trial lengths should not be a problem for fitting TRFs), I would be entirely convinced of the result. For now, I am unsure on how to interpret this finding.</p>
<p>Despite my doubts on the appropriateness of condensed speech representations and single-point regression for acoustic features in particular, the current methodology allows the authors to explore their research questions, and the results support their conclusions.</p>
<p>This work presents an interesting finding on the limits of attentional bias in a cocktail-party scenario, suggesting that fundamentally different neural attentional filters are employed by listeners with high-frequency hearing loss, even in terms of the tracking of speech acoustics. Moreover, the rich dataset collected by the authors is a great contribution to open science and will offer opportunities for re-analysis.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100056.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Jixing</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5210-6224</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Wang</surname>
<given-names>Qixuan</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhou</surname>
<given-names>Qian</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Yang</surname>
<given-names>Lu</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Shen</surname>
<given-names>Yutong</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Huang</surname>
<given-names>Shujian</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wang</surname>
<given-names>Shaonan</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Pylkkänen</surname>
<given-names>Liina</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Huang</surname>
<given-names>Zhiwu</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews</p>
<disp-quote content-type="editor-comment">
<p><bold>eLife Assessment</bold></p>
<p>This valuable study investigates how hearing impairment affects neural encoding of speech, in particular the encoding of hierarchical linguistic information. The current analysis provides incomplete evidence that hearing impairment affects speech processing at multiple levels, since the novel analysis based on HM-LSTM needs further justification. The advantage of this method should also be further explained. The study can also benefit from building a stronger link between neural and behavioral data.</p>
</disp-quote>
<p>We sincerely thank the editors and reviewers for their detailed and constructive feedback.</p>
<p>We have revised the manuscript to address all of the reviewers’ comments and suggestions. The primary strength of our methods lies in the use of the HM-LSTM model, which simultaneously captures linguistic information at multiple levels, ranging from phonemes to sentences. As such, this model can be applied to other questions regarding hierarchical linguistic processing. We acknowledge that our current behavioral results from the intelligibility test may not fully differentiate between the perception of lower-level acoustic/phonetic information and higher-level meaning comprehension. However, it remains unclear what type of behavioral test would effectively address this distinction. We aim to xplore this connection further in future studies.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>The authors are attempting to use the internal workings of a language hierarchy model, comprising phonemes, syllables, words, phrases, and sentences, as regressors to predict EEG recorded during listening to speech. They also use standard acoustic features as regressors, such as the overall envelope and the envelopes in log-spaced frequency bands. This is valuable and timely research, including the attempt to show differences between normal-hearing and hearing-impaired people in these regards. I will start with a couple of broader questions/points, and then focus my comments on three aspects of this study: The HM-LSTM language model and its usage, the time windows of relevant EEG analysis, and the usage of ridge regression.</p>
<p>Firstly, as far as I can tell, the OSF repository of code, data, and stimuli is not accessible without requesting access. This needs to be changed so that reviewers and anybody who wants or needs to can access these materials.</p>
</disp-quote>
<p>It is my understanding that keeping the repository private during the review process and making them public after acceptance is standard practice. As far as I understand, although the OSF repository was private, anyone with the link should be able to access it. I have now made the repository public.</p>
<disp-quote content-type="editor-comment">
<p>What is the quantification of model fit? Does it mean that you generate predicted EEG time series from deconvolved TRFs, and then give the R2 coefficient of determination between the actual EEG and predicted EEG constructed from the convolution of TRFs and regressors? Whether or not this is exactly right, it should be made more explicit.</p>
</disp-quote>
<p>Model fit was measured by spatiotemporal cluster permutation tests (Maris &amp; Oostenveld, 2007) on the contrasts of the timecourses of the z-transformed coefficient of determination (R<sup>2</sup>). For instance, to assess whether words from the attended stimuli better predict EEG signals during the mixed speech compared to words from the unattended stimuli, we used the 150dimensional vectors corresponding to the word layer from our LSTM model for the attended and unattended stimuli as regressors. We then fit these regressors to the EEG signals at 9 time points (spanning -100 ms to 300 ms around the sentence offsets, with 50 ms intervals). We then conducted one-tailed two-sample t-tests to determine whether the differences in the contrasts of the R<sup>2</sup> timecourses were statistically significant. Note that we did not perform TRF analyses. We have clarified this description in the “Spatiotemporal clustering analysis” section of the “Methods and Materials” on p.10 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>About the HM-LSTM:</p>
<p>• In the Methods paragraph about the HM-LSTM, a lot more detail is necessary to understand how you are using this model. Firstly, what do you mean that you &quot;extended&quot; it, and what was that procedure?</p>
</disp-quote>
<p>The original HM-LSTM model developed by Chung et al. (2017) consists of only two levels: the word level and the phrase level (Figure 1b from their paper). By “extending” the model, we mean that we expanded its architecture to include five levels: phoneme, syllable, word, phrase, and sentence. Since our input consists of phoneme embeddings, we cannot directly apply their model, so we trained our model on the WenetSpeech corpus (Zhang et al., 2021), which provides phoneme-level transcripts. We have added this clarification on p.4 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>• And generally, this is the model that produces most of the &quot;features&quot;, or regressors, whichever word we like, for the TRF deconvolution and EEG prediction, correct?</p>
</disp-quote>
<p>Yes, we extracted the 2048-dimensional hidden layer activity from the model to represent features for each sentence in our speech stimuli at the phoneme, syllable, word, phrase and sentence levels. But we did not perform any TRF deconvolution, we fit these features (downsampled to 150-dimension using PCA) to the EEG signals at 9 timepoints around the offset of each sentence using ridge regression. We have now added a multivariate TRF (mTRF) analysis following Reviewer 3’s suggestions, and the results showed similar patterns to the current results (see Figure S2). We have added the clarification in the “Ridge regression at different time latencies” section of the “Methods and Materials” on p.10 of the manuscript.</p>
<p>Resutls from the mTRF analyses were added on p.7 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>• A lot more detail is necessary then, about what form these regressors take, and some example plots of the regressors alongside the sentences.</p>
</disp-quote>
<p>The linguistic regressors are just 5 150-dimensional vectors, each corresponding to one linguistic level, as shown in Figure 1B.</p>
<disp-quote content-type="editor-comment">
<p>• Generally, it is necessary to know what these regressors look like compared to other similar language-related TRF and EEG/MEG prediction studies. Usually, in the case of e.g. Lalor lab papers or Simon lab papers, these regressors take the form of single-sample event markers, surrounded by zeros elsewhere. For example, a phoneme regressor might have a sample up at the onset of each phoneme, and a word onset regressor might have a sample up at the onset of each word, with zeros elsewhere in the regressor. A phoneme surprisal regressor might have a sample up at each phoneme onset, with the value of that sample corresponding to the rarity of that phoneme in common speech. Etc. Are these regressors like that? Or do they code for these 5 linguistic levels in some other way? Either way, much more description and plotting is necessary in order to compare the results here to others in the literature.</p>
</disp-quote>
<p>No, these regressors were not like that. They were 150-dimensional vectors (after PCA dimension reduction) extracted from the hidden layers of the HM-LSTM model. After training the model on the WenetSpeech corpus, we ran it on our speech stimuli and extracted representations from the five hidden layers to correspond to the five linguistic levels. As mentioned earlier, we did not perform TRF analyses; instead, we used ridge regression to predict EEG signals around the offset of each sentence, a method commonly employed in the literature (e.g., Caucheteux &amp; King, 2022; Goldstein et al., 2022; Schmitt et al., 2021; Schrimpf et al., 2021). For instance, Goldstein et al. (2022) used word embeddings from GPT-2 to predict ECoG activity surrounding the onset of each word during naturalistic listening. We have included these literatures on p.3 in the manuscript, and the method is illustrated in Figure 1B.</p>
<disp-quote content-type="editor-comment">
<p>• You say that the 5 regressors that are taken from the trained model's hidden layers do not have much correlation with each other. However, the highest correlations are between syllable and sentence (0.22), and syllable and word (0.17). It is necessary to give some reason and interpretation of these numbers. One would think the highest correlation might be between syllable and phoneme, but this one is almost zero. Why would the syllable and sentence regressors have such a relatively high correlation with each other, and what form do those regressors take such that this is the case?</p>
</disp-quote>
<p>All the regressors are represented as 2048-dimensional vectors derived from the hidden layers of the trained HM-LSTM model. We applied the trained model to all 284 sentences in our stimulus text, generating a set of 284 × 2048-dimensional vectors. Next, we performed Principal Component Analysis (PCA) on the 2048 dimensions and extracted the first 100 principal components (PCs), resulting in 284 × 100-dimensional vectors for each regressor. These 284 × 100 matrices were then flattened into 28,400-dimensional vectors. Subsequently, we computed the correlation matrix for the z-transformed 28,400-dimensional vectors of our five linguistic regressors. The code for this analysis, lstm_corr.py, can be found in our OSF repository. We have added a section “Correlation among linguistic features” in “Materials and Methods” on p.10 of the manuscript.</p>
<p>We consider the observed coefficients of 0.17 and 0.22 to be relatively low compared to prior model-brain alignment studies which report correlation coefficients above 0.5 for linguistic regressors (e.g., Gao et al., 2024; Sugimoto et al., 2024). In Chinese, a single syllable can also function as a word, potentially leading to higher correlations between regressors for syllables and words. However, we refrained from overinterpreting the results to suggest a higher correlation between syllable and sentence compared to syllable and word. A paired ttest of the syllable-word coefficients versus syllable-sentence coefficients across the 284 sentences revealed no significant difference (t(28399)=-3.96, p=1). We have incorporated this information into p.5 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>• If these regressors are something like the time series of zeros along with single sample event markers as described above, with the event marker samples indicating the onset of the relevant thing, then one would think e.g. the syllable regressor would be a subset of the phoneme regressor because the onset of every syllable is a phoneme. And the onset of every word is a syllable, etc.</p>
</disp-quote>
<p>All the regressors are aligned to 9 time points surrounding sentence offsets (-100 ms to 300 ms with a 50 ms interval). This is because all our regressors are taken from the HM-LSTM model, where the input is the phoneme representation of a sentence (e.g., “zh ə_4 y ie_3 j iəu_4 x iaŋ_4 sh uei_3 y ii_2 y aŋ_4”). For each unit in the sentence, the model generates five 2048dimensional vectors, each corresponding to the five linguistic levels of the entire sentence. We have added the clarification on p.11 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>For the time windows of analysis:</p>
<p>• I am very confused, because sometimes the times are relative to &quot;sentence onset&quot;, which would mean the beginning of sentences, and sometimes they are relative to &quot;sentence offset&quot;, which would mean the end of sentences. It seems to vary which is mentioned. Did you use sentence onsets, offsets, or both, and what is the motivation?</p>
<p>• If you used onsets, then the results at negative times would not seem to mean anything, because that would be during silence unless the stimulus sentences were all back to back with no gaps, which would also make that difficult to interpret.</p>
<p>• If you used offsets, then the results at positive times would not seem to mean anything, because that would be during silence after the sentence is done. Unless you want to interpret those as important brain activity after the stimuli are done, in which case a detailed discussion of this is warranted.</p>
</disp-quote>
<p>Thank you very much for pointing this out. All instances of “sentence onset” were typos and should be corrected to “sentence offset.” We chose offset because the regressors are derived from the hidden layer activity of our HM-LSTM model, which processes the entire sentence before generating outputs. We have now corrected all the typos. In continuous speech, there is no distinct silence period following sentence offsets. Additionally, lexical or phrasal processing typically occurs 200 ms after stimulus offsets (Bemis &amp; Pylkkanen, 2011; Goldstein et al., 2022; Li et al., 2024; Li &amp; Pylkkänen, 2021). Therefore, we included a 300 ms interval after sentence offsets in our analysis, as our regressors encompass linguistic levels up to the sentence level. We have added this motivation on p.11 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>• For the plots in the figures where the time windows and their regression outcomes are shown, it needs to be explicitly stated every time whether those time windows are relative to sentence onset, offset, or something else.</p>
</disp-quote>
<p>Completely agree and thank you very much for the suggestion. We have now added this information on Figure 4-6.</p>
<disp-quote content-type="editor-comment">
<p>• Whether the running correlations are relative to sentence onset or offset, the fact that you can have numbers outside of the time of the sentence (negative times for onset, or positive times for offset) is highly confusing. Why would the regressors have values outside of the sentence, meaning before or after the sentence/utterance? In order to get the running correlations, you presumably had the regressor convolved with the TRF/impulse response to get the predicted EEG first. In order to get running correlation values outside the sentence to correlate with the EEG, you would have to have regressor values at those time points, correct? How does this work?</p>
</disp-quote>
<p>As mentioned earlier, we did not perform TRF analyses or convolve the regressors. Instead, we conducted regression analyses at each of the 9 time points surrounding the sentence offsets, following standard methods commonly used in model-brain alignment studies (e.g., Gao et al., 2024; Goldstein et al., 2022). The time window of -100 to 300 ms was selected based on prior findings that lexical and phrasal processing typically occurs 200–300 ms after word offsets (Bemis &amp; Pylkkanen, 2011; Goldstein et al., 2022; Li et al., 2024; Li &amp; Pylkkänen, 2021). Additionally, we included the -100 to 200 ms time period in our analysis to examine phoneme and syllable level processing (cf. Gwilliams et al., 2022). We have added the clarification on p. of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>• In general, it seems arbitrary to choose sentence onset or offset, especially if the comparison is the correlation between predicted and actual EEG over the course of a sentence, with each regressor. What is going on with these correlations during the middle of the sentences, for example? In ridge regression TRF techniques for EEG/MEG, the relevant measure is often the overall correlation between the predicted and actual, calculated over a longer period of time, maybe the entire experiment. Here, you have calculated a running comparison between predicted and actual, and thus the time windows you choose to actually analyze can seem highly cherry-picked, because this means that most of the data is not actually analyzed.</p>
</disp-quote>
<p>The rationale for choosing sentence offsets instead of onsets is that we are aligning the HM-LSTM model’s activity with EEG responses, and the input to the model consists of phoneme representations of the entire sentence at one time. In other words, the model needs to process the whole sentence before generating representations at each linguistic level. Therefore, the corresponding EEG responses should also align with the sentence offsets, occurring after participants have seen the complete sentence. The ridge regression followed the common practice in model-brain alignment studies (e.g., Gao et al., 2024; Goldstein et al., 2022; Huth et al., 2016; Schmitt et al., 2021; Schrimpf et al., 2021), and the time window is not cherrypicked but based on prior literature reporting lexical and sublexical processing at these time period (e.g., Bemis &amp; Pylkkanen, 2011; Goldstein et al., 2022; Gwilliams et al., 2022; Li et al., 2024; Li &amp; Pylkkänen, 2021).</p>
<disp-quote content-type="editor-comment">
<p>• In figures 5 and 6, some of the time window portions that are highlighted as significant between the two lines have the lines intersecting. This looks like, even though you have found that the two lines are significantly different during that period of time, the difference between those lines is not of a constant sign, even during that short period. For instance, in figure 5, for the syllable feature, the period of 0 - 200 ms is significantly different between the two populations, correct? But between 0 and 50, normal-hearing are higher, between 50 and 150, hearing-impaired are higher, and between 150 and 200, normal-hearing are higher again, correct? But somehow they still end up significantly different overall between 0 and 200 ms. More explanation of occurrences like these is needed.</p>
</disp-quote>
<p>The intersecting lines in Figures 5 and represent the significant time windows for withingroup comparisons (i.e., significant model fit compared to 0). They do not depict betweengroup comparisons, as no significant contrasts were found between the groups. For example, in Figure 1, the significant time windows for the acoustic models are shown separately for the hearing-impaired and normal-hearing groups. No significant differences were observed, as indicated by the sensor topography. We have now clarified this point in the captions for Figures 5 and 6.</p>
<disp-quote content-type="editor-comment">
<p>Using ridge regression:</p>
<p>• What software package(s) and procedure(s) were specifically done to accomplish this? If this is ridge regression and not just ordinary least squares, then there was at least one non-zero regularization parameter in the process. What was it, how did it figure in the modeling and analysis, etc.?</p>
</disp-quote>
<p>The ridge regression was performed using customary python codes, making heavy use of the sklearn (v1.12.0) package. We used ridge regression instead of ordinary least squares regression because all our linguistic regressors are 150-dimensional dense vectors, and our acoustic regressors are 130-dimension vectors (see “Acoustic features of the speech stimuli” in “Materials and Methods”). We kept the default regularization parameter (i.e., 1). This ridge regression methods is commonly used in model-brain alignment studies, where the regressors are high-dimensional vectors taken from language models (e.g., Gao et al., 2024; Goldstein et al., 2022; Huth et al., 2016; Schmitt et al., 2021; Schrimpf et al., 2021). The code ridge_lstm.py can be found in our OSF repository, and we have added the more detailed description on p.11 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>• It sounds like the regressors are the hidden layer activations, which you reduced from 2,048 to 150 non-acoustic, or linguistic, regressors, per linguistic level, correct? So you have 150 regressors, for each of 5 linguistic levels. These regressors collectively contribute to the deconvolution and EEG prediction from the resulting TRFs, correct? This sounds like a lot of overfitting. How much correlation is there from one of these 150 regressors to the next? Elsewhere, it sounds like you end up with only one regressor for each of the 5 linguistic levels. So these aspects need to be clarified.</p>
<p>• For these regressors, you are comparing the &quot;regression outcomes&quot; for different conditions; &quot;regression outcomes&quot; are the R2 between predicted and actual EEG, which is the coefficient of determination, correct? If this is R2, how is it that you have some negative numbers in some of the plots? R2 should be only positive, between 0 and 1.</p>
</disp-quote>
<p>Yes we reduced 2048-dimensional vectors for each of the 5 linguistic levels to 150 using PCA, mainly for saving computational resources. We used ridge regression, following the standard practice in the field (e.g., Gao et al., 2024; Goldstein et al., 2022; Huth et al., 2016; Schmitt et al., 2021; Schrimpf et al., 2021).</p>
<p>Yes, the regression outcomes are the R<sup>2</sup> values representing the fit between the predicted and actual EEG data. However, we reported normalized R<sup>2</sup> values which are ztransformed in the plots. All our spatiotemporal cluster permutation analyses were conducted using the <italic>z</italic>-transformed R<sup>2</sup> values. We have added this clarification both in the figure captions and on p.11 of the manuscript. As a side note, R<sup>2</sup> values can be negative because they are not the square of a correlation coefficient. Rather, R<sup>2</sup> compares the fit of the chosen model to that of a horizontal straight line (the null hypothesis). If the chosen model fits the data worse than the horizontal line, then R<sup>2</sup> value becomes negative: <ext-link ext-link-type="uri" xlink:href="https://www.graphpad.com/support/faq/how-can-rsup2sup-be-negative">https://www.graphpad.com/support/faq/how-can-rsup2sup-be-negative</ext-link></p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>This study compares neural responses to speech in normal-hearing and hearing-impaired listeners, investigating how different levels of the linguistic hierarchy are impacted across the two cohorts, both in a single-talker and multi-talker listening scenario. It finds that, while normal-hearing listeners have a comparable cortical encoding of speech-in-quiet and attended speech from a multi-talker mixture, participants with hearing impairment instead show a reduced cortical encoding of speech when it is presented in a competing listening scenario. When looking across the different levels of the speech processing hierarchy in the multi-talker condition, normal-hearing participants show a greater cortical encoding of the attended compared to the unattended stream in all speech processing layers - from acoustics to sentencelevel information. Hearing-impaired listeners, on the other hand, only have increased cortical responses to the attended stream for the word and phrase levels, while all other levels do not differ between attended and unattended streams.</p>
<p>The methods for modelling the hierarchy of speech features (HM-LSTM) and the relationship between brain responses and specific speech features (ridge-regression) are appropriate for the research question, with some caveats on the experimental procedure. This work offers an interesting insight into the neural encoding of multi-talker speech in listeners with hearing impairment, and it represents a useful contribution towards understanding speech perception in cocktail-party scenarios across different hearing abilities. While the conclusions are overall supported by the data, there are limitations and certain aspects that require further clarification.</p>
<p>(1) In the multi-talker section of the experiment, participants were instructed to selectively attend to the male or the female talker, and to rate the intelligibility, but they did not have to perform any behavioural task (e.g., comprehension questions, word detection or repetition), which could have demonstrated at least an attempt to comply with the task instructions. As such, it is difficult to determine whether the lack of increased cortical encoding of Attended vs. Unattended speech across many speech features in hearing-impaired listeners is due to a different attentional strategy, which might be more oriented at &quot;getting the gist&quot; of the story (as the increased tracking of only word and phrase levels might suggest), or instead it is due to hearing-impaired listeners completely disengaging from the task and tuning back in for selected key-words or word combinations. Especially the lack of Attended vs. Unattended cortical benefit at the level of acoustics is puzzling and might indicate difficulties in performing the task. I think this caveat is important and should be highlighted in the Discussion section. RE: Thank you very much for the suggestion. We admit that the hearing-impaired listeners might adopt different attentional strategies or potentially disengage from the task due to comprehension difficulties. However, we would like to emphasize that our hearing-impaired participants have extended high-frequency (EHF) hearing loss, with impairment only at frequencies above 8 kHz. Their condition is likely not severe enough to cause them to adopt a markedly different attentional strategy for this task. Moreover, it is possible that our normalhearing listeners may also adopt varying attentional strategies, yet the comparison still revealed notable differences.We have added the caveat in the Discussion section on p.8 of the manuscript.</p>
<p>(2) In the EEG recording and preprocessing section, you state that the EEG was filtered between 0.1Hz and 45Hz. Why did you choose this very broadband frequency range? In the literature, speech responses are robustly identified between 0.5Hz/1Hz and 8Hz. Would these results emerge using a narrower and lower frequency band? Considering the goal of your study, it might also be interesting to run your analysis pipeline on conventional frequency bands, such as Delta and Theta, since you are looking into the processing of information at different temporal scales.</p>
</disp-quote>
<p>Indeed, we have decomposed the epoched EEG time series for each section into six classic frequency bands components (delta 1–3 Hz, theta 4–7 Hz, alpha 8–12 Hz, beta 12–20 Hz, gamma 30–45 Hz) by convolving the data with complex Morlet wavelets as implemented in MNE-Python (version 0.24.0). The number of cycles in the Morlet wavelets was set to frequency/4 for each frequency bin. The power values for each time point and frequency bin were obtained by taking the square root of the resulting time-frequency coefficients. These power values were normalized to reflect relative changes (expressed in dB) with respect to the 500 ms pre-stimulus baseline. This yielded a power value for each time point and frequency bin for each section. We specifically examined the delta and theta bands, and computed the correlation between the regression outcome (R<sup>2</sup> in the shape of number of subject * sensor * time were flattened for computing correlation) for the five linguistic predictors from these bands and those obtained using data from all frequency bands. The results showed high correlation coefficients (see the correlation matrix in Supplementary Figures S2 for the attended and unattended speech). Therefore, we opted to use the epoched EEG data from all frequency bands for our analyses. We have added this clarification in the Results section on p.5 and the “EEG recording and preprocessing” section in “Materials and Methods” on p.11 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(3) A paragraph with more information on the HM-LSTM would be useful to understand the model used without relying on the Chung et al. (2017) paper. In particular, I think the updating mechanism of the model should be clarified. It would also be interesting to modify the updating factor of the model, along the lines of Schmitt et al. (2021), to assess whether a HM-LSTM with faster or slower updates can better describe the neural activity of hearing-impaired listeners. That is, perhaps the difference between hearing-impaired and normal-hearing participants lies in the temporal dynamics, and not necessarily in a completely different attentional strategy (or disengagement from the stimuli, as I mentioned above).</p>
</disp-quote>
<p>Thank you for the suggestion. We have added more details on our HM-LSTM model on p.10 “Hierarchical multiscale LSTM model” in “Materials and Methods”: Our HM-LSTM model consists of 4 layers, at each layer, the model implements a COPY or UPDATE operation at each time step <italic>t</italic>. The COPY operation maintains the current cell state of without any changes until it receives a summarized input from the lower layer. The UPDATE operation occurs when a linguistic boundary is detected in the layer below, but no boundary was detected at the previous time step <italic>t-1</italic>. In this case, the cell updates its summary representation, similar to standard RNNs. We agree that exploring modifications to the model’s updating factor would be an interesting direction. However, since we have already observed contrasts between normal-hearing and hearing-impaired listeners using the current model’s update parameters, we believe discussing additional hypotheses would overextend the scope of this paper.</p>
<disp-quote content-type="editor-comment">
<p>(4) When explaining how you extracted phoneme information, you mention that &quot;the inputs to the model were the vector representations of the phonemes&quot;. It is not clear to me whether you extracted specific phonetic features (e.g., &quot;p&quot; sound vs. &quot;b&quot; sound), or simply the phoneme onsets. Could you clarify this point in the text, please?</p>
</disp-quote>
<p>The model inputs were individual phonemes from two sentences, each transformed into a 1024-dimensional vector using a simple lookup table. This lookup table stores embeddings for a fixed dictionary of all unique phonemes in Chinese. This approach is a foundational technique in many advanced NLP models, enabling the representation of discrete input symbols in a continuous vector space. We have added this clarification on p.10 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Summary:</p>
<p>The authors aimed to investigate how the brain processes different linguistic units (from phonemes to sentences) in challenging listening conditions, such as multi-talker environments, and how this processing differs between individuals with normal hearing and those with hearing impairments. Using a hierarchical language model and EEG data, they sought to understand the neural underpinnings of speech comprehension at various temporal scales and identify specific challenges that hearing-impaired listeners face in noisy settings.</p>
<p>Strengths:</p>
<p>Overall, the combination of computational modeling, detailed EEG analysis, and comprehensive experimental design thoroughly investigates the neural mechanisms underlying speech comprehension in complex auditory environments.</p>
<p>The use of a hierarchical language model (HM-LSTM) offers a data-driven approach to dissect and analyze linguistic information at multiple temporal scales (phoneme, syllable, word, phrase, and sentence). This model allows for a comprehensive neural encoding examination of how different levels of linguistic processing are represented in the brain.</p>
<p>The study includes both single-talker and multi-talker conditions, as well as participants with normal hearing and those with hearing impairments. This design provides a robust framework for comparing neural processing across different listening scenarios and groups.</p>
<p>Weaknesses:</p>
<p>The analyses heavily rely on one specific computational model, which limits the robustness of the findings. The use of a single DNN-based hierarchical model to represent linguistic information, while innovative, may not capture the full range of neural coding present in different populations. A low-accuracy regression model-fit does not necessarily indicate the absence of neural coding for a specific type of information. The DNN model represents information in a manner constrained by its architecture and training objectives, which might fit one population better than another without proving the non-existence of such information in the other group. To address this limitation, the authors should consider evaluating alternative models and methods. For example, directly using spectrograms, discrete phoneme/syllable/word coding as features, and performing feature-based temporal response function (TRF) analysis could serve as valuable baseline models. This approach would provide a more comprehensive evaluation of the neural encoding of linguistic information.</p>
</disp-quote>
<p>Our acoustic features are indeed direct the broadband envelopes and the log-mel spectrograms of the speech streams. The amplitude envelope of the speech signal was extracted using the Hilbert transform. The 129-dimension spectrogram and 1-dimension envelope were concatenated to form a 130-dimension acoustic feature at every 10 ms of the speech stimuli. Given the duration of our EEG recordings, which span over 10 minutes, conducting multivariate TRF (mTRF) analysis with such high-dimensional predictors was not feasible. Instead, we used ridge regression to predict EEG responses across 9 temporal latencies, ranging from -100 ms to +300 ms, with additional 50 ms latencies surrounding sentence offsets. To evaluate the model's performance, we extracted the <italic>R<sup>2</sup></italic> values at each latency, providing a temporal profile of regression performance over the analyzed time period. This approach is conceptually similar to TRF analysis.</p>
<p>We agree that including baseline models for the linguistic features is important, and we have now added results from mTRF analysis using phoneme, syllable, word, phrase, and sentence rates as discrete predictors (i.e., marking a value of 1 at each unit boundary offset). Our EEG data spans the entire 10-minute duration for each condition, sampled at 10-ms intervals. The TRF results for our main comparison—attended versus unattended conditions— showed similar patterns to those observed using features from our HM-LSTM model. At the phoneme and syllable levels, normal-hearing listeners showed marginally significantly higher TRF weights for attended speech compared to unattended speech at approximately -80 to 150 ms after phoneme offsets (t=2.75, Cohen’s d=0.87, p=0.057), and 120 to 210 ms after syllable offsets (t=3.96, Cohen’s d=0.73d = 0.73, p=0.083). At the word and phrase levels, normalhearing listeners exhibited significantly higher TRF weights for attended speech compared to unattended speech at 190 to 290 ms after word offsets (t=4, Cohen’s d=1.13, p=0.049), and around 120 to 290 ms after phrase offsets (t=5.27, Cohen’s d=1.09, p=0.045). For hearing-impaired listeners, marginally significant effects were observed at 190 to 290 ms after word offsets (t=1.54, Cohen’s d=0.6, p=0.059), and 180 to 290 ms after phrase offsets (t=3.63, Cohen’s d=0.89, p=0.09). These results have been added on p.7 of the manuscript, and the corresponding figure is included as Supplementary F2.</p>
<disp-quote content-type="editor-comment">
<p>It is not entirely clear if the DNN model used in this study effectively serves the authors' goal of capturing different linguistic information at various layers. Specifically, the results presented in Figure 3C are somewhat confusing. While the phonemes are labeled, the syllables, words, phrases, and sentences are not, making it difficult to interpret how the model distinguishes between these levels of linguistic information. The claim that &quot;Hidden-layer activity for samevowel sentences exhibited much more similar distributions at the phoneme and syllable levels compared to those at the word, phrase and sentence levels&quot; is not convincingly supported by the provided visualizations. To strengthen their argument, the authors should use more quantified metrics to demonstrate that the model indeed captures phrase, word, syllable, and phoneme information at different layers. This is a crucial prerequisite for the subsequent analyses and claims about the hierarchical processing of linguistic information in the brain.</p>
<p>Quantitative measures such as mutual information, clustering metrics, or decoding accuracy for each linguistic level could provide clearer evidence of the model's effectiveness in this regard.</p>
</disp-quote>
<p>In Figure 3C, we used color-coding to represent the activity of five hidden layers after dimensionality reduction. Each dot on the plot corresponds to one test sentence. Only phonemes are labeled because each syllable in our test sentences contains the same vowels (see Table S1). The results demonstrate that the phoneme layer effectively distinguishes different phonemes, while the higher linguistic layers do not. We believe these findings provide evidence that different layers capture distinct linguistic information. Additionally, we computed the correlation coefficients between each pair of linguistic predictors, as shown in Figure 3B. We think this analysis serves a similar purpose to computing the mutual information between pairs of hidden-layer activities for our constructed sentences. Furthermore, the mTRF results based on rate models of the linguistic features we presented earlier align closely with the regression results using the hidden-layer activity from our HM-LSTM model. This further supports the conclusion that our model successfully captures relevant information across these linguistic levels. We have added the clarification on p.5 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>The formulation of the regression analysis is somewhat unclear. The choice of sentence offsets as the anchor point for the temporal analysis, and the focus on the [-100ms, +300ms] interval, needs further justification. Since EEG measures underlying neural activity in near real-time, it is expected that lower-level acoustic information, which is relatively transient, such as phonemes and syllables, would be distributed throughout the time course of the entire sentence. It is not evident if this limited time window effectively captures the neural responses to the entire sentence, especially for lower-level linguistic features. A more comprehensive analysis covering the entire time course of the sentence, or at least a longer temporal window, would provide a clearer understanding of how different linguistic units are processed over time. Additionally, explaining the rationale behind choosing this specific time window and how it aligns with the temporal dynamics of speech processing would enhance the clarity and validity of the regression analysis.</p>
</disp-quote>
<p>Thank you for pointing this out. We chose this time window as lexical or phrasal processing typically occurs 200 ms after stimulus offsets (Bemis &amp; Pylkkanen, 2011; Goldstein et al., 2022; Li et al., 2024; Li &amp; Pylkkänen, 2021). Additionally, we included the -100 to 200 ms time period in our analysis to examine phoneme and syllable level processing (e.g., Gwilliams et al., 2022). Using the entire sentence duration was not feasible, as the sentences in the stimuli vary in length, making statistical analysis challenging. Additionally, since the stimuli consist of continuous speech, extending the time window would risk including linguistic units from subsequent sentences. This would introduce ambiguity as to whether the EEG responses correspond to the current or the following sentence. We have added this clarification on p.12 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>As I mentioned, I think the OSF repo needs to be changed to give anyone access. I would recommend pursuing the lines of thought I mentioned in the public review to make this study complete and to allow it to fit into the already existing literature to facilitate comparisons.</p>
</disp-quote>
<p>Yes the OSF folder is now public. We have made revisions following all reviewers’ suggestions.</p>
<disp-quote content-type="editor-comment">
<p>There are some typos in figure labels, e.g. 2B.</p>
</disp-quote>
<p>Thank you for pointing it out! We have now revised the typo in Figure 2B.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>(1) I was able to access all of the audio files and code for the study, but no EEG data was shared in the OSF repository. Unless there is some ethical and/or legal constraint, my understanding of eLife's policy is that the neural data should be made publicly available as well.</p>
</disp-quote>
<p>The preprocessed EEG data in .npy format in the OSF repository.</p>
<disp-quote content-type="editor-comment">
<p>(2) The line-plots in Figures 4B,5B, and 6B have very similar colours. They would be easier to interpret if you changed the line appearance as well as the colours. E.g., dotted line for hearingimpaired listeners, thick line for normal-hearing.</p>
</disp-quote>
<p>Thank you for the suggestion! We have now used thicker lines for normal-impaired listeners in all our line plots.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<p>(1) The authors may consider presenting raw event-related potentials (ERPs) or spatiotemporal response profiles before delving into the more complex regression encoding analysis. This would provide a clearer foundational understanding of the neural activity patterns. For example, it is not clear if the main claims, such as the neural activity in the normal-hearing group encoding phonetic information in attended speech better than in unattended speech, are directly observable. Showing ERP differences or spatiotemporal response pattern differences could support these claims more straightforwardly. Additionally, training pattern classifiers to test if different levels of information can be decoded from EEG activity in specific groups could provide further validation of the findings.</p>
</disp-quote>
<p>We have now included results from more traditional mTRF analyses using phoneme, syllable, word, phrase, and sentence rates as baseline models (see p.7 of the manuscript and Figure S3). The results show similar patterns to those observed in our current analyses. While we agree that classification analyses would be very interesting, our regression analyses have already demonstrated distinct EEG patterns for each linguistic level. Consequently, classification analyses would likely yield similar results unless a different method for representing linguistic information at these levels is employed. To the best of our knowledge, no other computational model currently exists that can simultaneously represent these linguistic levels.</p>
<disp-quote content-type="editor-comment">
<p>(2) Is there any behavioral metric suggesting that these hearing-impaired participants do have deficits in comprehending long sentences? The self-rated intelligibility is useful, but cannot fully distinguish between perceiving lower-level phonetic information vs longer sentence comprehension.</p>
</disp-quote>
<p>In the current study, we included only self-rated intelligibility tests. We acknowledge that this approach might not fully distinguish between the perception of lower-level phonetic information and higher-level sentence comprehension. However, it remains unclear what type of behavioral test would effectively address this distinction. Furthermore, our primary aim was to use the behavioral results to demonstrate that our hearing-impaired listeners experienced speech comprehension difficulties in multi-talker environments, while relying on the EEG data to investigate comprehension challenges at various linguistic levels.</p>
<disp-quote content-type="editor-comment">
<p>Minor:</p>
<p>(1) Page 2, second line in Introduction, &quot;Phonemes occur over ...&quot; should be lowercase.</p>
</disp-quote>
<p>According to APA format, the first word after the colon is capitalized if it begins a complete sentence (<ext-link ext-link-type="uri" xlink:href="https://blog.apastyle.org/apastyle/2011/06/capitalization-after-colons.html">https://blog.apastyle.org/apastyle/2011/06/capitalization-after-colons.html</ext-link>). Here</p>
<p>the sentence is a complete sentence so we used uppercase for “phonemes”.</p>
<disp-quote content-type="editor-comment">
<p>(2) Page 8, second paragraph &quot;...-100ms to 100ms relative to sentence onsets&quot;, should it be onsets or offsets?</p>
</disp-quote>
<p>This is typo and it should be offsets. We have now revised it.</p>
<p>References</p>
<p>Bemis, D. K., &amp; Pylkkanen, L. (2011). Simple composition: An MEG investigation into the comprehension of minimal linguistic phrases. Journal of Neuroscience, 31(8), 2801– 2814.</p>
<p>Gao, C., Li, J., Chen, J., &amp; Huang, S. (2024). Measuring meaning composition in the human brain with composition scores from large language models. In L.-W. Ku, A. Martins, &amp; V. Srikumar (Eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 11295–11308). Association for Computational Linguistics.</p>
<p>Goldstein, A., Zada, Z., Buchnik, E., Schain, M., Price, A., Aubrey, B., Nastase, S. A., Feder, A., Emanuel, D., Cohen, A., Jansen, A., Gazula, H., Choe, G., Rao, A., Kim, C., Casto, C., Fanda, L., Doyle, W., Friedman, D., … Hasson, U. (2022). Shared computational principles for language processing in humans and deep language models. Nature Neuroscience, 25(3), Article 3.</p>
<p>Gwilliams, L., King, J.-R., Marantz, A., &amp; Poeppel, D. (2022). Neural dynamics of phoneme sequences reveal position-invariant code for content and order. Nature Communications, 13(1), Article 1.</p>
<p>Huth, A. G., de Heer, W. A., Griffiths, T. L., Theunissen, F. E., &amp; Gallant, J. L. (2016). Natural speech reveals the semantic maps that tile human cerebral cortex. Nature, 532(7600), 453–458.</p>
<p>Li, J., Lai, M., &amp; Pylkkänen, L. (2024). Semantic composition in experimental and naturalistic paradigms. Imaging Neuroscience, 2, 1–17.</p>
<p>Li, J., &amp; Pylkkänen, L. (2021). Disentangling semantic composition and semantic association in the left temporal lobe. Journal of Neuroscience, 41(30), 6526–6538.</p>
<p>Maris, E., &amp; Oostenveld, R. (2007). Nonparametric statistical testing of EEG- and MEG-data. Journal of Neuroscience Methods, 164(1), 177–190.</p>
<p>Schmitt, L.-M., Erb, J., Tune, S., Rysop, A. U., Hartwigsen, G., &amp; Obleser, J. (2021). Predicting speech from a cortical hierarchy of event-based time scales. Science Advances, 7(49), eabi6070.</p>
<p>Schrimpf, M., Blank, I. A., Tuckute, G., Kauf, C., Hosseini, E. A., Kanwisher, N., Tenenbaum, J. B., &amp; Fedorenko, E. (2021). The neural architecture of language: Integrative modeling converges on predictive processing. Proceedings of the National Academy of Sciences, 118(45), e2105646118.</p>
<p>Sugimoto, Y., Yoshida, R., Jeong, H., Koizumi, M., Brennan, J. R., &amp; Oseki, Y. (2024). Localizing Syntactic Composition with Left-Corner Recurrent Neural Network Grammars. Neurobiology of Language, 5(1), 201–224.</p>
</body>
</sub-article>
</article>