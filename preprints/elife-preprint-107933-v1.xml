<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">107933</article-id>
<article-id pub-id-type="doi">10.7554/eLife.107933</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107933.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Modality-Agnostic Decoding of Vision and Language from fMRI</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5609-6628</contrib-id>
<name>
<surname>Nikolaus</surname>
<given-names>Mitja</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
<email>mitja.nikolaus@cnrs.fr</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4521-1640</contrib-id>
<name>
<surname>Mozafari</surname>
<given-names>Milad</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Berry</surname>
<given-names>Isabelle</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Asher</surname>
<given-names>Nicholas</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7078-1055</contrib-id>
<name>
<surname>Reddy</surname>
<given-names>Leila</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>VanRullen</surname>
<given-names>Rullen</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01ahyrz84</institution-id><institution>Université de Toulouse</institution></institution-wrap>, <city>Toulouse</city>, <country country="FR">France</country></aff>
<aff id="a2"><label>2</label><institution>Torus AI, Toulouse</institution>, <city>Toulouse</city>, <country country="FR">France</country>;</aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01ahyrz84</institution-id><institution>Université de Toulouse</institution></institution-wrap>, <city>Toulouse</city>, <country country="FR">France</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>†</label><p>These authors contributed equally to this work</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-09-03">
<day>03</day>
<month>09</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP107933</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-06-14">
<day>14</day>
<month>06</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-06-08">
<day>08</day>
<month>06</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.06.08.658221"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Nikolaus et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Nikolaus et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-107933-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Humans perform tasks involving the manipulation of inputs regardless of how these signals are perceived by the brain, thanks to representations that are agnostic to the stimulus modality. Investigating such modality-agnostic representations requires experimental datasets with multiple modalities of presentation. In this paper, we introduce and analyze SemReps-8K, a new large-scale fMRI dataset of 6 subjects watching both images and short text descriptions of such images, as well as conditions during which the subjects were imagining visual scenes. The multimodal nature of this dataset enables the development of modality-agnostic decoders, trained to predict which stimulus a subject is seeing, irrespective of the modality in which the stimulus is presented. Further, we performed a searchlight analysis revealing that large areas of the brain contain modality-agnostic representations. Such areas are also particularly suitable for decoding visual scenes from the mental imagery condition. The dataset will be made publicly available.</p>
</abstract>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00rbzpz17</institution-id>
<institution>Agence Nationale de la Recherche</institution>
</institution-wrap>
</funding-source>
<award-id>ANR-18-CE37-0007-01</award-id>
</award-group>
<award-group id="funding-1a">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00rbzpz17</institution-id>
<institution>Agence Nationale de la Recherche</institution>
</institution-wrap>
</funding-source>
<award-id>ANR-19-PI3A-0004</award-id>
</award-group>
<award-group id="funding-2">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/0472cxd90</institution-id>
<institution>European Research Council</institution>
</institution-wrap>
</funding-source>
<award-id award-id-type="doi">10.3030/101096017</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Several regions in the human brain have developed a high degree of specialization for particular lower-level perceptive as well as higher-level cognitive functions (<xref ref-type="bibr" rid="c56">Kanwisher, 2010</xref>). For many higher-level functions, it is crucial to be able to manipulate inputs regardless of the modality in which a stimulus was perceived by the brain. Such manipulations can be performed thanks to representations that are abstracted away from particularities of specific modalities, and are therefore <italic>modality-agnostic</italic>. A range of theories have been developed to explain how and where in the human brain such abstract representations are created (<xref ref-type="bibr" rid="c22">Damasio, 1989</xref>; <xref ref-type="bibr" rid="c13">Binder et al., 2009</xref>; <xref ref-type="bibr" rid="c75">Martin, 2016</xref>; <xref ref-type="bibr" rid="c8">Barsalou, 2016</xref>; <xref ref-type="bibr" rid="c97">Ralph et al., 2017</xref>).</p>
<p>In order to study how modality-agnostic information is represented in the brain and to exploit it for modality-agnostic decoding, large multimodal neuroimaging datasets with well-controlled stimuli across modalities are required. While large-scale datasets exist for vision (<xref ref-type="bibr" rid="c50">Huth et al., 2012</xref>; <xref ref-type="bibr" rid="c20">Chang et al., 2019</xref>; <xref ref-type="bibr" rid="c3">Allen et al., 2022</xref>), language (<xref ref-type="bibr" rid="c17">Brennan and Hale, 2019</xref>; <xref ref-type="bibr" rid="c83">Nastase et al., 2021</xref>; <xref ref-type="bibr" rid="c102">Schoffelen et al., 2019</xref>; <xref ref-type="bibr" rid="c116">Tang et al., 2023a</xref>), and video (naturalistic movies) (<xref ref-type="bibr" rid="c2">Aliko et al., 2020</xref>; <xref ref-type="bibr" rid="c86">Visconti di Oleggio Castello et al., 2020</xref>; <xref ref-type="bibr" rid="c15">Boyle et al., 2020</xref>), none of these contain a controlled set of equivalent stimuli that are presented separately in both modalities. For instance, the different modalities in movies (vision and language) are complementary but do not always carry the same semantics. Further, they are not presented separately but simultaneously, impeding a study of the respective activity pattern caused by each modality in isolation.</p>
<p>Here, we present SemReps-8K, a new large-scale multimodal fMRI dataset of 6 subjects each viewing more than 8,000 stimuli which are presented separately in one of two modalities, as images of visual scenes or as descriptive captions of such images. In addition, the dataset also contains 3 imagery conditions for each subject, where they had to imagine an visual scene based on a caption description they had received before the start of the fMRI experiment. We exploit this new data to develop decoders that are specifically trained to leverage modality-agnostic patterns in the brain. Such modality-agnostic decoders are trained on brain imaging data from multiple modalities, which we demonstrate here for the case of vision and language. In contrast to <italic>modality-specific</italic> decoders that can be applied only in the modality that they were trained on, <italic>modality-agnostic</italic> decoders can be applied to decode stimuli from multiple modalities, even without knowing a priori the modality the stimulus was presented in.</p>
<p>We find that modality-agnostic decoders trained on this dataset perform on par with their respective modality-specific counterparts for decoding images, despite the additional challenge of uncertainty about the stimulus modality. For decoding captions, the modality-agnostic decoders even outperform the respective modality-specific decoders (because the former, but not the latter, can leverage the additional training data from the other image modality).</p>
<p>Additionally, we use this novel kind of decoders for a searchlight analysis to localize regions with modality-agnostic representations in the brain. Previous studies that aimed to localize modality-agnostic patterns were based on limited and rather simple stimulus sets and did not always agree on the exact location and extent of such regions (e.g. <xref ref-type="bibr" rid="c121">Vandenberghe et al., 1996</xref>; <xref ref-type="bibr" rid="c105">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c25">Devereux et al., 2013</xref>; <xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c55">Jung et al., 2018</xref>). We design a searchlight analysis based on a combination of modality-agnostic decoders and cross-decoding. The results reveal that modality-agnostic patterns can be found in a widespread left-lateralized network across the brain, encompassing virtually all regions that have been proposed previously.</p>
<p>Finally, we find that modality-agnostic decoders trained only on data with perceptual input also generalize to conditions during which the subjects were performing mental imagery. There is a large overlap in the areas that we identified as modality-agnostic and those that are suitable for decoding mental imagery.</p>
</sec>
<sec id="s2">
<title>Related Work</title>
<sec id="s2a">
<title>Modality-agnostic representations</title>
<p>Decades of neuro-anatomical research (e.g. based on clinical lesions) and electrophysiology in non-human and human primates, as well as modern experiments leveraging recent brain imaging techniques have provided evidence that the activation patterns in certain brain areas are modality-specific; for example, the occipital cortex responds predominantly to visual stimulation (<xref ref-type="bibr" rid="c36">Felleman and Van Essen, 1991</xref>; <xref ref-type="bibr" rid="c103">Sereno et al., 1995</xref>; <xref ref-type="bibr" rid="c45">Grill-Spector and Malach, 2004</xref>), and a commonly left-lateralized network responds to language processing tasks (<xref ref-type="bibr" rid="c129">Zola-Morgan, 1995</xref>; <xref ref-type="bibr" rid="c35">Fedorenko et al., 2010</xref>, <xref ref-type="bibr" rid="c34">2011</xref>; <xref ref-type="bibr" rid="c40">Friederici, 2017</xref>; <xref ref-type="bibr" rid="c16">Brennan, 2022</xref>).</p>
<p>More recent research has started to focus on higher-level regions that respond with <italic>modality-agnostic</italic> patterns, i.e., patterns that are abstracted away from any modality-specific information. A modality-agnostic region responds with similar patterns to input stimuli of the same meaning, even if they are presented in different modalities (e.g. the word “cat” and picture of a cat). Such regions have also been described as abstract/conceptual (<xref ref-type="bibr" rid="c12">Binder, 2016</xref>), modality-invariant (<xref ref-type="bibr" rid="c73">Man et al., 2012</xref>), modality-independent (<xref ref-type="bibr" rid="c29">Dirani and Pylkkänen, 2024</xref>), supramodal (<xref ref-type="bibr" rid="c101">Sanchez et al., 2020</xref>), or amodal (<xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>) (although see distinction made in <xref ref-type="bibr" rid="c8">Barsalou, 2016</xref>).</p>
<p>Several theories and frameworks on how the brain forms modality-agnostic representations from modality-specific inputs have been proposed. The convergence zones view proposes that information coming from modality-specific sensory cortices is integrated in multiple convergence zones that are distributed across the cortex, predominantly in temporal and parietal lobes (<xref ref-type="bibr" rid="c22">Damasio, 1989</xref>; <xref ref-type="bibr" rid="c120">Tranel et al., 1997</xref>; <xref ref-type="bibr" rid="c78">Meyer and Damasio, 2009</xref>). These convergence zones are organized hierarchically, learned associations are used to create abstractions from lower-level to higher-level feature representations (<xref ref-type="bibr" rid="c107">Simmons and Barsalou, 2003</xref>; <xref ref-type="bibr" rid="c78">Meyer and Damasio, 2009</xref>). A perceived stimulus first causes activity in the related low-level modality-specific region (e.g. visual cortex), subsequently higher-level convergence zones serve as relays that cause associated activity in other regions of the brain (e.g. the language network) (<xref ref-type="bibr" rid="c78">Meyer and Damasio, 2009</xref>; <xref ref-type="bibr" rid="c58">Kiefer and Pulvermüller, 2012</xref>). According to <xref ref-type="bibr" rid="c12">Binder, 2016</xref>, the most high-level convergence zones can become so abstract that they are representing amodal symbols.</p>
<p>The GRAPES framework (Grounding Representations in Action, Perception, and Emotion Systems) also suggests that representations are distributed across temporal and parietal areas of the cortex. More specifically, they are hypothesized to be situated in areas connected to the perception and manipulation of the environment, as well as in the language system (<xref ref-type="bibr" rid="c74">Martin, 2009</xref>, <xref ref-type="bibr" rid="c75">2016</xref>). According to this theory, conceptual knowledge is organized in domains: For example, semantic information related to object form and object motion is represented within specific visual processing systems, regardless of the stimulus modality, and both for perception as well as imagination.</p>
<p>The hub-and-spoke theory states that cross-modal interactions are mediated by a single modality-agnostic hub, located in the anterior temporal lobes (<xref ref-type="bibr" rid="c100">Rogers et al., 2004</xref>; <xref ref-type="bibr" rid="c64">Lambon Ralph et al., 2006</xref>; <xref ref-type="bibr" rid="c89">Patterson and Lambon Ralph, 2016</xref>; <xref ref-type="bibr" rid="c97">Ralph et al., 2017</xref>). The hub contains a “continuous distributed representation space that expresses conceptual similarities among items even though its dimensions are not independently interpretable” (<xref ref-type="bibr" rid="c41">Frisby et al., 2023</xref>, p. 262). The spokes form the links between the hubs and the modality-specific association cortices. Most importantly, semantic representations are not solely based in the hub, for a given concept all spokes that are linked to modalities in which the concept can be experienced do contribute to the semantic representation. This explains why selective damage to spokes can cause category-specific deficits (<xref ref-type="bibr" rid="c93">Pobric et al., 2010</xref>).</p>
<p>This is conceptually similar to some aspects of the Global Workspace Theory, which assumes both a multimodal convergence of inputs towards a specific (network of) region(s), and the possibility of flexibly recruiting unimodal regions into this Global Workspace (<xref ref-type="bibr" rid="c6">Baars, 1993</xref>, <xref ref-type="bibr" rid="c7">2005</xref>).</p>
<p>While these and other theories partly disagree on <italic>how</italic> modality-agnostic information is represented in the brain, they agree that such information is distributed across the cortex, and possibly overlapping with the semantic network (<xref ref-type="bibr" rid="c13">Binder et al., 2009</xref>; <xref ref-type="bibr" rid="c50">Huth et al., 2012</xref>; <xref ref-type="bibr" rid="c4">Andrews et al., 2014</xref>; <xref ref-type="bibr" rid="c130">Zwaan, 2016</xref>).</p>
</sec>
<sec id="s2b">
<title>Decoding of vision and language from fMRI</title>
<p>Early approaches of brain decoding focused on identifying and reconstructing limited sets of simple visual stimuli (<xref ref-type="bibr" rid="c48">Haxby et al., 2001</xref>; <xref ref-type="bibr" rid="c21">Cox and Savoy, 2003</xref>; <xref ref-type="bibr" rid="c57">Kay et al., 2008</xref>; <xref ref-type="bibr" rid="c82">Naselaris et al., 2009</xref>; <xref ref-type="bibr" rid="c84">Nishimoto et al., 2011</xref>). Soon after, attempts to decode linguistic stimuli could identify single words and short paragraphs with the help of models trained to predict features extracted from word embeddings (<xref ref-type="bibr" rid="c92">Pereira et al., 2018</xref>).</p>
<p>More recently, large-scale open source fMRI datasets for both vision and language have become available (<xref ref-type="bibr" rid="c20">Chang et al., 2019</xref>; <xref ref-type="bibr" rid="c3">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="c102">Schoffelen et al., 2019</xref>; <xref ref-type="bibr" rid="c117">Tang et al., 2023b</xref>) and allowed for the training of decoding models for a larger range and more complex naturalistic stimuli with the help of features extracted from deep learning models. For example, modality-specific decoders for vision can be trained by mapping the brain activity of subjects viewing naturalistic images to feature representation spaces of computational models of the same modality (i.e. vision models) (<xref ref-type="bibr" rid="c104">Shen et al., 2019</xref>; <xref ref-type="bibr" rid="c10">Beliy et al., 2019</xref>; <xref ref-type="bibr" rid="c68">Lin et al., 2022</xref>; <xref ref-type="bibr" rid="c115">Takagi and Nishimoto, 2023</xref>; <xref ref-type="bibr" rid="c88">Ozcelik and VanRullen, 2023</xref>). Moreover, a range of studies provided evidence that certain representations can transfer between vision and language by evaluating decoders in a modality that they were not trained on (<xref ref-type="bibr" rid="c105">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c73">Man et al., 2012</xref>; <xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c106">Simanova et al., 2014</xref>; <xref ref-type="bibr" rid="c55">Jung et al., 2018</xref>). The performance in such <italic>cross-modal decoding</italic> evaluations always lags behind when compared to within-modality decoding. One explanation is that modality-specific decoders are not explicitly encouraged to pick up on modality-agnostic features during training, and modality-specific features do not transfer to other modalities.</p>
<p>To address this limitation, we here propose to directly train <italic>modality-agnostic decoders</italic>, i.e. models that are exposed to multiple stimulus modalities during training in order to make it more likely that they are leveraging representations that are modality-agnostic. Training this kind of decoder is enabled by the multimodal nature of our fMRI dataset: The stimuli are taken from COCO, a multimodal dataset of images with associated descriptive captions (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>). During the experiment the subjects are exposed to stimuli in both modalities (images and captions) in separate trials. Crucially, we can map the brain activity of each trial (e.g. the subject viewing an image) to modality-agnostic features extracted from both modalities (the image and the corresponding caption) when training the decoder models. After training, a single modality-agnostic decoder can be used to decode stimuli from multiple modalities, leveraging representations that are common to all modalities.</p>
</sec>
<sec id="s2c">
<title>Decoding of mental imagery</title>
<p>Apart from decoding perceived stimuli, it is also possible to decode representations when subjects were performing mental imagery, without being exposed to any perceptual input. Different theories on mental imagery processes emphasize either the role of the early visual areas (<xref ref-type="bibr" rid="c60">Kosslyn et al., 1999</xref>; <xref ref-type="bibr" rid="c90">Pearson, 2019</xref>) or the role of the high-level visual areas in the ventral temporal cortex and frontoparietal networks (<xref ref-type="bibr" rid="c111">Spagna et al., 2021</xref>; <xref ref-type="bibr" rid="c46">Hajhajate et al., 2022</xref>; <xref ref-type="bibr" rid="c71">Liu et al., 2025</xref>). There is evidence for both kinds of theories in the form of neuroimaging studies that used decoding to identify stimuli during mental imagery. Some of these found relevant patterns in the early visual cortex (<xref ref-type="bibr" rid="c1">Albers et al., 2013</xref>; <xref ref-type="bibr" rid="c81">Naselaris et al., 2015</xref>), others highlighted the role of higher-level areas in the ventral visual processing stream (<xref ref-type="bibr" rid="c114">Stokes et al., 2009</xref>; <xref ref-type="bibr" rid="c98">Reddy et al., 2010</xref>; <xref ref-type="bibr" rid="c65">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="c122">VanRullen and Reddy, 2019</xref>; <xref ref-type="bibr" rid="c14">Boccia et al., 2019</xref>) as well as the precuneus and the intraparietal sulcus (<xref ref-type="bibr" rid="c54">Johnson and Johnson, 2014</xref>). These discrepancies can possibly be explained by differences in experimental design: For example, the early visual cortex might only become involved if the task requires the imagination of high-resolution details, which are represented in lower levels of the visual processing hierarchy (<xref ref-type="bibr" rid="c61">Kosslyn and Thompson, 2003</xref>).</p>
<p>Crucially, it has been shown that decoders trained exclusively on trials <italic>with</italic> perceptual input can generalize to imagery trials (<xref ref-type="bibr" rid="c114">Stokes et al., 2009</xref>; <xref ref-type="bibr" rid="c98">Reddy et al., 2010</xref>; <xref ref-type="bibr" rid="c65">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="c54">Johnson and Johnson, 2014</xref>; <xref ref-type="bibr" rid="c81">Naselaris et al., 2015</xref>), providing evidence that representations formed during perception overlap to some degree with representations formed during mental imagery (<xref ref-type="bibr" rid="c28">Dijkstra et al., 2019</xref>).</p>
<p>In our study, we explored to what extent these findings hold true for more varied and complex stimuli. Following previous approaches, we use decoders trained exclusively on trials where subjects were viewing images and captions, and evaluated them on their ability to decode the imagery trials. We additionally hypothesized that mental imagery should be at least as conceptual as it is sensory and therefore primarily recruit modality-agnostic representations. Consequently, modality-agnostic decoders should be ideally suited to decode mental imagery and outperform modality-specific decoders on that task.</p>
</sec>
<sec id="s2d">
<title>Methods for localizing modality-agnostic regions</title>
<p>The first evidence for the existence of modality-agnostic regions came from observations of patients with lesions in particular cortical regions which lead to deficits in the retrieval and use of knowledge across modalities (<xref ref-type="bibr" rid="c125">Warrington and Shallice, 1984</xref>; <xref ref-type="bibr" rid="c124">Warrington and Mccarthy, 1987</xref>; <xref ref-type="bibr" rid="c42">Gainotti, 2000</xref>; <xref ref-type="bibr" rid="c23">Damasio et al., 2004</xref>). Semantic impairments across modalities have also been observed in patients with the neurodegenerative disorder semantic dementia (<xref ref-type="bibr" rid="c123">Warrington, 1975</xref>; <xref ref-type="bibr" rid="c110">Snowden et al., 1989</xref>; <xref ref-type="bibr" rid="c51">Jefferies et al., 2009</xref>).</p>
<p>In early work exploring the possible locations of modality-agnostic regions in healthy subjects, brain activity was recorded using imaging techniques while they were presented with a range of concepts in two modalities (e.g. words and pictures). Regions that were active during semantic processing of stimuli in the first modality were compared to regions that were active during semantic processing of stimuli in the second modality. The conjunction of these regions was proposed to be modality-agnostic (<xref ref-type="bibr" rid="c121">Vandenberghe et al., 1996</xref>; <xref ref-type="bibr" rid="c79">Moore and Price, 1999</xref>; <xref ref-type="bibr" rid="c18">Bright et al., 2004</xref>).</p>
<p>While this methodology allows for the identification of candidate regions in which semantic processing of multiple modalities occurs, it can not be used to probe the information represented in these regions. In order to compare the information content (i.e. multivariate patterns) of brain regions, researchers have developed Representational Similarity Analysis (RSA, <xref ref-type="bibr" rid="c63">Kriegeskorte et al., 2008</xref>) as well as encoding and decoding analyses (<xref ref-type="bibr" rid="c80">Naselaris et al., 2011</xref>). More specifically, RSA has been used to find modality-agnostic regions by comparing activation patterns of a candidate region when subjects are viewing stimuli from different modalities (<xref ref-type="bibr" rid="c25">Devereux et al., 2013</xref>; <xref ref-type="bibr" rid="c47">Handjaras et al., 2016</xref>; <xref ref-type="bibr" rid="c72">Liuzzi et al., 2017</xref>). This comparison is performed in an indirect way, by measuring the correlation of dissimilarity matrices of activation patterns. In turn, cross-decoding analysis can be leveraged to identify modality-agnostic regions by training a classifier to predict the category of a stimulus in a given modality, and then evaluating its performance to predict the category of stimuli that were presented in another modality (<xref ref-type="bibr" rid="c105">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c73">Man et al., 2012</xref>; <xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c106">Simanova et al., 2014</xref>; <xref ref-type="bibr" rid="c55">Jung et al., 2018</xref>). However, all these studies relied on a predefined set of stimulus categories, and can therefore not be easily extended to more realistic and complex stimuli, as we perceive them in our everyday life.</p>
<p>We summarize candidates for modality-agnostic regions that have been identified by previous studies in Appendix 3. This overview reveals substantial disagreement regarding the possible locations of modality-agnostic patterns in the brain. For example, <xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref> found modality-agnostic representations in the left ventral temporal cortex (fusiform, parahippocampal, and perirhinal cortex), middle and inferior temporal gyrus, angular gyrus, parts of the prefrontal cortex as well as the precuneus. <xref ref-type="bibr" rid="c105">Shinkareva et al., 2011</xref> found a larger network of left-lateralized regions, including additionally the left superior temporal, inferior parietal, supramarginal, inferior and inferior occipital, precentral and postcentral gyrus, supplementary motor area, intraparietal sulcus, cuneus, posterior cingulum as well as the right fusiform gyrus and the superior parietal gyrus, paracentral lobule on both hemispheres. In contrast, <xref ref-type="bibr" rid="c55">Jung et al., 2018</xref> found modality-agnostic representations only in the right prefrontal cortex. These diverging results can probably be explained by the limited number of stimuli as well as the use of artificially constructed stimuli in certain studies.</p>
<p>Recent advances in machine learning have enabled another generation of fMRI analyses based on large-scale naturalistic datasets. Here, we present a new multimodal dataset of subjects viewing both images and text. Most importantly, the dataset contains a large number of naturalistic stimuli in the form of complex visual scenes and full sentence descriptions of the same type of complex scenes, instead of pictures of single objects and words as commonly used in previous studies. This data enables the development of modality-agnostic decoders that are explicitly trained to leverage features that are shared across modalities. Further, we use this data to localize modality-agnostic regions in the brain by applying decoders in a multimodal searchlight analysis.</p>
</sec>
</sec>
<sec id="s3">
<title>Methods</title>
<sec id="s3a">
<title>fMRI Experiment</title>
<p>Six subjects (2 female, age between 20 and 50 years, all right-handed and fluent English speakers) participated in the experiment after providing informed consent. The study was performed in accordance with French national ethical regulations (Comité de Protection des Personnes, ID 2019-A01920-57). We collected functional MRI data using a 3T Philips ACHIEVA scanner (gradient echo pulse sequence, TR=2s, TE=30ms, 46 slices with a 32-channel head coil, slice thickness=3mm with 0.2mm gap, in-plane voxel dimensions 3×3mm). At the start of each session, we further acquired high-resolution anatomical images for each subject (voxel size=1mm<sup>3</sup>, TR=8.13ms, TE=3.74ms, 170 sagittal slices).</p>
<p>Scanning was spanned over 10 sessions (except for sub-01: 11 sessions), each consisting of 13 to 16 runs during which the subjects were presented with 86 stimuli. Each run started and ended with an 8s fixation period. The stimulus type varied randomly inside each run between images and captions. Each stimulus was presented for 2.5 seconds at the center of the screen (visual angle: 14.6 degrees; captions were displayed in white on a dark gray background (font: “Consolas”), the inter-stimulus interval was 1s. Every 10 stimuli there was a fixation trial that lasted for 2.5s. Every 5min there was a longer fixation trial for 16s.</p>
<p>Subjects performed a one-back matching task: They were instructed to press a button when-ever the stimulus matched the immediately preceding one (cf. <xref rid="fig1" ref-type="fig">Figure 1</xref> Panel A). In case the previous stimulus was of the same modality (e.g. two captions in a row), the subjects were instructed to press a button if the stimuli matched exactly. In the cross-modal case (e.g. an image followed by a caption), the button had to be pressed if the caption was a valid description of the image, and vice versa. Positive one-back trials occurred on average every 10 stimuli.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Panel A: Setup of the main fMRI experiment.</title><p>Subjects were seeing images and captions in random alternation. Whenever the current stimulus matched the previous stimulus, the subjects were instructed to press a button (one-back matching task). Images and captions for illustration only; actual size and stimuli as described in the text. Panel B: Number of distinct training stimuli (excluding trials that were one-back targets or during which the subject pressed the response button). There was an additional set of 140 stimuli (70 images and 70 captions) used for testing. Panel C: Setup of the fMRI experiment for the imagery trials. Subjects were instructed to remember 3 image descriptions with corresponding indices (numbers 1 to 3). One of these indices was displayed during the instruction phase, followed by a fixation phase, and then the subjects were imagining the visual scene for 10s.</p></caption>
<graphic xlink:href="658221v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Images and captions were taken from the training and validation sets of the COCO dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>). This dataset contains 5 matching captions for each image, of which we only considered the shortest one in order to fit on the screen and to ensure a comparable length for all captions. Spelling errors were corrected manually. As our training set, a random subset of images and another random subset of captions were selected for each subject. All these stimuli were presented only a single time. Information on the number of training stimuli for each subject is shown in <xref rid="fig1" ref-type="fig">Figure 1</xref> Panel B. Additionally, a shared subset of 140 stimuli (70 images and 70 captions) was presented repeatedly to each subject in order to reduce noise, serving as our test set (on average: 26 times, min: 22, max: 31). Contrary to the training stimuli which were randomly selected from the COCO dataset, the 70 test stimuli were chosen by hand to avoid including multiple scenes that could match the same semantic description. The 70 chosen images as well as their 70 corresponding captions constituted the test set. These stimuli were inserted randomly between the training stimuli.</p>
<p>Note that for each stimulus presented to the subject (e.g. an image), we also have access to the corresponding stimulus in the other modality (the corresponding caption from the COCO dataset), allowing us to estimate model features based on both modalities (vision model features extracted from the image and language model features extracted from the corresponding caption) as well as multimodal features extracted from both the image and the caption.</p>
<p>In addition to these perceptual trials, there were 3 imagery trials for each subject (see also <xref rid="fig1" ref-type="fig">Figure 1</xref> Panel C). Prior to the first fMRI scanning session, each subject was presented with a set of 20 captions (manually selected to be diverse and easy to visualize) that were not part of the perceptual trials, and they selected 3 captions for which they felt comfortable imagining a corresponding image. Then, they learned a mapping of each caption to a number (1, 2, and 3) so that they could be instructed to perform mental imagery of a specific stimulus, without having to present them with the caption again. The imagery trials occurred every second run, either at the beginning or the end of the run, so that each of the 3 imagery conditions were repeated on average 26 times (min: 23, max: 29). At the start of the imagery trial, the imagery instruction number was presented for 2s, then there was a 1s fixation period followed by the actual imagery period during which a light gray box was depicted for 10s on a dark gray background (the same background that was also used for trials with perceptual input). The light gray box was meant to represent the area in which the mental image should be “projected”. At the end of the experiment, the subjects drew sketches of the images they had been imagining during the imagery trials.</p>
</sec>
<sec id="s3b">
<title>fMRI Preprocessing</title>
<p>Preprocessing of the fMRI data was performed using SPM12 (<xref ref-type="bibr" rid="c5">Ashburner et al., 2014</xref>) via nipype (<xref ref-type="bibr" rid="c44">Gorgolewski et al., 2011</xref>). We applied slice time correction and realignment for each subject. Each session was coregistered with an anatomical scan of the respective subject’s first session (down-sampled to 2mm<sup>3</sup>). We created and applied explicit gray matter masks for each subject based on their anatomical scans using a maximally lenient threshold (probability&gt;0).</p>
<p>In order to obtain beta-values for each stimulus, for each subject we fit a GLM (using SPM12) on data from all sessions. We included regressors for train images, train captions, test images, test captions, imagery trials, fixations, blank screens, button presses, and one-back target trials. One-back target trials as well as trials in which the participant pressed the button were excluded in the calculation of all training and test stimulus betas. As output of these GLMs we obtained beta-values for each training and test caption and image as well as the imagery trials.</p>
<p>Finally, we transformed the volume-space data to surface space Freesurfer (<xref ref-type="bibr" rid="c39">Fischl, 2012</xref>). We used trilinear interpolation and the fsaverage template in the highest possible resolution (163,842 vertices on each hemisphere) as target.</p>
</sec>
<sec id="s3c">
<title>Modality-Agnostic Decoders</title>
<p>The multimodal nature of our dataset allowed for the training of modality-agnostic decoders. We trained decoders by fitting ridge regression models that take fMRI beta-values as input and predict latent representations extracted from a pretrained deep learning model. Further details on decoder training can be found in <xref rid="fig2" ref-type="fig">Figure 2</xref> as well as Appendix 1.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Training of modality-specific and modality-agnostic decoders.</title><p>Panel A: Modality-specific decoders are trained on fMRI data of one modality (e.g. subjects viewing images) by mapping it to features extracted from the same stimuli. Panel B: Modality-agnostic decoders are trained jointly on fMRI data of both modalities (subjects viewing images and captions). Panel C: To train decoders, features can be either extracted unimodally from the corresponding images or captions, or by creating multimodal features based on both modalities. For example, to train a modality-agnostic decoder based on features from a unimodal language model, we map the fMRI data of subjects viewing captions to features extracted from the respective captions using this language model, as well as the fMRI data of subjects viewing images to features extracted by the language model from the corresponding captions. We can also train modality-specific decoders on features from another modality, for example by mapping fMRI data of subjects viewing images to features extracted from the corresponding captions using a language model (cf. crosses on orange bars in <xref rid="fig4" ref-type="fig">Figure 4</xref> or using multimodal features (cf. crosses on blue bars in <xref rid="fig4" ref-type="fig">Figure 4</xref>).</p></caption>
<graphic xlink:href="658221v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>While modality-specific decoders are trained only on brain imaging data of a single modality, modality-agnostic decoders are trained on brain imaging data from multiple modalities and therefore allow for decoding of stimuli irrespective of their modality.</p>
<p>More specifically, in our case the modality-specific decoders are trained on fMRI beta-values from one stimulus modality, e.g. when subjects were watching images (cf. <xref rid="fig2" ref-type="fig">Figure 2</xref> panel A). Conversely, modality-agnostic decoders are trained jointly using fMRI data from both stimulus modalities (images and captions; cf. <xref rid="fig2" ref-type="fig">Figure 2</xref> panel B). For all decoders, the features that serve as regression targets can either be unimodal (e.g. extracted from images using a vision model) or multimodal (e.g. extracted from both stimulus modalities using a multimodal model, cf. <xref rid="fig2" ref-type="fig">Figure 2</xref> panel C).</p>
<p>We considered features extracted from a range of vision, language, and multimodal models: For vision features, we considered ResNet (<xref ref-type="bibr" rid="c49">He et al., 2016</xref>), ViT (<xref ref-type="bibr" rid="c30">Dosovitskiy et al., 2020</xref>), and DI-NOv2 (<xref ref-type="bibr" rid="c87">Oquab et al., 2023</xref>); for language features BERT (<xref ref-type="bibr" rid="c26">Devlin et al., 2019</xref>), GPT2 (<xref ref-type="bibr" rid="c96">Radford et al., 2019</xref>), Llama2 (<xref ref-type="bibr" rid="c119">Touvron et al., 2023</xref>), mistral and mixtral (<xref ref-type="bibr" rid="c53">Jiang et al., 2023</xref>). Regarding multimodal features, we extracted features from VisualBERT (<xref ref-type="bibr" rid="c67">Li et al., 2019</xref>), BridgeTower (<xref ref-type="bibr" rid="c127">Xu et al., 2023</xref>), ViLT (<xref ref-type="bibr" rid="c59">Kim et al., 2021</xref>), CLIP (<xref ref-type="bibr" rid="c95">Radford et al., 2021</xref>), ImageBind (<xref ref-type="bibr" rid="c43">Girdhar et al., 2023</xref>), Flava (<xref ref-type="bibr" rid="c108">Singh et al., 2022</xref>), Blip2 (<xref ref-type="bibr" rid="c66">Li et al., 2023</xref>), SigLip (<xref ref-type="bibr" rid="c128">Zhai et al., 2023</xref>), and Paligemma2 (<xref ref-type="bibr" rid="c113">Steiner et al., 2024</xref>). In order to estimate the effect of model training, we further extracted features from a randomly initialized ImageBind model as a baseline. Further details on feature extraction can be found in Appendix 1. All decoders were evaluated on the held-out test data (140 stimuli, 70 captions and 70 images) using pairwise accuracy calculated using cosine distance. Prior to calculating the pairwise accuracy, the model predictions for all stimuli were standardized to have mean of 0 and standard deviation of 1. In the case of imagery decoding, the model predictions were standardized separately. In the case of cross-modal decoding (e.g. mapping an image stimulus into the latent space of a language model), a trial was counted as correct if the caption corresponding to the image (according to the ground-truth in COCO) was closest.</p>
<p><xref rid="fig3" ref-type="fig">Figure 3</xref> provides an overview on the evaluation metrics. A modality-specific decoder for images can be evaluated on its ability to decode images (Panel A, top) and in a cross-decoding setup for captions (Panel B, bottom). In the same way, we can compute the respective evaluation metrics for modality-specific decoders trained on captions. For the case of modality-agnostic decoders, we evaluate performance for decoding both images and captions using the same single decoder that is trained on both modalities (Panel C).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Evaluation of modality-specific and modality-agnostic decoders.</title><p>The matrices display cosine similarity scores between features extracted from the candidate stimuli and features predicted by the decoder. The evaluation metric is pairwise accuracy, which is calculated row-wise: For a given matrix row, we compare the similarity score of the target stimulus on the diagonal (in green) with the similarity scores of all other candidate stimuli (in red). Panel A: Within-modality decoding metrics of modality-specific decoders. To compute within-modality accuracy for image decoding, a modality-specific decoder trained on images is evaluated on all stimuli that were presented as images. To compute within-modality accuracy for caption decoding, a modality-specific decoder trained on captions is evaluated on all caption stimuli. Panel B: Cross-modality decoding metrics of modality-specific decoders. To compute cross-modality accuracy for image decoding, a modality-specific decoder trained on captions is evaluated on all stimuli that were presented as images. To compute cross-modality accuracy for caption decoding, a modality-specific decoder trained on images is evaluated on all caption stimuli. Panel C: Metrics for modality-agnostic decoders. To compute modality-agnostic accuracy for image decoding, a modality-agnostic decoder is evaluated on all stimuli that were presented as images. The same decoder is evaluated on caption stimuli to compute modality-agnostic accuracy for caption decoding. Here we show feature extraction based on unimodal features for modality-specific decoders and based on multimodal features for the modality-agnostic decoder, in practice the feature extraction can be unimodal or multimodal for any decoder type (see also <xref rid="fig2" ref-type="fig">Figure 2</xref>).</p></caption>
<graphic xlink:href="658221v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s4">
<title>Results</title>
<sec id="s4a">
<title>Modality-Agnostic Decoders</title>
<p>We first compared the performance of modality-specific and modality-agnostic decoders that are trained on the whole brain fMRI data based on different unimodal and multimodal features. The average pairwise accuracy scores are presented in <xref rid="fig4" ref-type="fig">Figure 4</xref>. <xref rid="fig5" ref-type="fig">Figure 5</xref> presents pairwise accuracy scores separately for decoding images and for decoding captions. Results for individual subjects can be found in Appendix 5.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Average decoding scores for modality-agnostic decoders (bars), compared to modality-specific decoders trained on data from subjects viewing captions (·) or on data from subjects viewing images (×).</title><p>The metric is pairwise accuracy (see also <xref rid="fig3" ref-type="fig">Figure 3</xref>). Error bars indicate 95% confidence intervals for modality-agnostic decoders. Chance performance is at 0.5.</p></caption>
<graphic xlink:href="658221v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Decoding accuracy for decoding captions (top) and for decoding images (bottom).</title><p>The bars indicate modality-agnostic decoding accuracy. The crosses (×) in the top row and the dots (·) in the bottom row indicate within-modality decoding scores. The dots (·) in the top row indicate cross-decoding scores for images, the crosses (×) in the bottom row indicate cross-decoding scores for captions (see also <xref rid="fig3" ref-type="fig">Figure 3</xref>).</p></caption>
<graphic xlink:href="658221v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>When analyzing the average decoding accuracy (<xref rid="fig4" ref-type="fig">Figure 4</xref>), we find that modality-agnostic decoders perform better than modality-specific decoders, irrespective of the features that the decoders were trained on. This high performance (which can be attributed to the large training dataset used by modality-agnostic decoders) is achieved despite the additional challenge of not knowing the modality of the stimulus the subject was seeing.</p>
<p>Further, we observed that modality-agnostic decoders based on the best multimodal features (imagebind: 85.71% ± 2.58%) do not perform substantially better than decoders based on the best language features (GPT2-large: 85.31%±2.83%) and only slightly better than decoders trained on the best vision features (Dino-giant: 82.02%±2.43%). This result suggests that high-performing modality-agnostic decoders do not necessarily need to rely on multimodal features; features extracted from language models can lead to equally high performance. When comparing the different architecture types of models for multimodal feature extraction (dual stream vs. single stream with early fusion vs. single stream with late fusion; cf. Panel C in <xref rid="fig2" ref-type="fig">Figure 2</xref>), we only observed a slight performance disadvantage for single-stream models with early fusion (Mean accuracy values for dual stream models: 85.04%; for single stream models with early fusion: 81.01%; for single stream models with late fusion 83.63%). We performed a repeated measures ANOVA (grouping the data by subject), comparing the decoding accuracy values of modality-agnostic decoders based on different families of multimodal features. The only significant effect was: model_family_single_stream_early_fusion ∶ β = −0.04, <italic>SE</italic> = 0.011, <italic>p</italic> &lt; 1<italic>e</italic> − 3.</p>
<p>When analyzing the performance specifically for decoding images (<xref rid="fig5" ref-type="fig">Figure 5</xref>, top), we find that modality-agnostic decoders perform as well as the modality-specific decoders trained on images (crosses in top row are at the same level as the bars in <xref rid="fig5" ref-type="fig">Figure 5</xref>; we find no statistically significant difference in their performances. We performed a repeated measures ANOVA (grouping the data by subject), comparing the image decoding accuracy values of modality-agnostic decoders with those of modality-specific decoders trained on images. The resulting p-value for the effect of the decoder_type was <italic>p</italic> = 0.73.) Further, modality-agnostic decoders even outperform modality-specific decoders trained on captions for decoding captions (dots in the bottom row are lower than bars in <xref rid="fig5" ref-type="fig">Figure 5</xref>). We performed a repeated measures ANOVA (grouping the data by subject), comparing the caption decoding accuracy values of modality-agnostic decoders with those of modality-specific decoders trained on captions. The result was: decoder_type∶ β = 0.036, <italic>SE</italic> = 0.006, <italic>p</italic> &lt; 1 ⋅ 10<sup>−8</sup>. In other words, even if we know that a brain pattern was recorded in response to the subject reading a caption, we are more likely to decode it accurately if we choose to apply a decoder trained using both modalities, than if we apply the appropriate decoder, trained only on captions.</p>
<p>Furthermore, we found that the cross-modal decoding performance for decoding visual stimuli (images) using decoders trained on linguistic stimuli (captions) is higher than the cross-modal decoding performance in the other direction, corroborating similar results from <xref ref-type="bibr" rid="c116">Tang et al., 2023a</xref> on movies and audio books (dots in top row are higher than crosses in bottom row in <xref rid="fig5" ref-type="fig">Figure 5</xref>).</p>
</sec>
<sec id="s4b">
<title>Qualitative Decoding Results</title>
<p>To obtain a better understanding of the decoding performance of the modality-agnostic decoders, we inspected the decoding results for 5 randomly selected test stimuli. We created a large candidate set of 41,118 stimuli by combining the test stimuli and the training stimuli from all subjects. For each stimulus, we ranked these candidate set stimuli based on their similarity to the predicted feature vector. As the test stimuli were shared among all subjects, we could average the prediction feature vectors across subjects to obtain the best decoding results.</p>
<p><xref rid="fig6" ref-type="fig">Figure 6</xref> presents the results for decoding images using a modality-agnostic decoder trained on ImageBind features. We display the target stimulus along with the top-5 ranked test stimuli. We can observe some clear success cases (the train in the first row) but also failure cases (the teddy bear decoded as pizza). For the other stimuli, some aspects such as the high-level semantic class (e.g. vehicle, animal) are correctly decoded: For the cars on the highway (last row), the top-ranked images depict trains, which are also vehicles. For the dog (2nd row), the top images contain cats of similar colors. Regarding the giraffe (3rd row), the model appears to have picked up on the fact that there was a body of water depicted in the image.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Decoding examples for image decoding using a modality-agnostic decoder.</title><p>The first column shows the image the subject was seeing and the 5 following columns show the candidate stimuli with highest similarity to the predicted features, in descending order. We display both the image and the caption of the candidate stimuli because the decoder is based on multimodal features that are extracted from both modalities. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Note that these qualitative results are not directly comparable with previous work on retrieval or reconstruction using the NSD dataset (<xref ref-type="bibr" rid="c3">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="c68">Lin et al., 2022</xref>; <xref ref-type="bibr" rid="c115">Takagi and Nishimoto, 2023</xref>; <xref ref-type="bibr" rid="c88">Ozcelik and VanRullen, 2023</xref>), as our data was collected on a 3T MRI scanner with lower signal-to-noise-ratio than NSD’s 7T MRI scanner.</p>
<p>The ranking results for decoding captions are depicted in <xref rid="fig7" ref-type="fig">Figure 7</xref>. The results are somewhat similar to the image decoding results, stimuli that were decoded successfully when presented as image such as the train are also decoded successfully when presented as caption; cases that were failures in the case of image decoding (e.g. the teddy bear) also fail here. However, the top-ranked stimuli for the dog (2nd row) do not always contain animals (the decoder seems to have picked up on the presence of a vehicle in the caption), but the cars on the highway (last row) get decoded rather successfully.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Decoding examples for caption decoding using a modality-agnostic decoder.</title><p>For details see caption of <xref rid="fig6" ref-type="fig">Figure 6</xref>. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v1_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We additionally provide qualitative results for modality-specific decoders in Appendix 2. These results generally reflect the observations from the quantitative results: Modality-agnostic decoders perform similarly to modality-specific decoders evaluated in a within-modality decoding setup, but substantially better than modality-specific decoders when evaluated in cross-decoding setups.</p>
</sec>
<sec id="s4c">
<title>Modality-Agnostic Regions</title>
<p>To provide insight into the spatial organization of modality-agnostic representations in the brain, we performed a surface-based searchlight analysis.</p>
<p>Modality-agnostic regions should contain patterns that generalize between stimulus modalities. Therefore, such regions should allow for decoding of stimuli in both modalities using a decoder that is trained to pick up on modality-agnostic features, i.e. the decoding performance for images and captions of a modality-agnostic decoder should both be above chance. However, as a modality-agnostic decoder is trained on stimuli from both modalities, it could have learned to leverage certain features to project stimuli from one modality and different features to project stimuli from the other modality. We added two conditions to control that the representations directly transfer between the modalities by additionally training two modality-specific decoders and evaluating them according to their cross-decoding performance, i.e. we require that their decoding performance in the modality they were not trained on is above chance. These four conditions are summarized at the top of <xref rid="fig8" ref-type="fig">Figure 8</xref>.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
<caption><title>Searchlight method to identify modality-agnostic ROIs.</title><p>The top plots show performance (pairwise accuracy averaged over subjects) of modality-agnostic decoders for decoding images (top left) and decoding captions (top right). In the second row, we display cross-decoding performances: On the left, modality-specific decoders trained on captions are evaluated on images. On the right, modality-specific decoders trained on images are evaluated on captions. We identified modality-agnostic ROIs as clusters in which all 4 decoding accuracies are above chance by taking the minimum of the respective t-values at each location, then performed TFCE to calculate cluster values. The plot only shows left medial views of the brain to illustrate the method, different views of all resulting clusters are shown in <xref rid="fig9" ref-type="fig">Figure 9</xref>.</p></caption>
<graphic xlink:href="658221v1_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We used ImageBind features for these searchlight analyses as they led to the highest decoding performance when using the whole brain data. The decoders were trained based on the surface projection of the fMRI beta-values. For each vertex, we defined a searchlight with a fixed size by selecting the 750 closest vertices, corresponding to an average radius of ∼ 9.4mm. Details on how this size was selected are outlined in Appendix 4.</p>
<p>We trained and evaluated a modality-agnostic decoder and modality-specific decoders for both modalities on the beta-values for each searchlight location and each subject, providing us with a decoding accuracy scores for each location on the cortex. Then we performed t-tests to identify locations in which the decoding performance is above chance (<italic>acc</italic> &gt; 0.5). We aggregated all 4 comparisons by taking the minimum of the 4 t-values at each spatial location. Finally, we performed a threshold-free cluster analysis (TFCE, <xref ref-type="bibr" rid="c109">Smith and Nichols, 2009</xref>) to identify modality-agnostic ROIs (<xref rid="fig8" ref-type="fig">Figure 8</xref>, bottom). We used the default hyperparameters of ℎ = 2 and <italic>e</italic> = 1 for surface-based TFCE (<xref ref-type="bibr" rid="c52">Jenkinson et al., 2012</xref>).</p>
<p>To estimate the statistical significance of the resulting clusters we performed a permutation test. For each subject, we evaluated the decoders 100 times with shuffed labels to create a surrogate distribution. Then, we sampled 10,000 permutations of the 6 subjects’ surrogate distributions and calculated group-level statistics (TFCE values) for each of them. Based on this null distribution, we calculated p-values for each cluster. To control for multiple comparisons across space, we took the maximum TFCE score across vertices for each permutation (<xref ref-type="bibr" rid="c109">Smith and Nichols, 2009</xref>).</p>
<p>The results of the surface-based searchlight analysis are presented in <xref rid="fig9" ref-type="fig">Figure 9</xref>. The analysis revealed that modality-agnostic patterns are actually widespread across the brain, especially on the left hemisphere. Peak cluster values were found in the left supramarginal gyrus, inferior parietal gyrus and posterior superior temporal sulcus. Regions belonging to the precuneus, isthmus of the cingulate gyrus, parahippocampus, middle temporal gyrus, inferior temporal gyrus and fusiform gyrus also showed high cluster values.</p>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9.</label>
<caption><title>Searchlight results for modality-agnostic regions.</title><p>Maps thresholded at TFCE value of 1508, which is the significance threshold value for which <italic>p</italic> &lt; 10<sup>−4</sup> based on the permutation testing. Regions with highest cluster values are outlined and annotated based on the Desikan-Killiany atlas (<xref ref-type="bibr" rid="c24">Desikan et al., 2006</xref>).</p></caption>
<graphic xlink:href="658221v1_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s4d">
<title>Imagery Decoding</title>
<p>Finally, we evaluated the ability of decoders trained on the fMRI data with perceptual input to decode stimuli during the imagery conditions.</p>
<p>For a modality-agnostic decoder trained on the whole-brain data, the imagery pairwise decoding accuracy reaches 84.48% (averaged across subjects and model features) when using the 3 imagery stimuli as the candidate set. Note that we used the ground-truth caption and corresponding image from COCO in this candidate set, and not the sketches drawn by the subjects. When the whole test set is added to the candidate set (in total: 73 stimuli), the average pairwise accuracy drops to 72.47%. This substantial drop in performance is most likely explained by the fact that the predicted features for the imagery trials were standardized using only 3 stimuli, and this transformation emphasized differences that enabled distinguishing the 3 imagery trials but do not generalize to the larger test set. We also attempted decoding without standardization of the predicted feature vectors, but this led to much lower performance.</p>
<p>As expected, we also found that modality-agnostic decoders are better suited for imagery decoding than modality-specific decoders. We compared the imagery decoding accuracy of both decoder types taking into account the results for all features and all subjects. To this end, we performed two repeated measures ANOVAs (grouping the data by subject), once comparing the accuracy values of modality-agnostic decoders with those of modality-specific decoders trained on images, and once comparing modality-agnostic decoders to modality-specific decoders trained on captions. The average decoding accuracies were 69.42% for a modality-specific decoder trained on images and 70.02% for a modality-specific decoder trained on captions (vs. 72.47% for a modality-agnostic decoder, as mentioned above). In both comparisons, the accuracy values for the two decoder types were significantly different (when comparing modality-agnostic decoders to modality-specific decoders trained on images: decoder_type ∶ β = 0.03, <italic>SE</italic> = 0.011, <italic>p</italic> &lt; 0.01; and when comparing to modality-specific decoders trained on captions: decoder_type ∶ β = 0.024, <italic>SE</italic> = 0.012, <italic>p</italic> &lt; 0.04).</p>
<p>Appendix 6 presents qualitative decoding results for the imagery trials for each subject as well as the sketches of the mental images drawn at the end of the experiment. As expected, the results are worse than those for perceived stimuli, but for several subjects it was possible to decode some major semantic concepts.</p>
<p>We further computed the imagery decoding accuracy during the searchlight analysis. <xref rid="fig10" ref-type="fig">Figure 10</xref> shows the result clusters for decoding imagery (using the whole test set + the 3 imagery trials as potential candidates).</p>
<fig id="fig10" position="float" orientation="portrait" fig-type="figure">
<label>Figure 10.</label>
<caption><title>Searchlight results for imagery decoding.</title><p>Maps thresholded at TFCE values that surpass the significance threshold of <italic>p</italic> &lt; 10<sup>−4</sup> based on a permutation test. Maps thresholded at TFCE value of 3897, which is the significance threshold value for which <italic>p</italic> &lt; 10<sup>−4</sup> based on the permutation testing We used the pairwise accuracy for imagery decoding using the large candidate set of 73 stimuli. We outlined the same regions as in <xref rid="fig9" ref-type="fig">Figure 9</xref> to facilitate comparison.</p></caption>
<graphic xlink:href="658221v1_fig10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We observe that many regions that were found to contain modality-agnostic patterns (cf. <xref rid="fig9" ref-type="fig">Figure 9</xref>) are also regions in which decoding of mental imagery is possible.</p>
<p>One main difference is that the imagery decoding clusters appear to be less left-lateralized than the modality-agnostic region clusters (peak cluster values can be found both on the right inferior parietal cortex and bilaterally in the precuneus). To estimate overlap of the regions allowing for imagery decoding and modality-agnostic regions we calculated the correlation between the TFCE values that were used for identifying modality-agnostic regions (<xref rid="fig9" ref-type="fig">Figure 9</xref>) and the TFCE values for imagery decoding (<xref rid="fig10" ref-type="fig">Figure 10</xref>). The Pearson correlation score for the left hemisphere is 0.41 (<italic>p</italic> &lt; 1<italic>e</italic> − 8), and for the right hemisphere 0.62 (<italic>p</italic> &lt; 1<italic>e</italic> − 8). Importantly, these correlation scores are substantially higher when compared to the correlation with decoding accuracy of modality-specific decoders: The correlation between the TFCE values for imagery decoding and TFCE values for image decoding of a modality-specific decoder trained on images is 0.28 on the left hemisphere and 0.40 on the right hemisphere. When using TFCE values based on the caption decoding accuracy of a modality-specific decoder trained on captions we obtain 0.19 on the left hemisphere and 0.45 on the right hemisphere.</p>
</sec>
</sec>
<sec id="s5">
<title>Discussion</title>
<p>In this work, we introduced a new large-scale multimodal fMRI dataset that enables the development of models for modality-agnostic decoding of visual and linguistic stimuli using a single model. These modality-agnostic decoders were specifically trained to pick up on modality-agnostic patterns, enabling a performance increase over modality-specific decoders when decoding linguistic stimuli in the form of captions.</p>
<p>According to a range of theories, modality-agnostic representations are tightly linked to (lexical-) semantic representations (<xref ref-type="bibr" rid="c107">Simmons and Barsalou, 2003</xref>; <xref ref-type="bibr" rid="c13">Binder et al., 2009</xref>; <xref ref-type="bibr" rid="c77">Meschke and Gallant, 2024</xref>). Most importantly, a range of studies that aimed to identify brain regions linked to semantic/conceptual representations by asking subjects to perform tasks that require semantic processing of words found evidence for such regions that overlap to a high degree with the regions identified in our study (<xref ref-type="bibr" rid="c37">Fernandino et al., 2016</xref>; <xref ref-type="bibr" rid="c76">Martin et al., 2018</xref>; <xref ref-type="bibr" rid="c19">Carota et al., 2021</xref>; <xref ref-type="bibr" rid="c38">Fernandino et al., 2022</xref>; <xref ref-type="bibr" rid="c118">Tong et al., 2022</xref>). A strong link between these systems could also explain our result that modality-agnostic decoders based on unimodal representations from language models are performing as well as decoders based on multimodal representations (cf. <xref rid="fig4" ref-type="fig">Figure 4</xref>), as well as the partial left-lateralization of the identified modality-agnostic regions.</p>
<p>In a second analysis, we additionally leveraged our dataset to localize modality-agnostic regions in the brain by searching for areas in which decoding of both stimulus modalities is possible using modality-agnostic decoders as well as in a cross-decoding setup. This approach lead to the identification of a large network involving temporal, parietal, and frontal regions, and peak cluster values on the left hemisphere (cf. <xref rid="fig9" ref-type="fig">Figure 9</xref>). All areas with high cluster values confirm findings from previous studies: The left precuneus (<xref ref-type="bibr" rid="c105">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c94">Popham et al., 2021</xref>; <xref ref-type="bibr" rid="c47">Handjaras et al., 2016</xref>), posterior cingulate/ retrosplenial cortex (<xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c47">Handjaras et al., 2016</xref>), supramarginal gyrus (<xref ref-type="bibr" rid="c105">Shinkareva et al., 2011</xref>), inferior parietal cortex (<xref ref-type="bibr" rid="c73">Man et al., 2012</xref>; <xref ref-type="bibr" rid="c121">Vandenberghe et al., 1996</xref>; <xref ref-type="bibr" rid="c105">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c25">Devereux et al., 2013</xref>; <xref ref-type="bibr" rid="c94">Popham et al., 2021</xref>; <xref ref-type="bibr" rid="c106">Simanova et al., 2014</xref>; <xref ref-type="bibr" rid="c47">Handjaras et al., 2016</xref>), superior temporal sulcus (<xref ref-type="bibr" rid="c73">Man et al., 2012</xref>), middle temporal gyrus (<xref ref-type="bibr" rid="c121">Vandenberghe et al., 1996</xref>; <xref ref-type="bibr" rid="c105">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c25">Devereux et al., 2013</xref>; <xref ref-type="bibr" rid="c47">Handjaras et al., 2016</xref>), inferior temporal gyrus (<xref ref-type="bibr" rid="c121">Vandenberghe et al., 1996</xref>; <xref ref-type="bibr" rid="c105">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c106">Simanova et al., 2014</xref>; <xref ref-type="bibr" rid="c47">Handjaras et al., 2016</xref>), fusiform gyrus (<xref ref-type="bibr" rid="c121">Vandenberghe et al., 1996</xref>; <xref ref-type="bibr" rid="c79">Moore and Price, 1999</xref>; <xref ref-type="bibr" rid="c18">Bright et al., 2004</xref>; <xref ref-type="bibr" rid="c105">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c106">Simanova et al., 2014</xref>), and parahippocampus (<xref ref-type="bibr" rid="c121">Vandenberghe et al., 1996</xref>). However, previous studies have led to contradicting results regarding the locality of modality-agnostic regions (they were identifying varying subsets of these regions; see also Appendix 3), probably due to the limited number and artificial nature of stimuli employed. Our method identified <italic>almost all</italic> of the previously proposed regions as regions with modality-agnostic patterns, highlighting the advantage of this large multimodal dataset in which subjects are viewing photographs of complex natural scenes and reading full English sentences. The left superior occipital gyrus was not identified in our study, but in previous studies by <xref ref-type="bibr" rid="c121">Vandenberghe et al., 1996</xref>; <xref ref-type="bibr" rid="c105">Shinkareva et al., 2011</xref>. However, we found that a major part of the left superior occipital <italic>sulcus</italic> represents modality-agnostic information. Further, <xref ref-type="bibr" rid="c55">Jung et al., 2018</xref> found modality-agnostic patterns in the right superior frontal gyrus. One major difference between their study and ours is that they used auditory input as a second modality instead of text. Further work is required to investigate to what extent the modality-agnostic regions identified in our work generalize to all modalities.</p>
<p>The fact that the presence of modality-agnostic patterns is positively correlated with the imagery decoding performance in different locations provides further evidence that the identified patterns are truly modality-agnostic. We further found that decoders trained exclusively on data for which participants were exposed to perceptual input do generalize to imagery trials, confirming previous findings that were based on more limited stimulus sets (<xref ref-type="bibr" rid="c114">Stokes et al., 2009</xref>; <xref ref-type="bibr" rid="c98">Reddy et al., 2010</xref>; <xref ref-type="bibr" rid="c65">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="c54">Johnson and Johnson, 2014</xref>; <xref ref-type="bibr" rid="c81">Naselaris et al., 2015</xref>). Regarding the representations involved in mental imagery, we found that modality-agnostic decoders outperform modality-specific decoders in terms of imagery decoding. This finding can be seen as support for the involvement of modality-agnostic representations in mental imagery, as modality-agnostic decoders were trained explicitly to pick up on such patterns.</p>
<p>The findings of our searchlight analysis for imagery decoding suggest that mental imagery indeed involves a large network of regions across both hemispheres of the cerebral cortex. This includes high-level visual areas, parietal areas such as the precuneus and inferior parietal cortex and several frontal regions, but also parts of the early visual cortex. Results are highly similar on both hemispheres, highlighting the involvement of large-scale bilateral brain networks during mental imagery of complex scenes.</p>
<p>While there are lesion studies on hemispheric asymmetries that suggest that regions in the left hemisphere are crucial for mental imagery (<xref ref-type="bibr" rid="c33">Farah, 1984</xref>; <xref ref-type="bibr" rid="c9">Bartolomeo, 2002</xref>), a more recent review that additionally considers evidence from neuroimaging and direct cortical stimulation studies suggests that frontoparietal networks in both hemispheres are involved in mental imagery, and that lateralization patterns can be found in the temporal lobes (<xref ref-type="bibr" rid="c70">Liu et al., 2022</xref>). Such lateralization was found to depend on the nature of the imagined items, the imagination of objects and words involving the left inferior temporal cortex while the imagination of faces and people was found to be more right-lateralized and the imagination of complex scenes (as in our study) leads to significant activity in both hemispheres (<xref ref-type="bibr" rid="c85">O’Craven and Kanwisher, 2000</xref>; <xref ref-type="bibr" rid="c112">Steel et al., 2021</xref>; <xref ref-type="bibr" rid="c111">Spagna et al., 2021</xref>). Crucially, in more recent decoding studies, results were either observed bilaterally, or the analyses did not target hemispheric asymmetries (<xref ref-type="bibr" rid="c98">Reddy et al., 2010</xref>; <xref ref-type="bibr" rid="c65">Lee et al., 2012</xref>). But see <xref ref-type="bibr" rid="c114">Stokes et al., 2009</xref> in which perception-to-imagery generalization of single letters was left-lateralized. Our work shows for the first time results of a searchlight analysis of imagery decoding of complex visual scenes. Above-chance decoding is possible on both hemispheres, with the highest decoding accuracies in the precuneus and the right inferior parietal cortex and the superior temporal sulcus. Future investigations with larger sets of imagined scenes could address the question whether lateralization patterns depend on the nature of the imagined objects.</p>
<p>It remains an open question whether the activation patterns in the modality-agnostic regions identified in our study relate to abstract concepts or to lower-level features that are shared between the two modalities. <xref ref-type="bibr" rid="c12">Binder, 2016</xref> puts this dichotomy into question, considering that “there is no absolute demarcation between embodied/perceptual and abstract/conceptual representation in the brain.” (p. 1098). The author argues for a hierarchical system in which representational patterns become increasingly abstract, creating a continuum between actual experiential information up to higher-level conceptual information (see also <xref ref-type="bibr" rid="c4">Andrews et al., 2014</xref>).</p>
<p>According to the results of our searchlight analysis, the anterior temporal lobes are not among the regions with the highest probability of being modality-agnostic, contradicting the hypothesis of the hub-and-spoke theory that these areas are the major semantic hub in the brain. However, MRI signals from these regions have a lower signal-to-noise ratio with standard fMRI pulse sequences (<xref ref-type="bibr" rid="c27">Devlin et al., 2000</xref>; <xref ref-type="bibr" rid="c31">Embleton et al., 2010</xref>). A more targeted study with an adapted fMRI protocol would be required to shed light on the nature of patterns in these regions. More generally, the hub-and-spoke theory also puts emphasis on the role of spokes for the formation of conceptual representations (<xref ref-type="bibr" rid="c93">Pobric et al., 2010</xref>; <xref ref-type="bibr" rid="c97">Ralph et al., 2017</xref>). Future work could be aimed at testing the hub-and-spoke theory proposal that features in hierarchically lower level representation spaces of the spokes are not directly relatable to features in the representation space of the hubs: Object representations in the spokes are based on interpretable features (e.g. shape, color, affordances of an object) and get translated into another representational format that is representing conceptual similarities (but its dimensions do not directly map to interpretable features) in the semantic hub (<xref ref-type="bibr" rid="c41">Frisby et al., 2023</xref>). To test this hypothesis, modality-agnostic representations in the anterior temporal lobes (measured with targeted fMRI pulse sequences) could be compared to representations in candidate regions for modality-specific spokes using RSA.</p>
<p>The modality-agnostic regions we found in the searchlight analysis can also be seen as candidates for convergence zones, in which increasingly abstract representations are formed (<xref ref-type="bibr" rid="c22">Damasio, 1989</xref>; <xref ref-type="bibr" rid="c120">Tranel et al., 1997</xref>; <xref ref-type="bibr" rid="c78">Meyer and Damasio, 2009</xref>). To obtain further insight into the hierarchical organization of these zones, future work could take advantage of the improved temporal resolution of other brain imaging techniques such as MEG to explore in which areas modality-agnostic patterns are formed first, and how they are being transformed when spreading to higher-level areas of the brain (<xref ref-type="bibr" rid="c29">Dirani and Pylkkänen, 2024</xref>; <xref ref-type="bibr" rid="c11">Benchetrit et al., 2024</xref>).</p>
<p>In line with the GRAPES framework, (<xref ref-type="bibr" rid="c74">Martin, 2009</xref>, <xref ref-type="bibr" rid="c75">2016</xref>), we found that modality-agnostic representations are distributed across temporal and parietal areas. To test the related hypothesis that conceptual information is organized in domains, we plan to use RSA to understand which kind of semantic information is represented in the different modality-agnostic regions identified.</p>
<p>Finally, our results can be interpreted with respect to the Global Workspace Theory. All modality-agnostic regions are good candidate regions for a global workspace. They could, however, also be part of modality-specific modules that get activated in a modality-agnostic fashion through a “broadcast” operation as a stimulus is perceived consciously (<xref ref-type="bibr" rid="c6">Baars, 1993</xref>, <xref ref-type="bibr" rid="c7">2005</xref>). To distinguish these two cases, an experimental manipulation of attention could be used: according to Global Workspace Theory, attention is required for information to enter the workspace, but not for the workspace signals to reach other brain regions via broadcast. In the future, we plan to investigate how modality-agnostic patterns are modulated by attention, by analyzing additional test sessions from the same subjects in which they were instructed in specific runs to pay attention to only one of the modalities. These sessions will be released as part of another future dataset and publication.</p>
<p>To conclude, the results from our searchlight analysis so far are in line with all major theories on modality-agnostic representations that were considered. As this dataset will be shared publicly, more targeted investigations can be performed by the research community in order to adjudicate between different theories.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>This research was funded by grants from the French Agence Nationale de la Recherche (ANR: AI-REPS grant number ANR-18-CE37-0007-01 and ANITI grant number ANR-19-PI3A-0004) as well as the European Union (ERC Advanced grant GLoW, 101096017). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.</p>
<p>We thank the Inserm/UPS UMR1214 Technical Platform for their help in setting up and for the acquisitions of the MRI sequences.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Albers</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kok</surname> <given-names>P</given-names></string-name>, <string-name><surname>Toni</surname> <given-names>I</given-names></string-name>, <string-name><surname>Dijkerman</surname> <given-names>HC</given-names></string-name>, <string-name><surname>de Lange</surname> <given-names>F</given-names></string-name></person-group>. <article-title>Shared Representations for Working Memory and Mental Imagery in Early Visual Cortex</article-title>. <source>Current Biology</source>. <year>2013</year> Aug; <volume>23</volume>(<issue>15</issue>):<fpage>1427</fpage>–<lpage>1431</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0960982213006908">https://linkinghub.elsevier.com/retrieve/pii/S0960982213006908</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cub.2013.05.065</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aliko</surname> <given-names>S</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Gheorghiu</surname> <given-names>F</given-names></string-name>, <string-name><surname>Meliss</surname> <given-names>S</given-names></string-name>, <string-name><surname>Skipper</surname> <given-names>JI</given-names></string-name></person-group>. <article-title>A naturalistic neuroimaging database for understanding the brain using ecological stimuli</article-title>. <source>Scientific Data</source>. <year>2020</year> Oct; <volume>7</volume>(<issue>1</issue>):<fpage>347</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41597-020-00680-2</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Allen</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>St-Yves</surname> <given-names>G</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Breedlove</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Prince</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Dowdle</surname> <given-names>LT</given-names></string-name>, <string-name><surname>Nau</surname> <given-names>M</given-names></string-name>, <string-name><surname>Caron</surname> <given-names>B</given-names></string-name>, <string-name><surname>Pestilli</surname> <given-names>F</given-names></string-name>, <string-name><surname>Charest</surname> <given-names>I</given-names></string-name>, <string-name><surname>Hutchinson</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Naselaris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Kay</surname> <given-names>K</given-names></string-name></person-group>. <article-title>A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence</article-title>. <source>Nature Neuroscience</source>. <year>2022</year>; <volume>25</volume>(<issue>1</issue>):<fpage>116</fpage>–<lpage>126</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41593-021-00962-x</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andrews</surname> <given-names>M</given-names></string-name>, <string-name><surname>Frank</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vigliocco</surname> <given-names>G</given-names></string-name></person-group>. <article-title>Reconciling Embodied and Distributional Accounts of Meaning in Language</article-title>. <source>Topics in Cognitive Science</source>. <year>2014</year> Jul; <volume>6</volume>(<issue>3</issue>):<fpage>359</fpage>–<lpage>370</lpage>. <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/10.1111/tops.12096">https://onlinelibrary.wiley.com/doi/10.1111/tops.12096</ext-link>, doi: <pub-id pub-id-type="doi">10.1111/tops.12096</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ashburner</surname> <given-names>J</given-names></string-name>, <string-name><surname>Barnes</surname> <given-names>G</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>CC</given-names></string-name>, <string-name><surname>Daunizeau</surname> <given-names>J</given-names></string-name>, <string-name><surname>Flandin</surname> <given-names>G</given-names></string-name>, <string-name><surname>Friston</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kiebel</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kilner</surname> <given-names>J</given-names></string-name>, <string-name><surname>Litvak</surname> <given-names>V</given-names></string-name>, <string-name><surname>Moran</surname> <given-names>R</given-names></string-name></person-group>. <source>SPM12. Wellcome Trust Centre for Neuroimaging</source>. <year>2014</year>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Baars</surname> <given-names>BJ</given-names></string-name></person-group>. <source>A cognitive theory of consciousness</source>. <publisher-name>Cambridge University Press</publisher-name>; <year>1993</year>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baars</surname> <given-names>BJ</given-names></string-name></person-group>. <article-title>Global workspace theory of consciousness: toward a cognitive neuroscience of human experience</article-title>. <source>Progress in Brain Research</source>, vol. <volume>150</volume>, <year>2005</year>. p. <fpage>45</fpage>–<lpage>53</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S0079-6123(05)50004-9</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barsalou</surname> <given-names>LW</given-names></string-name></person-group>. <article-title>On Staying Grounded and Avoiding Quixotic Dead Ends</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>2016</year>; <volume>23</volume>(<issue>4</issue>):<fpage>1122</fpage>–<lpage>1142</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bartolomeo</surname> <given-names>P</given-names></string-name></person-group>. <article-title>The Relationship Between Visual Perception and Visual Mental Imagery: A Reappraisal of the Neuropsychological Evidence</article-title>. <source>Cortex</source>. <year>2002</year> Jan; <volume>38</volume>(<issue>3</issue>):<fpage>357</fpage>–<lpage>378</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0010945208706658">https://linkinghub.elsevier.com/retrieve/pii/S0010945208706658</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/S0010-9452(08)70665-8</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Beliy</surname> <given-names>R</given-names></string-name>, <string-name><surname>Gaziv</surname> <given-names>G</given-names></string-name>, <string-name><surname>Hoogi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Strappini</surname> <given-names>F</given-names></string-name>, <string-name><surname>Golan</surname> <given-names>T</given-names></string-name>, <string-name><surname>Irani</surname> <given-names>M</given-names></string-name></person-group>. <source>From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI</source>. <publisher-loc>In</publisher-loc>: <publisher-name>NeurIPS</publisher-name>; <year>2019</year>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Benchetrit</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Banville</surname> <given-names>H</given-names></string-name>, <string-name><surname>King</surname> <given-names>JR</given-names></string-name></person-group> <article-title>Brain decoding: toward real-time reconstruction of visual perception</article-title>. In: <conf-name>The Twelfth International Conference on Learning Representations</conf-name>; <year>2024</year>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Binder</surname> <given-names>JR</given-names></string-name></person-group>. <article-title>In defense of abstract conceptual representations</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>2016</year> Aug; <volume>23</volume>(<issue>4</issue>):<fpage>1096</fpage>–<lpage>1108</lpage>. <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.3758/s13423-015-0909-1">http://link.springer.com/10.3758/s13423-015-0909-1</ext-link>, doi: <pub-id pub-id-type="doi">10.3758/s13423-015-0909-1</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Binder</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Desai</surname> <given-names>RH</given-names></string-name>, <string-name><surname>Graves</surname> <given-names>WW</given-names></string-name>, <string-name><surname>Conant</surname> <given-names>LL</given-names></string-name></person-group>. <article-title>Where Is the Semantic System? A Critical Review and Meta-Analysis of 120 Functional Neuroimaging Studies</article-title>. <source>Cerebral Cortex</source>. <year>2009</year>; <volume>19</volume>(<issue>12</issue>):<fpage>2767</fpage>–<lpage>2796</lpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boccia</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sulpizio</surname> <given-names>V</given-names></string-name>, <string-name><surname>Teghil</surname> <given-names>A</given-names></string-name>, <string-name><surname>Palermo</surname> <given-names>L</given-names></string-name>, <string-name><surname>Piccardi</surname> <given-names>L</given-names></string-name>, <string-name><surname>Galati</surname> <given-names>G</given-names></string-name>, <string-name><surname>Guariglia</surname> <given-names>C</given-names></string-name></person-group>. <article-title>The dynamic contribution of the high-level visual cortex to imagery and perception</article-title>. <source>Human Brain Mapping</source>. <year>2019</year> Jan; <volume>40</volume>(<issue>8</issue>):<fpage>2449</fpage>–<lpage>2463</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6865452/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6865452/</ext-link>, doi: <pub-id pub-id-type="doi">10.1002/hbm.24535</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Boyle</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Pinsard</surname> <given-names>B</given-names></string-name>, <string-name><surname>Boukhdhir</surname> <given-names>A</given-names></string-name>, <string-name><surname>Belleville</surname> <given-names>S</given-names></string-name>, <string-name><surname>Brambatti</surname> <given-names>S</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Cohen-Adad</surname> <given-names>J</given-names></string-name>, <string-name><surname>Cyr</surname> <given-names>A</given-names></string-name>, <string-name><surname>Fuente Rainville</surname> <given-names>P</given-names></string-name>, <string-name><surname>Bellec</surname> <given-names>P</given-names></string-name></person-group>. <article-title>The Courtois project on neuronal modelling-first data release</article-title>. In: <conf-name>26th annual meeting of the organization for human brain mapping</conf-name>; <year>2020</year>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Brennan</surname> <given-names>JR</given-names></string-name></person-group>. <source>Language and the brain: a slim guide to neurolinguistics</source>. <publisher-name>Oxford University Press</publisher-name>; <year>2022</year>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brennan</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Hale</surname> <given-names>JT</given-names></string-name></person-group>. <article-title>Hierarchical structure guides rapid linguistic predictions during naturalistic listening</article-title>. <source>PLOS One</source>. <year>2019</year> Jan; <volume>14</volume>(<issue>1</issue>):<fpage>e0207741</fpage>. <ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0207741">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0207741</ext-link>, doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0207741</pub-id>, publisher: Public Library of Science.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bright</surname> <given-names>P</given-names></string-name>, <string-name><surname>Moss</surname> <given-names>H</given-names></string-name>, <string-name><surname>Tyler</surname> <given-names>LK</given-names></string-name></person-group>. <article-title>Unitary vs multiple semantics: PET studies of word and picture processing</article-title>. <source>Brain and Language</source>. <year>2004</year>; <volume>89</volume>(<issue>3</issue>):<fpage>417</fpage>–<lpage>432</lpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carota</surname> <given-names>F</given-names></string-name>, <string-name><surname>Nili</surname> <given-names>H</given-names></string-name>, <string-name><surname>Pulvermüller</surname> <given-names>F</given-names></string-name>, <string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name></person-group>. <article-title>Distinct fronto-temporal substrates of distributional and taxonomic similarity among words: evidence from RSA of BOLD signals</article-title>. <source>NeuroImage</source>. <year>2021</year>; <volume>224</volume>:<fpage>117408</fpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname> <given-names>N</given-names></string-name>, <string-name><surname>Pyles</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Marcus</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gupta</surname> <given-names>A</given-names></string-name>, <string-name><surname>Tarr</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Aminoff</surname> <given-names>EM.</given-names></string-name></person-group> <article-title>BOLD5000, a public fMRI dataset while viewing 5000 visual images</article-title>. <source>Scientific Data</source>. <year>2019</year>; <volume>6</volume>(<issue>1</issue>):<fpage>49</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41597-019-0052-3</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cox</surname> <given-names>DD</given-names></string-name>, <string-name><surname>Savoy</surname> <given-names>RL</given-names></string-name></person-group>. <article-title>Functional magnetic resonance imaging (fMRI) “brain reading”: detecting and classifying distributed patterns of fMRI activity in human visual cortex</article-title>. <source>NeuroImage</source>. <year>2003</year> Jun; <volume>19</volume>(<issue>2</issue>):<fpage>261</fpage>–<lpage>270</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1053811903000491">https://www.sciencedirect.com/science/article/pii/S1053811903000491</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/S1053-8119(03)00049-1</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Damasio</surname> <given-names>AR</given-names></string-name></person-group>. <article-title>The Brain Binds Entities and Events by Multiregional Activation from Convergence Zones</article-title>. <source>Neural Computation</source>. <year>1989</year>; <volume>1</volume>(<issue>1</issue>):<fpage>123</fpage>–<lpage>132</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Damasio</surname> <given-names>H</given-names></string-name>, <string-name><surname>Tranel</surname> <given-names>D</given-names></string-name>, <string-name><surname>Grabowski</surname> <given-names>T</given-names></string-name>, <string-name><surname>Adolphs</surname> <given-names>R</given-names></string-name>, <string-name><surname>Damasio</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Neural systems behind word and concept retrieval</article-title>. <source>Cognition</source>. <year>2004</year> May; <volume>92</volume>(<issue>1-2</issue>):<fpage>179</fpage>–<lpage>229</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0010027703002312">https://linkinghub.elsevier.com/retrieve/pii/S0010027703002312</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cognition.2002.07.001</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desikan</surname> <given-names>RS</given-names></string-name>, <string-name><surname>Ségonne</surname> <given-names>F</given-names></string-name>, <string-name><surname>Fischl</surname> <given-names>B</given-names></string-name>, <string-name><surname>Quinn</surname> <given-names>BT</given-names></string-name>, <string-name><surname>Dickerson</surname> <given-names>BC</given-names></string-name>, <string-name><surname>Blacker</surname> <given-names>D</given-names></string-name>, <string-name><surname>Buckner</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Dale</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Maguire</surname> <given-names>RP</given-names></string-name>, <string-name><surname>Hyman</surname> <given-names>BT</given-names></string-name>, <string-name><surname>Albert</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Killiany</surname> <given-names>RJ</given-names></string-name></person-group>. <article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title>. <source>NeuroImage</source>. <year>2006</year>; <volume>31</volume>(<issue>3</issue>):<fpage>968</fpage>–<lpage>980</lpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Devereux</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Clarke</surname> <given-names>A</given-names></string-name>, <string-name><surname>Marouchos</surname> <given-names>A</given-names></string-name>, <string-name><surname>Tyler</surname> <given-names>LK</given-names></string-name></person-group>. <article-title>Representational Similarity Analysis Reveals Commonalities and Differences in the Semantic Processing of Words and Objects</article-title>. <source>The Journal of Neuroscience</source>. <year>2013</year>; <volume>33</volume>(<issue>48</issue>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Devlin</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>K</given-names></string-name>, <string-name><surname>Toutanova</surname> <given-names>K</given-names></string-name></person-group>. <article-title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</article-title>. <conf-name>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</conf-name> <publisher-loc>Minneapolis, Minnesota</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>; <year>2019</year>. p. <fpage>4171</fpage>–<lpage>4186</lpage>. <ext-link ext-link-type="uri" xlink:href="https://aclanthology.org/N19-1423">https://aclanthology.org/N19-1423</ext-link>, doi: <pub-id pub-id-type="doi">10.18653/v1/N19-1423</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Devlin</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Russell</surname> <given-names>RP</given-names></string-name>, <string-name><surname>Davis</surname> <given-names>MH</given-names></string-name>, <string-name><surname>Price</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>J</given-names></string-name>, <string-name><surname>Moss</surname> <given-names>HE</given-names></string-name>, <string-name><surname>Matthews</surname> <given-names>PM</given-names></string-name>, <string-name><surname>Tyler</surname> <given-names>LK</given-names></string-name></person-group>. <article-title>Susceptibility-Induced Loss of Signal: Comparing PET and fMRI on a Semantic Task</article-title>. <source>NeuroImage</source>. <year>2000</year> Jun; <volume>11</volume>(<issue>6</issue>):<fpage>589</fpage>–<lpage>600</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1053811900905950">https://linkinghub.elsevier.com/retrieve/pii/S1053811900905950</ext-link>, doi: <pub-id pub-id-type="doi">10.1006/nimg.2000.0595</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dijkstra</surname> <given-names>N</given-names></string-name>, <string-name><surname>Bosch</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Van Gerven</surname> <given-names>MAJ</given-names></string-name></person-group>. <article-title>Shared Neural Mechanisms of Visual Perception and Imagery</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2019</year> May; <volume>23</volume>(<issue>5</issue>):<fpage>423</fpage>–<lpage>434</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1364661319300592">https://linkinghub.elsevier.com/retrieve/pii/S1364661319300592</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.tics.2019.02.004</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dirani</surname> <given-names>J</given-names></string-name>, <string-name><surname>Pylkkänen</surname> <given-names>L</given-names></string-name></person-group>. <article-title>MEG Evidence That Modality-Independent Conceptual Representations Contain Semantic and Visual Features</article-title>. <source>Journal of Neuroscience</source>. <year>2024</year> Jul; <volume>44</volume>(<issue>27</issue>). <ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/content/44/27/e0326242024">https://www.jneurosci.org/content/44/27/e0326242024</ext-link>, doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0326-24.2024</pub-id>, publisher: Society for Neuroscience Section: Research Articles.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Dosovitskiy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Beyer</surname> <given-names>L</given-names></string-name>, <string-name><surname>Kolesnikov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Weissenborn</surname> <given-names>D</given-names></string-name>, <string-name><surname>Zhai</surname> <given-names>X</given-names></string-name>, <string-name><surname>Unterthiner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Dehghani</surname> <given-names>M</given-names></string-name>, <string-name><surname>Minderer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Heigold</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gelly</surname> <given-names>S</given-names></string-name>, <string-name><surname>Uszkoreit</surname> <given-names>J</given-names></string-name>, <string-name><surname>Houlsby</surname> <given-names>N</given-names></string-name></person-group>. <article-title>An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale</article-title>. In: <conf-name>International Conference on Learning Representations</conf-name>; <year>2020</year>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Embleton</surname> <given-names>KV</given-names></string-name>, <string-name><surname>Haroon</surname> <given-names>HA</given-names></string-name>, <string-name><surname>Morris</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Ralph</surname> <given-names>MAL</given-names></string-name>, <string-name><surname>Parker</surname> <given-names>GJM</given-names></string-name></person-group>. <article-title>Distortion correction for diffusion-weighted MRI tractography and fMRI in the temporal lobes</article-title>. <source>Human Brain Mapping</source>. <year>2010</year>; <volume>31</volume>(<issue>10</issue>):<fpage>1570</fpage>– <lpage>1587</lpage>. <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.20959">https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.20959</ext-link>, doi: <pub-id pub-id-type="doi">10.1002/hbm.20959</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fairhall</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Caramazza</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Brain Regions That Represent Amodal Conceptual Knowledge</article-title>. <source>Journal of Neuroscience</source>. <year>2013</year>; <volume>33</volume>(<issue>25</issue>):<fpage>10552</fpage>–<lpage>10558</lpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Farah</surname> <given-names>MJ</given-names></string-name></person-group>. <article-title>The neurological basis of mental imagery: A componential analysis</article-title>. <source>Cognition</source>. <year>1984</year> Dec; <volume>18</volume>(<issue>1-3</issue>):<fpage>245</fpage>–<lpage>272</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/001002778490026X">https://linkinghub.elsevier.com/retrieve/pii/001002778490026X</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/0010-0277(84)90026-X</pub-id>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fedorenko</surname> <given-names>E</given-names></string-name>, <string-name><surname>Behr</surname> <given-names>MK</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name></person-group>. <article-title>Functional specificity for high-level linguistic processing in the human brain</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2011</year> Sep; <volume>108</volume>(<issue>39</issue>):<fpage>16428</fpage>–<lpage>16433</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/10.1073/pnas.1112937108">https://www.pnas.org/doi/10.1073/pnas.1112937108</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.1112937108</pub-id>, publisher: Proceedings of the National Academy of Sciences.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fedorenko</surname> <given-names>E</given-names></string-name>, <string-name><surname>Hsieh</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Nieto-Castañón</surname> <given-names>A</given-names></string-name>, <string-name><surname>Whitfield-Gabrieli</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name></person-group>. <article-title>New Method for fMRI Investigations of Language: Defining ROIs Functionally in Individual Subjects</article-title>. <source>Journal of Neurophysiology</source>. <year>2010</year> Aug; <volume>104</volume>(<issue>2</issue>):<fpage>1177</fpage>–<lpage>1194</lpage>. doi: <pub-id pub-id-type="doi">10.1152/jn.00032.2010</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Felleman</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Van Essen</surname> <given-names>DC</given-names></string-name></person-group>. <article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title>, <source>Cerebral cortex</source>. <year>1991</year> Jan; <volume>1</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>47</lpage>. doi: <pub-id pub-id-type="doi">10.1093/cercor/1.1.1-a</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fernandino</surname> <given-names>L</given-names></string-name>, <string-name><surname>Binder</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Desai</surname> <given-names>RH</given-names></string-name>, <string-name><surname>Pendl</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Humphries</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>WL</given-names></string-name>, <string-name><surname>Conant</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Seidenberg</surname> <given-names>MS</given-names></string-name></person-group>. <article-title>Concept Representation Reflects Multimodal Abstraction: A Framework for Embodied Semantics</article-title>. <source>Cerebral Cortex</source>. <year>2016</year>; <volume>26</volume>(<issue>5</issue>):<fpage>2018</fpage>–<lpage>2034</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fernandino</surname> <given-names>L</given-names></string-name>, <string-name><surname>Tong</surname> <given-names>JQ</given-names></string-name>, <string-name><surname>Conant</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Humphries</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Binder</surname> <given-names>JR</given-names></string-name></person-group>. <article-title>Decoding the information structure underlying the neural representation of concepts</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2022</year>; <volume>119</volume>(<issue>6</issue>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fischl</surname> <given-names>B.</given-names></string-name></person-group> <article-title>FreeSurfer</article-title>. <source>NeuroImage</source>. <year>2012</year>; <volume>62</volume>(<issue>2</issue>):<fpage>774</fpage>–<lpage>781</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id>, place: Netherlands Publisher: Elsevier Science.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friederici</surname> <given-names>AD</given-names></string-name></person-group>. <article-title>Language in Our Brain: The Origins of a Uniquely Human Capacity</article-title>. <source>The MIT Press</source>; <year>2017</year>. <ext-link ext-link-type="uri" xlink:href="https://direct.mit.edu/books/oa-monograph/3653/Language-in-Our-BrainThe-Origins-of-a-Uniquely">https://direct.mit.edu/books/oa-monograph/3653/Language-in-Our-BrainThe-Origins-of-a-Uniquely</ext-link>, doi: <pub-id pub-id-type="doi">10.7551/mitpress/11173.001.0001</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frisby</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Halai</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Cox</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Lambon Ralph</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Rogers</surname> <given-names>TT</given-names></string-name></person-group>. <article-title>Decoding semantic representations in mind and brain</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2023</year> Mar; <volume>27</volume>(<issue>3</issue>):<fpage>258</fpage>–<lpage>281</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1364661322003230">https://linkinghub.elsevier.com/retrieve/pii/S1364661322003230</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.tics.2022.12.006</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gainotti</surname> <given-names>G</given-names></string-name></person-group>. <article-title>What the locus of brain lesion tells us about the nature of the cognitive defect underlying category-specific disorders: a review</article-title>. <source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source>. <year>2000</year> Sep; <volume>36</volume>(<issue>4</issue>):<fpage>539</fpage>–<lpage>559</lpage>. doi: <pub-id pub-id-type="doi">10.1016/s0010-9452(08)70537-9</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Girdhar</surname> <given-names>R</given-names></string-name>, <string-name><surname>El-Nouby</surname> <given-names>A</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Singh</surname> <given-names>M</given-names></string-name>, <string-name><surname>Alwala</surname> <given-names>KV</given-names></string-name>, <string-name><surname>Joulin</surname> <given-names>A</given-names></string-name>, <string-name><surname>Misra</surname> <given-names>I</given-names></string-name></person-group>, <article-title>ImageBind: One Embedding Space To Bind Them All</article-title>. In: <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>; <year>2023</year>. p. <fpage>15180</fpage>–<lpage>15190</lpage>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gorgolewski</surname> <given-names>K</given-names></string-name>, <string-name><surname>Burns</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Madison</surname> <given-names>C</given-names></string-name>, <string-name><surname>Clark</surname> <given-names>D</given-names></string-name>, <string-name><surname>Halchenko</surname> <given-names>YO</given-names></string-name>, <string-name><surname>Waskom</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Ghosh</surname> <given-names>SS</given-names></string-name></person-group>. <article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in python</article-title>. <source>Frontiers in neuroinformatics</source>. <year>2011</year>; <volume>5</volume>:<fpage>12318</fpage>. Publisher: Frontiers.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grill-Spector</surname> <given-names>K</given-names></string-name>, <string-name><surname>Malach</surname> <given-names>R</given-names></string-name></person-group>. <article-title>The Human Visual Cortex</article-title>. <source>Annual Review of Neuroscience</source>. <year>2004</year> Jul; <volume>27</volume>(<issue>1</issue>):<fpage>649</fpage>–<lpage>677</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.annualreviews.org/doi/10.1146/annurev.neuro.27.070203.144220">https://www.annualreviews.org/doi/10.1146/annurev.neuro.27.070203.144220</ext-link>, doi: <pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144220</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hajhajate</surname> <given-names>D</given-names></string-name>, <string-name><surname>Kaufmann</surname> <given-names>BC</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Siuda-Krzywicka</surname> <given-names>K</given-names></string-name>, <string-name><surname>Bartolomeo</surname> <given-names>P</given-names></string-name></person-group>. <article-title>The connectional anatomy of visual mental imagery: evidence from a patient with left occipito-temporal damage</article-title>. <source>Brain Structure &amp; Function</source>. <year>2022</year> Dec; <volume>227</volume>(<issue>9</issue>):<fpage>3075</fpage>–<lpage>3083</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s00429-022-02505-x</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Handjaras</surname> <given-names>G</given-names></string-name>, <string-name><surname>Ricciardi</surname> <given-names>E</given-names></string-name>, <string-name><surname>Leo</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lenci</surname> <given-names>A</given-names></string-name>, <string-name><surname>Cecchetti</surname> <given-names>L</given-names></string-name>, <string-name><surname>Cosottini</surname> <given-names>M</given-names></string-name>, <string-name><surname>Marotta</surname> <given-names>G</given-names></string-name>, <string-name><surname>Pietrini</surname> <given-names>P</given-names></string-name></person-group>. <article-title>How concepts are encoded in the human brain: A modality independent, category-based cortical organization of semantic knowledge</article-title>. <source>NeuroImage</source>. <year>2016</year> Jul; <volume>135</volume>:<fpage>232</fpage>–<lpage>242</lpage>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haxby</surname> <given-names>JV</given-names></string-name>, <string-name><surname>Gobbini</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Furey</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Ishai</surname> <given-names>A</given-names></string-name>, <string-name><surname>Schouten</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Pietrini</surname> <given-names>P</given-names></string-name></person-group>. <article-title>Distributed and Overlapping Representations of Faces and Objects in Ventral Temporal Cortex</article-title>. <source>Science</source>. <year>2001</year> Sep; <volume>293</volume>(<issue>5539</issue>):<fpage>2425</fpage>–<lpage>2430</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/10.1126/science.1063736">https://www.science.org/doi/10.1126/science.1063736</ext-link>, doi: <pub-id pub-id-type="doi">10.1126/science.1063736</pub-id>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>He</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Ren</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sun</surname> <given-names>J</given-names></string-name></person-group>. <article-title>Deep Residual Learning for Image Recognition</article-title>. In: <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>; <year>2016</year>. p. <fpage>770</fpage>–<lpage>778</lpage>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huth</surname> <given-names>AG</given-names></string-name>, <string-name><surname>Nishimoto</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vu</surname> <given-names>AT</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name></person-group>. <article-title>A continuous semantic space describes the representation of thousands of object and action categories across the human brain</article-title>. <source>Neuron</source>. <year>2012</year> Dec; <volume>76</volume>(<issue>6</issue>):<fpage>1210</fpage>–<lpage>1224</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3556488/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3556488/</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.014</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jefferies</surname> <given-names>E</given-names></string-name>, <string-name><surname>Patterson</surname> <given-names>K</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>RW</given-names></string-name>, <string-name><surname>Ralph</surname> <given-names>MAL</given-names></string-name></person-group>. <article-title>Comprehension of concrete and abstract words in semantic dementia</article-title>. <source>Neuropsychology</source>. <year>2009</year> Jul; <volume>23</volume>(<issue>4</issue>):<fpage>492</fpage>. <pub-id pub-id-type="pmcid">PMC2801065</pub-id>, doi: <pub-id pub-id-type="doi">10.1037/a0015452</pub-id>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jenkinson</surname> <given-names>M</given-names></string-name>, <string-name><surname>Beckmann</surname> <given-names>CF</given-names></string-name>, <string-name><surname>Behrens</surname> <given-names>TEJ</given-names></string-name>, <string-name><surname>Woolrich</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>SM.</given-names></string-name></person-group> <article-title>FSL</article-title>. <source>NeuroImage</source>. <year>2012</year> Aug; <volume>62</volume>(<issue>2</issue>):<fpage>782</fpage>–<lpage>790</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1053811911010603">https://www.sciencedirect.com/science/article/pii/S1053811911010603</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.015</pub-id>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Jiang</surname> <given-names>AQ</given-names></string-name>, <string-name><surname>Sablayrolles</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mensch</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bamford</surname> <given-names>C</given-names></string-name>, <string-name><surname>Chaplot</surname> <given-names>DS</given-names></string-name>, <string-name><given-names>Casas</given-names> <surname>Ddl</surname></string-name>, <string-name><surname>Bressand</surname> <given-names>F</given-names></string-name>, <string-name><surname>Lengyel</surname> <given-names>G</given-names></string-name>, <string-name><surname>Lample</surname> <given-names>G</given-names></string-name>, <string-name><surname>Saulnier</surname> <given-names>L</given-names></string-name>, <string-name><surname>Lavaud</surname> <given-names>LR</given-names></string-name>, <string-name><surname>Lachaux</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Stock</surname> <given-names>P</given-names></string-name>, <string-name><surname>Scao</surname> <given-names>TL</given-names></string-name>, <string-name><surname>Lavril</surname> <given-names>T</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>T</given-names></string-name>, <string-name><surname>Lacroix</surname> <given-names>T</given-names></string-name>, <string-name><surname>Sayed</surname> <given-names>WE</given-names></string-name>, <collab>Mistral 7B</collab></person-group>. <article-title>Mistral 7B</article-title>, <source>arXiv</source>; <year>2023</year>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.2310.06825</pub-id>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>MK</given-names></string-name></person-group>. <article-title>Decoding individual natural scene representations during perception and imagery</article-title>. <source>Frontiers in Human Neuroscience</source>. <year>2014</year> Feb; <volume>8</volume>. doi: <pub-id pub-id-type="doi">10.3389/fnhum.2014.00059</pub-id>, publisher: Frontiers.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jung</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Larsen</surname> <given-names>B</given-names></string-name>, <string-name><surname>Walther</surname> <given-names>DB</given-names></string-name></person-group>. <article-title>Modality-Independent Coding of Scene Categories in Prefrontal Cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2018</year> Jun; <volume>38</volume>(<issue>26</issue>):<fpage>5969</fpage>–<lpage>5981</lpage>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name></person-group>. <article-title>Functional specificity in the human brain: A window into the functional architecture of the mind</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2010</year> Jun; <volume>107</volume>(<issue>25</issue>):<fpage>11163</fpage>–<lpage>11170</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/abs/10.1073/pnas.1005062107">https://www.pnas.org/doi/abs/10.1073/pnas.1005062107</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.1005062107</pub-id>, publisher: Proceedings of the National Academy of Sciences.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kay</surname> <given-names>KN</given-names></string-name>, <string-name><surname>Naselaris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Prenger</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name></person-group>. <article-title>Identifying natural images from human brain activity</article-title>. <source>Nature</source>. <year>2008</year> Mar; <volume>452</volume>(<issue>7185</issue>):<fpage>352</fpage>–<lpage>355</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/nature06713">https://www.nature.com/articles/nature06713</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/nature06713</pub-id>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kiefer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Pulvermüller</surname> <given-names>F</given-names></string-name></person-group>. <article-title>Conceptual representations in mind and brain: Theoretical developments, current evidence and future directions</article-title>. <source>Cortex</source>. <year>2012</year> Jul; <volume>48</volume>(<issue>7</issue>):<fpage>805</fpage>–<lpage>825</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0010945211001018">https://linkinghub.elsevier.com/retrieve/pii/S0010945211001018</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cortex.2011.04.006</pub-id>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Kim</surname> <given-names>W</given-names></string-name>, <string-name><surname>Son</surname> <given-names>B</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>I</given-names></string-name></person-group>. <article-title>ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</article-title>. In: <conf-name>Proceedings of the 38th International Conference on Machine Learning</conf-name>; <year>2021</year>. p. <fpage>5583</fpage>–<lpage>5594</lpage>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v139/kim21k.html">https://proceedings.mlr.press/v139/kim21k.html</ext-link>, </mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kosslyn</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Pascual-Leone</surname> <given-names>A</given-names></string-name>, <string-name><surname>Felician</surname> <given-names>O</given-names></string-name>, <string-name><surname>Camposano</surname> <given-names>S</given-names></string-name>, <string-name><surname>Keenan JP</surname>, <given-names>L W</given-names></string-name>, <string-name><surname>Thompson</surname>, <given-names>Ganis G</given-names></string-name>, <string-name><surname>Sukel</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Alpert</surname> <given-names>NM</given-names></string-name></person-group>. <article-title>The Role of Area 17 in Visual Imagery: Convergent Evidence from PET and rTMS</article-title>. <source>Science</source>. <year>1999</year> Apr; <volume>284</volume>(<issue>5411</issue>):<fpage>167</fpage>–<lpage>170</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/10.1126/science.284.5411.167">https://www.science.org/doi/10.1126/science.284.5411.167</ext-link>, doi: <pub-id pub-id-type="doi">10.1126/science.284.5411.167</pub-id>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kosslyn</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Thompson</surname> <given-names>WL</given-names></string-name></person-group>. <article-title>When is early visual cortex activated during visual mental imagery?</article-title> <source>Psychological Bulletin</source>. <year>2003</year>; <volume>129</volume>(<issue>5</issue>):<fpage>723</fpage>–<lpage>746</lpage>. doi: <pub-id pub-id-type="doi">10.1037/0033-2909.129.5.723</pub-id>.</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Krasnowska-Kieraś</surname> <given-names>K</given-names></string-name>, <string-name><surname>Wróblewska</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Empirical Linguistic Study of Sentence Embeddings</article-title>. <conf-name>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics Florence, Italy: Association for Computational Linguistics</conf-name>; <year>2019</year>. p. <fpage>5729</fpage>–<lpage>5739</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.aclweb.org/anthology/P19-1573">https://www.aclweb.org/anthology/P19-1573</ext-link>, doi: <pub-id pub-id-type="doi">10.18653/v1/P19-1573</pub-id>.</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name>, <string-name><surname>Mur</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bandettini</surname> <given-names>PA</given-names></string-name></person-group>. <article-title>Representational similarity analysis-connecting the branches of systems neuroscience</article-title>. <source>Frontiers in Systems Neuroscience</source>. <year>2008</year> Nov; <volume>2</volume>. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/systems-neuroscience/articles/10.3389/neuro.06.004.2008/full">https://www.frontiersin.org/journals/systems-neuroscience/articles/10.3389/neuro.06.004.2008/full</ext-link>, doi: <pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id>, publisher: Frontiers.</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lambon Ralph</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Lowe</surname> <given-names>C</given-names></string-name>, <string-name><surname>Rogers</surname> <given-names>TT</given-names></string-name></person-group>. <article-title>Neural basis of category-specific semantic deficits for living things: evidence from semantic dementia, HSVE and a neural network model</article-title>. <source>Brain</source>. <year>2006</year> Nov; <volume>130</volume>(<issue>4</issue>):<fpage>1127</fpage>–<lpage>1137</lpage>. <ext-link ext-link-type="uri" xlink:href="https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/awm025">https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/awm025</ext-link>, doi: <pub-id pub-id-type="doi">10.1093/brain/awm025</pub-id>.</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname> <given-names>SH</given-names></string-name>, <string-name><surname>Kravitz</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Baker</surname> <given-names>CI</given-names></string-name></person-group>. <article-title>Disentangling visual imagery and perception of real-world objects</article-title>. <source>NeuroImage</source>. <year>2012</year> Feb; <volume>59</volume>(<issue>4</issue>):<fpage>4064</fpage>–<lpage>4073</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1053811911012195">https://www.sciencedirect.com/science/article/pii/S1053811911012195</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.055</pub-id>.</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Li</surname> <given-names>J</given-names></string-name>, <string-name><surname>Li</surname> <given-names>D</given-names></string-name>, <string-name><surname>Savarese</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hoi</surname> <given-names>S</given-names></string-name></person-group>. <article-title>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</article-title>. In: <conf-name>International Conference on Machine Learning</conf-name>; <year>2023</year>.</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Li</surname> <given-names>LH</given-names></string-name>, <string-name><surname>Yatskar</surname> <given-names>M</given-names></string-name>, <string-name><surname>Yin</surname> <given-names>D</given-names></string-name>, <string-name><surname>Hsieh</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>KW</given-names></string-name></person-group>. <article-title>VisualBERT: A Simple and Performant Baseline for Vision and Language</article-title>. <source>arXiv:190803557 [cs]</source>. <year>2019</year> Aug; <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1908.03557">http://arxiv.org/abs/1908.03557</ext-link>, arXiv: 1908.03557.</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sprague</surname> <given-names>T</given-names></string-name>, <string-name><surname>Singh</surname> <given-names>AK</given-names></string-name></person-group>. <article-title>Mind Reader: Reconstructing complex images from brain activities</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2022</year> Dec; <volume>35</volume>:<fpage>29624</fpage>–<lpage>29636</lpage>.</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname> <given-names>TY</given-names></string-name>, <string-name><surname>Maire</surname> <given-names>M</given-names></string-name>, <string-name><surname>Belongie</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hays</surname> <given-names>J</given-names></string-name>, <string-name><surname>Perona</surname> <given-names>P</given-names></string-name>, <string-name><surname>Ramanan</surname> <given-names>D</given-names></string-name>, <string-name><surname>Dollár</surname> <given-names>P</given-names></string-name>, <string-name><surname>Zitnick</surname> <given-names>CL</given-names></string-name></person-group>. <article-title>Microsoft COCO: Common Objects in Context</article-title>. In: <source>Computer Vision – ECCV 2014</source>, vol. <volume>8693</volume>; <year>2014</year>. p. <fpage>740</fpage>–<lpage>755</lpage>.</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Spagna</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bartolomeo</surname> <given-names>P</given-names></string-name></person-group>. <article-title>Hemispheric asymmetries in visual mental imagery</article-title>. <source>Brain Structure &amp; Function</source>. <year>2022</year> Mar; <volume>227</volume>(<issue>2</issue>):<fpage>697</fpage>–<lpage>708</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s00429-021-02277-w</pub-id>.</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zhan</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hajhajate</surname> <given-names>D</given-names></string-name>, <string-name><surname>Spagna</surname> <given-names>A</given-names></string-name>, <string-name><surname>Dehaene</surname> <given-names>S</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Bartolomeo</surname> <given-names>P</given-names></string-name></person-group>. <article-title>Visual mental imagery in typical imagers and in aphantasia: A millimeter-scale 7-T fMRI study</article-title>. <source>Cortex</source>. <year>2025</year> Apr; <volume>185</volume>:<fpage>113</fpage>–<lpage>132</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0010945225000474">https://www.sciencedirect.com/science/article/pii/S0010945225000474</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cortex.2025.01.013</pub-id>.</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liuzzi</surname> <given-names>AG</given-names></string-name>, <string-name><surname>Bruffaerts</surname> <given-names>R</given-names></string-name>, <string-name><surname>Peeters</surname> <given-names>R</given-names></string-name>, <string-name><surname>Adamczuk</surname> <given-names>K</given-names></string-name>, <string-name><surname>Keuleers</surname> <given-names>E</given-names></string-name>, <string-name><surname>De Deyne</surname> <given-names>S</given-names></string-name>, <string-name><surname>Storms</surname> <given-names>G</given-names></string-name>, <string-name><surname>Dupont</surname> <given-names>P</given-names></string-name>, <string-name><surname>Vandenberghe</surname> <given-names>R</given-names></string-name></person-group>. <article-title>Cross-modal representation of spoken and written word meaning in left pars triangularis</article-title>. <source>NeuroImage</source>. <year>2017</year> Apr; <volume>150</volume>:<fpage>292</fpage>–<lpage>307</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S105381191730143X">https://linkinghub.elsevier.com/retrieve/pii/S105381191730143X</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.02.032</pub-id>.</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Man</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kaplan</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Damasio</surname> <given-names>A</given-names></string-name>, <string-name><surname>Meyer</surname> <given-names>K</given-names></string-name></person-group>. <article-title>Sight and Sound Converge to Form Modality-Invariant Representations in Temporoparietal Cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2012</year>; <volume>32</volume>(<issue>47</issue>):<fpage>16629</fpage>–<lpage>16636</lpage>.</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Martin</surname> <given-names>A</given-names></string-name></person-group>. <chapter-title>Circuits in Mind: The Neural Foundations for Object Concepts</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Gazzaniga</surname> <given-names>MS</given-names></string-name></person-group>, editor. <source>The Cognitive Neurosciences</source>, 4 ed. <publisher-name>The MIT Press</publisher-name>; <year>2009</year>. <ext-link ext-link-type="uri" xlink:href="https://direct.mit.edu/books/book/5453/chapter/3965022/Circuits-in-Mind-The-Neural-Foundations-for-Object">https://direct.mit.edu/books/book/5453/chapter/3965022/Circuits-in-Mind-The-Neural-Foundations-for-Object</ext-link>, doi: <pub-id pub-id-type="doi">10.7551/mitpress/8029.003.0091</pub-id>.</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martin</surname> <given-names>A</given-names></string-name></person-group>. <article-title>GRAPES—Grounding representations in action, perception, and emotion systems: How object properties and categories are represented in the human brain</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>2016</year>; <volume>23</volume>(<issue>4</issue>):<fpage>979</fpage>– <lpage>990</lpage>.</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martin</surname> <given-names>CB</given-names></string-name>, <string-name><surname>Douglas</surname> <given-names>D</given-names></string-name>, <string-name><surname>Newsome</surname> <given-names>RN</given-names></string-name>, <string-name><surname>Man</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Barense</surname> <given-names>MD</given-names></string-name></person-group>. <article-title>Integrative and distinctive coding of visual and conceptual object features in the ventral visual stream</article-title>. <source>eLife</source>. <year>2018</year>; <volume>7</volume>.</mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Meschke</surname> <given-names>E</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>J</given-names></string-name></person-group>. <source>Mapping Multimodal Conceptual Representations within the Lexical-Semantic Brain System</source>. In: <publisher-name>CCN</publisher-name>; <year>2024</year>.</mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meyer</surname> <given-names>K</given-names></string-name>, <string-name><surname>Damasio</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Convergence and divergence in a neural architecture for recognition and memory</article-title>. <source>Trends in Neurosciences</source>. <year>2009</year> Jul; <volume>32</volume>(<issue>7</issue>):<fpage>376</fpage>–<lpage>382</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.cell.com/trends/neurosciences/abstract/S0166-2236(09)00090-3">https://www.cell.com/trends/neurosciences/abstract/S0166-2236(09)00090-3</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.tins.2009.04.002</pub-id>, publisher: Elsevier.</mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moore</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Price</surname> <given-names>CJ</given-names></string-name></person-group>. <article-title>Three Distinct Ventral Occipitotemporal Regions for Reading and Object Naming</article-title>. <source>NeuroImage</source>. <year>1999</year>; <volume>10</volume>(<issue>2</issue>):<fpage>181</fpage>–<lpage>192</lpage>.</mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Naselaris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Kay</surname> <given-names>KN</given-names></string-name>, <string-name><surname>Nishimoto</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL.</given-names></string-name></person-group> <article-title>Encoding and decoding in fMRI</article-title>. <source>NeuroImage</source>. <year>2011</year> May; <volume>56</volume>(<issue>2</issue>):<fpage>400</fpage>–<lpage>410</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1053811910010657">https://www.sciencedirect.com/science/article/pii/S1053811910010657</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.073</pub-id>.</mixed-citation></ref>
<ref id="c81"><label>81.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Naselaris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Olman</surname> <given-names>CA</given-names></string-name>, <string-name><surname>Stansbury</surname> <given-names>DE</given-names></string-name>, <string-name><surname>Ugurbil</surname> <given-names>K</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name></person-group>. <article-title>A voxel-wise encoding model for early visual areas decodes mental images of remembered scenes</article-title>. <source>NeuroImage</source>. <year>2015</year> Jan; <volume>105</volume>:<fpage>215</fpage>–<lpage>228</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1053811914008428">https://www.sciencedirect.com/science/article/pii/S1053811914008428</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.10.018</pub-id>.</mixed-citation></ref>
<ref id="c82"><label>82.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Naselaris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Prenger</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Kay</surname> <given-names>KN</given-names></string-name>, <string-name><surname>Oliver</surname> <given-names>M</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name></person-group>. <article-title>Bayesian Reconstruction of Natural Images from Human Brain Activity</article-title>. <source>Neuron</source>. <year>2009</year> Sep; <volume>63</volume>(<issue>6</issue>):<fpage>902</fpage>–<lpage>915</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0896627309006850">https://linkinghub.elsevier.com/retrieve/pii/S0896627309006850</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2009.09.006</pub-id>.</mixed-citation></ref>
<ref id="c83"><label>83.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nastase</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>YF</given-names></string-name>, <string-name><surname>Hillman</surname> <given-names>H</given-names></string-name>, <string-name><surname>Zadbood</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hasenfratz</surname> <given-names>L</given-names></string-name>, <string-name><surname>Keshavarzian</surname> <given-names>N</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Honey</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Yeshurun</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Regev</surname> <given-names>M</given-names></string-name>, <string-name><surname>Nguyen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>CHC</given-names></string-name>, <string-name><surname>Baldassano</surname> <given-names>C</given-names></string-name>, <string-name><surname>Lositsky</surname> <given-names>O</given-names></string-name>, <string-name><surname>Simony</surname> <given-names>E</given-names></string-name>, <string-name><surname>Chow</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Leong</surname> <given-names>YC</given-names></string-name>, <string-name><surname>Brooks</surname> <given-names>PP</given-names></string-name>, <string-name><surname>Micciche</surname> <given-names>E</given-names></string-name>, <string-name><surname>Choe</surname> <given-names>G</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>The “Narratives” fMRI dataset for evaluating models of naturalistic language comprehension</article-title>. <source>Scientific Data</source>. <year>2021</year> Sep; <volume>8</volume>(<issue>1</issue>):<fpage>250</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41597-021-01033-3">https://www.nature.com/articles/s41597-021-01033-3</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41597-021-01033-3</pub-id>.</mixed-citation></ref>
<ref id="c84"><label>84.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nishimoto</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vu</surname> <given-names>AT</given-names></string-name>, <string-name><surname>Naselaris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Benjamini</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>B</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name></person-group>. <article-title>Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies</article-title>. <source>Current Biology</source>. <year>2011</year> Oct; <volume>21</volume>(<issue>19</issue>):<fpage>1641</fpage>–<lpage>1646</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.cell.com/current-biology/abstract/S0960-9822(11)00937-7">https://www.cell.com/current-biology/abstract/S0960-9822(11)00937-7</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cub.2011.08.031</pub-id>, publisher: Elsevier.</mixed-citation></ref>
<ref id="c85"><label>85.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Craven</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name></person-group>. <article-title>Mental Imagery of Faces and Places Activates Corresponding Stimulus-Specific Brain Regions</article-title>. <source>Journal of Cognitive Neuroscience</source>. <year>2000</year> Nov; <volume>12</volume>(<issue>6</issue>):<fpage>1013</fpage>–<lpage>1023</lpage>. <ext-link ext-link-type="uri" xlink:href="https://direct.mit.edu/jocn/article/12/6/1013/3493/Mental-Imagery-of-Faces-and-Places-Activates">https://direct.mit.edu/jocn/article/12/6/1013/3493/Mental-Imagery-of-Faces-and-Places-Activates</ext-link>, doi: <pub-id pub-id-type="doi">10.1162/08989290051137549</pub-id>.</mixed-citation></ref>
<ref id="c86"><label>86.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Visconti di Oleggio Castello</surname> <given-names>M</given-names></string-name>, <string-name><surname>Chauhan</surname> <given-names>V</given-names></string-name>, <string-name><surname>Jiahui</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gobbini</surname> <given-names>MI</given-names></string-name></person-group>. <article-title>An fMRI dataset in response to “The Grand Budapest Hotel”, a socially-rich, naturalistic movie</article-title>. <source>Scientific Data</source>. <year>2020</year>; <volume>7</volume>(<issue>1</issue>):<fpage>383</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41597-020-00735-4</pub-id>.</mixed-citation></ref>
<ref id="c87"><label>87.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Oquab</surname> <given-names>M</given-names></string-name>, <string-name><surname>Darcet</surname> <given-names>T</given-names></string-name>, <string-name><surname>Moutakanni</surname> <given-names>T</given-names></string-name>, <string-name><surname>Vo</surname> <given-names>H</given-names></string-name>, <string-name><surname>Szafraniec</surname> <given-names>M</given-names></string-name>, <string-name><surname>Khalidov</surname> <given-names>V</given-names></string-name>, <string-name><surname>Fernandez</surname> <given-names>P</given-names></string-name>, <string-name><surname>Haziza</surname> <given-names>D</given-names></string-name>, <string-name><surname>Massa</surname> <given-names>F</given-names></string-name>, <string-name><surname>El-Nouby</surname> <given-names>A</given-names></string-name>, <string-name><surname>Assran</surname> <given-names>M</given-names></string-name>, <string-name><surname>Ballas</surname> <given-names>N</given-names></string-name>, <string-name><surname>Galuba</surname> <given-names>W</given-names></string-name>, <string-name><surname>Howes</surname> <given-names>R</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>PY</given-names></string-name>, <string-name><surname>Li</surname> <given-names>SW</given-names></string-name>, <string-name><surname>Misra</surname> <given-names>I</given-names></string-name>, <string-name><surname>Rabbat</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sharma</surname> <given-names>V</given-names></string-name>, <string-name><surname>Synnaeve</surname> <given-names>G</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>DINOv2: Learning Robust Visual Features without Supervision</article-title>. <source>arXiv</source>; <year>2023</year>. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2304.07193">http://arxiv.org/abs/2304.07193</ext-link>.</mixed-citation></ref>
<ref id="c88"><label>88.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Ozcelik</surname> <given-names>F</given-names></string-name>, <string-name><surname>VanRullen</surname> <given-names>R</given-names></string-name></person-group>, <article-title>Natural scene reconstruction from fMRI signals using generative latent diffusion</article-title>. <source>arXiv</source>; <year>2023</year>. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2303.05334">http://arxiv.org/abs/2303.05334</ext-link>.</mixed-citation></ref>
<ref id="c89"><label>89.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Patterson</surname> <given-names>K</given-names></string-name>, <string-name><surname>Lambon Ralph</surname> <given-names>MA</given-names></string-name></person-group>. <article-title>The Hub-and-Spoke Hypothesis of Semantic Memory</article-title>. <source>Neurobiology of Language Elsevier</source>; <year>2016</year>.p. <fpage>765</fpage>–<lpage>775</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/B9780124077942000614">https://linkinghub.elsevier.com/retrieve/pii/B9780124077942000614</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/B978-0-12-407794-2.00061-4</pub-id>.</mixed-citation></ref>
<ref id="c90"><label>90.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pearson</surname> <given-names>J</given-names></string-name></person-group>. <article-title>The human imagination: the cognitive neuroscience of visual mental imagery</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2019</year> Oct; <volume>20</volume>(<issue>10</issue>):<fpage>624</fpage>–<lpage>634</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41583-019-0202-9">https://www.nature.com/articles/s41583-019-0202-9</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41583-019-0202-9</pub-id>.</mixed-citation></ref>
<ref id="c91"><label>91.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname> <given-names>F</given-names></string-name>, <string-name><surname>Varoquaux</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gramfort</surname> <given-names>A</given-names></string-name>, <string-name><surname>Michel</surname> <given-names>V</given-names></string-name>, <string-name><surname>Thirion</surname> <given-names>B</given-names></string-name>, <string-name><surname>Grisel</surname> <given-names>O</given-names></string-name>, <string-name><surname>Blondel</surname> <given-names>M</given-names></string-name>, <string-name><surname>Prettenhofer</surname> <given-names>P</given-names></string-name>, <string-name><surname>Weiss</surname> <given-names>R</given-names></string-name></person-group>, <article-title>Dubourg V</article-title>. <article-title>Scikit-learn: Machine learning in Python</article-title>. <source>Journal of machine Learning research</source> <year>2011</year>; <volume>12</volume>:<fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation></ref>
<ref id="c92"><label>92.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname> <given-names>F</given-names></string-name>, <string-name><surname>Lou</surname> <given-names>B</given-names></string-name>, <string-name><surname>Pritchett</surname> <given-names>B</given-names></string-name>, <string-name><surname>Ritter</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gershman</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name>, <string-name><surname>Botvinick</surname> <given-names>M</given-names></string-name>, <string-name><surname>Fedorenko</surname> <given-names>E</given-names></string-name></person-group>. <article-title>Toward a universal decoder of linguistic meaning from brain activation</article-title>. <source>Nature Communications</source>. <year>2018</year> Mar; <volume>9</volume>(<issue>1</issue>):<fpage>963</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-018-03068-4">https://www.nature.com/articles/s41467-018-03068-4</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41467-018-03068-4</pub-id>.</mixed-citation></ref>
<ref id="c93"><label>93.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pobric</surname> <given-names>G</given-names></string-name>, <string-name><surname>Jefferies</surname> <given-names>E</given-names></string-name>, <string-name><surname>Ralph</surname> <given-names>MAL</given-names></string-name></person-group>. <article-title>Category-Specific versus Category-General Semantic Impairment Induced by Transcranial Magnetic Stimulation</article-title>. <source>Current Biology</source>. <year>2010</year> May; <volume>20</volume>(<issue>10</issue>):<fpage>964</fpage>–<lpage>968</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.cell.com/current-biology/abstract/S0960-9822(10)00456-2">https://www.cell.com/current-biology/abstract/S0960-9822(10)00456-2</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cub.2010.03.070</pub-id>, publisher: Elsevier.</mixed-citation></ref>
<ref id="c94"><label>94.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Popham</surname> <given-names>SF</given-names></string-name>, <string-name><surname>Huth</surname> <given-names>AG</given-names></string-name>, <string-name><surname>Bilenko</surname> <given-names>NY</given-names></string-name>, <string-name><surname>Deniz</surname> <given-names>F</given-names></string-name>, <string-name><surname>Gao</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Nunez-Elizalde</surname> <given-names>AO</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name></person-group>. <article-title>Visual and linguistic semantic representations are aligned at the border of human visual cortex</article-title>. <source>Nature Neuroscience</source>. <year>2021</year>; <volume>24</volume>(<issue>11</issue>):<fpage>1628</fpage>– <lpage>1636</lpage>.</mixed-citation></ref>
<ref id="c95"><label>95.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Radford</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Hallacy</surname> <given-names>C</given-names></string-name>, <string-name><surname>Ramesh</surname> <given-names>A</given-names></string-name>, <string-name><surname>Goh</surname> <given-names>G</given-names></string-name>, <string-name><surname>Agarwal</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sastry</surname> <given-names>G</given-names></string-name>, <string-name><surname>Askell</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mishkin</surname> <given-names>P</given-names></string-name>, <string-name><surname>Clark</surname> <given-names>J</given-names></string-name>, <string-name><surname>Krueger</surname> <given-names>G</given-names></string-name>, <string-name><surname>Sutskever</surname> <given-names>I</given-names></string-name></person-group>. <article-title>Learning Transferable Visual Models From Natural Language Supervision</article-title>. <conf-name>Proceedings of the 38th International Conference on Machine Learning</conf-name>. <year>2021</year>; p. <fpage>16</fpage>.</mixed-citation></ref>
<ref id="c96"><label>96.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Radford</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Child</surname> <given-names>R</given-names></string-name>, <string-name><surname>Luan</surname> <given-names>D</given-names></string-name>, <string-name><surname>Amodei</surname> <given-names>D</given-names></string-name>, <string-name><surname>Sutskever</surname> <given-names>I</given-names></string-name></person-group>. <article-title>Language models are unsupervised multitask learners</article-title>. <source>OpenAI blog</source>. <year>2019</year>; <volume>1</volume>(<issue>8</issue>):<fpage>9</fpage>.</mixed-citation></ref>
<ref id="c97"><label>97.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ralph</surname> <given-names>MAL</given-names></string-name>, <string-name><surname>Jefferies</surname> <given-names>E</given-names></string-name>, <string-name><surname>Patterson</surname> <given-names>K</given-names></string-name>, <string-name><surname>Rogers</surname> <given-names>TT</given-names></string-name></person-group>. <article-title>The neural and computational bases of semantic cognition</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2017</year>; <volume>18</volume>(<issue>1</issue>):<fpage>42</fpage>–<lpage>55</lpage>.</mixed-citation></ref>
<ref id="c98"><label>98.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reddy</surname> <given-names>L</given-names></string-name>, <string-name><surname>Tsuchiya</surname> <given-names>N</given-names></string-name>, <string-name><surname>Serre</surname> <given-names>T</given-names></string-name></person-group>. <article-title>Reading the mind’s eye: decoding category information during mental imagery</article-title>. <source>NeuroImage</source>. <year>2010</year> Apr; <volume>50</volume>(<issue>2</issue>):<fpage>818</fpage>–<lpage>825</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2823980/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2823980/</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.11.084</pub-id>.</mixed-citation></ref>
<ref id="c99"><label>99.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Reimers</surname> <given-names>N</given-names></string-name>, <string-name><surname>Gurevych</surname> <given-names>I</given-names></string-name></person-group>. <article-title>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</article-title>. <conf-name>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</conf-name>, <year>2019</year>. p. <fpage>3980</fpage>–<lpage>3990</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.aclweb.org/anthology/D19-1410">https://www.aclweb.org/anthology/D19-1410</ext-link>, doi: <pub-id pub-id-type="doi">10.18653/v1/D19-1410</pub-id>.</mixed-citation></ref>
<ref id="c100"><label>100.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rogers</surname> <given-names>TT</given-names></string-name>, <string-name><surname>Lambon Ralph</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Garrard</surname> <given-names>P</given-names></string-name>, <string-name><surname>Bozeat</surname> <given-names>S</given-names></string-name>, <string-name><surname>McClelland</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Hodges</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Patterson</surname> <given-names>K</given-names></string-name></person-group>. <article-title>Structure and Deterioration of Semantic Memory: A Neuropsychological and Computational Investigation</article-title>. <source>Psychological Review</source>. <year>2004</year>; <volume>111</volume>(<issue>1</issue>):<fpage>205</fpage>–<lpage>235</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.apa.org/doi/10.1037/0033-295X.111.1.205">https://doi.apa.org/doi/10.1037/0033-295X.111.1.205</ext-link>, doi: <pub-id pub-id-type="doi">10.1037/0033-295X.111.1.205</pub-id>.</mixed-citation></ref>
<ref id="c101"><label>101.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sanchez</surname> <given-names>G</given-names></string-name>, <string-name><surname>Hartmann</surname> <given-names>T</given-names></string-name>, <string-name><surname>Fuscà</surname> <given-names>M</given-names></string-name>, <string-name><surname>Demarchi</surname> <given-names>G</given-names></string-name>, <string-name><surname>Weisz</surname> <given-names>N</given-names></string-name></person-group>. <article-title>Decoding across sensory modalities reveals common supramodal signatures of conscious perception</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2020</year> Mar; <volume>117</volume>(<issue>13</issue>):<fpage>7437</fpage>–<lpage>7446</lpage>. <ext-link ext-link-type="uri" xlink:href="https://pnas.org/doi/full/10.1073/pnas.1912584117">https://pnas.org/doi/full/10.1073/pnas.1912584117</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.1912584117</pub-id>.</mixed-citation></ref>
<ref id="c102"><label>102.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schoffelen</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Oostenveld</surname> <given-names>R</given-names></string-name>, <string-name><surname>Lam</surname> <given-names>NHL</given-names></string-name>, <string-name><surname>Uddén</surname> <given-names>J</given-names></string-name>, <string-name><surname>Hultén</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hagoort</surname> <given-names>P</given-names></string-name></person-group>. <article-title>A 204-subject multimodal neuroimaging dataset to study language processing</article-title>. <source>Scientific Data</source>. <year>2019</year>; <volume>6</volume>(<issue>1</issue>):<fpage>17</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41597-019-0020-y</pub-id>.</mixed-citation></ref>
<ref id="c103"><label>103.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sereno</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Dale</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Reppas</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Kwong</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Belliveau</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Brady</surname> <given-names>TJ</given-names></string-name>, <string-name><surname>Rosen</surname> <given-names>BR</given-names></string-name>, <string-name><surname>Tootell</surname> <given-names>RBH</given-names></string-name></person-group>. <article-title>Borders of Multiple Visual Areas in Humans Revealed by Functional Magnetic Resonance Imaging</article-title>. <source>Science</source>. <year>1995</year> May; <volume>268</volume>(<issue>5212</issue>):<fpage>889</fpage>–<lpage>893</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/10.1126/science.7754376">https://www.science.org/doi/10.1126/science.7754376</ext-link>, doi: <pub-id pub-id-type="doi">10.1126/science.7754376</pub-id>.</mixed-citation></ref>
<ref id="c104"><label>104.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname> <given-names>G</given-names></string-name>, <string-name><surname>Horikawa</surname> <given-names>T</given-names></string-name>, <string-name><surname>Majima</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kamitani</surname> <given-names>Y</given-names></string-name></person-group>. <article-title>Deep image reconstruction from human brain activity</article-title>. <source>PLOS Computational Biology</source>. <year>2019</year> Jan; <volume>15</volume>(<issue>1</issue>):<fpage>e1006633</fpage>. <ext-link ext-link-type="uri" xlink:href="https://dx.plos.org/10.1371/journal.pcbi.1006633">https://dx.plos.org/10.1371/journal.pcbi.1006633</ext-link>, doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006633</pub-id>.</mixed-citation></ref>
<ref id="c105"><label>105.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shinkareva</surname> <given-names>SV</given-names></string-name>, <string-name><surname>Malave</surname> <given-names>VL</given-names></string-name>, <string-name><surname>Mason</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Mitchell</surname> <given-names>TM</given-names></string-name>, <string-name><surname>Just</surname> <given-names>MA</given-names></string-name></person-group>. <article-title>Commonality of neural representations of words and pictures</article-title>. <source>NeuroImage</source>. <year>2011</year>; <volume>54</volume>(<issue>3</issue>):<fpage>2418</fpage>–<lpage>2425</lpage>.</mixed-citation></ref>
<ref id="c106"><label>106.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simanova</surname> <given-names>I</given-names></string-name>, <string-name><surname>Hagoort</surname> <given-names>P</given-names></string-name>, <string-name><surname>Oostenveld</surname> <given-names>R</given-names></string-name>, <string-name><surname>van Gerven</surname> <given-names>MAJ</given-names></string-name></person-group>. <article-title>Modality-Independent Decoding of Semantic Information from the Human Brain</article-title>. <source>Cerebral Cortex</source>. <year>2014</year>; <volume>24</volume>(<issue>2</issue>):<fpage>426</fpage>–<lpage>434</lpage>.</mixed-citation></ref>
<ref id="c107"><label>107.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simmons</surname> <given-names>WK</given-names></string-name>, <string-name><surname>Barsalou</surname> <given-names>LW</given-names></string-name></person-group>. <article-title>The similarity-in-topography principle: Reconciling theories of conceptual deficits</article-title>. <source>Cognitive Neuropsychology</source>. <year>2003</year>; <volume>20</volume>(<issue>3-6</issue>):<fpage>451</fpage>–<lpage>486</lpage>.</mixed-citation></ref>
<ref id="c108"><label>108.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Singh</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hu</surname> <given-names>R</given-names></string-name>, <string-name><surname>Goswami</surname> <given-names>V</given-names></string-name>, <string-name><surname>Couairon</surname> <given-names>G</given-names></string-name>, <string-name><surname>Galuba</surname> <given-names>W</given-names></string-name>, <string-name><surname>Rohrbach</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kiela</surname> <given-names>D</given-names></string-name></person-group><source>. FLAVA: A Foundational Language and Vision Alignment Model</source>. <publisher-loc>In</publisher-loc>: <publisher-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</publisher-name>; <year>2022</year>. p. <fpage>15638</fpage>–<lpage>15650</lpage>.</mixed-citation></ref>
<ref id="c109"><label>109.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Nichols</surname> <given-names>TE</given-names></string-name></person-group>. <article-title>Threshold-free cluster enhancement: Addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title>. <source>NeuroImage</source>. <year>2009</year>; <volume>44</volume>(<issue>1</issue>):<fpage>83</fpage>–<lpage>98</lpage>.</mixed-citation></ref>
<ref id="c110"><label>110.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Snowden</surname> <given-names>J</given-names></string-name>, <string-name><surname>Goulding</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Neary</surname> <given-names>D</given-names></string-name></person-group>. <article-title>Semantic dementia: A form of circumscribed cerebral atrophy</article-title>. <source>Behavioural Neurology</source>. <year>1989</year>; <volume>2</volume>(<issue>3</issue>):<fpage>124043</fpage>. <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/abs/10.1155/1989/124043">https://onlinelibrary.wiley.com/doi/abs/10.1155/1989/124043</ext-link>, doi: <pub-id pub-id-type="doi">10.1155/1989/124043</pub-id>.</mixed-citation></ref>
<ref id="c111"><label>111.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spagna</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hajhajate</surname> <given-names>D</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bartolomeo</surname> <given-names>P</given-names></string-name></person-group>. <article-title>Visual mental imagery engages the left fusiform gyrus, but not the early visual cortex: A meta-analysis of neuroimaging evidence</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>. <year>2021</year> Mar; <volume>122</volume>:<fpage>201</fpage>–<lpage>217</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0149763420307041">https://www.sciencedirect.com/science/article/pii/S0149763420307041</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neubiorev.2020.12.029</pub-id>.</mixed-citation></ref>
<ref id="c112"><label>112.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Billings</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Silson</surname> <given-names>EH</given-names></string-name>, <string-name><surname>Robertson</surname> <given-names>CE</given-names></string-name></person-group>. <article-title>A network linking scene perception and spatial memory systems in posterior cerebral cortex</article-title>. <source>Nature Communications</source>. <year>2021</year> May; <volume>12</volume>(<issue>1</issue>):<fpage>2632</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-021-22848-z">https://www.nature.com/articles/s41467-021-22848-z</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41467-021-22848-z</pub-id>, publisher: Nature Publishing Group.</mixed-citation></ref>
<ref id="c113"><label>113.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Steiner</surname> <given-names>A</given-names></string-name>, <string-name><surname>Pinto</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Tschannen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Keysers</surname> <given-names>D</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Bitton</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Gritsenko</surname> <given-names>A</given-names></string-name>, <string-name><surname>Minderer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sherbondy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Long</surname> <given-names>S</given-names></string-name>, <string-name><surname>Qin</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ingle</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bugliarello</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kazemzadeh</surname> <given-names>S</given-names></string-name>, <string-name><surname>Mesnard</surname> <given-names>T</given-names></string-name>, <string-name><surname>Alabdulmohsin</surname> <given-names>I</given-names></string-name>, <string-name><surname>Beyer</surname> <given-names>L</given-names></string-name>, <string-name><surname>Zhai</surname> <given-names>X</given-names></string-name></person-group>, <article-title>PaliGemma 2: A Family of Versatile VLMs for Transfer</article-title>. <source>arXiv</source>; <year>2024</year>. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2412.03555">http://arxiv.org/abs/2412.03555</ext-link>, doi: <pub-id pub-id-type="doi">10.48550/arXiv.2412.03555</pub-id>.</mixed-citation></ref>
<ref id="c114"><label>114.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname> <given-names>M</given-names></string-name>, <string-name><surname>Thompson</surname> <given-names>R</given-names></string-name>, <string-name><surname>Cusack</surname> <given-names>R</given-names></string-name>, <string-name><surname>Duncan</surname> <given-names>J</given-names></string-name></person-group>. <article-title>Top-Down Activation of Shape-Specific Population Codes in Visual Cortex during Mental Imagery</article-title>. <source>The Journal of Neuroscience</source>. <year>2009</year> Feb; <volume>29</volume>(<issue>5</issue>):<fpage>1565</fpage>–<lpage>1572</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.4657-08.2009">https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.4657-08.2009</ext-link>, doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4657-08.2009</pub-id>.</mixed-citation></ref>
<ref id="c115"><label>115.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Takagi</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Nishimoto</surname> <given-names>S</given-names></string-name></person-group>. <article-title>High-Resolution Image Reconstruction With Latent Diffusion Models From Human Brain Activity</article-title>. In: <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>; <year>2023</year>. p. <fpage>14453</fpage>–<lpage>14463</lpage>.</mixed-citation></ref>
<ref id="c116"><label>116.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Tang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Du</surname> <given-names>M</given-names></string-name></person-group>, <source>Vo VA. Brain encoding models based on multimodal transformers can transfer across language and vision</source>. <publisher-name>NeurIPS</publisher-name>; <year>2023</year>.</mixed-citation></ref>
<ref id="c117"><label>117.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname> <given-names>J</given-names></string-name>, <string-name><surname>LeBel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Jain</surname> <given-names>S</given-names></string-name>, <string-name><surname>Huth</surname> <given-names>AG</given-names></string-name></person-group>. <article-title>Semantic reconstruction of continuous language from non-invasive brain recordings</article-title>. <source>Nature Neuroscience</source>. <year>2023</year> May; <volume>26</volume>(<issue>5</issue>):<fpage>858</fpage>–<lpage>866</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41593-023-01304-9">https://www.nature.com/articles/s41593-023-01304-9</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41593-023-01304-9</pub-id>.</mixed-citation></ref>
<ref id="c118"><label>118.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tong</surname> <given-names>J</given-names></string-name>, <string-name><surname>Binder</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Humphries</surname> <given-names>C</given-names></string-name>, <string-name><surname>Mazurchuk</surname> <given-names>S</given-names></string-name>, <string-name><surname>Conant</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Fernandino</surname> <given-names>L</given-names></string-name></person-group>. <article-title>A Distributed Network for Multimodal Experiential Representation of Concepts</article-title>. <source>Journal of Neuroscience</source>. <year>2022</year>; <volume>42</volume>(<issue>37</issue>):<fpage>7121</fpage>–<lpage>7130</lpage>.</mixed-citation></ref>
<ref id="c119"><label>119.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Touvron</surname> <given-names>H</given-names></string-name>, <string-name><surname>Martin</surname> <given-names>L</given-names></string-name>, <string-name><surname>Stone</surname> <given-names>K</given-names></string-name>, <string-name><surname>Albert</surname> <given-names>P</given-names></string-name>, <string-name><surname>Almahairi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Babaei</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Bashlykov</surname> <given-names>N</given-names></string-name>, <string-name><surname>Batra</surname> <given-names>S</given-names></string-name>, <string-name><surname>Bhargava</surname> <given-names>P</given-names></string-name>, <string-name><surname>Bhosale</surname> <given-names>S</given-names></string-name>, <string-name><surname>Bikel</surname> <given-names>D</given-names></string-name>, <string-name><surname>Blecher</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ferrer</surname> <given-names>CC</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Cucurull</surname> <given-names>G</given-names></string-name>, <string-name><surname>Esiobu</surname> <given-names>D</given-names></string-name>, <string-name><surname>Fernandes</surname> <given-names>J</given-names></string-name>, <string-name><surname>Fu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Fu</surname> <given-names>W</given-names></string-name>, <string-name><surname>Fuller</surname> <given-names>B</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Llama 2: Open Foundation and Fine-Tuned Chat Models</article-title>. <source>arXiv</source>; <year>2023</year>. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2307.09288">http://arxiv.org/abs/2307.09288</ext-link>, doi: <pub-id pub-id-type="doi">10.48550/arXiv.2307.09288</pub-id>.</mixed-citation></ref>
<ref id="c120"><label>120.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tranel</surname> <given-names>D</given-names></string-name>, <string-name><surname>Damasio</surname> <given-names>H</given-names></string-name>, <string-name><surname>Damasio</surname> <given-names>AR</given-names></string-name></person-group>. <article-title>A neural basis for the retrieval of conceptual knowledge</article-title>. <source>Neuropsychologia</source>. <year>1997</year> Oct; <volume>35</volume>(<issue>10</issue>):<fpage>1319</fpage>–<lpage>1327</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0028393297000857">https://www.sciencedirect.com/science/article/pii/S0028393297000857</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/S0028-3932(97)00085-7</pub-id>.</mixed-citation></ref>
<ref id="c121"><label>121.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vandenberghe</surname> <given-names>R</given-names></string-name>, <string-name><surname>Price</surname> <given-names>C</given-names></string-name>, <string-name><surname>Wise</surname> <given-names>R</given-names></string-name>, <string-name><surname>Josephs</surname> <given-names>O</given-names></string-name>, <string-name><surname>Frackowiak</surname> <given-names>RS</given-names></string-name></person-group>. <article-title>Functional anatomy of a common semantic system for words and pictures</article-title>. <source>Nature</source>. <year>1996</year>; <fpage>383</fpage>(<issue>6597</issue>).</mixed-citation></ref>
<ref id="c122"><label>122.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>VanRullen</surname> <given-names>R</given-names></string-name>, <string-name><surname>Reddy</surname> <given-names>L</given-names></string-name></person-group>. <article-title>Reconstructing faces from fMRI patterns using deep generative neural networks</article-title>. <source>Communications Biology</source>. <year>2019</year> May; <volume>2</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>10</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s42003-019-0438-y">https://www.nature.com/articles/s42003-019-0438-y</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s42003-019-0438-y</pub-id>.</mixed-citation></ref>
<ref id="c123"><label>123.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Warrington</surname> <given-names>EK</given-names></string-name></person-group>. <article-title>The Selective Impairment of Semantic Memory</article-title>. <source>Quarterly Journal of Experimental Psychology</source>. <year>1975</year> Nov; <volume>27</volume>(<issue>4</issue>):<fpage>635</fpage>–<lpage>657</lpage>. <ext-link ext-link-type="uri" xlink:href="https://journals.sagepub.com/doi/10.1080/14640747508400525">https://journals.sagepub.com/doi/10.1080/14640747508400525</ext-link>, doi: <pub-id pub-id-type="doi">10.1080/14640747508400525</pub-id>.</mixed-citation></ref>
<ref id="c124"><label>124.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Warrington</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Mccarthy</surname> <given-names>RA</given-names></string-name></person-group>. <article-title>Categories of knowledge: Further fractionations and an attempted integration</article-title>. <source>Brain: a Journal of Neurology</source>. <year>1987</year>;.</mixed-citation></ref>
<ref id="c125"><label>125.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Warrington</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Shallice</surname> <given-names>T</given-names></string-name></person-group>. <article-title>Category Specific Semantic Impairments</article-title>. <source>Brain: a Journal of Neurology</source>. <year>1984</year></mixed-citation></ref>
<ref id="c126"><label>126.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Wolf</surname> <given-names>T</given-names></string-name>, <string-name><surname>Debut</surname> <given-names>L</given-names></string-name>, <string-name><surname>Sanh</surname> <given-names>V</given-names></string-name>, <string-name><surname>Chaumond</surname> <given-names>J</given-names></string-name>, <string-name><surname>Delangue</surname> <given-names>C</given-names></string-name>, <string-name><surname>Moi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Cistac</surname> <given-names>P</given-names></string-name>, <string-name><surname>Rault</surname> <given-names>T</given-names></string-name>, <string-name><surname>Louf</surname> <given-names>R</given-names></string-name>, <string-name><surname>Funtowicz</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Transformers: State-of-the-art natural language processing</article-title>. In: <conf-name>Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations</conf-name>; <year>2020</year>. p. <fpage>38</fpage>–<lpage>45</lpage>.</mixed-citation></ref>
<ref id="c127"><label>127.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Xu</surname> <given-names>X</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>C</given-names></string-name>, <string-name><surname>Rosenman</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lal</surname> <given-names>V</given-names></string-name>, <string-name><surname>Che</surname> <given-names>W</given-names></string-name>, <string-name><surname>Duan</surname> <given-names>N</given-names></string-name></person-group>. <article-title>BridgeTower: Building Bridges between Encoders in Vision-Language Representation Learning</article-title>. <conf-name>Proceedings of the AAAI Conference on Artificial Intelligence</conf-name>. <year>2023</year> Jun; <volume>37</volume>(<issue>9</issue>):<fpage>10637</fpage>–<lpage>10647</lpage>. <ext-link ext-link-type="uri" xlink:href="https://ojs.aaai.org/index.php/AAAI/article/view/26263">https://ojs.aaai.org/index.php/AAAI/article/view/26263</ext-link>, doi: <pub-id pub-id-type="doi">10.1609/aaai.v37i9.26263</pub-id>.</mixed-citation></ref>
<ref id="c128"><label>128.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Zhai</surname> <given-names>X</given-names></string-name>, <string-name><surname>Mustafa</surname> <given-names>B</given-names></string-name>, <string-name><surname>Kolesnikov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Beyer</surname> <given-names>L</given-names></string-name></person-group>. <article-title>Sigmoid Loss for Language Image Pre-Training</article-title>. In: <conf-name>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</conf-name>; <year>2023</year>. p. <fpage>11941</fpage>–<lpage>11952</lpage>. <ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/document/10377550/">https://ieeexplore.ieee.org/document/10377550/</ext-link>, doi: <pub-id pub-id-type="doi">10.1109/ICCV51070.2023.01100</pub-id>.</mixed-citation></ref>
<ref id="c129"><label>129.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zola-Morgan</surname> <given-names>S</given-names></string-name></person-group>. <article-title>Localization of Brain Function: The Legacy of Franz Joseph Gall (1758-1828)</article-title>. <source>Annual Review of Neuroscience</source>. <year>1995</year> Mar; <volume>18</volume>(Volume 18, 1995):<fpage>359</fpage>–<lpage>383</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.annualreviews.org/content/journals/10.1146/annurev.ne.18.030195.002043">https://www.annualreviews.org/content/journals/10.1146/annurev.ne.18.030195.002043</ext-link>, doi: <pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.002043</pub-id>, publisher: Annual Reviews.</mixed-citation></ref>
<ref id="c130"><label>130.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zwaan</surname> <given-names>RA</given-names></string-name></person-group>. <article-title>Situation models, mental simulations, and abstract concepts in discourse comprehension</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>2016</year> Aug; <volume>23</volume>(<issue>4</issue>):<fpage>1028</fpage>–<lpage>1034</lpage>. <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.3758/s13423-015-0864-x">http://link.springer.com/10.3758/s13423-015-0864-x</ext-link>, doi: <pub-id pub-id-type="doi">10.3758/s13423-015-0864-x</pub-id>.</mixed-citation></ref>
</ref-list>
<app-group>
<app>
<label>Appendix 1</label><title>Feature extraction details</title>
<sec id="s7">
<p>For each target stimulus (image or caption), our database also contained an equivalent stimulus in the other modality (caption or image). In this way, we could extract model features from the corresponding image for vision models, the corresponding caption for language models, and an multimodal representation of both image and caption for the multimodal models. We used publicly available pretrained models implemented in the HuggingFace Transformers library (<xref ref-type="bibr" rid="c126">Wolf et al., 2020</xref>) or from their respective authors’ repositories.</p>
<p>Model versions for unimodal models are as indicated in <xref rid="fig4" ref-type="fig">Figure 4</xref>. For multimodal models, the exact version for CLIP was clip-vit-large-patch14, for ViLT vilt-b32-mlm, for Visual-BERT visualbert-nlvr2-coco-pre, for Imagebind imagebind_huge, for Bridgetower bridgetower-large-itm-mlm-itc, for Flava flava-full, for SigLip siglip-so400m-patch14-384, and for Paligemma2 paligemma2-3b-pt-224.</p>
<p>We extracted language features from all models by averaging the outputs for each token, as this was established as common practice for the extraction of sentence embeddings from Transformer-based language models (e.g. <xref ref-type="bibr" rid="c62">Krasnowska-Kieraś and Wróblewska, 2019</xref>; <xref ref-type="bibr" rid="c99">Reimers and Gurevych, 2019</xref>).</p>
<p>For Transformer-based vision models, we compared representations extracted by averaging the outputs for each patch with representations extracted from [CLS] tokens in Table 1. We found that for almost all models, the mean features allow for higher decoding accuracies. For all experiments reported in the main paper we therefore only considered this method.</p>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Appendix 1—table 1.</label>
<caption><title>Feature comparison for vision models.</title><p>Pairwise accuracy for modality-agnostic decoders based on vision features extracted by averaging the last hidden states of all patches (“vision_features_mean”) compared to when using features extracted from <monospace>[CLS]</monospace> tokens (“vision_features_cls”). The method leading to the best decoding performance for each model is highlighted in bold.</p></caption>
<graphic xlink:href="658221v1_tbls1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>For multimodal models, we also compared a range of techniques for feature extraction. The results are shown in Table 2.</p>
<table-wrap id="tbls2" orientation="portrait" position="float">
<label>Appendix 1—table 2.</label>
<caption><title>Feature comparison for multimodal models.</title><p>The features are either based on the [CLS] tokens from the fused representations (“fused_cls”), averaging over the fused tokens (“fused_mean”), or averaging over tokens from intermediate vision and language stream outputs (“avg”). For the last case, for some models the vision and language features can also be either based on [CLS] or based on averaging over all tokens/patches. The method leading to the best decoding performance for each model is highlighted in bold.</p></caption>
<graphic xlink:href="658221v1_tbls2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>For dual-stream multimodal models, we averaged the vision and language features to create the final multimodal feature representation. For single-stream multimodal features we compared using representations extracted by averaging the outputs for each token with representations extracted from [CLS] token and found that the averaged output leads to better performance in almost all cases (see Table 2). Further, some single-stream models (Flava, Paligemma2 and BLIP2) allow for feature extraction based on intermediate vision and language representations in addition to a direct extraction of multimodal features (based on fused representations from the multimodal stream). We found that averaging features from these intermediate vision and language representations leads to better performance (see Table 2). For all results reported in the main paper we used the feature extraction method leading to best performance for each model.</p>
</sec>
<sec id="s8">
<title>Decoder training details</title>
<p>The decoders were linear ridge-regression models as implemented in the scikit-learn library (<xref ref-type="bibr" rid="c91">Pedregosa et al., 2011</xref>). All training data was standardized to have mean of 0 and standard deviation of 1. The test data was standardized using the mean and standard deviation of the training data. The regularization hyperparameter α was optimized using 5-fold cross validation on the training set (values considered: α ∊ {1<italic>e</italic>3, 1<italic>e</italic>4, 1<italic>e</italic>5, 1<italic>e</italic>6, 1<italic>e</italic>7}). Afterwards, a final model was trained using the best α on the whole training set.</p>
</sec>
</app>
<app>
<label>Appendix 2</label><title>Qualitative Decoding Results for Modality-Specific Decoders</title>
<sec id="s9">
<p>In this section we present qualitative decoding results for modality-specific decoders for the same examples as presented in Section Qualitative Decoding Results for modality-agnostic decoders. The results show that modality-agnostic decoders are as good as modality-specific decoders when evaluated in a within-modality decoding setup (<xref rid="fig1" ref-type="fig">Figures 1</xref> and <xref rid="fig3" ref-type="fig">3</xref>) but substantially better than modality-specific decoders when evaluated in a cross-decoding setup (<xref rid="fig2" ref-type="fig">Figures 2</xref> and <xref rid="fig4" ref-type="fig">4</xref>).</p>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 2—figure 1.</label>
<caption><title>Decoding examples for image decoding using a modality-specific decoder trained on images (within-modality decoding).</title><p>The first column shows the image the subject was seeing and the 5 following columns show the candidate stimuli with highest similarity to the predicted features, in descending order. We display both the image and the caption of the candidate stimuli because the decoder is based on multimodal features that are extracted from both modalities. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v1_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 2—figure 2.</label>
<caption><title>Decoding examples for caption decoding using a modality-specific decoder trained on images (cross-modality decoding).</title><p>For details see caption of <xref rid="fig1" ref-type="fig">Figure 1</xref> All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v1_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 2—figure 3.</label>
<caption><title>Decoding examples for caption decoding using a modality-specific decoder trained on captions (within-modality decoding).</title><p>For details see caption of <xref rid="fig1" ref-type="fig">Figure 1</xref> All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v1_figs3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 2—figure 4.</label>
<caption><title>Decoding examples for image decoding using a modality-specific decoder trained on captions (cross-modality decoding).</title><p>For details see caption of <xref rid="fig1" ref-type="fig">Figure 1</xref> All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v1_figs4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</app>
<app>
<label>Appendix 3</label><title>Candidates for Modality-agnostic regions</title>
<sec id="s11">
<p>Table 1 shows candidates for modality-agnostic regions that were identified by previous work. We only considered fMRI experiments involving multiple stimulus modalities. Many other studies relied on unimodal stimuli and semantic tasks to identify modality-agnostic/ conceptual regions, these are however less directly comparable to our setup.</p>
<table-wrap id="tbls3" orientation="portrait" position="float">
<label>Appendix 3—table 1.</label>
<caption><title>Candidates for modality-agnostic regions as identified by previous work.</title><p>All there regions were also found in out analysis, except for the 2 regions marked with an asterisk (*).</p></caption>
<graphic xlink:href="658221v1_tbls3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</app>
<app>
<label>Appendix 4</label><title>Searchlight size</title>
<sec id="s13">
<p>In order to optimize the size of the searchlight for this analysis, we first ran a searchlight analysis with a fixed radius of 10mm using a modality-agnostic decoder on the data from the first subject (sub-01). Due to the shape of the cortex this leads of searchlights that contain varying numbers of vertices (on average: 897.4; max: 1580; min: 399). By observing the decoding scores as function of the number of vertices we find that performance peaks at 750 vertices (cf. Table 1). The final searchlight analyses was therefore performed with a searchlight of a fixed number of 750 vertices. The average radius with this number of vertices was 9.41mm (max: 13.65mm).</p>
<table-wrap id="tbls4" orientation="portrait" position="float">
<label>Appendix 4—table 1.</label>
<caption><title>Average decoding scores (for images and captions) by number of vertices.</title><p>Scores calculated based on the results of a searchlight analysis with a radius of 10mm. The accuracy values were grouped into bins based on the number of vertices that the searchlight was based on.</p></caption>
<graphic xlink:href="658221v1_tbls4.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</app>
<app>
<label>Appendix 5</label><title>Per-subject results</title>
<sec id="s15">
<p>Results for individual subjects can be found in <xref rid="fig1" ref-type="fig">Figure 1</xref>. Among all subjects, we found similar converging results for decoding accuracies when comparing models, feature modalities, and modality-agnostic with modality-specific decoders.</p>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 5—figure 1.</label>
<caption><title>Pairwise accuracy per subject.</title><p>For details refer to <xref rid="fig5" ref-type="fig">Figure 5</xref></p></caption>
<graphic xlink:href="658221v1_figs5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</app>
<app>
<label>Appendix 6</label><title>Qualitative Imagery Decoding Results</title>
<sec id="s17">
<p>The following figures 1-6 present qualitative decoding results for the imagery conditions using a modality-agnostic decoder. We present the results separately for each subject as each subject chose an individual set of 3 stimuli to perform mental imagery on. In each plot, the leftmost column shows the caption that was used as initial instruction as well as the subject’s sketch that they drew at the end of the experiment.</p>
<p>We used the same large candidate set of 41K stimuli as for the qualitative decoding results for the other conditions (see also Section Qualitative Decoding Results).</p>
<p>Overall, we found that the decoding quality for imagery stimuli lags behind that for trials with perceived stimuli. This was expected and confirms the quantitative results reported in Section Imagery Decoding. Still, in several cases some of the concepts are decoded correctly (e.g. a women in the first row of <xref rid="fig1" ref-type="fig">Figure 1</xref>; winter sports in the second row of <xref rid="fig3" ref-type="fig">Figure 3</xref>; laptops/screens and multiple people in the third row of <xref rid="fig3" ref-type="fig">Figure 3</xref>).</p>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 6—figure 1.</label>
<caption><title>Imagery decoding for Subject 1 using a modality-agnostic decoder.</title><p>The first column shows the caption that was used to stimulate imagery and on top of it the sketch of their mental image that the subjects draw at the end of the experiment. The 5 following columns show the candidate stimuli with highest similarity to the predicted features, in descending order. We display both the image and the caption of the candidate stimuli because the decoder is based on multimodal features that are extracted from both modalities. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v1_figs6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs7" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 6—figure 2.</label>
<caption><title>Imagery decoding for Subject 2 using a modality-agnostic decoder.</title><p>For further details refer to the caption of <xref rid="fig1" ref-type="fig">Figure 1</xref>. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v1_figs7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs8" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 6—figure 3.</label>
<caption><title>Imagery decoding for Subject 3 using a modality-agnostic decoder.</title><p>For further details refer to the caption of <xref rid="fig1" ref-type="fig">Figure 1</xref>. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v1_figs8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs9" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 6—figure 4.</label>
<caption><title>Imagery decoding for Subject 4 using a modality-agnostic decoder.</title><p>For further details refer to the caption of <xref rid="fig1" ref-type="fig">Figure 1</xref>. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v1_figs9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs10" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 6—figure 5.</label>
<caption><title>Imagery decoding for Subject 5 using a modality-agnostic decoder.</title><p>For further details refer to the caption of <xref rid="fig1" ref-type="fig">Figure 1</xref>. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v1_figs10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs11" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 6—figure 6.</label>
<caption><title>Imagery decoding for Subject 6 using a modality-agnostic decoder. For further details refer to the caption of Figure 1.</title><p>All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v1_figs11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107933.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This manuscript introduces a potentially <bold>valuable</bold> large-scale fMRI dataset pairing vision and language, and employs rigorous decoding analyses to investigate how the brain represents visual, linguistic, and imagined content. The current manuscript blurs the line between a resource paper and a theoretical contribution, and the evidence for truly modality-agnostic representations remains <bold>incomplete</bold> at this stage. Clarifying the conceptual aims and strengthening both the dataset technicality and the quantitative analyses would improve the manuscript's significance for the fields of cognitive neuroscience and multimodal AI.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107933.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors introduce a densely-sampled dataset where 6 participants viewed images and sentence descriptions derived from the MS Coco database over the course of 10 scanning sessions. The authors further showcase how image and sentence decoders can be used to predict which images or descriptions were seen, using pairwise decoding across a set of 120 test images. The authors find decodable information widely distributed across the brain, with a left-lateralized focus. The results further showed that modality-agnostic models generally outperformed modality-specific models, and that data based on captions was not explained better by caption-based models but by modality-agnostic models. Finally, the authors decoded imagined scenes.</p>
<p>Strengths:</p>
<p>(1) The dataset presents a potentially very valuable resource for investigating visual and semantic representations and their interplay.</p>
<p>(2) The introduction and discussion are very well written in the context of trying to understand the nature of multimodal representations and present a comprehensive and very useful review of the current literature on the topic.</p>
<p>Weaknesses:</p>
<p>(1) The paper is framed as presenting a dataset, yet most of it revolves around the presentation of findings in relation to what the authors call modality-agnostic representations, and in part around mental imagery. This makes it very difficult to assess the manuscript, whether the authors have achieved their aims, and whether the results support the conclusions.</p>
<p>(2) While the authors have presented a potential use case for such a dataset, there is currently far too little detail regarding data quality metrics expected from the introduction of similar datasets, including the absence of head-motion estimates, quality of intersession alignment, or noise ceilings of all individuals.</p>
<p>(3) The exact methods and statistical analyses used are still opaque, making it hard for a reader to understand how the authors achieved their results. More detail in the manuscript would be helpful, specifically regarding the exact statistical procedures, what tests were performed across, or how data were pooled across participants.</p>
<p>(4) Many findings (e.g., Figure 6) are still qualitative but could be supported by quantitative measures.</p>
<p>(5) Results are significant in regions that typically lack responses to visual stimuli, indicating potential bias in the classifier. This is relevant for the interpretation of the findings. A classification approach less sensitive to outliers (e.g., 70-way classification) could avoid this issue. Given the extreme collinearity of the experimental design, regressors in close temporal proximity will be highly similar, which could lead to leakage effects.</p>
<p>(6) The manuscript currently lacks a limitations section, specifically regarding the design of the experiment. This involves the use of the overly homogenous dataset Coco, which invites overfitting, the mixing of sentence descriptions and visual images, which invites imagery of previously seen content, and the use of a 1-back task, which can lead to carry-over effects to the subsequent trial.</p>
<p>(7) I would urge the authors to clarify whether the primary aim is the introduction of a dataset and showing the use of it, or whether it is the set of results presented. This includes the title of this manuscript. While the decoding approach is very interesting and potentially very valuable, I believe that the results in the current form are rather descriptive, and I'm wondering what specifically they add beyond what is known from other related work. This includes imagery-related results. This is completely fine! It just highlights that a stronger framing as a dataset is probably advantageous for improving the significance of this work.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107933.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study introduces SemReps-8K, a large multimodal fMRI dataset collected while subjects viewed natural images and matched captions, and performed mental imagery based on textual cues. The authors aim to train modality-agnostic decoders--models that can predict neural representations independently of the input modality - and use these models to identify brain regions containing modality-agnostic information. They find that such decoders perform comparably or better than modality-specific decoders and generalize to imagery trials.</p>
<p>Strengths:</p>
<p>(1) The dataset is a substantial and well-controlled contribution, with &gt;8,000 image-caption trials per subject and careful matching of stimuli across modalities - an essential resource for testing theories of abstract and amodal representation.</p>
<p>(2) The authors systematically compare unimodal, multimodal, and cross-modal decoders using a wide range of deep learning models, demonstrating thoughtful experimental design and thorough benchmarking.</p>
<p>(3) Their decoding pipeline is rigorous, with informative performance metrics and whole-brain searchlight analyses, offering valuable insights into the cortical distribution of shared representations.</p>
<p>(4) Extension to mental imagery decoding is a strong addition, aligning with theoretical predictions about the overlap between perception and imagery.</p>
<p>Weaknesses:</p>
<p>While the decoding results are robust, several critical limitations prevent the current findings from conclusively demonstrating truly modality-agnostic representations:</p>
<p>(1) Shared decoding ≠ abstraction: Successful decoding across modalities does not necessarily imply abstraction or modality-agnostic coding. Participants may engage in modality-specific processes (e.g., visual imagery when reading, inner speech when viewing images) that produce overlapping neural patterns. The analyses do not clearly disambiguate shared representational structure from genuinely modality-independent representations. Furthermore, in Figure 5, the modality-agnostic encoder did not perform better than the modality-specific decoder trained on images (in decoding images), but outperformed the modality-specific decoder trained on captions (in decoding captions). This asymmetry contradicts the premise of a truly &quot;modality-agnostic&quot; encoder. Additionally, given the similar performance between modality-agnostic decoders based on multimodal versus unimodal features, it remains unclear why neural representations did not preferentially align with multimodal features if they were truly modality-independent.</p>
<p>(2) The current analysis cannot definitively conclude that the decoder itself is modality-agnostic, making &quot;Qualitative Decoding Results&quot; difficult to interpret in this context. This section currently provides illustrative examples, but lacks systematic quantitative analyses.</p>
<p>(3) The use of mental imagery as evidence for modality-agnostic decoding is problematic. Imagery involves subjective, variable experiences and likely draws on semantic and perceptual networks in flexible ways. Strong decoding in imagery trials could reflect semantic overlap or task strategies rather than evidence of abstraction.</p>
<p>The manuscript presents a methodologically sophisticated and timely investigation into shared neural representations across modalities. However, the current evidence does not clearly distinguish between shared semantics, overlapping unimodal processes, and true modality-independent representations. A more cautious interpretation is warranted. Nonetheless, the dataset and methodological framework represent a valuable resource for the field.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107933.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors recorded brain responses while participants viewed images and captions. The images and captions were taken from the COCO dataset, so each image has a corresponding caption, and each caption has a corresponding image. This enabled the authors to extract features from either the presented stimulus or the corresponding stimulus in the other modality. The authors trained linear decoders to take brain responses and predict stimulus features. &quot;Modality-specific&quot; decoders were trained on brain responses to either images or captions, while &quot;modality-agnostic&quot; decoders were trained on brain responses to both stimulus modalities. The decoders were evaluated on brain responses while the participants viewed and imagined new stimuli, and prediction performance was quantified using pairwise accuracy. The authors reported the following results:</p>
<p>(1) Decoders trained on brain responses to both images and captions can predict new brain responses to either modality.</p>
<p>(2) Decoders trained on brain responses to both images and captions outperform decoders trained on brain responses to a single modality.</p>
<p>(3) Many cortical regions represent the same concepts in vision and language.</p>
<p>(4) Decoders trained on brain responses to both images and captions can decode brain responses to imagined scenes.</p>
<p>Strengths:</p>
<p>This is an interesting study that addresses important questions about modality-agnostic representations. Previous work has shown that decoders trained on brain responses to one modality can be used to decode brain responses to another modality. The authors build on these findings by collecting a new multimodal dataset and training decoders on brain responses to both modalities.</p>
<p>To my knowledge, SemReps-8K is the first dataset of brain responses to vision and language where each stimulus item has a corresponding stimulus item in the other modality. This means that brain responses to a stimulus item can be modeled using visual features of the image, linguistic features of the caption, or multimodal features derived from both the image and the caption. The authors also employed a multimodal one-back matching task, which forces the participants to activate modality-agnostic representations. Overall, SemReps-8K is a valuable resource that will help researchers answer more questions about modality-agnostic representations.</p>
<p>The analyses are also very comprehensive. The authors trained decoders on brain responses to images, captions, and both modalities, and they tested the decoders on brain responses to images, captions, and imagined scenes. They extracted stimulus features using a range of visual, linguistic, and multimodal models. The modeling framework appears rigorous, and the results offer new insights into the relationship between vision, language, and imagery. In particular, the authors found that decoders trained on brain responses to both images and captions were more effective at decoding brain responses to imagined scenes than decoders trained on brain responses to either modality in isolation. The authors also found that imagined scenes can be decoded from a broad network of cortical regions.</p>
<p>Weaknesses:</p>
<p>The characterization of &quot;modality-agnostic&quot; and &quot;modality-specific&quot; decoders seems a bit contradictory. There are three major choices when fitting a decoder: the modality of the training stimuli, the modality of the testing stimuli, and the model used to extract stimulus features. However, the authors characterize their decoders based on only the first choice-&quot;modality-specific&quot; decoders were trained on brain responses to either images or captions, while &quot;modality-agnostic&quot; decoders were trained on brain responses to both stimulus modalities. I think that this leads to some instances where the conclusions are inconsistent with the methods and results.</p>
<p>First, the authors suggest that &quot;modality-specific decoders are not explicitly encouraged to pick up on modality-agnostic features during training&quot; (line 137) while &quot;modality-agnostic decoders may be more likely to leverage representations that are modality-agnostic&quot; (line 140). However, whether a decoder is required to learn modality-agnostic representations depends on both the training responses and the stimulus features. Consider the case where the stimuli are represented using linguistic features of the captions. When you train a &quot;modality-specific&quot; decoder on image responses, the decoder is forced to rely on modality-agnostic information that is shared between the image responses and the caption features. On the other hand, when you train a &quot;modality-agnostic&quot; decoder on both image responses and caption responses, the decoder has access to the modality-specific information that is shared by the caption responses and the caption features, so it is not explicitly required to learn modality-agnostic features. As a result, while the authors show that &quot;modality-agnostic&quot; decoders outperform &quot;modality-specific&quot; decoders in most conditions, I am not convinced that this is because they are forced to learn more modality-agnostic features.</p>
<p>Second, the authors claim that &quot;modality-specific decoders can be applied only in the modality that they were trained on, while &quot;modality-agnostic decoders can be applied to decode stimuli from multiple modalities, even without knowing a priori the modality the stimulus was presented in&quot; (line 47). While &quot;modality-agnostic&quot; decoders do outperform &quot;modality-specific&quot; decoders in the cross-modality conditions, it is important to note that &quot;modality-specific&quot; decoders still perform better than expected by chance (figure 5). It is also important to note that knowing about the input modality still improves decoding performance even for &quot;modality-agnostic&quot; decoders, since it determines the optimal feature space-it is better to decode brain responses to images using decoders trained on image features, and it is better to decode brain responses to captions using decoders trained on caption features.</p>
</body>
</sub-article>
</article>