<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">89674</article-id>
<article-id pub-id-type="doi">10.7554/eLife.89674</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89674.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A neuronal least-action principle for real-time learning in cortical circuits</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3622-0497</contrib-id>
<name>
<surname>Senn</surname>
<given-names>Walter</given-names>
</name>
<xref ref-type="author-notes" rid="n1">*</xref>
<xref ref-type="aff" rid="a1">a</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dold</surname>
<given-names>Dominik</given-names>
</name>
<xref ref-type="author-notes" rid="n1">*</xref>
<xref ref-type="aff" rid="a1">a</xref>
<xref ref-type="aff" rid="a2">b</xref>
<xref ref-type="aff" rid="a3">c</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kungl</surname>
<given-names>Akos F.</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
<xref ref-type="aff" rid="a2">b</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ellenberger</surname>
<given-names>Benjamin</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
<xref ref-type="aff" rid="a6">f</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jordan</surname>
<given-names>Jakob</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bengio</surname>
<given-names>Yoshua</given-names>
</name>
<xref ref-type="aff" rid="a4">d</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sacramento</surname>
<given-names>João</given-names>
</name>
<xref ref-type="aff" rid="a5">e</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Petrovici</surname>
<given-names>Mihai A.</given-names>
</name>
<xref ref-type="author-notes" rid="n1">*</xref>
<xref ref-type="aff" rid="a1">a</xref>
<xref ref-type="aff" rid="a2">b</xref>
</contrib>
<aff id="a1"><label>a</label><institution>Department of Physiology, University of Bern</institution></aff>
<aff id="a2"><label>b</label><institution>Kirchhoff-Institute for Physics, Heidelberg University</institution></aff>
<aff id="a3"><label>c</label><institution>European Space Research and Technology Centre, European Space Agency</institution></aff>
<aff id="a4"><label>d</label><institution>MILA, University of Montreal</institution></aff>
<aff id="a5"><label>e</label><institution>Department of Computer Science</institution>, ETH Zurich</aff>
<aff id="a6"><label>f</label><institution>Insel Data Science Center, University Hospital Bern</institution></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>École Normale Supérieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>#</label>Corresponding author; email: <email>senn@pyl.unibe.ch</email></corresp>
<fn id="n1" fn-type="equal"><label>*</label><p>Equal contribution</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-08-22">
<day>22</day>
<month>08</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP89674</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-06-05">
<day>05</day>
<month>06</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-06-06">
<day>06</day>
<month>06</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.25.534198"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Senn et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Senn et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-89674-v1.pdf"/>
<abstract>
<title>A<sc>bstract</sc></title>
<p>One of the most fundamental laws of physics is the principle of least action. Motivated by its predictive power, we introduce a neural least-action principle that we apply to motor control. The central notion is the somato-dendritic mismatch error within individual neurons. The principle postulates that the somato-dendritic mismatch errors across all neurons in a cortical network are minimized by the voltage dynamics. Ongoing synaptic plasticity reduces the somato-dendritic mismatch error within each neuron and performs gradient descent on the output cost in real time. The neuronal activity is prospective, ensuring that dendritic errors deep in the network are prospectively corrected to eventually reduce motor errors. The neuron-specific errors are represented in the apical dendrites of pyramidal neurons, and are extracted by a cortical microcircuit that ‘explains away’ the feedback from the periphery. The principle offers a general theoretical framework to functionally describe real-time neuronal and synaptic processing.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>- Label in Fig. 5 b2 corrected.
- Additional citations in the Intro on the least action principle in neural network and neuroscience literature.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Wigner’s remark about the ‘unreasonable effectiveness’ of mathematics in allowing us to understand physical phenomena (<xref ref-type="bibr" rid="c69">Wigner, 1959</xref>) is famously contrasted by Gelfand’s quip about its ‘unreasonable ineffectiveness’ in doing the same for biology (<xref ref-type="bibr" rid="c10">Borovik, 2021</xref>). Considering the component of randomness that is inherent to evolution, this may not be all that surprising. However, while this argument holds just as well for the brain at the cellular level, ultimately brains are computing devices. At the level of computation, machine learning and neuroscience have revealed near-optimal strategies for information processing and storage, and evolution is likely to have found similar principles through trial and error (<xref ref-type="bibr" rid="c23">Hassabis <italic>et al</italic>., 2017</xref>). Thus, we have reason to hope for the existence of fundamental principles of cortical computation that are similar to those we have found in the physical sciences. Eventually, it is important for such approaches to relate these principles back to brain phenomenology and connect function to structure and dynamics.</p>
<p>In physics, a fundamental measure of ‘effort’ is the action of a system, which nature seeks to ‘minimize’. Given an appropriate description of interactions between the systems constituents, the least-action principle can be used to derive the equations of motion of any physical system (<xref ref-type="bibr" rid="c17">Feynman <italic>et al</italic>., 2011</xref>; <xref ref-type="bibr" rid="c12">Coopersmith, 2017</xref>). Here, we suggest that in biological information processing, a similar principle holds for prediction errors, which are of obvious relevance for cognition and behavior. Based on such errors, we formulate a neuronal least-action (NLA) principle which can be used to derive neuronal dynamics and map them to observed dendritic morphologies and cortical microcircuits. Within this framework, local synaptic plasticity at basal and apical dendrites can be derived by stochastic gradient descent on errors. The errors that are minimized refer to the errors in output neurons that are typically thought to represent motor trajectories, planned and encoded in cortical motor areas and ultimately in the spinal cord and muscles. In the context of motor control, a phenomenological minimal action principle has previously been proposed that guides the planning and execution of movements (<xref ref-type="bibr" rid="c16">Feldman &amp; Levin, 2009</xref>). The present paper formalizes this principle and reformulates the classical equilibrium point hypothesis (<xref ref-type="bibr" rid="c33">Latash, 2010</xref>) in a dynamical setting. Other attempts exist to link neural networks with the least action principle, for instance by directly learning to reproduce a given trajectory (<xref ref-type="bibr" rid="c4">Amirikian &amp; Lukashin, 1992</xref>), by minimizing data transport through a network (<xref ref-type="bibr" rid="c28">Karkar <italic>et al</italic>., 2021</xref>), or by minimizing the free energy (<xref ref-type="bibr" rid="c18">Friston, 2010</xref>; <xref rid="c19" ref-type="bibr">Friston <italic>et al</italic>., 2022</xref>).</p>
<p>Our theory considers the minimization of a mismatch between the effective output trajectory and an internally or externally imposed target trajectory. The strength by which the output trajectory is pushed towards the target trajectory is referred to as ‘nudging strength’. For weak nudging, the feedback slightly adapts the internal network states so that the output activities closer follow their targets. For a target that is fixed in time, the dynamics reduces to the Equilibrium Propagation (<xref ref-type="bibr" rid="c53">Scellier &amp; Bengio, 2017</xref>). For strong nudging, the output neurons are clamped to tightly follow the target trajectory (<xref rid="c56" ref-type="bibr">Song <italic>et al</italic>., 2022</xref>; <xref rid="c43" ref-type="bibr">Meulemans, Zucchet, <italic>et al</italic>., 2022</xref>). Each network neuron performs gradient descent on its own dendritic prediction error (<xref ref-type="bibr" rid="c64">Urbanczik &amp; Senn, 2014</xref>), and with this eventually also contributes to a reduction of the output errors. For both versions, weak and strong nudging, synaptic plasticity in an interneuron feedback circuitry supports the extraction of neuron-specific dendritic errors (<xref ref-type="bibr" rid="c52">Sacramento <italic>et al</italic>., 2018</xref>), providing a cortical implementation of the suggested ‘real-time dendritic error propagation (rt-DeEP)’.</p>
<p>The NLA principle can be put into the history of energy-based models used to understand neuronal processing and learning in recurrent neural networks (<xref ref-type="bibr" rid="c27">Hopfield, 1982</xref>), artificial intelligence (<xref ref-type="bibr" rid="c36">LeCun <italic>et al</italic>., 2006</xref>), and biological versions of deep learning (<xref ref-type="bibr" rid="c53">Scellier &amp; Bengio, 2017</xref>; <xref rid="c50" ref-type="bibr">Richards <italic>et al</italic>., 2019</xref>; <xref rid="c56" ref-type="bibr">Song <italic>et al</italic>., 2022</xref>). As a continuous-time theory of cortical processing that instantaneously corrects neuronal activities throughout the network, it makes a link to optimal feedback control (<xref ref-type="bibr" rid="c62">Todorov &amp; Jordan, 2002</xref>) that has recently been considered in terms of energy-based models at equilibria (<xref rid="c43" ref-type="bibr">Meulemans, Zucchet, <italic>et al</italic>., 2022</xref>). It can further be seen in the tradition of predictive coding (<xref ref-type="bibr" rid="c49">Rao &amp; Ballard, 1999</xref>; <xref rid="c7" ref-type="bibr">Bastos <italic>et al</italic>., 2012</xref>), where cortical feedback connections try to explain away lower-level activities. Yet, our prospective coding extrapolates from current quantities to predict activity in the future, not the activity at the current point in time, as in classical predictive coding. The NLA principle combines energy-based models with prospective coding in which neuronal integration delays are compensated on the fly (as also done in <xref rid="c22" ref-type="bibr">Haider <italic>et al</italic>., 2021</xref>). The prospective coding within each neuron may overcome putative delays that extend across the whole cortex and would allow for a real-time processing of stimuli while instantaneously correcting for ongoing errors. In this sense, the NLA represents an alternative to the recently suggested forward-forward algorithm which circumvents the delay problem by avoiding error propagation altogether (<xref ref-type="bibr" rid="c25">Hinton, 2022</xref>).</p>
<p>The paper is organized as follows: we first define the prospective somato-dendritic mismatch error, construct out of this the mismatch energy of a network, and ‘minimise’ this energy to obtain the error-corrected, prospective voltage dynamics of the network neurons. We then show that the prospective error coding leads to an instantaneous and joint processing of low-pass filtered input signals and backpropagated errors. Applied to motor control, the instantaneous processing is interpreted as a moving equilibrium hypothesis according to which sensory inputs, network state, motor commands and muscle feedback are in a self-consistent equilibrium at any point of the movement. We then derive a local learning rule that globally minimizes the somato-dendritic mismatch errors across the network, and show how this learning can be implemented through error-extracting cortical microcircuits and dendritic predictive plasticity.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Somato-dendritic mismatch errors and the Lagrangian of cortical circuits</title>
<p>We consider a network of neurons – identified as pyramidal cells – with firing rates <italic>r</italic><sub><italic>i</italic></sub>(<italic>t</italic>) in continuous time <italic>t</italic>. The somatic voltage <italic>u</italic><sub><italic>i</italic></sub> of pyramidal neuron <italic>i</italic> is driven by the close-by basal input current, ∑<sub><italic>j</italic></sub> <italic>W</italic><sub><italic>ij</italic></sub><italic>r</italic><sub><italic>j</italic></sub>, with presynaptic rates <italic>r</italic><sub><italic>j</italic></sub> and synaptic weights <italic>W</italic><sub><italic>ij</italic></sub>, and an additional distal apical input <italic>e</italic><sub><italic>i</italic></sub> that will be learned to represent a prospective prediction error at any moment in time (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). While in classical rate-based neuron models the firing rate <italic>r</italic><sub><italic>i</italic></sub> of a neuron is a function of the somatic voltage, <italic>ρ</italic>(<italic>u</italic><sub><italic>i</italic></sub>), we postulate that the firing rate is prospective and extrapolates from <italic>ρ</italic>(<italic>u</italic><sub><italic>i</italic></sub>) into the future with the temporal derivative, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline1.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline2.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> representing the temporal derivative of <italic>ρ</italic>(<italic>u</italic><sub><italic>i</italic></sub>(<italic>t</italic>)). There is experimental evidence for such prospective coding in cortical pyramidal neurons to which we return later (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Somato-dendritic mismatch energies and the neuronal least-action (NLA) principle.</title>
<p><bold>(a1)</bold> Sketch of a cross-cortical network of pyramidal neurons described by NLA. <bold>(a2)</bold> Correspondence between elements of NLA and biological observables such as membrane voltages and synaptic weights. <bold>(b1)</bold> The NLA principle postulates that small variations <italic>δ</italic><bold><italic>ũ</italic></bold> (dashed) of the trajectories <bold><italic>ũ</italic></bold> (solid) leave the action invariant, <italic>δA</italic> = 0. It is formulated in the look-ahead coordinates <bold><italic>ũ</italic></bold> (symbolized by the spyglass) in which ‘hills’ of the Lagrangian (shaded grey zones) are foreseen by the prospective voltage so that the trajectory can turn by early enough to surround them. <bold>(b2)</bold> In the absence of output nudging (<italic>β</italic> = 0), the trajectory <bold><italic>u</italic></bold>(<italic>t</italic>) is solely driven by the sensory input, and prediction errors and energies vanish (<italic>L</italic> = 0, outer blue trajectory at bottom). When nudging the output neurons towards a target voltage (<italic>β &gt;</italic> 0), somato-dendritic prediction errors appear, the energy increases (red dashed arrows symbolising the growing ‘volcano’) and the trajectory <bold><italic>u</italic></bold>(<italic>t</italic>) moves out of the <italic>L</italic> = 0 hyperplane, riding on top of the ‘volcano’ (red trajectory). Synaptic plasticity <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline538.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> reduces the somato-dendritic mismatch along the trajectory by optimally ‘shoveling down the volcano’ (blue dashed arrows) while the trajectory settles in a new place on the <italic>L</italic> = 0 hyperplane (inner blue trajectory at bottom).</p></caption>
<graphic xlink:href="534198v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Prospective coding in cortical pyramidal neurons enables instantaneous voltage-to-voltage transfer.</title>
<p><bold>(a1)</bold> The instantaneous spike rate of cortical pyramidal neurons (top) in response to sinusoidally modulated noisy input current (bottom) is phase-advanced with respect to the input (adapted from Köndgen <italic>et al</italic>., 2008). <bold>(a2)</bold> Similiarly, in NLA, the instantaneous firing rate of a model neuron <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline539.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, black) is phase-advanced with respect to the underlying voltage (<italic>u</italic>, red, postulating that the low-pass filtered rate is a function of the voltage, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline540.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>). <bold>(b)</bold> Dendritic input in the apical tree (here called <italic>ē</italic>) is instantaneously causing a somatic voltage modulation (<italic>u</italic>, (modeling data from <xref ref-type="bibr" rid="c63">Ulrich, 2002</xref>)). The low-pass filtering with <italic>τ</italic> along the dendritic shaft is compensated by a lookahead mechanism in the dendrite <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline541.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. In <xref ref-type="bibr" rid="c63">Ulrich (2002)</xref> a phase advance is observed even with respect to the dendritic input current, not only the dendritic voltage, although only for slow modulations (as here). <bold>(c)</bold> While the voltage of the first neuron (<italic>u</italic><sub>1</sub>) integrates the input rates <italic>r</italic><sub>in</sub> from the past (bottom black upward arrows), the output rate <italic>r</italic><sub>1</sub> of that first neuron looks ahead in time, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline542.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (red dashed arrows pointing into the future). The voltage of the second neuron (<italic>u</italic><sub>2</sub>) integrates the prospective rates <italic>r</italic><sub>1</sub> (top black upwards arrows). By doing so, it inverts the lookahead operation, resulting in an instantaneous transfer from <italic>u</italic><sub>1</sub>(<italic>t</italic>) to <italic>u</italic><sub>2</sub>(<italic>t</italic>) (blue arrow and circles).</p></caption>
<graphic xlink:href="534198v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The second central notion of the theory is the prospective somato-dendritic mismatch error in the individual network neurons, <italic>e</italic><sub><italic>i</italic></sub>(<italic>t</italic>). It is defined as a mismatch between the prospective voltage, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline3.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and the weighted prospective input rates, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline4.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. If the prospective error is low-pass filtered with time constant <italic>τ</italic>, it takes the form <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline5.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline6.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the corresponding low-pass filtered firing rate of the presynaptic neuron <italic>j</italic> (Methods). We refer to <italic>ē</italic><sub><italic>i</italic></sub> as somato-dendritic mismatch error of neuron <italic>i</italic>.</p>
<p>We next interpret the mismatch error <italic>ē</italic><sub><italic>i</italic></sub> in terms of the morphology of pyramidal neurons with basal and apical dendrites. While the error is formed in the apical dendrite, this error is added to the somatic voltage and, from the perspective of the basal inputs, it becomes a somato-dendritic mismatch error. The mismatch error tells the difference between ‘what a neuron does’, which is based on the somatic voltage <italic>u</italic><sub><italic>i</italic></sub>, and ‘what the basal inputs think it should do’, which is based on its own input <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline7.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (<xref ref-type="fig" rid="fig1">Fig. 1a2</xref>). The two quantities may deviate because the neuron gets the apical input <italic>ē</italic><sub><italic>i</italic></sub> that integrates in the soma, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline8.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula><sub><italic>i</italic></sub>, but not in the basal dendrite. What cannot be predicted from the somatic voltage <italic>u</italic><sub><italic>i</italic></sub> by the basal input remains as ‘somato-basal’ mismatch error, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline9.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and this is just the apical error <italic>ē</italic><sub><italic>i</italic></sub>.</p>
<p>Associated with this mismatch error is the somato-dendritic mismatch energy defined for each network neuron <italic>i</italic> ∈ 𝒩 as the squared mismatch error,
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="534198v2_eqn1.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
On a subset of output neurons of the whole network, 𝒪 ⊆ 𝒩, a cost is defined as a function of the somatic voltage and some instructive reference signal such as a targets or a reward. When a target trajectory <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline10.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is available, the cost is defined at each time point as a squared target error
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="534198v2_eqn2.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Much more general cost functions and mismatch energies are conceivable, encompassing e.g. conductance-based neurons or including further dynamic variables (see SI). The cost represents a performance measure for the entire network that produces the output voltages <italic>u</italic><sub><italic>o</italic></sub>(<italic>t</italic>) in response to some input rates <bold><italic>r</italic></bold><sub>in</sub>(<italic>t</italic>). The cost directly relates to behavioral or cognitive measures such as the ability of an animal or human to perform a particular task in real time. The target could be provided by explicit external supervision, for example target movements in time encoded by <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline11.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, it could represent an expected reward signal, or it could arise via self-supervision from other internal prediction errors.</p>
<p>We define the Lagrangian (or total ‘energy’) of the network as a sum across all mismatch energies and costs, weighted by the nudging strength <italic>β</italic> of the output neurons,
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="534198v2_eqn3.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The low-pass filtered presynaptic rates, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline12.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, also encompass the external input neurons. Due to the prospective coding, the Lagrangian can be minimal at any moment in time while the network dynamics evolves. This is different from the classical predictive coding (<xref ref-type="bibr" rid="c49">Rao &amp; Ballard, 1999</xref>) and energy-based approaches (<xref ref-type="bibr" rid="c53">Scellier &amp; Bengio, 2017</xref>; <xref rid="c56" ref-type="bibr">Song <italic>et al</italic>., 2022</xref>), where a stimulus needs to be fixed in time while the network relaxes to a steady state, and only there the prediction error is minimized. Since we postulated that the firing rates are prospective and linearly extrapolate into the future, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline13.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and because any quantity can be reconstructed from its low-pass filtering at any point in time, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline14.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, we conclude that the low-pass filtered rate is a function of the instantaneous somatic voltage, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline15.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, see SI. As a consequence, also the Lagrangian becomes a function of the instantaneous voltage <italic>u</italic>(<italic>t</italic>) and does not depend on past voltage values, although it depends on past firing rates.</p>
<p>The previous mathematical operation of extrapolating from the voltage to future rates, and going back to the current voltage via low-pass filtering, is the key ingredient for the magically sounding instantaneous processing, see <xref rid="fig2" ref-type="fig">Fig. 2c</xref>.</p>
<p>To get the instantaneity it is not really necessary to precisely look into the future. It is only necessary to undo the temporal operation of the postsynaptic neuron, and this is achieved by encoding the presynaptic voltage in the prospective rate. We come back to this instantaneity, including its practical limitations, in the overnext section.</p>
</sec>
<sec id="s2b">
<title>The least-action principle expressed for prospective voltages</title>
<p>Motivated by the prospective firing in pyramidal neurons, we postulate that cortical networks strive to look into the future to prevent instantaneous errors. Each neuron tries to move along a trajectory that minimizes its own mismatch error <italic>ē</italic><sub><italic>i</italic></sub> across time (<xref rid="fig1" ref-type="fig">Fig. 1b1</xref>). The ‘neuronal currency’ with which each neuron ‘trades’ with others to choose its own error-minimizing trajectory is the future discounted membrane potential,
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="534198v2_eqn4.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
These prospective voltages <italic>ũ</italic> are the ‘canonical coordinates’ entering the NLA principle, and in these prospective coordinates the overall network searches for a ‘least action trajectory’. Since from <italic>ũ</italic> we can recover the instantaneous voltage via <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline16.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (see SI), we can replacIe <italic>u</italic> inlthe Lagrangian and obtain <italic>L</italic> as a function of our new canonical coordinates <italic>ũ</italic> and the ‘velocities’ <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline17.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, i.e. <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline18.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where bold fonts represent vectors. Inspired by the least-action principle from physics, we can now define the neuronal action <italic>A</italic> as a time integral of the Lagrangian,
<disp-formula id="eqn5">
<alternatives><graphic xlink:href="534198v2_eqn5.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The NLA principle postulates that the trajectory <bold><italic>ũ</italic></bold>(<italic>t</italic>) keeps the action <italic>A</italic> stationary with respect to small variations <italic>δ</italic><bold><italic>ũ</italic></bold> (<xref rid="fig1" ref-type="fig">Fig. 1b1</xref>). In other words, nature chooses a trajectory such that, when deviating a little bit from it, say by <italic>δ</italic><bold><italic>ũ</italic></bold>, the value of <italic>A</italic> will not change (or at most up to second order in the variation), formally <italic>δA</italic> = 0. The equations of motion that keep the action stationary are known to satisfy the Euler-Lagrange equations
<disp-formula id="eqn6">
<alternatives><graphic xlink:href="534198v2_eqn6.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Applying these equations to our Lagrangian yields a prospective version of the classical leaky integrator voltage dynamics, with rates <bold><italic>r</italic></bold> and errors <bold><italic>e</italic></bold> that are looking into the future (Methods),
<disp-formula id="eqn7a">
<alternatives><graphic xlink:href="534198v2_eqn7a.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn7b">
<alternatives><graphic xlink:href="534198v2_eqn7b.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The ‘ ·’ denotes the component-wise product, and the weight matrix splits into weights from input neurons and weights from network neurons, <bold><italic>W</italic></bold> = (<bold><italic>W</italic></bold><sub>in</sub>, <bold><italic>W</italic></bold><sub>net</sub>). While for output neurons a target error can be defined, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline19.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, for non-output neurons <italic>i</italic> no target exist and we hence set <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline20.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. In a control theoretic framework, the neuronal dynamics (<xref ref-type="disp-formula" rid="eqn7a">Eq. 7a</xref>) represents the state trajectory, and the adjoint error dynamics (<xref ref-type="disp-formula" rid="eqn7b">Eq. 7b</xref>) represents the integrated costate trajectory (<xref ref-type="bibr" rid="c60">Todorov, 2006</xref>).</p>
<p>Formally, the voltage dynamics in <xref ref-type="disp-formula" rid="eqn7a">Eq. 7a</xref> specifies an implicit differential equation since <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline21.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> also appears on the right-hand side. This is because the prospective rates <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline22.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> imply <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline23.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> through <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline24.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Likewise, the prospective errors <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline25.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with <bold><italic>ē</italic></bold> given in <xref ref-type="disp-formula" rid="eqn7b">Eq. 7b</xref> and plugged into <xref ref-type="disp-formula" rid="eqn7a">Eq. 7a</xref>, imply <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline26.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> through <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline27.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Nevertheless, the voltage dynamics can be run by replacing <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline28.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> on the right-hand side of <xref ref-type="disp-formula" rid="eqn7a">Eq. 7a</xref> by the temporal derivative <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline29.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> from the previous time step (technically, the Hessian (<bold>1 − <italic>Wρ</italic></bold>′ − <bold><italic>ē</italic></bold>′) is required to be strictly positive definite, see Methods and SI). This ensures that the voltage dynamics of <xref ref-type="disp-formula" rid="eqn7a">Eq. 7</xref> can be implemented in cortical neurons with a prospective firing and a prospective dendritic error (see <xref rid="fig2" ref-type="fig">Fig. 2</xref>).</p>
<p>The error expression in <xref ref-type="disp-formula" rid="eqn7b">Eq. 7b</xref> is reminiscent of error backpropagation (<xref ref-type="bibr" rid="c51">Rumelhart <italic>et al</italic>., 1986</xref>) and can in fact be mathematically linked to this (Methods). Formally, the errors are backpropagated via transposed network matrix, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline30.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, modulated by <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline31.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the derivative of <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline32.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> with respect to the underlying voltage. While the transpose can be constructed with various local methods (see <xref ref-type="bibr" rid="c3">Akrout <italic>et al</italic>., 2019</xref>; <xref rid="c40" ref-type="bibr">Max <italic>et al</italic>., 2022</xref>) in our simulations we mainly adhere to the phenomenon of feedback alignment (<xref rid="c38" ref-type="bibr">Lillicrap, Cownden, <italic>et al</italic>., 2016</xref>) and consider fixed and randomized feedback weights <bold><italic>B</italic></bold> (unless stated differently). Recent control theoretical work is exploiting the same prospective coding technique as expressed in <xref ref-type="disp-formula" rid="eqn7a">Eq. 7</xref> to tackle general time-varying optimization problems (see <xref ref-type="bibr" rid="c55">Simonetto <italic>et al</italic>., 2020</xref> for a review and the SI for the detailed connection).</p>
</sec>
<sec id="s2c">
<title>Prospective coding in neurons and instantaneous propagation</title>
<p>The prospective rates and errors entering via <bold><italic>r</italic></bold> and <bold><italic>e</italic></bold> in the NLA (<xref ref-type="disp-formula" rid="eqn7a">Eq. 7</xref>) are consistent with the prospective coding observed in cortical pyramidal neurons <italic>in vitro</italic> (<xref rid="c30" ref-type="bibr">Köndgen <italic>et al</italic>., 2008</xref>). Upon sinusoidal current injection into the soma, the somatic firing rate is advanced with respect to its voltage (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>), effectively compensating for the delay caused by the current integration. Likewise, sinusoidal current injection in the apical tree causes a lag-less voltage response in the soma (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>, <xref ref-type="bibr" rid="c63">Ulrich, 2002</xref>). While the rates and errors in general can be reconstructed from their low-pass filterings via <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline33.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline34.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, they become prospective in time because <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline35.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <bold><italic>ē</italic></bold> are themselves instantaneous functions of the voltage <bold><italic>u</italic></bold>, and hence <bold><italic>r</italic></bold> and <bold><italic>e</italic></bold> depend on <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline36.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The derivative of the membrane potential implicitly also appears in the firing mechanism of Hodgkin-Huxley-type conductances, with a quick depolarization leading to a stronger sodium influx due to the dynamics of the gating variables (<xref ref-type="bibr" rid="c26">Hodgkin &amp; Huxley, 1952</xref>). This advances the action potential as compared to a firing that would only depend on <bold><italic>u</italic></bold>, not <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline37.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, giving an intuition of how such a prospective coding may arise. A similar prospective coding has been observed for retinal ganglion cells (<xref ref-type="bibr" rid="c45">Palmer <italic>et al</italic>., 2015</xref>) and cerebellar Purkinje cells (<xref rid="c44" ref-type="bibr">Ostojic <italic>et al</italic>., 2015</xref>), making a link from the visual input to the motor control.</p>
<p>To understand the instantaneous propagation through the network, we low-pass filter the dynamic equation <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline38.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (obtained by rearranging <xref ref-type="disp-formula" rid="eqn7a">Eq. 7a</xref>), with <bold><italic>ē</italic></bold> given by <xref ref-type="disp-formula" rid="eqn7b">Eq. 7b</xref>, to obtain the somatic voltage <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline39.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. At any point in time, the voltage is in a moving equilibrium between forward and backpropagating inputs. Independently of the network architecture, whether recurrent or not, the output is an instantaneous function of the low-pass filtered input and a putative correction towards the target, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline40.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, see <xref rid="fig2" ref-type="fig">Fig. 2c</xref> and Methods. The mapping again expresses an instantaneous propagation of voltages throughout the network in response to both, the low-pass filtered input <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline41.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and feedback error <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline42.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This instantaneity is independent of the network size, and in a feed-forward network is independent of its depths (see also <xref rid="c22" ref-type="bibr">Haider <italic>et al</italic>., 2021</xref>, where the instantaneity is on the rates, not the voltages). In the absence of the look-ahead activity, each additional layer slows down the network relaxation time.</p>
<p>Notice that an algorithmic implementation of the time-continuous dynamics of a <italic>N</italic> -layer feedforward network would still need <italic>N</italic> calculation steps until information from layer 1 reaches layer <italic>N</italic>. However, this does not imply that an analog implementation of the prospective dynamics will encounter delays. To see why, consider a finite step-change Δ<bold><italic>u</italic></bold><sub>1</sub> in the voltage of layer 1. In the absence of the look-ahead, Δ<bold><italic>u</italic></bold><sub>1</sub> were mapped within the infinitesimal time interval <italic>dt</italic> to an infinitesimal change <italic>d</italic><bold><italic>u</italic></bold><sub>2</sub> in the voltages of layer 2. But with a prospective firing rate, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline43.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, a step-change Δ<bold><italic>u</italic></bold><sub>1</sub> translates to a delta-function in <bold><italic>r</italic></bold><sub>1</sub>, this in turn to a step-change in the low-pass filtered rates <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline44.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and therefore within <italic>dt</italic> to a step-change Δ<bold><italic>u</italic></bold><sub>2</sub> in the voltages <bold><italic>u</italic></bold><sub>2</sub> of the postsynaptic neurons (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>). Iterating this argument, a step-change Δ<bold><italic>u</italic></bold><sub>1</sub> propagates ‘instantaneously’ through <italic>N</italic> layers within the ‘infinitesimal’ time interval <italic>N dt</italic> to a step-change Δ<bold><italic>u</italic></bold><sub><italic>N</italic></sub> in the last layer. When run in a biophysical device in continuous time that exactly implements the dynamical equations (<xref ref-type="disp-formula" rid="eqn7a">Eq. 7</xref>), the implementation becomes an instantaneous computation. Yet, in a biophysical device information has to be moved across space. This typically introduces further propagation delays that may not be captured in our formalism where low-pass filtering and prospective coding cancel each other exactly. Nevertheless, analog computation in continuous time, as formalized here, offers an idea to ‘instantaneously’ realize an otherwise time consuming numerical recipe run on time-discrete computing systems that operate with a finite clock cycle.</p>
</sec>
<sec id="s2d">
<title>Prospective control and the moving equilibrium hypothesis</title>
<p>Crucially, at the level of the voltage dynamics (<xref ref-type="disp-formula" rid="eqn7a">Eq. 7a</xref>) the correction is based on the prospective error <bold><italic>e</italic></bold>. This links our framework to optimal control theory and motor control where delays are also taken into account, so that a movement can be corrected early enough (<xref ref-type="bibr" rid="c70">Wolpert &amp; Ghahramani, 2000</xref>; <xref ref-type="bibr" rid="c62">Todorov &amp; Jordan, 2002</xref>; <xref ref-type="bibr" rid="c61">Todorov, 2004</xref>). The link between energy-based models and optimal control was recently drawn for strong nudging (<italic>β</italic> → ∞) to learn individual equilibrium states (<xref rid="c43" ref-type="bibr">Meulemans, Zucchet, <italic>et al</italic>., 2022</xref>). Our prospective error <bold><italic>e</italic></bold>(<italic>t</italic>) appears as a ‘controller’ that, when looking at the output neurons, pushes the voltage trajectories towards the target trajectories. Depending on the nudging strength <italic>β</italic>, the control is tighter or weaker. For infinitely large <italic>β</italic>, the voltages of the output neurons are clamped to the time-dependent target voltages, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline45.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (implying <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline46.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>), while their errors, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline47.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, instantaneously correct all network neurons. For small <italic>β</italic>, the output voltages are only weakly controlled, and they are dominated by the forward input, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline48.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>In the context of motor control, our network mapping <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline49.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> can be seen as a forward internal model that quickly calculates an estimate of the future muscle length <bold><italic>u</italic></bold><sub><bold><italic>o</italic></bold></sub> based on some motor plans, sensory inputs, and the current proprioceptive feedback (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>). Forward models help to overcome delays in the execution of the motor plan by predicting the outcome, so that the intended motor plans and commands can be corrected on the fly (<xref ref-type="bibr" rid="c29">Kawato, 1999</xref>; <xref ref-type="bibr" rid="c70">Wolpert &amp; Ghahramani, 2000</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Moving equilibrium hypothesis for motor control and real-time learning of cortical activity.</title>
<p><bold>(a)</bold> A voluntary movement trajectory can be specified by the target length of the muscles in time, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline543.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, encoded through the <italic>γ</italic>-innervation of muscle spindles, and the deviation of the effective muscle lengths from the target, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline544.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The <italic>I</italic><sub><italic>a</italic></sub>-afferents emerging from the spindles prospectively encode the error, so that their low-pass filtering is roughly proportional to the length deviation, truncated at zero (red). The moving equilibrium hypothesis states that the low-pass filtered input <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline545.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, composed of the movement plan <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline546.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and the sensory input (here encoding the state of the plant e.g. through visual and proprioceptive input, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline547.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline548.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>), together with the low-pass filtered error feedback from the spindles, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline549.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, instantaneously generate the muscle lengths, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline550.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and are thus at any point in time in an instantaneous equilibrium (defined by <xref ref-type="disp-formula" rid="eqn7a">Eq. 7</xref>). <bold>(b1)</bold> Intracortical iEEG activity recorded from 56 deep electrodes and projected to the brain surface. Red nodes symbolize the 56 iEEG recording sites modeled alternately as input or output neurons, and blue nodes symbolize the 40 ‘hidden’ neurons for which no data is available, but used to reproduce the iEEG activity. <bold>(b2)</bold> Corresponding NLA network. During training, the voltages of the output neurons were nudged by the iEEG targets (black input arrows, but for all red output neurons). During testing, nudging was removed for 14 out of these 56 neurons (here, represented by neurons 1, 2, 3). <bold>(c1)</bold> Voltage traces for the 3 example neurons in a2, before (blue) and after (red) training, overlaid with their iEEG target traces (grey). <bold>(c2)</bold> Total cost, integrated over a window of 8 s of the 56 output nodes during training with sequences of the same duration. The cost for the test sequences was evaluated on a 8 s window not used during training.</p></caption>
<graphic xlink:href="534198v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The observation that muscle spindles prospectively encode the muscle length and velocity (<xref ref-type="bibr" rid="c15">Dimitriou &amp; Edin, 2010</xref>)) suggests that the prospective coding in the internal forward model mirrors the prospective coding in the effective forward pathway. This forward pathway leads from motor plan to spindle feedback, integrating also cerebellar and brainstem feedback (<xref ref-type="bibr" rid="c29">Kawato, 1999</xref>). Based on the motor plans, the intended spindle lengths and the effective muscle innervation are communicated via descending pathway to activate the <italic>γ</italic>-and <italic>α</italic>-motoneurons, respectively (<xref rid="c37" ref-type="bibr">Li <italic>et al</italic>., 2015</xref>). The transform from the intended arm trajectory to the intended spindle lengths via <italic>γ</italic>-innervation is mainly determined by the joint geometry. The transform from the intended arm trajectory to the force generating <italic>α</italic>-innervation, however, needs to also take account of the internal and external forces, and this is engaging our network <bold><italic>W</italic></bold>.</p>
<p>When we prepare an arm movement, spindles in antagonistic muscles pairs that measure the muscle length are tightened or relaxed before the movement starts (<xref ref-type="bibr" rid="c46">Papaioannou &amp; Dimitriou, 2021</xref>). According to the classical equilibrium-point hypothesis (<xref ref-type="bibr" rid="c16">Feldman &amp; Levin, 2009</xref>; <xref ref-type="bibr" rid="c33">Latash, 2010</xref>), top-down input adjusts the activation threshold of the spindles through (<italic>γ</italic>-)innervation from the spinal cord so that slight deviations from the equilibrium position can be signaled (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>). We postulate that this <italic>γ</italic>-innervation acts also during the movement, setting an instantaneous target <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline50.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for the spindle lengths. The effective lengths of the muscle spindles is <bold><italic>u</italic></bold><sub><bold><italic>o</italic></bold></sub>, and the spindles are prospectively signaling back the deviation from the target through the <italic>I</italic><sub><italic>a</italic></sub>-afferents (<xref ref-type="bibr" rid="c15">Dimitriou &amp; Edin, 2010</xref>; <xref ref-type="bibr" rid="c14">Dimitriou, 2022</xref>). The low-pass filtered <italic>I</italic><sub><italic>a</italic></sub>-afferents may be approximated by a threshold-nonlinearity, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline51.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with <italic>β</italic> being interpreted as spindle gain (<xref ref-type="bibr" rid="c34">Latash, 2018</xref>). Combining the feedback from agonistic and antagonistic muscle pairs allows for extracting the scaled target error <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline52.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Taking account of the prospective feedback, we postulate the <italic>moving equilibrium hypothesis</italic> according to which the instructional inputs, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline53.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the spindle feedback, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline54.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and the muscle lengths, <bold><italic>u</italic></bold><sub><bold><italic>o</italic></bold></sub>, are at any point of the movement in a dynamic equilibrium. The moving equilibrium hypothesis extends the classical equilibrium-point hypothesis from the spatial to the temporal domain.</p>
<p>With regard to the interpretation of the prospective feedback error <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline55.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> as spindle activity, it is worth noticing that in humans the spindle activity is not only ahead of the muscle activation (<xref ref-type="bibr" rid="c15">Dimitriou &amp; Edin, 2010</xref>), but also shares the property of a motor error (<xref ref-type="bibr" rid="c13">Dimitriou, 2016</xref>). The experiments show that during the learning of a gated hand movement, spindle activity is initially stronger when making movement errors, and it returns back to baseline with the success of learning. We next address how the synaptic strengths <bold><italic>W</italic></bold> specifying the forward model can be optimally adapted to capture this learning.</p>
</sec>
<sec id="s2e">
<title>Local plasticity at basal synapses minimizes the global cost in real time</title>
<p>The general learning paradigm starts with input time series <italic>r</italic><sub>in(<italic>t</italic>),<italic>i</italic></sub> and target time series<inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline56.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, while assuming that the target series are an instantaneous function of the low-pass filtered input series, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline57.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The low-pass filtering in the individual inputs could be with respect to any time constant <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline58.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (that may also be learned, see SI), but for the network neurons we assume the same time constants for the voltage integration and the prospective rate. The goal of learning is to adapt the synaptic strengths <bold><italic>W</italic></bold> in the student network so that this approximates the target mapping, <bold><italic>F</italic></bold><sub><italic>W</italic></sub> ≈ <bold><italic>F</italic></bold>*. This will also reduce the cost <italic>C</italic> defined on the output neurons in terms of the deviation of the voltage from the target, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline59.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="eqn2">Eq. 2</xref>).</p>
<p>The problem of changing synaptic weights to correct the behavior of downstream neurons, potentially multiple synapses away, is typically referred to as the credit assignment problem and is notoriously challenging in physical or biological substrates operating in continuous time. A core aspect of the NLA principle is how it relates the cost <italic>C</italic> to the Lagrangian <italic>L</italic> and eventually to somato-dendritic prediction errors <bold><italic>ē</italic></bold> that can be reduced through synaptic plasticity <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline60.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. We define this synaptic plasticity as partial derivative of the Lagrangian with respect to the weights, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline61.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Since the somato-dendritic mismatch error is <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline62.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, this leads to the local learning rule of the form ‘postsynaptic error times low-pass filtered presynaptic rate’,
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="534198v2_eqn8.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
This plasticity rule runs simultaneously to the neuronal dynamics in the presence of a given nudging strength <italic>β</italic> that tells how strongly the voltage of an output neurons is pushed towards the target, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline63.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The learning rule is local in space since <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline64.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is represented as voltage of the basal dendrites, and the somatic voltage <bold><italic>u</italic></bold> may be read out at the synaptic site on the basal dendrite from the backpropagating action potentials that sample <bold><italic>u</italic></bold> at a given time (<xref ref-type="bibr" rid="c64">Urbanczik &amp; Senn, 2014</xref>). The basal voltage <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline65.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> becomes the dendritic prediction of the somatic activity <bold><italic>u</italic></bold>, interpreting <xref ref-type="disp-formula" rid="eqn8">Eq. 8</xref> as ‘dendritic predictive plasticity’.</p>
<p>We have derived the neuronal dynamics as a path that keeps the action stationary. Without external teaching signal, the voltage trajectory wriggles on the bottom of the energy landscape (<italic>L</italic> = 0, <xref rid="fig1" ref-type="fig">Fig. 1b1</xref>). If the external nudging is turned on, <italic>β &gt;</italic> 0, errors emerge and hills grow out of the landscape. The trajectory still tries to locally minimize the action, but it is lifted upwards on the hills (<italic>L &gt;</italic> 0, <xref rid="fig1" ref-type="fig">Fig. 1b2</xref>). Synaptic plasticity reshapes the landscape so that, while keeping <italic>β</italic> fixed, the errors are reduced and the landscape again flattens. The transformed trajectory settles anew in another place (inside the ‘volcano’ in <xref rid="fig1" ref-type="fig">Fig. 1b2</xref>). Formally, the local plasticity rule (<xref ref-type="disp-formula" rid="eqn8">Eq. 8</xref>) is shown to perform gradient descent on the Lagrangian. In the energy landscape picture, plasticity ‘shovels off’ earth along the voltage path so that this is lowered most efficiently. The error that is back-propagated through the network tells at any point on the voltage trajectory how much to ‘dig’ in each direction, i.e. how to adapt the basal input in each neuron in order to optimally lower the local error.</p>
<p>The following theorem tells that synaptic plasticity <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline66.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> pushes the network mapping <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline67.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> towards the target mapping <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline68.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> at any moment in time. The convergence of the mapping is a consequence of the fact the plasticity reduces the Lagrangian <italic>L</italic> = <italic>E</italic><sup>M</sup> + <italic>βC</italic> along its gradient.</p>
<statement id="the1">
<label>Theorem 1</label>
<title>(real-time Dendritic Error Propagation, rt-DeEP)</title>
<p><italic>Consider an arbitrary network</italic> <bold><italic>W</italic></bold> <italic>with voltage and error dynamics following <xref ref-type="disp-formula" rid="eqn7a">Eq. 7</xref>. Then the local plasticity rule</italic> <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline69.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> <italic>(<xref ref-type="disp-formula" rid="eqn8">Eq. 8</xref>), acting at each moment along the voltage trajectories, is gradient descent</italic>
<list list-type="roman-lower">
<list-item><p><italic>on the Lagrangian L for any nudging strength β</italic> ≥ 0, <italic>i</italic>.<italic>e</italic>. <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline70.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, <italic>with</italic> <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline71.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p></list-item>
<list-item><p><italic>on the cost C for small nudging, β</italic> → 0, <italic>while up-scaling the error to</italic> <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline72.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, <italic>i</italic>.<italic>e</italic>. <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline73.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p></list-item>
</list></p>
</statement>
<p>The gradient statements hold at any point in time (long enough after initialization), even if the input trajectories <bold><italic>r</italic></bold><sub>in</sub>(<italic>t</italic>) contain delta-functions and the target trajectories <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline74.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> contain step-functions.</p>
<p>Loosely speaking, the NLA enables the network to localize an otherwise global problem: what is good for a single neuron becomes good for the entire network. In the limit of strong nudging (<italic>β</italic>→ ∞), the learning rule performs gradient descent on the mismatch energies <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline75.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> in the individual neurons in the network. If the network architecture is powerful enough so that after learning all the mismatch energies vanish, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline76.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, then the cost will also vanish, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline77.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This is because for the output neurons the mismatch error includes the target error. In the limit of weak nudging (<italic>β →</italic>0), the learning rule performs gradient descent on <italic>C</italic>, and with this also finds a local minimum of the mismatch energies.</p>
<p>In the case of weak nudging and a single equilibrium state, the NLA algorithm reduces to the Equilibrium Propagation algorithm (<xref ref-type="bibr" rid="c53">Scellier &amp; Bengio, 2017</xref>) that minimizes the cost <italic>C</italic> for a constant input and a constant target. In the case of strong nudging and a single equilibrium state, the NLA principle reduces to the Least Control Principle (<xref rid="c43" ref-type="bibr">Meulemans, Zucchet, <italic>et al</italic>., 2022</xref>) that minimizes the mismatch energy <italic>E</italic><sup>M</sup> for a constant input and a constant target. While in the Least Control Principle the inputs and outputs are clamped to fixed values, the output errors are backpropagated and the network equilibrates in a steady state where the corrected network activities reproduce the clamped output activities. This state is called the ‘prospective configuration’ in (<xref rid="c56" ref-type="bibr">Song <italic>et al</italic>., 2022</xref>) because neurons deep in the network are informed about the distal target and are correspondingly adapted to be consistent with this distal target. In the NLA principle, after an initial transient, this relaxation process occurs on the fly while inputs and targets dynamically change, and hence the network moves along a continuous sequence of prospective configurations.</p>
<p>In the motor control example, the theorem tells that a given target motor trajectory <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline78.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is learned to be re-produced by the forward model <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline79.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> through the dendritic predictive plasticity (<xref ref-type="disp-formula" rid="eqn8">Eq. 8</xref>), applied in the various layers of the forward model. We give an example below for such a learning that involves different sensory modalities to train the forward model.</p>
</sec>
<sec id="s2f">
<title>Reproducing intracortical EEG recordings and recognizing handwritten digits</title>
<p>To illustrate the framework, we consider a recurrently connected network that learns to represent intracortical electroencephalogram (iEEG) data from epileptic patients (<xref rid="fig3" ref-type="fig">Fig. 3b</xref>). For each electrode, we assign a neuron within this network to represent the activity of the cell cluster recorded in the corresponding iEEG signal via its membrane potential. During learning, a randomly selected subset of electrode neurons are nudged towards the target activity from recorded data while learned to be reproduced by the other neurons. After learning, we can present only a subset of electrode neurons with previously unseen recordings and observe how the activity of the other neurons closely matches the recordings of their respective electrodes (<xref rid="fig3" ref-type="fig">Fig. 3c</xref>). The network derived from NLA is thus able to learn complex correlations between signals evolving in real time by encoding them into its connectivity structure.</p>
<p>As an example of visual processing in the NLA framework, we next consider a well-studied image recognition task, here reformulated in a challenging time-continuous setting, and interpreted as motor task where 1 out of 10 fingers has to be bent upon seeing a corresponding visual stimulus (see <xref rid="fig3" ref-type="fig">Fig. 3a</xref>). In the context of our moving equilibrium hypothesis, we postulate that during the learning phase, but not the testing phase, an auditory signal identifying the correct finger sets the target spindle lengths of the 10 finger flexors, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline80.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, i.e. a desired contraction for the correct finger in response to the visual input <bold><italic>r</italic></bold><sub>in</sub>(<italic>t</italic>), and a desired relaxation for the 9 incorrect fingers.</p>
<p>We train a hierarchical three-layer network on images of handwritten digits (MNIST, <xref ref-type="bibr" rid="c35">LeCun, 1998</xref>), with image presentation times between 0.5<italic>τ</italic> (= 5 ms, with <italic>τ</italic> the membrane time constant) and 20<italic>τ</italic> (= 200 ms). <xref rid="fig4" ref-type="fig">Fig. 4a-c1</xref> depict the most challenging scenario with the shortest presentation time. Synaptic plasticity is continuously active, despite the network never reaching a steady state (<xref rid="fig4" ref-type="fig">Fig. 4b1</xref>). Due to the lookahead firing rates in NLA, the mismatch errors <italic>ē</italic><sub><italic>i</italic></sub>(<italic>t</italic>) propagate without lag throughout the network, as explained above. As a consequence, our mismatch errors are equal to the errors obtained from classical error backpropagation applied at each time step to the purely forward network, without error-correcting the voltage, and instead, at each layer <italic>l</italic> considering only the forward voltage updates <bold><italic>u</italic></bold><sub><italic>l</italic></sub> = <bold><italic>W</italic></bold><sub><italic>l</italic></sub> <italic>ρ</italic>(<bold><italic>u</italic></bold><sub><italic>l</italic>−1</sub>), see <xref rid="fig4" ref-type="fig">Fig. 4b2</xref>. The network eventually learned to implement the mapping <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline81.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> with a performance comparable to error-backpropagation at each <italic>dt</italic>, despite the short presentation time of only 5 ms (<xref rid="fig4" ref-type="fig">Fig. 4c1</xref>). The limiting stimulus presentation time in the order of <italic>τ</italic><sub>in</sub> (<xref rid="fig4" ref-type="fig">Fig. 4c2</xref>) is own to the fact that for simultaneous step-changes in the input rates and the target we required <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline82.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, instead of <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline83.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The time constant for low-pass filtering the input rates was chosen to be <italic>τ</italic><sub>in</sub> = 10 ms, like the time constants <italic>τ</italic> for the prospective rates and the downstream membranes, but <italic>τ</italic><sub>in</sub> could have also been chosen much smaller to improve performance at even shorter presentation times.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>On-the-fly learning of finger responses to visual input with real-time Dendritic Error Propagation (rt-DeEP).</title>
<p><bold>(a)</bold> Functionally feedforward network with handwritten digits as visual input (<inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline551.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> in <xref ref-type="fig" rid="fig3">Fig. 3a</xref>, here from the MNIST data set, 5 ms presentation time per image), backprojections enabling credit assignment, and activity of the 10 output neurons interpreted as commands for the 10 fingers (forward architecture: 784× 500 ×10 neurons). <bold>(b)</bold> Example voltage trace (b1) and local error (b2) of a hidden neuron in NLA (red) compared to an equivalent network without lookahead rates (orange). Note that neither network achieves a steady state due to the extremely short input presentation times. Errors calculated via exact backpropagation, i.e. by using the error backpropagation algorithm on a purely feedforward NLA network at every simulation time step (with output errors scaled by <italic>β</italic>), shown for comparison (blue). <bold>(c)</bold> Comparison of network models during and after learning. Color scheme as in (b). <bold>(c1)</bold> The test error under NLA evolves during learning on par with classical error backpropagation performed each Euler <italic>dt</italic> based on the feedforward activities. In contrast, networks without lookahead rates are incapable of learning such rapidly changing stimuli. <bold>(c2)</bold> With increasing presentation time, the performance under NLA further improves, while networks without lookahead rates stagnate at high error rates. This is caused by transient, but long-lasting misrepresentation of errors following stimulus switches: when plasticity is turned off during transients and is only active in the steady state, comparably good performance can be achieved (dashed orange). <bold>(d)</bold> Receptive fields of 6 hidden-layer neurons after training, demonstrating that even for very brief image presentation times (5ms), the combined neuronal and synaptic dynamics are capable of learning useful feature extractors such as edge filters.</p></caption>
<graphic xlink:href="534198v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The instantaneous voltage propagation reduces an essential constraint of previous models of bio-plausible error backpropagation (e.g., <xref ref-type="bibr" rid="c53">Scellier &amp; Bengio (2017)</xref>, <xref ref-type="bibr" rid="c67">Whittington &amp; Bogacz (2017)</xref>, and <xref ref-type="bibr" rid="c52">Sacramento <italic>et al</italic>. (2018)</xref>, with reviews <xref rid="c50" ref-type="bibr">Richards <italic>et al</italic>. (2019)</xref>, <xref ref-type="bibr" rid="c68">Whittington &amp; Bogacz (2019)</xref>, and <xref rid="c39" ref-type="bibr">Lillicrap, Santoro, <italic>et al</italic>. (2020)</xref>): without lookahead firing rates, networks need much longer to correctly propagate errors across layers, with each layer roughly adding another membrane time constant of 10 ms, and thus cannot cope with realistic input presentation times. In fact, in networks without lookahead output, learning is only successful if plasticity is switched off while the network dynamics is not stationary (<xref rid="fig4" ref-type="fig">Fig. 4c2</xref>). As a comparison, neuronal response latency in the primary visual cortex (V1) of rats to flashing stimuli are in the order of 50 ms if the cortex is in a synchronized state, shortens to roughly 40 ms if in a desynchronized state (<xref ref-type="bibr" rid="c66">Wang <italic>et al</italic>., 2014</xref>), and potentially shortens further if the area is preactivated through expectations (<xref ref-type="bibr" rid="c9">Blom <italic>et al</italic>., 2020</xref>).</p>
</sec>
<sec id="s2g">
<title>Implementation in cortical microcircuits</title>
<p>So far, we did not specify how errors <bold><italic>e</italic></bold> appearing in the differential equation for the voltage (<xref ref-type="disp-formula" rid="eqn7a">Eq. 7a</xref>) are transmitted across the network in a biologically plausible manner. Building on <xref ref-type="bibr" rid="c52">Sacramento <italic>et al</italic>., 2018</xref>, we propose a cortical microcircuit to enable this error transport, with all neuron dynamics evolving according to the NLA principle. Although the idea applies for arbitrarily connected networks, we use the simpler case of functionally feedforward networks to illustrate the flow of information in these microcircuits (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Hierarchical plastic microcircuits implement real-time Dendritic Error Learning (rt-DeEL).</title>
<p><bold>(a)</bold> Microcircuit with ‘top-down’ input (originating from peripheral motor activity, blue line) that is explained away by the lateral input via interneurons (dark red), with the remaining activity representing the error <italic>ē</italic><sub><italic>l</italic></sub>. Plastic connections are denoted with a small red arrow and nudging with a dashed line. <bold>(b1)</bold> Simulated network with 784-300-10 pyramidal-neurons and a population of 40 interneurons in the hidden layer used for the MNIST learning task where the handwritten digits have to be associated to the 10 fingers. <bold>(b2)</bold> Test errors for rt-DeEL with joint tabula rasa learning of the forward and lateral weights of the microcircuit. A similar performance is reached as with classical error backpropagation. For comparability, we also show the performance of a shallow network (dashed line). <bold>(b3)</bold> Angle derived from the Frobenius norm between the lateral pathway <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline552.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and the feedback pathway <bold><italic>B</italic></bold><sub><italic>l</italic></sub><bold><italic>W</italic></bold><sub><italic>l</italic>+1</sub>. During training, both pathways align to allow correct credit assignment throughout the network. Indices are dropped in the axis label for readability.</p></caption>
<graphic xlink:href="534198v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>For such an architecture, pyramidal neurons in area <italic>l</italic> (that is a ‘layer’ of the feedforward network) are accompanied by a pool of interneurons in the same layer (area). The dendrites of the interneurons integrate in time (with time constant <italic>τ</italic>) lateral input from pyramidal neurons of the same layer (<bold><italic>r</italic></bold><sub><italic>l</italic></sub>) through plastic weights <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline84.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Additionally, interneurons receive ‘top-down nudging’ from pyramidal neurons in the next layer through randomly initialized and fixed backprojecting synapses <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline85.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> targeting the somatic region, and interneuron nudging strength <italic>β</italic><sup>I</sup>. The notion of ‘top-down’ originates from the functionally feed-forward architecture leading from sensory to ‘higher cortical areas’. In the context of motor control, the highest ‘area’ is last stage controlling the muscle lengths, being at the same time the first stage for the proprioceptive input (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>).</p>
<p>According to the biophysics of the interneuron, the somatic membrane potential becomes a convex combination of the two types of afferent input (<xref ref-type="bibr" rid="c64">Urbanczik &amp; Senn, 2014</xref>),
<disp-formula id="eqn9">
<alternatives><graphic xlink:href="534198v2_eqn9.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
In the biological implementation, the feedback input is mediated by the low-pass filtered firing rates <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline86.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, not by <bold><italic>u</italic></bold><sub><italic>l</italic>+1</sub> as expressed in the above equation. Yet, we argue that for a threshold-linear <italic>ρ</italic> the ‘top-down nudging’ by the rate <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline87.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is effectively reduced to a nudging by the voltage <bold><italic>u</italic></bold><sub><italic>l</italic>+1</sub>. This is because errors are only backpropagated when the slope of the transfer function is positive, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline88.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and hence when the upper-layer voltage is in the linear regime. For more general transfer functions, we argue that short-term synaptic depression may invert the low-pass filtered presynaptic rate back to the presynaptic membrane potential, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline89.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, provided that the recovery time constant <italic>τ</italic> matches the membrane time constant (see end of the section and Methods).</p>
<p>Apical dendrites of pyramidal neurons in each layer receive top-down input from the pyramidal population in the upper layer through synaptic weights <bold><italic>B</italic></bold><sub><italic>l</italic></sub>. These top-down weights could be learned to predict the lower-layer activity (<xref ref-type="bibr" rid="c49">Rao &amp; Ballard, 1999</xref>) or to become the transposed of the forward weight matrix (<xref rid="c40" ref-type="bibr">Max <italic>et al</italic>., 2022</xref>), but for simplicity we randomly initialized them and keep them fixed (<xref rid="c39" ref-type="bibr">Lillicrap, Santoro, <italic>et al</italic>., 2020</xref>). Beside the top-down projections the apical dendrites also receive lateral input via an interneuron population in the same layer through synaptic weights <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline90.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> that are plastic and will be learned to obtain suitable dendritic errors. The ‘-’ sign is suggestive for these interneurons to subtract away the top-down input entering through <bold><italic>B</italic></bold><sub><italic>l</italic></sub> (while the weights can still be positive or negative). Assuming again a conversion of rates to voltages, also for the inhibitory neurons that may operate in a linear regime, the overall apical voltage becomes
<disp-formula id="eqn10">
<alternatives><graphic xlink:href="534198v2_eqn10.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
What cannot be explained away from the top-down input <bold><italic>B</italic></bold><sub><italic>l</italic></sub><bold><italic>u</italic></bold><sub><italic>l</italic>+1</sub> by the lateral feedback, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline91.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, remains as dendritic prediction error <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline92.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> in the apical tree (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>). If the top-down and lateral feedback weights are learned as outlined next, these apical prediction errors take the role of the backpropagated errors in the classical backprop algorithm.</p>
<p>To adjust the interneuron circuit in each layer (‘area’), the synaptic strengths from pyramidal-to-interneurons, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline93.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, are learned to minimize the interneuron mismatch energy, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline94.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The interneurons, while being driven by the lateral inputs <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline95.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, learn to reproduce the upper-layer activity that also nudges the interneuron voltage. Learning is accomplished if the upper-layer activity, in the absence of an additional error on the upper layer, is fully reproduced in the interneurons by the lateral input.</p>
<p>Once the interneurons learned to represent the ‘error-free’ upper-layer activity, they can be used to explain away the top-down activities that also project to the apical trees. The synaptic strengths from the inter-to-pyramidal neurons, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline96.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, are learned to minimize the apical mismatch energy, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline97.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. While in the absence of an upper-layer error, the top-down activity <bold><italic>B</italic></bold><sub><italic>l</italic></sub><bold><italic>u</italic></bold><sub><italic>l</italic>+1</sub> can be fully cancelled by the interneuron activity <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline98.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, a neuron-specific error will remain in the apical dendrites of the lower-level pyramidal neurons if there was an error endowed in the upper-layer neurons. Gradient descent learning on these two energies results in the learning rules for the P-to-I and I-to-P synapses,
<disp-formula id="eqn11">
<alternatives><graphic xlink:href="534198v2_eqn11.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The following theorem on dendritic error learning tells that the plasticity in the lateral feedback loop leads to an appropriate error representation in the apical dendrites of pyramidal neurons.</p>
<statement id="the2">
<label>Theorem 2</label>
<title>(real-time Dendritic Error Learning, rt-DeEL)</title>
<p><italic>Consider a cortical microcircuit composed of pyramidal and interneurons, as illustrated in <xref rid="fig5" ref-type="fig">Fig. 5a</xref> (with dimensionality constraints specified in Methods, and fixed or dynamic weights</italic> <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline99.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> <italic>to the interneurons). Then the inter-to-pyramidal synapses evolve at each layer l (‘cortical area’) such that the lateral feedback circuit aligns with the top-down feedback circuit</italic>,
<disp-formula id="eqn12">
<alternatives><graphic xlink:href="534198v2_eqn12.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<italic>In the presence of an output nudging, the apical voltages of the layer-l pyramidal neurons (<xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref>) then represent the ‘B-backpropagated’ error</italic> <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline100.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. <italic>When modulated by the postsynaptic rate derivative</italic>, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline101.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, <italic>this apical error ensures the correct representation of errors</italic> <bold><italic>ē</italic></bold><sub><italic>l</italic></sub> <italic>for the real-time dendritic error propagation (rt-DeEP, <xref ref-type="statement" rid="the1">Theorem 1</xref>)</italic>,
<disp-formula id="eqn13">
<alternatives><graphic xlink:href="534198v2_eqn13.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
</p>
</statement>
</sec>
<sec id="s2h">
<title>Simultaneously learning apical errors and basal signals</title>
<p>Microcircuits following these neuronal and synaptic dynamics are able to learn the classification of hand-written digits from the MNIST dataset while learning the apical signal representation (<xref rid="fig5" ref-type="fig">Fig. 5b1-2</xref>). In this case, feedforward weights <bold><italic>W</italic></bold><sub><italic>l</italic></sub> and lateral weights <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline102.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline103.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are all adapted simultaneously. Including the <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline104.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>-plasticity (by turning on the interneuron nudging from the upper layer, <italic>β</italic><sup>I</sup> <italic>&gt;</italic> 0 in <xref ref-type="disp-formula" rid="eqn9">Eq. 9</xref>), greatly speeds up the learning.</p>
<p>With and without <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline105.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>-plasticity, the lateral feedback via interneurons (with effective weigh <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline106.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>) learns to align with the forward-backward feedback via upper layer pyramidal neurons (with effective weight <bold><italic>B</italic></bold><sub><italic>l</italic></sub><bold><italic>W</italic></bold><sub><italic>l</italic>+1</sub>, <xref rid="fig5" ref-type="fig">Fig. 5b3</xref>). The microcircuit extracts the gradient-based errors (<xref ref-type="disp-formula" rid="eqn13">Eq. 13</xref>), while the forward weights use these errors to reduce these errors to first minimize the neuron-specific mismatch errors, and eventually the output cost.</p>
<p>Since the apical voltage <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline106a.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> appears as a postsynaptic factor in the plasticity rule for the interneurons <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline107.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, this I-to-P plasticity can be interpreted as Hebbian plasticity of inhbitory neurons, consistent with earlier suggestions (<xref ref-type="bibr" rid="c65">Vogels <italic>et al</italic>., 2012</xref>; <xref ref-type="bibr" rid="c5">Bannon <italic>et al</italic>., 2020</xref>). The plasticity <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline108.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> of the P-to-I synapses, in the same way as the plasticity for the forward synapses <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline109.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>s, can be interpreted as learning from the dendritic prediction of somatic activity (<xref ref-type="bibr" rid="c64">Urbanczik &amp; Senn, 2014</xref>).</p>
<p>Crucially, choosing a large enough interneuron population, the simultaneous learning of the lateral microcircuit and the forward network can be accomplished without fine-tuning of parameters. As an instance in case, all weights shared the same learning rate. Such stability bolsters the biophysical plausibility of our NLA framework and improves over the previous, more heuristic approach (<xref ref-type="bibr" rid="c52">Sacramento <italic>et al</italic>., 2018</xref>; <xref ref-type="bibr" rid="c41">Mesnard <italic>et al</italic>., 2019</xref>). The stability may be related to the nested gradient descent learning according to which somatic and apical mismatch errors in pyramidal neurons, and somatic mismatch errors in inhibitory neurons are minimized.</p>
<p>Finally, since errors are defined at the level of membrane voltages (<xref ref-type="disp-formula" rid="eqn11">Eq. 11</xref>), synapses need a mechanism by which they can recover the presynaptic voltage errors from their afferent firing rates. While for threshold-linear transfer functions this is not too involved (Methods), more general neuronal nonlinearities must be matched by corresponding synaptic nonlinearities. <xref ref-type="bibr" rid="c47">Pfister <italic>et al</italic>. (2010)</xref> have previously illustrated how spiking neurons can leverage short-term synaptic depression to estimate the membrane potential of their presynaptic partners. Here, we assume a similar mechanism in the context of our rate-based neurons. The monotonically increasing neuronal activation function, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline110.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, can be approximately compensated by a vesicle release probability that monotonically decreases with the low-pass filtered presynaptic rate <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline111.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (see SI and <xref rid="fig6" ref-type="fig">Fig. 6</xref> therein). If properly matched, this leads to a linear relationship between the presynaptic membrane potential <bold><italic>u</italic></bold><sub><italic>l</italic>+1</sub> and the postsynaptic voltage contribution.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Recovering presynaptic potentials through short term depression.</title>
<p><bold>(a1)</bold> Relative voltage response of a depressing cortical synapse (recreated from <xref ref-type="bibr" rid="c1">Abbott <italic>et al</italic>., 1997</xref>), identified as synaptic release probability <italic>p</italic>. <bold>(a2)</bold> The product of the low-pass filtered presynaptic firing rate <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline553.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> times the synaptic release probability <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline554.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is proportional to the presynaptic membrane potential, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline555.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. <bold>(a3)</bold> Average in vivo firing rate of a neuron in the visual cortex as a function of the somatic membrane potential (recreated from <xref ref-type="bibr" rid="sc2">Anderson <italic>et al</italic>., 2000</xref>), which can be qualitatively identified as the stationary rate <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline556.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> derived in <xref ref-type="disp-formula" rid="eqn43">Eq. 43</xref>.</p></caption>
<graphic xlink:href="534198v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We introduced a least-action principle for neuronal networks from which we derived the membrane potential dynamics of the involved neurons, together with gradient descent plasticity of the synapses in the network. The central notion of the theory is the somato-dendritic mismatch error in each individual neuron. Given the various findings on the dendritic integration of top-down signals (<xref ref-type="bibr" rid="c32">Larkum, 2013</xref>; <xref rid="c57" ref-type="bibr">Takahashi <italic>et al</italic>., 2020</xref>), we suggest that the error is formed in the apical dendrite of pyramidal neurons. Active dendritic and somatic processes compensate for delays caused by the dendritic error propagation and the somatic integration. This leads to the prospective firing of pyramidal neurons that are jointly driven by the forward input on the basal dendrites and the error on the apical dendrite. We derived a local plasticity rule for synapses on the basal tree that, by extracting the apical feedback error, reduces the prospective somato-dendritic mismatch error in an individual neuron. This plasticity is shown to also globally reduce the instantaneous cost at output neurons (defined by deviations from target voltages) at any moment in time while stimuli and targets may continuously change. Motor control offers an intuitive context where self-correcting prospective networks may enter. We put forward the moving equilibrium hypothesis, according to which sensory input, motor commands and muscle spindle feedback are in a recurrent equilibrium at any moment during the movement.</p>
<p>We further showed how a local microcircuit can serve to calculate a neuron-specific error in the apical tree of each individual pyramidal neuron. The apical tree of pyramidal neurons receives feedback from higher-level neurons in the network, while local inhibitory neurons try on the fly to ‘explain away’ the top-down expectations. What cannot be explained away remains as apical prediction error. This prediction error eventually induces error-correcting plasticity at the sensory-driven input on the basal tree. To find the correct error representation in the apical tree, plasticity of the inter-to-pyramidal neurons seeks to homeostatically drive the apical de-or hyperpolarized voltage back to rest. While the network synapses targetting the basal tree are performing gradient descent on the global cost, the microcircuit synapses involved in the lateral feedback are gradient descent on local error functions, both at any moment in time.</p>
<p>Our work builds on three general lines of research on formalizing the learning capabilities of cortical networks. The first line refers to the use of an energy function to jointly infer the neuronal dynamics and synaptic plasticity, originally formulated for discrete-time networks (<xref ref-type="bibr" rid="c27">Hopfield, 1982</xref>; <xref ref-type="bibr" rid="c2">Ackley <italic>et al</italic>., 1985</xref>), and recently extended to continuous-time networks (<xref ref-type="bibr" rid="c53">Scellier &amp; Bengio, 2017</xref>). The second line refers to understanding error-backpropagation in the brain (<xref ref-type="bibr" rid="c51">Rumelhart <italic>et al</italic>., 1986</xref>; <xref ref-type="bibr" rid="c71">Xie &amp; Seung, 2003</xref>; <xref ref-type="bibr" rid="c67">Whittington &amp; Bogacz, 2017</xref>; <xref ref-type="bibr" rid="c68">Whittington &amp; Bogacz, 2019</xref>; <xref rid="c39" ref-type="bibr">Lillicrap, Santoro, <italic>et al</italic>., 2020</xref>). The third line refers to the use of dendritic compartmentalization in various kinds of computation (<xref ref-type="bibr" rid="c54">Schiess <italic>et al</italic>., 2016</xref>; <xref ref-type="bibr" rid="c48">Poirazi &amp; Papoutsi, 2020</xref>), recently linked to deep learning (<xref ref-type="bibr" rid="c21">Guerguiev <italic>et al</italic>., 2017</xref>; <xref ref-type="bibr" rid="c52">Sacramento <italic>et al</italic>., 2018</xref>; <xref rid="c22" ref-type="bibr">Haider <italic>et al</italic>., 2021</xref>).</p>
<p>With regard to energy functions, the NLA principle adds a variational approach to characterize continuous-time neuronal trajectories and plasticity. Variational approaches are studied in the context of optimal control theory where a cost integral is minimized across time, constrained to some network dynamics (<xref ref-type="bibr" rid="c62">Todorov &amp; Jordan, 2002</xref>; <xref rid="c42" ref-type="bibr">Meulemans, Farinha, <italic>et al</italic>., 2021</xref>). The NLA represents a unifying notion that allows to infer both, the network dynamics and its optimal control from a single Lagrangian. The error we derive represents prospective control variables that are applied to the voltages of each network neurons so that they push the output neurons towards their target trajectory. The full expression power of this control theoretic framework has yet to be proven when it is extended to genuine temporal processing that includes longer time constants, for instance inherent in a slow threshold adaptation (<xref rid="c8" ref-type="bibr">Bellec <italic>et al</italic>., 2020</xref>). The NLA principle can also treat the case of strong feedback studied so far in relaxation networks only (<xref rid="c43" ref-type="bibr">Meulemans, Zucchet, <italic>et al</italic>., 2022</xref>; <xref rid="c56" ref-type="bibr">Song <italic>et al</italic>., 2022</xref>). Our rt-DeEP Theorem makes a statement for real-time gradient descent learning while the network is in a moving equilibrium, linking to motor learning in the presence of perturbing force fields (<xref ref-type="bibr" rid="c24">Herzfeld <italic>et al</italic>., 2014</xref>) or perturbing visual inputs (<xref ref-type="bibr" rid="c13">Dimitriou, 2016</xref>).</p>
<p>With regard to error-backpropagation, the NLA principle relies on feedback alignment (<xref rid="c38" ref-type="bibr">Lillicrap, Cownden, <italic>et al</italic>., 2016</xref>) to correctly interpret the apical errors. Other works have explored learning of feedback weights (<xref ref-type="bibr" rid="c3">Akrout <italic>et al</italic>., 2019</xref>; <xref rid="c31" ref-type="bibr">Kunin <italic>et al</italic>., 2020</xref>; <xref rid="c40" ref-type="bibr">Max <italic>et al</italic>., 2022</xref>). Since these are complementary to the principles suggested here, a promising direction would be to explore how feedback weights can be learned in NLA microcircuits to improve credit assignment in deeper networks.</p>
<p>With regard to dendritic and neuronal processing, we emphasize the prospective nature of apical voltages and somatic firing (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). The prospective coding of errors and signals overcomes the various delays inherent in our previous approach (<xref ref-type="bibr" rid="c52">Sacramento <italic>et al</italic>., 2018</xref>; <xref ref-type="bibr" rid="c41">Mesnard <italic>et al</italic>., 2019</xref>). Each neuron of the network instantaneously corrects its voltage so that the output neurons are pushed towards the target trajectories. Ongoing synaptic plasticity adjusts the synaptic strengths so that the errors in each neuron are reduced at any moment in time, without needing to wait for network relaxations (<xref ref-type="bibr" rid="c53">Scellier &amp; Bengio, 2017</xref>; <xref rid="c56" ref-type="bibr">Song <italic>et al</italic>., 2022</xref>). Yet, because in the present framework the input rates are still low-pass filtered, step inputs cannot be instantaneously propagated, only their low-pass filterings. <xref rid="c22" ref-type="bibr">Haider <italic>et al</italic>., 2021</xref> offers a framework that is also suited instantaneously process step inputs.</p>
<p>Motivated by the predictive power of the least-action principle in physics, we ask about experimental confirmation and predictions of the NLA in biology. Given its axiomatic approach, it appears astonishing to find various preliminary matches at the dendritic, somatic, interneuron, synaptic and even behavioural level. Some of these are: (1) the prospective coding of pyramidal neuron firing (<xref rid="c30" ref-type="bibr">Köndgen <italic>et al</italic>., 2008</xref>); (2) the prospective processing of apical signals while propagating to the soma (<xref ref-type="bibr" rid="c63">Ulrich, 2002</xref>); (3) the basal synaptic plasticity on pyramidal neurons and synaptic plasticity on interneurons, driven by the postsynaptic activity that is ‘unexplained’ by the distal dendritic voltage (<xref ref-type="bibr" rid="c64">Urbanczik &amp; Senn, 2014</xref>); (4) the Hebbian homeostatic plasticity of interneurons targeting the apical dendritic tree of pyramidal neurons (<xref ref-type="bibr" rid="c5">Bannon <italic>et al</italic>., 2020</xref>); (5) the short-term synaptic depression at top-down synapses targetting inhibitory neurons and apical dendrites (akin to <xref ref-type="bibr" rid="c1">Abbott <italic>et al</italic>., 1997</xref>, but with a faster recovery time constant) that invert the presynaptic activation function (see also <xref ref-type="bibr" rid="c47">Pfister <italic>et al</italic>., 2010</xref>); (6) the modulation of the apical contribution to the somatic voltage by the slope of the somatic activation function (for instance by downregulating apical NMDA receptors with increasing rate of backpropagating action potentials, <xref ref-type="bibr" rid="c59">Theis <italic>et al</italic>., 2018</xref>); and (7) the role of muscle spindles in the prospective encoding of motor errors during motor learning (<xref ref-type="bibr" rid="c13">Dimitriou, 2016</xref>; <xref ref-type="bibr" rid="c46">Papaioannou &amp; Dimitriou, 2021</xref>). More theoretical and experimental work is required to explore these various links.</p>
<p>Overall, our approach introduces a method from theoretical physics to computational neuroscience and couples it with a normative perspective on the dynamical processing of neurons and synapses within global cortical networks and local microcircuits. Given its physical underpinnings, the approach may inspire the rebuilding of computational principles of cortical neurons and circuits in neuromorphic hardware (<xref ref-type="bibr" rid="c6">Bartolozzi <italic>et al</italic>., 2022</xref>). A step in this direction, building on the instantaneous computational capabilities with slowly integrating neurons, has been done (<xref rid="c22" ref-type="bibr">Haider <italic>et al</italic>., 2021</xref>) and a next challenge is to generalize the NLA principle to spiking neurons (<xref ref-type="bibr" rid="c72">Zenke &amp; Ganguli, 2018</xref>; <xref rid="c20" ref-type="bibr">Göltz <italic>et al</italic>., 2021</xref>) and longer temporal processing.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We would like to thank Federico Benitez, Jonathan Binas, Paul Haider, Kevin Max, Alexander Mathis, and Alexander Meulemans and Jean-Pascal Pfister for helpful discussions, Kaspar Schindler for providing the human intracortical EEG data, and the late Karlheinz Meier for his dedication and support throughout the early stages of the project. WS particularly thanks Nicolas Zucchet for inspiring mathematical discussions and hints to literature on time-varying optimal control. This work has received funding from the European Union 7th Framework Programme under grant agreement 604102 (HBP), the Horizon 2020 Framework Programme under grant agreements 720270, 785907 and 945539 (HBP) and the Swiss National Science Foundation (SNSF, Sinergia grant CRSII5180316; João Sacramento is supported by SNSF Ambizione grant PZ00P3_186027). We further express our particular gratitude towards the Manfred Stärk Foundation for their continued support. We acknowledge the use of Fenix Infrastructure resources, which are partially funded from the European Union’s Horizon 2020 research and innovation programme through the ICEI project under the grant agreement No. 800858.</p>
</ack>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Euler-Lagrange equations as inverse low-pass filters</title>
<p>The theory is based on the lookahead of neuronal quantities. In general, the lookahead of a trajectory <italic>x</italic>(<italic>t</italic>) is defined as <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline112.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with lookahead operator
<disp-formula id="eqn14">
<alternatives><graphic xlink:href="534198v2_eqn14.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The lookahead operator is the inverse of the low-pass filter operator denoted by a bar,
<disp-formula id="eqn15">
<alternatives><graphic xlink:href="534198v2_eqn15.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
This low-pass filtering can also be characterized by the differential equation <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline113.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, see SI. Hence, applying the low-pass filtering to <italic>x</italic> and then the lookahead operator ℒ to <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline114.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and using the Leibnitz rule for differentiating an integral, we calculate <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline115.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. In turn, applying first the lookahead, and then the low-pass filtering, also yields the original trace back, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline116.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (see SI).</p>
<p>We consider an arbitrary network architecture with network neurons that are recurrently connected and that receive external input through an overall weight matrix <bold><italic>W</italic></bold> = (<bold><italic>W</italic></bold><sub>in</sub>, <bold><italic>W</italic></bold><sub>net</sub>), aggregated column-wise. The instantaneous presnyaptic firing rates are <bold><italic>r</italic></bold> = (<bold><italic>r</italic></bold><sub>in</sub>, <bold><italic>r</italic></bold><sub>net</sub>), interpreted as a single column vector. A subset of network neurons are output neurons, 𝒪 ⊆ 𝒩, for which target voltages <bold><italic>u</italic></bold>* may be imposed. Rates and voltages may change in time <italic>t</italic>. Network neurons are assigned a voltage <bold><italic>u</italic></bold>, generating the low-pass filtered rate <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline117.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and a low-pass filtered error <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline118.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. We further define output errors <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline119.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for <italic>o</italic> ∈ 𝒪, and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline120.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for non-output neurons <italic>i</italic> ∈ 𝒩 \ 𝒪. With this, the Lagrangian from <xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref> takes the form
<disp-formula id="eqn16">
<alternatives><graphic xlink:href="534198v2_eqn16.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
We next use that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline121.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with the <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline122.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> operator defined in <xref ref-type="disp-formula" rid="eqn4">Eq. 4</xref>, to write out the Lagrangian <italic>L</italic> in the canonical coordinates <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline123.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> as (see also <xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref>)
<disp-formula id="eqn17">
<alternatives><graphic xlink:href="534198v2_eqn17.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The neuronal dynamics is derived from requiring a stationary action (see <xref ref-type="disp-formula" rid="eqn5">Eq. 5</xref>), which is generally solved by the Euler-Lagrange equations <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline124.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (see <xref ref-type="disp-formula" rid="eqn6">Eq. 6</xref>). Because <italic>ũ</italic> only arises in <italic>L</italic> in the compound <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline125.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the derivative of <italic>L</italic> with respect to <italic>ũ</italic> is identical to the derivative with respect to <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline126.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>,
<disp-formula id="eqn18">
<alternatives><graphic xlink:href="534198v2_eqn18.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Using the lookahead operator <xref ref-type="disp-formula" rid="eqn14">Eq. 14</xref>, the Euler-Lagrange equations can then be rewritten as
<disp-formula id="eqn19">
<alternatives><graphic xlink:href="534198v2_eqn19.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Since <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline127.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline128.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the derivative of <italic>L</italic> with respect to <bold><italic>ũ</italic></bold> is the same as the derivative of <italic>L</italic> with respect to <bold><italic>u</italic></bold>, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline129.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Plugging this into <xref ref-type="disp-formula" rid="eqn19">Eq. 19</xref>, the Euler-Lagrange equations become a function of <bold><italic>u</italic></bold> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline130.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>,
<disp-formula id="eqn20">
<alternatives><graphic xlink:href="534198v2_eqn20.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The solution of this differential equation is <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline131.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and hence any trajectory <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline132.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> which satisfy the Euler-Lagrange equations will hence cause <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline133.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> to converge to zero with a characteristic time scale of <italic>τ</italic>. Since we require that the initialisation is at <italic>t</italic><sub>0</sub> = −∞, we conclude that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline134.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, as required in the rt-DeEP Theorem.</p>
</sec>
<sec id="s4b">
<title>Deriving the network dynamics from the Euler-Lagrange equations</title>
<p>We now derive the equations of motion from the Euler-Lagrange equations. Noticing that <bold><italic>u</italic></bold> enters in <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline135.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> twice, directly and through <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline136.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and once in the output error <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline137.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, we calculate from <xref ref-type="disp-formula" rid="eqn16">Eq. 16</xref>, using <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline138.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <bold><italic>W</italic></bold> = (<bold><italic>W</italic></bold><sub>in</sub>, <bold><italic>W</italic></bold><sub>net</sub>),
<disp-formula id="eqn21">
<alternatives><graphic xlink:href="534198v2_eqn21.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Next, we apply the lookahead operator to this expression, as required by the Euler-Lagrange equations <xref ref-type="disp-formula" rid="eqn19">Eq. 19</xref>. In general <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline139.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and we set for <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline140.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> the expression on the right-hand side of <xref ref-type="disp-formula" rid="eqn21">Eq. 21</xref>, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline141.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, which at the same time is <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline142.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Hence, the Euler-Lagrange equations in the form of <xref ref-type="disp-formula" rid="eqn20">Eq. 20</xref>, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline143.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, translate into
<disp-formula id="eqn22">
<alternatives><graphic xlink:href="534198v2_eqn22.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
To move from the middle to the last equality we replaced <bold><italic>e</italic></bold> by <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline144.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. In the last equality we interpret <bold><italic>e</italic></bold> as the sum of the two errors, <bold><italic>e</italic></bold> = <bold><italic>ϵ</italic></bold> + <italic>β</italic><bold><italic>e</italic></bold>*, again using the middle equality. This proves <xref ref-type="disp-formula" rid="eqn7a">Eq. 7</xref>.</p>
<p>Notice that the differential equation <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline145.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="eqn22">Eq. 22</xref> represents an implicit ordinary differential equation as on the right-hand side not only <bold><italic>u</italic></bold>, but also <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline146.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> appears (in <bold><italic>r</italic></bold> and <bold><italic>e</italic></bold>). The uniqueness of the solution <bold><italic>u</italic></bold>(<italic>t</italic>) for a given initial condition is only guaranteed if it can be converted into an explicit ordinary differential equation (see Sect. C).</p>
<p>In taking the temporal derivative we assumed small learning rates such that terms including <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline147.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> can be neglected. The derived dynamics for the membrane potential of a neuron <italic>u</italic><sub><italic>i</italic></sub> in <xref ref-type="disp-formula" rid="eqn22">Eq. 22</xref> show the usual leaky behavior of biological neurons. However, both presynaptic rates <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline148.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and prediction errors <italic>ē</italic><sub><italic>i</italic></sub> enter the equation of motion with lookaheads, i.e., they are advanced (<inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline149.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline150.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>), cancelling the low-pass filtering. Since <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline151.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the rate and error, <italic>r</italic><sub><italic>i</italic></sub> and <italic>e</italic><sub><italic>i</italic></sub>, can also be seen as nonlinear extrapolations from the voltage and its derivative into the future.</p>
<p>The instantaneous transmission of information throughout the network at the level of the voltages can now be seen by low-pass filtering <xref ref-type="disp-formula" rid="eqn22">Eq. 22</xref> with initialization far back in the past,
<disp-formula id="eqn23">
<alternatives><graphic xlink:href="534198v2_eqn23.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
with <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline152.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> interpreted as column vector, and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline153.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Hence, solving the voltage dynamics for <bold><italic>u</italic></bold> (<xref ref-type="disp-formula" rid="eqn7a">Eq. 7a</xref>), with apical voltage <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline154.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> derived from <xref ref-type="disp-formula" rid="eqn7b">Eq. 7b</xref>, yields the somatic voltage <bold><italic>u</italic></bold> satisfying the self-consistency equation (<xref ref-type="disp-formula" rid="eqn23">Eq. 23</xref>) at any time. In other words, <bold><italic>u</italic></bold> and <bold><italic>ē</italic></bold> ‘propagate instantaneously’.</p>
</sec>
<sec id="s4c">
<title>Deriving the error backpropagation formula</title>
<p>For clarity, we derive the error backpropagation algorithm for layered networks here. These can be seen as a special case of a general network with membrane potentials <bold><italic>u</italic></bold> and all-to-all weight matrix <bold><italic>W</italic></bold> (as introduced in appendix H), where the membrane potentials decompose into layerwise membrane potential vectors <bold><italic>u</italic></bold><sub><italic>l</italic></sub> and the weight matrix into according block diagonal matrices <bold><italic>W</italic></bold><sub><italic>l</italic></sub> (with <bold><italic>W</italic></bold><sub><italic>l</italic></sub> being the weights that project into layer <italic>l</italic>).</p>
<p>Assuming a network with <italic>N</italic> layers, by low-pass filtering the equations of motion we get
<disp-formula id="eqn24">
<alternatives><graphic xlink:href="534198v2_eqn24.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
∀<italic>l</italic> ∈ [1, <italic>N</italic>], with the output error <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline155.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The error <bold><italic>e</italic></bold> = <bold><italic>ϵ</italic></bold> + <italic>β</italic><bold><italic>e</italic></bold>* we obtain from the general dynamics with <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline156.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, see <xref ref-type="disp-formula" rid="eqn21">Eq. 21</xref> and <xref ref-type="disp-formula" rid="eqn22">Eq. 22</xref>, translates to an iterative formula for the error at the current layer <italic>l</italic> given the error at the downstream layer <italic>l</italic>+1, inherited from the drive <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline157.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> of that downstream layer via <bold><italic>W</italic></bold><sub><italic>l</italic>+1</sub>,
<disp-formula id="eqn25">
<alternatives><graphic xlink:href="534198v2_eqn25.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
and <bold><italic>ē</italic></bold><sub><italic>N</italic></sub> = <italic>β</italic> <bold><italic>ē</italic></bold>* for the output layer. The learning rule that reduces <bold><italic>ē</italic></bold><sub><italic>l</italic></sub> by gradient descent is proportional to this error and the presynaptic rate, as stated by <xref ref-type="statement" rid="the1">Theorem 1</xref>, is
<disp-formula id="eqn26">
<alternatives><graphic xlink:href="534198v2_eqn26.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
for <italic>l</italic> = 1…<italic>N</italic>. <xref ref-type="disp-formula" rid="eqn25">Eq. 25</xref> and <xref ref-type="disp-formula" rid="eqn26">Eq. 26</xref> together take the form of the error backpropagation algorithm, where an output error is iteratively propagated through the network and used to adjust the weights in order to reduce the output cost <italic>C</italic>. From this, it is easy to see that without output nudging (i.e., <italic>β</italic> = 0), the output error vanishes and consequently all other prediction errors vanish as well, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline158.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for all <italic>l≤ N</italic>. This also means that in the absence of nudging, no weight updates are performed by the plasticity rule.</p>
<p>The learning rule for arbitrary connectivities is obtained in the same way by dropping the layer-wise notation. In this case, low-pass filtering the equations of motion yields <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline159.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, as calculated in <xref ref-type="disp-formula" rid="eqn23">Eq. 23</xref>, and the low-pass filtered error <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline160.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, as inferred from <xref ref-type="disp-formula" rid="eqn21">Eqs 21</xref> and <xref ref-type="disp-formula" rid="eqn22">22</xref>. Hence, the plasticity rule in general reads
<disp-formula id="eqn27">
<alternatives><graphic xlink:href="534198v2_eqn27.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
</p>
</sec>
<sec id="s4d">
<title>Proving <xref ref-type="statement" rid="the1">Theorem 1</xref> (rt-DeEP)</title>
<p>The implicit assumption in <xref ref-type="statement" rid="the1">Theorem 1</xref> is that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline161.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> exists in the distributional sense for <italic>t &gt;</italic> −∞, which is the case for delta-functions in <bold><italic>r</italic></bold><sub>in</sub> and step-functions in <bold><italic>u</italic></bold>*. Both parts (<italic>i</italic>) and (<italic>ii</italic>) of the Theorem are based on the requirement of stationary action <italic>δA</italic> = 0, and hence on <bold><italic>u</italic></bold> satisfying the Euler-Lagrange equations in the form of <xref ref-type="disp-formula" rid="eqn22">Eq. 22</xref>, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline162.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. From the solution <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline163.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> we conclude that for initialization at <italic>t</italic><sub>0</sub> = −∞ we have <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline164.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for all <italic>t</italic>. It is the latter stronger condition that we require in the proof. With this, the main ingredient of the proof follows is the mathematical argument of <xref ref-type="bibr" rid="c53">Scellier &amp; Bengio, 2017</xref>, according to which the total and partial derivative of <italic>L</italic> with respect to <bold><italic>W</italic></bold> are identical, and this in our case is true for any time <italic>t</italic>,
<disp-formula id="eqn28">
<alternatives><graphic xlink:href="534198v2_eqn28.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
For convenience we considered <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline165.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> to be a column vector, deviating from the standard notations (see tutorial end of SI). Analogously to <xref ref-type="disp-formula" rid="eqn28">Eq. 28</xref>, we infer <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline166.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Reading <xref ref-type="disp-formula" rid="eqn28">Eq. 28</xref> from the right to the left, we conclude that the learning rule <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline167.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for all <italic>β &gt;</italic> 0 is gradient descent on <italic>L</italic>, i.e. <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline168.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This total derivative of <italic>L</italic> can be analyzed for large and small <italic>β</italic>.</p>
<p>(<italic>i</italic>) We show that in the limit of large <italic>β</italic>, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline169.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> becomes gradient descent on the mismatch energy <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline170.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. For this we first show that there is a solution of the self-consistency equation <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline171.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> that is uniformly bounded for all <italic>t</italic> and <italic>β</italic>. For this we assume that the transfer function <italic>ρ</italic>(<italic>u</italic>) is non-negative, monotonically increasing and bounded, that its derivative <italic>ρ</italic>′(<italic>u</italic>) is bounded too, and that the input rates <bold><italic>r</italic></bold><sub>in</sub> and the target potentials <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline172.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are also uniformly bounded. To show that under these conditions we always find an uniformly bounded solution <bold><italic>u</italic></bold>(<italic>t</italic>), we first consider the case where the output voltages are clamped to the target, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline173.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> such that <bold><italic>ē</italic></bold>* = 0. For simplicity we assume that <italic>ρ</italic>′(<italic>u</italic>) = 0 for <italic>u c</italic><sub>0</sub>. For voltages <bold><italic>u</italic></bold> with <bold><italic>u</italic></bold><sub><italic>i</italic></sub> ≤ <italic>c</italic><sub>0</sub> the recurrent input current <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline174.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is bounded, say <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline175.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for some <italic>c</italic><sub>1</sub> <italic>&gt; c</italic><sub>0</sub>. When including the error term <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline176.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the total current still remains uniformly bounded, say <bold><italic>F</italic></bold>(<bold><italic>u</italic></bold>)<sub><italic>j</italic></sub>| ≤ <italic>c</italic><sub>2</sub> for all <bold><italic>u</italic></bold> with <bold><italic>u</italic></bold><sub><italic>i</italic></sub> ≤ <italic>c</italic><sub>0</sub>. Because for larger voltages <bold><italic>u</italic></bold><sub><italic>i</italic></sub> <italic>&gt; c</italic><sub>0</sub> the error term vanishes due to a vanishing derivative <italic>ρ</italic> (<bold><italic>u</italic></bold><sub><italic>i</italic></sub>) = 0, the mapping <bold><italic>F</italic></bold>(<bold><italic>u</italic></bold>) maps the <italic>c</italic><sub>2</sub>-box <bold><italic>u</italic></bold> (for which |<bold><italic>u</italic></bold><sub><italic>i</italic></sub>| ≤<italic>c</italic><sub>2</sub>) onto itself. Brouwer’s fixed point theorem then tells us that there is a fixed point <bold><italic>u</italic></bold> = <bold><italic>F</italic></bold>(<bold><italic>u</italic></bold>) within the <italic>c</italic><sub>2</sub>-box. The theorem requires the continuity of <bold><italic>F</italic></bold>, and this is assured if the neuronal transfer function <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline177.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is continuous.</p>
<p>We next relax the voltages of the output neurons from their clamped stage, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline178.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Remember that these voltages satisfy <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline179.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> at any time <italic>t</italic>. We determine the correction term <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline180.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> such that in the limit <italic>β</italic> → <italic>∞</italic> we get <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline181.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The correction remains finite, and in the limit must be equal to <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline182.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. For arbitrary large nudging strength <italic>β</italic>, the output voltage <bold><italic>u</italic></bold><sub><bold><italic>o</italic></bold></sub> deviates arbitrary little from the target voltage, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline183.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with target error <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline184.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> shrinking like <italic>c</italic><sub>2</sub><italic>/β</italic>. Likewise, also for non-output neurons <italic>i</italic>, the self-consistency solution <bold><italic>u</italic></bold><sub><italic>i</italic></sub> = <bold><italic>F</italic></bold>(<bold><italic>u</italic></bold>)<sub><italic>i</italic></sub> deviates arbitrarily little from the solution of the clamped state. To ensure the smooth drift of the fixed point while 1<italic>/β</italic> deviates from 0 we require that the Jacobian of <bold><italic>F</italic></bold> at the fixed point is invertible.</p>
<p>Because the output <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline185.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> shrinks with 1<italic>/β</italic>, the cost shrinks quadratically with increasing nudging strength, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline186.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and hence the cost term <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline187.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> that enters in <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline188.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> vanishes in the limit <italic>β</italic> → ∞. In this large <italic>β</italic> limit, where <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline189.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and hence the outputs are clamped, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline190.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the Lagrangian reduces to the mismatch energy, <italic>L</italic> = <italic>E</italic><sup>M</sup>. Along the least-action trajectories we therefore get <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline191.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The first equality uses <xref ref-type="disp-formula" rid="eqn28">Eq. 28</xref>, and the second uses <italic>L</italic> = <italic>E</italic><sup>M</sup> just derived for <italic>β</italic> = ∞. This is statement (<italic>i</italic>) of <xref ref-type="statement" rid="the1">Theorem 1</xref>. In the case of successful learning, <italic>E</italic><sup>M</sup> = 0, we also conclude that the cost vanishes, <italic>C</italic> = 0. This is the case because <italic>E</italic><sup>M</sup> = 0 implies <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline192.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for all output neurons <italic>o</italic>. Since <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline193.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, we conclude that <bold><italic>ē</italic></bold><sub><italic>o</italic></sub> = 0, and if the output neurons do not feed back to the network (which we can assume without loss of generality), we conclude that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline194.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>(<italic>ii</italic>) To consider the case of small <italic>β</italic>, we use that the cost <italic>C</italic> can be expressed as <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline195.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This is a direct consequence of how <italic>C</italic> enters in <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline196.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, see <xref ref-type="disp-formula" rid="eqn16">Eq. 16</xref> and <xref ref-type="bibr" rid="c53">Scellier &amp; Bengio, 2017</xref>. We now put this together with <xref ref-type="disp-formula" rid="eqn28">Eq. 28</xref> and the finding that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline197.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Since for the Lipschitz continuous function <italic>L</italic> in <italic>u, W</italic> and <italic>β</italic> (<italic>L</italic> is even smooth in these arguments), the total derivatives interchange (which is a consequence of the Moore-Osgood theorem applied to the limits of the difference quotients), we then get at any <italic>t</italic>,
<disp-formula id="eqn29">
<alternatives><graphic xlink:href="534198v2_eqn29.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The last expression is calculated from the specific form of the Lagrangian (<xref ref-type="disp-formula" rid="eqn17">Eq. 17</xref>), using that by definition <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline198.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Finally, in the absence of output nudging, <italic>β</italic> = 0, we can assume vanishing errors, <bold><italic>ē</italic></bold> = 0, as they solve the self-consistency equation, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline199.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for all <italic>t</italic>, see <xref ref-type="disp-formula" rid="eqn27">Eq. 27</xref>. For these solutions we have <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline200.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Writing out the total derivative of the function <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline201.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> with respect to <italic>β</italic> at <italic>β</italic> = 0 as limit of the difference quotient, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline202.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, using that <bold><italic>g</italic></bold>(0) = 0, we calculate at any <italic>t</italic>,
<disp-formula id="eqn30">
<alternatives><graphic xlink:href="534198v2_eqn30.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here we assume that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline203.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is evaluated at <italic>β &gt;</italic> 0 (that itself approaches 0), while <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline204.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is evaluated at <italic>β</italic> = 0. Combining <xref ref-type="disp-formula" rid="eqn29">Eq. 29</xref> and <xref ref-type="disp-formula" rid="eqn30">30</xref> yields the cost gradient at any <italic>t</italic>,
<disp-formula id="eqn31">
<alternatives><graphic xlink:href="534198v2_eqn31.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
This justifies the gradient learning rule <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline205.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="eqn27">Eq. 27</xref>. Learning is stochastic gradient descent on the expected cost, where stochasticity enters in the randomization of the stimulus and target sequences <bold><italic>r</italic></bold><sub>in</sub>(<italic>t</italic>) and <bold><italic>u</italic></bold>*(<italic>t</italic>). For the regularity statement, see ‘From implicit to explicit differential equations’ in the SI. Notice that this proof works for a very general form of the Lagrangian <italic>L</italic>, until the specific expression for <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline206.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. For a proof in terms of partial derivatives only, see SI, where also a primer on partial and total derivatives is found.</p>
</sec>
<sec id="s4e">
<title>Instantaneous gradient descent on <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline207.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula></title>
<p>The cost <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline208.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> at each time <italic>t</italic> is a function of the voltage <bold><italic>u</italic></bold><sub><bold><italic>o</italic></bold></sub> of the output neurons and the corresponding targets. In a feedforward network, due to the instantaneity of the voltage propagation (<xref ref-type="disp-formula" rid="eqn23">Eq. 23</xref>), <bold><italic>u</italic></bold><sub><bold><italic>o</italic></bold></sub> is in the absence of output nudging (<italic>β</italic> = 0) an instantaneous function of the voltage at the first layer, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline209.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. For initialisation at <italic>t</italic> = −∞, the second term vanishes for all <italic>t</italic> and hence <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline210.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The output voltage <bold><italic>u</italic></bold><sub><bold><italic>o</italic></bold></sub>(<italic>t</italic>) therefore becomes a function <bold><italic>F</italic></bold><sub><italic>W</italic></sub> of the low-pass filtered input rate <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline211.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> that captures the instantaneous network mapping, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline212.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and with this the cost also becomes an instantaneous function of <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline213.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline214.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, namely <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline215.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>For a general network, again assuming <italic>t</italic><sub>0</sub> = −∞, the voltage is determined by the vanishing gradient <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline216.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> with <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline217.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, see <xref ref-type="disp-formula" rid="eqn21">Eq. 21</xref>. For the inclusive treatment of the initial transient see SI, Sects C and D. Remember that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline218.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline219.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. For a given <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline220.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline221.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> at time <italic>t</italic>, the equation <bold><italic>F</italic></bold>(<bold><italic>u</italic></bold>, <italic>t</italic>) = 0 can be locally solved for <bold><italic>u</italic></bold> if the Hessian <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline222.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is invertible, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline223.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This mapping can be restricted to the output voltages <bold><italic>u</italic></bold><sub><bold><italic>o</italic></bold></sub> on the left-hand side, while replacing <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline224.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> in the argument on the right-hand side (even if this again introduces <bold><italic>u</italic></bold><sub><bold><italic>o</italic></bold></sub> there). With this we obtain the instantaneous mapping <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline225.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> from the low-pass filtered input and the output error to the output itself. Notice that for functional feedforward network, the network weight matrix <bold><italic>W</italic></bold><sub>net</sub> is lower triangular, and for small enough <italic>β</italic> the Hessian <bold><italic>H</italic></bold> is therefore always positive definite.</p>
</sec>
<sec id="s4f">
<title>Proving <xref ref-type="statement" rid="the2">Theorem 2</xref> (rt-DeEL)</title>
<p>Here we restrict ourselves to layered network architectures. To prove <xref ref-type="statement" rid="the2">Theorem 2</xref> first assume that interneurons receive no nudging (<italic>β</italic><sup>I</sup> = 0) and only the lateral interneuron-to-pyramidal weights <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline225a.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are plastic. This is already sufficient to prove the rt-DeEL theorem. Yet, simulations showed that shaping the lateral pyramidal-to-interneuron weights <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline226.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> so that it mimics the upper layer activity helps tremendously in learning a correct error representation. We consider this case of learning <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline227.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> later.</p>
<p>If the microcircuits is ought to correctly implement error backpropagation, all local prediction errors <bold><italic>ē</italic></bold><sub><italic>l</italic></sub> must vanish in the absence of output nudging (<italic>β</italic> = 0) as there is no target error. Consequently, any remaining errors in the network are caused by a misalignment of the lateral microcircuit, and we show how learning the interneuron-to-pyramidal weights <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline228.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>s corrects for such misalignments.</p>
<p>To define the gradient descent plasticity of the weights <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline228a.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> from the interneurons to the pyramidal neurons, we consider the apical error formed by the difference of top-down input and interneuron input, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline229.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and define the apical mismatch energy as <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline230.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Gradient descent along this energy with respect to <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline230a.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> yields
<disp-formula id="eqn32">
<alternatives><graphic xlink:href="534198v2_eqn32.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
evaluated online while presenting input patterns from the data distribution to the network. We assume that the apical contribution to the somatic voltage is further modulated by the somatic spike rate, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline231.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. After successful learning, the top-down input <bold><italic>B</italic></bold><sub><italic>l</italic></sub><bold><italic>u</italic></bold><sub><italic>l</italic>+1</sub> is fully subtracted away by the lateral input in the apical compartment, and we have
<disp-formula id="eqn33">
<alternatives><graphic xlink:href="534198v2_eqn33.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Once this condition is reached, the network achieves a state where, over the activity space spanned by the data, top-down prediction errors throughout the network vanish,
<disp-formula id="eqn34">
<alternatives><graphic xlink:href="534198v2_eqn34.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
We show that this top-down prediction error, after the successful learning of the microcircuit, shares the properties of error-backpropagation for a suitable backprojection weights <bold><italic>B</italic></bold>.</p>
<p>Due to the vanishing prediction errors, pyramidal cells only receive bottom-up input <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline232.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Using this expression as well as the expression for interneuron membrane potentials without top-down nudging (<italic>β</italic><sup>I</sup> = 0 in <xref ref-type="disp-formula" rid="eqn9">Eq. 9</xref>), <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline233.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and plugging both into <xref ref-type="disp-formula" rid="eqn33">Eq. 33</xref>, we get
<disp-formula id="eqn35">
<alternatives><graphic xlink:href="534198v2_eqn35.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Assuming that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline234.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> has full rank and the low-pass filtered rates <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline235.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> span the full <italic>n</italic><sub><italic>l</italic></sub> dimension of layer <italic>l</italic> when sampled across the data set we conclude that
<disp-formula id="eqn36">
<alternatives><graphic xlink:href="534198v2_eqn36.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
In other words, the loop via upper layer and back is learned to be matched by a lateral loop through the interneurons. <xref ref-type="disp-formula" rid="eqn36">Eq. 36</xref> imposes a restriction on the minimal number of interneurons <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline236.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> at layer <italic>l</italic>. In fact, the matrix product <bold><italic>B</italic></bold><sub><italic>l</italic></sub><bold><italic>W</italic></bold><sub><italic>l</italic>+1</sub> maps a <italic>n</italic><sub><italic>l</italic></sub>-dimensional space onto itself via <italic>n</italic><sub><italic>l</italic>+1</sub>-dimensional space. The maximal rank of the this matrix product is limited by the smallest dimension, i.e. rank(<bold><italic>B</italic></bold><sub><italic>l</italic></sub><bold><italic>W</italic></bold><sub><italic>l</italic>+1</sub>) ≤ min(<italic>n</italic><sub><italic>l</italic></sub>, <italic>n</italic><sub><italic>l</italic>+1</sub>). Analogously, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline237.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. But since the two ranks are the same according to <xref ref-type="disp-formula" rid="eqn36">Eq. 36</xref>, we conclude that in general <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline238.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> must hold, i.e. there should be at least as many interneurons at layer <italic>l</italic> as the lowest number of pyramidal neurons at either layer <italic>l</italic> or <italic>l</italic>+1. Note that by choosing <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline239.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> as in (<xref ref-type="bibr" rid="c52">Sacramento <italic>et al</italic>., 2018</xref>) (or <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline240.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> as in this work), the conditions is fulfilled.</p>
<p>With <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline241.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <xref ref-type="disp-formula" rid="eqn36">Eq. 36</xref>, the top-down prediction error from <xref ref-type="disp-formula" rid="eqn34">Eq. 34</xref>, in the presence of output nudging (<italic>β &gt;</italic> 0), can be written in the backpropagation form
<disp-formula id="eqn37a">
<alternatives><graphic xlink:href="534198v2_eqn37a.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn37b">
<alternatives><graphic xlink:href="534198v2_eqn37b.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn37c">
<alternatives><graphic xlink:href="534198v2_eqn37c.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Finally, the simulations showed that learning the lateral weights in the microcircuit greatly benefits from also adapting the pyramidal-to-interneuron weights <bold><italic>W</italic></bold><sup>IP</sup> by gradient descent on <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline242.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, using top-down nudging of the inhibitory neurons (<italic>β</italic><sup>I</sup> <italic>&gt;</italic> 0),
<disp-formula id="eqn38">
<alternatives><graphic xlink:href="534198v2_eqn38.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
After learning we have <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline243.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and plugging in <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline244.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="eqn9">Eq. 9</xref>), we obtain <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline245.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Since <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline246.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, we conclude as before,
<disp-formula id="eqn39">
<alternatives><graphic xlink:href="534198v2_eqn39.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The top-down weights <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline247.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> that nudge the lower-layer interneurons has randomized entries and may be considered as full rank. If there are less pyramidal neurons in the upper layer than interneurons in the lower layer, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline248.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> selects a subspace in the interneuron space of dimension <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline249.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This seems to simplify the learning of the interneuron-to-pyramidal cell connections <italic>W</italic><sup>PI</sup>. In fact, this learning now has only to match the <italic>n</italic><sub><italic>l</italic>+1</sub>-dimensional interneuron subspace embedded in <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline249a.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> dimensions to an equal (<italic>n</italic><sub><italic>l</italic>+1</sub>-)dimensional pyramidal cell subspace emedded in <italic>n</italic><sub><italic>l</italic></sub> dimensions.</p>
<p>Learning of the interneuron-to-pyramidal cell connections works with the interneuron nudging as before, and combining <xref ref-type="disp-formula" rid="eqn36">Eqs 36</xref> with 39 yields the ‘loop consistency’
<disp-formula id="eqn40">
<alternatives><graphic xlink:href="534198v2_eqn40.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The learning of the microcircuit was described in the absence of output nudging. Conceptually, this is not a problem as one could introduce a pre-learning phase where the lateral connections are first correctly aligned before learning of the feedforward weights begins. In simulations we find that both the lateral connections as well as the forward connections can be trained simultaneously, without the need for such a pre-learning phase. We conjecture that this is due to the fact that our plasticity rules are gradient descent on the energy functions <italic>L, E</italic><sup>PI</sup> and <italic>E</italic><sup>IP</sup> respectively.</p>
</sec>
<sec id="s4g">
<title>Simulation details</title>
<p>The voltage dynamics is solved by a forward-Euler scheme <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline250.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The derivative <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline251.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is calculated either through (<italic>i</italic>) the implicit differential equation (<xref ref-type="disp-formula" rid="eqn7a">Eq. 7</xref>) yielding <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline252.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, or (<italic>ii</italic>) by isolating <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline253.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and solving for the explicit differential equation <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline254.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, as explained after <xref ref-type="disp-formula" rid="eqn49">Eq. 49</xref> in the SI.</p>
<p>(<italic>i</italic>) The implicit differential equation, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline255.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, see <xref ref-type="disp-formula" rid="eqn22">Eq. 22</xref>, is iteratively solved by assigning <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline256.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and calculating the error <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline257.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> with <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline258.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline259.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>This iteration exponentially converges to a fixed point <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline260.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> on a time scale <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline261.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where 1 − <italic>k &gt;</italic> 0 is the smallest Eigenvalue of the Hessian <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline262.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, see SI, Sect. C.</p>
<p>(<italic>ii</italic>) The explicit differential equation is obtained by eliminating the <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline263.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> from the right-hand side of the implicit differential equation. Since <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline264.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> enters linearly we get <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline265.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> with <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline266.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The explicit form is obtained by matrix inversion, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline267.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, as the Hessian is invertible if it is strictly positive definite. The external input and the target enter through <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline268.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where the derivative of the target voltage is only added for the output neurons <bold><italic>o</italic></bold>. This explicit differential equation is shown to be contractive in the sense that for each input trajectory <bold><italic>r</italic></bold><sub>in</sub>(<italic>t</italic>) and target trajectory <bold><italic>u</italic></bold>*(<italic>t</italic>), the voltage trajectory <bold><italic>u</italic></bold>(<italic>t</italic>) is locally attracting for neighbouring trajectories. This local attracting trajectory is the vanishing-gradient trajectory <bold><italic>F</italic></bold>(<bold><italic>u</italic></bold>, <italic>t</italic>) = 0, and the gradient remains 0 even if the input contains delta-functions, see SI Sect. D.</p>
<p>Solving the explicit differential equation seems to be more robust when the learning rate for <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline269.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> gets larger. The explicit form is also less sensitive to large Euler steps <italic>dt</italic>, see SI Sect. C. By this reason, the ordinary differential equations (ODE) were solved in the explicit form when including plasticity <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline270.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The algorithms are summarized as follows, once without interneurons (Algo 1), and once with interneurons (Algo 2):</p>
</sec>
<sec id="s4h">
<title>Details for <xref rid="fig3" ref-type="fig">Fig. 3b</xref></title>
<p>Color coded snapshot of cortical local field potentials (LFPs) in a human brain from 56 deep iEEG electrodes at various locations, converted with the sigmoidal voltage-to-rate function <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline271.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and plotted onto a standard Talairach Brain (<xref ref-type="bibr" rid="c58">Talairach &amp; Tournoux, 1988</xref>). The iEEG data is from a patient with pharmacoresistant epilepsy and electrodes implanted during presurgical evaluation, extracted from the data release of <xref ref-type="bibr" rid="c11">Burrello <italic>et al</italic>., 2019</xref>. The locations of the electrodes are chosen in accordance with plausibilty, as the original positions of the electrodes were omitted due to ethical standards to prevent patient identification.</p>
</sec>
<sec id="s4i">
<title>Details for <xref rid="fig3" ref-type="fig">Fig. 3c</xref></title>
<p>Simulations of the voltage dynamics (<xref ref-type="disp-formula" rid="eqn7a">Eq. 7a</xref>) and weight dynamics (<xref ref-type="disp-formula" rid="eqn8">Eq. 8</xref>), with learning rate <italic>η</italic> = 10<sup><italic>−</italic>3</sup>, step size <italic>dt</italic> = 1 ms for the forward Euler integration, membrane time constant <italic>τ</italic> = 10 ms and logistic activation function. Weights were initialized randomly from a normal distribution 𝒩(0, 0.1<sup>2</sup>) with a cut-off at ±0.3. The number of neurons in the network 𝒩 was <italic>n</italic> = 96, among them 56 output neurons 𝒪 ⊂ 𝒩 that were simultaneously nudged, and 40 hidden neurons. During training, all output neurons were nudged simultaneously (with <italic>β</italic> = 0.1), whereas during testing, only 42 out of 56 neurons were nudged, the remaining 14 left to reproduce the traces. Data points of the iEEG signal were sampled with a frequency of 512Hz. For simplicity, we therefore assumed that successive data points are separated by 2ms, and up-sampled the signal via simple interpolation to 1ms resolution as required by our integration scheme. Furthermore, the raw values were normalized by dividing them by a factor of 200 to ensure that they are approximately in a range of ±1 −2. Training and testing was done on two separate 8s traces of the iEEG recording. Same data as in <xref rid="fig3" ref-type="fig">Fig. 3b1</xref>.</p>
<statement id="alg1">
<label>Algorithm 1</label>
<title>with projection neurons only, for <xref rid="fig3" ref-type="fig">Figs 3</xref> &amp; <xref ref-type="fig" rid="fig4">4</xref></title>
<p>(using the explicit ODE, i.e. Step 12 instead of 11)</p>
<p><fig id="alg1a" position="float" fig-type="figure">
<graphic xlink:href="534198v2_alg1.tif" mimetype="image" mime-subtype="tiff"/>
</fig></p>
</statement>
<statement id="alg2">
<label>Algorithm 2</label>
<title>including plastic interneurons, for <xref rid="fig5" ref-type="fig">Fig. 5</xref></title>
<p>(using the explicit ODE, i.e. Step 13 instead of 12)</p>
<p><fig id="alg2a" position="float" fig-type="figure">
<graphic xlink:href="534198v2_alg2.tif" mimetype="image" mime-subtype="tiff"/>
</fig></p>
</statement>
</sec>
<sec id="s4j">
<title>Details for <xref rid="fig4" ref-type="fig">Fig. 4</xref></title>
<p>Simulation of the neuronal and synaptic dynamics as given by <xref ref-type="disp-formula" rid="eqn7a">Eq. 7a</xref>, <xref ref-type="disp-formula" rid="eqn7b">Eq. 7b</xref> and <xref ref-type="disp-formula" rid="eqn8">Eq. 8</xref>. For 5 ms, 10 ms and 50 ms presentation time, we used an integration step size of <italic>dt</italic> = 0.05 ms, <italic>dt</italic> = 0.1 ms and <italic>dt</italic> = 0.5 ms, respectively (and <italic>dt</italic> = 1 ms otherwise). As an activation function, we used the step-linear function (hard sigmoidal) with <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline272.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for <italic>u</italic>≤ 0, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline273.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for <italic>u</italic>≥ 1 and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline274.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> in between. The learning rate was initially set to <italic>η</italic> = 10<sup><italic>−</italic>3</sup> and then reduced to <italic>η</italic> = 10<sup><italic>−</italic>4</sup> after 22 000 s. The nudging strength was <italic>β</italic> = 0.1 and the membrane time constant <italic>τ</italic> = 10 ms. In these simulations (and only for these) we assumed that at each presynaptic layer <italic>l</italic> = 0, 1, .., <italic>n</italic> 1 there is a first neuron indexed by 0 that fires with constant rate <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline275.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, effectively allowing the postsynaptic neurons <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline276.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> to learn a bias through the first column of the weight matrix <bold><italic>W</italic></bold><sub><italic>l</italic>+1</sub>. Weights were initialized randomly from a normal distribution 𝒩(0, 0.01<sup>2</sup>) with a cut-off at ±0.03. For an algorithmic conversion see the scheme below. In <xref rid="fig4" ref-type="fig">Fig. 4c1</xref>, ‘rt-DeEP w/o lookahead’ is based on the dynamics <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline277.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. For ‘<inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline278.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> w/o error + backprop’, we use <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline279.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> as the forward model (so without error terms on the membrane potential, but a prospective <bold><italic>r</italic></bold>), and calculate weight updates using error backpropagation. In <xref rid="fig4" ref-type="fig">Fig. 4c2</xref>, we provide three controls: the test error for (i) a standard shallow artificial neural network trained on MNIST (black dashed line), (ii) rt-DeEP without prospective coding (as in <xref rid="fig4" ref-type="fig">Fig. 4c1</xref>), but in c2 with plasticity only turned on when the network is completely stationary, i.e., after waiting for several 100ms, such that synaptic weights are not changed during transients (orange dashed line, denoted by ‘w/o transients’), and (iii) an equivalent artificial neural network, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline280.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, trained using error backpropagation (black dashed line, ‘standard backprop’).</p>
</sec>
<sec id="s4k">
<title>Details for <xref rid="fig5" ref-type="fig">Fig. 5</xref></title>
<p>Simulation of neuronal and synaptic dynamics with plastic microcircuit, i.e., the pyramidal-to-interneuron and lateral weights of the microcircuit learned during training.</p>
<p>For the results shown in <xref rid="fig5" ref-type="fig">Fig. 5c2</xref>, the following parameters were used. As an activation function, we used a hard sigmoid function and the membrane time constant was set to <italic>τ</italic> = 10ms. Image presentation time is 100 ms. Forward, pyramidal-to-interneuron and interneuron-to-pyramidal weights were initialized randomly from a normal distribution 𝒩(0, 0.01<sup>2</sup>) with a cut-off at ±0.03. All learning rates were chosen equal <italic>η</italic> = 10<sup><italic>−</italic>3</sup> and were subsequently reduced to <italic>η</italic> = 10<sup><italic>−</italic>4</sup> after 22000s training time. The nudging parameters were set to <italic>β</italic> = 0.1 and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline281.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The feedback connections <bold><italic>B</italic></bold><sub><italic>l</italic></sub> and the nudging matrices <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline281a.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> were initialized randomly from a normal distribution 5 · 𝒩(0, 0.01<sup>2</sup>) with a cut-off at ±0.15. The used integration step size was <italic>dt</italic> = 0.25ms. All weights were trained simultaneously. For an algorithmic conversion see the scheme below. The interneuron membrane potential was calculated by <xref ref-type="disp-formula" rid="eqn9">Eq. 9</xref> with a linear transfer function.</p>
</sec>
</sec>
<sec id="s5">
<title>Author contributions</title>
<p>WS, MAP and DD designed the conceptual approach. WS, DD, MAP, AFK, JJ, JS and YB contributed to different aspects of the framework and model. WS, DD, MAP and AFK derived different components of the theory. DD, BE and AFK performed the simulations. MAP, DD, JJ and BE wrote the first draft of the manuscript. WS wrote the final draft.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name>, <string-name><surname>Varela</surname>, <given-names>J. a.</given-names></string-name>, <string-name><surname>Sen</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Nelson</surname>, <given-names>S. B.</given-names></string-name> <article-title>Synaptic depression and cortical gain control</article-title>. <source>Science</source> <volume>275</volume>, <fpage>220</fpage>–<lpage>224</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Ackley</surname>, <given-names>D. H.</given-names></string-name>, <string-name><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name> &amp; <string-name><surname>Sejnowski</surname>, <given-names>T. J.</given-names></string-name> <article-title>A learning algorithm for Boltzmann machines</article-title>. <source>Cognitive Science</source> <volume>9</volume>, <fpage>147</fpage>–<lpage>169</lpage> (<year>1985</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="other"><string-name><surname>Akrout</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Humphreys</surname>, <given-names>P. C.</given-names></string-name>, <string-name><surname>Lillicrap</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Tweed</surname>, <given-names>D.</given-names></string-name> <article-title>Deep learning without weight transport</article-title>. <source>arXiv</source> (<year>2019</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Amirikian</surname>, <given-names>B. R.</given-names></string-name> &amp; <string-name><surname>Lukashin</surname>, <given-names>A. V.</given-names></string-name> <article-title>A neural network learns trajectory of motion from the least action principle</article-title>. <source>Biological Cybernetics</source> <volume>66</volume>, <fpage>261</fpage>–<lpage>264</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Bannon</surname>, <given-names>N. M.</given-names></string-name>, <string-name><surname>Chistiakova</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Volgushev</surname>, <given-names>M.</given-names></string-name> <article-title>Synaptic Plasticity in Cortical Inhibitory Neurons: What Mechanisms May Help to Balance Synaptic Weight Changes?</article-title> <source>Frontiers in Cellular Neuroscience</source> <volume>14</volume> (<year>2020</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Bartolozzi</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Indiveri</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Donati</surname>, <given-names>E.</given-names></string-name> <article-title>Embodied neuromorphic intelligence</article-title>. <source>Nature Communications</source> <volume>13</volume>, <fpage>1</fpage>–<lpage>14</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Bastos</surname>, <given-names>A. M.</given-names></string-name> <etal>et al.</etal> <article-title>Canonical Microcircuits for Predictive Coding</article-title>. <source>Neuron</source> <volume>76</volume>, <fpage>695</fpage>–<lpage>711</lpage>. <ext-link ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0896627312009592">http://linkinghub.elsevier.com/retrieve/pii/S0896627312009592</ext-link> (<year>2012</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Bellec</surname>, <given-names>G.</given-names></string-name> <etal>et al.</etal> <article-title>A solution to the learning dilemma for recurrent networks of spiking neurons</article-title>. <source>Nature Communications</source> <volume>11</volume>, <fpage>1</fpage>–<lpage>15</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Blom</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Feuerriegel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Bode</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Hogendoorn</surname>, <given-names>H.</given-names></string-name> <article-title>Predictions drive neural representations of visual events ahead of incoming sensory information</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>117</volume>, <fpage>7510</fpage>–<lpage>7515</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Borovik</surname>, <given-names>A.</given-names></string-name> <article-title>A mathematician’s view of the unreasonable ineffectiveness of mathematics in biology</article-title>. <source>Biosystems</source> <volume>205</volume>, <fpage>104410</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="other"><string-name><surname>Burrello</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cavigelli</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Schindler</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Benini</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Rahimi</surname>, <given-names>A.</given-names></string-name> <source>Laelaps: An Energy-Efficient Seizure Detection Algorithm from Long-term Human iEEG Recordings without False Alarms in Proceedings of the 2019 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</source> (<year>2019</year>), <fpage>752</fpage>–<lpage>757</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="book"><string-name><surname>Coopersmith</surname>, <given-names>J.</given-names></string-name> <source>The Lazy Universe: An Introduction to the Principle of Least Action</source> (<publisher-name>Oxford University Press</publisher-name>, <publisher-loc>New York</publisher-loc>, <year>2017</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Dimitriou</surname>, <given-names>M.</given-names></string-name> <article-title>Enhanced Muscle Afferent Signals during Motor Learning in Humans</article-title>. <source>Current Biology</source> <volume>26</volume>, <fpage>1062</fpage>– <lpage>1068</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2016.02.030</pub-id> (<year>2016</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Dimitriou</surname>, <given-names>M.</given-names></string-name> <article-title>Human muscle spindles are wired to function as controllable signal-processing devices</article-title>. <source>eLife</source> <volume>11</volume>, <fpage>1</fpage>–<lpage>14</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Dimitriou</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Edin</surname>, <given-names>B. B.</given-names></string-name> <article-title>Human muscle spindles act as forward sensory models</article-title>. <source>Current Biology</source> <volume>20</volume>, <fpage>1763</fpage>– <lpage>1767</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2010.08.049</pub-id> (<year>2010</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="other"><string-name><surname>Feldman</surname>, <given-names>A. G.</given-names></string-name> &amp; <string-name><surname>Levin</surname>, <given-names>M. F.</given-names></string-name> <source>in Progress in Motor Control</source> (ed <person-group person-group-type="editor"><string-name><surname>Sternad</surname>, <given-names>D.</given-names></string-name></person-group>) <fpage>699</fpage>–<lpage>726</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="web"><string-name><surname>Feynman</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Leighton</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Sands</surname>, <given-names>M.</given-names></string-name> <source>The Feynman Lectures on Physics, Vol. II: Mainly Electromagnetism and Matter</source>, Chapt. 19 <ext-link ext-link-type="uri" xlink:href="https://www.feynmanlectures.caltech.edu/II_19.html">https://www.feynmanlectures.caltech.edu/II_19.html</ext-link> (Basic Books, <year>2011</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name> <article-title>The free-energy principle: a unified brain theory?</article-title> <source>Nature Reviews Neuroscience</source> <volume>11</volume>, <fpage>127</fpage>–<lpage>138</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="web"><string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal> <article-title>A free energy principle made simpler but not too simple</article-title>. <source>arXiv, 1–42</source>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2201.06387">https://arxiv.org/abs/2201.06387</ext-link> (<year>2022</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Göltz</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title>Fast and energy-efficient neuromorphic deep learning with first-spike times</article-title>. <source>Nature Machine Intelligence</source> <volume>3</volume>, <fpage>823</fpage>–<lpage>835</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Guerguiev</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lillicrap</surname>, <given-names>T. P.</given-names></string-name> &amp; <string-name><surname>Richards</surname>, <given-names>B. A.</given-names></string-name> <article-title>Towards deep learning with segregated dendrites</article-title>. <source>eLife</source> <volume>6</volume>, <fpage>1</fpage>–<lpage>37</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Haider</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal> <article-title>Latent Equilibrium: Arbitrarily fast computation with arbitrarily slow neurons</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>34</volume> (<year>2021</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Hassabis</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kumaran</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Botvinick</surname>, <given-names>M.</given-names></string-name> <article-title>Neuroscience-Inspired Artificial Intelligence</article-title>. <source>Neuron</source> <volume>95</volume>, <fpage>245</fpage>–<lpage>258</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2017.06.011</pub-id> (<year>2017</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Herzfeld</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Vaswani</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Marko</surname>, <given-names>M. K.</given-names></string-name> &amp; <string-name><surname>Shadmehr</surname>, <given-names>R.</given-names></string-name> <article-title>A memory of errors in sensorimotor learning</article-title>. <source>Science</source> <volume>345</volume>, <fpage>1349</fpage>–<lpage>1353</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/25123484">http://www.ncbi.nlm.nih.gov/pubmed/25123484</ext-link> (<year>2014</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="other"><string-name><surname>Hinton</surname>, <given-names>G.</given-names></string-name> <source>The Forward-Forward Algorithm : Some Preliminary Investigations in NeurIPS</source> (<year>2022</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Hodgkin</surname>, <given-names>A. L.</given-names></string-name> &amp; <string-name><surname>Huxley</surname>, <given-names>A. F.</given-names></string-name> <article-title>A quantitative description of membrane current and its application to conduction and excitation in nerve</article-title>. <source>Bulletin of Mathematical Biology</source> <volume>117</volume>, <fpage>500</fpage>–<lpage>544</lpage> (<year>1952</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Hopfield</surname>, <given-names>J. J.</given-names></string-name> <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proc. Nat. Acad. Sci. USA</source> <volume>79</volume>, <fpage>2554</fpage>–<lpage>2558</lpage> (<year>1982</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="other"><string-name><surname>Karkar</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ayed</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>de Bézenac</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Gallinari</surname>, <given-names>P.</given-names></string-name> <article-title>A Principle of Least Action for the Training of Neural Networks</article-title>. <source>Lecture Notes in Computer Science 12458 LNAI, 101–117</source>. arXiv: <pub-id pub-id-type="arxiv">2009.08372</pub-id> (<year>2021</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Kawato</surname>, <given-names>M.</given-names></string-name> <article-title>Internal models for motor control and trajectory planning</article-title>. <source>Current Opinion in Neurobiology</source> <volume>9</volume>, <fpage>718</fpage>–<lpage>727</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Köndgen</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal> <article-title>The dynamical response properties of neocortical neurons to temporally modulated noisy inputs in vitro</article-title>. <source>Cerebral cortex</source> <volume>18</volume>, <fpage>2086</fpage>–<lpage>2097</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="other"><string-name><surname>Kunin</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal> <source>Two routes to scalable credit assignment without weight symmetry in International Conference on Machine Learning</source> (<year>2020</year>), <fpage>5511</fpage>–<lpage>5521</lpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Larkum</surname>, <given-names>M.</given-names></string-name> <article-title>A cellular mechanism for cortical associations: An organizing principle for the cerebral cortex</article-title>. <source>Trends in Neurosciences</source> <volume>36</volume>, <fpage>141</fpage>–<lpage>151</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Latash</surname>, <given-names>M. L.</given-names></string-name> <article-title>Motor synergies and the equilibrium-point hypothesis</article-title>. <source>Motor Control</source> <volume>14</volume>, <fpage>294</fpage>–<lpage>322</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Latash</surname>, <given-names>M. L.</given-names></string-name> <article-title>Muscle coactivation: Definitions, mechanisms, and functions</article-title>. <source>Journal of Neurophysiology</source> <volume>120</volume>, <fpage>88</fpage>–<lpage>104</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="web"><string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name> <source>The MNIST database of handwritten digits</source>. <ext-link ext-link-type="uri" xlink:href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</ext-link> (<year>1998</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="book"><string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Chopra</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hadsell</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Ranzato</surname>, <given-names>M. A.</given-names></string-name> &amp; <string-name><surname>Huang</surname>, <given-names>F. J.</given-names></string-name> <source>xsin Predicting Structured Data</source> (eds <person-group person-group-type="editor"><string-name><surname>Bakir</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Hofman</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Schoelkopf</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Smola</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Taskar</surname>, <given-names>B</given-names></string-name></person-group>.)<fpage>1</fpage>–<lpage>59</lpage> (<publisher-name>MIT Press</publisher-name><year>2006</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Li</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>Coordinated alpha and gamma control of muscles and spindles in movement and posture</article-title>. <source>Frontiers in Computational Neuroscience</source> <volume>9</volume>, <fpage>1</fpage>–<lpage>15</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Lillicrap</surname>, <given-names>T. P.</given-names></string-name>, <string-name><surname>Cownden</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Tweed</surname>, <given-names>D. B.</given-names></string-name> &amp; <string-name><surname>Akerman</surname>, <given-names>C. J.</given-names></string-name> <article-title>Random synaptic feedback weights support error backpropagation for deep learning</article-title>. <source>Nature communications</source> <volume>7</volume>, <fpage>1</fpage>–<lpage>10</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Lillicrap</surname>, <given-names>T. P.</given-names></string-name>, <string-name><surname>Santoro</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Marris</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Akerman</surname>, <given-names>C. J.</given-names></string-name> &amp; <string-name><surname>Hinton</surname>, <given-names>G.</given-names></string-name> <article-title>Backpropagation and the brain</article-title>. <source>Nature Reviews Neuroscience</source> <volume>21</volume>, <fpage>335</fpage>–<lpage>346</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="web"><string-name><surname>Max</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal> <article-title>Learning efficient backprojections across cortical hierarchies in real time</article-title>. <source>arXiv, 1–31</source>. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2212.10249">http://arxiv.org/abs/2212.10249</ext-link> (<year>2022</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="other"><string-name><surname>Mesnard</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Vignoud</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Sacramento</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Senn</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> <article-title>Ghost Units Yield Biologically Plausible Backprop in Deep Neural Networks</article-title>. <source>arXiv</source> (<year>2019</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Meulemans</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Farinha</surname>, <given-names>M. T.</given-names></string-name>, <etal>et al.</etal> <article-title>Credit Assignment in Neural Networks through Deep Feedback Control</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>34</volume> (<year>2021</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Meulemans</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zucchet</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Kobayashi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>von Oswald</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Sacramento</surname>, <given-names>J.</given-names></string-name> <article-title>The least-control principle for local learning at equilibrium</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>35</volume> (<year>2022</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Ostojic</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>Neuronal morphology generates high-frequency firing resonance</article-title>. <source>Journal of Neuroscience</source> <volume>35</volume>, <fpage>7056</fpage>–<lpage>7068</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Palmer</surname>, <given-names>S. E.</given-names></string-name>, <string-name><surname>Marre</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Berry</surname>, <given-names>M. J.</given-names></string-name> &amp; <string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name> <article-title>Predictive information in a sensory population</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>112</volume>, <fpage>6908</fpage>–<lpage>6913</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Papaioannou</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Dimitriou</surname>, <given-names>M.</given-names></string-name> <article-title>Goal-dependent tuning of muscle spindle receptors during movement preparation</article-title>. <source>Science Advances</source> <volume>7</volume>, <fpage>1</fpage>–<lpage>14</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Pfister</surname>, <given-names>J.-P.</given-names></string-name>, <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Lengyel</surname>, <given-names>M.</given-names></string-name> <article-title>Synapses with short-term plasticity are optimal estimators of presynaptic membrane potentials</article-title>. <source>Nature Neuroscience</source> <volume>13</volume>, <fpage>1271</fpage>–<lpage>1275</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><surname>Poirazi</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Papoutsi</surname>, <given-names>A.</given-names></string-name> <article-title>Illuminating dendritic function with computational models</article-title>. <source>Nature Reviews Neuroscience</source> <volume>21</volume>, <fpage>303</fpage>–<lpage>321</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Rao</surname>, <given-names>R. P. N.</given-names></string-name> &amp; <string-name><surname>Ballard</surname>, <given-names>D. H.</given-names></string-name> <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nature Neuroscience</source> <volume>2</volume>, <fpage>79</fpage>–<lpage>87</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.nature.com/doifinder/10.1038/4580">http://www.nature.com/doifinder/10.1038/4580</ext-link> (<year>1999</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Richards</surname>, <given-names>B. A.</given-names></string-name> <etal>et al.</etal> <article-title>A deep learning framework for neuroscience</article-title>. <source>Nature neuroscience</source> <volume>22</volume>, <fpage>1761</fpage>–<lpage>1770</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>Rumelhart</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name> &amp; <string-name><surname>Williams</surname>, <given-names>R. J.</given-names></string-name> <article-title>Learning Representations by Back-propagating Errors</article-title>. <source>Nature</source> <volume>323</volume>, <fpage>533</fpage>–<lpage>536</lpage> (<year>1986</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><surname>Sacramento</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ponte Costa</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Senn</surname>, <given-names>W.</given-names></string-name> <article-title>Dendritic cortical microcircuits approximate the backpropagation algorithm</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>31</volume>, <fpage>8721</fpage>–<lpage>8732</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><surname>Scellier</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> <article-title>Equilibrium propagation: Bridging the gap between energy-based models and backpropagation</article-title>. <source>Frontiers in computational neuroscience</source> <volume>11</volume>, <fpage>24</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><surname>Schiess</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Urbanczik</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Senn</surname>, <given-names>W.</given-names></string-name> <article-title>Somato-dendritic Synaptic Plasticity and Error-backpropagation in Active Dendrites</article-title>. <source>PLoS Computational Biology</source> <volume>12</volume>, <fpage>1</fpage>–<lpage>18</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><surname>Simonetto</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dall’Anese</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Paternain</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Leus</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Giannakis</surname>, <given-names>G. B.</given-names></string-name> <article-title>Time-Varying Convex Optimization: Time-Structured Algorithms and Applications</article-title>. <source>Proceedings of the IEEE</source> <volume>108</volume>, <fpage>2032</fpage>–<lpage>2048</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="other"><string-name><surname>Song</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal> <article-title>Inferring Neural Activity Before Plasticity: A Foundation for Learning Beyond Backpropagation</article-title>. <source>bioRxiv</source> (<year>2022</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><string-name><surname>Takahashi</surname>, <given-names>N.</given-names></string-name> <etal>et al.</etal> <article-title>Active dendritic currents gate descending cortical outputs in perception</article-title>. <source>Nature Neuroscience</source> <volume>23</volume>, <fpage>1277</fpage>–<lpage>1285</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-020-0677-8</pub-id> (<year>2020</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="book"><string-name><surname>Talairach</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Tournoux</surname>, <given-names>P.</given-names></string-name> <source>Co-planar stereotaxic atlas of the human brain: 3-Dimensional proportional system: An approach to cerebral imaging</source> (<publisher-name>Thieme Medical Publishers, Inc</publisher-name>., <publisher-loc>New York</publisher-loc> <year>1988</year>).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><string-name><surname>Theis</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Rózsa</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Katona</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Schmitz</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Johenning</surname>, <given-names>F. W.</given-names></string-name> <article-title>Voltage gated calcium channel activation by backpropagating action potentials downregulates NMDAR function</article-title>. <source>Frontiers in Cellular Neuroscience</source> <volume>12</volume>, <fpage>1</fpage>–<lpage>14</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="book"><string-name><surname>Todorov</surname>, <given-names>E.</given-names></string-name> <source>in The Bayesian Brain</source> (ed<person-group person-group-type="editor"><string-name><surname>Doya</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ishii</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pouget</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Rao</surname>,<given-names>R. P</given-names></string-name></person-group>.) <volume>12</volume>: <fpage>1</fpage>–<lpage>28</lpage> (<publisher-name>MIT Press</publisher-name>, <year>2006</year>).</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><string-name><surname>Todorov</surname>, <given-names>E.</given-names></string-name> <article-title>Optimality principles in sensorimotor control</article-title>. <source>Nature Neuroscience</source> <volume>7</volume>, <fpage>907</fpage>–<lpage>915</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><string-name><surname>Todorov</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Jordan</surname>, <given-names>M. I.</given-names></string-name> <article-title>Optimal feedback control as a theory of motor coordination</article-title>. <source>Nature Neuroscience</source> <volume>5</volume>, <fpage>1226</fpage>–<lpage>1235</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><string-name><surname>Ulrich</surname>, <given-names>D.</given-names></string-name> <article-title>Dendritic resonance in rat neocortical pyramidal cells</article-title>. <source>Journal of Neurophysiology</source> <volume>87</volume>, <fpage>2753</fpage>–<lpage>2759</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><string-name><surname>Urbanczik</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Senn</surname>, <given-names>W.</given-names></string-name> <article-title>Learning by the Dendritic Prediction of Somatic Spiking</article-title>. <source>Neuron</source> <volume>81</volume>, <fpage>521</fpage>–<lpage>528</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><string-name><surname>Vogels</surname>, <given-names>T. P.</given-names></string-name>, <string-name><surname>Sprekeler</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Zenke</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Clopath</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name> <article-title>Inhibitory Plasticity Balances Excitation and Inhibition in Sensory Pathways and Memory Networks</article-title>. <source>Science</source> <volume>334</volume>, <fpage>1569</fpage>–<lpage>1573</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>X. D.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Yao</surname>, <given-names>H.</given-names></string-name> <article-title>Cumulative latency advance underlies fast visual processing in desynchronized brain state</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>111</volume>, <fpage>515</fpage>–<lpage>520</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><string-name><surname>Whittington</surname>, <given-names>J. C.</given-names></string-name> &amp; <string-name><surname>Bogacz</surname>, <given-names>R.</given-names></string-name> <article-title>An approximation of the error backpropagation algorithm in a predictive coding network with local Hebbian synaptic plasticity</article-title>. <source>Neural computation</source> <volume>29</volume>, <fpage>1229</fpage>–<lpage>1262</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><string-name><surname>Whittington</surname>, <given-names>J. C.</given-names></string-name> &amp; <string-name><surname>Bogacz</surname>, <given-names>R.</given-names></string-name> <article-title>Theories of error back-propagation in the brain</article-title>. <source>Trends in cognitive sciences</source> <volume>23</volume>, <fpage>235</fpage>–<lpage>250</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><string-name><surname>Wigner</surname>, <given-names>E.</given-names></string-name> <article-title>The Unreasonable Effectiveness of Mathematics in the Natural Sciences. Richard Courant lecture in mathematical sciences</article-title>. <source>Communications in pure and applied mathematics</source> <volume>13</volume>, <fpage>1</fpage>–<lpage>14</lpage> (<year>1959</year>).</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name> &amp; <string-name><surname>Ghahramani</surname>, <given-names>Z.</given-names></string-name> <article-title>Computational principles of motor neuroscience</article-title>. <source>Nature Neuroscience</source> <volume>3</volume>, <fpage>1212</fpage>–<lpage>1217</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><string-name><surname>Xie</surname>, <given-names>X.</given-names></string-name> &amp; <string-name><surname>Seung</surname>, <given-names>H. S.</given-names></string-name> <article-title>Equivalence of backpropagation and contrastive Hebbian learning in a layered network</article-title>. <source>Neural computation</source> <volume>15</volume>, <fpage>441</fpage>–<lpage>454</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><string-name><surname>Zenke</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Ganguli</surname>, <given-names>S.</given-names></string-name> <article-title>SuperSpike: Supervised Learning in Multilayer Spiking Neural Networks</article-title>. <source>Neural Computation</source> <volume>30</volume>, <fpage>1514</fpage>–<lpage>1541</lpage> (<year>2018</year>).</mixed-citation></ref>
</ref-list>
<sec id="s6">
<title>Supplementary information</title>
<sec id="s6a">
<title>A Extracting the presynaptic voltage error</title>
<sec id="s6a1">
<title>Threshold-linear transfer functions</title>
<p>There is an important special case where the presynaptic voltage error can directly be extracted from presynaptic firing rates, without need to invert the transfer function via synaptic depression as shown below. This is the case when voltage errors in the upper layers are small, and the voltage-to-rate transfer function has derivatives <italic>ρ</italic>′ = 0 or 1, so that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline281b.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The condition is satisfied, for instance, for a doubly threshold-linear function (a ‘doubly rectified linear unit’, ReLu) defined by <italic>ρ</italic>(<italic>ŭ</italic>) = 0 for <italic>ŭ &lt;</italic> 0 and <italic>ρ</italic>(<italic>ŭ</italic>) = <italic>ŭ</italic> for 0 ≤ <italic>ŭ</italic> ≤<italic>r</italic><sub>max</sub>, while <italic>ρ</italic>(<italic>ŭ</italic>) = <italic>r</italic><sub>max</sub> for larger voltages. In this case we calculate
<disp-formula id="eqn41a">
<alternatives><graphic xlink:href="534198v2_eqn41a.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn41b">
<alternatives><graphic xlink:href="534198v2_eqn41b.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The approximation uses the Taylor expansion in <bold><italic>u</italic></bold><sub><italic>l</italic>+1</sub> and assumes that <bold><italic>ē</italic></bold><sub><italic>l</italic>+1</sub> is small. The crucial point of <xref ref-type="disp-formula" rid="eqn41a">Eq. 41</xref> is that the mismatch error defined on the voltage, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline282.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, can be factorized into a product of the postsynaptic rate derivative, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline283.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and the apical error, and hence it can be expressed as an error defined on the rate. Restricted to the segment <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline284.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> where the transfer function is linear and errors do not vanish, the same microcircuit delivers the feedback <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline285.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> to the apical tree through the top-down projections, and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline286.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> through the lateral connections from the interneurons. While the plasticity rules for <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline287.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline288.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> stay the same, the top-down nudging of the interneurons, see <xref ref-type="disp-formula" rid="eqn9">Eq. 9</xref>, can then be formulated based on the rate instead of the upper layer voltage, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline289.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with transfer function of the interneuron again the (doubly) threshold-linear <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline290.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Since voltages and rates are identical in the segment <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline291.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for each component, <xref ref-type="disp-formula" rid="eqn36">Eqs 36</xref> with 39 can still be inferred.</p>
</sec>
<sec id="s6a2">
<title>Rate-to-voltage inversion by short-term synaptic depression</title>
<p>We wish to readout the voltage error also for other nonlinear transfer functions than clipped ReLu’s. To do so, we take inspiration from the classical short-term synaptic depression model (<xref ref-type="bibr" rid="sc14">Tsodyks &amp; Markram, 1997</xref>; <xref ref-type="bibr" rid="c1">Abbott <italic>et al</italic>., 1997</xref>; <xref rid="c1" ref-type="bibr">Varela <italic>et al</italic>., 1997</xref>), see also <xref rid="fig6" ref-type="fig">Fig. 6a1</xref>. We consider a dynamic vesicle release probability that is proportional to the pool size of available vesicles, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline292.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and this pool size is postulated to depend on past presynaptic rates,
<disp-formula id="eqn42">
<alternatives><graphic xlink:href="534198v2_eqn42.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline293.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the low-pass filtered presynaptic rate, <italic>a</italic> and <italic>d</italic> are constants. The proportionality factor is <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline294.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, making a probability out of the vesicle pools size. The effective synaptic strength <italic>B</italic> of a ‘backprojecting top-down’ connection is the product of the absolute synaptic strength <italic>B</italic><sub>∘</sub> and the vesicle pool size <italic>v</italic>, i.e. <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline295.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The contribution to the postsynaptic current of the synapse is <italic>Wr</italic>, and the contribution to the postsynaptic voltage is <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline296.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>We search for an activation function <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline297.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> such that the postsynaptic voltage contribution is the scaled presynaptic potential, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline298.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Plugging in the above expression for <italic>B</italic> yields <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline299.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and dividing <italic>B</italic> out, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline300.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, see <xref rid="fig6" ref-type="fig">Fig. 6a2</xref>. With the expression for <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline301.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="eqn42">Eq. 42</xref> we obtain a quadratic equation in <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline302.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> that is solved by the non-negative and monotonically increasing function
<disp-formula id="eqn43">
<alternatives><graphic xlink:href="534198v2_eqn43.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
with ‘smooth’ threshold <italic>θ</italic> = (1 + <italic>a</italic>)<italic>/d</italic> and asymptote <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline303.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This gives us a transfer function <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline304.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> that qualitatively matches those observed for pyramidal neurons recorded in the steady state (<xref ref-type="bibr" rid="sc2">Anderson <italic>et al</italic>., 2000</xref>; <xref ref-type="bibr" rid="sc10">Rauch <italic>et al</italic>., 2003</xref>), see <xref rid="fig6" ref-type="fig">Fig. 6a3</xref>.</p>
<p>The approach generalizes to other pairs of strictly monotonic neuronal activation and depression functions <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline305.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, as long as <italic>u</italic> is not driven below some minimal value, here <italic>u</italic><sub>rest</sub> = 0, corresponding to <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline306.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The last requirement can, for instance, be accomplished by offsetting the activation function into a regime that guarantees that <italic>u</italic> stays positive.</p>
<p>In our simulations for <xref rid="fig5" ref-type="fig">Fig. 5</xref>, we did not explicitly implement a dynamic vesicle pool, i.e. the right-hand side of <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline307.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, but instead directly used the recovered membrane potentials <italic>u</italic>.</p>
</sec>
</sec>
<sec id="s6b">
<title>B Looking back and forward in time with derivatives</title>
<p>Since dealing with extrapolations into the future is a crucial notion of the paper we present here some of the calculations. The discounted future voltage was introduced in <xref ref-type="disp-formula" rid="eqn4">Eq. 4</xref> as
<disp-formula id="ueqn1">
<alternatives><graphic xlink:href="534198v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
To show that <italic>ũ</italic> satisfies <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline308.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, we need to apply the Leibniz integral rule in calculating the derivative <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline309.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This leads to
<disp-formula id="ueqn2">
<alternatives><graphic xlink:href="534198v2_ueqn2.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Multiplying this equation by <italic>τ</italic> and using the definition of <italic>ũ</italic> yields <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline310.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, or <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline311.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>By applying the Leibniz integral rule one also shows that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline312.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, defined in <xref ref-type="disp-formula" rid="eqn15">Eq. 15</xref>,
<disp-formula id="ueqn3">
<alternatives><graphic xlink:href="534198v2_ueqn3.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
solves <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline313.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This differential equation can be written as <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline314.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with lookahead operator ℒ defined in <xref ref-type="disp-formula" rid="eqn14">Eq. 14</xref>. To show that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline315.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, one applies partial integration to <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline316.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Note that the equality <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline317.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> only holds if we integrate from −∞, and hence if the initialization of the trajectory is far back in the past as compared to the time constant <italic>τ</italic>.</p>
<sec id="s6b1">
<title>Uniqueness of the rate function</title>
<p>In the main text we concluded from the postulate <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline318.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and the general relation <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline319.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline320.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This conclusion is a consequence of the uniqueness of a solution of an ordinary differential equation for a given initial condition (that may include delta-functions on the right-hand side, see e.g. <xref ref-type="bibr" rid="sc9">Nedeljkov &amp; Oberguggenberger, 2012</xref>. In fact, we may consider both variables <italic>ρ</italic> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline321.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> as solutions of the differential equation <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline322.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Because the solution is unique, we conclude that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline323.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
</sec>
<sec id="s6b2">
<title>Learning the input time constants</title>
<p>In our applications we assumed that the input rates in the original mapping <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline324.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are low-pass filtered by a common time constant <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline325.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> that is also shared as membrane time constant of the neurons. The general setting of learning to map time series is that input time series, <bold><italic>r</italic></bold><sub>in</sub>(<italic>t</italic>) are low-pass filtered with given time constants <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline326.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and the target output time series <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline327.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are a function of these low-pass filtered inputs, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline328.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>To learn to reproduce the input time constants <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline329.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> in the student network by <bold><italic>τ</italic></bold><sub>in</sub>, we assume that the inputs converge to neurons <bold><italic>u</italic></bold><sub>1</sub> is that only receive external inputs. Because <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline330.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the gradient rule for the input time constant is
<disp-formula id="eqn44">
<alternatives><graphic xlink:href="534198v2_eqn44.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
This local learning rule also globally reduces the Lagrangian, and in the limits of <italic>β</italic> → ∞ it is gradient descent on the mismatch energy, while in the limit <italic>β</italic> → 0 it is gradient descent on the cost. The proof works as in <xref ref-type="statement" rid="the1">Theorem 1</xref>. To learn a more complex mapping of time series that includes more complex temporal processing beyond a function of merely <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline331.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, additional variables need to be introduced that form memories (see ‘Generalizations:…’ below).</p>
</sec>
</sec>
<sec id="s6c">
<title>C From implicit to explicit differential equations</title>
<p>The original Euler-Lagrange equation is <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline332.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, <xref ref-type="disp-formula" rid="eqn20">Eq. 20</xref>, with the Jacobian <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline333.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. We have shown that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline334.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is equivalent to the voltage dynamics given in <xref ref-type="disp-formula" rid="eqn22">Eq. 22</xref>.</p>
<sec id="s6c1">
<title>Implicit differential equation</title>
<p>The implicit differential equation can be written as
<disp-formula id="eqn45">
<alternatives><graphic xlink:href="534198v2_eqn45.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The partial derivative of <bold><italic>F</italic></bold>(<bold><italic>u</italic></bold>, <italic>t</italic>) with respect to time, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline335.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, captures the external drive from the inputs and output targets that do not depend on <bold><italic>u</italic></bold>, but may directly depend on time. In fact, instead of the argument <italic>t</italic> of <bold><italic>f</italic></bold>, we could consider the two arguments <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline336.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline337.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and we can then write
<disp-formula id="eqn46">
<alternatives><graphic xlink:href="534198v2_eqn46.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>δ</italic><sub><italic>io</italic></sub> is the Kronecker delta and equal to 1 if <italic>i</italic> is an output neuron, and 0 else. The partial derivatives of <bold><italic>f</italic></bold> with respect to <bold><italic>u</italic></bold> represents the (symmetric) Hessian of the Lagrangian, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline338.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with <italic>δ</italic><sub><italic>ij</italic></sub> being the Kronecker-delta, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline339.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> defined in <xref ref-type="disp-formula" rid="eqn21">Eq. 21</xref> and <bold><italic>ē</italic></bold>* defined above <xref ref-type="disp-formula" rid="eqn16">Eq. 16</xref>. Remember that <bold><italic>W</italic></bold> = (<bold><italic>W</italic></bold><sub>in</sub>, <bold><italic>W</italic></bold><sub>net</sub>) and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline340.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. In vectorial notation the Hessian of the Lagrangian is
<disp-formula id="eqn47">
<alternatives><graphic xlink:href="534198v2_eqn47.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <bold>1</bold> is the identity matrix.</p>
<p>Fixing the arguments <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline341.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> of <bold><italic>K</italic></bold> in <xref ref-type="disp-formula" rid="eqn45">Eq. 45</xref>, we need to find a fixed point of the mapping <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline342.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. In the argument <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline343.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the mapping is affine, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline344.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with matrix <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline345.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The Banach fixed point theorem asserts that if <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline346.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is strictly contracting with <italic>k</italic>, i.e. if <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline347.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for all pairs of inputs and 0 ≤ <italic>k &lt;</italic> 1, then the iteration (here with iteration time step <italic>dt</italic>) e locally converges to a fixed point <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline348.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Because <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline349.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the mapping is <italic>k</italic>-contractive if the eigenvalues of <bold><italic>L</italic></bold> have absolute value smaller than <italic>k</italic>. This is the case if the Hessian <bold><italic>H</italic></bold>(<bold><italic>u</italic></bold>) = <bold>1</bold> − <bold><italic>L</italic></bold>(<bold><italic>u</italic></bold>) = <bold>1</bold> − <bold><italic>W</italic></bold> <italic>ρ</italic>′(<bold><italic>u</italic></bold>) − <bold><italic>ē</italic></bold>′(<bold><italic>u</italic></bold>), with <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline350.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, is strictly positive definite. Crucially, because the mapping is affine in <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline351.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>s, the convergence is global.</p>
<p>For strict positive definiteness of the Hession, the Banach fixed point theorem asserts that during (global) convergence the distance to the fixed point is bounded by a constant times <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline352.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with iteration index <italic>i</italic> and ‘virtual’ Euler step <italic>dt</italic>. In an analogue physical device that implements exactly this feedback circuit in continuous time, the <italic>dt</italic> becomes truly infinitesimal and in this sense the convergence is instantaneous. If <italic>dt</italic> remains finite, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline353.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> converges to a moving target because the mapping <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline354.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>) changes with time. The target should not change quicker than the time scale <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline354a.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> of the <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline355.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> convergence. Given a time course of the input rate <bold><italic>r</italic></bold><sub>in</sub>(<italic>t</italic>) and target <bold><italic>u</italic></bold><sub><italic>o</italic></sub>*(<italic>t</italic>) that has bounded variation, the <italic>dt</italic> can be chosen so that convergence becomes arbitrary quick, and in the limit instantaneous.</p>
<p>If <bold><italic>r</italic></bold><sub>in</sub>(<italic>t</italic>) contains well-separated delta-functions, while otherwise still having bounded variations, the reasoning still applies since at any time point in time, except at the time point of the singularity, <bold><italic>f</italic></bold>(<bold><italic>u</italic></bold>, <italic>t</italic>) = 0. This is shown in Appendix D below.</p>
<p>There is a caveat for the strictly positive definiteness of the Hessian, when the learning rate becomes too big and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline356.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> starts to change the neuronal dynamics. In this case, the Hessian becomes <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline357.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and the Eigenvalues may become negative due to the <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline358.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> term. Simulations can in fact become unstable with a big learning rate, and this is more pronounced if also the Euler <italic>dt</italic> is large. The explicit differential equation avoids the fast iteration towards a moving target and hence allows for larger <italic>dt</italic>. This in particular pays out in the presence of a high learning rate (although the Cholesky decomposition also requires positive definiteness). By this reason, the large-scale simulations involving plasticity are performed with the explicit form described next.</p>
</sec>
<sec id="s6c2">
<title>Explicit differential equation</title>
<p>To isolate <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline359.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> in the implicit differential equation, we rewrite ℒ <bold><italic>f</italic></bold> = 0 again as
<disp-formula id="eqn48">
<alternatives><graphic xlink:href="534198v2_eqn48.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Plugging the Hessian <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline360.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> from <xref ref-type="disp-formula" rid="eqn47">Eq. 47</xref> into <xref ref-type="disp-formula" rid="eqn48">Eq. 48</xref>, we obtain the voltage dynamics from <xref ref-type="disp-formula" rid="eqn22">Eq. 22</xref> in the equivalent form
<disp-formula id="eqn49">
<alternatives><graphic xlink:href="534198v2_eqn49.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
In our applications, the Hessian <bold><italic>H</italic></bold> appears to be invertible (although this may not be the case for arbitrary networks), and <xref ref-type="disp-formula" rid="eqn49">Eq. 49</xref> can be solved for the unique <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline361.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> using the Cholesky decompositions. In fact, if <bold><italic>H</italic></bold> is invertible, the system implicit ordinary differential equations from <xref ref-type="disp-formula" rid="eqn49">Eq. 49</xref> can be converted into a system of explicit ordinary differential equations (with <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline362.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <bold><italic>H</italic></bold> given in <xref ref-type="disp-formula" rid="eqn47">Eq. 47</xref>),
<disp-formula id="eqn50">
<alternatives><graphic xlink:href="534198v2_eqn50.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
and as such it has a unique solution for any given initial condition. Because <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline363.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, see <xref ref-type="disp-formula" rid="eqn46">Eq. 46</xref>, the regularity requirement for <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline364.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> to be integrable is satisfied even if <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline365.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline366.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> contain step functions (and <bold><italic>r</italic></bold><sub>in</sub>(<italic>t</italic>) delta-functions), see Sect. D for details.</p>
<p>Even in the presence of such step-functions, the Euler-Lagrange equations ℒ <bold><italic>f</italic></bold> = 0 lead to an <bold><italic>f</italic></bold> that is a decaying exponential, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline367.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. For initialization at <italic>t</italic> = −∞ we have <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline368.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> at any time. In fact, <xref ref-type="disp-formula" rid="eqn50">Eq. 50</xref> is equivalent to ℒ <bold><italic>f</italic></bold> = 0, and hence any solution of <xref ref-type="disp-formula" rid="eqn50">Eq. 50</xref>, even if <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline369.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> contains a delta-function, is also a solution of ℒ <bold><italic>f</italic></bold> = 0. Possible jumps in <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline370.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> or <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline371.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are compensated by the jumps they induce in <bold><italic>u</italic></bold> (see below for the full mathematical description with a simple example). To give an intuition, we assume that a recurrent network of our prospevtive neurons has separate fixed points for the two constant input currents <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline372.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline373.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This network still shows an overall relaxation time of <bold><italic>τ</italic></bold><sub>in</sub> (but not longer!) when the input rates instantaneously switch from <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline374.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> to <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline375.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Nevertheless, at any moment during this relaxation process, gradient learning of the mapping <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline376.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> towards <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline377.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is still guaranteed (<xref ref-type="statement" rid="the1">Theorem 1</xref>).</p>
<p>In the case of a functional feedforward network, the network weight matrix <bold><italic>W</italic></bold><sub>net</sub> is lower triangular. This is itself a lower triangular matrix, but now with unit diagonal, and as such it is invertible. For feedforward netowrks, <bold><italic>H</italic></bold> remains invertible also for small <italic>β &gt;</italic> 0, since then the emerging upper diagonal entries remain small compared to the diagonal entries 1. It also remains invertible for growing nudging strengths, <italic>β</italic> → ∞, provided that the weighted target errors <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline378.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>s remain small, see (<italic>i</italic>) in the proof of <xref ref-type="statement" rid="the1">Theorem 1</xref>.</p>
</sec>
<sec id="s6c3">
<title>Link to time-varying optimal control</title>
<p>The explicit differential equation, <xref ref-type="disp-formula" rid="eqn50">Eq. 50</xref>, is a special case of the one in <xref rid="c55" ref-type="bibr">Simonetto, Dall’Anese, <italic>et al</italic>., 2020</xref>, (<xref ref-type="disp-formula" rid="eqn20">Eq. 20</xref>), where the function to be minimized (their <italic>f</italic>) can take a general (Lipschitz continuous) form (hence their <italic>f</italic> is our Lagrangian, <italic>f</italic> = <italic>L</italic>). To avoid inverting the Hessian, an iteration algorithm can be applied similar to our implicit form form, although more involved to deal with the more general form of <italic>L</italic> (<xref ref-type="bibr" rid="sc12">Simonetto &amp; Dall’Anese, 2017</xref>). The idea of tracking the solution of a time-varying optimization problem with a linear look-ahead in time has been introduced introduced in <xref ref-type="bibr" rid="sc17">Zhao &amp; Swamy, 1998</xref>.</p>
</sec>
</sec>
<sec id="s6d">
<title>D Contraction analysis and delta-function inputs</title>
<sec id="s6d1">
<title>Stability</title>
<p>We show that the voltage dynamics obtained from <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline379.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is always stable, provided that the Hessian <bold><italic>H</italic></bold> is invertible. For this we rewrite <xref ref-type="disp-formula" rid="eqn49">Eq. 49</xref> in the form <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline380.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where the explicit time dependence of <italic>E</italic> and <bold><italic>f</italic></bold> is a short-cut to express the dependence on <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline381.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Stated in this generality, the stability analysis also applies to the voltage dynamics derived in the Latent Equilibrium (<xref rid="c22" ref-type="bibr">Haider <italic>et al</italic>., 2021</xref>).</p>
<p>According to the implicit function theorem, at any point in time when <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline382.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is invertible, we can locally write <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline383.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>s. When absorbing the dependence of <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline384.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> on <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline385.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> into a time dependence, we can rewrite this differential equation as <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline386.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, see <xref ref-type="disp-formula" rid="eqn50">Eq. 50</xref>. This differential equation is contractive and thus stable if the Jacobian of <bold><italic>g</italic></bold> with respect to <bold><italic>u</italic></bold> is uniformly negative definite (<xref ref-type="bibr" rid="sc7">Lohmiller &amp; Slotine, 1998</xref>). The contraction analysis tells that locally, where <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline387.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is invertible, we can express <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline388.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> as a function <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline389.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> that has derivative <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline390.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. For the <bold><italic>u</italic></bold>-component we get the Jacobian
<disp-formula id="eqn51">
<alternatives><graphic xlink:href="534198v2_eqn51.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
According to <xref ref-type="disp-formula" rid="eqn49">Eq. 49</xref> we have <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline391.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and calculate <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline392.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline393.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> specified above. Since according to <xref ref-type="disp-formula" rid="eqn46">Eq. 46</xref> the partial derivative with respect to <italic>t</italic> does not depend on <bold><italic>u</italic></bold>, we also calculate <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline394.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Hence, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline395.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The term <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline396.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> may cause a violation of the positive definiteness. However, this appears only transiently after initialization since the gradient <bold><italic>f</italic></bold> exponentially quickly converges to 0 after initialization. If <italic>t</italic><sub>0</sub> = −∞, the term <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline397.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> vanishes, and with it also <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline398.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This asserts local contraction property of the fixed point trajectory as then <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline399.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Hence, around a point <bold><italic>u</italic></bold><sub><italic>∘</italic></sub> on the trajectory, the linear approximation of the dynamics is <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline400.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, showing an exponential local contraction to <bold><italic>u</italic></bold><sub><italic>∘</italic></sub>.</p>
</sec>
<sec id="s6d2">
<title>Global convergence</title>
<p>The above stability analysis yields only the strict contraction property after the convergence of the gradient to <bold><italic>f</italic></bold> = 0. We have shown that the iteration of <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline401.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> globally converges to a fixed point <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline402.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, see <xref ref-type="disp-formula" rid="eqn45">Eq. 45</xref>. Let us therefore assume that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline403.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> satisfies this fixed point equation. This defines a trajectory <bold><italic>u</italic></bold>(<italic>t</italic>) that globally converges to the fixed points <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline404.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. In fact, these fixed points are characterized by the vanishing gradients, <bold><italic>f</italic></bold> = 0, and these vanishing gradients are globally reached. This is because <bold><italic>F</italic></bold>(<bold><italic>u</italic></bold>(<italic>t</italic>), <italic>t</italic>), as a function of time, exponentially decays to 0 in each component, as noticed above. To restate this in a compound version, we consider an arbitrary initialization <bold><italic>u</italic></bold>(<italic>t</italic><sub>0</sub>) for which in general <bold><italic>f</italic></bold>(<bold><italic>u</italic></bold>(<italic>t</italic><sub>0</sub>), <italic>t</italic><sub>0</sub>) ≠ 0. Because at any time we have <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline405.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the length of <bold><italic>F</italic></bold>(<bold><italic>u</italic></bold>(<italic>t</italic>), <italic>t</italic>) exponentially converges to 0, as expressed by its temporal derivative,
<disp-formula id="eqn52">
<alternatives><graphic xlink:href="534198v2_eqn52.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Hence, starting at any initial point, the voltage dynamics finds a self-consistency solution of <bold><italic>f</italic></bold>(<bold><italic>u</italic></bold>(<italic>t</italic>), <italic>t</italic>) = 0, or equivalently of <bold><italic>u</italic></bold> = <bold><italic>W</italic></bold> <italic>ρ</italic>(<bold><italic>u</italic></bold>) + <bold><italic>ē</italic></bold>(<bold><italic>u</italic></bold>), with <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline406.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
</sec>
<sec id="s6d3">
<title>Delta-function inputs keep <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline407.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula></title>
<p>We next explain in more details why delta-function in the input rates <bold><italic>r</italic></bold><sub>in</sub>(<italic>t</italic>) for the NLA the stationarity (‘equilibrium’) condition <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline408.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is always satisfied (the delta-function in <bold><italic>r</italic></bold><sub>in</sub> for the NLA corresponding to step-function in <bold><italic>r</italic></bold><sub>in</sub> for the Latent Equilibrium). We reconsider the explicit differential equation <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline409.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> given in <xref ref-type="disp-formula" rid="eqn50">Eq. 50</xref>, with <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline410.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <bold><italic>H</italic></bold> given in <xref ref-type="disp-formula" rid="eqn47">Eq. 47</xref>.</p>
<p>To simplify matters, we consider a single delta-function at <italic>t</italic> = 0 as input in the absence of output nudging. In this case we get <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline411.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where the input matrix <bold><italic>W</italic></bold><sub>in</sub> is typically sparse (not all network neurons receive external input), and <bold><italic>δ</italic></bold><sub>in</sub>(<italic>t</italic>) is a vector of delta-functions restricted to the input neurons. Following <xref ref-type="bibr" rid="sc9">Nedeljkov &amp; Oberguggenberger, 2012</xref>, Proposition 2.1, we can then write the explicit differential equation, <xref ref-type="disp-formula" rid="eqn49">Eq. 49</xref>, in the form
<disp-formula id="eqn53">
<alternatives><graphic xlink:href="534198v2_eqn53.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline411a.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is globally Lipschitz continuous. Due to the Lipschitz continuity the change in <bold><italic>u</italic></bold> evoked by <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline411b.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> during a small time interval [− <italic>ε, ε</italic>] around <italic>t</italic> = 0 vanishes when this interval shrinks, <italic>ε</italic> →0. To quantifies the change in <bold><italic>u</italic></bold> during these intervals it is therefore enough to consider <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline412.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, or equivalently <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline413.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. To estimate the jump induced by the delta-functions, we consider some mollifier <italic>ϕ</italic><sub><italic>ε</italic></sub>(<italic>t</italic>) = <italic>ε</italic><sup><italic>−</italic>1</sup><italic>ϕ</italic>(<italic>t/ε</italic>), where <italic>ϕ</italic>(<italic>t</italic>) is a smooth function on the interval [−1, 1] with integral 1. By <bold><italic>ϕ</italic></bold><sub>in,<italic>ε</italic></sub>(<italic>t</italic>) we denote the vector of mollifiers centered at the delta-functions of the input neurons. We now consider the two differential equations, with the second approximating the first on the interval [−<italic>ε, ε</italic>], but without regular term <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline413a.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>,
<disp-formula id="eqn54a">
<alternatives><graphic xlink:href="534198v2_eqn54a.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn54b">
<alternatives><graphic xlink:href="534198v2_eqn54b.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
We assume that for all <italic>t</italic>∈ [−<italic>ε, ε</italic>] the matrices <bold><italic>H</italic></bold>(<bold><italic>u</italic></bold><sub><italic>ε</italic></sub>(<italic>t</italic>)) and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline413b.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are invertible, so that the two <xref ref-type="disp-formula" rid="eqn54a">Eqs 54a,b</xref> can be turned into an explicit differential equations. Analogously to the 1-dimensional case (<xref ref-type="bibr" rid="sc9">Nedeljkov &amp; Oberguggenberger, 2012</xref>), we conclude that the solution of <xref ref-type="disp-formula" rid="eqn54a">Eqs 54a</xref>,<xref ref-type="disp-formula" rid="eqn54b">b</xref> on the interval [−<italic>ε, ε</italic>] converge to each other, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline414.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for <italic>ε</italic>→ 0. As a consequence, the jump of <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline415.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> at <italic>t</italic> = 0 converges to the corresponding jump of <bold><italic>u</italic></bold><sub><italic>ε</italic></sub> in the various dimensions.</p>
<p>To calculate the jump of <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline416.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> we have to integrate <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline417.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> across the time interval [−<italic>ε, ε</italic>]. Instead of integrating <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline418.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, we first integrate <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline419.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> given in <xref ref-type="disp-formula" rid="eqn54b">Eq. 54b</xref>. Moving from right to left yields
<disp-formula id="eqn55">
<alternatives><graphic xlink:href="534198v2_eqn55.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <bold><italic>I</italic></bold><sub>in</sub> is the index vector of the input neurons having a delta-function, i.e. <italic>I</italic><sub>in,<italic>j</italic></sub> = 1 if there is a delta-input, and else 0. Note that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline420.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Because <bold><italic>H</italic></bold> is itself a derivative, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline421.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, we can explicitly calculate the latter integral (also for <italic>β &gt;</italic> 0, but for clarity here only done for <italic>β</italic> = 0). The last integral in <xref ref-type="disp-formula" rid="eqn55">Eq. 55</xref> is defined as a vector with <italic>i</italic>-th component being
<disp-formula id="eqn56">
<alternatives><graphic xlink:href="534198v2_eqn56.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where in the second last equality we used that <italic>H</italic><sub><italic>ij</italic></sub>(<bold><italic>u</italic></bold>) = <italic>δ</italic><sub><italic>ij</italic></sub> −<italic>W</italic><sub><italic>ij</italic></sub><italic>ρ</italic>′(<italic>u</italic><sub><italic>j</italic></sub>)) does only depend on the component <italic>u</italic><sub><italic>j</italic></sub>, see <xref ref-type="disp-formula" rid="eqn47">Eq. 47</xref>. In the last equality we introduced the ‘network voltage error’ <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline422.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Following the 1-dimensional case treated in <xref ref-type="bibr" rid="sc9">Nedeljkov &amp; Oberguggenberger, 2012</xref>, Proposition 1.2 we introduce the ‘jump function’ (called <italic>G</italic>(<italic>y</italic>) in the cited work)
<disp-formula id="eqn57">
<alternatives><graphic xlink:href="534198v2_eqn57.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
with <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline423.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> thought to represent <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline424.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> at some time <italic>t</italic><sub><italic>∘</italic></sub> before the delta-kick sets in. With this setting, <xref ref-type="disp-formula" rid="eqn55">Eq. 55</xref> turns into
<disp-formula id="eqn58">
<alternatives><graphic xlink:href="534198v2_eqn58.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
In the limit <italic>ε</italic> → 0 we get a relation between <bold><italic>u</italic></bold>(<italic>t</italic>) immediately before and after the jump, <bold><italic>u</italic></bold>(−0) and <bold><italic>u</italic></bold>(+0), using that in this limit the boundary points of the trajectories also converge, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline425.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>,
<disp-formula id="eqn59">
<alternatives><graphic xlink:href="534198v2_eqn59.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
We now assume that the function <bold><italic>J</italic></bold>(<bold><italic>u</italic></bold>) = <italic>τ</italic> (<bold><italic>v</italic></bold> − <bold><italic>v</italic></bold><sub><italic>∘</italic></sub>) is invertible around the jump. This is the case if the Jacobian <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline426.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is invertible, and because <bold><italic>v</italic></bold> = <bold><italic>u</italic></bold> − <bold><italic>W</italic></bold><sub>net</sub><italic>ρ</italic>(<bold><italic>u</italic></bold>), we require invertability of <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline427.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with Hessian defined in <xref ref-type="disp-formula" rid="eqn47">Eq. 47</xref>.</p>
<p>In the case of invertability we get the voltage after the jump as
<disp-formula id="eqn60">
<alternatives><graphic xlink:href="534198v2_eqn60.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
We next calculate the jump in <bold><italic>v</italic></bold>. This is easy since <bold><italic>v</italic></bold> = <bold><italic>u</italic></bold> − <bold><italic>W</italic></bold><sub>net</sub><italic>ρ</italic>(<bold><italic>u</italic></bold>) linearly enters in the function <bold><italic>J</italic></bold>(<bold><italic>u</italic></bold>) = <italic>τ</italic> (<bold><italic>v</italic></bold> − <bold><italic>v</italic></bold><sub><italic>∘</italic></sub>). Plugging the explicit expression for <bold><italic>J</italic></bold> into <xref ref-type="disp-formula" rid="eqn59">Eq. 59</xref> we get <italic>τ</italic> (<bold><italic>v</italic></bold>(+0) − <bold><italic>v</italic></bold><sub><italic>∘</italic></sub>) = <italic>τ</italic> (<bold><italic>v</italic></bold>(−0) − <bold><italic>v</italic></bold><sub><italic>∘</italic></sub>) + <bold><italic>W</italic></bold><sub>in</sub><bold><italic>I</italic></bold><sub>in</sub>, or
<disp-formula id="eqn61">
<alternatives><graphic xlink:href="534198v2_eqn61.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Knowing the jump in <bold><italic>v</italic></bold> helps to show that the equilibrium condition <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline428.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is always satisfied, even immediately after the delta-in put, provided the initialization is at <italic>t</italic><sub>0</sub> = −∞. To show this, remember that in the absence of nudging we have <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline429.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The jump size of <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline430.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for a delta-function at <italic>t</italic> = 0, <bold><italic>r</italic></bold><sub>in</sub>(<italic>t</italic>) = <bold><italic>δ</italic></bold><sub>in</sub>(<italic>t</italic>) is <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline431.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This is because <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline432.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> satisfies the differential equations <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline433.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, provided that <italic>t</italic><sub>0</sub> = −∞. Hence,
<disp-formula id="eqn62">
<alternatives><graphic xlink:href="534198v2_eqn62.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
With <xref ref-type="disp-formula" rid="eqn61">Eqs 61</xref> and <xref ref-type="disp-formula" rid="eqn62">62</xref> we conclude that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline434.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> throughout.</p>
</sec>
</sec>
<sec id="s6e">
<title>E Example of a single recurrently connected neuron</title>
<p>To get an intuition for the instantaneity in a the recurrent case we consider the example of a single, recurrently connected neuron. We also put this into the context of the Latent Equilibrium (<xref rid="c22" ref-type="bibr">Haider <italic>et al</italic>., 2021</xref>). Consider the weight vector <bold><italic>W</italic></bold> = (<italic>W</italic><sub>in</sub>, <italic>W</italic><sub>net</sub>) with an input an input rate <italic>r</italic><sub>in</sub>(<italic>t</italic>) driving the postsynaptic voltage <italic>u</italic>. The postsynaptic rate is <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline435.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and its low-pass filter with respect to <italic>τ</italic> is <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline436.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. As always, the low-pass filtering reaches back to an initialization at <italic>t</italic><sub>0</sub> = −∞, see <xref ref-type="disp-formula" rid="eqn15">Eq. 15</xref>. The Lagrangian has the form
<disp-formula id="eqn63">
<alternatives><graphic xlink:href="534198v2_eqn63.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The Euler-Lagrange equations <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline437.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are derived from <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline438.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Applying the look-ahead operator ℒ (<xref ref-type="disp-formula" rid="eqn14">Eq. 14</xref>), and abbreviating <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline439.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the Euler-Lagrange equations deliver the voltage dynamics,
<disp-formula id="eqn64">
<alternatives><graphic xlink:href="534198v2_eqn64.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
To simplify matters, we consider the nudging-free case, <italic>β</italic> = 0. This implies that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline440.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. With <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline441.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, we obtain the differential equation
<disp-formula id="eqn65">
<alternatives><graphic xlink:href="534198v2_eqn65.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Abbreviating <italic>v</italic> = <italic>u</italic> − <italic>W</italic><sub>net</sub><italic>ρ</italic>(<italic>u</italic>) as ‘network voltage error’, the above differential equation reads as
<disp-formula id="eqn66">
<alternatives><graphic xlink:href="534198v2_eqn66.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Integrating the effective voltage dynamics (<xref ref-type="disp-formula" rid="eqn66">Eq. 66</xref>), assuming initialization infinitely far in the past, is equal to <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline442.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This equation is equivalent to the Euler-Lagrange equation <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline443.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> being integrate, and because the solution of the Euler Lagrange equation is <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline444.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, we have (using <italic>t</italic> = −∞)
<disp-formula id="eqn67">
<alternatives><graphic xlink:href="534198v2_eqn67.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
</p>
<sec id="s6e1">
<title>Voltage dynamics for a delta-function input</title>
<p>We next apply a delta-function in the input rate, say <italic>r</italic><sub>in</sub>(<italic>t</italic>) = <italic>δ</italic>(<italic>t</italic>) and consider the dynamics at the level of the voltage, <xref ref-type="disp-formula" rid="eqn65">Eq. 65</xref>. As in <xref ref-type="bibr" rid="sc9">Nedeljkov &amp; Oberguggenberger, 2012</xref>, Proposition 1.2 we introduce the ‘jump function’ (called <italic>G</italic>(<italic>y</italic>) in the cited work)
<disp-formula id="eqn68">
<alternatives><graphic xlink:href="534198v2_eqn68.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
As in <xref ref-type="bibr" rid="sc9">Nedeljkov &amp; Oberguggenberger, 2012</xref>, Proposition 1.2, we show that the voltage <italic>u</italic> makes a unique jump at the moment of the delta function that <italic>J</italic>(<italic>u</italic>) is invertible around the jump.</p>
<p>We set <italic>v</italic><sub><italic>∘</italic></sub> = <italic>u</italic><sub><italic>∘</italic></sub>− <italic>W</italic><sub>net</sub><italic>ρ</italic>(<italic>u</italic><sub><italic>∘</italic></sub>). Here, <italic>u</italic><sub><italic>∘</italic></sub> is some voltage before the jump, say <italic>u</italic><sub><italic>∘</italic></sub> = <italic>u</italic>(−1) evaluated at time <italic>t</italic> = −1, when the jump is at <italic>t</italic> = 0. When <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline445.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the voltage immediately before the jump, the voltage immediately after the jump is <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline446.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> specified by
<disp-formula id="eqn69">
<alternatives><graphic xlink:href="534198v2_eqn69.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The reason is that the <italic>W</italic><sub>in</sub>-scaled delta-function triggers a step of size <italic>W</italic><sub>in</sub> when integrating over it as done in <xref ref-type="disp-formula" rid="eqn68">Eq. 68</xref>. The new value <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline447.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is unique if <italic>J</italic> is invertible, and looking at the defining integral in <xref ref-type="disp-formula" rid="eqn68">Eq. 68</xref>, this is the case if <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline448.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>The jump in <italic>u</italic> translates into a jump in <italic>v</italic> = <italic>u</italic> − <italic>W</italic><sub>net</sub><italic>ρ</italic>(<italic>u</italic>) from <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline449.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> to <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline450.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This endpoint can also be expressed as
<disp-formula id="eqn70">
<alternatives><graphic xlink:href="534198v2_eqn70.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
To check this, we assume without loss of generality that <italic>v</italic><sub><italic>∘</italic></sub> = <italic>u</italic><sub><italic>∘</italic></sub> <italic>W</italic><sub>net</sub><italic>ρ</italic>(<italic>u</italic><sub><italic>∘</italic></sub>) = 0. Then <italic>J</italic>(<italic>u</italic>) = <italic>τ</italic> (<italic>u W</italic><sub>net</sub><italic>ρ</italic>(<italic>u</italic>)) = <italic>τv</italic> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline451.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> according to <xref ref-type="disp-formula" rid="eqn69">Eq. 69</xref>. Since also <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline452.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, we conclude that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline453.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, as claimed above.</p>
<p>We finally show that even far away from the initialization, the stationarity condition <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline454.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> holds before and immediately after the jump. In fact, for <italic>t</italic><sub>0</sub> = −∞, the evolution of the ‘network voltage error’ becomes
<disp-formula id="eqn71">
<alternatives><graphic xlink:href="534198v2_eqn71.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here we used that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline455.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and according to <xref ref-type="disp-formula" rid="eqn70">Eq. 70</xref> the <italic>v</italic> jumps to <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline456.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Remember that for initialization far in the past, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline457.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is equivalent to <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline458.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, see <xref ref-type="disp-formula" rid="eqn67">Eq. 67</xref>. We therefore have to calculate the jump in <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline458a.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> induced by the delta-input. Since <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline459.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the solution of <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline460.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for <italic>t</italic><sub>0</sub> = −∞, we find that
<disp-formula id="eqn72">
<alternatives><graphic xlink:href="534198v2_eqn72.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Combing the two <xref ref-type="disp-formula" rid="eqn71">Eqs 71</xref> and <xref ref-type="disp-formula" rid="eqn72">72</xref> proves that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline461.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> holds true any moment in time, provided the initialization is far in the past.</p>
<p>One may ask why the delta-kink is different from resetting <italic>v</italic> at a new intialization off from 0. The reason is that at <italic>t</italic> = 0 there is a cause for the jump in <italic>r</italic>(0), while at <italic>t</italic><sub>0</sub> there is no cause in <italic>r</italic>(<italic>t</italic><sub>0</sub>). In fact, there is no jump initially, just the start of <italic>v</italic> at some initial condition. Differently from the initialization at <italic>t</italic><sub>0</sub>, where <italic>v</italic>(<italic>t</italic><sub>0</sub>) <italic>&gt;</italic> 0 implies <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline462.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for finite <italic>t</italic> − <italic>t</italic><sub>0</sub> <italic>&gt;</italic> 0, the jump of <italic>v</italic>(0) at <italic>t</italic> = 0 to a positive value does leave <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline463.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for all <italic>t &gt;</italic> 0, provided <italic>t</italic><sub>0</sub> = −∞.</p>
</sec>
<sec id="s6e2">
<title>Linear transfer function</title>
<p>We first consider the case of a linear transfer-function <italic>ρ</italic>(<italic>u</italic>) = 0 (or threshold linear, being in the linear regime). Then the differential equation becomes
<disp-formula id="eqn73">
<alternatives><graphic xlink:href="534198v2_eqn73.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
With initialization at <italic>t</italic> = −∞ and low-pass filtering <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline464.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> defined in <xref ref-type="disp-formula" rid="eqn15">Eq. 15</xref> the solution is
<disp-formula id="eqn74">
<alternatives><graphic xlink:href="534198v2_eqn74.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The point is that the time constant is <italic>τ</italic> and is not <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline465.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, as this would be the case without prospective firing rate. In fact, for the ‘classical’ differential equation,
<disp-formula id="eqn75">
<alternatives><graphic xlink:href="534198v2_eqn75.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
and <italic>ρ</italic> the identity, we obtain the differential equation <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline466.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> with <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline467.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and solution <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline468.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> that is now the low-pass filtering with respect to the effective time constant.</p>
</sec>
<sec id="s6e3">
<title>Sigmoidal transfer function</title>
<p>For a sigmoidal transfer function <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline469.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, a positive feedback weight <italic>W</italic><sub>net</sub> <italic>&gt;</italic> 0, and a constant external input <italic>r</italic><sub>in</sub>, say, the solutions <italic>u</italic>(<italic>t</italic>) of <xref ref-type="disp-formula" rid="eqn64">Eq. 64</xref> either converge to a fixed point or diverge. When converging, the voltage satisfies the fixed point condition
<disp-formula id="eqn76">
<alternatives><graphic xlink:href="534198v2_eqn76.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
This fixed point equation can be numerically solved by time-discrete iteration process. But it can also be solved by a time-continuous process that underlies a neural or neuromorphic implementation. The prospective firing rate introduced in the NLA can be seen as a method to quickly find the fixed point in continuous time. When directly solving the implicit differential equation (as opposed to convert this into an explicit differential equation using e.g. the Cholesky decomposition), the fixed point is potentially found with a fewer number of steps.</p>
<p>To estimate the speed of convergence, we look at the initial speed when taking off at initial condition <italic>u</italic>(0) between the unstable and stable fixed point. The initial speed for the classical differential equation, <xref ref-type="disp-formula" rid="eqn75">Eq. 75</xref>, and the NLA version, <xref ref-type="disp-formula" rid="eqn64">Eq. 64</xref>, are, respectively,
<disp-formula id="eqn77a">
<alternatives><graphic xlink:href="534198v2_eqn77a.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn77b">
<alternatives><graphic xlink:href="534198v2_eqn77b.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where we set Δ<italic>u</italic>(0) = −<italic>u</italic>(0) + <italic>W</italic><sub>net</sub><italic>ρ</italic>(<italic>u</italic>(0)) + <italic>W</italic><sub>in</sub><italic>r</italic><sub>in</sub>(0). As <italic>W</italic><sub>net</sub><italic>ρ</italic>′ <italic>&gt;</italic> 0, the initial convergence speed of the NLA solution is larger. The scheme has some resemblance to the Newton algorithm of finding zero’s of a function by using its derivative.</p>
</sec>
</sec>
<sec id="s6f">
<title>F Generalizations: NLA for conductance-based neurons and more dynamic variables</title>
<p>The mismatch energies and costs can be generalized in different ways. Here we focus on a biophysical version of the mismatch energy that includes conductance-based neurons. This also relates to the least-action principle in physics. But the NLA can also be generalized to include other dynamica variables such as adaptive thresholds or synaptic short-term plasticity.</p>
<sec id="s6f1">
<title>Equivalent somato-dentritic circuit</title>
<p>For conductance based synapses, the excitatory and inhibitory conductances, <italic>g</italic><sub>E</sub> and g<sub>I</sub>, are driven by the presynaptic firing rates and have the form <italic>g</italic><sub>E</sub>(<italic>t</italic>) = <italic>W</italic><sub>E</sub> <italic>r</italic>(<italic>t</italic>), and analogously <italic>g</italic><sub>I</sub>(<italic>t</italic>) = <italic>W</italic><sub>I</sub> <italic>r</italic>(<italic>t</italic>). The dynamics of a somatic voltage <italic>u</italic> and a dendritic voltage <italic>v</italic> reads as
<disp-formula id="eqn78">
<alternatives><graphic xlink:href="534198v2_eqn78.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn79">
<alternatives><graphic xlink:href="534198v2_eqn79.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>c</italic> and <italic>c</italic><sub>d</sub> are the somatic and dendritic capacitances, <italic>E</italic><sub>L<italic>/</italic>E<italic>/</italic>I</sub> the reversal potentials for the leak, the excitatory and inhibitory currents, <italic>g</italic><sub>sd</sub> the transfer conductance from the dendrite to the soma, and <italic>g</italic><sub>ds</sub> in the other direction.</p>
<p>We consider the case when the dendritic capacitance <italic>c</italic><sub>d</sub> is small as compared to the sum of conductances <italic>g</italic><sub>d</sub> on the right-hand-side of <xref ref-type="disp-formula" rid="eqn79">Eq. 79</xref>, yielding a fast dendritic time constant. In this case we can solve this equation in the steady state for <italic>v</italic>, plug this into <xref ref-type="disp-formula" rid="eqn78">Eq. 78</xref>, and get
<disp-formula id="eqn80">
<alternatives><graphic xlink:href="534198v2_eqn80.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
with effective reversal potential <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline470.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, total conductance <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline471.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, feedforward dendritic voltage <italic>v</italic><sub>ff</sub> = (<italic>g</italic><sub>L</sub><italic>E</italic><sub>L</sub> + <italic>g</italic><sub>E</sub><italic>E</italic><sub>E</sub> + <italic>g</italic><sub>I</sub><italic>E</italic><sub>I</sub>)<italic>/g</italic><sub>ff</sub> and feedforward dendritic conductance <italic>g</italic><sub>ff</sub> = <italic>g</italic><sub>L</sub> + <italic>g</italic><sub>E</sub> + <italic>g</italic><sub>I</sub>. Because <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline472.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the conductance depends on the presynaptic voltage and its derivative. <xref ref-type="disp-formula" rid="eqn80">Equation 80</xref> describes the effective circuit that has the identical voltage time course as <xref ref-type="disp-formula" rid="eqn78">Eqs 78</xref> and <xref ref-type="disp-formula" rid="eqn79">79</xref> with <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline473.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, but with a single time-dependent ‘battery voltage’ <italic>V</italic> and Ohmic resistance <italic>R</italic> = 1<italic>/g</italic>.</p>
</sec>
<sec id="s6f2">
<title>Somato-dentritic mismatch power and action</title>
<p>The synaptic inputs <italic>g</italic><sub>E</sub>(<italic>t</italic>) and <italic>g</italic><sub>I</sub>(<italic>t</italic>) are continuously driving <italic>V</italic> (<italic>t</italic>), and the best what one can hope for the dynamics of <italic>u</italic> is that it traces <italic>V</italic> with some integration delay determined by the time constant <italic>τ</italic> = <italic>c/g</italic>. In fact, if <italic>u</italic> follows the dynamics of <xref ref-type="disp-formula" rid="eqn80">Eq. 80</xref>, then <italic>u</italic> becomes the low-pass filtered target potential, <italic>u</italic> = <italic>V</italic>, where <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline473a.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is filtered with the dynamic time constant <italic>τ</italic> (<italic>t</italic>). The defining equations for the low-pass filtering is
<disp-formula id="eqn81">
<alternatives><graphic xlink:href="534198v2_eqn81.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
and this self-consistency equation is equivalent to the explicit form
<disp-formula id="eqn82">
<alternatives><graphic xlink:href="534198v2_eqn82.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
To capture the voltage dynamics with our NLA principle we recall that the somatic voltage <italic>u</italic> can be nudged by an ‘apical voltage’ <italic>ē</italic> that causes a somatic voltage error <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline474.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The voltage error drives a current <italic>I</italic> = <italic>gē</italic> through the conductance <italic>g</italic>. The electrical power of this current <italic>I</italic> driven by the voltage <italic>ē</italic> is <italic>P</italic> = <italic>Iē</italic><sup>2</sup>. This motivates the definition of the mismatch power in a network of <italic>N</italic> neurons by
<disp-formula id="eqn83">
<alternatives><graphic xlink:href="534198v2_eqn83.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
𝒫 is a virtual power that, nevertheless, is related to some physical flow of ions. Assume we could measure all the ions flowing in the original circuit of <xref ref-type="disp-formula" rid="eqn78">Eqs 78</xref> and <xref ref-type="disp-formula" rid="eqn79">79</xref> (in the limit of small ratio <italic>C</italic><sub>d</sub><italic>/g</italic><sub>d</sub>). From this flow, delete the ion flow that cancels at the level of electrical charge exchange due to the counter directed flow. The remaining effective ion flow defines an effective current flowing through the conductance <italic>g</italic> with driving force (<italic>V u</italic>), <xref ref-type="disp-formula" rid="eqn80">Eq. 80</xref>. If it were only this effective current in the reduced circuit, the voltage <italic>u</italic>(<italic>t</italic>), starting at <italic>u</italic>(0), would converge with time-constant <italic>τ</italic> to the low-pass filtering <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline475.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Without additional ‘hidden’ current, the voltage <italic>u</italic> would then instantaneously follow <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline476.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> that is itself given by the forward dendritic input conductances. The deviation of <italic>u</italic> from <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline477.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, caused by some initial conditions in <italic>u</italic> or by a feedback current from the network affecting <italic>u</italic>, builds the mismatch power 𝒫. The feedback may originate from a target imposed downstream, and the neuron is ‘free’ in how to dynamically match <italic>u</italic> and <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline478.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. It is therefore tempting to see 𝒫 as a ‘free power’, and the NLA principle as minimizing the corresponding ‘free energy’. In fact, the free-energy principle says that any self-organizing system that is in a dynamic equilibrium with its environment (here <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline479.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> in the absence of output nudging) must minimize its free energy (that here builds up by imposing a target; <xref ref-type="bibr" rid="c18">Friston, 2010</xref>).</p>
<p>The NLA principle states that the time-integral of 𝒫 is minimized with respect to the look-ahead voltage <italic>ũ</italic>. We therefore define the physical mismatch energy as
<disp-formula id="eqn84">
<alternatives><graphic xlink:href="534198v2_eqn84.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
that has the units of energy. E<sub>M</sub> takes the role of our neural action (<italic>A</italic> in the main text) for conductance-based neurons.</p>
</sec>
<sec id="s6f3">
<title>Euler-Lagrange equations for conductance-based neurons</title>
<p>The NLA for conductance-based neurons seeks to minimize <italic>A</italic> = ∫ 𝒫 (<bold><italic>u</italic></bold>) dt with respect to variations of <bold><italic>u</italic></bold>, such tha <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline480.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. In the simplest example of the main text we considered prospective rates, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline481.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, so that the low-pass filtered rates become a function of the instantaneous voltage, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline482.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. These low-pass filtered presynaptic rates, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline483.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, determine the postsynaptic voltage <italic>u</italic>. Analogously, the low-pass filtered reversal potential, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline484.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, determines the postsynaptic voltage <italic>u</italic>, and we again postulate that <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline485.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is an instantaneous function of the presynaptic voltage, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline486.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Here we argue that active dendritic mechanisms advance to postsynaptic reversal potential <italic>V</italic>, so that the delayed <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline487.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> again becomes instantaneous, similarly to the advancement of the apical dendritic potential observed in cortical pyramidal neurons (<xref ref-type="bibr" rid="c63">Ulrich, 2002</xref>), see also <xref rid="fig2" ref-type="fig">Fig. 2b</xref>. With this instantaneity, the stationarity of the action with respect to generalized (compact and non-compact) variations, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline488.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, translates to the condition <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline489.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>Calculating <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline490.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> with 𝒫 given in <xref ref-type="disp-formula" rid="eqn83">Eq. 83</xref> and <bold><italic>τ</italic></bold> = <italic>c/</italic><bold><italic>g</italic></bold> for the total conductance <bold><italic>g</italic></bold>(<bold><italic>u</italic></bold>) specified after <xref ref-type="disp-formula" rid="eqn80">Eq. 80</xref> is a bit more demanding. For a probabilistic version, where 𝒫 is derived from the negative log-likelihood of a Gaussian density of the voltage, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline491.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the calculation is done (<xref ref-type="bibr" rid="sc6">Jordan <italic>et al</italic>., 2022</xref>). In the probabilistic version, there is an additional normalization term that enters here as log <bold><italic>g</italic></bold>. In the deterministic version considered here, this log term is not present and we calculate
<disp-formula id="eqn85">
<alternatives><graphic xlink:href="534198v2_eqn85.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Notice that the transpose <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline491a.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> selects the downstream network neuron to backpropagate from there the first- and second-order errors. From <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline492.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <bold><italic>τ</italic></bold> = <italic>c/</italic><bold><italic>g</italic></bold> we conclude that
<disp-formula id="eqn86">
<alternatives><graphic xlink:href="534198v2_eqn86.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
We next apply the look-ahead operator to the expression in this <xref ref-type="disp-formula" rid="eqn86">Eq. 86</xref>. Assuming an initialization at <italic>t</italic><sub>0</sub> = −∞, the condition <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline493.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> becomes equivalent to <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline494.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and hence <xref ref-type="disp-formula" rid="eqn86">Eq. 86</xref> becomes equivalent to
<disp-formula id="eqn87">
<alternatives><graphic xlink:href="534198v2_eqn87.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
A learning rule of the form <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline495.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> with an appropriate postsynaptic error <bold><italic>ē</italic></bold><sub>post</sub> can again be derived (see <xref ref-type="bibr" rid="sc6">Jordan <italic>et al</italic>., 2022</xref> for a single neuron in the probabilistic framework). But this, with the above sketch, needs to be worked out yet.</p>
</sec>
<sec id="s6f4">
<title>Generalizations: long memories, reinforcement learning</title>
<p>One could also extend the NLA principle by adding e.g. threshold adaptation that endows the dynamics with additional and longer time constants. For this, the rate function is parametrized by an additional threshold, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline496.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, an the Lagrangian is added by an error term on the threshold. Such an error addition can take the form <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline497.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline498.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> now represents a low-pass filtering of the rate with a long threshold adaptation time constant <italic>τ</italic><sub><italic>ϑ</italic></sub>. Neurons that show an additional threshold adaptation will still be able to instantaneously transmit a voltage jump through a prospective firing rate, but this will now also depend on the neuron’s history. Short-term plasticity may be included in the same way. Due to the history dependence, the stationarity of the action <italic>δA</italic> = 0 cannot anymore be reduced to the stationarity of the Lagrangian at any moment in time. As a consequence, the errors will look-ahead into to future more than only based on the local derivatives.</p>
<p>Generalizations are further possible for the cost function that favor voltage regions with high cost, say corresponding to punishment, or negative cost, corresponding to punishment to reward. These extensions will be considered in future work.</p>
</sec>
<sec id="s6f5">
<title>Voltage dynamics from the physical least-action principle</title>
<p>We have shown that through the look-ahead in Hamilton’s least-action principle, the notion of friction enters through the backdoor. In the least action formalism in physics, friction is directly introduced by extending the Hamiltonian principle to a generalized D’Alembert principle, where at the level of the Euler-Lagrange equations the generalized force is equated to the dissipation force (<xref ref-type="bibr" rid="sc3">Flannery, 2005</xref>).</p>
<p>The electro-chemical properties of a membrane can be captured by an equivalent circuit consisting of a battery voltage <italic>V</italic>, a conductance <italic>C</italic> and a resistance <italic>R</italic>, arranged in parallel. The voltage dynamics is derived from the Euler-Lagrange equations that are added by a dissipative force. Formally, in the absence of an inductance defining the kinetic energy, the Lagrangian <bold>ℒ</bold> becomes identical to the potential energy <bold>ℒ</bold> = <bold>𝒰</bold> with
<disp-formula id="eqn88">
<alternatives><graphic xlink:href="534198v2_eqn88.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn89">
<alternatives><graphic xlink:href="534198v2_eqn89.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here, <bold>ℱ</bold> is the dissipative Rayleigh energy related to friction and <italic>Q</italic> represents the charge across the membrane. According to D’Alembert’s principle, the dynamics is characterized by the Euler-Lagrange equation with dissipative force, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline499.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and this equation reduces to <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline500.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Identifying the charge by means of voltage across the capacitance, <italic>Q</italic> = <italic>Cu</italic>, this equation can also be written as <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline501.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, or <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline502.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> with <italic>τ</italic> = <italic>RC</italic>, just as also derived in <xref ref-type="disp-formula" rid="eqn87">Eq. 87</xref>. Loosly speaking, the minimization of the energy (<bold>ℰ</bold><sub>M</sub> = ∫𝒫 dt) by looking ahead is equivalent to a minimization without looking ahead, but taking account of friction.</p>
</sec>
</sec>
<sec id="s6g">
<title>G A tutorial on total and partial derivatives as used in the paper</title>
<p>The proof of <xref ref-type="statement" rid="the1">Theorem 1</xref> given in the Methods makes use of partial and total derivatives and follows the notation of <xref ref-type="bibr" rid="c53">Scellier &amp; Bengio, 2017</xref> and <xref ref-type="bibr" rid="c43">Meulemans <italic>et al</italic>., 2022</xref>. As there is some variability (and community-dependent sloppiness) in the notation of partial and total derivatives, we provide some explanations on how this notation is interpreted, and why, for instance, total derivatives commute with each other, and also partial with each other (although not total with partial).</p>
<list list-type="roman-lower">
<list-item><p>In a differential geometric setting, the derivative of real-valued function <italic>E</italic>(<bold><italic>u</italic></bold>) on a point <bold><italic>u</italic></bold> of a manifold (like the flat Euclidean space) is considered as a mapping of tangent vectors at <italic>u</italic> to the real numbers. When interpreting the derivative as mapping, the <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline503.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is living in the dual space of <italic>u</italic> and is therefore a row vector if is a column vector. If a function <bold><italic>f</italic></bold>(<bold><italic>u</italic></bold>) of <bold><italic>u</italic></bold> is a vector valued, then its derivative is a matrix with entries <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline504.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where <italic>i</italic> is indexing the rows (i.e. running down) and <italic>j</italic> is indexing the columns (i.e. running right). When <bold><italic>r</italic></bold> = <italic>ρ</italic>(<bold><italic>u</italic></bold>) is a column vector with <italic>ρ</italic> applied to each component of <bold><italic>u</italic></bold>, we consider the (partial) derivative <bold><italic>r</italic></bold>′ = <italic>ρ</italic>′(<bold><italic>u</italic></bold>) for convenience as column vector with components <italic>ρ</italic>′(<italic>u</italic><sub><italic>i</italic></sub>). To strictly follow the formalism, it should be a diagonal matrix.</p></list-item>
<list-item><p>Because we introduced the error vector <bold><italic>ē</italic></bold> as column vector, it is easier to write <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline505.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> where now <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline506.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is also considered as column vector. To be consistent with the above, we should have written <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline507.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> The .<sup>T</sup> appeared us as too heavy so that we neglected it, where it did not have further mathematical consequences (hoping it does not cause confusions). Sticking here to a column vector also renders the backpropagation error to be a column vector in <xref ref-type="disp-formula" rid="eqn21">Eq. 21</xref>. This error then gets the classical form with the weight transpose, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline508.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p></list-item>
<list-item><p>We typically have real-valued functions of the form <italic>E</italic>(<bold><italic>u</italic></bold><sub><bold><italic>θ</italic></bold></sub>, <bold><italic>θ</italic></bold>), with <bold><italic>θ</italic></bold> = (<bold><italic>W</italic></bold>, <italic>β</italic>) being a vector of parameters, and <bold><italic>u</italic></bold> being a function of <bold><italic>θ</italic></bold>. To get the total derivative of <italic>E</italic> with respect to <bold><italic>θ</italic></bold> we consider the values <italic>E</italic> as a function of <bold><italic>θ</italic></bold>. This can be done by introducing a new function (<bold><italic>θ</italic></bold>) defined as (<bold><italic>θ</italic></bold>) = <italic>E</italic>(<bold><italic>u</italic></bold><sub><bold><italic>θ</italic></bold></sub>, <bold><italic>θ</italic></bold>), where the components <italic>θ</italic><sub><italic>i</italic></sub> are considered as independent variables. The total derivative of <italic>E</italic> with respect to <bold><italic>θ</italic></bold> is then defined as vector-valued function <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline509.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for a <italic>n</italic>-dimensional <bold><italic>θ</italic></bold>. It can be helpful to think of the components of this total derivative as a (total) directional derivatives in the unit directions. For the last (<italic>n</italic>-th) unit direction Δ<bold><italic>θ</italic></bold><sup>(<italic>n</italic>)</sup> = (0, .., 0, 1), for instance, we have <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline510.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p></list-item>
<list-item><p>The cost gradient, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline511.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> has the same dimension as <bold><italic>W</italic></bold>. Recall that by the cost gradient we mean <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline512.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where C is defined as <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline513.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with the voltage <bold><italic>u</italic></bold><sub><bold>o</bold></sub> of the output neurons being itself a function of <bold><italic>W</italic></bold>.</p></list-item>
<list-item><p>To calculate the partial derivative <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline514.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> with respect to <bold><italic>θ</italic></bold>, we fix the first argument <bold><italic>u</italic></bold><sub><bold><italic>θ</italic></bold></sub>, even if for <bold><italic>u</italic></bold> we often plugged in the components of the trajectory <bold><italic>u</italic></bold> = <bold><italic>u</italic></bold><sub><bold><italic>θ</italic></bold></sub>(<italic>t</italic>) that now does depend on <bold><italic>θ</italic></bold>. In contrast, the total derivative is <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline515.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Here, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline516.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is a row vector, as also <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline517.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, consistent with the convention (that we have broken in <xref ref-type="disp-formula" rid="eqn28">Eq. 28</xref> to keep vectors as columns). When <bold><italic>u</italic></bold> is considered as trajectory, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline517a.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> does not vanish in general, but it does when <bold><italic>u</italic></bold> is simply considered as independent variable.</p></list-item>
<list-item><p>When replacing the argument <bold><italic>u</italic></bold> in <italic>E</italic>(<bold><italic>u, θ</italic></bold>) by <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline518.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> we get the ‘Lagrangian’<inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline519.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The partial derivative of <italic>L</italic> with respect to <bold><italic>ũ</italic></bold>, for instance, is then <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline520.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The partial derivative <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline521.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> considers <italic>L</italic> as a function of independent arguments <bold><italic>ũ</italic></bold>, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline522.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <bold><italic>θ</italic></bold>.</p></list-item>
<list-item><p>We also used that the total derivatives can be exchanged, in the current example <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline523.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This is generally true for derivatives of Lipschitz continuous functions, for which derivatives exist almost everywhere. The total derivatives (where they exist) then commute because the difference quotients in <bold><italic>W</italic></bold> and <italic>β</italic> are uniformly bounded. The Moore-Osgood theorem tells that two limits, of which at least one is uniform in the other, can be commuted. This also applies to the double difference quotients involved in the definition of <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline524.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Remember that the total derivative, for instance with respect to the <italic>n</italic>-th parameter, can be written as <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline525.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p></list-item>
</list>
</sec>
<sec id="s6h">
<title>H Proof of <xref ref-type="statement" rid="the1">Theorem 1</xref>, part (<italic>ii</italic>), using only partial derivatives</title>
<p>This Section proves <xref ref-type="disp-formula" rid="eqn29">Eqs 29</xref>–<xref ref-type="disp-formula" rid="eqn31">31</xref> in terms with only partial derivatives, banning nested functions that require to deal with total derivatives. The 3 equations are also the core for the proof for Equilibrium Propagation <xref ref-type="bibr" rid="c53">Scellier &amp; Bengio, 2017</xref>, although there only applied in the steady state after the network converged to a constant activity.</p>
<p>We assume a network of <italic>d</italic> neurons whose membrane potential is given by <bold><italic>u</italic></bold> ∈ ℝ<sup><italic>d</italic></sup> and which are connected via weights <bold><italic>W</italic></bold> ∈ ℝ<sup><italic>d×d</italic></sup>. By <bold><italic>∇</italic></bold><sub><bold><italic>u</italic></bold></sub>, we denote the gradient with respect to the membrane potentials, i.e., <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline526.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Similarly, <bold><italic>∇</italic></bold><sub><bold><italic>W</italic></bold></sub> is a matrix containing the derivatives with respect to the weights, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline527.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>To prove the rt-DeEP theorem (<xref ref-type="statement" rid="the1">Theorem 1</xref>), we first have to make a few definitions and observations:</p>
<list list-type="order">
<list-item><p>For a given (<bold><italic>W</italic></bold>, <italic>β</italic>), the dynamics yield certain membrane potentials <italic>f</italic><sub><bold><italic>u</italic></bold></sub> ∈ ℝ<sup><italic>d</italic></sup>. Formally, we define this as
<disp-formula id="eqn90">
<alternatives><graphic xlink:href="534198v2_eqn90.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The <italic>i</italic>th element of <italic>f</italic><sub><bold><italic>u</italic></bold></sub> is denoted by <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline528.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and hence <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline529.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p></list-item>
<list-item><p>We define the following functions:</p>
<list list-type="bullet">
<list-item><p>the mismatch energy <italic>E</italic><sup>M</sup> : ℝ<sup><italic>d</italic></sup> × ℝ<sup><italic>d×d</italic></sup> → ℝ, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline530.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula></p></list-item>
<list-item><p>the cost function <italic>C</italic> : ℝ<sup><italic>d</italic></sup> ⟶ ℝ <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline531.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula></p></list-item>
<list-item><p>the Lagrangian <italic>L</italic> : ℝ<sup><italic>d</italic></sup> × ℝ<sup><italic>d×d</italic></sup> × ℝ → ℝ, (<bold><italic>u, W</italic></bold>, <italic>β</italic>) 1→ <italic>L</italic>(<bold><italic>u, W</italic></bold>, <italic>β</italic>) = <italic>E</italic><sup>M</sup>(<bold><italic>u, W</italic></bold>) + <italic>βC</italic>(<bold><italic>u</italic></bold>).</p>
<p>To make the dependency of the cost and energies on <italic>β</italic> and <bold><italic>W</italic></bold> explicit, we further introduce three auxiliary functions <italic>F</italic><sub>M</sub>, <italic>F</italic><sub><italic>C</italic></sub> and <italic>F</italic><sub><italic>L</italic></sub>:</p></list-item>
<list-item><p>for the mismatch energy <italic>F</italic><sub>M</sub> : ℝ<sup><italic>d×d</italic></sup> × ℝ → ℝ, (<bold><italic>W</italic></bold>, <italic>β</italic>) 1→ <italic>F</italic><sub>M</sub>(<bold><italic>W</italic></bold>, <italic>β</italic>) = <italic>E</italic><sup>M</sup> (<italic>f</italic><sub><bold><italic>u</italic></bold></sub>(<bold><italic>W</italic></bold>, <italic>β</italic>), <bold><italic>W</italic></bold>),</p></list-item>
<list-item><p>for the cost function <italic>F</italic><sub><italic>C</italic></sub> : ℝ<sup><italic>d×d</italic></sup> × ℝ → ℝ, (<bold><italic>W</italic></bold>, <italic>β</italic>) 1→ <italic>F</italic><sub><italic>C</italic></sub>(<bold><italic>W</italic></bold>, <italic>β</italic>) = <italic>C</italic> (<italic>f</italic><sub><bold><italic>u</italic></bold></sub>(<bold><italic>W</italic></bold>, <italic>β</italic>)),</p></list-item>
<list-item><p>for the Lagrangian <italic>F</italic><sub><italic>L</italic></sub> : ℝ<sup><italic>d×d</italic></sup> × ℝ → ℝ, (<bold><italic>W</italic></bold>, <italic>β</italic>) 1→ <italic>F</italic><sub><italic>L</italic></sub>(<bold><italic>W</italic></bold>, <italic>β</italic>) = <italic>F</italic><sub>M</sub>(<bold><italic>W</italic></bold>, <italic>β</italic>) + <italic>βF</italic><sub><italic>C</italic></sub>(<bold><italic>W</italic></bold>, <italic>β</italic>)</p></list-item></list></list-item>
<list-item><p>The Euler-Lagrange equations can be written as <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline532.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Hence, far enough away from initialization and for smooth enough input (and targets) we have <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline533.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> at all times, even when changing the network input continuously. Note that both the cost and the mismatch energies are defined on low-pass-filtered signals, and it is with respect to the low-pass filtered external input that the low-pass-filtered output error is minimized.</p></list-item>
<list-item><p>Without output nudging (i.e., <italic>β</italic> = 0), the output error vanishes and consequently all other prediction errors vanish as well, <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline534.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This can be easily shown for layered network architectures and holds true for arbitrary connections (e.g., recurrent networks) as long as <italic>f</italic><sub><bold><italic>u</italic></bold></sub> uniquely exists, i.e., <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline535.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> has a unique solution for <bold><italic>u</italic></bold>. From the form of the mismatch energy, we then get
<disp-formula id="eqn91">
<alternatives><graphic xlink:href="534198v2_eqn91.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Since we are assuming smooth functions, this also implies that
<disp-formula id="eqn92">
<alternatives><graphic xlink:href="534198v2_eqn92.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
</p></list-item>
<list-item><p>From the assumption of well-behaved (smooth) functions, it also follows that partial derivatives commute <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline536.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p></list-item>
</list>
<p>Our goal is to find a plasticity rule that minimizes the cost <italic>C</italic>, which we do by calculating <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline536a.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Similar to <xref ref-type="bibr" rid="c53">Scellier &amp; Bengio (2017)</xref>, to achieve this, we first calculate the partial derivatives of <italic>F</italic><sub><italic>L</italic></sub> with respect to the nudging strength <italic>β</italic>
<disp-formula id="eqn93a">
<alternatives><graphic xlink:href="534198v2_eqn93a.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn93b">
<alternatives><graphic xlink:href="534198v2_eqn93b.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn93c">
<alternatives><graphic xlink:href="534198v2_eqn93c.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn93d">
<alternatives><graphic xlink:href="534198v2_eqn93d.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
and the weights <bold><italic>W</italic></bold>
<disp-formula id="eqn94a">
<alternatives><graphic xlink:href="534198v2_eqn94a.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn94b">
<alternatives><graphic xlink:href="534198v2_eqn94b.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn94c">
<alternatives><graphic xlink:href="534198v2_eqn94c.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn94d">
<alternatives><graphic xlink:href="534198v2_eqn94d.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn94e">
<alternatives><graphic xlink:href="534198v2_eqn94e.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
or in vectorized form
<disp-formula id="eqn95">
<alternatives><graphic xlink:href="534198v2_eqn95.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
With these identities in place, we can calculate the plasticity rule:
<disp-formula id="eqn96a">
<alternatives><graphic xlink:href="534198v2_eqn96a.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn96b">
<alternatives><graphic xlink:href="534198v2_eqn96b.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn96c">
<alternatives><graphic xlink:href="534198v2_eqn96c.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn96d">
<alternatives><graphic xlink:href="534198v2_eqn96d.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn96e">
<alternatives><graphic xlink:href="534198v2_eqn96e.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn96f">
<alternatives><graphic xlink:href="534198v2_eqn96f.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn96g">
<alternatives><graphic xlink:href="534198v2_eqn96g.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where we used that limits can be exchanged for smooth functions. Using the definition of <italic>E</italic><sup>M</sup>, we obtain a plasticity rule that minimizes the cost function
<disp-formula id="eqn97">
<alternatives><graphic xlink:href="534198v2_eqn97.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
In practice, the prefactor <inline-formula><alternatives><inline-graphic xlink:href="534198v2_inline537.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is absorbed into the learning rate.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="sc1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name>, <string-name><surname>Varela</surname>, <given-names>J. a.</given-names></string-name>, <string-name><surname>Sen</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Nelson</surname>, <given-names>S. B.</given-names></string-name> <article-title>Synaptic depression and cortical gain control</article-title>. <source>Science</source> <volume>275</volume>, <fpage>220</fpage>–<lpage>224</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="sc2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Anderson</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Lampl</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Gillespie</surname>, <given-names>D. C.</given-names></string-name> &amp; <string-name><surname>Ferster</surname>, <given-names>D.</given-names></string-name> <article-title>The contribution of noise to contrast invariance of orientation tuning in cat visual cortex</article-title>. <source>Science</source> <volume>290</volume>, <fpage>1968</fpage>–<lpage>1972</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="sc3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Flannery</surname>, <given-names>M. R.</given-names></string-name> <article-title>The enigma of nonholonomic constraints</article-title>. <source>American Journal of Physics</source> <volume>73</volume>, <fpage>265</fpage>–<lpage>272</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="sc4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name> <article-title>The free-energy principle: a unified brain theory?</article-title> <source>Nature Reviews Neuroscience</source> <volume>11</volume>, <fpage>127</fpage>–<lpage>138</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="sc5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Haider</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal> <article-title>Latent Equilibrium: Arbitrarily fast computation with arbitrarily slow neurons</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>34</volume> (<year>2021</year>).</mixed-citation></ref>
<ref id="sc6"><label>6.</label><mixed-citation publication-type="web"><string-name><surname>Jordan</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sacramento</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wybo</surname>, <given-names>W. A. M.</given-names></string-name>, <string-name><surname>Petrovici</surname>, <given-names>M. A.</given-names></string-name> &amp; <string-name><surname>Senn</surname>, <given-names>W.</given-names></string-name> <article-title>Learning Bayes-optimal dendritic opinion pooling</article-title>. <source>arXiv</source>. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2104.13238">http://arxiv.org/abs/2104.13238</ext-link> (<year>2022</year>).</mixed-citation></ref>
<ref id="sc7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Lohmiller</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Slotine</surname>, <given-names>J.-j. E.</given-names></string-name> <article-title>On Contraction Analysis for Non-linear Systems</article-title>. <source>Automatica</source> <volume>34</volume>, <fpage>683</fpage>–<lpage>696</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="sc8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Meulemans</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zucchet</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Kobayashi</surname>, <given-names>S.</given-names></string-name>, von <string-name><surname>Oswald</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Sacramento</surname>, <given-names>J.</given-names></string-name> <article-title>The least-control principle for local learning at equilibrium</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>35</volume> (<year>2022</year>).</mixed-citation></ref>
<ref id="sc9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Nedeljkov</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Oberguggenberger</surname>, <given-names>M.</given-names></string-name> <article-title>Ordinary differential equations with delta function terms</article-title>. <source>Publications de l’Institut Mathematique</source> <volume>91</volume>, <fpage>125</fpage>–<lpage>135</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="sc10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Rauch</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>La Camera</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Lüscher</surname>, <given-names>H.-R.</given-names></string-name>, <string-name><surname>Senn</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Fusi</surname>, <given-names>S.</given-names></string-name> <article-title>Neocortical Pyramidal Cells Respond as Integrate- and-Fire Neurons to In Vivo-Like Input Currents</article-title>. <source>Journal of Neurophysiology</source> <volume>90</volume>, <fpage>1598</fpage>–<lpage>1612</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="sc11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Scellier</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> <article-title>Equilibrium propagation: Bridging the gap between energy-based models and backpropagation</article-title>. <source>Frontiers in computational neuroscience</source> <volume>11</volume>, <fpage>24</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="sc12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Simonetto</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Dall’Anese</surname>, <given-names>E.</given-names></string-name> <article-title>Prediction-Correction Algorithms for Time-Varying Constrained Optimization</article-title>. <source>IEEE Transactions on Signal Processing</source> <volume>65</volume>, <fpage>5481</fpage>–<lpage>5494</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="sc13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Simonetto</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dall’Anese</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Paternain</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Leus</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Giannakis</surname>, <given-names>G. B.</given-names></string-name> <article-title>Time-Varying Convex Optimization: Time-Structured Algorithms and Applications</article-title>. <source>Proceedings of the IEEE</source> <volume>108</volume>, <fpage>2032</fpage>–<lpage>2048</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="sc14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Tsodyks</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Markram</surname>, <given-names>H.</given-names></string-name> <article-title>The neural code between neocortical pyramidal neurons depends on neurotransmitter release probability</article-title>. <source>Proc. Nat. Acad. Sci. USA</source> <volume>94</volume>, <fpage>719</fpage>–<lpage>723</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="sc15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Ulrich</surname>, <given-names>D.</given-names></string-name> <article-title>Dendritic resonance in rat neocortical pyramidal cells</article-title>. <source>Journal of Neurophysiology</source> <volume>87</volume>, <fpage>2753</fpage>–<lpage>2759</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="sc16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Varela</surname>, <given-names>J. A.</given-names></string-name> <etal>et al.</etal> <article-title>A quantitative description of short-term plasticity at excitatory synapses in layer 2/3 of rat primary visual cortex</article-title>. <source>Journal of Neuroscience</source> <volume>17</volume>, <fpage>7926</fpage>–<lpage>7940</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="sc17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Zhao</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Swamy</surname>, <given-names>M. N.</given-names></string-name> <article-title>Novel technique for tracking time-varying minimum and its applications</article-title>. <source>Canadian Conference on Electrical and Computer Engineering</source> <volume>2</volume>, <fpage>910</fpage>–<lpage>913</lpage> (<year>1998</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89674.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>École Normale Supérieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This manuscript describes a potentially <bold>important</bold> theoretical framework to link predictive coding, error-based learning, and neuronal dynamics. The provided evidence is <bold>solid</bold> but would be made more robust if the different lines of argument were more directly connected. Improving the exposition of the manuscript would make it more accessible to a broader audience.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89674.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The manuscript considers a hierarchical network of neurons, of the type that can be found in the sensory cortex, and assumes that they aim to constantly predict sensory inputs that may change in time. The paper describes the dynamics of neurons and rules of synaptic plasticity that minimize the integral of prediction errors over time.</p>
<p>The manuscript describes and analyses the model in great detail, and presents multiple and diverse simulations illustrating the model's functioning. However, the manuscript could be made more accessible and easier to read. The paper may help to understand the organization of cortical neurons, their properties, as well as the function of their particular components (such as apical dendrites).</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89674.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Neuroscientists often state that we have no theory of the brain. The example of theoretical physics is often cited, where numerous and quite complex phenomena are explained by a compact mathematical description. Lagrangian and Hamiltonian pictures provide such powerful 'single equation'. These frameworks are referred to as 'energy', an elegant way to turn numerous differential equations into a single compact relationship between observable quantities (state variables like position and speed) and scaling constants (like the gravity constant or the Planck constant). Such energy-pictures have been used in theoretical neuroscience since the 1980s.</p>
<p>The manuscript &quot;neuronal least-action principle for real-time learning in cortical circuits&quot; by Walter Senn and collaborators describes a theoretical framework to link predictive coding, error-based learning, and neuronal dynamics. The central concept is that an energy function combining self-supervised and supervised objectives is optimized by realistic neuronal dynamics and learning rules when considering the state of a neuron as a mixture of the current membrane potential and its rate of change. As compared with previous energy functions in theoretical neuroscience, this theory captures a more extensive range of observations while satisfying normative constraints. Particularly, no theory had to my knowledge related to adaptive dynamics widely observed in the brain (referred to as prospective coding in the text, but is sometimes referred to as adaptive coding or redundancy reduction) with the dynamics of learning rules.</p>
<p>The manuscript first exposes the theory of two previously published papers by the same group on somato-dendritic error with apical and basal dendrites. These dynamics are then related to an energy function, whose optimum recovers the dynamics. The rest of the manuscript illustrates how features of this model fit either normative or observational constraints. Learning follows a combination of self-supervised learning (learning to predict the next step) and supervised learning (learning to predict an external signal). The credit assignment problem is solved by an apical-compartment projecting a set of interneurons with learning rules whose role is to align many weight matrices to avoid having to do multiplexing. An extensive method section and supplementary material expand on mathematical proofs and makes more explicit the mathematical relationship between different frameworks.</p>
<p>Experts would say that much of the article agglomerates previous theoretical papers by the same authors that have been published recently either in archival servers or in conference proceedings. A number of adaptations to previous theoretical results were necessary, so the present article is not easily reduced to a compendium of previous pre-prints. However, the manuscript is by no means easy to read as there are several inconsistencies and it lacks a single thread. Also, there remains a few thorny assumptions (unobserved details of the learning rules or soma-dendrites interactions), but the theory is likely going to be regarded as an important step towards a comprehensive theory of the brain.</p>
</body>
</sub-article>
</article>