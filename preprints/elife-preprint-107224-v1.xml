<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">107224</article-id>
<article-id pub-id-type="doi">10.7554/eLife.107224</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107224.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>A theory and recipe to construct general and biologically plausible integrating continuous attractor neural networks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8071-6628</contrib-id>
<name>
<surname>Claudi</surname>
<given-names>Federico</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
<email>federicoclaudi@protonmail.com</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3047-8334</contrib-id>
<name>
<surname>Chandra</surname>
<given-names>Sarthak</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4738-2539</contrib-id>
<name>
<surname>Fiete</surname>
<given-names>Ila R</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/042nb2s44</institution-id><institution>Department of Brain and Cognitive Sciences &amp; McGovern Institute, MIT</institution></institution-wrap>, <city>Cambridge</city>, <country country="US">United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/042nb2s44</institution-id><institution>Integrative Computational Neuroscience Center and Yang-Tan Collective, MIT</institution></institution-wrap>, <city>Cambridge</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Ã‰cole Normale SupÃ©rieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country country="FR">France</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Frank</surname>
<given-names>Michael J</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Brown University</institution>
</institution-wrap>
<city>Providence</city>
<country country="US">United States</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>*</label><p>Both authors contributed equally to this work.</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-07-28">
<day>28</day>
<month>07</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP107224</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-04-25">
<day>25</day>
<month>04</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-05-11">
<day>11</day>
<month>05</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.05.07.652608"/>
</event>
</pub-history>
<permissions>
<copyright-statement>Â© 2025, Claudi et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Claudi et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-107224-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Across the brain, circuits with continuous attractor dynamics underpin the representation and storage in memory of continuous variables for motor control, navigation, and mental computations. The represented variables have various dimensions and topologies (lines, rings, euclidean planes), and the circuits exhibit continua of fixed points to store these variables, and the ability to use input velocity signals to update and maintain the representation of unobserved variables, effectively integrating the incoming velocity signal. Integration constitutes a general computational strategy that enables variable state estimation when direct observation of the variable is not possible, suggesting that it may play a critical role in other cognitive processes. While some neural network models for integration exist, a comprehensive theory for constructing neural circuits with a given topology and integration capabilities is lacking. Here, we present a theoretically-driven design framework, Manifold Attractor Direct Engineering (MADE), to automatically, analytically, and explicitly construct biologically plausible continuous attractor neural networks with diverse user-specified topologies. We show how these attractor networks can be endowed with accurate integration functionality through biologically realistic circuit mechanisms. MADE networks closely resemble biological circuits where the attractor mechanisms have been characterized. Additionally, MADE offers innovative and minimal circuit models for uncharacterized topologies, enabling a systematic approach to developing and testing mathematical theories related to cognition and computation in the brain.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The brains of species from insects to mammals contain circuits specialized to represent and integrate continuous variables (<xref rid="fig1" ref-type="fig">Figure 1A</xref>) [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>]: the head direction circuits in mammals [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c5">5</xref>], fish [<xref ref-type="bibr" rid="c6">6</xref>], and flies [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c10">10</xref>], the oculomotor system of vertebrates [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>], and grid cell networks in mammals [<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c18">18</xref>] (see <xref rid="fig1" ref-type="fig">Figure 1B,C,D</xref>). These circuits receive velocity inputs, representing the rate of change of the represented variable, and update their internal state in proportion to the instantaneous velocity [<xref ref-type="bibr" rid="c1">1</xref>]. The oculomotor circuit integrates head velocity signals to counter-rotate the eyes and hold the gaze fixed during head movements [<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c11">11</xref>]; it also integrates saccadic velocity commands to generate stable fixations at different gaze angles between saccades [<xref ref-type="bibr" rid="c13">13</xref>]. In the head direction and grid cell circuits for spatial navigation, self-movement cues from turning and walking update the internal pose estimates [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>]. This so-called path integration (PI) computation underpins behaviors that are core for survival [<xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c24">24</xref>].</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Existing continuous attractor network models and the biological systems where they are found.</title>
<p><bold>A</bold>. Left, schematic representation of a spatially embedded set of neurons and their connections. The neural connectivity constrains the patterns of neural co-activation, thus determining the dimensionality and topology of neural activity in the state space. Center, schematic representation of neural activity states, in this case forming a continuous manifold in state space. Right, schematic representation of the states of a (latent) variable in the external world. <bold>B</bold>,<bold>C</bold>,<bold>D</bold>. Examples of integrator circuits. Top row, integration in the oculomotor system. Center row, head direction system. Bottom row shows the grid cell system. <bold>B</bold>. Schematic representation of CAN models architecture for line, ring and torus attractors. <bold>C</bold>. Schematic illustration of the continuous manifolds of fixed points predicted and found to exist in the corresponding circuits, adapted from published work [<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c47">47</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c48">48</xref>, <xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c46">46</xref>, <xref ref-type="bibr" rid="c44">44</xref>]. <bold>D</bold>. Schematic illustration of variable manifolds.</p></caption>
<graphic xlink:href="652608v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Integration may also underlie the representation and mapping of other continuous domains including auditory sound spaces, parametric image variations, and emotional/aggression states [<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c32">32</xref>], and thus support inference, reasoning, planning, and imagination in all these domains.</p>
<p>Neural network models of these integrator circuit fall under the category of continuous attractor networks (CANs) [<xref ref-type="bibr" rid="c33">33</xref>, <xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c18">18</xref>]. All continuous attractor models posit recurrent circuitry to generate a continuous set of states that persist in the absence of external inputs (continuous attractors). However, not all CAN models are integrators: integrators must additionally contain a mechanism for updating the internal state based on velocity inputs. CAN models generate extensive predictions about circuit connectivity, activity, population dynamics, and lesions, and have stimulated extensive experimental work across species and circuits to test their predictions. Core novel predictions of these models have subsequently been validated via physiology, imaging, and connectomics: the dynamics and connectivity of the oculomotor integrator [<xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c38">38</xref>] have been shown to match the hypothesized circuit model in considerable detail. The one-dimensional ring attractor dynamics, including fixed point dynamics, isometric representation in the head direction circuit in mammals matches [<xref ref-type="bibr" rid="c3">3</xref>] the predicted population dynamics of the ring integrator models in detail [<xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c34">34</xref>]. In insects, the connectivity and physical layout of the head direction circuit form an actual ring [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c10">10</xref>] and exhibit some of the shift-like asymmetries hypothesized by a subset of the ring attractor models [<xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c35">35</xref>]. In the grid cell system, the invariant two-dimensional population dynamics [<xref ref-type="bibr" rid="c40">40</xref>, <xref ref-type="bibr" rid="c41">41</xref>, <xref ref-type="bibr" rid="c42">42</xref>, <xref ref-type="bibr" rid="c43">43</xref>] and its localization [<xref ref-type="bibr" rid="c44">44</xref>] to the predicted torus of fixed point states [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c45">45</xref>, <xref ref-type="bibr" rid="c46">46</xref>] has been directly observed in experiments. Thus, when available, circuit models have propelled a conceptual understanding of the structure and function of the mechanisms involved in integration, memory, and control of continuous variables, and driven experiments that have confirmed their mechanistic hypotheses</p>
<p>These models have been hand-crafted through intuition and insight, individually for each circuit or system in the brain. It is remarkable that the corresponding biological circuits have been found possess a structure, in the population dynamics and when direct physical comparisons have been possible in the circuit architecture, that closely matches these models [<xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c40">40</xref>, <xref ref-type="bibr" rid="c41">41</xref>, <xref ref-type="bibr" rid="c42">42</xref>, <xref ref-type="bibr" rid="c43">43</xref>, <xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c1">1</xref>]. This suggests that mathematically guided and conceptually minimal models are well-matched to the biology of the brain. Yet we lack a general mathematical theory to allow researchers to automatically construct such models for other continuous variables of a given dimension and topology, to generate predictions for future experiments and for potential use in machine learning applications involving such input variables.</p>
<p>Recent efforts to overcome this limitation center on training networks via gradient learning to perform continuous integration tasks on the desired variable [<xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c50">50</xref>, <xref ref-type="bibr" rid="c51">51</xref>, <xref ref-type="bibr" rid="c52">52</xref>]. However, the difficulties of this approach for the formation of continuous attractor networks is that it is in-efficient, and the results are not usually interpretable. Specifically, training on <italic>M</italic> -dimensional manifolds requires of order <italic>k</italic><sup>â„³</sup> samples [<xref ref-type="bibr" rid="c53">53</xref>, <xref ref-type="bibr" rid="c54">54</xref>], scaling exponentially with manifold dimension. In the few cases where the results become interpretable, it is only through mapping onto the original â€œhand designedâ€ models. The combination of these factors and the striking match between biology and the minimal hand-crafted models suggests that a set of simple and general mathematical principles are used by biology to build such circuits and if discovered, can be used to directly construct circuit models for integration of arbitrary continuous variables.</p>
<p>Here, we present such a small set of mathematical principles to directly construct minimal, interpretable continuous attractor networks that integrate variables of rich topologies and geometries. The theoretical framework converts directly into a practical recipe for constructing integrating attractor networks of desired dimension and topology. Existing integration networks known from biology appear as special cases of this framework. We name the method MADE (Manifold Attractor Direct Engineering). Thus, MADE can serve as a generally useful framework for making circuit predictions about connectivity and function in neural integrators not yet discovered, including in high-level areas that perform various cognitive tasks.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>Integration is the task of estimating the value of some (potentially latent) continuous variable <italic>x</italic>(<italic>t</italic>), based on an initial condition and inputs conveying information about<inline-formula><inline-graphic xlink:href="652608v1_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, its instantaneous rate of change. For a variable to be integrable, it must be continuous and lie on a â€˜differentiable manifoldâ€™: a smooth, continuous space that at small scales is similar to Euclidean space, though globally it may be non-Euclidean, with complex topology. For a neural circuit to integrate, its representations must form a differential manifold, and if the velocity signal is zero then the read out state should not change over time. In constructing a neural circuit that can integrate a given variable, we therefore need two components: a network that possesses a manifold of states that support a stable readout value, whose dimension and topology matches the variable, and a mechanism to allow velocity inputs to move states along the manifold. In what follows, we derive a general theory for achieving both with neural circuits, assuming that the stable readouts are stable population states on the manifold.</p>
<sec id="s2a">
<title>Theory: Continuous attractor manifolds of desired dimension and topology</title>
<p>Here we describe the theoretical elements sufficient to construct a neural network possessing a continuous set of attractor states with desired intrinsic dimensionality <italic>d</italic> (e.g., <italic>d</italic> = 1 for a ring lattice and <italic>d</italic> = 2 for a plane) and desired topology specified by a manifold ğ’«.</p>
<p>Consider a set of <italic>N</italic> neurons and spatially embed them, equally spaced (in a lattice), according to the desired manifold topology ğ’«. With this embedding, each neuron has a unique <italic>d</italic>-dimensional coordinate <italic>Î¸</italic><sub><italic>i</italic></sub>. This spatial organization is used for the specification of network connectivity, <italic>W</italic><sub><italic>ij</italic></sub> = <italic>W</italic> (<italic>Î¸</italic><sub><italic>i</italic></sub>, <italic>Î¸</italic><sub><italic>j</italic></sub>); it may but need not mirror the actual locations of neurons in neural tissue [<xref ref-type="bibr" rid="c18">18</xref>]. We use rate-based neurons with standard recurrent weighted sums and point-wise neural nonlinearity given by the function <italic>f</italic>. The activation of the neuron at <italic>Î¸</italic><sub><italic>i</italic></sub> is denoted<inline-formula><inline-graphic xlink:href="652608v1_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. For better analytical transparency â€” so that weights and activations can be written as functions instead of lists of numbers â€” we follow others [<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c55">55</xref>] and take the continuum neural field limit. The discrete lattice of positions on the neural manifold ğ’« and neural activations become<inline-formula><inline-graphic xlink:href="652608v1_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, respectively. Additionally, âˆ‘<sub><italic>i</italic></sub> â†’ âˆ« <italic>dÎ¸</italic>, âˆ‘<sub><italic>j</italic></sub> <italic>W</italic><sub><italic>ij</italic></sub><italic>s</italic><sub><italic>j</italic></sub> â†’ âˆ« <italic>W (Î¸,Î¸</italic><sup>â€²</sup>)<italic>s (Î¸</italic><sup>â€²</sup>)<italic>dÎ¸</italic><sup>â€²</sup>, so that the neural network equations are:
<disp-formula id="eqn1">
<graphic xlink:href="652608v1_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>We will use the rectifying nonlinearity, <italic>f</italic> (<italic>x</italic>) = <italic>x</italic> if <italic>x &gt;</italic> 0 and <italic>f</italic> (<italic>x</italic>) = 0 if <italic>x</italic> â‰¤ 0. Derivations that follow are conceptually and qualitatively independent of this continuum limit.</p>
<p>We seek interaction weights consistent with the formation, through symmetry breaking, of a single activity bump state that can be positioned anywhere on the neural manifold ğ’«. The set of such bump states will form the continuous set of attractor states of desired dimension and topology.</p>
<p>Let <italic>W</italic> be a kernel function, <italic>W</italic> (<italic>Î¸, Î¸</italic><sup>â€²</sup>) = <italic>k</italic>(<italic>d</italic>(<italic>Î¸, Î¸</italic><sup>â€²</sup>)), where <italic>d</italic>(<italic>Î¸, Î¸</italic><sup>â€²</sup>) is a distance metric defined on ğ’«, and <italic>k</italic> is a continuous scalar function that is symmetric about the origin (see <xref rid="fig2" ref-type="fig">Figure 2A</xref>). Analogous to prior work [<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c48">48</xref>, <xref ref-type="bibr" rid="c56">56</xref>], we set <italic>k</italic> to be locally excitatory and globally inhibitory. To avoid runaway excitability, we make it strictly inhibition-dominated (<italic>k</italic>(<italic>d</italic>) â‰¤ 0 for all <italic>d</italic>) as in [<xref ref-type="bibr" rid="c57">57</xref>, <xref ref-type="bibr" rid="c18">18</xref>]; network activity can be non-zero because of a compensatory spatially- and temporally-constant excitatory feed-forward drive <italic>b &gt;</italic> 0. Specifically, <italic>k</italic>(<italic>d</italic>) = âˆ’<italic>k</italic><sub>0</sub> + <italic>k</italic><sub>1</sub>(<italic>d</italic>), where <italic>k</italic><sub>0</sub> <italic>&gt;</italic> 0 is a positive number and <italic>k</italic><sub>1</sub>(<italic>d</italic>) â†’ 0 as <italic>d</italic> â†’ <italic>âˆ</italic> with <italic>k</italic><sub>1</sub>(0) = <italic>k</italic><sub>0</sub>.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>CAN construction and activity manifolds.</title>
<p><bold>A</bold>.Left, neural lattice ğ’« for the Plane (top) and Torus (bottom) attractor networks. Black circles indicate the location of an example neuron, shades of green represent distance from other points on the lattice. Bottom right, inhibitory connectivity strength between the example neurons and all other points on the neural lattice. Middle inset, three examples of valid connectivity kernel functions k. <bold>B</bold>. Neural manifold in state space (top,right) and activity patterns on the neural lattice ğ’« (top,left). Bottom row shows three activity patterns with bumps at different locations corresponding to different points on the activity manifold ğ’©.</p></caption>
<graphic xlink:href="652608v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Let the kernelâ€™s length scale be given by <italic>Ïƒ</italic>, i.e., <italic>k</italic><sub>1</sub>(<italic>d</italic>) <italic>â‰ˆ</italic> 0 for <italic>d</italic> â‰¥ <italic>Ïƒ</italic>, with <italic>Ïƒ</italic> selected to be much smaller than the distances <italic>L</italic> over which the manifold ğ’« has curvature. Thus, within any ball <italic>V</italic><sub>l</sub> of radius <italic>l</italic> such that <italic>Ïƒ â‰ª l â‰ª L</italic>, ğ’« is flat. Since <italic>Ïƒ</italic> is the only spatial scale being introduced in the dynamics, we qualitatively expect that a localized bump state within the ball will have a spatial scale of O(<italic>Ïƒ</italic>). The conditions for the formation of a stable single bump state are thus the same as those for a globally flat manifold.</p>
<p>Since <italic>W</italic> is symmetric, <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref> can be described through an energy function [<xref ref-type="bibr" rid="c58">58</xref>], and a stable steady state must exist. If the homogeneous state (all neurons equally active) were unstable, there must exist some other stable state, with broken symmetry. If the symmetry broken state is localized, we would refer to it as a bump state. Thus, we seek conditions under which the homogeneous steady state is unstable. The homogeneous steady state <italic>s</italic>(<italic>x</italic>) = <italic>s</italic><sub>0</sub> must satisfy
<disp-formula id="eqn2">
<graphic xlink:href="652608v1_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>We derive the existence and stability of the homogeneous state (Appendix 1) following the analysis in Ref. [<xref ref-type="bibr" rid="c59">59</xref>], to obtain two requirements for the formation of a stable bump state:first, the Fourier transform of the kernel <italic>k</italic><sub>1</sub>(<italic>d</italic>), which we denote as<inline-formula><inline-graphic xlink:href="652608v1_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, must be maximized at <italic>Ï‰</italic> = 0; and second, this maximum must be larger than 1<italic>/Ï„</italic>. If <italic>k</italic> attains a positive maximum value at <italic>Ï‰</italic> = 0, a rescaling can always make this maximum larger than 1<italic>/Ï„</italic>.</p>
<p>A broad sufficiency condition for the first requirement is if <italic>k</italic><sub>1</sub>(<italic>d</italic>) â‰¥ 0 for all <italic>d</italic>, then its Fourier transform is maximized at zero (proof in Appendix 1). This condition does not include all interaction kernels <italic>k</italic><sub>1</sub> whose Fourier transforms are maximized at zero, but is a sufficiently broad class.</p>
<p>Thus, up to a rescaling of the strength of the interaction, an interaction <italic>W</italic> (<italic>d</italic>(<italic>Î¸, Î¸</italic><sup>â€²</sup>)) will lead to the formation of a bump state if it can be rewritten as <italic>W</italic> (<italic>d</italic>(<italic>Î¸, Î¸</italic><sup>â€²</sup>)) = <italic>k</italic><sub>1</sub>(<italic>d</italic>(<italic>Î¸, Î¸</italic><sup>â€²</sup>)) âˆ’ <italic>k</italic><sub>0</sub> for: <italic>k</italic><sub>0</sub> â‰¥ 0; a kernel <italic>k</italic><sub>1</sub> that satisfies <italic>k</italic><sub>1</sub>(<italic>d</italic>) â‰¥ 0 and <italic>k</italic><sub>1</sub>(<italic>d</italic>) â†’ 0 for <italic>d</italic> â‰¥ <italic>Ïƒ</italic> ; and sufficiently small <italic>Ïƒ</italic> over which the manifold ğ’« is approximately flat. As a result, there is a set of stable fixed points corresponding to activity profiles that are in one-to-one correspondence with points on ğ’«: every stable single-bump activity pattern is centered at some point in ğ’«, and every point in ğ’« forms the center of some stable single-bump state (see <xref rid="fig2" ref-type="fig">Figures 2B</xref>). Thus, the set of stable states of the dynamics in <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref> form a continuous attractor manifold ğ’© that has a bijection with the manifold of the neural layout ğ’« and thus to the target manifold. Moreover, importantly for representation and integration of continuous variables, we show in Appendix 2 that ğ’« and ğ’© are isometric to each other, with respect to their intrinsic geodesic metrics.</p>
</sec>
<sec id="s2b">
<title>Theory: Integration on manifolds</title>
<p>The theoretical and practical frameworks outlined above show how to construct neural networks whose activity states possess a set of attractors forming a manifold ğ’© of desired dimension and topology. Here, given the desired manifold ğ’©, we describe how the constructed attractor network with states matching the topology and dimension of ğ’© can be augmented to endow them with the ability to perform velocity integration.</p>
<p>Note that to perform velocity integration of an external observed variable, the desired manifold ğ’© may, but need not, coincide in dimension and topology with the manifold on which the observed variable states lie. This possibility is exemplified by grid cells, where the manifold ğ’© of a grid module is ğ’©= ğ•‹<sup>2</sup> and is used to integrate animal velocities as animals move about in physical 2D space (thus â„³= â„<sup>2</sup>). In a future work, we will consider the question of which internal manifolds ğ’©, not necessarily of the same topology or dimension as â„³, permit accurate integration of velocities on â„³. Here we show how to equip networks with attractor manifold ğ’© with accurate path integration functionality for velocity inputs of matching dimensionality.</p>
<p>Previous models [<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c18">18</xref>] constructed offset interactions between multiple copies of a continuous attractor network to permit external inputs to drive the state along the mani-fold. Here, we analytically derive the conditions required for an external input that has no knowledge about the structure and state of the continuous attractor network to generate appropriate movements along the nonlinear attractor manifolds of given topology, and show that offset interactions are necessary solutions.</p>
<p>For simplicity, consider a one-dimensional manifold with linear transfer function <italic>f</italic>. The stable bump states are fixed points of <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>:
<disp-formula id="eqn3">
<graphic xlink:href="652608v1_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>s</italic>(<italic>Î¸</italic>) denotes an activity bump centered at any point in ğ’«. Consider two such activity bump states: <italic>s</italic><sub>0</sub>(<italic>Î¸</italic>) centered at <italic>Î¸</italic><sub>0</sub> and <italic>s</italic><sub>0</sub>(<italic>Î¸</italic> + <italic>Ïµ</italic>) centered at <italic>Î¸</italic><sub>0</sub> âˆ’ <italic>Ïµ</italic>. For the neural state to move from <italic>s</italic><sub>0</sub>(<italic>Î¸</italic>) to <italic>s</italic><sub>0</sub>(<italic>Î¸</italic> + <italic>Ïµ</italic>) in time Î”<italic>t</italic>, the time derivative <italic>âˆ‚s/âˆ‚t</italic> must equal
<disp-formula id="ueqn1">
<graphic xlink:href="652608v1_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>The movement speed is <italic>v</italic> = <italic>Ïµ/</italic>Î”<italic>t</italic>. Multiplying by <italic>Ï„</italic> on both sides, we have
<disp-formula id="eqn4">
<graphic xlink:href="652608v1_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>We can add 0 to the equation above, in the form (âˆ’<italic>s</italic><sub>0</sub> + âˆ« <italic>W</italic> (<italic>Î¸</italic> âˆ’ <italic>Î¸</italic><sup>â€²</sup>)<italic>s</italic><sub>0</sub>(<italic>Î¸</italic><sup>â€²</sup>)<italic>dÎ¸</italic><sup>â€²</sup> + <italic>b</italic>), which is zero because of the equality of <xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref>), to obtain:
<disp-formula id="eqn5">
<graphic xlink:href="652608v1_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Comparing this expression to <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>, we see that moving the bump with velocity <italic>v</italic> can be achieved by adding a feedforward input drive <inline-formula><inline-graphic xlink:href="652608v1_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to the continuous attractor network. Though this appears to be a simple way to drive the activity bump on the manifold, it would require the external input to â€œknowâ€ the current value of <inline-formula><inline-graphic xlink:href="652608v1_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which varies along the manifold. Thus, the external input would need to know both the shape and current state on the internal neural activity manifold.</p>
<p>Observing that <inline-formula><inline-graphic xlink:href="652608v1_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (from <xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref>), and grouping like terms, we obtain
<disp-formula id="eqn6">
<graphic xlink:href="652608v1_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>This expression has now â€œinternalizedâ€ the desired input to move the bump, converting it into the weight asymmetry term <inline-formula><inline-graphic xlink:href="652608v1_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, similar to [<xref ref-type="bibr" rid="c34">34</xref>]. The weight asymmetry is internal to the network, thus the velocity external input would not need to be aware of the internal state or shape on the attractor manifold to drive the bump. However, the external input would be required to dynamically modulate the degree of weight asymmetry, a biologically unrealistic requirement. As a final step, observe that for small <italic>Ï„Ïµ/</italic>Î”<italic>t</italic> â‰¡ <italic>Î´</italic>, by Taylor expansion,<inline-formula><inline-graphic xlink:href="652608v1_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula> Thus, we obtain
<disp-formula id="eqn7">
<graphic xlink:href="652608v1_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Because we have that <italic>Î´</italic> = <italic>Ï„Ïµ/</italic>Î”<italic>t</italic> = <italic>Ï„v</italic>, the equation above results in a moving bump along the internal state-space manifold of fixed points ğ’© with speed <italic>v</italic> = <italic>Î´/Ï„</italic>, without any external velocity input or temporally varying modulation of network weights. The network corresponds to the original continuous attractor network constructed in the previous section, with the modification that the weights, instead of being symmetric, have a small offset in a particular direction <italic>Î´</italic> along the neural circuit manifold ğ’«. The speed of bump movement on ğ’© is proportional to the magnitude of the offset, |<italic>Î´</italic>|, and inversely proportional to the neural time-constant.</p>
<p>This continuous-speed flow may form a periodic cycle on specific manifolds (e.g. Ref.[<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c35">35</xref>]). In these cases, the network is a limit cycle attractor. On generic manifolds, however, this flow need not close periodically on itself. The result will be a quasiperiodic attractor dynamics [<xref ref-type="bibr" rid="c60">60</xref>]. We therefore refer to these as Quasiperiodic Attractor Networks (QANs). The flow of activity patterns in a QAN defines a constant vector field Î¨ on ğ’©.</p>
<p>For several attractor manifolds ğ’© of dimension <italic>d</italic> (in particular, â€˜parallelizable manifoldsâ€™ such as the Euclidean spaces â„<sup>d</sup> and the Torii ğ•‹<sup>d</sup>) it is possible to construct <italic>d</italic> QANs with linearly independent flows, and 2<italic>d</italic> QANs with two mutually opposing flows in each of <italic>d</italic> dimensions (defined by weight matrices <italic>W</italic> (<italic>Î¸</italic> âˆ’ <italic>Î¸</italic><sup>â€²</sup> Â± <italic>Î´</italic><sub>m</sub>), where <italic>Î´</italic><sub>m</sub> is a displacement vector of norm |<italic>Î´</italic>| along the <italic>m</italic><sup>th</sup> manifold dimension). Each sets up a constant vector field Î¨<sub>Â±m</sub> on ğ’©. For these manifold topologies [<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c18">18</xref>], opposing-pair QANs numbering 2<italic>d</italic>, where <italic>d</italic> is the manifold dimension, can generate smooth non-vanishing flows of any direction at every point and are thus sufficient to construct integrators. The combined dynamics is given by:
<disp-formula id="eqn8">
<graphic xlink:href="652608v1_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>s</italic><sub>Ïƒm</sub> indicates neural activities in the individual QANs and <inline-formula><inline-graphic xlink:href="652608v1_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is an input carrying information about the rate of change of the external variable in the <italic>m</italic><sup>th</sup> direction.</p>
<p>Coupled in this way, the QANs form a network whose combined activity state moves on ğ’© in a way controlled by the velocity inputs<inline-formula><inline-graphic xlink:href="652608v1_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which modulate the activity levels of the individual QANs. When <inline-formula><inline-graphic xlink:href="652608v1_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for all <italic>Ïƒ, m</italic>, the action of the opposing QANs along each dimension restores the symmetry of the system and <italic>s</italic> remains stationary (it does not flow along ğ’©). Otherwise, the terms <inline-formula><inline-graphic xlink:href="652608v1_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula> differentially modulate the activation of the QANs, causing the activity bump on ğ’« to flow in the direction of the positively modulated QANs. The result is a time-varying vector field Î¨<sub><italic>t</italic></sub>. For accurate path integration, the component vector fields must be smooth and the set of QANs must generate a complete basis set of non-vanishing vector fields at every point on ğ’©. This condition is satisfied by using 2<italic>d</italic> QANs for Euclidean spaces â„<sup>d</sup> and Torii ğ•‹<sup>d</sup>, thus the prescription above is sufficient for integration on these manifolds.</p>
<p>On other manifolds, 2<italic>d</italic> opposing QANs for the <italic>d</italic> manifold dimensions are not sufficient for accurate integration. For instance, in the case of even-dimensional spheres, the hairy ball theorem states that every continuous tangent vector field must vanish at some point(s) [<xref ref-type="bibr" rid="c61">61</xref>, <xref ref-type="bibr" rid="c62">62</xref>, <xref ref-type="bibr" rid="c63">63</xref>]. In other words, a continuous vector field Î¨<sub>Â±m</sub> generated by the QAN prescription above will be zero somewhere on the sphere; at that location, the QAN will not be able to drive bump movement; thus, <italic>d</italic> QAN pairs will not suffice for good integration everywhere. Further, on non-orientable manifolds such as the MÃ¶bius band, it is not possible to define continuous vector fields that are globally orthogonal everywhere and smooth. Thus, while the approach above provides a unified way to construct integrating continuous attractor networks â€” including all those with a single bump state currently found in the neuroscience literature [<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c47">47</xref>, <xref ref-type="bibr" rid="c48">48</xref>, <xref ref-type="bibr" rid="c56">56</xref>] â€” it needs to be further generalized for manifolds that do not permit non-vanishing continuous tangent vector fields everywhere.</p>
</sec>
<sec id="s2c">
<title>Generalization: Killing vector fields</title>
<p>To enable accurate path integration over a significantly wider set of manifolds (excluding the Klein bottle), we now broaden and further generalize the concepts developed above. The approach replaces the constant weight offset vector fields Î¨<sub>Â±m</sub> with the more generally applicable Killing vector fields [<xref ref-type="bibr" rid="c62">62</xref>]: Killing fields are vector fields on a manifold whose flows preserve the structure of the manifold, i.e., they are continuous isometries on the manifold. Conceptually, if each point of an object on the manifold is displaced by the corresponding Killing vector, it will move without distortion. Killing fields form a â€˜vector spaceâ€™, such that linear combinations of Killing fields are also Killing fields. The manifold isometric property of Killing fields means that activity patterns are rigidly translated over ğ’« through the flow Î¨<sub><italic>t</italic></sub> without changes in area, a necessary condition for accurate integration [<xref ref-type="bibr" rid="c34">34</xref>].</p>
<p>To generate Killing fields in each QAN, the constant weight offsets are replaced by an appropriate position-dependent offset:
<disp-formula id="eqn9">
<graphic xlink:href="652608v1_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where Â±<italic>Î´</italic><sub>m</sub>(<italic>Î¸</italic>) is the offset vector of the <italic>Ïƒ, m</italic><sup>th</sup> QAN at coordinates <italic>Î¸</italic> on ğ’«. This allows for weight offsets to vary at different locations on the manifold ğ’© consistent with non-constant</p>
<p>Killing fields required on the sphere (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). This simple change, and allowing the number of QANs to be larger than 2<italic>d</italic>, endows a much broader class of continuous attractor manifolds including spheres and MÃ¶bius band with integration functionality. For a two-dimensional sphere, three basis Killing fields (<italic>d</italic><sub><italic>kill</italic></sub> = 3) are required (each corresponding to rotational symmetry along one principal axis; <xref rid="fig3" ref-type="fig">Figure 3C</xref>). Although each field vanishes at two points on the sphere, at least two fields are non-vanishing and point in independent directions along the manifold at any point, forming an overcomplete basis such that it is possible for the network to perform accurate path integration.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Quasiperiodic Attractor Networks for Path Integration.</title>
<p><bold>A</bold>. Schematic representation of a desired 2D spherical set of fixed points in state space and corresponding connectivity on ğ’«. <bold>B</bold>. Example activity bump plotted on the neural manifold ğ’«. <bold>C</bold>. Schematic illustration of Killing vector fields for the sphere manifold, left, and resulting offset connectivity weights on ğ’«, right. <bold>D</bold>. Schematic illustration of the QAN approach to velocity integration. Left two panels, relationship between changes in the variable on â„³ and on the neural ğ’© manifold, and associated tangent vectors. Center, each QAN receives a velocity-dependent input based on the tangent vectors at left projected onto its Killing fields, and the activity of all networks is combined. Right: this results in a trajectory in the state-space ğ’©, which corresponds to velocity integration of inputs from â„³.</p></caption>
<graphic xlink:href="652608v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Finally, we generalize how an external manifold â„³ may be mapped to the internal integrating manifold ğ’©, by mapping velocity vectors in the external space to the QANs within the network. Throughout, our construction seeks to make ğ’« and ğ’© isometric, and indeed they are, as shown in Appendix 2. However, as noted at the start of this section, ğ’© need not exactly match the topology of the external variable: ğ’© = T<sup>2</sup> of a grid module represents positions on â„³ = â„<sup>2</sup> of the externals partial variable. Similarly, the dimensionality of ğ’© could equal or exceed that of â„³: a planar integrator network is capable of integrating an external one-dimensional variable if the velocity inputs are one-dimensional. For instance, grid cell responses on a linear track appear to be generated as a slice through their 2D manifold of states [<xref ref-type="bibr" rid="c41">41</xref>, <xref ref-type="bibr" rid="c64">64</xref>].</p>
<p>Define <italic>Ï€</italic> as the mapping of â„³ to ğ’© (which can be the identity map or the isomorphism map when â„³ and ğ’© are isomorphic, such as when head direction is represented in a ring attractor, or a many-to-one map as when spatial position is represented in a single grid module). The Jacobian <italic>Ï€</italic><sub>â‹†</sub> is a map from the tangent space of â„³ to the tangent space of ğ’©: it is the operator that maps tangent vectors from â„³ (i.e.<inline-formula><inline-graphic xlink:href="652608v1_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula>) to tangent vectors of ğ’© [<xref ref-type="bibr" rid="c63">63</xref>, <xref ref-type="bibr" rid="c65">65</xref>]. In other words, the velocity vector <inline-formula><inline-graphic xlink:href="652608v1_inline15.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is â€˜pushed forwardâ€™ through the map <italic>Ï€</italic> into <inline-formula><inline-graphic xlink:href="652608v1_inline16.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (<xref rid="fig3" ref-type="fig">Figure 3D</xref>). The coupled system dynamics can be written as
<disp-formula id="eqn10">
<graphic xlink:href="652608v1_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>W</italic><sub>Ïƒm</sub> refers to the Killing-field weights from <xref ref-type="disp-formula" rid="eqn9">Eq. 9</xref>, <italic>d</italic><sub>k<italic>i</italic>ll</sub> defines the minimal number of independent Killing fields. The term <inline-formula><inline-graphic xlink:href="652608v1_inline17.gif" mimetype="image" mime-subtype="gif"/></inline-formula> refers to the projection of the velocity pushed through <italic>Ï€</italic> onto the (<italic>Ïƒ, m</italic>)<sup>th</sup> QAN. Note that in general, the Jacobian <italic>Ï€</italic><sub>â‹†</sub> maps the tangent space at a specific point on â„³ to the tangent space at a specific point on ğ’©, making it dependent i principle on both <italic>x</italic> âˆˆ â„³ and <italic>s</italic> âˆˆ ğ’©. Thus, neural circuits generating <inline-formula><inline-graphic xlink:href="652608v1_inline18.gif" mimetype="image" mime-subtype="gif"/></inline-formula> would require access to both the integratorâ€™s neural state and the external variable. While the neural state <italic>s</italic> is available to the brain, <italic>x</italic> cannot be directly observed. However, if the integrator network maintains an accurate estimate of this variable â€” an expected property of a reliable integrator â€” then the brain can instead evaluate <italic>Ï€</italic><sub>â‹†</sub> at the integratorâ€™s state on â„³ as a proxy for <italic>x</italic>.</p>
<p>The constant vector fields on the ring and torus manifolds described above (and effectively discovered in previous work) are Killing fields. Therefore, this approach encompasses previous work and provides a broader general framework for constructing minimal biologically plausible continuous attractor neural networks capable of path integration on spaces of various dimension and topology. Next, we demonstrate how to practically construct the networks, the examine the effectiveness of the approach through extensive numerical simulations of path integration in MADE integrator networks.</p>
</sec>
<sec id="s2d">
<title>Practical construction of CAN integrators with MADE</title>
<p>With the complete conceptual and mathematical frameworks in place, we now illustrate through numerical simulation how to apply the MADE prescription to construct various CANs and integrators of desired dimension and topology. The simulations also allow us to validate the functionality of the resulting CANs and integrators. For simplicity, here we focus our description on one and two-dimensional surfaces, allowing us to construct line, ring, plane, cylinder, torus, sphere, MÃ¶bius band and Klein bottle topologies and geometries (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). The procedures outlined here can be straightforwardly generalized to apply to manifolds of different dimensionality.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Stationary states and manifold topologies of the MADE CANs</title>
<p><bold>A</bold>. Desired population activity manifold topology for CANs constructed with MADE for several manifolds (from top to bottom): line, ring, plane, cylinder, torus, sphere, Mobius band and Klein bottle. <bold>B</bold>. Distance functions over the neural lattice ğ’« for selected example neurons. <bold>C</bold>. Low dimensional embedding of the neural activity manifold ğ’©. <bold>D</bold>. Betti number and persistent homology bar code for each CANâ€™s neural population states (in ğ’©). <bold>E</bold>. Left: Activity of one example neuron over ğ’© (low dimensional embedding). Right: Stationary population activity states form localized bumps on the neural lattice ğ’«.</p></caption>
<graphic xlink:href="652608v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We first construct a neural surface ğ’« that is isometric to the target state-space manifold ğ’©. For the sphere attractor, we construct ğ’« as an embedding of the two-dimensional unit sphere in â„<sup>3</sup>, and for the Klein bottle attractor ğ’« was an embedding of a finite cylinder manifold with appropriate identification of the cylinder end-points to each other in â„<sup>4</sup>). For several other manifolds (including all others from <xref rid="fig4" ref-type="fig">Fig. 4</xref>), which admit a flat metric, we define a rectangular two-dimensional space [0, <italic>L</italic><sub>1</sub>] Ã— [0, <italic>L</italic><sub>2</sub>] (<xref rid="fig4" ref-type="fig">Figure 4B</xref>) and provide an appropriate distance function on the rectangular space. For example, for the torus manifold, <italic>L</italic><sub>1</sub> = <italic>L</italic><sub>2</sub> = 2<italic>Ï€</italic>, and distances are computed respecting the periodic boundary conditions that identify 0 and 2<italic>Ï€</italic> as the same point.</p>
<p>Given ğ’«, we next approximately evenly place neurons on the surface. For manifolds with a flat metric, this involved placing neurons on an <italic>n</italic> Ã— <italic>n</italic> rectangular lattice on this space, where <italic>n</italic><sup>2</sup> is the total number of neurons. For the sphere, we spaced neurons at regular intervals along a Fibonacci spiral over the unit sphere (see Methods)to approximate an even placement on the sphere. Thus, for each neuron we define their ğ’« coordinates <italic>Î¸</italic><sub><italic>i</italic></sub>.</p>
<p>Next, we computed the connectivity of the network <italic>W</italic><sub><italic>ij</italic></sub>, which depends on the (geodesic) distances <italic>d</italic>(<italic>Î¸</italic><sub><italic>i</italic></sub>, <italic>Î¸</italic><sub><italic>j</italic></sub>) between pairs of neurons with coordinates <italic>Î¸</italic><sub><italic>i</italic></sub> and <italic>Î¸</italic><sub><italic>j</italic></sub> on ğ’«. With appropriate coordinate parametrization for the neurons, these geodesic distances can be computed via analytical expressions (for instance, as Euclidean distance with periodic boundary conditions on a torus attractor), or via a simple numerical computation (see Methods). Connectivity is then given by the <italic>n</italic><sup>2</sup> Ã— <italic>n</italic><sup>2</sup> matrix with entries <italic>W</italic><sub><italic>i</italic>,j</sub> = <italic>k</italic>(<italic>d</italic>(<italic>Î¸</italic><sub><italic>i</italic></sub>, <italic>Î¸</italic><sub><italic>j</italic></sub>)), where <italic>k</italic> is a kernel function (<xref rid="fig2" ref-type="fig">Figures 2A</xref>, <xref rid="fig3" ref-type="fig">3A</xref>, <xref rid="fig4" ref-type="fig">4B</xref>) satisfying the requirements described earlier for the formation of activity bump states (see also Appendix 1). We used a scaled Gaussian kernel such that the connectivity between pairs of neurons was strictly negative and <italic>W</italic><sub><italic>ij</italic></sub> = 0 if <italic>d</italic>(<italic>Î¸</italic><sub><italic>i</italic></sub>, <italic>Î¸</italic><sub><italic>j</italic></sub>) = 0 (see Methods). Other choices of kernels yield similar results (data not shown). Neural activity is simulated based on these weights according to <xref ref-type="disp-formula" rid="eqn2">Eq. 2</xref>. We will provide Python and Julia code that implements the MADE prescription for CANs (see Methods).</p>
</sec>
<sec id="s2e">
<title>Validation of CAN states and dynamics</title>
<p>To validate the MADE CANs, we first characterize where the states of the constructed networks localize. To do so, we sample population activity data from each model by randomly initializing each network and allowing the initial state to settle to a stationary state (see Methods). This state forms one population vector sample; we repeat the process 2500 times for each network. We apply nonlinear dimensionality reduction via ISOMAP, which has proven useful for the visualization of nonlinear low-dimensional manifolds in real data [<xref ref-type="bibr" rid="c3">3</xref>], to the resulting point cloud of stationary population activity states. The resulting structures (<xref rid="fig4" ref-type="fig">Figure 4C</xref>) visually matched the desired manifolds (<xref rid="fig4" ref-type="fig">Figure 4A</xref>): the population responses of the MADE CANs localize to low-dimensional sets of states that appear homeomorphic to ğ’©.</p>
<p>To quantify the structure of the resulting population states, we use persistent homology, a Topological Data Analysis [<xref ref-type="bibr" rid="c66">66</xref>, <xref ref-type="bibr" rid="c67">67</xref>] technique that has been applied with success in neuroscience [<xref ref-type="bibr" rid="c68">68</xref>, <xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c44">44</xref>]. Persistent homology supplies Betti numbers that characterize the topology of the set of stationary states of each network (see Methods). Betti numbers catalog the number of â€œcavitiesâ€ of each dimension present on a manifold; the first three Betti numbers correspond to the number of connected components, rings and two dimensional cavities, respectively. Betti numbers donâ€™t provide a complete or unique description of manifold structure (e.g., the ring and the cylinder share the same Betti numbers while having different dimensionality), but they provide a quantitative confirmation that the MADE CANs match their intended targets. The Betti numbers of all MADE CANs population states match those of their target manifolds (<xref rid="fig4" ref-type="fig">Figure 4 D</xref>).</p>
<p>We next visualize the instantaneous population activity states as functions on the neural lattice. The localized kernel connectivity on the manifold was expected to stabilize single activity bump states on the manifold. A stationary population activity state can be directly visualized on the neural lattice by coloring neurons according to their activity level. Indeed, we see that the stationary population states correspond to localized bumps of activation on the neural lattice ğ’« and activity manifold ğ’© (<xref rid="fig4" ref-type="fig">Figure 4E</xref>).</p>
<p>Next, we characterize the intrinsic dimensionality [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c69">69</xref>] of the stationary states of the MADE CANS. Intrinsic dimensionality at a point on a manifold is the numbers of degrees of freedom of movement along the manifold at that point. Intrinsic dimensionality would allow one to distinguish, for example, a ring (one dimensional) from a cylinder (two dimensional). Dimensionality is generally a difficult (and ill-posed) quantity to estimate in noisy data, and existing works use various methods [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c70">70</xref>, <xref ref-type="bibr" rid="c71">71</xref>, <xref ref-type="bibr" rid="c72">72</xref>, <xref ref-type="bibr" rid="c73">73</xref>]. For MADE CANs, which we can run in a noiseless setting, intrinsic manifold dimension is well-defined.</p>
<p>We adopt an approach [<xref ref-type="bibr" rid="c73">73</xref>, <xref ref-type="bibr" rid="c72">72</xref>] based on estimating the dimensionality of the tangent space to a manifold (see Methods)(<xref rid="fig5" ref-type="fig">Figure 5 A</xref>, left). The tangent space <italic>T</italic><sub><italic>s</italic></sub> ğ’© at a point <italic>s</italic> âˆˆ ğ’© is the best linear approximation of the manifold at that point and has the same dimensionality as the underlying manifold [<xref ref-type="bibr" rid="c74">74</xref>, <xref ref-type="bibr" rid="c63">63</xref>, <xref ref-type="bibr" rid="c65">65</xref>]. We consider the set <italic>S</italic> of points in a small neighborhood of <italic>s</italic> (see Methods)and apply PCA to determine the number of large principal components needed to describe the data. This gives us the dimensionality of the tangent space at <italic>s</italic> and thus the local intrinsic dimension of the manifold.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Dimensionality and attractor dynamics of the MADE CANs.</title>
<p><bold>A</bold>, Left, tangent planes approach to computing the intrinsic manifold dimension (schematic) of ğ’©. Right, estimated tangent space dimension for each manifold, which estimates the low intrinsic dimensionality of the CAN networks. <bold>B</bold> Cumulative manifold variance explained by global PCA analysis: the slow saturation of the curves shows that the linear (embedding) dimension of the manifolds can be large. <bold>C</bold> Numerical simulations to probe attractor dynamics. Inset: activity manifold, perturbation vector (black) and on-manifold (red) and off-manifold (blue) components of the perturbation. Main plot: Time-varying distance from the starting point in the off-manifold and along-manifold dimensions.</p></caption>
<graphic xlink:href="652608v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Repeating this analysis across multiple randomly selected sample points <italic>s</italic> for each MADE CAN, we confirmed that all manifolds had the expected intrinsic dimensionality given their topology: line 1 Â± 0.0 (mean Â± standard deviation, across multiple repeats), ring: 1 Â± 0.0, torus: 2 Â± 0.0, sphere: 2 Â± 0.0, MÃ¶bius band: 1.96 Â± 0.16, cylinder: 2 Â± 0.0 and plane: 2.05 Â± 0.23 (<xref rid="fig5" ref-type="fig">Figure 5 A</xref>). By contrast to the small intrinsic dimensionality of the constructed CAN manifolds, their extrinsic linear dimensionality, estimated by the minimum number of principal components required to represent the manifold as a whole, is large (<xref rid="fig5" ref-type="fig">Figure 5 B</xref>).</p>
<p>Finally, we examined whether the stationary manifolds of the MADE CANs are neutral attractor states, with rapid decay of off-manifold perturbations, together with no state drift along the manifold in the absence of noise and external inputs [<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c1">1</xref>]. First, we consider manifold stability by computing Betti numbers of the population states in networks simulated with varying noise conditions, and find that except in the most severe noise case, we recover the same Betti numbers for the noisy dynamics â€“ indirectly showing that the manifold is attractive and robust to noise (see Methods)(<xref rid="figS1" ref-type="fig">Figure S1</xref>). Second, we more directly perturb the neural population state with a randomly oriented vector of fixed magnitude (see Methods), repeating this experiment for multiple initial states and random perturbations, and observe the dynamics by which the perturbed state evolves. To quantify on- and off-manifold dynamics following perturbation, we again used PCA to estimate the manifoldâ€™s tangent space in the neighborhood of the initial state. The distance between the perturbed and initial (pre-perturbation) states along the tangent space dimension was considered the on-manifold perturbation component; the rest (along the remaining <italic>N</italic> âˆ’<italic>d</italic> dimensions) was the off-manifold perturbation(see Methods). We find very limited on-manifold drift and strong decay of the off-manifold component of the perturbation, as intended (<xref rid="fig5" ref-type="fig">Figure 5 C</xref>).</p>
</sec>
<sec id="s2f">
<title>Practical construction of integrators with MADE</title>
<p>To generate the QANs that combine to create neural integrator circuits, we slightly modify the connectivity structure of MADE CANs. We start with the same procedure as before to construct ğ’« and compute the distance function <italic>d</italic>. For a QAN indexed by <italic>Ïƒ, m</italic> we simply apply a shift <inline-formula><inline-graphic xlink:href="652608v1_inline19.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to the <italic>i</italic><sup>th</sup> neurons coordinates before computing <italic>d</italic> such that <inline-formula><inline-graphic xlink:href="652608v1_inline20.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. For some manifolds with a flat metric (e.g. plane, torus) <inline-formula><inline-graphic xlink:href="652608v1_inline21.gif" mimetype="image" mime-subtype="gif"/></inline-formula> was identical for all points <italic>Î¸</italic> âˆˆ ğ’« and was taken to be a vector of magnitude |<italic>Î´</italic>| oriented along the <italic>m</italic><sup>th</sup> direction on ğ’«. In others (e.g. the sphere), the offset vector varied as a function of position along the manifold. For each dimension <italic>m</italic> we defined a Killing Vector field Î¨<sub>Â±m</sub> and evaluated it at <italic>Î¸</italic><sub><italic>i</italic></sub> to obtain the offset vector (see Methods). Given an external velocity signal for a trajectory on â„³, we use the map <italic>Ï€</italic> from â„³ to ğ’© to obtain the inputs to each QAN. Network activity is simulated based on weights and these inputs according to <xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref>. We will provide Python and Julia code to implement the MADE prescription for neural integrators (see Methods).</p>
</sec>
<sec id="s2g">
<title>Validation of MADE integrators</title>
<p>To examine the performance of each MADE integrator in representing and tracking time-varying external variables, we provide the circuit with the velocity of a simulated random trajectories of the variable <italic>x</italic>(<italic>t</italic>) âˆˆ â„³ and track how the networkâ€™s internal state changes. We first consider how the firing of a single cell varies with the external variable, by plotting its tuning curve or firing response as a function of the external variable, estimated over a long velocity trjectory (<xref rid="fig6" ref-type="fig">Figure 6A</xref>). The existence of a localized activity bump (<xref rid="fig6" ref-type="fig">Figure 6A</xref>, top three panels) means that the circuit has correctly inferred external position: the cell fires at a specific position and not other random positions, and the network has transferred the internal bump activity pattern into a corresponding pattern as a function of location on the external manifold â„³. In cases where the external manifold â„³ is not isomorphic to the internal manifold ğ’©, such as when a plane in â„³ is represented by a cylinder or a torus in ğ’©, a continued linear trajectory along one direction in â„³ corresponds to a periodic traversal on ğ’©, and thus one would expect repeating bumps in the tuning curve along that dimension, as we find (<xref rid="fig6" ref-type="fig">Figure 6A</xref>, panels 4-5). Note that based on the details of how we periodically connected the boundaries of our rectangular neural lattice to obtain a torus, we would obtain a square grid tuning curve (as shown) or a triangular grid tuning curve (as previously described for grid cells in [<xref ref-type="bibr" rid="c48">48</xref>, <xref ref-type="bibr" rid="c56">56</xref>]). Finally, the tuning curves for the sphere and Mobius strip are single bumps, as expected (<xref rid="fig6" ref-type="fig">Figure 6A</xref>, last two panels).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Numerical simulations of path integration performance with MADE path integrators.</title>
<p><bold>A</bold>. Tuning curves of single example neurons as a function of the external (latent) variable x. Insets show the manifold topologies of the external variable (red) and neural population states (blue): these pairings might be of identical manifolds, or e.g. a 2D Euclidean manifold in x could be mapped to a cylinder or torus, etc. in the neural population states. <bold>B</bold>. Example input trajectory (red) and decoded trajectory from the neural population response (blue). <bold>C</bold>. Decoding error across multiple simulations for various external-neural manifold pairs. Decoding error is shown as percentage of trajectory length over â„³. Colored boxes show the interquartile range, white lines the mean, circles outliers and vertical lines the 95<sup><italic>th</italic></sup> percentile confidence interval. <bold>D</bold>. Same as <bold>B</bold> but for torus attractors with varying amounts of noise. <bold>E</bold>. Left: Killing and non-Killing weight offsets for the torus (top) and sphere (bottom). Right: Same as <bold>C</bold> for integrators correctly constructed with Killing weight offsets, and with the non-Killing weight offsets from the left. <bold>F</bold>. Same as <bold>C</bold> for MÃ¶bius to MÃ¶bius (left) and cylinder to MÃ¶bius mappings (right).</p></caption>
<graphic xlink:href="652608v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We can more directly quantify how closely the network tracks the external variable <italic>x</italic>(<italic>t</italic>) by decoding it from the networkâ€™s internal state <bold>s</bold>(<italic>t</italic>), as <inline-formula><inline-graphic xlink:href="652608v1_inline22.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where <italic>Ï‰</italic> is an offset used to account for the fact that, in some cases, ğ’© was periodic while â„³ was not (e.g. torus and plane, respectively) (see Methods). When â„³ and ğ’© are chosen such that <italic>Ï€</italic> is either an identity map or a periodic mapping, the networks show very accurate integration over periods of several seconds of simulated activity (<xref rid="fig6" ref-type="fig">Figure 6B,C</xref>). Decoding error remains low even in the presence of moderate noise (<xref rid="fig6" ref-type="fig">Figure 6D</xref>) (see Methods). Thus, MADE networks support accurate integration, even in non-trivial scenarios such as the cylinder-torus manifold pairing and even on the MÃ¶bius band manifold, which have not been described previously.</p>
<p>We performed additional experiments on circuits requiring Killing vector fields to integrate. To show the necessity of Killing fields, we built torus (ğ’© = torus, â„³ = plane) and sphere (â„³ = ğ’© = sphere) integrator networks, but varied the QAN weight offsets relative to the Killing field prescription. For the torus, we varied the orientation of the offset vectors, while for the sphere we changed their lengths to be of constant magnitude everywhere (except at two poles, where the magnitudes were left at 0), (see <xref rid="fig6" ref-type="fig">Figure 6E</xref>, left), (see Methods). The constant-magnitude non-Killing field on the sphere may be considered a direct extension of the constant offset vector fields used for flat manifolds and used in all prior work in the construction of neural integrators. In both cases, we observed a dramatic deterioration in integration accuracy, <xref rid="fig6" ref-type="fig">Figure6E</xref> (right). The result underscores the importance of Killing fields for integration on manifolds with a non-flat metric.</p>
<p>Finally, we considered integrating velocities from a cylindrical external variable on a network with MÃ¶bius band topology. Both manifolds are two-dimensional with one periodic and one non-periodic dimension. However, while a rectangle is glued without a twist to make a cylinder (which has two surfaces, inner and outer), it is glued with a twist to make a MÃ¶bius band (which has a single surface) with the consequence that there is no simple continuous mapping between the two. Proceeding naively by simply mapping the two manifolds onto each other by ignoring the flipped boundary of the MÃ¶bius band, it is unsurprising that integration is significantly less accurate (<xref rid="fig6" ref-type="fig">Figure6F</xref>). In future work, it will be interesting to consider which pairings of external to neural manifolds will provably permit accurate path integration.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<sec>
<title>Summary</title>
<p>Here, we have presented MADE, a mathematical theory and recipe for constructing biologically plausible neural CANs and integrator networks of desired topologies and geometries, with single- or multi-bump tuning curves. The mathematical theory unifies existing biologically plausible continuous attractor and integrator models involving bump-like activation functions, which emerge as specific cases of the MADE theory.</p>
<p>The theory provides a first-principles derivation showing that multiple copies of a basic network must be coupled together for integration with biological constraints, in part to relieve demands for rapid synaptic modulation and in part to remove the need for velocity estimating regions from knowing the full nonlinear structure and current state of the integrator network. It also predicts that manifolds without a flat metric will require an overcomplete set of network pairs in the form of QAN networks, relative to the intrinsic dimensionality of the manifold: thus, integration on a two-dimensional spherical surface requires more than 2 QAN pairs.</p>
<p>We envision MADE to be useful to distinct fields: for deep-learning models that might require accurate low-dimensional neural network attractors and integrators, and for neuroscience, where MADE provides de novo models and novel circuit-level mechanistic predictions for the structure of other possible integrators in brain that may be uncovered in the future.</p>
<p>Indeed, given recent discoveries that path-integrating neural circuits generalizably represent multiple cognitive variables, it is likely that such circuits are used by the brain to perform cognitive tasks in which variables of interest are not directly observed and only information about their rate of changes is available (e.g., mental object rotation) [<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c75">75</xref>, <xref ref-type="bibr" rid="c27">27</xref>]. MADE models could then act as test beds to generate mechanistic hypotheses for the network dynamics underpinning integration computation in such cognitive tasks.</p>
</sec>
<sec>
<title>Activity bumps and tuning curves</title>
<p>MADE provides a basic prescription for the construction of continuous attractor and integrator networks of a desired dimension and topology. We numerically implemented a particular (Gaussian) kernel shape to illustrate the framework. The shape of the population activity bumps that result will depend on the kernel shape, which can be varied and selected as desired, according to the constraints supplied by our theory. Recent theoretical work on symmetry breaking for pattern formation also suggests that the set of potential kernels forms a large function space.</p>
<p>The tuning curve shapes of single cells depends both on the population activity bump shape as well as on the mapping from the external variable manifold to the internal neural state space manifold. As we have seen, if the external manifold is unbounded in some dimension but the internal representation is compact and periodic, then the spatial tuning curve will be periodic in that dimension. More subtle details of the mapping can affect the geometry of the periodic mapping, as we have described above.</p>
<p>We have focused our illustrations on simple and non-trivial manifolds of intrinsic dimension â‰¤ 2 for visualization and convenience. However, the theory and recipe for continuous attractor and integrator network construction generalizes in a straightforward manner to manifolds of higher dimension and different topologies.</p>
</sec>
<sec>
<title>Related work</title>
<p>Computational models first described attractor networks [<xref ref-type="bibr" rid="c76">76</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c11">11</xref>] and the mechanisms by which they could enable velocity integration [<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c47">47</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c46">46</xref>, <xref ref-type="bibr" rid="c48">48</xref>, <xref ref-type="bibr" rid="c18">18</xref>] long before experimental data verified the existence of such mechanisms. Intriguingly and surprisingly, in every case experimentally probed to date, the proposed neural circuit models closely resemble the hand-designed attractor models [<xref ref-type="bibr" rid="c40">40</xref>, <xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c42">42</xref>, <xref ref-type="bibr" rid="c43">43</xref>, <xref ref-type="bibr" rid="c44">44</xref>, <xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c13">13</xref>]. Why is this the case? Presumably this match arises because the models were minimal in the sense that they implemented the essential elements and symmetries required to form the desired attractor, and circuits in the brain evolving under efficiency pressures arrived at similarly minimal models. MADE adopts a very similar mathematically minimal approach, recovering all of the known integrator models with bump-like tuning (except for the oculomotor integrator, which does not have bump-like responses).</p>
<p>An alternative approach to building models of integrating circuits in brains is to train artificial neural networks to perform tasks requiring integration [<xref ref-type="bibr" rid="c50">50</xref>, <xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c51">51</xref>]. After training, the networksâ€™ solution is analyzed to reverse engineer the relation between network connectivity, neural dynamics and task performance [<xref ref-type="bibr" rid="c50">50</xref>, <xref ref-type="bibr" rid="c77">77</xref>, <xref ref-type="bibr" rid="c78">78</xref>, <xref ref-type="bibr" rid="c79">79</xref>]. However, such approaches often fail to provide novel testable predictions or interpretable mechanisms to guide further experimental investigations, unless there was already a hand-crafted model available to which the trained network could be compared.</p>
<p>The network engineering approach [<xref ref-type="bibr" rid="c80">80</xref>, <xref ref-type="bibr" rid="c81">81</xref>, <xref ref-type="bibr" rid="c82">82</xref>, <xref ref-type="bibr" rid="c83">83</xref>, <xref ref-type="bibr" rid="c84">84</xref>, <xref ref-type="bibr" rid="c82">82</xref>, <xref ref-type="bibr" rid="c85">85</xref>, <xref ref-type="bibr" rid="c86">86</xref>, <xref ref-type="bibr" rid="c87">87</xref>] constructs circuits starting from the detailed desired dynamics of a system (precise states, fixed points, or specific tuning curves), then directly searching or solving for some network connectivity with those dynamics. Typically, these works further constrain the problem to make it well-posed by searching for low-rank weights or the lowest-dimensional embedding space for the dynamics while satisfying the desired properties. These methods are complementary to our approach: they permit construction of a broader set of dynamical systems, for instance trajectories ending in discrete fixed points, stable and unstable fixed points, etc., while our focus is specifically on biologically plausible continuous attractors that integrate. Conversely, those approaches do not provide a framework for building biologically realistic continuous attractor networks that integrate and lack known matches or easy interpretability to compare with biological circuits in known cases.</p>
<p>In conclusion, MADE allows for easy generation of interpretable, mechanistic, models of CAN networks that can integrate. We hope that MADE will endow researches with tools required to generate detailed, testable, hypotheses about the neural underpinnings of integration in diverse settings and in various cognitive processes, accelerating our understanding of the critical role that this class of computations play in many aspects of brain function and allowing for easy incorporation of such circuits in deep learning applications.</p>
</sec>
</sec>
<sec id="s4">
<title>Methods</title>
<p>All simulations and figures were implemented in custom Julia code available at GeneralAttractorsTheory. We will provide a minimal Python package for creating CANs and QANs using MADE: MADE-Python.</p>
<sec id="s4a">
<title>CAN construction</title>
<p>In MADE, CAN engineering depends on computations of the pair-wise on-manifold distances between neurons in a lattice ğ’«. Thus, we begin by specifying a set of <italic>n</italic> equally spaced points on ğ’«. For the Line attractor, <italic>n</italic> = 256 and ğ’« was taken to be the interval [âˆ’6, 6]. For the Ring attractor, <italic>n</italic> = 256 and ğ’« was taken to be the interval [0, 2<italic>Ï€</italic>] with the two ends identified (i.e. we ensured not to have a neuron at <italic>Î¸</italic><sub><italic>i</italic></sub> = 0 and one at <italic>Î¸</italic><sub><italic>i</italic></sub> = 2<italic>Ï€</italic>). For all remaining networks, <italic>n</italic> = 48<sup>2</sup> was used. The following rectangular intervals were used: for the plane attractor ğ’« = [âˆ’10, <xref ref-type="bibr" rid="c10">10</xref>] Ã— [âˆ’10, 10], cylinder: ğ’« = [âˆ’5, 5] Ã— [0, 2<italic>Ï€</italic>], torus: ğ’«[0, 2<italic>Ï€</italic>] Ã— [0, 2<italic>Ï€</italic>], MÃ¶bius band: ğ’« = [âˆ’2, 2] Ã— [0, 2<italic>Ï€</italic>] and Klein Bottle: ğ’«[0, 2<italic>Ï€</italic>] Ã— [0, 2<italic>Ï€</italic>]. For the sphere attractor, the <italic>n</italic> points were chosen to be on a Fibonacci spiral on the unit sphere embedded in â„<sup>3</sup>.</p>
<p>Next, to implement custom manifold-specific distance metrics <italic>d</italic> we used the Julia package Distances.jl. The standard Euclidean metric was used for the line and plane attractor, for the ring a one dimensional periodic Euclidean metric (period 2<italic>Ï€</italic>) was used, for the torus a two dimensional periodic Euclidean metric (period 2<italic>Ï€</italic> in each direction) and for the Cylinder a heterogeneous periodic and standard Euclidean metric for the periodic and non-periodic dimensions respectively. For the sphere the great arc spherical distance for points on the unit sphere (implemented in the Manifolds.jl package [<xref ref-type="bibr" rid="c88">88</xref>]) was used. For the MÃ¶bius band a custom metric function was used to account for the non-orientable nature of the manifold.</p>
<p>For the Klein Bottle, a different approach was used. First, we defined an embedding of the Klein Bottle in â„<sup>4</sup> mapping each lattice point <italic>Î¸</italic> = (<italic>u, v</italic>) to a point <italic>q</italic> âˆˆ â„<sup>4</sup>:
<disp-formula id="ueqn2">
<graphic xlink:href="652608v1_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Next, we computed the pairwise Euclidean Distance in â„<sup>4</sup> for the embedded points and selected the 8 nearest neighbors of each point. We then constructed a graph where each node was a lattice point and two nodes were connected if one belong to the neighborhood of the other. Each edge was assigned a weight equal to the Euclidean distance between the two points. Thus, the graph structure was taken to represent the local topological structure (connectivity) of the Klein Bottle. Given two points <italic>Î¸</italic><sub><italic>i</italic></sub>, <italic>Î¸</italic><sub><italic>j</italic></sub> then, their on-manifold distance was given by summing the edge weights (local distances) along the shortest path on the graph from the node corresponding to <italic>Î¸</italic><sub><italic>i</italic></sub> to the one corresponding to <italic>Î¸</italic><sub><italic>j</italic></sub> as a way to numerically approximate the geodesic distance between them.</p>
<p>Following computation of pairwise distances, the connection weights between two neurons was computed using as kernel function:
<disp-formula id="ueqn3">
<graphic xlink:href="652608v1_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
yielding strictly non-positive values for the connection strength. This gave a connectivity pattern characterized by global, long-distance inhibition, and no, or reduced, inhibition locally such that a localized pattern of activation on the neural lattice ğ’« would remain localized and not result in activation of all neurons in the network. The parameters <italic>Î±, Ïƒ</italic> were varied based on the CAN topology and are indicated in <xref rid="tbl1" ref-type="table">table 1</xref></p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>Kernel function parameters</title></caption>
<graphic xlink:href="652608v1_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s4b">
<title>CAN simulation</title>
<p>Network dynamics were approximate to discrete time using forward Euler integration with Î”<italic>t</italic> = 0.5ms using:
<disp-formula id="ueqn4">
<graphic xlink:href="652608v1_ueqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <bold>s</bold>(<italic>t</italic>) is a vector representing the activity of each neuron in the network at time <italic>t, Ï„</italic> = 5<italic>ms</italic> was used as time constant. The constant input <italic>b</italic> = 0.5 was used throughout. The term <italic>Î·</italic>(<italic>t</italic>) was used to simulate Poisson noise in the recurrent dynamics, it represents a vector of length <italic>n</italic> whose entries are given by: <inline-formula><inline-graphic xlink:href="652608v1_inline23.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where <italic>Ïƒ</italic><sub>no<italic>i</italic>se</sub> âˆˆ {0, 1.5, 3, 5}. Unless explicitly stated, <italic>Ïƒ</italic><sub>no<italic>i</italic>se</sub> = 0 was used.</p>
<p>For each CAN, 2500 simulations of 25ms in duration were performed to generate data for the analysis of the activity manifold topology. We chose 25ms since we observed this to be sufficient for the network to settle into a steady state (i.e. one in which the networkâ€™s activity does not change between simulation steps).</p>
<p>For the first 15ms of each simulation, the activity of neurons at a distance <italic>d &gt;</italic> 0.5 from a selected neuron <italic>Î¸</italic><sub>0</sub> (randomly selected for each simulation) was artificially set to 0 to induce the formation of a stable bump of activity around <italic>Î¸</italic><sub>0</sub> to promote uniform coverage of the entire manifold. The final activation vector <bold>s</bold>(<italic>T</italic>) for each simulation was then stored for subsequent analysis. For the torus attractor network, additional simulations were performed varying the noise parameter to assess the effect of noise on the attractor dynamics.</p>
</sec>
<sec id="s4c">
<title>Attractor manifold analysis</title>
<p>The final activation vector of each of 2500 CAN simulations for each manifold were collected into a matrix of dimensionality <italic>n</italic> Ã— 2500 with <italic>n</italic> being the number of neurons in the network. For networks other than the line and ring attractors in which <italic>n &gt;</italic> 400 a first dimensionality reduction step using PCA was performed to reduce the data to a 400 Ã— 2500 dimensional matrix. Then, further reduction to three dimensional data for visualization (<xref rid="fig4" ref-type="fig">Figure 4</xref>) was achieved using Isomap [<xref ref-type="bibr" rid="c89">89</xref>]. To reduce computation Isomap was fitted to 10% randomly selected data points and then used to embed the entire dataset for visualization. For subsequent Topological Data Analysis (TDA) point cloud data was subjected to PCA dimensionality reduction to generate a 200 Ã— 2500 data matrix and Isomap was then used to further reduce dimensionality to 10 (Isomap fitted to 10% of the data) [<xref ref-type="bibr" rid="c3">3</xref>].</p>
</sec>
<sec id="s4d">
<title>Topological data analysis</title>
<p>To perform persistent homology analysis the Julia packages Ripserer.jl and PersistenceDiagrams.jl [<xref ref-type="bibr" rid="c90">90</xref>] were used. To reduce computation, the TDA filtration was computed using a subset of randomly selected data points (20% of the entire dataset) to obtain the persistence diagrams shown in <xref rid="fig4" ref-type="fig">Figure 4 A</xref>. Only intervals with a lifespan <italic>&gt;</italic> 7 were kept to remove features due to noise and the number of persistent intervals of each dimension (up to two dimensional cavities) were counted to obtain Betti numbers, which were then compared with those expected for manifolds of the given topology.</p>
</sec>
<sec id="s4e">
<title>Visualizing neural tuning curves</title>
<p>To visualize neural activation turning curves over ğ’© in <xref rid="fig4" ref-type="fig">Figure 4</xref>, we used PCA and ISOMAP to reduce the dimensionality of neural activity to three dimensions. We thus obtained 2500 low dimensional points which we colored according to the activity of one selected neuron in the corresponding neural state. To visualize activity over the neural lattice ğ’«, we started by selecting one random neural state from the 2500 simulations. Then, we uniformly sampled ğ’« and for each location <italic>Î¸</italic><sub><italic>i</italic></sub> âˆˆ ğ’« we identified the closest neuron in the CAN (by coordinates). We then colored each point in ğ’« according to the activation of the closest neuron.</p>
</sec>
<sec id="s4f">
<title>Intrinsic manifold dimensionality analysis</title>
<p>To estimate the manifoldâ€™s intrinsic dimensionality all data points in the <italic>n</italic>-dimensional state space were utilized. Pairwise Euclidean distance between each data point was computed to obtain each data pointâ€™s <italic>k</italic> nearest neighbors (using the NearestNeighbors.jl package). While Euclidean distance in state space does not necessary match on-manifold geodesic distance on ğ’© in general, on a sufficiently small scale a manifoldâ€™s Euclidean structure makes this approximation acceptable. Next, 250 random data points (10% of the total) were selected for estimation of local dimensionality in their neighborhood. For each, the <italic>k</italic> closest points were selected and PCA fitted to the data. The number <italic>d</italic> of principal components required to explain at least 75% of the data was used as estimate of local dimensionality and the manifoldâ€™s intrinsic dimensionality was taken to be the average across repeats. Thus, the dimensionality estimation procedure depended on two hyperparameters: <italic>k</italic> and the percentage of variance explained. Preliminary tests on artificially generated data with known dimensionality and variable Gaussian noise were used to select the parameters used here, and weâ€™ve found the estimated intrinsic dimensionality to be robust across a wide range of parameters values (data not shown). For the analyses shown here we used <italic>k</italic> = 500 throughout. Our preliminary tests showed that much smaller values of <italic>k</italic> resulted in noisy estimates (especially in the face of noise) and very large values of <italic>k</italic> led to an overestimation of the manifold intrinsic dimensionality (likely due to the higher global embedding dimensionality).</p>
</sec>
<sec id="s4g">
<title>Attractor dynamics analysis</title>
<p>To explicitly quantify attractor dynamics, a torus network was constructed as described above and simulated without external stimuli for a simulation time of 250ms (given 100 random initializations). Next, the networkâ€™s state was perturbed by addition with a random vector <italic>v</italic> of the same dimensionality as the network activity. For each of 100 simulations the vector was chosen to have a random orientation but fixed magnitude. The magnitude was computed to be 50% of the average distance between states on the torus manifold and the origin. Following the stimulus, the simulation was continued for 750ms more for a total simulation time of 1000ms. Data for each simulation was collected for the analysis of on-vs off-manifold dynamics. For each repeat the state just prior to stimulus application was used as seed for local PCA using k-nearest points from the point cloud data used for previous estimation of manifold topology, as described above (i.e. the steady states from previous simulations without the inputs were used to estimate the tangent plane to the manifold). The top two principal components were retained as approximation of the manifoldâ€™s tangent plane. The neural trajectory was then decomposed into on-manifold and off-manifold components by projection onto the tangent plane and remaining <italic>N</italic> âˆ’ 2 dimensions. The euclidean distance in each subspace from the initial condition over time was then computed to asses drift following stimulus application and averaged across repeats.</p>
</sec>
<sec id="s4h">
<title>QAN construction</title>
<p>To construct quasi-periodic attractor networks for integration, first a choice of variable (â„³) and neural lattice (ğ’«) manifolds was made, ensuring that an identity or periodic mapping existed between the two (unless explicitly stated otherwise). Next, a map <italic>Ï€</italic> : â„³ â†’ ğ’« and its inverse <italic>Ï€</italic><sup>âˆ’1</sup> were defined (e.g. mapping each point on the plane, ğ’«, to a corresponding point on the torus, ğ’©). To compute connection weights in each QAN, points on ğ’« were selected as before and the same distance metrics and kernel functions were used applying an offset to the neuronsâ€™ coordinates during distance computation. For the Line and Ring attractor, two QANs were constructed using the offset vectors <italic>Î´</italic><sub>Â±m</sub> = Â±0.15. For the Plane, Cylinder, Torus, MÃ¶bius attractors four QANs were constructed using <italic>Î´</italic><sub>Â±1</sub> = [Â±0.25, 0] and <italic>Î´</italic><sub>Â±1</sub> = [0, Â±0.25] as offset vectors. For the sphere attractor, six QANs were constructed using as offset vectors <italic>Î´</italic><sub>1Â±</sub> = Â±[0, âˆ’<italic>z, y</italic>], <italic>Î´</italic><sub>2Â±</sub> = Â±[<italic>z</italic>, 0, âˆ’<italic>x</italic>] and <italic>Î´</italic><sub>3Â±</sub> = Â±[âˆ’<italic>y, x</italic>, 0] where [<italic>x, y, z</italic>] represents coordinates on the unit sphere embedded in three dimensional euclidean space. For the sphere, therefore, the offset vector magnitude varied as a function of position on the sphere to ensure that Killing vector fields were used (which are constant for the other manifolds used). The same vectors were used to compute the velocity-dependent stimulus <inline-formula><inline-graphic xlink:href="652608v1_inline24.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to each QAN. For some simulations, non-Killing vector fields where used. In the Torus, the offset vectors<inline-formula><inline-graphic xlink:href="652608v1_inline25.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>Î¸</italic> = [<italic>x, y</italic>], and <inline-formula><inline-graphic xlink:href="652608v1_inline26.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where used (where <italic>R</italic> is the rotation matrix <italic>R</italic> = [[0, <xref ref-type="bibr" rid="c1">1</xref>], [âˆ’1, 0]]. For the sphere, the same vectors as above were used, except they were normalized to be of unit length everywhere on the sphere (except where they vanished).</p>
</sec>
<sec id="s4i">
<title>QAN dynamics simulation</title>
<p>Similarly to CANs, network dynamics were simulated using forward Euler integration. For each QAN in a network performing integration the discrete time dynamics were:
<disp-formula id="ueqn5">
<graphic xlink:href="652608v1_ueqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <bold>s</bold>(<italic>t</italic>) =âˆ‘<sub>Â±m</sub> <bold>s</bold><sub>Â±m</sub> and <inline-formula><inline-graphic xlink:href="652608v1_inline27.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where <italic>J</italic> is the Jacobian of the map <italic>Ï€</italic> evaluated at a point <inline-formula><inline-graphic xlink:href="652608v1_inline28.gif" mimetype="image" mime-subtype="gif"/></inline-formula> decoded from the neural state <bold>s</bold> and <italic>Î´</italic><sub>Â±m</sub> is the offset vector at a point <italic>Î¸</italic> âˆˆ ğ’« corresponding to the location of the neuron with highest activation in the network. The velocity input <inline-formula><inline-graphic xlink:href="652608v1_inline29.gif" mimetype="image" mime-subtype="gif"/></inline-formula> was computed by simulating the random walk of a point particle in the variable manifold â„³.</p>
<p>To assess integration accuracy we generated 50 random trajectory (each corresponding to 1 second of simulated time) and simulated integration with the QANs. For each simulation, a trajectory <inline-formula><inline-graphic xlink:href="652608v1_inline30.gif" mimetype="image" mime-subtype="gif"/></inline-formula> was decoded from neural activity and compared to the input trajectory <italic>Î³</italic> âˆˆ â„³. The simulation error was computed as a fraction of the trajectory length and was given by:
<disp-formula id="ueqn6">
<graphic xlink:href="652608v1_ueqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>d</italic> is a metric function for â„³ as described above and <italic>L</italic><sub>Î³</sub> the trajectory length of <italic>Î³</italic> on â„³. More precisely, for decoding we used <inline-formula><inline-graphic xlink:href="652608v1_inline31.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where<inline-formula><inline-graphic xlink:href="652608v1_inline32.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Here <italic>Ï‰</italic> is a correction factor used only when â„³ and ğ’© had different topologies such that â„³ had non-periodic dimensions (plane) and ğ’© had periodic dimensions (torus). At each decoding step, we added or subtracted 2<italic>Ï€</italic> to <italic>Ï‰</italic> when necessary to account for the neural state â€œwrapping aroundâ€ the boundary dimension(s). Here <italic>Ï‰</italic> is a <italic>d</italic>-dimensional vector and each value is set to 0 for non-periodic dimensions in ğ’© and is <italic>k</italic>2<italic>Ï€</italic> for some integer <italic>k</italic> otherwise.</p>
<p>To generate the trajectories, we first defined a set of 2<italic>d</italic> vector fields Î¦<sub><italic>i</italic></sub> over â„³, each corresponding to a vector field Î¨<sub><italic>i</italic></sub> over ğ’«. Then, we generated smoothly varying vectors <italic>A</italic><sup><italic>i</italic></sup> such that at each time <italic>t</italic> the velocity vector <inline-formula><inline-graphic xlink:href="652608v1_inline33.gif" mimetype="image" mime-subtype="gif"/></inline-formula> was given by <inline-formula><inline-graphic xlink:href="652608v1_inline34.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. These weights vectors were given by the sum of two sine waves with random periods and scaled to have amplitude <italic>&lt;</italic> 0.1. We then computed <bold>X</bold>, the trajectory over â„³ by, at each time step, computing <inline-formula><inline-graphic xlink:href="652608v1_inline35.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="652608v1_inline36.gif" mimetype="image" mime-subtype="gif"/></inline-formula>(with <italic>dt</italic> the simulation time step, 0.5). Finally, we computed <bold>V</bold> the set of inputs to the QANs. For each time point <italic>t</italic>, the input <italic>v</italic><sub><italic>j</italic></sub> to the <italic>j</italic><sup>th</sup> QAN was given by <inline-formula><inline-graphic xlink:href="652608v1_inline37.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where <italic>Ï€</italic><sub>â‹†</sub> was the push forward of the map <italic>Ï€</italic> : â„³ â†’ ğ’©.</p>
<p>In some conditions we artificially injected Poisson noise in the QAN neural dynamics as described previously to assess the effect of noise on path integration.</p>
</sec>
<sec id="s4j">
<title>Neural tuning curves on â„³</title>
<p>To visualize neural tuning curves with respect to â„³ in <xref rid="fig6" ref-type="fig">Figure 6</xref>, we generated a single trajectory densely sampling from â„³ (5-10 seconds of simulated time). After simulating path integration, we selected one random neuron to visualize its tuning curve. The visualization method varied based on the manifold topology. For one dimensional manifolds we simply plotted decoded value, <italic>x</italic>, against the neuronâ€™s activity. For most two dimensional manifolds, with the exception of the sphere, we generated a heatmap by binning <italic>x</italic> and quantifying the average neuronâ€™s activity for samples from each bin. A small amount of noise was added to <italic>x</italic> before binning to improve visualization. For the sphere, we first sampled 2000 points uniformly distributed on â„³. Then, for each point we looked at the closest decoded value. We then colored each point on the sphere according to the neural activity value at the corresponding sample.</p>
</sec>
<sec id="s4k">
<title>Non Killing fields and non-periodic manifold mapping</title>
<p>To demonstrated that path integration depended on the weight offset vector fields Î¨ being Killing fields we generated two variants of the torus and sphere QANs. For the torus, we kept the magnitude and relative orientation of the offset vector fields constant, but gradually rotated their position by an angle <italic>cos</italic>(<italic>Î¸</italic><sub>1</sub>) (i.e. only as a function of position along one manifold dimension). This ensured that vector fields at the boundary conditions were identical, as expected. For the sphere, we started with the Killing vector fields we had, and simply normalized each vector such that all vectors had constant length. We then ran 50 simulations using random trajectories as described previously.</p>
<p>To assess path integration when no trivial or periodic mapping between â„³ and ğ’© existed, we performed path integration simulations with â„³ as a cylinder and ğ’© as a sphere. We used the same procedure described above to generate 50 random trajectories over the cylinder and computing the corresponding velocity vectors over ğ’«.</p>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mikail</given-names> <surname>Khona</surname></string-name> and <string-name><given-names>Ila R</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Attractor and integrator networks in the brain</article-title>. <source>Nat. Rev. Neurosci</source>., <volume>23</volume>(<issue>12</issue>):<fpage>744</fpage>â€“<lpage>766</lpage>, <month>December</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>James J</given-names> <surname>Knierim</surname></string-name> and <string-name><given-names>Kechen</given-names> <surname>Zhang</surname></string-name></person-group>. <article-title>Attractor dynamics of spatially correlated neural activity in the limbic system</article-title>. <source>Annu. Rev. Neurosci</source>., <volume>35</volume>:<fpage>267</fpage>â€“<lpage>285</lpage>, <month>March</month> <year>2012</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Rishidev</given-names> <surname>Chaudhuri</surname></string-name>, <string-name><given-names>Berk</given-names> <surname>GerÃ§ek</surname></string-name>, <string-name><given-names>Biraj</given-names> <surname>Pandey</surname></string-name>, <string-name><given-names>Adrien</given-names> <surname>Peyrache</surname></string-name>, and <string-name><given-names>Ila</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>The intrinsic attractor manifold and population dynamics of a canonical cognitive circuit across waking and sleep</article-title>. <source>Nat. Neurosci</source>., <volume>22</volume>(<issue>9</issue>):<fpage>1512</fpage>â€“<lpage>1520</lpage>, <month>September</month> <year>2019</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>JS</given-names> <surname>Taube</surname></string-name>, <string-name><given-names>RU</given-names> <surname>Muller</surname></string-name>, and <string-name><given-names>JB</given-names> <surname>Ranck</surname>, <suffix>Jr.</suffix></string-name></person-group> <article-title>Head-direction cells recorded from the postsubiculum in freely moving rats. i. description and quantitative analysis</article-title>. <source>J. Neurosci</source>., <volume>10</volume>(<issue>2</issue>):<fpage>420</fpage>â€“<lpage>435</lpage>, <month>February</month> <year>1990</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jeffrey S</given-names> <surname>Taube</surname></string-name></person-group>. <article-title>The head direction signal: origins and sensory-motor integration</article-title>. <source>Annu. Rev. Neurosci</source>., <volume>30</volume>:<fpage>181</fpage>â€“<lpage>207</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Luigi</given-names> <surname>Petrucco</surname></string-name>, <string-name><given-names>Hagar</given-names> <surname>Lavian</surname></string-name>, <string-name><given-names>You Kure</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>Fabian</given-names> <surname>Svara</surname></string-name>, <string-name><given-names>Vilim</given-names> <surname>Å tih</surname></string-name>, and <string-name><given-names>Ruben</given-names> <surname>Portugues</surname></string-name></person-group>. <article-title>Neural dynamics and architecture of the heading direction circuit in zebrafish</article-title>. <source>Nat. Neurosci</source>., <volume>26</volume>(<issue>5</issue>):<fpage>765</fpage>â€“<lpage>773</lpage>, <month>May</month> <year>2023</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Daniel B</given-names> <surname>Turner-Evans</surname></string-name>, <string-name><given-names>Kristopher T</given-names> <surname>Jensen</surname></string-name>, <string-name><given-names>Saba</given-names> <surname>Ali</surname></string-name>, <string-name><given-names>Tyler</given-names> <surname>Paterson</surname></string-name>, <string-name><given-names>Arlo</given-names> <surname>Sheridan</surname></string-name>, <string-name><given-names>Robert P</given-names> <surname>Ray</surname></string-name>, <string-name><given-names>Tanya</given-names> <surname>Wolff</surname></string-name>, <string-name><given-names>J Scott</given-names> <surname>Lauritzen</surname></string-name>, <string-name><given-names>Gerald M</given-names> <surname>Rubin</surname></string-name>, <string-name><given-names>Davi D</given-names> <surname>Bock</surname></string-name>, and <string-name><given-names>Vivek</given-names> <surname>Jayaraman</surname></string-name></person-group>. <article-title>The neuroanatomical ultrastructure and function of a biological ring attractor</article-title>. <source>Neuron</source>, <volume>108</volume>(<issue>1</issue>):<fpage>145</fpage>â€“<lpage>163.e10,</lpage> <month>October</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Brad K</given-names> <surname>Hulse</surname></string-name>, <string-name><given-names>Hannah</given-names> <surname>Haberkern</surname></string-name>, <string-name><given-names>Romain</given-names> <surname>Franconville</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Turner-Evans</surname></string-name>, <string-name><given-names>Shin-Ya</given-names> <surname>Takemura</surname></string-name>, <string-name><given-names>Tanya</given-names> <surname>Wolff</surname></string-name>, <string-name><given-names>Marcella</given-names> <surname>Noorman</surname></string-name>, <string-name><given-names>Marisa</given-names> <surname>Dreher</surname></string-name>, <string-name><given-names>Chuntao</given-names> <surname>Dan</surname></string-name>, <string-name><given-names>Ruchi</given-names> <surname>Parekh</surname></string-name>, <string-name><given-names>Ann M</given-names> <surname>Hermundstad</surname></string-name>, <string-name><given-names>Gerald M</given-names> <surname>Rubin</surname></string-name>, and <string-name><given-names>Vivek</given-names> <surname>Jayaraman</surname></string-name></person-group>. <article-title>A connectome of the drosophila central complex reveals network motifs suitable for flexible navigation and context-dependent action selection</article-title>. <source>eLife</source>, <volume>10</volume>:<elocation-id>e66039</elocation-id>, <month>October</month> <year>2021</year>. <pub-id pub-id-type="doi">10.7554/eLife.66039</pub-id></mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jonathan</given-names> <surname>Green</surname></string-name>, <string-name><given-names>Atsuko</given-names> <surname>Adachi</surname></string-name>, <string-name><given-names>Kunal K</given-names> <surname>Shah</surname></string-name>, <string-name><given-names>Jonathan D</given-names> <surname>Hirokawa</surname></string-name>, <string-name><given-names>Pablo S</given-names> <surname>Magani</surname></string-name>, and <string-name><given-names>Gaby</given-names> <surname>Maimon</surname></string-name></person-group>. <article-title>A neural circuit architecture for angular integration in drosophila</article-title>. <source>Nature</source>, <volume>546</volume>(<issue>7656</issue>):<fpage>101</fpage>â€“<lpage>106</lpage>, <month>June</month> <year>2017</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Sung Soo</given-names> <surname>Kim</surname></string-name>, <string-name><surname>HervÃ©</surname> <given-names>Rouault</given-names></string-name> <string-name><given-names>Shaul</given-names> <surname>Druckmann</surname></string-name>, and <string-name><given-names>Vivek</given-names> <surname>Jayaraman</surname></string-name></person-group>. <article-title>Ring attractor dynamics in the drosophila central brain</article-title>. <source>Science</source>, <volume>356</volume>(<issue>6340</issue>):<fpage>849</fpage>â€“<lpage>853</lpage>, <month>May</month> <year>2017</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>SC</given-names> <surname>Cannon</surname></string-name> and <string-name><given-names>DA</given-names> <surname>Robinson</surname></string-name></person-group>. <article-title>An improved neural-network model for the neural integrator of the oculomotor system: more realistic neuron behavior</article-title>. <source>Biol. Cybern</source>., <volume>53</volume>(<issue>2</issue>):<fpage>93</fpage>â€“<lpage>108</lpage>, <year>1985</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E</given-names> <surname>Godaux</surname></string-name> and <string-name><given-names>G</given-names> <surname>Cheron</surname></string-name></person-group>. <article-title>The hypothesis of the uniqueness of the oculomotor neural integrator: direct experimental evidence in the cat</article-title>. <source>J. Physiol</source>., <volume>492</volume> (<issue>Pt 2</issue>) (Pt 2):<fpage>517</fpage>â€“<lpage>527</lpage>, <month>April</month> <year>1996</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>AF</given-names> <surname>Fuchs</surname></string-name>, <string-name><given-names>CA</given-names> <surname>Scudder</surname></string-name>, and <string-name><given-names>CR</given-names> <surname>Kaneko</surname></string-name></person-group>. <article-title>Discharge patterns and recruitment order of identified motoneurons and internuclear neurons in the monkey abducens nucleus</article-title>. <source>J. Neurophysiol</source>., <volume>60</volume>(<issue>6</issue>):<fpage>1874</fpage>â€“<lpage>1895</lpage>, <month>December</month> <year>1988</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E</given-names> <surname>Aksay</surname></string-name>, <string-name><given-names>R</given-names> <surname>Baker</surname></string-name>, <string-name><given-names>HS</given-names> <surname>Seung</surname></string-name>, and <string-name><given-names>DW</given-names> <surname>Tank</surname></string-name></person-group>. <article-title>Anatomy and discharge properties of pre-motor neurons in the goldfish medulla that have eye-position signals during fixations</article-title>. <source>J. Neurophysiol</source>., <volume>84</volume>(<issue>2</issue>):<fpage>1035</fpage>â€“<lpage>1049</lpage>, <month>August</month> <year>2000</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H Sebastian</given-names> <surname>Seung</surname></string-name></person-group>. <article-title>Continuous attractors and oculomotor control</article-title>. <source>Neural Netw</source>., <volume>11</volume>(<issue>7</issue>):<fpage>1253</fpage>â€“<lpage>1258</lpage>, <month>October</month> <year>1998</year>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Torkel</given-names> <surname>Hafting</surname></string-name>, <string-name><given-names>Marianne</given-names> <surname>Fyhn</surname></string-name>, <string-name><given-names>Sturla</given-names> <surname>Molden</surname></string-name>, <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Microstructure of a spatial map in the entorhinal cortex</article-title>. <source>Nature</source>, <volume>436</volume>(<issue>7052</issue>):<fpage>801</fpage>â€“<lpage>806</lpage>, <month>August</month> <year>2005</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Hanne</given-names> <surname>Stensola</surname></string-name>, <string-name><given-names>Tor</given-names> <surname>Stensola</surname></string-name>, <string-name><given-names>Trygve</given-names> <surname>Solstad</surname></string-name>, <string-name><given-names>Kristian</given-names> <surname>FrÃ¸land</surname></string-name>, <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>The entorhinal grid map is discretized</article-title>. <source>Nature</source>, <volume>492</volume>(<issue>7427</issue>):<fpage>72</fpage>â€“<lpage>78</lpage>, <month>December</month> <year>2012</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yoram</given-names> <surname>Burak</surname></string-name> and <string-name><given-names>Ila R</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Accurate path integration in continuous attractor network models of grid cells</article-title>. <source>PLoS Comput. Biol</source>., <volume>5</volume>(<issue>2</issue>):<fpage>e1000291</fpage>, <month>February</month> <year>2009</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Stanley</given-names> <surname>Heinze</surname></string-name>, <string-name><given-names>Ajay</given-names> <surname>Narendra</surname></string-name>, and <string-name><given-names>Allen</given-names> <surname>Cheung</surname></string-name></person-group>. <article-title>Principles of insect path integration</article-title>. <source>Curr. Biol</source>., <volume>28</volume>(<issue>17</issue>):<fpage>R1043</fpage>â€“<lpage>R1058</lpage>, <month>September</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Malcolm G</given-names> <surname>Campbell</surname></string-name>, <string-name><given-names>Alexander</given-names> <surname>Attinger</surname></string-name>, <string-name><given-names>Samuel A</given-names> <surname>Ocko</surname></string-name>, <string-name><given-names>Surya</given-names> <surname>Ganguli</surname></string-name>, and <string-name><given-names>Lisa M</given-names> <surname>Giocomo</surname></string-name></person-group>. <article-title>Distance-tuned neurons drive specialized path integration calculations in medial entorhinal cortex</article-title>. <source>Cell Rep</source>., <volume>36</volume>(<issue>10</issue>), <month>September</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>PE</given-names> <surname>Sharp</surname></string-name>, <string-name><given-names>HT</given-names> <surname>Blair</surname></string-name>, and <string-name><given-names>J</given-names> <surname>Cho</surname></string-name></person-group>. <article-title>The anatomical and computational basis of the rat head-direction cell signal</article-title>. <source>Trends Neurosci</source>., <volume>24</volume>(<issue>5</issue>):<fpage>289</fpage>â€“<lpage>294</lpage>, <month>May</month> <year>2001</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>PE</given-names> <surname>Sharp</surname></string-name>, <string-name><given-names>Amanda</given-names> <surname>Tinkelman</surname></string-name>, and <string-name><given-names>Jeiwon</given-names> <surname>Cho</surname></string-name></person-group>. <article-title>Angular velocity and head direction signals recorded from the dorsal tegmental nucleus of gudden in the rat: implications for path integration in the head direction cell circuit</article-title>. <source>Behav. Neurosci</source>., <volume>115</volume>(<issue>3</issue>):<fpage>571</fpage>â€“<lpage>588</lpage>, <month>June</month> <year>2001</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name>, <string-name><given-names>Emilio</given-names> <surname>Kropff</surname></string-name>, and <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Place cells, grid cells, and the brainâ€™s spatial representation system</article-title>. <source>Annu. Rev. Neurosci</source>., <volume>31</volume>:<fpage>69</fpage>â€“<lpage>89</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J</given-names> <surname>Oâ€™Keefe</surname></string-name></person-group>. <article-title>A computational theory of the hippocampal cognitive map</article-title>. <source>Prog. Brain Res</source>., <year>1990</year>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Dmitriy</given-names> <surname>Aronov</surname></string-name> and <string-name><given-names>David W</given-names> <surname>Tank</surname></string-name></person-group>. <article-title>Engagement of neural circuits underlying 2D spatial navigation in a rodent virtual reality system</article-title>. <source>Neuron</source>, <volume>84</volume>(<issue>2</issue>):<fpage>442</fpage>â€“<lpage>456</lpage>, <month>October</month> <year>2014</year>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nathaniel J</given-names> <surname>Killian</surname></string-name>, <string-name><given-names>Michael J</given-names> <surname>Jutras</surname></string-name>, and <string-name><given-names>Elizabeth A</given-names> <surname>Buffalo</surname></string-name></person-group>. <article-title>A map of visual space in the primate entorhinal cortex</article-title>. <source>Nature</source>, <volume>491</volume>(<issue>7426</issue>):<fpage>761</fpage>â€“<lpage>764</lpage>, <month>November</month> <year>2012</year>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alexandra O</given-names> <surname>Constantinescu</surname></string-name>, <string-name><given-names>Jill X</given-names> <surname>Oâ€™Reilly</surname></string-name>, and <string-name><given-names>Timothy EJ</given-names> <surname>Behrens</surname></string-name></person-group>. <article-title>Organizing conceptual knowledge in humans with a gridlike code</article-title>. <source>Science</source>, <volume>352</volume>(<issue>6292</issue>):<fpage>1464</fpage>â€“<lpage>1468</lpage>, <month>June</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>James CR</given-names> <surname>Whittington</surname></string-name>, <string-name><given-names>David</given-names> <surname>McCaffary</surname></string-name>, <string-name><given-names>Jacob JW</given-names> <surname>Bakermans</surname></string-name>, and <string-name><given-names>Timothy EJ</given-names> <surname>Behrens</surname></string-name></person-group>. <article-title>How to build a cognitive map</article-title>. <source>Nat. Neurosci</source>., <volume>25</volume>(<issue>10</issue>):<fpage>1257</fpage>â€“<lpage>1272</lpage>, <month>October</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Honi</given-names> <surname>Sanders</surname></string-name>, <string-name><given-names>Matthew</given-names> <surname>Wilson</surname></string-name>, <string-name><given-names>Mirko</given-names> <surname>Klukas</surname></string-name>, <string-name><given-names>Sugandha</given-names> <surname>Sharma</surname></string-name>, and <string-name><given-names>Ila</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Efficient inference in structured spaces</article-title>. <source>Cell</source>, <volume>183</volume>(<issue>5</issue>):<fpage>1147</fpage>â€“<lpage>1148</lpage>, <month>November</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ling L</given-names> <surname>Dong</surname></string-name> and <string-name><given-names>Ila R</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Grid cells in cognition: Mechanisms and function</article-title>. <source>Annu. Rev. Neurosci</source>., <volume>47</volume>(<issue>1</issue>):<fpage>345</fpage>â€“<lpage>368</lpage>, <month>August</month> <year>2024</year>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Abhiram</given-names> <surname>Iyer</surname></string-name>, <string-name><given-names>Sarthak</given-names> <surname>Chandra</surname></string-name>, <string-name><given-names>Sugandha</given-names> <surname>Sharma</surname></string-name>, and <string-name><given-names>Ila R</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Flexible mapping of abstract domains by grid cells via self-supervised extraction and projection of generalized velocity signals</article-title>. In <conf-name>The Thirty-eighth Annual Conference on Neural Information Processing Systems</conf-name>, <year>2024</year>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mengyu</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Aditya</given-names> <surname>Nair</surname></string-name>, <string-name><given-names>Nestor</given-names> <surname>Coria</surname></string-name>, <string-name><given-names>Scott W</given-names> <surname>Linderman</surname></string-name>, and <string-name><given-names>David J</given-names> <surname>Anderson</surname></string-name></person-group>. <article-title>Encoding of female mating dynamics by a hypothalamic line attractor</article-title>. <source>Nature</source>, pages <fpage>1</fpage>â€“<lpage>9</lpage>, <month>August</month> <year>2024</year>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Peter E</given-names> <surname>Welinder</surname></string-name>, <string-name><given-names>Yoram</given-names> <surname>Burak</surname></string-name>, and <string-name><given-names>Ila R</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Grid cells: the position code, neural network models of activity, and the problem of learning</article-title>. <source>Hippocampus</source>, <volume>18</volume>(<issue>12</issue>):<fpage>1283</fpage>â€“<lpage>1300</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K</given-names> <surname>Zhang</surname></string-name></person-group>. <article-title>Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory</article-title>. <source>J. Neurosci</source>., <volume>16</volume>(<issue>6</issue>):<fpage>2112</fpage>â€“<lpage>2126</lpage>, <month>March</month> <year>1996</year>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Xiaohui</given-names> <surname>Xie</surname></string-name>, <string-name><given-names>Richard H R</given-names> <surname>Hahnloser</surname></string-name>, and <string-name><given-names>H Sebastian</given-names> <surname>Seung</surname></string-name></person-group>. <article-title>Double-ring network model of the head-direction system</article-title>. <source>Phys. Rev. E Stat. Nonlin. Soft Matter Phys</source>., <volume>66</volume>(<issue>4 Pt 1</issue>):<fpage>041902</fpage>, <month>October</month> <year>2002</year>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R</given-names> <surname>Ben-Yishai</surname></string-name>, <string-name><given-names>RL</given-names> <surname>Bar-Or</surname></string-name>, and <string-name><given-names>H</given-names> <surname>Sompolinsky</surname></string-name></person-group>. <article-title>Theory of orientation tuning in visual cortex</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source>., <volume>92</volume>(<issue>9</issue>):<fpage>3844</fpage>â€“<lpage>3848</lpage>, <month>April</month> <year>1995</year>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>HS</given-names> <surname>Seung</surname></string-name>, <string-name><given-names>DD</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>Y Y</given-names> <surname>Reis</surname></string-name>, and <string-name><given-names>DW</given-names> <surname>Tank</surname></string-name></person-group>. <article-title>Stability of the memory of eye position in a recurrent network of conductance-based model neurons</article-title>. <source>Neuron</source>, <volume>26</volume>(<issue>1</issue>):<fpage>259</fpage>â€“<lpage>271</lpage>, <month>April</month> <year>2000</year>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ashwin</given-names> <surname>Vishwanathan</surname></string-name>, <string-name><given-names>Kayvon</given-names> <surname>Daie</surname></string-name>, <string-name><given-names>Alexandro D</given-names> <surname>Ramirez</surname></string-name>, <string-name><given-names>Jeff W</given-names> <surname>Lichtman</surname></string-name>, <string-name><given-names>Emre R F</given-names> <surname>Aksay</surname></string-name>, and <string-name><given-names>H Sebastian</given-names> <surname>Seung</surname></string-name></person-group>. <article-title>Electron microscopic reconstruction of functionally identified cells in a neural integrator</article-title>. <source>Curr. Biol</source>., <volume>27</volume>(<issue>14</issue>):<fpage>2137</fpage>â€“<lpage>2147.e3,</lpage> <month>July</month> <year>2017</year>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>WE</given-names> <surname>Skaggs</surname></string-name>, <string-name><given-names>JJ</given-names> <surname>Knierim</surname></string-name>, <string-name><given-names>S S</given-names> <surname>Kudrimoti</surname></string-name>, and <string-name><given-names>BL</given-names> <surname>McNaughton</surname></string-name></person-group>. <article-title>A model of the neural basis of the ratâ€™s sense of direction</article-title>. <source>Adv. Neural Inf. Process. Syst</source>., <volume>7</volume>:<fpage>173</fpage>â€“<lpage>180</lpage>, <year>1995</year>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Kijung</given-names> <surname>Yoon</surname></string-name>, <string-name><given-names>Michael A</given-names> <surname>Buice</surname></string-name>, <string-name><given-names>Caswell</given-names> <surname>Barry</surname></string-name>, <string-name><given-names>Robin</given-names> <surname>Hayman</surname></string-name>, <string-name><given-names>Neil</given-names> <surname>Burgess</surname></string-name>, and <string-name><given-names>Ila R</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Specific evidence of low-dimensional continuous attractor dynamics in grid cells</article-title>. <source>Nat. Neurosci</source>., <volume>16</volume>(<issue>8</issue>):<fpage>1077</fpage>â€“<lpage>1084</lpage>, <month>August</month> <year>2013</year>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Kijung</given-names> <surname>Yoon</surname></string-name>, <string-name><given-names>Sam</given-names> <surname>Lewallen</surname></string-name>, <string-name><given-names>Amina A</given-names> <surname>Kinkhabwala</surname></string-name>, <string-name><given-names>David W</given-names> <surname>Tank</surname></string-name>, and <string-name><given-names>Ila R</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Grid cell responses in 1D environments assessed as slices through a 2D lattice</article-title>. <source>Neuron</source>, <volume>89</volume>(<issue>5</issue>):<fpage>1086</fpage>â€“<lpage>1099</lpage>, <month>March</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Sean G</given-names> <surname>Trettel</surname></string-name>, <string-name><given-names>John B</given-names> <surname>Trimper</surname></string-name>, <string-name><given-names>Ernie</given-names> <surname>Hwaun</surname></string-name>, <string-name><given-names>Ila R</given-names> <surname>Fiete</surname></string-name>, and <string-name><given-names>Laura Lee</given-names> <surname>Colgin</surname></string-name></person-group>. <article-title>Grid cell co-activity patterns during sleep reflect spatial overlap of grid fields during active behaviors</article-title>. <source>Nat. Neurosci</source>., <volume>22</volume>(<issue>4</issue>):<fpage>609</fpage>â€“<lpage>617</lpage>, <month>April</month> <year>2019</year>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>RJ</given-names> <surname>Gardner</surname></string-name>, <string-name><given-names>L</given-names> <surname>Lu</surname></string-name>, <string-name><given-names>T</given-names> <surname>Wernle</surname></string-name>, <string-name><given-names>MB</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>EI</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Correlation structure of grid cells is preserved during sleep</article-title>. <source>Nat. Neurosci</source>., <year>2019</year>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Richard J</given-names> <surname>Gardner</surname></string-name>, <string-name><given-names>Erik</given-names> <surname>Hermansen</surname></string-name>, <string-name><given-names>Marius</given-names> <surname>Pachitariu</surname></string-name>, <string-name><given-names>Yoram</given-names> <surname>Burak</surname></string-name>, <string-name><given-names>Nils A</given-names> <surname>Baas</surname></string-name>, <string-name><given-names>Benjamin A</given-names> <surname>Dunn</surname></string-name>, <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Toroidal topology of population activity in grid cells</article-title>. <source>Nature</source>, <volume>602</volume>(<issue>7895</issue>):<fpage>123</fpage>â€“<lpage>128</lpage>, <month>February</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mark C</given-names> <surname>Fuhs</surname></string-name> and <string-name><given-names>David S</given-names> <surname>Touretzky</surname></string-name></person-group>. <article-title>A spin glass model of path integration in rat medial entorhinal cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>26</volume>(<issue>16</issue>):<fpage>4266</fpage>â€“<lpage>4276</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Bruce L</given-names> <surname>McNaughton</surname></string-name>, <string-name><given-names>Francesco P</given-names> <surname>Battaglia</surname></string-name>, <string-name><given-names>Ole</given-names> <surname>Jensen</surname></string-name>, <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Path integration and the neural basis of the â€˜cognitive mapâ€™</article-title>. <source>Nat. Rev. Neurosci</source>., <volume>7</volume>(<issue>8</issue>):<fpage>663</fpage>â€“<lpage>678</lpage>, <month>August</month> <year>2006</year>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A David</given-names> <surname>Redish</surname></string-name>, <string-name><given-names>Adam N</given-names> <surname>Elga</surname></string-name>, and <string-name><given-names>David S</given-names> <surname>Touretzky</surname></string-name></person-group>. <article-title>A coupled attractor model of the rodent head direction system</article-title>. <source>Network: computation in neural systems</source>, <volume>7</volume>(<issue>4</issue>):<fpage>671</fpage>, <year>1996</year>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Y</given-names> <surname>Burak</surname></string-name> and <string-name><given-names>I</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Do we understand the emergent dynamics of grid cell activity?</article-title> <source>J. Neurosci</source>., <volume>26</volume>:<fpage>9352</fpage>â€“<lpage>9354</lpage>, <month>September</month> <year>2006</year>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Andrea</given-names> <surname>Banino</surname></string-name>, <string-name><given-names>Caswell</given-names> <surname>Barry</surname></string-name>, <string-name><given-names>Benigno</given-names> <surname>Uria</surname></string-name>, <string-name><given-names>Charles</given-names> <surname>Blundell</surname></string-name>, <string-name><given-names>Timothy</given-names> <surname>Lillicrap</surname></string-name>, <string-name><given-names>Piotr</given-names> <surname>Mirowski</surname></string-name>, <string-name><given-names>Alexander</given-names> <surname>Pritzel</surname></string-name>, <string-name><given-names>Martin J</given-names> <surname>Chadwick</surname></string-name>, <string-name><given-names>Thomas</given-names> <surname>Degris</surname></string-name>, <string-name><given-names>Joseph</given-names> <surname>Modayil</surname></string-name>, <string-name><given-names>Greg</given-names> <surname>Wayne</surname></string-name>, <string-name><given-names>Hubert</given-names> <surname>Soyer</surname></string-name>, <string-name><given-names>Fabio</given-names> <surname>Viola</surname></string-name>, <string-name><given-names>Brian</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Ross</given-names> <surname>Goroshin</surname></string-name>, <string-name><given-names>Neil</given-names> <surname>Rabinowitz</surname></string-name>, <string-name><given-names>Razvan</given-names> <surname>Pascanu</surname></string-name>, <string-name><given-names>Charlie</given-names> <surname>Beattie</surname></string-name>, <string-name><given-names>Stig</given-names> <surname>Petersen</surname></string-name>, <string-name><given-names>Amir</given-names> <surname>Sadik</surname></string-name>, <string-name><given-names>Stephen</given-names> <surname>Gaffney</surname></string-name>, <string-name><given-names>Helen</given-names> <surname>King</surname></string-name>, <string-name><given-names>Koray</given-names> <surname>Kavukcuoglu</surname></string-name>, <string-name><given-names>Demis</given-names> <surname>Hassabis</surname></string-name>, <string-name><given-names>Raia</given-names> <surname>Hadsell</surname></string-name>, and <string-name><given-names>Dharshan</given-names> <surname>Kumaran</surname></string-name></person-group>. <article-title>Vector-based navigation using grid-like representations in artificial agents</article-title>. <source>Nature</source>, <volume>557</volume>(<issue>7705</issue>):<fpage>429</fpage>â€“<lpage>433</lpage>, <month>May</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Christopher J</given-names> <surname>Cueva</surname></string-name> and <string-name><given-names>Xue-Xin</given-names> <surname>Wei</surname></string-name></person-group>. <article-title>Emergence of grid-like representations by training recurrent neural networks to perform spatial localization</article-title>. <source>Arxiv</source>, <month>March</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ben Sorscher</surname>, <given-names>Gabriel C Mel</given-names></string-name>, <string-name><given-names>Samuel A</given-names> <surname>Ocko</surname></string-name>, <string-name><given-names>Lisa</given-names> <surname>Giocomo</surname></string-name>, and <string-name><given-names>Surya</given-names> <surname>Ganguli</surname></string-name></person-group>. <article-title>A unified theory for the computational and mechanistic origins of grid cells</article-title>. <source>Neuron</source> <month>December</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>A</given-names> <surname>Nayebi</surname></string-name>, <string-name><given-names>A</given-names> <surname>Attinger</surname></string-name>, <string-name><given-names>M</given-names> <surname>Campbell</surname></string-name></person-group>, and others. <article-title>Explaining heterogeneity in medial entorhinal cortex with task-driven neural networks</article-title>. <conf-name>Advances in Neural Information Processing Systems</conf-name>, <volume>34</volume> <year>2021</year>.</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Akhilan</given-names> <surname>Boopathy</surname></string-name>, <string-name><given-names>Sunshine</given-names> <surname>Jiang</surname></string-name>, <string-name><given-names>William</given-names> <surname>Yue</surname></string-name>, <string-name><given-names>Jaedong</given-names> <surname>Hwang</surname></string-name>, <string-name><given-names>Abhiram</given-names> <surname>Iyer</surname></string-name>, and <string-name><given-names>Ila</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Breaking neural network scaling laws with modularity</article-title>. <source>arXiv</source>, <month>September</month> <year>2024</year>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Qiyao</given-names> <surname>Liang</surname></string-name>, <string-name><given-names>Ziming</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Mitchell</given-names> <surname>Ostrow</surname></string-name>, and <string-name><given-names>Ila</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>How diffusion models learn to factorize and compose</article-title>. <source>arXiv</source>, <month>August</month> <year>2024</year>.</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Si</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>K</given-names> <surname>Hamaguchi</surname></string-name>, and <string-name><given-names>S</given-names> <surname>Amari</surname></string-name></person-group>. <article-title>Dynamics and computation of continuous attractors</article-title>. <source>Neural Comput</source>., <volume>20</volume>:<fpage>994</fpage>â€“<lpage>1025</lpage>, <month>April</month> <year>2008</year>.</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alexis</given-names> <surname>Guanella</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Kiper</surname></string-name>, and <string-name><given-names>Paul</given-names> <surname>Verschure</surname></string-name></person-group>. <article-title>A model of grid cells based on a twisted torus topology</article-title>. <source>Int. J. Neural Syst</source>., <volume>17</volume>(<issue>4</issue>):<fpage>231</fpage>â€“<lpage>240</lpage>, <month>August</month> <year>2007</year>.</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Albert</given-names> <surname>Compte</surname></string-name>, <string-name><given-names>Nicolas</given-names> <surname>Brunel</surname></string-name>, <string-name><given-names>Patricia S</given-names> <surname>Goldman-Rakic</surname></string-name>, and <string-name><given-names>Xiao-Jing</given-names> <surname>Wang</surname></string-name></person-group>. <article-title>Synaptic mechanisms and network dynamics underlying spatial working memory in a cortical network model</article-title>. <source>Cerebral cortex</source>, <volume>10</volume>(<issue>9</issue>):<fpage>910</fpage>â€“<lpage>923</lpage>, <year>2000</year>.</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>John J</given-names> <surname>Hopfield</surname></string-name></person-group>. <article-title>Neurons with graded response have collective computational properties like those of two-state neurons</article-title>. <source>Proceedings of the national academy of sciences</source>, <volume>81</volume>(<issue>10</issue>):<fpage>3088</fpage>â€“<lpage>3092</lpage>, <year>1984</year>.</mixed-citation></ref>
<ref id="c59"><label>[59]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Mikail</given-names> <surname>Khona</surname></string-name>, <string-name><given-names>Sarthak</given-names> <surname>Chandra</surname></string-name>, and <string-name><given-names>Ila</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Spontaneous emergence of topologically robust grid cell modules: A multiscale instability theory</article-title>. <source>bioRxiv</source> <month>October</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c60"><label>[60]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Edward</given-names> <surname>Ott</surname></string-name></person-group>. <source>Quasiperiodicity</source>, page <fpage>212</fpage>â€“<lpage>245</lpage>. <publisher-name>Cambridge University Press</publisher-name>, <edition>2</edition> edition, <year>2002</year>.</mixed-citation></ref>
<ref id="c61"><label>[61]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Katsumi</given-names> <surname>Nomizu</surname></string-name></person-group>. <article-title>On local and global existence of killing vector fields</article-title>. <source>Annals of Mathematics</source>, pages <fpage>105</fpage>â€“<lpage>120</lpage>, <year>1960</year>.</mixed-citation></ref>
<ref id="c62"><label>[62]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Sharief</given-names> <surname>Deshmukh</surname></string-name> and <string-name><given-names>Olga</given-names> <surname>Belova</surname></string-name></person-group>. <article-title>On killing vector fields on riemannian manifolds</article-title>. <source>Mathematics</source>, <volume>9</volume>(<issue>3</issue>):<fpage>259</fpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c63"><label>[63]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>John M</given-names> <surname>Lee</surname></string-name> and <string-name><given-names>John M</given-names> <surname>Lee</surname></string-name></person-group>. <source>Smooth manifolds</source>. <publisher-name>Springer</publisher-name>, <year>2012</year>.</mixed-citation></ref>
<ref id="c64"><label>[64]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>John H</given-names> <surname>Wen</surname></string-name>, <string-name><given-names>Ben</given-names> <surname>Sorscher</surname></string-name>, <string-name><given-names>Emily A Aery</given-names> <surname>Jones</surname></string-name>, <string-name><given-names>Surya</given-names> <surname>Ganguli</surname></string-name>, and <string-name><given-names>Lisa M</given-names> <surname>Giocomo</surname></string-name></person-group>. <article-title>One-shot entorhinal maps enable flexible navigation in novel environments</article-title>. <source>Nature</source>, <volume>635</volume>(<issue>8040</issue>):<fpage>943</fpage>â€“<lpage>950</lpage>, <month>November</month> <year>2024</year>.</mixed-citation></ref>
<ref id="c65"><label>[65]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>John M</given-names> <surname>Lee</surname></string-name></person-group>. <source>Introduction to Riemannian manifolds</source>, volume <volume>2</volume>. <publisher-name>Springer</publisher-name>, <year>2018</year>.</mixed-citation></ref>
<ref id="c66"><label>[66]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Louis</given-names> <surname>Kang</surname></string-name>, <string-name><given-names>Boyan</given-names> <surname>Xu</surname></string-name>, and <string-name><given-names>Dmitriy</given-names> <surname>Morozov</surname></string-name></person-group>. <article-title>State space discovery in spatial representation circuits with persistent cohomology</article-title>. <source>bioRxiv</source> <month>October</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c67"><label>[67]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Gard</given-names> <surname>Spreemann</surname></string-name>, <string-name><given-names>Benjamin</given-names> <surname>Dunn</surname></string-name>, <string-name><given-names>Magnus Bakke</given-names> <surname>Botnan</surname></string-name>, and <string-name><given-names>Nils A</given-names> <surname>Baas</surname></string-name></person-group>. <article-title>Using persistent homology to reveal hidden information in neural data</article-title>. <source>arXiv</source> <month>October</month> <year>2015</year>.</mixed-citation></ref>
<ref id="c68"><label>[68]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Robert</given-names> <surname>Ghrist</surname></string-name></person-group>. <article-title>Barcodes: The persistent topology of data</article-title>. <source>Bull. Am. Math. Soc</source>., <volume>45</volume>(<issue>01</issue>):<fpage>61</fpage>â€“<lpage>76</lpage>, <month>October</month> <year>2007</year>.</mixed-citation></ref>
<ref id="c69"><label>[69]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Mehrdad</given-names> <surname>Jazayeri</surname></string-name> and <string-name><given-names>Srdjan</given-names> <surname>Ostojic</surname></string-name></person-group>. <article-title>Interpreting neural computations by examining intrinsic and embedding dimensionality of neural activity</article-title>. <source>arXiv</source> <month>July</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c70"><label>[70]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>CÃ¡tia</given-names> <surname>Fortunato</surname></string-name>, <string-name><given-names>Jorge</given-names> <surname>Bennasar-VÃ¡zquez</surname></string-name>, <string-name><given-names>Junchol</given-names> <surname>Park</surname></string-name>, <string-name><given-names>Joanna C</given-names> <surname>Chang</surname></string-name>, <string-name><given-names>Lee E</given-names> <surname>Miller</surname></string-name>, <string-name><given-names>Joshua T</given-names> <surname>Dudman</surname></string-name>, <string-name><given-names>Matthew G</given-names> <surname>Perich</surname></string-name>, and <string-name><given-names>Juan A</given-names> <surname>Gallego</surname></string-name></person-group>. <article-title>Nonlinear manifolds underlie neural population activity during behaviour</article-title>. <source>bioRxiv</source> <month>July</month> <year>2023</year>.</mixed-citation></ref>
<ref id="c71"><label>[71]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ege</given-names> <surname>Altan</surname></string-name>, <string-name><given-names>Sara A</given-names> <surname>Solla</surname></string-name>, <string-name><given-names>Lee E</given-names> <surname>Miller</surname></string-name>, and <string-name><given-names>Eric J</given-names> <surname>Perreault</surname></string-name></person-group>. <article-title>Estimating the dimensionality of the manifold underlying multi-electrode neural recordings</article-title>. <source>PLoS Comput. Biol</source>., <volume>17</volume>(<issue>11</issue>):<fpage>e1008591</fpage>, <month>November</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c72"><label>[72]</label><mixed-citation publication-type="report"><person-group person-group-type="author"><string-name><given-names>Anna V</given-names> <surname>Little</surname></string-name>, <string-name><given-names>Mauro</given-names> <surname>Maggioni</surname></string-name>, and <string-name><given-names>Lorenzo</given-names> <surname>Rosasco</surname></string-name></person-group>. <source>Multiscale geometric methods for estimating intrinsic dimension</source>. <publisher-name>MIT</publisher-name> <ext-link ext-link-type="uri" xlink:href="http://hdl.handle.net/1721.1/72597">http://hdl.handle.net/1721.1/72597</ext-link>. Accessed: <date-in-citation content-type="access-date">2022-9-16</date-in-citation>.</mixed-citation></ref>
<ref id="c73"><label>[73]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K</given-names> <surname>Fukunaga</surname></string-name> and <string-name><given-names>DR</given-names> <surname>Olsen</surname></string-name></person-group>. <article-title>An algorithm for finding intrinsic dimensionality of data</article-title>. <source>IEEE Trans. Comput</source>., <volume>C-20</volume>(<issue>2</issue>):<fpage>176</fpage>â€“<lpage>183</lpage>, <month>February</month> <year>1971</year>.</mixed-citation></ref>
<ref id="c74"><label>[74]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Thomas F</given-names> <surname>Banchoff</surname></string-name> and <string-name><given-names>Stephen</given-names> <surname>Lovett</surname></string-name></person-group>. <source>Differential geometry of curves and surfaces</source>. <publisher-name>CRC Press</publisher-name>, <year>2022</year>.</mixed-citation></ref>
<ref id="c75"><label>[75]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Miriam L R</given-names> <surname>Meister</surname></string-name> and <string-name><given-names>Elizabeth A</given-names> <surname>Buffalo</surname></string-name></person-group>. <article-title>Neurons in primate entorhinal cortex represent gaze position in multiple spatial reference frames</article-title>. <source>J. Neurosci</source>., <volume>38</volume>(<issue>10</issue>):<fpage>2430</fpage>â€“ <lpage>2441</lpage>, <month>March</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c76"><label>[76]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Shun-ichi</given-names> <surname>Amari</surname></string-name></person-group>. <article-title>Dynamics of pattern formation in lateral-inhibition type neural fields</article-title>. <source>Biological cybernetics</source>, <volume>27</volume>(<issue>2</issue>):<fpage>77</fpage>â€“<lpage>87</lpage>, <year>1977</year>.</mixed-citation></ref>
<ref id="c77"><label>[77]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>David</given-names> <surname>Sussillo</surname></string-name> and <string-name><given-names>Omri</given-names> <surname>Barak</surname></string-name></person-group>. <article-title>Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks</article-title>. <source>Neural Comput</source>., <volume>25</volume>(<issue>3</issue>):<fpage>626</fpage>â€“<lpage>649</lpage>, <month>March</month> <year>2013</year>.</mixed-citation></ref>
<ref id="c78"><label>[78]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Niru</given-names> <surname>Maheswaranathan</surname></string-name>, <string-name><given-names>Alex H</given-names> <surname>Williams</surname></string-name>, <string-name><given-names>Matthew D</given-names> <surname>Golub</surname></string-name>, <string-name><given-names>Surya</given-names> <surname>Ganguli</surname></string-name>, and <string-name><given-names>David</given-names> <surname>Sussillo</surname></string-name></person-group>. <article-title>Universality and individuality in neural dynamics across large populations of recurrent networks</article-title>. <source>Adv. Neural Inf. Process. Syst</source>., <volume>2019</volume>:<fpage>15629</fpage>â€“<lpage>15641</lpage>, <month>December</month> <year>2019</year>.</mixed-citation></ref>
<ref id="c79"><label>[79]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Rylan</given-names> <surname>Schaeffer</surname></string-name>, <string-name><given-names>Mikail</given-names> <surname>Khona</surname></string-name>, <string-name><given-names>Leenoy</given-names> <surname>Meshulam</surname></string-name>, <string-name><given-names>International Brain</given-names> <surname>Laboratory</surname></string-name>, and <string-name><given-names>Ila Rani</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Reverse-engineering recurrent neural network solutions to a hierarchical inference task for mice</article-title>. <source>bioRxiv</source> <month>June</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c80"><label>[80]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Francesca</given-names> <surname>Mastrogiuseppe</surname></string-name> and <string-name><given-names>Srdjan</given-names> <surname>Ostojic</surname></string-name></person-group>. <article-title>Linking connectivity, dynamics, and computations in Low-Rank recurrent neural networks</article-title>. <source>Neuron</source>, <volume>99</volume>(<issue>3</issue>):<fpage>609</fpage>â€“<lpage>623.e29,</lpage> <month>August</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c81"><label>[81]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Federico</given-names> <surname>Claudi</surname></string-name> and <string-name><given-names>Tiago</given-names> <surname>Branco</surname></string-name></person-group>. <article-title>Differential geometry methods for constructing Manifold-Targeted recurrent neural networks</article-title>. <source>Neural Comput</source>., <volume>34</volume>(<issue>8</issue>):<fpage>1790</fpage>â€“<lpage>1811</lpage>, <month>July</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c82"><label>[82]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Chris</given-names> <surname>Eliasmith</surname></string-name></person-group>. <article-title>A unified approach to building and controlling spiking attractor networks</article-title>. <source>Neural Comput</source>., <volume>17</volume>(<issue>6</issue>):<fpage>1276</fpage>â€“<lpage>1314</lpage>, <month>June</month> <year>2005</year>.</mixed-citation></ref>
<ref id="c83"><label>[83]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ran</given-names> <surname>Darshan</surname></string-name> and <string-name><given-names>Alexander</given-names> <surname>Rivkind</surname></string-name></person-group>. <article-title>Learning to represent continuous variables in heterogeneous neural networks</article-title>. <source>Cell Rep</source>., <volume>39</volume>(<issue>1</issue>):<fpage>110612</fpage>, <month>April</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c84"><label>[84]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Chris</given-names> <surname>Eliasmith</surname></string-name> and <string-name><given-names>Charles H</given-names> <surname>Anderson</surname></string-name></person-group>. <source>Neural Engineering: Computation, Representation, and Dynamics in Neurobiological Systems</source>. <publisher-name>MIT Press</publisher-name>, <year>2003</year>.</mixed-citation></ref>
<ref id="c85"><label>[85]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Omri</given-names> <surname>Barak</surname></string-name> and <string-name><given-names>Sandro</given-names> <surname>Romani</surname></string-name></person-group>. <article-title>Mapping Low-Dimensional dynamics to High-Dimensional neural activity: A derivation of the ring model from the neural engineering framework</article-title>. <source>Neural Comput</source>., <volume>33</volume>(<issue>3</issue>):<fpage>827</fpage>â€“<lpage>852</lpage>, <month>March</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c86"><label>[86]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Eli</given-names> <surname>Pollock</surname></string-name> and <string-name><given-names>Mehrdad</given-names> <surname>Jazayeri</surname></string-name></person-group>. <article-title>Engineering recurrent neural networks from task-relevant manifolds and dynamics</article-title>. <source>PLoS Comput. Biol</source>., <volume>16</volume>(<issue>8</issue>):<fpage>e1008128</fpage>, <month>August</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c87"><label>[87]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Manuel</given-names> <surname>Beiran</surname></string-name>, <string-name><given-names>Alexis</given-names> <surname>Dubreuil</surname></string-name>, <string-name><given-names>Adrian</given-names> <surname>Valente</surname></string-name>, <string-name><given-names>Francesca</given-names> <surname>Mastrogiuseppe</surname></string-name>, and <string-name><given-names>Srdjan</given-names> <surname>Ostojic</surname></string-name></person-group>. <article-title>Shaping dynamics with multiple populations in Low-Rank recurrent networks</article-title>. <source>Neural Comput</source>., <volume>33</volume>(<issue>6</issue>):<fpage>1572</fpage>â€“<lpage>1615</lpage>, <month>May</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c88"><label>[88]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Seth D</given-names> <surname>Axen</surname></string-name>, <string-name><given-names>Mateusz</given-names> <surname>Baran</surname></string-name>, <string-name><given-names>Ronny</given-names> <surname>Bergmann</surname></string-name>, and <string-name><given-names>Krzysztof</given-names> <surname>Rzecki</surname></string-name></person-group>. <article-title>Manifolds.jl: An extensible julia framework for data analysis on manifolds</article-title>. <source>arXiv</source> <month>June</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c89"><label>[89]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Joshua B</given-names> <surname>Tenenbaum</surname></string-name>, <string-name><given-names>Vin</given-names> <surname>de Silva</surname></string-name>, and <string-name><given-names>John C</given-names> <surname>Langford</surname></string-name></person-group>. <article-title>A global geometric framework for nonlinear dimensionality reduction</article-title>. <source>science</source>, <volume>290</volume>(<issue>5500</issue>):<fpage>2319</fpage>â€“<lpage>2323</lpage>, <year>2000</year>.</mixed-citation></ref>
<ref id="c90"><label>[90]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Matija</given-names> <surname>ÄŒufar</surname></string-name></person-group>. <article-title>Ripserer.jl: flexible and efficient persistent homology computation in julia</article-title>. <source>Journal of Open Source Software</source>, <volume>5</volume>(<issue>54</issue>):<fpage>2614</fpage>, <year>2020</year>.</mixed-citation></ref>
</ref-list>
<app-group>
<app id="s5">
<title>Supplementary Information</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1:</label>
<caption><p>Torus CAN activity manifold (top) and persistence diagram (bottom) for varying noise intensity levels (columns).</p></caption>
<graphic xlink:href="652608v1_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s5a">
<label>1</label>
<title>Kernels constructed through distance metrics produce single bump states</title>
<p>Here we estimate the conditions on the interactions that lead to the formation of bump states on the lattice of neurons, ğ’«.</p>
<p>As earlier, consider an interaction weight matrix <italic>W</italic> (<italic>Î¸, Î¸</italic><sup>â€²</sup>) = <italic>k</italic>(<italic>d</italic>(<italic>Î¸, Î¸</italic><sup>â€²</sup>)). We rewrite the kernel <italic>k</italic> as <italic>k</italic>(<italic>d</italic>) = âˆ’<italic>k</italic><sub>0</sub> + <italic>k</italic><sub>1</sub>(<italic>d</italic>), where <italic>k</italic><sub>1</sub>(<italic>d</italic>) â†’ 0 as <italic>d</italic> â†’ <italic>âˆ</italic> and <italic>k</italic><sub>1</sub>(0) = <italic>k</italic><sub>0</sub> <italic>&gt;</italic> 0; and correspondingly write <italic>W</italic> (<italic>d</italic>(<italic>Î¸, Î¸</italic><sup>â€²</sup>)) = âˆ’<italic>W</italic><sub>0</sub> + <italic>W</italic><sub>1</sub>(<italic>d</italic>(<italic>Î¸, Î¸</italic><sup>â€²</sup>)). We assume that the kernel <italic>k</italic> has a length scale <italic>Ïƒ</italic>, i.e., <italic>k</italic><sub>1</sub>(<italic>d</italic>) <italic>â‰ˆ</italic> 0 for <italic>d</italic> â‰¥ <italic>Ïƒ</italic>.</p>
<p>Since <italic>Ïƒ</italic> is the only spatial scale being introduced in the dynamics, we qualitatively expect the localized bump states will have a scale of O(<italic>Ïƒ</italic>). If <italic>Ïƒ</italic> is much smaller than the distances over which the manifold ğ’« has curvature, ğ’« will be approximately flat within a ball <italic>V</italic><sub>Ïƒ</sub> centered on any <italic>x</italic> âˆˆ ğ’«. In this approximation, the conditions for the formation of a stable bump state are the same as those for the formation on a bump state on a globally flat manifold.</p>
<p>To examine the conditions for the existence of a bump state, we will first calculate the homogeneous steady state supported by <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>. Next, we note that since <italic>W</italic> is symmetric in this case, thus <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref> can be described through an energy function[<xref ref-type="bibr" rid="c58">58</xref>], and thus a stable steady state must exist. If the homogeneous state is unstable, there must then exist a stable symmetry broken state of the system. If this symmetry broken state is localized, we refer to it as the bump state.</p>
<p>The homogeneous steady state <italic>s</italic>(<italic>x</italic>) = <italic>s</italic><sub>0</sub> must satisfy
<disp-formula id="ueqn7">
<graphic xlink:href="652608v1_ueqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>V</italic> is the volume of the manifold, âˆ«<italic>dÎ¸</italic>, and <inline-formula><inline-graphic xlink:href="652608v1_inline38.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Rearranging, we obtain
<disp-formula id="eqn11">
<graphic xlink:href="652608v1_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Since the kernel <italic>k</italic><sub>1</sub> is supported on a small volume of the entire manifold, <inline-formula><inline-graphic xlink:href="652608v1_inline39.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and thus the right-hand side of <xref ref-type="disp-formula" rid="eqn11">Eq. 11</xref> is positive, consistent with the assumed rectifying nonlinearity <italic>f</italic> of <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>.</p>
<p>To examine the stability of this homogeneous state, consider a small perturbation, <italic>s</italic>(<italic>x, Î¸</italic>) = <italic>s</italic><sub>0</sub> + exp(<italic>Î±</italic>(<italic>Ï‰</italic>)<italic>t</italic> + <italic>iÏ‰</italic> Â· <italic>Î¸</italic>) to <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>. Following the analysis in Ref. [<xref ref-type="bibr" rid="c59">59</xref>], we obtain
<disp-formula id="eqn12">
<graphic xlink:href="652608v1_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where â„± [<italic>W</italic>] is the Fourier transform of the interaction <italic>W</italic> .
<disp-formula id="ueqn8">
<graphic xlink:href="652608v1_ueqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>Î´</italic>(<italic>Ï‰</italic>) is the Dirac delta function, obtained from the Fourier transform of a constant. Thus, the homogeneous steady state will be unstable if â„± [<italic>W</italic>](<italic>Ï‰</italic>) <italic>&gt;</italic> 1<italic>/Ï„</italic> for some <italic>Ï‰</italic>. Since <italic>Î±</italic>(<italic>Ï‰</italic>) denotes the rate of exponential growth, the maxima of <xref ref-type="disp-formula" rid="eqn12">Eq. 12</xref> will determine the dominant growing mode. If â„±[<italic>W</italic>] were maximized at |<italic>Ï‰</italic>| <italic>&gt;</italic> 0, then the growing perturbation would have a periodic component, and would thus likely not form a localized mode. Instead, if â„±[<italic>W</italic><sub>1</sub>](<italic>Ï‰</italic>) were maximized at <italic>Ï‰</italic> = 0, then â„±[<italic>W</italic>](<italic>Ï‰</italic>) will be maximized at <italic>Ï‰</italic> â†’ 0 (â„±[<italic>W</italic>](<italic>Ï‰</italic>) cannot be maximized strictly at <italic>Ï‰</italic> = 0 itself due to the âˆ’2<italic>Ï€W</italic><sub>0</sub><italic>Î´</italic>(<italic>Ï‰</italic>) contribution to â„±[<italic>W</italic>](<italic>Ï‰</italic>)). In this case, the growing perturbation will be unimodal, likely leading to the formation of a localized state.</p>
<p>Thus, for the formation of a stable bump state on a general manifold, we obtain two requirements: First, the Fourier transform of the kernel <italic>k</italic><sub>1</sub>(<italic>d</italic>) must be maximized at <italic>Ï‰</italic> = 0; and second, this maximum must be larger than 1<italic>/Ï„</italic>. If we are solely interested in interaction shapes that lead to bump formation, we assume we have freedom to rescale the interactions. Thus, if a positive maximum is attained at <italic>Ï‰</italic> = 0 a rescaling can always make this maximum larger than 1<italic>/Ï„</italic>. Thus, we primarily focus on the first requirement.</p>
<p>While we do not provide an exhaustive classification of interaction kernels <italic>k</italic><sub>1</sub> whose Fourier transforms are maximized at zero, we provide a broad sufficient condition â€” if <italic>k</italic><sub>1</sub>(<italic>d</italic>) â‰¥ 0 for all <italic>d</italic>, then its Fourier transform will be maximized at zero. This can be proved as:
<disp-formula id="ueqn9">
<graphic xlink:href="652608v1_ueqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Thus, we finally conclude that, up to a rescaling of the strength of the interaction, an interaction <italic>W</italic> (<italic>d</italic>(<italic>Î¸, Î¸</italic><sup>â€²</sup>)) will lead to the formation of a bump state if it can be rewritten as <italic>W</italic> (<italic>d</italic>(<italic>Î¸, Î¸</italic><sup>â€²</sup>)) = <italic>k</italic><sub>1</sub>(<italic>d</italic>(<italic>Î¸, Î¸</italic><sup>â€²</sup>)) âˆ’ <italic>k</italic><sub>0</sub> for: <italic>k</italic><sub>0</sub> â‰¥ 0; a kernel <italic>k</italic><sub>1</sub> that satisfies <italic>k</italic><sub>1</sub>(<italic>d</italic>) â‰¥ 0 and <italic>k</italic><sub>1</sub>(<italic>d</italic>) â†’ 0 for <italic>d</italic> â‰¥ <italic>Ïƒ</italic> ; and sufficiently small <italic>Ïƒ</italic> over which the manifold ğ’« is approximately flat.</p>
</sec>
<sec id="s5b">
<label>2</label>
<title>Manifold of single bump states ğ’© is isometric to manifold of neural lattice ğ’«</title>
<p>Here we will show that the manifold ğ’© of neural activity, formed through single bump states at each point of the neural lattice ğ’«, is isometric to ğ’«. Specifically, we provide a distance metric <italic>d</italic><sub><italic>N</italic></sub> on the manifold ğ’©, such that (ğ’©, <italic>d</italic><sub><italic>N</italic></sub>) is isometric to (ğ’«, <italic>d</italic><sub><italic>P</italic></sub>), where <italic>d</italic><sub><italic>p</italic></sub> represents the geodesic distance considered as the distance metric during the MADE construction described in the main text.</p>
<p>While we will not prove this in complete generality for any ğ’«, we will assume that if ğ’« has a sufficiently large separation of lengthscales (as assumed in the previous section), it will suffice to show this result for ğ’« given as the flat Eucldiean manifold â„<sup>n</sup> (and correspondingly, <italic>d</italic><sub><italic>p</italic></sub> being the usual <italic>L</italic><sub>2</sub> metric).</p>
<p>To prove the existence of an isometry, we first argue that ğ’© and ğ’« are diffeomorphic. In Apx. 1, we argued that the prescribed connectivity kernel leads to the formation of activity bump states centered at any <italic>x</italic> âˆˆ ğ’«. Define the function <italic>f</italic> from ğ’« to ğ’© to characterize the shape of the activity bump, i.e., for any <italic>x</italic><sub>0</sub> âˆˆ ğ’«, we let <inline-formula><inline-graphic xlink:href="652608v1_inline40.gif" mimetype="image" mime-subtype="gif"/></inline-formula> be the shape of the activity bump centered at <italic>x</italic><sub>0</sub>. Since these activity bump states are generated through radially symmetric kernel interaction functions, the bump states <italic>f</italic> (<italic>x</italic> âˆ’ <italic>x</italic><sub>0</sub>) must also be radially symmetric, i.e., <italic>f</italic> (<italic>x</italic> âˆ’ <italic>x</italic><sub>0</sub>) = <italic>F</italic> (|<italic>x</italic> âˆ’ <italic>x</italic><sub>0</sub>|). In this case, we can see that Î¦ : <italic>x</italic> â†’ <italic>f</italic><sub>x</sub> is now a diffeomorphism, since it is a smooth function and has a smooth inverse (the inverse map is simply computing the center of the radially symmetric activity bump).</p>
<p>Note that a direct <italic>L</italic><sub>2</sub> norm on ğ’© does not suffice, since for sufficiently distant <italic>x</italic><sub>0</sub> and <italic>x</italic><sub>1</sub>,</p>
<p>Next, we examine candidate metrics on ğ’© that may lead to an isometry with (ğ’«, <italic>L</italic><sub>2</sub>). the distance between <inline-formula><inline-graphic xlink:href="652608v1_inline41.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="652608v1_inline42.gif" mimetype="image" mime-subtype="gif"/></inline-formula> given by <inline-formula><inline-graphic xlink:href="652608v1_inline43.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is approximately <inline-formula><inline-graphic xlink:href="652608v1_inline44.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Thus the distance between <inline-formula><inline-graphic xlink:href="652608v1_inline45.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="652608v1_inline46.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is bounded, whereas the distance between <italic>x</italic><sub>0</sub> and <italic>x</italic><sub>1</sub> is not, indicating that there cannot exist a direct isometry.</p>
<p>Instead, we construct here a metric of intrinsic length induced by the Riemannian metric on the tangents of ğ’©. For any two vectors <italic>u</italic>(<italic>x</italic>) and <italic>v</italic>(<italic>x</italic>) in <italic>T</italic><sub>s</sub> ğ’©, the tangent space of ğ’© at <italic>s</italic>. Define the Riemannian metric as <italic>g</italic>(<italic>u, v</italic>) = âŸ¨<italic>u, v</italic>âŸ© =âˆ« <italic>uvdx</italic>. Then, for any path <italic>Î³</italic>(<italic>t</italic>) âˆˆ ğ’©, we can define the length of the path <italic>L</italic>[<italic>Î³</italic>(<italic>t</italic>)] as
<disp-formula id="ueqn10">
<graphic xlink:href="652608v1_ueqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the norm of a tangent vector <italic>Î³</italic><sup>â€²</sup> is defined as<inline-formula><inline-graphic xlink:href="652608v1_inline47.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. This can now be used to define the geodesic metric between <inline-formula><inline-graphic xlink:href="652608v1_inline48.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="652608v1_inline49.gif" mimetype="image" mime-subtype="gif"/></inline-formula> on ğ’© given as the infimum of the lengths of all paths between <inline-formula><inline-graphic xlink:href="652608v1_inline50.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="652608v1_inline51.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Here we will show that under this geodesic metric, the spaces (ğ’©, <italic>d</italic><sub><italic>N</italic></sub>) and (<italic>P, d</italic><sub><italic>P</italic></sub>) are isometric. Specifically, we will show that the metric tensor (the Riemannian metric computed for coordinate basis vectors) is proportional to identity, the metric tensor for flat Euclidean space.</p>
<p>Assume that ğ’© is an <italic>n</italic> dimensional manifold. Let (<italic>x</italic><sup>1</sup>, â€¦<italic>x</italic><sup>n</sup>) be a coordinate chart in the neighborhood of a bump state <inline-formula><inline-graphic xlink:href="652608v1_inline52.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. A basis for the tangent space is then given by the differentials {<italic>âˆ‚/âˆ‚x</italic><sup>1</sup>, â€¦<italic>âˆ‚/âˆ‚x</italic><sup>n</sup>}. Note that since <italic>f</italic> (<italic>x</italic>) is radially symmetric, <italic>f</italic> (<italic>x</italic>) = <italic>F</italic> (|<italic>x</italic>|), the basis vectors can be simplified as
<disp-formula id="ueqn11">
<graphic xlink:href="652608v1_ueqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>We can now compute the metric tensor <italic>g</italic><sub><italic>ij</italic></sub> = <italic>g</italic>(<italic>âˆ‚/âˆ‚x</italic><sup><italic>i</italic></sup>, <italic>âˆ‚/âˆ‚x</italic><sup>j</sup>)
<disp-formula id="eqn13">
<graphic xlink:href="652608v1_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn14">
<graphic xlink:href="652608v1_eqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <xref ref-type="disp-formula" rid="eqn13">Eq. 13</xref> is obtained by performing the change of variables <italic>y</italic> = <italic>x</italic> âˆ’ <italic>x</italic><sub>0</sub>. From <xref ref-type="disp-formula" rid="eqn14">Eq. 14</xref> we can make two crucial observations: first, since the integrand is odd in <italic>y</italic><sup><italic>i</italic></sup> and <italic>y</italic><sup>j</sup>, thus <italic>g</italic><sub><italic>ij</italic></sub> = 0 for <italic>i</italic> â‰  <italic>j</italic>; second, <italic>g</italic><sub><italic>ii</italic></sub> is independent of <italic>x</italic><sub>0</sub>, and by symmetry is also independent of <italic>i</italic> â€” it is entirely determined by the shape of the bump state <italic>F</italic> (|<italic>x</italic>|). Thus, the metric tensor <italic>g</italic><sub><italic>ij</italic></sub> has constant entries on the diagonal, and zero on the off-diagonal elements, i.e., <italic>g</italic> is proportional to the identity matrix. We denote this proportionality constant as <italic>Î±</italic>.</p>
<p>The length of an infinitesimal line element <italic>ds</italic> is then given as<inline-formula><inline-graphic xlink:href="652608v1_inline53.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The length of a path <italic>Î³</italic> from <inline-formula><inline-graphic xlink:href="652608v1_inline54.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to <inline-formula><inline-graphic xlink:href="652608v1_inline55.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is then simply <inline-formula><inline-graphic xlink:href="652608v1_inline56.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which is the Euclidean path length from <italic>x</italic><sub>0</sub> to <italic>x</italic><sub>1</sub> scaled by <inline-formula><inline-graphic xlink:href="652608v1_inline57.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Thus, the geodesic metric from <inline-formula><inline-graphic xlink:href="652608v1_inline58.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to <inline-formula><inline-graphic xlink:href="652608v1_inline59.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the infimum of Euclidean path lengths, i.e., the Euclidean straight-line distance<inline-formula><inline-graphic xlink:href="652608v1_inline60.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We can additionally redefine a new metric <inline-formula><inline-graphic xlink:href="652608v1_inline61.gif" mimetype="image" mime-subtype="gif"/></inline-formula> on the tangent space as <italic>g/Î±</italic>, leading to the new geodesic distance to be exactly the Euclidean distance |<italic>x</italic><sub>0</sub> âˆ’ <italic>x</italic><sub>1</sub>|.</p>
<p>Thus, under the approximation of ğ’« being treated as a flat space without curvature at scales smaller than <italic>Ïƒ</italic>, the metric space (ğ’©, <italic>d</italic><sub><italic>N</italic></sub>) is thus isometric to the metric space (ğ’«, <italic>d</italic><sub><italic>P</italic></sub>).</p>
</sec>
<sec id="s5c">
<label>3</label>
<title>External velocities ignorant about network structure and state require shifted-kernel networks to control bump flow</title>
<p>In this section, for analytical simplicity, we will ignore the neural transfer function nonlinearity <italic>f</italic>.</p>
<p>The fixed points resulting from symmetric kernels in <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref> satisfy:
<disp-formula id="eqn15">
<graphic xlink:href="652608v1_eqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>s</italic>(<italic>Î¸</italic>) denotes an activity bump centered at any point in ğ’«. Consider two such activity bump states: <italic>s</italic><sub>0</sub>(<italic>Î¸</italic>) centered at <italic>Î¸</italic><sub>0</sub>, and a nearby state <italic>s</italic><sub>Ïµ</sub> centered at <italic>Î¸</italic><sub>0</sub>âˆ’<italic>Ïµ</italic>, i.e., <italic>s</italic><sub>Ïµ</sub>(<italic>Î¸</italic>) = <italic>s</italic><sub>0</sub>(<italic>Î¸</italic>+<italic>Ïµ</italic>). For a neural state <italic>s</italic>(<italic>Î¸</italic>) to move from <italic>s</italic><sub>0</sub> to <italic>s</italic><sub>Ïµ</sub> in time Î”<italic>t</italic>, the time derivative <italic>âˆ‚s/âˆ‚t</italic> must equal
<disp-formula id="eqn16">
<graphic xlink:href="652608v1_eqn16.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>We can use <xref ref-type="disp-formula" rid="eqn15">Eq. 15</xref> to evaluate this space derivative as
<disp-formula id="ueqn12">
<graphic xlink:href="652608v1_ueqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>W</italic><sub><italic>Î´</italic></sub> represents a kernel with a small offset <italic>Î´</italic>, i.e., <italic>W</italic><sub><italic>Î´</italic></sub> = <italic>W</italic> (<italic>Î¸</italic> âˆ’ <italic>Î¸</italic><sup>â€²</sup> âˆ’ <italic>Î´</italic>). We can insert this in <xref ref-type="disp-formula" rid="eqn16">Eq. 16</xref> to obtain
<disp-formula id="ueqn13">
<graphic xlink:href="652608v1_ueqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Comparing the above equation with <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>, we find that the neural time constant <italic>Ï„</italic> = <italic>Î´</italic>Î”<italic>t/Ïµ</italic>. Since the speed of the activity bump is <italic>Ïµ/</italic>Î”<italic>t</italic>, we obtain a speed of
<disp-formula id="eqn17">
<graphic xlink:href="652608v1_eqn17.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Thus, a network built with a kernel with offset <italic>Î´</italic> in particular direction leads to activity flow along that direction. Coupling multiple copies of such networks with opposing directions of kernel offsets leads to an equilibrium, with the bump state at a fixed position. This can be intuitively seen by noting that <italic>W</italic><sub><italic>Î´</italic></sub><italic>s</italic> + <italic>W</italic><sub>âˆ’<italic>Î´</italic></sub><italic>s â‰ˆ</italic> (<italic>W</italic> + <italic>Î´âˆ‚</italic><sub>Î¸</sub><italic>W</italic>)<italic>s</italic> + (<italic>W</italic> âˆ’ <italic>Î´âˆ‚</italic><sub>Î¸</sub><italic>W</italic>)<italic>s</italic> = 2<italic>Ws</italic>, and thus opposing offset kernels acting on the same state are equivalent to the state being acted on by a kernel with no offset.</p>
<p>To control the flow the bump in arbitrary directions, we will next demonstrate that the magnitude of the feed-forward input <italic>b</italic> in a particular subnetwork can bias the motion of the bump. To see this, we first consider <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref> scaled by a factor <italic>Î±</italic>,
<disp-formula id="ueqn14">
<graphic xlink:href="652608v1_ueqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Thus, scaling <italic>b</italic> by a factor <italic>Î±</italic> (i.e., <italic>b</italic> â†’ <italic>Î±b</italic>) results in an equivalent solution of the dynamical equation with the states <italic>s</italic> also scaled by the same factor <italic>Î±</italic> (i.e., <italic>s</italic>(<italic>Î¸</italic>) â†’ <italic>Î±s</italic>(<italic>Î¸</italic>)).</p>
<p>Consider two such coupled networks with opposing offsets, with feedforward inputs scaled by <italic>Î±</italic><sub>1</sub> = (1 + <italic>Î±</italic>)/2 and <italic>Î±</italic><sub>2</sub> = (1 âˆ’ <italic>Î±</italic>)<italic>/</italic>2. As noted above the neural firing rates can be assumed to be scaled by the same factors. Heuristically, we will assume that the firing rates of the coupled network can be approximated through individually scaled firing rates of independent offset networks. This leads to the effective interaction through the offset kernels as
<disp-formula id="ueqn15">
<graphic xlink:href="652608v1_ueqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Thus, the effective interaction is similar to that obtained by a kernel with an offset of <italic>Î´Î±</italic>, leading to a bump speed of <italic>Î´Î±/Ï„</italic>.</p>
<p>Finally, we note that while the above argument has been constructed for offsets along a single dimension, it readily generalizes to higher dimensions: For continuous and differentiable <italic>W</italic>, a directional derivative can be written as a linear combination of partial derivatives along coordinate axes, i.e.,
<disp-formula id="ueqn16">
<graphic xlink:href="652608v1_ueqn16.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Thus, subnetworks with differently scaled feedforward inputs lead to differently scaled firing rates <italic>s</italic> which leads to an interaction kernel that has an effective offset in the vector direction determined by the scaling coefficients. This effective offset in a particular direction causes the activity bump to flow along the manifold along that direction, leading to controllable flow of the activity bump through differential feed-forward inputs to the coupled network.</p>
</sec>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107224.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Ã‰cole Normale SupÃ©rieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study presents a theoretical framework for building continuous attractor networks that integrate with a wide range of topologies, which are of increasing relevance to neuroscientists. While the work offers <bold>solid</bold> evidence for most claims, the evidence supporting biological plausibility and key claims - such as the existence of a continuum of stable states and robustness across geometries - is currently <bold>incomplete</bold> and would benefit from further analysis or discussion. The study will be of interest to computational and systems neuroscientists working on neural dynamics and network models of cognition.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107224.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This is a theoretical study addressing the problem of constructing integrator networks for which the activity state and integrated variables display non-trivial topologies. Historically, researchers in theoretical neuroscience have focused on models with simple underlying geometries (e.g., circle, torus), for which analytical models could be more easily constructed. How these models can be generalised to complex scenarios is, however, a non-trivial question. This is furthermore a time-sensitive issue, as population recordings from the brain in complex tasks and environments increasingly require the ability to construct such models.</p>
<p>I believe the authors do a good job of explaining the challenges related to this problem. They also propose a class of models that, although not fully general, overcome many of these difficulties while appearing solid and well-functioning. This requires some non-trivial mathematics, which is nevertheless conveyed in a reasonably accessible form. The manuscript is well written, and both the methodology and the code are well documented.</p>
<p>That said, I believe the manuscript has two major limitations, which could be addressed in a revision. First, some of the assumptions underlying this class of models are somewhat restrictive but are not sufficiently discussed. Second, although the stated goal of the manuscript is to provide practical recipes for constructing integrator networks, the methods section is not very explicit about the specific steps required for different geometries. I elaborate on these limitations below.</p>
<p>(1) The authors repeatedly describe MADE as a technique for constructing integrators of specified &quot;topologies and geometries.&quot; What do they mean by &quot;geometries&quot;? Intuitively, I would associate geometry with properties beyond topology, such as embedding dimensionality or curvature. However, it is unclear to me to what extent these aspects are explicitly specified or controlled in MADE. It seems that geometry is only indirectly defined via the connectivity kernel, which itself obeys certain constraints (e.g., limited spatial scale; see below). I believe it is important for the authors to clarify what they mean by &quot;geometry.&quot; They should also specify which aspects are under their control, and whether, in fact, all geometries can be realized.</p>
<p>(2) The authors make two key assumptions: that connectivity is purely inhibitory and that the connectivity kernel has a small spatial scale. They state that under these conditions, the homogeneous fixed point becomes unstable, leading to a non-periodic state. However, it seems to me that they do not demonstrate that this emergent state is necessarily a bump localized in all manifold dimensions -- although this is assumed throughout the manuscript. Are other solutions possible or observed? For example, might the network converge to states that are localized in one dimension but extended in another, yielding e.g., stripe-like activity in the plane rather than bumps? In other words, does the proposed recipe guarantee convergence to bumps? This is a critical point and should be clarified.</p>
<p>(3) Related to the question above: What are the failure modes when these two assumptions are violated? Does the network always exhibit runaway activity (as suggested in the text), or can other types of solutions emerge? It would be useful if the authors could briefly discuss this.</p>
<p>(4) Again, related to the question above: can this formalism be extended to activity profiles beyond bumps? For example, periodic fields as seen in grid cells, or irregular fields as observed in many biological datasets -- particularly in naturalistic environments? These activity profiles are of key importance to neuroscientists, so I believe this is an important point that should at least be addressed in the Discussion. Can MADE be naturally extended to these scenarios? What are the challenges involved?</p>
<p>(5) Line 119: &quot;Since Ïƒ is the only spatial scale being introduced in the dynamics, we qualitatively expect that a localized bump state within the ball will have a spatial scale of O(Ïƒ).&quot;
Is this statement always true? I understand that the spatial scale of the synaptic inputs exchanged via recurrent interactions (i.e., the argument of the function f in Equation 1) is characterised by the spatial scale Ïƒ. But the non-linear function f could modify that spatial scale -- for example, by &quot;cutting&quot; the bump close to its tip. Where am I wrong? Could the authors clarify?</p>
<p>(6) The authors provide beautiful intuition about the problem of constructing integrators on non-trivial topologies and propose a mathematically grounded solution using Killing vectors. Of course, solutions based on Killing vectors are more complex than those with constant offsets, which raises the question: Is the brain capable of learning and handling such complex structures? Perhaps the authors could speculate in the Discussion about the biological plausibility of these mechanisms.</p>
<p>(7) A great merit of this paper is that it provides mathematical tools for neuroscience researchers to build integrators on non-trivial geometries. I found that, although all the necessary information is present in the Methods, the authors could improve the presentation by schematizing the steps required to build each type of model. It would be extremely useful if, for each considered geometry, the authors provided a short list of required components: the manifold P, the choice of distance, and the connectivity offsets defined by the Killing vectors. Currently, this information is presented, but scattered (not grouped by geometry).</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107224.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The work by Claudi et al. presents a framework for constructing continuous attractor neural networks (CANs) with user-defined topologies and integration capabilities. The framework unifies and generalizes classical attractor models and includes simulations across a range of topologies, including ring, torus, sphere, MÃ¶bius band, and Klein bottle. A key contribution of the paper is the introduction of Killing vectors to enable integration on non-parallelizable manifolds. However, the need for Killing vectors currently appears hypothetical, as biologically discovered manifolds-such as rings and tori-do not require them.</p>
<p>Moreover, throughout the manuscript, the authors claim to be addressing &quot;biologically plausible&quot; attractor networks, yet the constraints required by their construction - such as exact symmetry, fine-tuning of weights, and idealized geometry-seem incompatible with biological variability. It appears that &quot;biologically plausible&quot; is effectively used to mean &quot;capable of integration.&quot; While these issues do not diminish the contributions of the work, they should be acknowledged and addressed more explicitly in the text. I applaud the authors for their interesting work. Below are my major and minor concerns.</p>
<p>Strengths:</p>
<p>(1) Theoretical framework for integrating CANs</p>
<p>
The paper introduces a systematic method for constructing continuous attractor networks (CANs) with arbitrary topologies. This goes beyond classical models and includes novel topologies such as the MÃ¶bius band, sphere, and Klein bottle. The approach generalizes well-known ring and torus attractor models and provides a unified view of their construction, dynamics, and integration capabilities.</p>
<p>(2) Novel use of killing vector fields</p>
<p>
A key theoretical innovation is the introduction of Killing vectors to support velocity integration on non-parallelizable manifolds. This is mathematically elegant and extends the domain of tractable attractor models.</p>
<p>(3) Insightful simulations across manifolds</p>
<p>
The paper includes detailed simulations demonstrating bump attractor dynamics across a range of topologies.</p>
<p>Weaknesses:</p>
<p>(1) Biological plausibility is overstated</p>
<p>
Despite frequent use of the term &quot;biologically plausible,&quot; the models rely on assumptions (e.g., symmetric connectivity, perfect geometries, fine-tuning) that are not consistent with known biological networks, and the authors do not incorporate heterogeneity, noise, or constraints like Dale's law.</p>
<p>(2) Continuum of states not directly demonstrated</p>
<p>
The authors claim to generate a continuum of stable states but do not provide direct evidence (e.g., Jacobian analysis with zero eigenvalues along the manifold). This weakens the central claim about the nature of the attractor.</p>
<p>(3) Lack of clarity around assumptions</p>
<p>
Several assumptions and analyses (e.g., symmetry breaking, linearity, stability conditions) are introduced without justification or overstated. The analytical rigor in discussing alternative solutions and bifurcation behavior is limited.</p>
<p>(4) Scalability to high dimensions</p>
<p>
The authors claim their method scales better than learning-based approaches. This should be better discussed.</p>
<p>Major Concerns</p>
<p>(1) Biological plausibility</p>
<p>The claim that the proposed framework is &quot;biologically plausible&quot; is misleading, as it is unclear what the authors mean by this term. Biological plausibility could include features such as heterogeneity in synaptic weights, randomness in tuning curves, irregular geometries, or connectivity constraints consistent with known biological architectures (e.g., Dale's law, multiple cell types). None of these elements is implemented in the current framework. Furthermore, it is not clear whether the framework can be extended to include such features-for example, CANs with heterogeneous connections or tuning curves. The connectivity matrix is symmetric to allow an energy-based description and analytical tractability, which is fine, but not a biologically realistic constraint. I recommend removing or significantly qualifying the use of the term &quot;biologically plausible.&quot;</p>
<p>(2) Continuum of stable states</p>
<p>
While the authors claim their model generates a continuum of stable states, this is not demonstrated directly in their simulations or in a stability analysis (though there are some indirect hints). One way to provide evidence would be to compute the Jacobian at various points along the manifold and show that it possesses (approximately) zero eigenvalues in the tangent/on-manifold directions at each point (e.g., see SÃ¡godi et al. 2024 and others). It would be especially valuable to provide such analysis for the more complex topologies illustrated in the paper.</p>
<p>(3) Assumptions, limitations, and analytical rigor</p>
<p>
Some assumptions and derivations lack justification or are presented without sufficient detail. Examples include:</p>
<p>â€¢ Line 126: &quot;If the homogeneous state (all neurons equally active) were unstable, there must exist some other stable state, with broken symmetry.&quot; Is this guaranteed? In the ring model with ReLU activation, there could also be unbounded solutions-not just bump solutions-and, in principle, there could also be oscillatory or other solutions. In general, multiple states can co-exist, with differing stability. It appears the authors only analyze the homogeneous case and do not study the stability or bifurcations of other solutions, limiting their theoretical work.</p>
<p>â€¢ Line 122: &quot;The conditions for the formation...&quot; What are these conditions, precisely? A citation or elaboration would be helpful. Why is the assumption Ïƒâ‰ªL necessary, and how does it impact the construction or conclusions?</p>
<p>â€¢ The theory relies heavily on exact symmetries and fine-tuned parameters. Indeed, in line 106, the authors write: &quot;We seek interaction weights consistent with the formation, through symmetry breaking.&quot; Is this symmetry-breaking necessary for all CANs? Or is it a limitation specific to hand-crafted models (see also below)? There is insufficient discussion of such limitations. For example, it is difficult to envision how the authors' framework might form attractor manifolds with different geometries or heterogeneous tuning curves.</p>
<p>(4) Comparison with models of learned attractors</p>
<p>
While the connectivity patterns of learned attractors often resemble classical hand-crafted models (e.g., see also Vafidis et al. 2022), this is not always the case. If initial conditions include randomness or if the geometry of the attractor deviates from standard forms, the solutions can diverge significantly from hand-designed architectures. Such biologically realistic conditions highlight the limitations the hand-crafted CANs like those proposed here. I suggest updating the discussion accordingly.</p>
<p>(5) High-Dimensional Manifolds</p>
<p>
The authors argue that their method scales better than training-based approaches in high dimensions and that it is straightforward to extend their framework to generate high-dimensional CANs. It would be useful for the authors to elaborate further. First, it is unclear what k refers to in the expression k^M used in the introduction. Second, trained neural networks seem to exhibit inductive bias (e.g., Cantar et al. 2021; Bordelon &amp; Pehlevan 2022; Darshan &amp; Rivkind 2022), which may mitigate such scaling issues. To support their claim, the authors could also provide an example of a high-dimensional manifold and show that their framework efficiently supports a (semi-)continuum of stable states.</p>
</body>
</sub-article>
</article>