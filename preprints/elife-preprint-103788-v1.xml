<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">103788</article-id>
<article-id pub-id-type="doi">10.7554/eLife.103788</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.103788.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Differential destinations, dynamics, and functions of high- and low-order features in the feedback signal during object processing</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0009-8171-2184</contrib-id>
<name>
<surname>Hou</surname>
<given-names>Wenhao</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5547-923X</contrib-id>
<name>
<surname>He</surname>
<given-names>Sheng</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>hes@ibp.ac.cn</email>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Zhang</surname>
<given-names>Jiedong</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>zhangjiedong@gmail.com</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/048wy7h78</institution-id><institution>State Key Laboratory of Brain and Cognitive Science, Institute of Biophysics, Chinese Academy of Sciences</institution></institution-wrap>, <city>Beijing</city>, <country>China</country></aff>
<aff id="a2"><label>2</label><institution>Institute of AI, Hefei Comprehensive National Science Center</institution>, <city>Hefei</city>, <country>China</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qbk4x57</institution-id><institution>University of Chinese Academy of Sciences</institution></institution-wrap>, <city>Beijing</city>, <country>China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Kok</surname>
<given-names>Peter</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Moore</surname>
<given-names>Tirin</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University, Howard Hughes Medical Institute</institution>
</institution-wrap>
<city>Stanford</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="original-publication" iso-8601-date="2024-12-23">
<day>23</day>
<month>12</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP103788</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-11-01">
<day>01</day>
<month>11</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-11-01">
<day>01</day>
<month>11</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.11.01.621525"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Hou et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Hou et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-103788-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Brain is a hierarchical information processing system, in which the feedback signals from high-level to low-level regions are critical. The feedback signals may convey complex high-order features (e.g., category, identity) and simple low-order features (e.g., orientation, spatial frequency) to sensory cortex to interact with the feedforward information, but how these types of feedback information are represented and how they differ in facilitating visual processing is unclear. The current study used the peripheral object discrimination task, 7T fMRI, and MEG to isolate feedback from feedforward signals in human early visual cortex. The results showed that feedback signals conveyed both low-order features natively encoded in early visual cortex and high-order features generated in high-level regions, but with different spatial and temporal properties. The high-order feedback information targeted both superficial and deep layers, whereas the low-order feedback information reached only deep layers in V1. In addition, MEG results revealed that the feedback information from occipito-temporal to early visual cortex emerged around 200 ms after stimulus onset, and only the representational strength of high-order feedback information was significantly correlated with behavioral performance. These results indicate that the complex and simple components of feedback information play different roles in predictive processing mechanisms to facilitate sensory processing.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>As sensory information is processed in the primate brain, the neural system actively interprets the sensory input to generate a consistent and meaningful perception. This process relies on the interplay between feedforward and feedback processes in the neural processing hierarchy. Anatomically, feedback connections are widely distributed in the visual cortex<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. Functionally, behavioral and neural evidence has shown that neural feedback to early visual cortex is critical for many visual functions, such as object recognition and visual awareness<sup><xref ref-type="bibr" rid="c2">2</xref>–<xref ref-type="bibr" rid="c4">4</xref></sup>. However, limited knowledge of the nature of the feedback information hinders our understanding of how neural feedback interacts with visual input.</p>
<p>Significant transformations of neural representations occur across the visual processing hierarchy. Compared to early visual cortex, where neurons encode low-order features like orientation and spatial frequency<sup><xref ref-type="bibr" rid="c5">5</xref></sup>, neurons in higher-level cortex have larger receptive fields and more complex tuning of high-order features<sup><xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c7">7</xref></sup>, such as category and identity. What kind of information is conveyed by feedback signals? Based on predictive coding theory<sup><xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c9">9</xref></sup>, the feedback signal is compared to the input signal to generate prediction errors for further processing. However, the high-order representation is more abstract and invariant, and a feedback signal with such information may not be feasible for a direct “comparison”. For an effective comparison, it is reasonable to assume that the feedback signal should be in the form of low-order predicted information. However, previous evidence suggests that object category information could be decoded in the feedback signals<sup><xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c11">11</xref></sup>, it is unclear whether this category decoding originated from high-order object category information, or was driven by low-order (i.e., object shape and orientation) information<sup><xref ref-type="bibr" rid="c12">12</xref></sup>. Therefore, it is crucial to dissociate these two types of feature and investigate what kind of information is conveyed in feedback signals during object processing.</p>
<p>A notable feature of the primate brain is the presence of layered structures, with each layer serving a distinct function<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c13">13</xref>–<xref ref-type="bibr" rid="c15">15</xref></sup>. In the early visual cortex, feedforward signals are directed towards the middle layers, while both deep and superficial layers receive feedback signals. The output signals to downstream regions are generated in the superficial layer. The feedback information may originate from object processing areas in temporal cortex, but in the early visual cortex, the laminar profiles of the high- and low-order information in feedback signals are unknown. It is important to clarify where the different types of input and feedback information arrive in early visual cortex.</p>
<p>Plenty of evidence indicates that feedback signals can enhance visual processing efficiency, and further optimize behavioral performance<sup><xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup>. However, the behavioral implications of high- and low-order feedback information remain unclear. It is important to examine whether and when each type of feedback information correlates with behavioral performance during visual processing.</p>
<p>Examining neural information in the feedback signal can be challenging due to the highly tangled distribution of the feedforward and feedback signals during visual processing in the cortex. To isolate the feedback signals, we took advantage of a peripheral object recognition paradigm. Previous studies have shown that when objects are presented in the visual periphery for identification, their information is fed back to foveal V1<sup><xref ref-type="bibr" rid="c10">10</xref></sup>. The spatial separation between the input signal and feedback signal in the retinotopic visual cortex enabled the extraction of various types of feature information in the feedback signal without interference from feedforward visual input. Additionally, the high spatial resolution of 7T MRI and high temporal resolution of MEG recording allowed for the evaluation of laminar profiles and temporal dynamics of feedback signals respectively, as well as information flow among different cortical regions. The results indicate that, in addition to the low-order features, feedback signals also transmit high-order category information to V1. Importantly, the high-order information cannot be observed in feedforward processing in V1.</p>
<p>Interestingly, high- and low-order feedback information exhibit different laminar profiles. The quality of high-order information in feedback signal also shows a significant correlation with behavioral performance. The high-order information originates from object processing regions in the occipito-temporal cortex approximately 200 ms after visual input. Apparently, high-order feedback information from downstream regions and its interaction with locally encoded low-order information may be critical for efficient visual processing.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Orientation and category information in the feedback signal to V1 during peripheral object processing</title>
<p>In Experiment 1, peripheral object task was used during the 7T fMRI scan. The stimuli were novel objects from two categories (smoothie and cubie) and two orientations (vertical and horizontal) (see <xref rid="fig1" ref-type="fig">Figure 1A</xref>). In each trial, two objects from the same category with the same orientation were briefly presented (100 ms) in the peripheral visual field (7 degrees from fixation) (see <xref rid="fig1" ref-type="fig">Figure 1B</xref>). Participants were asked to judge whether the two objects were identical while maintaining fixation. To optimize fMRI signal in the experimental runs, a block design was used, with each block consists of eight trials testing objects from the same category and orientation (see <italic>Materials and Methods</italic> for more details).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Feedback information to foveal V1 in the peripheral object task (n=18).</title><p>(<bold>A</bold>) Four types of objects (two orientations x two object categories) were used in the experiment. (<bold>B</bold>) Two objects from the same type were presented in the periphery and participants had to discriminate whether they were identical. (<bold>C</bold>) Strong fMRI responses were found in peripheral V1 corresponding to the stimulus locations and high-level regions, but not in foveal V1. (<bold>D</bold>) Schematic representation of feedforward and feedback connections in different cortical layers of V1. (<bold>E</bold>) High-order category information from feedback signals could be decoded in foveal, but not peripheral, V1. (<bold>F</bold>) The category information could be decoded in superficial and deep layers of foveal V1, but absent in all layers of peripheral V1. (<bold>G-H</bold>) Low-order orientation information from feedback signals could be decoded only in the deep layer of foveal V1 and across layers in peripheral V1. Error bars reflect ±1 SEM. * indicates paired t-test with significance of p&lt; 0.05. ** indicates paired t-test with significance of p&lt; 0.01. † indicates marginal significance. The dashed line represents the chance level of decoding performance.</p></caption>
<graphic xlink:href="621525v1_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In addition to the twelve experimental runs, three localizer runs were included in the scan to identify regions of interest (ROIs) in each hemisphere. In one of the localizer runs, flashing checkerboards were presented either at the periphery where objects were presented during the task or at the fovea. ROIs of peripheral V1 and foveal V1 were identified using contrast maps between these two conditions. In the other two localizer runs, everyday objects and scrambled objects were presented in different blocks while participants performed a one-back task. These conditions were used to identify two high-level object processing regions: the lateral object complex (LOC)<sup><xref ref-type="bibr" rid="c18">18</xref></sup> in the ventral pathway and the posterior intraparietal sulcus (pIPS)<sup><xref ref-type="bibr" rid="c19">19</xref></sup> in the dorsal pathway.</p>
<p>The neural responses based on BOLD signal in the four ROIs were estimated. Univariate analysis shows significant fMRI responses in three ROIs (peripheral V1, LOC, pIPS) during the task, while no significant fMRI response was observed in foveal V1 (t(17)=-0.09, p=0.93) (<xref rid="fig1" ref-type="fig">Figure 1C</xref>) due to the absence of visual input in the fovea.</p>
<p>Multivariate analysis (Linear support vector machine, SVM) was used to evaluate the information encoded in each ROI by classifying the neural response patterns elicited by objects from different conditions. The neural patterns were classified based on low-order orientation information (horizontal or vertical) or high-order category information (cubie or smoothie). The classification accuracy was used to indicate the existence of information in each ROI.</p>
<p>Although the average fMRI responses in foveal V1 did not differ from the baseline, the neural response patterns there supported significantly above chance decoding accuracies of high-order categories (t(17)=3.14, p=0.003) (see <xref rid="fig1" ref-type="fig">Figure 1E</xref>). The object category information was also examined in other cortical regions. The decoding performances were significantly above chance in the LOC and pIPS (ts&gt;1.94, ps&lt;0.04), but not in the peripheral V1 (t(17)=-1.19, p=0.87) (see <xref rid="fig1" ref-type="fig">Figure 1E</xref>).</p>
<p>Further, the 7T fMRI’s sub-millimeter resolution enabled us to examine the laminar profiles of cortical regions. Previous neurophysiology studies have shown that in V1, feedforward and feedback signals differentially target different layers. Here, based on anatomical images of each participant, the gray matter was segmented into deep, middle, and superficial layers (<xref rid="fig1" ref-type="fig">Figure 1D</xref>). The neural responses in these layers from each ROI were extracted and analyzed separately.</p>
<p>In foveal V1, the high-order category information could be decoded in the superficial and deep layers (ts&gt;2.37, ps&lt;0.02), but not in the middle layer (t(17)=-0.62, p=0.73). In peripheral V1, none of the layers showed significant decoding performance (ts&lt;0.10, ps&gt;0.46)(<xref rid="fig1" ref-type="fig">Figure 1F</xref>). While the chance level decoding performance in peripheral V1 rejects the possibility that the high-order information is laterally communicated from peripheral V1, the laminar profile of information in foveal V1 more clearly demonstrates that high-order category information was transmitted to the deep and superficial layers of foveal V1 via feedback connections. In the two high-level regions LOC and pIPS, category information was successfully decoded in the middle and superficial layers (ts&gt;1.92, ps&lt;0.04), suggesting that the category information was present in the middle layer in these regions and further processed and outputted to other cortical regions through the superficial layer.</p>
<p>For the low-order orientation information, the decoding performance was significant in peripheral V1 and LOC (ts&gt;2.32, ps&lt;0.02), but not in pIPS (t(17)=0.00, p=0.50) (<xref rid="fig1" ref-type="fig">Figure 1G</xref>). In foveal V1, the orientation decoding performances were marginally significant (t(17)=1.50, p=0.08). Further layer analysis showed that in foveal V1, significant orientation decoding was observed in the deep layer (t(17)=2.43, p=0.01), but not in the middle and superficial layers (ts&lt; 0.98, ps&gt;0.17) (<xref rid="fig1" ref-type="fig">Figure 1H</xref>). In peripheral V1 where stimuli were presented, not surprisingly, significant orientation decoding performances were observed in the middle and superficial layers (ts&gt;3.06, ps&lt;0.004). In the two high-level visual regions, orientation information was only significant in the deep layer of LOC (t(16)=2.26, p=0.02) . The results indicate the presence of orientation information in the feedback signal in foveal V1, however the strength of the low-order orientation information appears to be weaker and less extensive across cortical layers compared to the high-order category information.</p>
</sec>
<sec id="s2b">
<title>High-order category information in V1 not driven by low-level features</title>
<p>Is the successful decoding of category information observed in foveal V1 truly driven by high-order information in the feedback signals, or could it be driven by some low-level confounding features embedded in the stimuli that are natively encoded in V1? To address this concern, Experiment 2 was conducted to examine the visual representation in foveal V1 with stimuli directly presented at the fovea, thus providing strong representation of low-level features in the feedforward processing. In addition, during the scan, participants were asked to perform a one-back task regarding the fixation color. This manipulation makes the object stimuli task-irrelevant, minimizing the feedback signal during object processing (<xref rid="fig2" ref-type="fig">Figure 2A</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Object representation in foveal V1 during a fixation task (n=18).</title><p>(<bold>A</bold>) The object stimulus was presented at the fovea as a task-irrelevant stimulus and participants performed a 1-back task to the fixation color. (<bold>B-C</bold>) Category information could not be decoded in foveal V1. (<bold>D-E</bold>) Orientation information could be decoded in all layers of foveal V1. Error bars reflect ±1 SEM. * indicates paired t-test with significance of p&lt; 0.05. ** indicates paired t-test with significance of p&lt; 0.01. The dashed line represents the chance level of decoding performance.</p></caption>
<graphic xlink:href="621525v1_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Decoding analysis revealed significant orientation information in foveal V1 (t(17)=6.94, p&lt;0.001), but no category information could be detected there (t(17)=0.38, p=0.35). Not surprisingly, there is significant category information in the LOC (t(16)=2.48, p=0.01) (<xref rid="fig2" ref-type="fig">Figure 2B</xref>&amp;D). Further layer analysis revealed that category information was significant in the middle and superficial layers of LOC (ts&gt;1.92, ps&lt;0.04), while orientation information was significant in all three layers of foveal V1 (ts&gt;5.35, ps&lt;0.001) (<xref rid="fig2" ref-type="fig">Figure 2C</xref>&amp;E). No significant category information was observed in either layer of the foveal V1 (ts&lt;0.80, ps&gt;0.21), indicating that without task-driven feedback signals, feedforward processing alone was insufficient to generate category information in foveal V1. These observations support the interpretation that the category information in foveal V1 observed in the peripheral object discrimination task was coming from the feedback signals.</p>
</sec>
<sec id="s2c">
<title>Temporal dynamics of feedback category and orientation information</title>
<p>The two 7T fMRI experiments revealed distinct neural representations conveyed in the feedback signal and their laminar profiles in V1. In Experiment 3, the temporal dynamics of high- and low-order feature representations in the feedback signal were examined using MEG. The MEG experiment included two tasks, the peripheral object task (as in Experiment 1) and the foveal object task (as in Experiment 2) (see <italic>Materials and Methods</italic> for more details). The order of the two task blocks was counterbalanced across participants.</p>
<p>In the peripheral object experiment (<xref rid="fig3" ref-type="fig">Figure 3A</xref>), to examine the neural representations of high- and low-order information, we trained and tested SVMs for them at each time point after the stimulus onset (see <italic>Materials and Methods</italic>). Source localization analysis was applied to map the MEG signals from the sensors to the cortex in each participant. Dynamic statistical parametric mapping (dSPM)<sup><xref ref-type="bibr" rid="c20">20</xref></sup> was used to extract neural response patterns in three cortical regions: the early visual cortex, the occipito-temporal cortex, and the posterior parietal cortex. The limited spatial resolution of the source localization method made it difficult to separate the neural responses between foveal and peripheral regions in the early visual cortex. To extract the neural representations in the foveal region in the early visual cortex, we applied cross-location decoding analysis to the data from the peripheral object task. During the task, two objects were presented in each trial, either at the upper-right and lower-left visual fields or at the upper-left and lower-right visual field, thus the visual information was sent to different retinotopic areas in the early visual cortex during feedforward processing. The neural representations elicited by feedforward processing from the two pairs of locations are spatially separated and could not be generalized across locations. However, during feedback processing, object information was fed back to the same foveal cortex, allowing for generalization of feedback neural representation across different peripheral locations. Here, when the decoders were trained and tested using data from objects presented at the same locations, the results showed that both category and orientation information could be decoded from all three cortical regions (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). Next, the decoders were trained and tested using data from different locations, and the decoding performance generally decreased in all cortical regions (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). To compare the temporal dynamics between same-location and cross-location decoding, we normalized the peak performances of the same-location decoding and cross-location decoding (<xref rid="fig3" ref-type="fig">Figure 3D</xref>). The results indicate that, for both category and orientation information in the early visual cortex, decoding performances were substantially delayed for cross-location data compared to the same-location data. This is consistent with the idea that feedback signals in the early visual cortex, which were slower than feedforward signals, supported the generalized cross-location decoding. In contrast, the dynamics of decoding performances showed location invariance in occipito-temporal cortex. In posterior parietal cortex, delayed performance was observed for cross-location decoding, possibly due to the extensive retinotopic representations in parietal cortex. Further latency analysis, which estimated the time from stimulus onset to 75% of peak decoding performance, showed consistent results that for both high- and low-order information, the latency was shortest in early visual cortex for same-location decoding, but was shortest in occipito-temporal cortex for cross-location decoding (Figure S1).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Temporal dynamics of object representation in different cortical regions (n=15).</title><p>(<bold>A</bold>) In the MEG experiment, two objects of the same type were presented in the periphery. The locations of the objects changed across trials. (<bold>B-C</bold>) Using the source localization algorithm, the neural responses of three cortical regions were extracted. In all regions, both category and orientation information could be decoded either within the same location set (<bold>B</bold>) or across different location sets (<bold>C</bold>). (<bold>D</bold>) Same-location and cross-location decoding performances were normalized and plotted together to facilitate their comparison. The dynamics of decoding performance were significantly delayed for cross-location decoding in early visual cortex (solid red and blue lines). The dashed black line represents the chance level of decoding performance. The gray bar on the time axis indicates the presentation time of the object stimuli. Colored shaded areas reflect ±1 SEM. The colored bars below the time courses indicate the significance (p&lt;0.01) of the decoding accuracy in a cluster permutation test. See also Figure S1.</p></caption>
<graphic xlink:href="621525v1_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2d">
<title>The transmission and behavioral relevance of high- and low-order feedback information</title>
<p>To evaluate the transmission of high- and low-order information between different cortical regions, Granger causality analysis was performed on the temporal dynamics of neural representation. First, the strength of neural representation at each time point in each ROI was estimated by calculating the distance from the neural response pattern to the decoding hyperplane in the high-dimensional space for each time point in each trial<sup><xref ref-type="bibr" rid="c21">21</xref></sup>. Then the Granger causality analysis tested whether the variation in neural representation strength at the current time in the target ROI could be explained by past neural representations in a source ROI, beyond the explanation provided by past representations in the target ROI itself (<xref rid="fig4" ref-type="fig">Figure 4A</xref>)<sup><xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref></sup>.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Information transmission between cortical regions revealed by Granger causality analysis (n=15).</title><p>(<bold>A</bold>) The strength of representation was estimated by the distance from the neural response pattern to the decoding hyperplane. The dynamics of representation strength were used to estimate information transmission between cortical regions. (<bold>B-C</bold>) The Granger causality of feedforward (blue) and feedback (yellow) category (<bold>B</bold>) and orientation (<bold>C</bold>) information between three cortical regions during peripheral object discrimination task. (<bold>D-E</bold>) The Granger causality of feedforward and feedback information of category (<bold>D</bold>) and orientation (<bold>E</bold>) during a fixation detection task with an object presented at the fovea. The directions and initial timings of information flow between cortical regions were also indicated by arrows and the onset times next to the arrows. The gray bar on the time axis indicates the presentation time of the object stimuli. The colored shaded areas reflect ±1 SEM. The colored bars below the time courses indicate the significance (p&lt;0.01) of Granger causality in a cluster permutation test.</p></caption>
<graphic xlink:href="621525v1_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In the peripheral object task, significant Granger causality in the feedforward direction was found for high-order category information from early visual cortex to occipito-temporal cortex (190 ms) and to posterior parietal cortex (100 ms). Notably, significant Granger causality was observed in the feedback direction from occipito-temporal cortex to early visual cortex emerging around 220 ms, and from posterior parietal cortex to early visual cortex emerging at 520 ms (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). For low-order orientation information, significant Granger causality in the feedforward direction was found from early visual cortex to occipito-temporal cortex and posterior parietal cortex, both emerging around 140 ms. For the feedback direction, significant Granger causality was observed from occipito-temporal cortex to early visual cortex emerging around 250 ms (<xref rid="fig4" ref-type="fig">Figure 4C</xref>). These results further support that feedback signals from occipito-temporal cortex to early visual cortex contain both high- and low-order information. The broad temporal overlap between feedforward and feedback processes, especially for the high-order category information, is consistent with a recurrent processing for visual object recognition.</p>
<p>In the foveal object task, feedforward category information was observed from the early visual cortex to both occipito-temporal cortex (200 ms) and posterior parietal cortex (310 ms). Feedback category information to the early visual cortex was observed from the occipito-temporal cortex (160 ms) and posterior parietal cortex (360 ms) (<xref rid="fig4" ref-type="fig">Figure 4E</xref>). For orientation information in the feedforward direction, significant Granger causality was found from early visual cortex to occipito-temporal cortex (120 ms) and to posterior parietal cortex (110 ms). For the feedback orientation information, significant Granger causality was observed from occipito-temporal cortex to early visual cortex emerging at 290 ms (<xref rid="fig4" ref-type="fig">Figure 4D</xref>). These results suggest that, for foveally presented objects, the feedforward-feedback recurrent processes between occipito-temporal cortex and early visual cortex occur in a narrower time window.</p>
<p>After tracking the dynamic transmission of feedback signals, we also examined the behavioral relevance of different types of information in the feedback signal at each time point to uncover the contribution of feedback signals to behavioral performance. The reaction time was recorded for each trial during the MEG session, which allowed us to calculate its correlation with the strength of time-resolved high-order and low-order information in the feedback signal during the peripheral object discrimination task. Similar to Granger causality analysis, the distance from the neural response pattern to the decoding hyperplane was used to estimate the strength of neural representation (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). To concentrate on the feedback signal in early visual cortex, we trained the hyperplane with data from trials in which objects were presented at different locations from the current trial (i.e., cross-location decoding). For each time point, the correlation between the reaction time and the neural representation strength across different trials was calculated for each participant (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). The positive correlation indicated that better representation quality linked with faster reaction time. The group-averaged results revealed significant behavioral correlations in the early visual cortex for high-order category information, emerging around 210 ms after stimulus onset, which is consistent with the results from Granger causality analysis. Additionally, significant behavioral correlations were observed in the occipito-temporal cortex (190 ms) (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). However, no significant behavioral correlation was observed in the early visual cortex for low-order orientation information. Apparently, neither high- nor low-order information in the parietal cortex was correlated with behavioral response.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Behavioral relevance of feedback information in the visual cortex (n=15).</title><p>(<bold>A</bold>) The strength of information representation at each time point of each trial was estimated by cross-location decoding, its correlation with reaction time across trials was calculated to estimate its behavioral relevance. (<bold>B</bold>) Significant behavioral relevance was observed for category information in early visual cortex between 200 and 400 ms after stimulus onset, consistent with the time window of feedback signals. Colored shaded areas reflect ±1 SEM. The colored bars below the time courses indicate the significance (p&lt;0.05) of the correlation in a cluster permutation test. (<bold>C</bold>) Schematic summary depiction of information flow among key cortical regions (top) and different types of feedback information from temporal cortex to different cortical layers of V1 (bottom).</p></caption>
<graphic xlink:href="621525v1_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Understanding what types of information are conveyed by feedback signals is critical to the investigation of the computational mechanism of feedback processing. Our results demonstrate that the feedback signals to V1 convey both low-order and high-order information. The orientation information, which is considered natively encoded in V1, could be detected in the feedback signals. The observation supports the notion that the feedback process elicits comparable neural representations to those of the feedforward process, enhancing processing efficiency by predicting the input signals<sup><xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c9">9</xref></sup>. Notably, reliable representations of high-order category information were also detected in the feedback signals to V1, while this information could not be observed in V1 in a feedforward dominated process. These results firmly establish that the category information in V1 originated from a high-level region in the visual hierarchy, rather than generated locally in V1. This suggests that during the recurrent processes for object identification, high-level regions communicate feedback signals to V1, not only contain local V1-native features that enable predictive error encoding, but also communicates more complex and invariant neural information to constrain the predictive processing.</p>
<p>Moreover, the current study found that the laminar profile of high-order information differed from that of low-order information in the feedback signals (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). Orientation representation was observed in the deep layer, while high-order category representation was found in both deep and superficial layers. The deep and superficial layers of V1 have different roles in visual processing. Both deep and superficial layers receive feedback signals from downstream high-level regions, but the superficial layer also generates output signals for further processing downstream. Specifically, within the superficial layer, output signals are generated in layers 2/3 and sent to the downstream regions targeting layer 4. Meanwhile, layer 1 receives feedback signals, that can modulate the output signals in layers 2/3<sup><xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c25">25</xref></sup>. Our study suggests that the orientation information in feedback signals may reflect the mechanisms predicting the input signal during object processing. The predicted orientation representation in deep layer of V1 was consistent with previous findings that the neural representations of expected orientation was only observed in the deep layer of V1<sup><xref ref-type="bibr" rid="c26">26</xref></sup>. This laminar profile was also observed in the V1 neural responses to subjective contours in illusory Kanizsa figures<sup><xref ref-type="bibr" rid="c27">27</xref></sup>, supporting the predicting coding theory. The high-order category information, which was shown to be fed back to both the superficial and deep layers, may supports additional mechanisms during the feedback processing. The category information observed in the superficial layer may arise from layer 1, and its function could be to modulate the output signals in layers 2/3. The feedback information to the superficial layer may be linked to task-driven top-down modulation, as similar laminar profiles have been observed previously in top-down attention and working memory tasks<sup><xref ref-type="bibr" rid="c28">28</xref>–<xref ref-type="bibr" rid="c31">31</xref></sup>. Therefore, we hypothesize that the high-order information in the feedback signal, especially in the superficial layer, guides and constrains feedforward processing to enhance processing efficiency in the object identification task. The functional relevance of the feedback signals, especially the high-order information, was also demonstrated in the MEG results, which consistently showed a significant trial-to-trial correlation between the high-order feedback information and the behavioral performances.</p>
<p>The next question is how high-order information was encoded in the early visual cortex. Recent two-photon imaging has revealed the existence of neurons in the superficial layer of V1 that are selective to complex patterns and features<sup><xref ref-type="bibr" rid="c32">32</xref>–<xref ref-type="bibr" rid="c35">35</xref></sup>. Although the distribution of such complex-feature selective neurons is very sparse, they likely play an important role in the representation and utilization of high-order feedback information. Meanwhile, in accordance with the sparse distribution of the complex feature selective neurons in V1, the feedback signals did not significantly increase the general fMRI response amplitude in foveal V1.</p>
<p>Currently, the response dynamics and connectivity patterns of these V1 neurons are unclear, future studies should compare high-order and low-order information dynamics across different cortical layers to further advance our understanding of how high-order feedback information interacts with feedforward visual processing to increase efficiency.</p>
<p>The Granger causality analysis from MEG results reveals that the backward information transition of both high-order category information and low-order orientation information from the occipito-temporal cortex to the early visual cortex, which emerged approximately 200 ms after the peripheral object onset. Consistently, the behavioral correlation results show that high-order information in the early visual cortex began to correlate with behavioral performance around 200 ms. These results suggest that functional-relevant high-order information reached foveal V1 approximately 200 ms after stimulus onset in the peripheral object identification task. The temporal dynamics of feedback information in this study is consistent with previous observations<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c37">37</xref></sup>. However, it is important to note that the temporal dynamics of the feedback process may vary depending on the visual inputs and task demands. On one hand, low-quality visual input, such as low-contrast or occluded objects, may delay the initial feedforward and model generating process<sup><xref ref-type="bibr" rid="c38">38</xref>–<xref ref-type="bibr" rid="c40">40</xref></sup>, which in turn delays the feedback signals. On the other hand, Previous research has shown that the foveal feedback mechanism exhibits some degree of flexibility. For instance, increasing the high-level operation of the peripheral task can delay the temporal dynamics of the foveal feedback<sup><xref ref-type="bibr" rid="c11">11</xref></sup>.</p>
<p>Additionally, there is also evidence that feedback process operates differently for peripheral vision compared with central vision, with the Central-peripheral Dichotomy theory proposing a weaker feedback for peripheral vision<sup><xref ref-type="bibr" rid="c41">41</xref></sup>. Therefore there are multiple factors that may lead to temporal variations of the feedback signal<sup><xref ref-type="bibr" rid="c16">16</xref></sup>.</p>
<p>In the current study, the peripheral object identification task required processing of both category and orientation information, and the results showed that both kinds of information were fed back to early visual cortex. Evidence has shown that the foveal feedback occurrence depends on the requirement for distinguishing fine object details in the periphery<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c42">42</xref></sup>. Object category feedback information was not observed in foveal V1 when participants were distinguishing between colors instead of distinguishing between object identities<sup><xref ref-type="bibr" rid="c10">10</xref></sup>. It is apparent that the feedback mechanism could flexibly select the information, especially the high-order information, sent back to the early visual cortex depending on the task requirements. However, it is unclear whether low-order information conveyed in the feedback signals is more independent of the task demand. Supposing the function of the low-order feedback information is to support the realization of predictive coding in early visual cortex, then the low-order information could be more intrinsic and task-invariant in feedback signals.</p>
<p>The hierarchical structure and extensive feedback connections are key features of the human neural system. Feedback signals are believed to be important for efficient neural processing, but the algorithm of these signals in neural computation is largely unknown. Our findings dissociated various types of information in the feedback signals, uncovered their laminar profiles, and traced their temporal dynamics across the cortical hierarchy. These findings reveal the multiple components in the feedback signals, and contribute to the comprehensive understanding of the interactive feedforward and feedback computational mechanisms, a key feature in human intelligence.</p>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>In the fMRI experiments, 22 participants were recruited, all of whom had normal or corrected-to-normal vision. In the data analysis, 4 participants were excluded due to excessive head movement. Therefore, data from 18 effective participants (10 females; aged 19-29) was included in the results. The MEG experiment recruited 15 participants (10 females; aged 19-29). Before the experiments, all participants gave written consent, and the institutional review board of the Institute of Biophysics, Chinese Academy of Sciences approved the protocols (#2017-IRB-004).</p>
</sec>
<sec id="s4b">
<title>fMRI experiments Stimuli and procedures</title>
<p>OpenGL was used to generate object stimuli. The stimuli consisted of two categories (cubie vs. smoothie) and two orientations (vertical vs. horizontal). For each category, 18 individual objects were created.</p>
<p>During the peripheral object task, objects from the same condition were presented in the upper left and lower right visual field, 7° away from the fixation. The object size was either 3° x 1.5° or 1.5° x 3° depending on the stimulus conditions. Objects were presented for 100 ms in each trial, followed by 1900 ms of blank screen. Participants were asked to judge whether the two objects were identical. To maximize the fMRI signals, block design was used, with each block containing 8 trials from the same condition. During each block, the fixation was presented for 200 ms repeatedly, with 300 ms interval between two fixations. The color of the fixation changed randomly, but this should be ignored in the peripheral task.</p>
<p>In the foveal task of the fMRI experiment, an object of the same size as in the peripheral task was presented at foveal for 100 ms, followed by a 1900 ms interval. All other parameters were the same as in the peripheral task. The participants completed a one-back task where they had to press a button when the fixation color was repeated between two adjacent presentations.</p>
<p>The fMRI experiment contained 12 task runs. Each run included 8 blocks, with 1 block from each condition in each task, interleaved with 12 second blank periods. The peripheral and foveal task blocks were presented alternately, and the orders were counterbalanced across runs. The orders of the object condition were pseudo-random across runs. The accuracy of behavioral responses in the peripheral object task was 67.4%±4.4%.</p>
<p>In addition to the task runs, three localizer runs were included in the scan. In one of the runs, flashing checkerboards with a size of 3° were presented in different blocks at either the foveal or peripheral visual field, which was used to localize the ROIs of foveal and peripheral V1.</p>
<p>The locations of the checkerboards matched the object locations in the task runs. The other two runs were used to localize LOC and pIPS. Everyday object images and phase-scrambled images were presented in different blocks.</p>
</sec>
<sec id="s4c">
<title>MRI scanning and preprocessing</title>
<p>MRI data were collected on a Siemens Magnetom 7 Tesla MRI system (passively shielded, 45mT/s slew rate) (Siemens, Erlangen, Germany), with a 32-channel receive 1-channel transmit head coil (NOVA Medical, Inc, Wilmington, MA, USA), at the Beijing MRI Center for Brain Research (BMCBR). High-resolution T1-weighted anatomical images (0.7 mm isotropic voxel size) were acquired with a MPRAGE sequence (256 sagittal slices, acquisition matrix = 320 × 320, Field of view (FOV) = 223 × 223 mm, GRAPPA factor = 3, TR = 4000 ms, TE = 3.05 ms, TI = 0 ms, flip angle = 0°, pixel bandwidth = 240 Hz per pixel). GE-EPI sequences were used to collect functional data in the main experiment (TR = 2000 ms, TE = 23 ms, 0.8 mm isotropic voxels, FOV = 128 × 128 mm, GRAPPA factor = 3, partial Fourier 6/8, 31 slices of 0.8 mm thickness, flip angle is about 80, pixel bandwidth = 1157 Hz per pixel). During the scan, GE-EPI images with reversed phase encoding direction from experiment functional scan were collected to correct the spatial distortion of EPI images.</p>
<p>MRI image data were analyzed with FreeSurfer (CorTechs Inc, Charlestown, MA, USA)<sup><xref ref-type="bibr" rid="c43">43</xref></sup>, AFNI (<ext-link ext-link-type="uri" xlink:href="http://afni.nimh.nih.gov">http://afni.nimh.nih.gov</ext-link>)<sup><xref ref-type="bibr" rid="c44">44</xref></sup>, and custom codes. For anatomical data, to reconstruct the cortical surfaces, anatomical data were further processed by FreeSurfer, including gray and white matter segmentation and identification of V1 region. SUMA and custom Python/MATLAB codes were used to generate equi-volume surfaces (<ext-link ext-link-type="uri" xlink:href="https://github.com/herrlich10/mripy">https://github.com/herrlich10/mripy</ext-link>). For each voxel, its volume percentages of WM, CSF, and different cortical layers (deep, middle, superficial) were calculated. For function data, preprocessing included slice-timing correction, motion correction, distortion correction using reversed-phase encoding EPI images, and intensity normalization. An additional spatial smoothing with a 2 mm Gaussian kernel was applied to the localizer data. Beta values of stimulus-evoked responses were estimated with GLM for each voxel.</p>
</sec>
<sec id="s4d">
<title>Regions of interest</title>
<p>Foveal V1 and peripheral V1 were defined on the surface of each participant, with contrast between the foveal and peripheral checkerboards (p&lt;0.01, uncorrected). The anatomical labeling of V1 was used to constrain the spatial locations of the ROIs. The LOC was defined as regions responding more strongly to everyday objects than to scrambled objects in ventral occipital-temporal cortex. The pIPS was located in the Intraparietal Sulcus, which responded more strongly to real objects than to the rest conditions. All ROIs were defined on the gray matter surface and then converted to volume. For the layer analysis, the voxels in the ROI could be further classified into deep, middle, and superficial layers based on their dominant volume percentages generated from the anatomical data analysis. Due to limited FOV size of the EPI sequence, the LOC was not covered in one participant.</p>
</sec>
<sec id="s4e">
<title>Correcting the vasculature-related signals</title>
<p>Two analyses were performed to remove vasculature-related signals. The distribution of beta values in each ROI was fitted by two Gaussian distributions. Voxels that fell into the higher response Gaussian distribution were excluded from further analyses<sup><xref ref-type="bibr" rid="c45">45</xref></sup>. Second, the mean EPI signal was calculated for each voxel and the spatial trend of EPI signal was removed in each ROI. Similar to the beta values analysis, two Gaussian distributions were used to fit the distribution of EPI signal in each ROI. Voxels falling into the lower response Gaussian distribution were also excluded from further analyses<sup><xref ref-type="bibr" rid="c45">45</xref></sup>.</p>
</sec>
<sec id="s4f">
<title>Decoding analysis</title>
<p>For each ROI and each participant, a linear classifier (<ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/~cjlin/libsvm">http://www.csie.ntu.edu.tw/~cjlin/libsvm</ext-link>)<sup><xref ref-type="bibr" rid="c46">46</xref></sup> was trained to classify the neural response patterns from different conditions. To estimate the decoding performances, supervised learning and a leave-one-run-out cross-validation approach were used. The neural response patterns of the same condition were averaged in each run. Before each training procedure for each ROI, the top 500 voxels were selected based on their t-value between two conditions.</p>
<p>For the layer analysis in each ROI, the top 300 voxels were selected for each layer. If a layer had fewer than 300 voxels, then all the voxels were included in the training procedure.</p>
</sec>
<sec id="s4g">
<title>MEG experiment Stimuli and procedures</title>
<p>The MEG experiment used similar stimuli and tasks as the fMRI experiment. For the peripheral object task, two objects were presented in the peripheral visual fields (7 ° from fixation) for 100 ms in each trial. The object locations were either upper left and lower right or upper right and lower left, which was random across trials. Participants had to determine whether the two objects were identical within 2100 ms of stimulus onset. Each participant completed 104 trials for every condition at each location set. For the foveal task, each trial lasted between 900 ms and 1300 ms randomly, and the object was presented at the fixation for 100 ms. Participants were instructed to press a button when the fixation color turned white. Trials in which a key was pressed were excluded from later analyses. 104 valid trials were collected for each object condition for each participant.</p>
<p>Each participant completed four task runs, each consisting of one peripheral task block and one foveal task block. The order of the two tasks was counterbalanced within each participant. The accuracy of the peripheral object task was 67.3% ± 5.1%.</p>
</sec>
<sec id="s4h">
<title>MEG data acquisition and preprocessing</title>
<p>The MEG experiment was conducted using a CTF system at the Beijing MRI Center for Brain Research (BMCBR). The data analysis was completed using the MNE Python toolbox<sup><xref ref-type="bibr" rid="c47">47</xref></sup> and custom codes. The raw data were sampled at 300 Hz and then down sampled to 100 Hz. A bandpass filter between 1 and 30 Hz was applied.</p>
<p>Independent Component Analysis was used to eliminate biological artifacts, including heartbeat and ocular artifacts. For each trial, the MEG data were baseline-corrected using a time window from 243 ms to 43 ms before stimulus onset. Trials with instantaneous distortion were excluded from further analysis, along with their adjacent trials. One participant did not complete all trials in the foveal task, resulting in approximately 92 trials per condition.</p>
</sec>
<sec id="s4i">
<title>Source reconstruction</title>
<p>The cortical surface was reconstructed for each individual participant based on the anatomical T1 data acquired in MRI scan. A boundary element model (BEM) was set up based on the inner skull boundary extracted via watershed algorithm<sup><xref ref-type="bibr" rid="c48">48</xref></sup>. To extract the source space and calculate the forward solution, the coordinate frames were aligned based on fiducials and digitizer points on the head surface. For each hemisphere, 4098 source points were generated to build the source space. A regularized noise covariance matrix was estimated using MEG data from 243 ms to 43 ms before stimulus onset. Source estimates were obtained using dynamic statistical parametric mapping (dSPM)<sup><xref ref-type="bibr" rid="c20">20</xref></sup>, a linear minimum-norm inverse method. A Loose value of 0.2 was used when calculating the inverse operator. The resulting source activations were projected onto the surface normal.</p>
<p>Three ROIs, early visual cortex, occipito-temporal cortex, and posterior-parietal cortex, were identified using the anatomical labels in the surface reconstructed by Freesurfer (see <xref rid="fig3" ref-type="fig">Figure 3B</xref>). The data from the source points within each ROI were extracted for further analyses.</p>
</sec>
<sec id="s4j">
<title>MEG decoding</title>
<p>The decoding analysis employed a neural decoding toolbox (<ext-link ext-link-type="uri" xlink:href="http://www.readout.info">http://www.readout.info</ext-link>)<sup><xref ref-type="bibr" rid="c49">49</xref></sup> and custom Python/MATLAB codes. The trials from each condition were randomly divided into eight splits, and were averaged within each split. Then a cross-validator were created with a leave-one-split-out procedure, similar to decoding analysis in the fMRI data. The 100 most informative channels or sources were selected based on the F value across all conditions, and were further used to train and test decoders.</p>
<p>The above process was repeated 20 times at each time point to estimate the temporal dynamics of high- and low-order information. In the peripheral task, in addition to training and testing decoders with data from identical stimulus locations, the decoders were also trained and tested across different stimulus locations to examine the location invariant visual representation. The temporal dynamics of decoding performances were smoothed with a Gaussian kernel with a half width of 100 ms.</p>
</sec>
<sec id="s4k">
<title>Granger causality analysis</title>
<p>The Granger causality analysis was conducted with the Multivariate Granger Causality (MVGC) toolbox <sup><xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref></sup> using the time courses of visual information represented in each ROI, which were estimated by the distances from the neural response pattern to the decoding hyperplane in the high-dimensional spaces of neural responses<sup><xref ref-type="bibr" rid="c21">21</xref></sup>. The similar training and testing procedures were applied to the MEG time courses as in the decoding analysis, but the distance of neural response pattern to the trained hyperplane was calculated at each time point. A further distance in the correct direction indicated a higher quality of neural representation at each time point. The distances were normalized by subtracting the mean and dividing by the standard deviation of trials. Next, autoregressive model was used to examine whether the quality of the neural representation in the current time in a target region could be explained by the past representation quality in a source region, beyond the explanation supplied by the past of the target region itself. The Granger causal influence was defined as ln(Ureduced/Ufull), where U is the unexplained variance by the model. To create baselines for statistical comparisons, the MEG data before the stimulus onset (-200 ms to 0 ms) were extracted and the same autoregressive model was applied to estimate the increase of variance explanation in the noise background<sup><xref ref-type="bibr" rid="c50">50</xref></sup>. For each pair of ROIs, we tested both directions of Granger causality with the all other ROIs serving as regression factors. The analysis was conducted for each time point using a 150 ms sliding time windows preceding this time point, and a model order (i.e., the number of time-lags) of 5 (10, 20, 30, 40, 50 ms) was selected. To enhance visibility, the time courses of the Granger causality influence were smoothed by a Gaussian kernel with a half width of 50 ms.</p>
</sec>
<sec id="s4l">
<title>Behavioral correlation of neural representation</title>
<p>For each time point in each trial, the distance from the neural response pattern to the decoding hyperplane was calculated to estimate the quality of neural representation. The decoding hyperplane was trained with the neural responses of objects presented at the location set different from the current trial (cross-location decoding). Then for each condition at each time point, the Pearson correlation coefficient was calculated between the decoding distances and the behavioral reaction times across trials. Finally, the correlation coefficients were averaged across all conditions to generate the time course of behavioral correlation of neural representation for each participant. This procedure was repeated for each brain region. The positive correlation coefficient meant that the better quality of neural representation related to a faster reaction. Only the data from correct trials were used to estimate the behavioral correlation. The trials with reaction time outside two SD were excluded from the analysis. For correlation calculation in each time point, the neural response patterns that were incorrected classified with high distance (outside two SD) were excluded. The time courses of behavioral correlation were smoothed by a Gaussian kernel with a half width of 100 ms.</p>
</sec>
<sec id="s4m">
<title>Significance testing</title>
<p>In the fMRI response analyses, two-tail t-test was used to assess whether the BOLD signal change was different from baseline. In the decoding analysis, one-tail t-test was used to examine whether the decoding accuracy exceeded the chance level. In the MEG data analyses, we used the nonparametric cluster-based permutation test <sup><xref ref-type="bibr" rid="c51">51</xref>,<xref ref-type="bibr" rid="c52">52</xref></sup> to correct multiple comparisons over time. For each time point, the sign of the effect (i.e., decoding accuracy vs. chance, Granger causality influence vs. baseline, behavioral correlation vs. zeros) was randomly flipped in each participant for 50000 times to get the null hypothesis distribution. Then the cluster-based permutation test was performed, where clusters were defined as continuous significant time points in the time-series. The effects in each cluster were summed and the most significant of them in the time-series was used to generated the corrected null hypotheses distribution.</p>
</sec>
</sec>
</body>
<back>
<sec id="s5" sec-type="data-availability">
<title>Data availability statement</title>
<p>All preprocessed data and code in this study have been uploaded to figshare. Prior to publication, reviewers can access these files anonymously through the private link: <ext-link ext-link-type="uri" xlink:href="https://figshare.com/s/df047314ef2d3fd4dc5b">https://figshare.com/s/df047314ef2d3fd4dc5b</ext-link>.</p>
<p>Raw data has not been deposited in a public repository due to file size, but is available upon request.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by the National Key Research and Development Program of China (Grant Nos. 2021ZD0204200 and 2021ZD0203800); Key Research Program of Frontier Sciences, Chinese Academy of Sciences (Grant No. KJZD-SW-L08); and CAS Project for Young Scientists in Basic Research (Grant No. YSBR-071). The authors would like to thank Dr. Peng Zhang and Dr. Chencan Qian for their help during data collection and analysis.</p>
</ack>
<sec id="s6">
<title>Additional information</title>
<sec id="s6a">
<title>Author contributions</title>
<p>W.H., S.H., and J.Z. conceived the project and designed the experiments. W.H. and J.Z. performed the experiments. W.H. and J.Z. developed the analysis pipeline and analyzed the data. W.H., S.H., and J.Z. wrote the manuscript.</p>
</sec>
<sec id="s7">
<title>Declaration of interests</title>
<p>The authors declare no competing interests.</p>
</sec>
</sec>
<sec id="d1e2973">
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1.</label>
<caption><p>Latency for category (<bold>A</bold>) and orientation (<bold>B</bold>) information in three cortical regions during peripheral object recognition task. Time courses of decoding accuracy were smoothed with a Gaussian kernel with a half-width of 150 ms, and latency was estimated as the time from stimulus onset to 75% of peak decoding performance. When calculating the group mean of latency, data exceeding 2 SD were excluded. The latency of cross-location decoding for category information was significantly longer in early visual cortex than in occipito-temporal cortex (t(11)=2.86, p=0.03), and the effect was marginally significant for orientation information (t(12)=2.52, p=0.06), indicating the decoding performances were driven by feedback signals. In same-location decoding, the latency of category information was similar between early visual cortex and occipito-temporal cortex, and the latency of orientation information is much shorter in early visual cortex (t(13)=4.01, p=0.001). * indicates paired t-test with significance of p&lt; 0.05. ** indicates paired t-test with significance of p&lt; 0.01. † indicates marginal significance (Holm-Bonferroni corrected).</p></caption>
<graphic xlink:href="621525v1_figs1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<ref-list>
<title>Reference</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Felleman</surname>, <given-names>D.J.</given-names></string-name>, and <string-name><surname>Van Essen</surname>, <given-names>D.C.</given-names></string-name></person-group> (<year>1991</year>). <article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title>. <source>Cereb Cortex</source> <volume>1</volume>, <fpage>1</fpage>–<lpage>47</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lamme</surname>, <given-names>V.A.</given-names></string-name>, <string-name><surname>Supèr</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Spekreijse</surname>, <given-names>H</given-names></string-name></person-group>. (<year>1998</year>). <article-title>Feedforward, horizontal, and feedback processing in the visual cortex</article-title>. <source>Curr. Opin. Neurobiol</source>. <volume>8</volume>, <fpage>529</fpage>–<lpage>535</lpage>. <pub-id pub-id-type="doi">10.1016/s0959-4388(98)80042-1</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kreiman</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Serre</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Beyond the feedforward sweep: feedback computations in the visual cortex</article-title>. <source>Ann. N. Y. Acad. Sci</source>. <volume>1464</volume>, <fpage>222</fpage>–<lpage>241</lpage>. <pub-id pub-id-type="doi">10.1111/nyas.14320</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kar</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>DiCarlo</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Fast Recurrent Processing via Ventrolateral Prefrontal Cortex Is Needed by the Primate Ventral Stream for Robust Core Visual Object Recognition</article-title>. <source>Neuron</source> <volume>109</volume>, <fpage>164</fpage>–<lpage>176.e5.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2020.09.035</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hubel</surname>, <given-names>D.H.</given-names></string-name>, and <string-name><surname>Wiesel</surname>, <given-names>T.N</given-names></string-name></person-group>. (<year>1977</year>). <article-title>Ferrier lecture. Functional architecture of macaque monkey visual cortex</article-title>. <source>Proc. R. Soc. Lond. B Biol. Sci</source>. <volume>198</volume>, <fpage>1</fpage>–<lpage>59</lpage>. <pub-id pub-id-type="doi">10.1098/rspb.1977.0085</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Albright</surname>, <given-names>T.D.</given-names></string-name>, <string-name><surname>Gross</surname>, <given-names>C.G.</given-names></string-name>, and <string-name><surname>Bruce</surname>, <given-names>C</given-names></string-name></person-group>. (<year>1984</year>). <article-title>Stimulus-selective properties of inferior temporal neurons in the macaque</article-title>. <source>J Neurosci</source> <volume>4</volume>, <fpage>2051</fpage>–<lpage>2062</lpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bao</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>She</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>McGill</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Tsao</surname>, <given-names>D.Y</given-names></string-name></person-group>. (<year>2020</year>). <article-title>A map of object space in primate inferotemporal cortex</article-title>. <source>Nature</source> <volume>583</volume>, <fpage>103</fpage>–<lpage>108</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-020-2350-5</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rao</surname>, <given-names>R.P.N.</given-names></string-name>, and <string-name><surname>Ballard</surname>, <given-names>D.H</given-names></string-name></person-group>. (<year>1999</year>). <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nat. Neurosci</source>. <volume>2</volume>, <fpage>79</fpage>–<lpage>87</lpage>. <pub-id pub-id-type="doi">10.1038/4580</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2005</year>). <article-title>A theory of cortical responses</article-title>. <source>Philos. Trans. R. Soc. Lond. B. Biol. Sci</source>. <volume>360</volume>, <fpage>815</fpage>–<lpage>836</lpage>. <pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Williams</surname>, <given-names>M.A.</given-names></string-name>, <string-name><surname>Baker</surname>, <given-names>C.I.</given-names></string-name>, <string-name><surname>Op De Beeck</surname> <given-names>H.P.</given-names></string-name>, <string-name><surname>Mok Shim</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Dang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Triantafyllou</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Feedback of visual object information to foveal retinotopic cortex</article-title>. <source>Nat. Neurosci</source>. <volume>11</volume>, <fpage>1439</fpage>–<lpage>1445</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2218</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fan</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Shao</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Kersten</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>He</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Temporally flexible feedback signal to foveal cortex for peripheral object recognition</article-title>. <source>Proc. Natl. Acad. Sci</source>. <volume>113</volume>, <fpage>11627</fpage>–<lpage>11632</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1606137113</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morgan</surname>, <given-names>A.T.</given-names></string-name>, <string-name><surname>Petro</surname>, <given-names>L.S.</given-names></string-name>, and <string-name><surname>Muckli</surname>, <given-names>L</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Scene Representations Conveyed by Cortical Feedback to Early Visual Cortex Can Be Described by Line Drawings</article-title>. <source>J. Neurosci</source>. <volume>39</volume>, <fpage>9410</fpage>–<lpage>9423</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0852-19.2019</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harris</surname>, <given-names>K.D.</given-names></string-name>, and <string-name><surname>Mrsic-Flogel</surname>, <given-names>T.D</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Cortical connectivity and sensory coding</article-title>. <source>Nature</source> <volume>503</volume>, <fpage>51</fpage>–<lpage>58</lpage>. <pub-id pub-id-type="doi">10.1038/nature12654</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rockland</surname>, <given-names>K.S.</given-names></string-name>, and <string-name><surname>Pandya</surname>, <given-names>D.N</given-names></string-name></person-group>. (<year>1979</year>). <article-title>Laminar origins and terminations of cortical connections of the occipital lobe in the rhesus monkey</article-title>. <source>Brain Res</source>. <volume>179</volume>, <fpage>3</fpage>–<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1016/0006-8993(79)90485-2</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wong-Riley</surname>, <given-names>M</given-names></string-name></person-group>. (<year>1978</year>). <article-title>Reciprocal connections between striate and prestriate cortex in squirrel monkey as demonstrated by combined peroxidase histochemistry and autoradiography</article-title>. <source>Brain Res</source>. <volume>147</volume>, <fpage>159</fpage>–<lpage>164</lpage>. <pub-id pub-id-type="doi">10.1016/0006-8993(78)90781-3</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wyatte</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Jilk</surname>, <given-names>D.J.</given-names></string-name>, and <string-name><surname>O’Reilly</surname>, <given-names>R.C</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Early recurrent feedback facilitates visual object recognition under challenging conditions</article-title>. <source>Front. Psychol</source>. <volume>5</volume>. <pub-id pub-id-type="doi">10.3389/fpsyg.2014.00674</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gilbert</surname>, <given-names>C.D.</given-names></string-name>, and <string-name><surname>Li</surname>, <given-names>W</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Top-down influences on visual processing</article-title>. <source>Nat. Rev. Neurosci</source>. <volume>14</volume>, <fpage>350</fpage>–<lpage>363</lpage>. <pub-id pub-id-type="doi">10.1038/nrn3476</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Kourtzi</surname>, <given-names>Z.</given-names></string-name>, and <string-name><surname>Kanwisher</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2001</year>). <article-title>The lateral occipital complex and its role in object recognition</article-title>. <source>Vision Res</source>. <volume>41</volume>, <fpage>1409</fpage>–<lpage>1422</lpage>. <pub-id pub-id-type="doi">10.1016/S0042-6989(01)00073-6</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kastner</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Jeong</surname>, <given-names>S.K.</given-names></string-name>, and <string-name><surname>Mruczek</surname>, <given-names>R.E.B</given-names></string-name></person-group>. (<year>2017</year>). <article-title>A brief comparative review of primate posterior parietal cortex: A novel hypothesis on the human toolmaker</article-title>. <source>Neuropsychologia</source> <volume>105</volume>, <fpage>123</fpage>–<lpage>134</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.01.034</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dale</surname>, <given-names>A.M.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>A.K.</given-names></string-name>, <string-name><surname>Fischl</surname>, <given-names>B.R.</given-names></string-name>, <string-name><surname>Buckner</surname>, <given-names>R.L.</given-names></string-name>, <string-name><surname>Belliveau</surname>, <given-names>J.W.</given-names></string-name>, <string-name><surname>Lewine</surname>, <given-names>J.D.</given-names></string-name>, and <string-name><surname>Halgren</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2000</year>). <article-title>Dynamic Statistical Parametric Mapping: Combining fMRI and MEG for High-Resolution Imaging of Cortical Activity</article-title>. <source>Neuron</source> <volume>26</volume>, <fpage>55</fpage>–<lpage>67</lpage>. <pub-id pub-id-type="doi">10.1016/S0896-6273(00)81138-1</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jia</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zamboni</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Kemper</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Rua</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Goncalves</surname>, <given-names>N.R.</given-names></string-name>, <string-name><surname>Ng</surname>, <given-names>A.K.T.</given-names></string-name>, <string-name><surname>Rodgers</surname>, <given-names>C.T.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Goebel</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Kourtzi</surname>, <given-names>Z</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Recurrent Processing Drives Perceptual Plasticity</article-title>. <source>Curr. Biol</source>. <volume>30</volume>, <fpage>4177</fpage>–<lpage>4187.e4.</lpage> <pub-id pub-id-type="doi">10.1016/j.cub.2020.08.016</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seth</surname>, <given-names>A.K</given-names></string-name></person-group>. (<year>2010</year>). <article-title>A MATLAB toolbox for Granger causal connectivity analysis</article-title>. <source>J. Neurosci. Methods</source> <volume>186</volume>, <fpage>262</fpage>–<lpage>273</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2009.11.020</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barnett</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Seth</surname>, <given-names>A.K</given-names></string-name></person-group>. (<year>2014</year>). <article-title>The MVGC multivariate Granger causality toolbox: A new approach to Granger-causal inference</article-title>. <source>J. Neurosci. Methods</source> <volume>223</volume>, <fpage>50</fpage>–<lpage>68</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.10.018</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shipp</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Structure and function of the cerebral cortex</article-title>. <source>Curr. Biol</source>. <volume>17</volume>, <fpage>R443</fpage>– <lpage>R449</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2007.03.044</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schuman</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Dellal</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Prönneke</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Machold</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Rudy</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Neocortical Layer 1: An Elegant Solution to Top-Down and Bottom-Up Integration</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>44</volume>, <fpage>221</fpage>–<lpage>252</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-neuro-100520-012117</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aitken</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Menelaou</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Warrington</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Koolschijn</surname>, <given-names>R.S.</given-names></string-name>, <string-name><surname>Corbin</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Callaghan</surname>, <given-names>M.F.</given-names></string-name>, and <string-name><surname>Kok</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Prior expectations evoke stimulus-specific activity in the deep layers of the primary visual cortex</article-title>. <source>PLOS Biol</source>. <volume>18</volume>, <fpage>e3001023</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3001023</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Bains</surname>, <given-names>L.J.</given-names></string-name>, <string-name><surname>van Mourik</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Norris</surname>, <given-names>D.G.</given-names></string-name>, and <string-name><surname>de Lange</surname>, <given-names>F.P.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Selective Activation of the Deep Layers of the Human Primary Visual Cortex by Top-Down Feedback</article-title>. <source>Curr. Biol</source>. <volume>26</volume>, <fpage>371</fpage>–<lpage>376</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2015.12.038</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Guo</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Qian</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>D.J.</given-names></string-name>, <string-name><surname>He</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Zhang</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Layer-dependent multiplicative effects of spatial attention on contrast responses in human early visual cortex</article-title>. <source>Prog. Neurobiol</source>. <volume>207</volume>, <fpage>101897</fpage>. <pub-id pub-id-type="doi">10.1016/j.pneurobio.2020.101897</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Kerkoerle</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Self</surname>, <given-names>M.W.</given-names></string-name>, and <string-name><surname>Roelfsema</surname>, <given-names>P.R.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Layer-specificity in the effects of attention and working memory on activity in primary visual cortex</article-title>. <source>Nat. Commun</source>. <volume>8</volume>, <fpage>13804</fpage>. <pub-id pub-id-type="doi">10.1038/ncomms13804</pub-id>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lawrence</surname>, <given-names>S.J.</given-names></string-name>, <string-name><surname>Norris</surname>, <given-names>D.G.</given-names></string-name>, and <string-name><surname>de Lange</surname>, <given-names>F.P.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Dissociable laminar profiles of concurrent bottom-up and top-down modulation in the human visual cortex</article-title>. <source>eLife</source> <volume>8</volume>, <elocation-id>e44422</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.44422</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lawrence</surname>, <given-names>S.J.D.</given-names></string-name>, <string-name><surname>Van Mourik</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Koopmans</surname>, <given-names>P.J.</given-names></string-name>, <string-name><surname>Norris</surname>, <given-names>D.G.</given-names></string-name>, and <string-name><surname>De Lange</surname>, <given-names>F.P.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Laminar Organization of Working Memory Signals in Human Visual Cortex</article-title>. <source>Curr. Biol</source>. <volume>28</volume>, <fpage>3435</fpage>–<lpage>3440.e4.</lpage> <pub-id pub-id-type="doi">10.1016/j.cub.2018.08.043</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Victor</surname>, <given-names>J.D.</given-names></string-name>, <string-name><surname>Mechler</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Repucci</surname>, <given-names>M.A.</given-names></string-name>, <string-name><surname>Purpura</surname>, <given-names>K.P.</given-names></string-name>, and <string-name><surname>Sharpee</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Responses of V1 neurons to two-dimensional hermite functions</article-title>. <source>J. Neurophysiol</source>. <volume>95</volume>, <fpage>379</fpage>–<lpage>400</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00498.2005</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vinje</surname>, <given-names>W.E.</given-names></string-name>, and <string-name><surname>Gallant</surname>, <given-names>J.L</given-names></string-name></person-group>. (<year>2000</year>). <article-title>Sparse Coding and Decorrelation in Primary Visual Cortex During Natural Vision</article-title>. <source>Science</source> <volume>287</volume>, <fpage>1273</fpage>–<lpage>1276</lpage>. <pub-id pub-id-type="doi">10.1126/science.287.5456.1273</pub-id>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>T.S.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Teo</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Jiang</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Complex Pattern Selectivity in Macaque Primary Visual Cortex Revealed by Large-Scale Two-Photon Imaging</article-title>. <source>Curr. Biol</source>. <volume>28</volume>, <fpage>38</fpage>–<lpage>48.e3.</lpage> <pub-id pub-id-type="doi">10.1016/j.cub.2017.11.039</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Jiang</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Lee</surname>, <given-names>T.S</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Large-scale two-photon imaging revealed super-sparse population codes in the V1 superficial layer of awake monkeys</article-title>. <source>eLife</source> <volume>7</volume>, <elocation-id>e33370</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.33370</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ge</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Qian</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>He</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Adaptation to feedback representation of illusory orientation produced from flash grab effect</article-title>. <source>Nat. Commun</source>. <volume>11</volume>, <fpage>3925</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-020-17786-1</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chambers</surname>, <given-names>C.D.</given-names></string-name>, <string-name><surname>Allen</surname>, <given-names>C.P.G.</given-names></string-name>, <string-name><surname>Maizey</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Williams</surname>, <given-names>M.A</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Is delayed foveal feedback critical for extra-foveal perception?</article-title> <source>Cortex</source> <volume>49</volume>, <fpage>327</fpage>–<lpage>335</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2012.03.007</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>VanRullen</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Thorpe</surname>, <given-names>S.J</given-names></string-name></person-group>. (<year>2001</year>). <article-title>The time course of visual processing: from early perception to decision-making</article-title>. <source>J. Cogn. Neurosci</source>. <volume>13</volume>, <fpage>454</fpage>–<lpage>461</lpage>. <pub-id pub-id-type="doi">10.1162/08989290152001880</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kovacs</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Vogels</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Orban</surname>, <given-names>G</given-names></string-name></person-group>. (<year>1995</year>). <article-title>Selectivity of macaque inferior temporal neurons for partially occluded shapes</article-title>. <source>J. Neurosci</source>. <volume>15</volume>, <fpage>1984</fpage>–<lpage>1997</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.15-03-01984.1995</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nielsen</surname>, <given-names>K.J.</given-names></string-name>, <string-name><surname>Logothetis</surname>, <given-names>N.K.</given-names></string-name>, and <string-name><surname>Rainer</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Dissociation Between Local Field Potentials and Spiking Activity in Macaque Inferior Temporal Cortex Reveals Diagnosticity-Based Encoding of Complex Objects</article-title>. <source>J. Neurosci</source>. <volume>26</volume>, <fpage>9639</fpage>–<lpage>9645</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2273-06.2006</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhaoping</surname>, <given-names>L</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Peripheral vision is mainly for looking rather than seeing</article-title>. <source>Neurosci. Res</source>. <volume>201</volume>, <fpage>18</fpage>–<lpage>26</lpage>. <pub-id pub-id-type="doi">10.1016/j.neures.2023.11.006</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>Q.</given-names></string-name>, and <string-name><surname>Shim</surname>, <given-names>W.M</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Modulating foveal representation can influence visual discrimination in the periphery</article-title>. <source>J. Vis</source>. <volume>16</volume>, <fpage>15</fpage>. <pub-id pub-id-type="doi">10.1167/16.3.15</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fischl</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2012</year>). <article-title>FreeSurfer</article-title>. <source>NeuroImage</source> <volume>62</volume>, <fpage>774</fpage>–<lpage>781</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cox</surname>, <given-names>R.W</given-names></string-name></person-group>. (<year>1996</year>). <article-title>AFNI: Software for Analysis and Visualization of Functional Magnetic Resonance Neuroimages</article-title>. <source>Comput. Biomed. Res</source>. <volume>29</volume>, <fpage>162</fpage>–<lpage>173</lpage>. <pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kay</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Jamison</surname>, <given-names>K.W.</given-names></string-name>, <string-name><surname>Vizioli</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Margalit</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Ugurbil</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2019</year>). <article-title>A critical assessment of data quality and venous effects in sub-millimeter fMRI</article-title>. <source>NeuroImage</source> <volume>189</volume>, <fpage>847</fpage>–<lpage>869</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.02.006</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname>, <given-names>C.-C.</given-names></string-name>, and <string-name><surname>Lin</surname>, <given-names>C.-J</given-names></string-name></person-group>. (<year>2011</year>). <article-title>LIBSVM: A library for support vector machines</article-title>. <source>ACM Trans. Intell. Syst. Technol</source>. <volume>2</volume>, <fpage>1</fpage>–<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1145/1961189.1961199</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gramfort</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2013</year>). <article-title>MEG and EEG data analysis with MNE-Python</article-title>. <source>Front. Neurosci</source>. <volume>7</volume>. <pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ségonne</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Dale</surname>, <given-names>A.M.</given-names></string-name>, <string-name><surname>Busa</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Glessner</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Salat</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hahn</surname>, <given-names>H.K.</given-names></string-name>, and <string-name><surname>Fischl</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2004</year>). <article-title>A hybrid approach to the skull stripping problem in MRI</article-title>. <source>NeuroImage</source> <volume>22</volume>, <fpage>1060</fpage>– <lpage>1075</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.03.032</pub-id>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meyers</surname>, <given-names>E.M</given-names></string-name></person-group>. (<year>2013</year>). <article-title>The neural decoding toolbox. Front</article-title>. <source>Neuroinformatics</source> <volume>7</volume>. <pub-id pub-id-type="doi">10.3389/fninf.2013.00008</pub-id>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kietzmann</surname>, <given-names>T.C.</given-names></string-name>, <string-name><surname>Spoerer</surname>, <given-names>C.J.</given-names></string-name>, <string-name><surname>Sörensen</surname>, <given-names>L.K.A.</given-names></string-name>, <string-name><surname>Cichy</surname>, <given-names>R.M.</given-names></string-name>, <string-name><surname>Hauk</surname>, <given-names>O.</given-names></string-name>, and <string-name><surname>Kriegeskorte</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title>. <source>Proc. Natl. Acad. Sci</source>. <volume>116</volume>, <fpage>21854</fpage>–<lpage>21863</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Oostenveld</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title>. <source>J. Neurosci. Methods</source> <volume>164</volume>, <fpage>177</fpage>–<lpage>190</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Spaak</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Cluster Test: Simple cluster-based permutation testing in arbitrary dimensions</article-title>. <source>Zenodo</source> <pub-id pub-id-type="doi">10.5281/zenodo.10877825</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103788.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kok</surname>
<given-names>Peter</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study reports <bold>important</bold> findings about the nature of feedback to primary visual cortex (V1) during object recognition. The state-of-the-art functional MRI evidence for the main claims is <bold>solid</bold>, although currently alternative explanations of the findings cannot be fully ruled out. The findings presented here are relevant to a number of scientific fields such as object recognition, categorisation and predictive coding.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103788.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study investigates spatial and temporal aspects of feedback information in the brain during categorization tasks. The authors found that feedback to V1 contained low-level features and was present in the deep layers of V1 originating presumably from occipito-temporal brain regions. High-level category feedback was found in the deep and the superficial layers and was directed to V1 from occipitotemporal and parietal cortices. This study raises a timely question in the fields of object categorization and predictive coding about the granularity of feedback and its separability by cortical depth and time course.</p>
<p>Here are a couple of concerns and questions:</p>
<p>The authors argue that low-level features in a feedback format could be decoded only from deep layers of V1 (and not superficial layers) during a perceptual categorization task. However, previous studies (Bergman et al., 2024; Iamshchinina et al., 2021) demonstrated that low-level features in the form of feedback can be decoded from both superficial and deep layers. While this result could be due to perceptual task or highly predictable orientation feature (orientation was kept the same throughout the experimental block), an alternative explanation is a weaker representation of orientation in the feedback (even before splitting by layers there is only a trend towards significance; also granger causality for orientation information in MEG part is lower than that for category in peripheral categorization task), because it is orthogonal to the task demand. It would be helpful if the authors added a statistical comparison of the strength of category and orientation representations in each layer and across the layers.</p>
<p>The authors argue that category feedback is not driven by low-level confounding features embedded in the stimuli. They demonstrate the ability to decode orientations, particularly well represented by V1, in the absence of category discrimination. However, the orientation is not a category-discriminating feature in this task. It could be that the category-discriminating features cannot be as well decoded from V1 activity patterns as orientations. Also, there are a number of these category discriminating features and it is unclear if it is a variation in their representational strength or merely the absence of the task-driven enhancement that preempts category decoding in V1 during the foveal task. In other words, I am not sure whether, if orientation was a category-specific feature (sharpies are always horizontal and smoothies are vertical), there would still be no category decoding.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103788.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This manuscript reports high-resolution functional MRI data and MEG data revealing additional mechanistic information about an established paradigm studying how foveal regions of the primary visual cortex (V1) are involved in processing peripheral visual stimuli. Because of the retinotopic organization of V1, peripheral stimuli should not evoke responses in the regions of V1 that represent stimuli in the center of the visual field (the fovea). However, functional MRI responses in foveal regions do reflect the characteristics of peripheral visual stimuli - this is a surprising finding first reported in 2008. The present study uses fMRI data with sub-millimeter resolution to study how responses at different depths in the foveal gray matter do or don't reflect peripheral object characteristics during 2 different tasks: one in which observers needed to make detailed judgments about object identity, and one in which observers needed to make more coarse judgments about object orientation. FMRI results reveal interesting and informative patterns in these two conditions. A follow-on MEG study yields information about the timing of these responses. Put together, the findings settle some questions in the field and add new information about the nature of visual feedback to V1.</p>
<p>Strengths:</p>
<p>(1) Rigorous and appropriate use of &quot;laminar fMRI&quot; techniques.</p>
<p>(2) The introduction does an excellent job of contextualizing the work.</p>
<p>(3) Control experiments and analyses are designed and implemented well</p>
<p>Weaknesses:</p>
<p>(1) While not necessarily a weakness, I do not fully agree with the description of the 2 kinds of feedback information as &quot;low-order&quot; and &quot;high-order&quot;. I understand the motivation to do this - orientation is typically considered a low-level visual feature. But when it's the orientation of an entire object, not a single edge, orientation can only be defined after the elements of the object are grouped. Also, the discrimination between spikies and smoothies requires detecting the orientations of particular edges that form the identifying features. To my mind, it would make more sense to refer to discrimination of object orientation as &quot;coarse&quot; feature discrimination, and orientation of object identity as &quot;fine&quot; feature discrimination. Thus, the sentence on line 83, for example, would read &quot;Interestingly, feedback with fine and coarse feature information exhibits different laminar profiles.&quot;.</p>
<p>(2) Figure 2 and text on lines 185, and 186: it is difficult to interpret/understand the findings in foveal ROIs for the foveal control task without knowing how big the ROI was. Foveal regions of V1 are grossly expanded by cortical magnification, such that the central half-degree can occupy several centimeters across the cortical surface. Without information on the spatial extent of the foveal ROI compared to the object size, we can't know whether the ROI included voxels whose population receptive fields were expected to include the edges of the objects.</p>
<p>(3) Line 143 and ROI section of the methods: in order for the reader to understand how robust the responses and analyses are, voxel counts should be provided for the ROIs that were defined, as well as for the number (fraction) of voxels excluded due to either high beta weights or low signal intensity (lines 505-511).</p>
<p>(4) I wasn't able to find mention of how multiple-comparisons corrections were performed for either the MEG or fMRI data (except for one Holm-Bonferonni correction in Figure S1), so it's unclear whether the reported p-values are corrected.</p>
</body>
</sub-article>
</article>