<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">107602</article-id>
<article-id pub-id-type="doi">10.7554/eLife.107602</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107602.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Ecology</subject>
</subj-group>
</article-categories><title-group>
<article-title>New idtracker.ai: rethinking multi-animal tracking as a representation learning problem to increase accuracy and reduce tracking times</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0006-6353-4079</contrib-id>
<name>
<surname>Torrents</surname>
<given-names>Jordi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8538-1345</contrib-id>
<name>
<surname>Costa</surname>
<given-names>Tiago</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5359-3426</contrib-id>
<name>
<surname>de Polavieja</surname>
<given-names>Gonzalo G</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>gonzalo.polavieja@neuro.fchampalimaud.org</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/007rd1x46</institution-id><institution>Champalimaud Research, Champalimaud Center for the Unknown - Lisbon</institution></institution-wrap>, <city>Lisbon</city>, <country country="PT">Portugal</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3588-7820</contrib-id><role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Cardona</surname>
<given-names>Albert</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution>
</institution-wrap>
<city>Cambridge</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>†</label><p>These authors contributed equally to this work</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-08-13">
<day>13</day>
<month>08</month>
<year>2025</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-12-22">
<day>22</day>
<month>12</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP107602</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-05-30">
<day>30</day>
<month>05</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-06-02">
<day>02</day>
<month>06</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.05.30.657023"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2025-08-13">
<day>13</day>
<month>08</month>
<year>2025</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.107602.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.107602.1.sa4">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.107602.1.sa3">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.107602.1.sa2">Reviewer #2 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.107602.1.sa1">Reviewer #3 (Public review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.107602.1.sa0">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Torrents et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Torrents et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-107602-v2.pdf"/>
<abstract>
<p>idTracker and idtracker.ai approach multi-animal tracking from video as an image classification problem. For this classification, both rely on segments of video where all animals are visible to extract images and their identity labels. When these segments are too short, tracking can become slow and inaccurate and, if they are absent, tracking is impossible. Here, we introduce a new idtracker.ai that reframes multi-animal tracking as a representation learning problem rather than a classification task. Specifically, we apply contrastive learning to image pairs that, based on video structure, are known to belong to the same or different identities. This approach maps animal images into a representation space where they cluster by animal identity. As a result, the new idtracker.ai eliminates the need for video segments with all animals visible, is more accurate, and tracks up to 700 times faster.</p>
</abstract>
<funding-group>
<award-group id="par-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001871</institution-id>
<institution>MEC | Fundação para a Ciência e a Tecnologia (FCT)</institution>
</institution-wrap>
</funding-source>
<award-id>PTDC/BIA-COM/5770/2020</award-id>
<principal-award-recipient>
<name>
<surname>Torrents</surname>
<given-names>Jordi</given-names>
</name>
</principal-award-recipient>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>In the new ms, we replaced our accuracy metric with the standard metric IDF1. IDF1 is the standard metric that is applied to systems in which the goal is to maintain consistent identities across time. The IDF1 accuracies we obtain are very similar (slightly higher) than the ones we had with our more basic metric. See also the section in Appendix 1 Computation of tracking accuracy explaining IDF1, lines 416-436. We have also made changes in the presentation for clarity and broader accessibility: 1) We have completely rewritten all the text of the ms until we start with contrastive learning. The old lines 20-89 are now lines 20-66, much shorter and easier to read. We have rewritten the first 3 paragraphs in the section The new idtracker.ai uses representation learning (lines 67-92), 2) We also made other changes in the main text to increase readability, including a better brief explanation of TRex, lines 153-157, and a better description of our occlusion tests in its dedicated extra section in Methods lines 239-251, 3) We have added a graphical legend to Figure 2a so it is visually clear what are the different elements of this figure, 4) To be more clear about the outputs of the new idtracker.ai, we have added in section Output of new idtracker.ai, the new lines 186-206 explaining all the outputs in detail. Additionally, we are now including the new Appendix 4 (Example workflow) to explain how to obtain all these outputs, 5) We improved and added some changes in the old section Protocol details for the new idtracker.ai in Appendix 3 to discuss the details of our approach (lines 538-886). It discusses in detail the steps of the algorithm, the network architecture, the loss function, the sampling strategy, the clustering and identity assignment, and the stopping criteria in training, 6) The section Network Architecture in Appendix 3 has been completely rewritten to give more detail and to include new results of the networks suggested by the reviewers, lines 550-611, 7) To cite previous work in detail and explain what we do differently, we have now added in Appendix 3 the new section Differences with previous work in contrastive/metric learning (lines 792-841).</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Main text</title>
<p>Video-tracking systems that attempt to follow individuals frame-by-frame can fail during oc-clusions, resulting in identity swaps that accumulate over time <xref ref-type="bibr" rid="c6">Branson et al. (2009)</xref>; <xref ref-type="bibr" rid="c41">Plum (2024)</xref>; <xref ref-type="bibr" rid="c10">Chen et al. (2023)</xref>; <xref ref-type="bibr" rid="c11">Chiara and Kim (2023)</xref>; <xref ref-type="bibr" rid="c35">Liu et al. (2023)</xref>; <xref ref-type="bibr" rid="c3">Bernardes et al. (2021)</xref>. idTracker <xref ref-type="bibr" rid="c40">Pérez-Escudero et al. (2014)</xref> introduced the paradigm of animal tracking by identification from the animal images. This approach, unfeasible for humans, avoids the accumulation of errors by identity swaps during occlusions. Its successor, idtracker.ai <xref ref-type="bibr" rid="c44">Romero-Ferrero et al. (2019)</xref>, built on this paradigm by incorporating deep learning and achieved accuracies often exceeding 99.9% in videos of up to 100 animals.</p>
<sec id="s1a">
<title>Limitation of the original idtracker.ai</title>
<p>The core idea of the original idtracker.ai is to use a segment of the video in which all animals are visible to extract a set of images and identity labels for each individual. These labeled images are used to train a convolutional neural network (CNN) with one class per animal. Once trained, the network assigns identities to other segments in which all animals are visible. Only segments with identity assignments that meet strict quality criteria are retained, and their images and labels are also used for further training of the CNN. This iterative process of training, assigning, and selecting continues until most of the animal images in the video have been assigned to identities. A second algorithm then tracks animals during crossings.</p>
<p>If no segment exists in which all animals are visible, idtracker.ai cannot start. More commonly, such segments can be short and result in a CNN of low quality. To improve performance in this case, idtracker.ai pretrains the CNN using the entire video, but this process is slow. As a consequence, when the segments in which all animals visible are too short, accuracy might be lower and tracking time longer.</p>
<p><xref rid="fig1" ref-type="fig">Figure 1a</xref> (blue line) gives the accuracies obtained with the original idtracker.ai in our bench-mark of 33 videos (see Appendix 1 for details of benchmarking). The first 15 videos of the bench-mark are videos of zebrafish, flies (drosophila) and mice with for which of the original idtracker.ai has accuracy of <italic>&gt;</italic> 99.9%. In the remaining videos the accuracy decreases, reaching 93.77% in video <italic>m</italic>_4_2, and 69.66% in video <italic>d</italic>_100_3, which lies outside the plotted range. These accuracies were computed using all animal images in the video while excluding animal crossings. <xref rid="figS1_1" ref-type="fig">Figure 1—figure Supplement 1a</xref> shows the corresponding results for complete trajectories that include crossings.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption>
<title>Performance for a benchmark of 33 videos of flies, zebrafish and mice.</title>
<p><bold>a</bold>. Median accuracy was computed using all images of animals in the videos excluding animal crossings. The videos are ordered by decreasing accuracy of the original idtracker.ai results for ease of visualization. <bold>b</bold>. Median tracking times are shown for the scale of hours and, in the inset, for the scale of days. The videos are ordered by increasing tracking times in the original idtracker.ai results for ease of visualization. <xref rid="tblS1" ref-type="table">Supplementary Table 1</xref>, <xref rid="tblS2" ref-type="table">Supplementary Table 2</xref>, <xref rid="tblS3" ref-type="table">Supplementary Table 3</xref> give more complete statistics (median, mean and 20-80 percentiles) for the original idtracker.ai (version 4 of the software), optimized v4 (version 5) and new idtracker.ai (version 6), respectively. The names of the videos in a. and b. start with a letter for the species (<italic>z,d,m</italic>), followed by the number of animals in the video, and possibly an extra number to distinguish the video if there are several of the same species and animal group size.</p>
</caption>
<graphic xlink:href="657023v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS1_1" position="float" fig-type="figure">
<label>Figure 1—figure supplement 1.</label>
<caption><title>Performance for the benchmark with full trajectories with animal crossings.</title>
<p><bold>a</bold>. Median accuracy was computed using all images of animals in the videos including animal crossings. <bold>b</bold>. Median tracking times. <xref rid="tblS1" ref-type="table">Supplementary Table 1</xref>, <xref rid="tblS2" ref-type="table">Supplementary Table 2</xref>, <xref rid="tblS3" ref-type="table">Supplementary Table 3</xref> and <xref rid="tblS4" ref-type="table">Supplementary Table 4</xref> give more complete statistics (median, mean and 20-80 percentiles) for the original idtracker.ai (version 4 of the software), optimized v4 (version 5), new idtracker.ai (version 6) and TRex, respectively.</p></caption>
<graphic xlink:href="657023v2_figS1_1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS1_2" position="float" fig-type="figure">
<label>Figure 1—figure supplement 2.</label>
<caption><title>Robustness to blurring and light conditions.</title>
<p><bold>First column:</bold> Unmodified video <italic>zebrafish_60_1</italic>. <bold>Second column:</bold> <italic>zebrafish_60_1</italic> with a gaussian blurring of 1 pixel plus a resolution reduction to 40% of the original plus MJPG video compression. Versions 5 of idtracker.ai fails to track this videos with a 82% accuracy. <bold>Third column:</bold> Videos of 60 zebrafish with manipulated light conditions (same test as in idtracker.ai <xref ref-type="bibr" rid="c44">Romero-Ferrero et al. (2019)</xref>). <bold>First row:</bold> Uniform light conditions across the arena (<italic>zebrafish_60_1</italic>). <bold>Second row:</bold> Similar setup but with lights off in the bottom and right side of the arena.</p></caption>
<graphic xlink:href="657023v2_figS1_2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS1_3" position="float" fig-type="figure">
<label>Figure 1—figure supplement 3.</label>
<caption><title>Memory usage across the different softwares.</title>
<p>The solid line is a logarithmic fit to the (RAM) memory peak as a function of the number of blobs in a video. The Python tool <monospace>psutil.virtual_memory().used</monospace> was used to measure the memory usage 5 times per second during the tracking. The presented value is the maximum measured value minus the baseline measured before the tracking start. Disclaimer: Both softwares include automatic optimizations that adjust based on machine resources, so results may vary on systems with less available memory. These results were measured on computers with the specifications in Methods.</p></caption>
<graphic xlink:href="657023v2_figS1_3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><xref rid="fig1" ref-type="fig">Figure 1b</xref> (blue line) shows the times that the original idtracker.ai takes to track each of the videos in the benchmark. Some of the videos take a few minutes to track, others a few hours, and six videos take more than three days, one nearly two weeks.</p>
</sec>
<sec id="s1b">
<title>Optimizing idtracker.ai without changes in the learning method</title>
<p>We first optimized idtracker.ai without changing how we identify animals. We improved data loading and redesigned the main objects in the software (see Appendix 2 for details). This version of the optimized original idtracker.ai (version 5 of the software) achieved higher accuracies, <xref rid="fig1" ref-type="fig">Figure 1a</xref> (orange line), and <xref rid="figS1_1" ref-type="fig">Figure 1—figure Supplement 1a</xref> (orange line) for results that include animal crossings. The mean accuracy across the benchmark is 99.63% without crossings and 99.49% with crossings, compared with 98.39% without crossings and 98.24% with crossings for the original id-tracker.ai.</p>
<p>Tracking times were also significantly reduced. As shown in <xref rid="fig1" ref-type="fig">Figure 1b</xref> (orange line), no video took longer than a day. On average, tracking is 13.5 times faster than with the original version and 120.1 times faster for the more difficult videos. Even so, tracking times of up to one day may still be a limiting factor in some pipelines.</p>
<p>To further improve both accuracy and tracking speed, we retained these optimizations while changing the core identification algorithm to eliminate the need for segments of video in which all animals are visible.</p>
</sec>
<sec id="s1c">
<title>The new idtracker.ai uses representation learning</title>
<p>We reformulate multi-animal tracking as a representation learning problem. In representation learning, we learn a transformation of the input data that makes it easier to perform downstream tasks <xref ref-type="bibr" rid="c57">Xing et al. (2002)</xref>; <xref ref-type="bibr" rid="c2">Bengio et al. (2013)</xref>; <xref ref-type="bibr" rid="c16">Ericsson et al. (2022)</xref>. In our case, the downstream task is to cluster images into animal identities without requiring identity labels.</p>
<p>This reformulation is made possible by the inherent structure of the video, illustrated in <xref rid="fig2" ref-type="fig">Figure 2a</xref>. First, we detect specific moments in the video when animals touch or cross paths. These are shown in <xref rid="fig2" ref-type="fig">Figure 2a</xref> as boxes with dashed borders that contain images of overlapping animals. We can then divide the rest of the video into individual fragments, each consisting of the set of images of a single individual between two animal crossings. <xref rid="fig2" ref-type="fig">Figure 2a</xref> shows 14 such fragments as rectangles with a gray background. In addition, a video with <italic>N</italic> animals may contain global fragments, that is, a collections of <italic>N</italic> individual fragments that coexist in one or more consecutive frames. An example is shown in <xref rid="fig2" ref-type="fig">Figure 2a</xref> by the five fragments with blue borders. These global fragments are used by the original idtracker.ai to train a CNN. In the new approach, we do not assume that global fragments exist.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Tracking by identification using deep contrastive learning.</title>
<p><bold>a</bold> Schematic representation of a video with five fish. <bold>b</bold> A ResNet18 network with 8 outputs generates a representation of each animal image as a point in an 8-dimensional space (here shown in 2D for visualization). Each pair of images corresponds to two points in this space, separated by a Euclidean distance. The ResNet18 network is trained to minimize this distance for positive pairs and maximize it for negative pairs. <bold>c</bold> 2D t-SNE visualizations of the learned 8-dimensional representation space. Each dot represents an image of an animal from the video. As training progresses, clusters corresponding to individual animals become clearer, plotted at training with 0, 2,000, 4,000 and 15,000 batches (each batch contains 400 positive and 400 negative pairs of images). The t-SNE plot at 15,000 training batches is also shown color-coded by human-validated ground-truth identities. The pink rectangle at 2,000 batches of training highlights clear clusters and the orange square fuzzy clusters. <bold>d</bold> The Silhouette score measures cluster coherence and increases during training, as illustrated for a video with 60 zebrafish. <bold>e</bold> A Silhouette score of 0.91 corresponds to a human-validated error rate of less than 1% per image.</p></caption>
<graphic xlink:href="657023v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Representation learning becomes feasible in this context because we can obtain both positive and negative image pairs without using identity labels. Positive pairs are images of the same individual obtained from within the same individual fragment (<xref rid="fig2" ref-type="fig">Figure 2a</xref>, green boxes). Negative pairs are images of different individuals taken from different individual fragments that coexist in time for one or more frames (<xref rid="fig2" ref-type="fig">Figure 2a</xref>, red boxes). We can then use the positive and negative pairs of images for contrastive learning, a self-supervised learning framework designed to learn a representation space in which positive examples are close together, and negative examples are far apart <xref ref-type="bibr" rid="c48">Schroff et al. (2015)</xref>; <xref ref-type="bibr" rid="c14">Dong and Shen (2018)</xref>; <xref ref-type="bibr" rid="c28">KAYA and BiLGE (2019)</xref>; <xref ref-type="bibr" rid="c7">Chen et al. (2020a</xref>,<xref ref-type="bibr" rid="c8">b)</xref>; <xref ref-type="bibr" rid="c19">Guo et al. (2020)</xref>; <xref ref-type="bibr" rid="c56">Wang et al. (2020)</xref>; <xref ref-type="bibr" rid="c58">Yang et al. (2020)</xref>. This formulation is supported by the success of deep metric and contrastive learning methods (see Appendix 3 for comparison with previous work).</p>
<p>We first evaluated which neural networks are suitable for contrastive learning with animal images. In addition to our previous CNN from idtracker.ai, we tested 31 networks from 10 different families of state-of-the-art convolutional neural networks and transformer architectures, selected for their compatibility with consumer-grade GPUs and ability to handle small input images (20 × 20 to 100 × 100 pixels) typical in collective animal behavior videos. Among these architectures, ResNet18 (v1) <xref ref-type="bibr" rid="c22">He et al. (2016a)</xref> without pretrained weights was the fastest to obtain low errors (see Appendix 3).</p>
<p>A ResNet18 with <italic>M</italic> outputs maps each input image to a point in an <italic>M</italic>-dimensional representation space (illustrated in <xref rid="fig2" ref-type="fig">Figure 2b</xref> as a point on a plane). Experiments showed that using <italic>M</italic> = 8 achieved faster convergence to low error (see Appendix 3). ResNet18 is trained using a contrastive loss function (<xref ref-type="bibr" rid="c12">Chopra et al. (2005)</xref>, see Appendix 3 for details). Each image in a positive or negative pair is input separately into the network, producing a point in the 8-dimensional representation space. For an image pair, we then obtain two points in an 8-dimensional space, separated by some (Euclidean) distance. The optimization of the loss function minimizes (or maximizes) this Euclidean distance for positive (or negative) pairs until the distance <italic>D</italic><sub>pos</sub> (or <italic>D</italic><sub>neg</sub>) is reached. The effect of <italic>D</italic><sub>pos</sub> is to prevent the collapse to a single of the positive images coming from the same fragment, allowing for a small region of the 8-dimensional representation space for all the positive pairs of the same identity. The effect of <italic>D</italic><sub>neg</sub> is to prevent excessive scatter of the points representing images from negative pairs. We empirically determined that <italic>D</italic><sub>neg</sub>/<italic>D</italic><sub>pos</sub> = 10 results in a faster method to obtain low error (see Appendix 3), and we use <italic>D</italic><sub>pos</sub> = 1 and <italic>D</italic><sub>neg</sub> = 10.</p>
<p>As the model trains, the representation space becomes increasingly structured, with similar data points forming coherent clusters. <xref rid="fig2" ref-type="fig">Figure 2c</xref> visualizes this progression using 2D t-SNE <xref ref-type="bibr" rid="c37">van der Maaten and Hinton (2008)</xref> plots of the 8-dimensional representation space. After 2, 000 training batches, initial clusters emerge, and by 15,000 batches, distinct clusters corresponding to individual animals are evident. Ground truth identities verified by humans confirm that each cluster corresponds to an animal identity (<xref rid="fig2" ref-type="fig">Figure 2c</xref>, colored clusters).</p>
<p>The method to select positive and negative pairs is critical for fast learning <xref ref-type="bibr" rid="c1">Awasthi et al. (2022)</xref>; <xref ref-type="bibr" rid="c29">Khosla et al. (2021)</xref>; <xref ref-type="bibr" rid="c47">Rösch et al. (2024)</xref>. This is because not all image pairs contribute equally to training. <xref rid="fig2" ref-type="fig">Figure 2c</xref> shows at 2, 000 training batches that some clusters are well-defined (e.g. those inside the pink rectangle) while others remain fuzzy (e.g. those inside the orange square). Images in well-defined clusters have negligible impact on the loss or weight updates, as positive pairs are already close and negative pairs are sufficiently separated. Sampling from these well-defined clusters, therefore, wastes time. In contrast, fuzzy clusters contain images that still contribute significantly to the loss and benefit from further training. To address this, we developed a sampling method that prioritizes pairs from underperforming clusters requiring additional learning, while maintaining baseline sampling for all clusters based on fragment size (see Appendix 3). This ensures consistent updates across the representation space and prevents forgetting in well-defined clusters.</p>
<p>To assign identities to animal images, we perform K-means clustering <xref ref-type="bibr" rid="c49">Sculley (2010)</xref> on the points representing all images of the video in the learned 8-dimensional representation space. Each image is then assigned to a cluster with a probability that increases the closer it is to the cluster center. To evaluate clustering quality, we compute the mean Silhouette score <xref ref-type="bibr" rid="c46">Rousseeuw (1987)</xref>, which quantifies intra-cluster cohesion and inter-cluster separation. A maximum value of 1 indicates ideal clustering. During training, the mean Silhouette score increases (<xref rid="fig2" ref-type="fig">Figure 2d</xref>). We empirically determined that a value of 0.91 for this index corresponds to an identity assignment error below 1% for a single image (<xref rid="fig2" ref-type="fig">Figure 2e</xref>). As a result, we use 0.91 as the stopping criterion for training (see Appendix 3).</p>
<p>After the assignment of identities to images of animals, we run some steps that are common to the previous idtracker.ai. For example, we make a final assignment of all images in fragments as each fragment must have all assignments to be the same, eliminating some errors in individual images. Also, an algorithm already present in idTracker assigns identities in the animal’s crossings taking into account that we know the identities before and after.</p>
<p>The new idtracker.ai (v6) has a higher accuracy than original idtracker.ai (v4) and than its optimized version (v5), <xref rid="fig1" ref-type="fig">Figure 1a</xref> (purple line). Its average accuracy in the benchmark is 99.92% and 99.82% without and with crossings, respectively, an important improvement over the original idtracker.ai v4 (98.39% and 98.24%) and its optimized version v5 (99.63% and 99.49%). It also gives much shorter times than the original idtracker.ai (v4) and its optimized version (v5), <xref rid="fig1" ref-type="fig">Figure 1b</xref> (purple line). It is on average 90 times faster than the original idtracker.ai (v4) and, for the more difficult videos, up to 712 times faster.</p>
<p>As for the original idtracker.ai, the new idtracker.ai can work well with lower resolutions, blur and video compression, and with inhomogeneous light (<xref rid="figS1_2" ref-type="fig">Figure 1—figure Supplement 2</xref>). We also compared the new idtracker.ai to TRex <xref ref-type="bibr" rid="c55">Walter and Couzin (2021)</xref>, which is based on idtracker.ai without pretraining and with additional operations like eroding crossings to make global fragments longer, posture image normalization, tracklet subsampling, and the use of uniqueness feedback during training. TRex gives comparable accuracies to the original idtracker.ai in the benchmark, and it is on average 31 times faster than the original idtracker.ai and up to 316 times faster (<xref rid="figS1_1" ref-type="fig">Figure 1—figure Supplement 1b</xref>). However, the new idtracker.ai is both more accurate and faster than TRex (<xref rid="figS1_1" ref-type="fig">Figure 1—figure Supplement 1</xref>). The mean accuracy of TRex across the benchmark is 98.14% and 97.89% excluding and including animal crossings, respectively. This is noticeably below the values for the new idtracker.ai of 99.92% and 99.82%, respectively. Also, the new idtracker.ai is on average 4.5 times faster and up to 16.5 times faster than TRex. Additionally, the new idtracker.ai has a memory peak lower than TRex (<xref rid="figS1_3" ref-type="fig">Figure 1—figure Supplement 3</xref>).</p>
</sec>
<sec id="s1d">
<title>No need for global fragments</title>
<p>The new idtracker.ai also works in videos in which the original idtracker.ai does not even track because there are no global fragments. Global fragments are absent in videos with very extensive animal occlusions, for example because animals touch or cross more frequently, parts of the setup are covered, or the camera focuses on only a specific region of the setup.</p>
<p>To systematically evaluate this, we digitally masked videos with a sector of angle <italic>θ</italic> (<xref rid="fig3" ref-type="fig">Figure 3</xref>). Masked pixels were enforced as background during segmentation, causing animals to disappear from the video when entering the occluded region. Consequently, each re-entry into the visible area requires re-identification with no knowledge of the identities in the occluded region (see Methods for more details about the occlusion tests).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption>
<title>Tracking with strong occlusions.</title>
<p>Accuracies when we mask a region of a video defined by an angle <italic>θ</italic> and the tracking system has no access to the information behind the mask. Light and dark gray region correspond to the angles for which no global fragments exist in the video. Dark gray regions correspond to angles for which the video has a fragment connectivity lower than 0.5, with the fragment connectivity defined as the average number of other fragments each fragment coexists with, divided by <italic>N</italic> − 1, with <italic>N</italic> the total number of animals; see <xref rid="figS3_1" ref-type="fig">Figure 3—figure Supplement 1</xref>, for an analysis justifying this value of 0.5. The original idtracker.ai (v4) and its optimized version (v5) cannot work in the gray regions, and new idtracker.ai is expected to deteriorate only in the dark gray region.</p>
</caption>
<graphic xlink:href="657023v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3_1" position="float" fig-type="figure">
<label>Figure 3—figure supplement 1.</label>
<caption><title>Fragment connectivity analysis.</title>
<p>The relation between fragment connectivity and the corresponding accuracy for every video and angle <italic>θ</italic> of the new id-tracker.ai in <xref rid="fig3" ref-type="fig">Figure 3</xref>. Fragment connectivity is calculated as the average number of other fragments each fragment coexists with, divided by <italic>N</italic> − 1, with <italic>N</italic> the total number of animals. Values below 0.5 are associated with low accuracy, and idtracker.ai alerts the user in such cases.</p></caption>
<graphic xlink:href="657023v2_figS3_1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The light and dark gray regions in <xref rid="fig3" ref-type="fig">Figure 3</xref> correspond to videos with no global fragments, and the original idtracker.ai and its optimized version declare tracking impossible in these regions. The new idtracker.ai, however, works well until approximately 1/4 of the setup is visible, and afterward it degrades. This shows the limit of the new idtracker.ai. The deterioration happens because, for the clustering process to be successful, we need enough coexisting individual fragments to have both positive and negative examples. We measure this using fragment connectivity, defined as the average number of other fragments a fragment coexists with, divided by <italic>N</italic> − 1, with <italic>N</italic> the total number of animals in the video. Empirically, we find that a fragment connectivity below 0.5 corresponds to low accuracies <xref rid="figS3_1" ref-type="fig">Figure 3—figure Supplement 1</xref>. The new idtracker.ai flags when this condition of low fragment connectivity takes place, which we indicate in <xref rid="fig3" ref-type="fig">Figure 3</xref> with the dark gray regions.</p>
</sec>
<sec id="s1e">
<title>Output of new idtracker.ai</title>
<p>The final output of the new idtracker.ai consists of the <italic>x</italic> − <italic>y</italic> coordinates for each identified animal and video frame. Additionally, it provides an estimation of the achieved accuracy, the Silhouette score as a measure of clustering quality, and the probability of correct identity assignment for each animal and frame. The new idtracker.ai also includes the following tools, for which we also give an example workflow in Appendix 4 and documentation at <ext-link ext-link-type="uri" xlink:href="https://idtracker.ai/latest/user_guide/tools.html">https://idtracker.ai/latest/user_guide/tools.html</ext-link>:</p>
<list list-type="bullet">
<list-item><p><monospace>idtrackerai_inspect_clusters</monospace>, a tool to visually inspect the learned representation space and check the clusters integrity.</p></list-item>
<list-item><p><monospace>idtrackerai_validate</monospace>, a graphic app to review and correct tracking results.</p></list-item>
<list-item><p><monospace>idtrackerai_video</monospace>, a graphic app to generate videos of the computed animal trajectories on top of the original video for visualization. This app also generates individual videos for each animal showing only its cropped region over time to be able to run pose estimators like the ones in <xref ref-type="bibr" rid="c32">Lauer et al. (2022)</xref>; <xref ref-type="bibr" rid="c39">Pereira et al. (2022)</xref>; <xref ref-type="bibr" rid="c50">Segalin et al. (2021)</xref>; <xref ref-type="bibr" rid="c53">Tang et al. (2025)</xref>; <xref ref-type="bibr" rid="c5">Biderman et al. (2024)</xref>.</p></list-item>
<list-item><p><monospace>idmatcher.ai</monospace>, a tool to match identities across multiple recordings (see Appendix 5).</p></list-item>
<list-item><p>Direct integration with SocialNet, a model of collective behavior introduced in <xref ref-type="bibr" rid="c24">Heras et al. (2019)</xref>.</p></list-item>
<list-item><p>Direct integration with <italic>trajectorytools</italic>, a Python package for 2D trajectory processing, and a set of Jupyter Notebooks that uses <italic>trajectorytools</italic> to analyze basic movement properties and spatial relationships between the animals.</p></list-item>
</list>
</sec>
</sec>
<sec id="s3">
<title>Methods</title>
<sec id="s3a">
<title>Software availability</title>
<p>id-tracker.ai is a free and open source project (license GPLv3). Information about its installation and usage can be found on the website <ext-link ext-link-type="uri" xlink:href="https://idtracker.ai/">https://idtracker.ai/</ext-link>. The source code is available in <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/polavieja_lab/idtrackerai">https://gitlab.com/polavieja_lab/idtrackerai</ext-link> and the package is pip-installable from PyPI. All versions can be found in these platforms, specifically <italic>“original idtracker</italic>.<italic>ai (v4)”</italic> as v4.0.12, <italic>“optimized v4 (v5)”</italic> as v5.2.12 and <italic>“new idtracker</italic>.<italic>ai (v6)”</italic> as v6.0.9. We only actively maintain and provide support for the latest version available, having the old ones for archive and reference only.</p>
</sec>
<sec id="s3c">
<title>Tested computer specifications</title>
<p>The software idtracker.ai depends on PyTorch and is thus compatible with any machine that can run PyTorch, including Windows, MacOS, and Linux systems. Although no specific hardware is required, a graphics card is highly recommended for hardware-accelerated machine-learning computations.</p>
<p>Version 6 of idtracker.ai was tested on computers running Ubuntu 24.04, Fedora 41, Windows 11 with NVIDIA GPUs from the 1000 to the 4000 series, and MacOS 15 with Metal chips. The benchmark results presented in this study were performed on a desktop computer running Ubuntu 24.04 LTS 64bit with a AMD Ryzen 9 5950X (32 cores at 3.4 GHz) processor, 128 GB RAM and an NVIDIA GeForce RTX 4090.</p>
</sec>
<sec id="s3d">
<title>Occlusion tests</title>
<p>We clarify here some details from the occlusion tests presented in the section 'No need for global fragments'. The occlusion tests were performed using the same set of videos as in the benchmark. For these tests, we defined an occlusion mask as a region of interest in the software, applied during the segmentation step. When video frames were converted into binary foreground-background images, all pixels inside the mask were treated as background.</p>
<p>The fragments are detected as in any other video. Specifically, this means that when one animal is hidden in the mask, the animal is lost and the fragment breaks. Fragments are not built adding artificial links.</p>
<p>For evaluation, the ground truth contained the positions of all animals at all times, but, obviously, only positions outside the mask were used to compute tracking accuracy. To avoid partial detections of the animals, trajectories within 15 pixels of the mask boundary (75 pixels for mice videos) were excluded from the evaluation.</p>
</sec>
</sec>
</body>
<back>
<sec id="das" sec-type="data-availability">
<title>Data availability</title>
<p>All videos used in this study, their tracking parameters and human-validated groundtruth can be found in our data repository at <ext-link ext-link-type="uri" xlink:href="https://idtracker.ai/">https://idtracker.ai/</ext-link>.</p>
</sec>
<sec id="s4" sec-type="supplementary">
<title>Supplementary tables</title>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Supplementary Table 1.</label>
<caption><title>Performance of original idtracker.ai (v4) in the benchmark.</title></caption>
<graphic xlink:href="657023v2_tblS1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>Supplementary Table 2.</label>
<caption><title>Performance of optimized v4 (v5) in the benchmark.</title></caption>
<graphic xlink:href="657023v2_tblS2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tblS3" orientation="portrait" position="float">
<label>Supplementary Table 3.</label>
<caption><title>Performance of new idtracker.ai (v6) in the benchmark.</title></caption>
<graphic xlink:href="657023v2_tblS3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tblS4" orientation="portrait" position="float">
<label>Supplementary Table 4.</label>
<caption><title>Performance of TRex in the benchmark.</title></caption>
<graphic xlink:href="657023v2_tblS4.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank Alfonso Perez-Escudero, Paco Romero-Ferrero, Francisco J. Hernandez Heras, Madalena Valente for discussions, and three anonymous reviewers for many suggestions. This work was supported by Fundaçao para a Ciência e Tecnologia PTDC/BIA-COM/5770/2020 (to G.G.dP.) and Champalimaud Foundation (to G.G.dP.).</p>
</ack>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
<sec id="s2">
<title>Author contributions</title>
<p>T.C. and G.G.dP. devised project and main algorithm, T.C. performed tests of the algorithm as stand alone, J.T. developed version 5, implemented the new algorithm into idtracker.ai architecture and made final tests with help from T.C., G.G.dP. supervised project, J.T. and T.C. wrote the Methods and Appendices with help from G.G.dP., and G.G.dP. wrote the main text with help from J.T and T.C.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Awasthi</surname> <given-names>P</given-names></string-name>, <string-name><surname>Dikkala</surname> <given-names>N</given-names></string-name>, <string-name><surname>Kamath</surname> <given-names>P</given-names></string-name></person-group>, <source>Do More Negative Samples Necessarily Hurt in Contrastive Learning?</source>; <year>2022</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2205.01789">https://arxiv.org/abs/2205.01789</ext-link>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bengio</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Courville</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vincent</surname> <given-names>P.</given-names></string-name></person-group> <article-title>Representation Learning: A Review and New Perspectives</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2013</year> <month>Aug</month>; <volume>35</volume>(<issue>8</issue>):<fpage>1798</fpage>–<lpage>1828</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2013.50</pub-id>, doi: <pub-id pub-id-type="doi">10.1109/TPAMI.2013.50</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bernardes</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Lima</surname> <given-names>MAP</given-names></string-name>, <string-name><surname>Guedes</surname> <given-names>RNC</given-names></string-name>, <string-name><surname>da Silva</surname> <given-names>CB</given-names></string-name>, <string-name><surname>Martins</surname> <given-names>GF</given-names></string-name></person-group>. <article-title>Ethoflow: Computer Vision and Artificial Intelligence-Based Software for Automatic Behavior Analysis</article-title>. <source>Sensors</source>. <year>2021</year>; <volume>21</volume>(<issue>9</issue>). doi: <pub-id pub-id-type="doi">10.3390/s21093237</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bernardin</surname> <given-names>K</given-names></string-name>, <string-name><surname>Stiefelhagen</surname> <given-names>R.</given-names></string-name></person-group> <article-title>Evaluating multiple object tracking performance: the CLEAR MOT metrics</article-title>. <source>EURASIP Journal on Image and Video Processing</source>. <year>2008</year>; <volume>2008</volume>(<issue>1</issue>):<fpage>246309</fpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Biderman</surname> <given-names>D</given-names></string-name>, <string-name><surname>Whiteway</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Hurwitz</surname> <given-names>C</given-names></string-name>, <string-name><surname>Greenspan</surname> <given-names>N</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>RS</given-names></string-name>, <string-name><surname>Vishnubhotla</surname> <given-names>A</given-names></string-name>, <string-name><surname>Warren</surname> <given-names>R</given-names></string-name>, <string-name><surname>Pedraja</surname> <given-names>F</given-names></string-name>, <string-name><surname>Noone</surname> <given-names>D</given-names></string-name>, <string-name><surname>Schartner</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Huntenburg</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Khanal</surname> <given-names>A</given-names></string-name>, <string-name><surname>Meijer</surname> <given-names>GT</given-names></string-name>, <string-name><surname>Noel</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Pan-Vazquez</surname> <given-names>A</given-names></string-name>, <string-name><surname>Socha</surname> <given-names>KZ</given-names></string-name>, <string-name><surname>Urai</surname> <given-names>AE</given-names></string-name>, <string-name><surname>Cunningham</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Sawtell</surname> <given-names>NB</given-names></string-name>, <string-name><surname>Paninski</surname> <given-names>L.</given-names></string-name></person-group> <article-title>Lightning Pose: improved animal pose estimation via semi-supervised learning, Bayesian ensembling and cloud-native open-source tools</article-title>. <source>Nature Methods</source>. <year>2024</year> <month>July</month>; <volume>21</volume>(<issue>7</issue>):<fpage>1316</fpage>–<lpage>1328</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41592-024-02319-1</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Branson</surname> <given-names>K</given-names></string-name>, <string-name><surname>Robie</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Bender</surname> <given-names>J</given-names></string-name>, <string-name><surname>Perona</surname> <given-names>P</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name></person-group>. <article-title>High-throughput ethomics in large groups of Drosophila</article-title>. <source>Nature Methods</source>. <year>2009</year>; <volume>6</volume>(<issue>6</issue>):<fpage>451</fpage>–<lpage>457</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.1328</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/nmeth.1328</pub-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Chen</surname> <given-names>T</given-names></string-name>, <string-name><surname>Kornblith</surname> <given-names>S</given-names></string-name>, <string-name><surname>Norouzi</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>G.</given-names></string-name></person-group> <article-title>A simple framework for contrastive learning of visual representations</article-title>. In: <conf-name>Proceedings of the 37th International Conference on Machine Learning ICML’20</conf-name>, JMLR.org; <year>2020a</year>. p. <fpage>11</fpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Chen</surname> <given-names>T</given-names></string-name>, <string-name><surname>Kornblith</surname> <given-names>S</given-names></string-name>, <string-name><surname>Swersky</surname> <given-names>K</given-names></string-name>, <string-name><surname>Norouzi</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>G.</given-names></string-name></person-group> <article-title>Big self-supervised models are strong semi-supervised learners</article-title>. In: <conf-name>Proceedings of the 34th International Conference on Neural Information Processing Systems NIPS ’20</conf-name>, <publisher-loc>Red Hook, NY, USA</publisher-loc>: <publisher-name>Curran Associates Inc</publisher-name>.; <year>2020b</year>. p. <fpage>13</fpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Chen</surname> <given-names>X</given-names></string-name>, <string-name><surname>He</surname> <given-names>K.</given-names></string-name></person-group> <article-title>Exploring Simple Siamese Representation Learning</article-title>. In: <conf-name>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>; <year>2021</year>. p. <fpage>15745</fpage>–<lpage>15753</lpage>. doi: <pub-id pub-id-type="doi">10.1109/CVPR46437.2021.01549</pub-id>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>R</given-names></string-name>, <string-name><surname>Fang</surname> <given-names>HS</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>YE</given-names></string-name>, <string-name><surname>Bal</surname> <given-names>A</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>H</given-names></string-name>, <string-name><surname>Rock</surname> <given-names>RR</given-names></string-name>, <string-name><surname>Padilla-Coreano</surname> <given-names>N</given-names></string-name>, <string-name><surname>Keyes</surname> <given-names>LR</given-names></string-name>, <string-name><surname>Zhu</surname> <given-names>H</given-names></string-name>, <string-name><surname>Li</surname> <given-names>YL</given-names></string-name>, <string-name><surname>Komiyama</surname> <given-names>T</given-names></string-name>, <string-name><surname>Tye</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>C.</given-names></string-name></person-group> <article-title>AlphaTracker: a multi-animal tracking and behavioral analysis tool</article-title>. <source>Frontiers in Behavioral Neuroscience</source>. <year>2023</year>; <volume>17</volume>. doi: <pub-id pub-id-type="doi">10.3389/fnbeh.2023.1111908</pub-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chiara</surname> <given-names>V</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>SY</given-names></string-name></person-group>. <article-title>AnimalTA: A highly flexible and easy-to-use program for tracking and analysing animal movement in different environments</article-title>. <source>Methods in Ecology and Evolution</source>. <year>2023</year>; <volume>14</volume>(<issue>7</issue>):<fpage>1699</fpage>–<lpage>1707</lpage>. doi: <pub-id pub-id-type="doi">10.1111/2041-210X.14115</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Chopra</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hadsell</surname> <given-names>R</given-names></string-name>, <string-name><surname>LeCun</surname> <given-names>Y.</given-names></string-name></person-group> <article-title>Learning a similarity metric discriminatively, with application to face verification</article-title>. In: <conf-name>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)</conf-name>, vol. <volume>1</volume>; <year>2005</year>. p. <fpage>539</fpage>–<lpage>546</lpage>. doi: <pub-id pub-id-type="doi">10.1109/CVPR.2005.202</pub-id>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crouse</surname> <given-names>DF</given-names></string-name></person-group>. <article-title>On implementing 2D rectangular assignment algorithms</article-title>. <source>IEEE Transactions on Aerospace and Electronic Systems</source>. <year>2016</year>; <volume>52</volume>(<issue>4</issue>):<fpage>1679</fpage>–<lpage>1696</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TAES.2016.140952</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Dong</surname> <given-names>X</given-names></string-name>, <string-name><surname>Shen</surname> <given-names>J.</given-names></string-name></person-group> <chapter-title>Triplet Loss in Siamese Network for Object Tracking</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Ferrari</surname> <given-names>V</given-names></string-name>, <string-name><surname>Hebert</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sminchisescu</surname> <given-names>C</given-names></string-name>, <string-name><surname>Weiss</surname> <given-names>Y</given-names></string-name>, editors</person-group>. <source>Computer Vision – ECCV 2018 Cham</source>: <publisher-name>Springer International Publishing</publisher-name>; <year>2018</year>. p. <fpage>472</fpage>–<lpage>488</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Dosovitskiy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Beyer</surname> <given-names>L</given-names></string-name>, <string-name><surname>Kolesnikov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Weissenborn</surname> <given-names>D</given-names></string-name>, <string-name><surname>Zhai</surname> <given-names>X</given-names></string-name>, <string-name><surname>Unterthiner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Dehghani</surname> <given-names>M</given-names></string-name>, <string-name><surname>Minderer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Heigold</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gelly</surname> <given-names>S</given-names></string-name>, <string-name><surname>Uszkoreit</surname> <given-names>J</given-names></string-name>, <string-name><surname>Houlsby</surname> <given-names>N</given-names></string-name></person-group>, <article-title>An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale</article-title> <source>arXiv</source>; <year>2020</year>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ericsson</surname> <given-names>L</given-names></string-name>, <string-name><surname>Gouk</surname> <given-names>H</given-names></string-name>, <string-name><surname>Loy</surname> <given-names>CC</given-names></string-name>, <string-name><surname>Hospedales</surname> <given-names>TM</given-names></string-name></person-group>. <article-title>Self-Supervised Representation Learning: Introduction, advances, and challenges</article-title>. <source>IEEE Signal Processing Magazine</source>. <year>2022</year>; <volume>39</volume>(<issue>3</issue>):<fpage>42</fpage>–<lpage>62</lpage>. doi: <pub-id pub-id-type="doi">10.1109/MSP.2021.3134634</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Grill</surname> <given-names>J</given-names></string-name>, <string-name><surname>Strub</surname> <given-names>F</given-names></string-name>, <string-name><surname>Altché</surname> <given-names>F</given-names></string-name>, <string-name><surname>Tallec</surname> <given-names>C</given-names></string-name>, <string-name><surname>Richemond</surname> <given-names>PH</given-names></string-name>, <string-name><surname>Buchatskaya</surname> <given-names>E</given-names></string-name>, <string-name><surname>Doersch</surname> <given-names>C</given-names></string-name>, <string-name><surname>Avila Pires</surname> <given-names>B</given-names></string-name>, <string-name><surname>Guo</surname> <given-names>ZD</given-names></string-name>, <string-name><surname>Gheshlaghi Azar</surname> <given-names>M</given-names></string-name>, <string-name><surname>Piot</surname> <given-names>B</given-names></string-name>, <string-name><surname>Kavukcuoglu</surname> <given-names>K</given-names></string-name>, <string-name><surname>Munos</surname> <given-names>R</given-names></string-name>, <string-name><surname>Valko</surname> <given-names>M.</given-names></string-name></person-group> <article-title>Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</article-title>. In: <conf-name>Advances in Neural Information Processing Systems (NeurIPS)</conf-name>; <year>2020</year>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Grunau</surname> <given-names>C</given-names></string-name>, <string-name><surname>Özüdoğru</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Rozhoň</surname> <given-names>V</given-names></string-name>, <string-name><surname>Tětek</surname> <given-names>J</given-names></string-name></person-group>, <article-title>A Nearly Tight Analysis of Greedy k-means++</article-title> <source>arXiv</source>; <year>2022</year>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guo</surname> <given-names>S</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>P</given-names></string-name>, <string-name><surname>Miao</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Shao</surname> <given-names>G</given-names></string-name>, <string-name><surname>Chapman</surname> <given-names>CA</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>X</given-names></string-name>, <string-name><surname>He</surname> <given-names>G</given-names></string-name>, <string-name><surname>Fang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>H</given-names></string-name>, <string-name><surname>Sun</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Shi</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Li</surname> <given-names>B.</given-names></string-name></person-group> <article-title>Automatic Identification of Individual Primates with Deep Learning Techniques</article-title>. <source>iScience</source>. <year>2020</year>; <volume>23</volume>(<issue>8</issue>):<fpage>101412</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.isci.2020.101412</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>He</surname> <given-names>K</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>X</given-names></string-name>, <string-name><surname>Xie</surname> <given-names>S</given-names></string-name>, <string-name><surname>Li</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Dollár</surname> <given-names>P</given-names></string-name>, <string-name><surname>Girshick</surname> <given-names>R.</given-names></string-name></person-group> <article-title>Masked Autoencoders Are Scalable Vision Learners</article-title>. In: <conf-name>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>; <year>2022</year>. p. <fpage>15979</fpage>–<lpage>15988</lpage>. doi: <pub-id pub-id-type="doi">10.1109/CVPR52688.2022.01553</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>He</surname> <given-names>K</given-names></string-name>, <string-name><surname>Fan</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Xie</surname> <given-names>S</given-names></string-name>, <string-name><surname>Girshick</surname> <given-names>R.</given-names></string-name></person-group> <article-title>Momentum Contrast for Unsupervised Visual Representation Learning</article-title>. In: <conf-name>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>; <year>2020</year>. p. <fpage>9726</fpage>–<lpage>9735</lpage>. doi: <pub-id pub-id-type="doi">10.1109/CVPR42600.2020.00975</pub-id>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>He</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Ren</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sun</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Deep Residual Learning for Image Recognition</article-title>. In: <conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>; <year>2016a</year>. p. <fpage>770</fpage>–<lpage>778</lpage>. doi: <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>He</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Ren</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sun</surname> <given-names>J.</given-names></string-name></person-group> <chapter-title>Identity Mappings in Deep Residual Networks</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Leibe</surname> <given-names>B</given-names></string-name>, <string-name><surname>Matas</surname> <given-names>J</given-names></string-name>, <string-name><surname>Sebe</surname> <given-names>N</given-names></string-name>, <string-name><surname>Welling</surname> <given-names>M</given-names></string-name></person-group>, editors. <source>Computer Vision – ECCV 2016</source> <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>; <year>2016b</year>. p. <fpage>630</fpage>–<lpage>645</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heras</surname> <given-names>FJH</given-names></string-name>, <string-name><surname>Romero-Ferrero</surname> <given-names>F</given-names></string-name>, <string-name><surname>Hinz</surname> <given-names>RC</given-names></string-name>, <string-name><surname>de Polavieja</surname> <given-names>GG</given-names></string-name></person-group>. <article-title>Deep attention networks reveal the rules of collective motion in zebrafish</article-title>. <source>PLOS Computational Biology</source>. <year>2019</year> 09; <volume>15</volume>(<issue>9</issue>):<fpage>1</fpage>–<lpage>23</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1007354</pub-id>, doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1007354</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Howard</surname> <given-names>AG</given-names></string-name>, <string-name><surname>Zhu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>B</given-names></string-name>, <string-name><surname>Kalenichenko</surname> <given-names>D</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>W</given-names></string-name>, <string-name><surname>Weyand</surname> <given-names>T</given-names></string-name>, <string-name><surname>Andreetto</surname> <given-names>M</given-names></string-name>, <string-name><surname>Adam</surname> <given-names>H</given-names></string-name></person-group>, <article-title>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</article-title> <source>arXiv</source>; <year>2017</year>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Huang</surname> <given-names>G</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>Z</given-names></string-name>, <string-name><surname>van der Maaten</surname> <given-names>L</given-names></string-name>, <string-name><surname>Weinberger</surname> <given-names>KQ</given-names></string-name></person-group>, <article-title>Densely Connected Convolutional Networks</article-title> <source>arXiv</source>; <year>2016</year>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Iandola</surname> <given-names>FN</given-names></string-name>, <string-name><surname>Han</surname> <given-names>S</given-names></string-name>, <string-name><surname>Moskewicz</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Ashraf</surname> <given-names>K</given-names></string-name>, <string-name><surname>Dally</surname> <given-names>WJ</given-names></string-name>, <string-name><surname>Keutzer</surname> <given-names>K</given-names></string-name></person-group>, <article-title>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</article-title> <source>arXiv</source>; <year>2016</year>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kaya</surname> <given-names>M</given-names></string-name>, <string-name><surname>BiLGE</surname> <given-names>HS</given-names></string-name></person-group>. <article-title>Deep Metric Learning: A Survey</article-title>. <source>Symmetry</source>. <year>2019</year>; <volume>11</volume>(<issue>9</issue>). doi: <pub-id pub-id-type="doi">10.3390/sym11091066</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Khosla</surname> <given-names>P</given-names></string-name>, <string-name><surname>Teterwak</surname> <given-names>P</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>C</given-names></string-name>, <string-name><surname>Sarna</surname> <given-names>A</given-names></string-name>, <string-name><surname>Tian</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Isola</surname> <given-names>P</given-names></string-name>, <string-name><surname>Maschinot</surname> <given-names>A</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>C</given-names></string-name>, <string-name><surname>Krishnan</surname> <given-names>D</given-names></string-name></person-group>, <source>Supervised Contrastive Learning</source>; <year>2021</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2004.11362">https://arxiv.org/abs/2004.11362</ext-link>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Kingma</surname> <given-names>DP</given-names></string-name>, <string-name><surname>Ba</surname> <given-names>J</given-names></string-name></person-group>, <source>Adam: A Method for Stochastic Optimization</source>; <year>2017</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Krizhevsky</surname> <given-names>A</given-names></string-name></person-group>, <article-title>One weird trick for parallelizing convolutional neural networks</article-title> <source>arXiv</source>; <year>2014</year>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lauer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>M</given-names></string-name>, <string-name><surname>Ye</surname> <given-names>S</given-names></string-name>, <string-name><surname>Menegas</surname> <given-names>W</given-names></string-name>, <string-name><surname>Schneider</surname> <given-names>S</given-names></string-name>, <string-name><surname>Nath</surname> <given-names>T</given-names></string-name>, <string-name><surname>Rahman</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Santo</surname> <given-names>VD</given-names></string-name>, <string-name><surname>Soberanes</surname> <given-names>D</given-names></string-name>, <string-name><surname>Feng</surname> <given-names>G</given-names></string-name>, <string-name><surname>Murthy</surname> <given-names>VN</given-names></string-name>, <string-name><surname>Lauder</surname> <given-names>G</given-names></string-name>, <string-name><surname>Dulac</surname> <given-names>C</given-names></string-name>, <string-name><surname>Mathis</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Mathis</surname> <given-names>A.</given-names></string-name></person-group> <article-title>Multi-animal pose estimation, identification and tracking with DeepLabCut</article-title>. <source>Nature Methods</source>. <year>2022</year> <month>April</month>; <volume>19</volume>(<issue>4</issue>):<fpage>496</fpage>–<lpage>504</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-022-01443-0</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/s41592-022-01443-0</pub-id>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Li</surname> <given-names>W</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>R</given-names></string-name>, <string-name><surname>Xiao</surname> <given-names>T</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>X.</given-names></string-name></person-group> <article-title>DeepReID: Deep Filter Pairing Neural Network for Person Re-identification</article-title>. In: <conf-name>2014 IEEE Conference on Computer Vision and Pattern Recognition</conf-name>; <year>2014</year>. p. <fpage>152</fpage>–<lpage>159</lpage>. doi: <pub-id pub-id-type="doi">10.1109/CVPR.2014.27</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liang</surname> <given-names>C</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>X</given-names></string-name>, <string-name><surname>Li</surname> <given-names>B</given-names></string-name>, <string-name><surname>Ye</surname> <given-names>X</given-names></string-name>, <string-name><surname>Zou</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Rethinking the Competition Between Detection and ReID in Multiobject Tracking</article-title>. <source>IEEE Transactions on Image Processing</source>. <year>2020</year>; <volume>31</volume>:<fpage>3182</fpage>–<lpage>3196</lpage>. <ext-link ext-link-type="uri" xlink:href="https://api.semanticscholar.org/CorpusID:225062305">https://api.semanticscholar.org/CorpusID:225062305</ext-link>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Liu</surname> <given-names>S</given-names></string-name>, <string-name><surname>Han</surname> <given-names>L</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>X</given-names></string-name>, <string-name><surname>Ren</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>F</given-names></string-name>, <string-name><surname>YingLiu</surname>, <given-names>Lin Y</given-names></string-name></person-group>, <source>FishMOT: A Simple and Effective Method for Fish Tracking Based on IoU Matching</source>; <year>2023</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2309.02975">https://arxiv.org/abs/2309.02975</ext-link>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Liu</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Lin</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Cao</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Hu</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wei</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Lin</surname> <given-names>S</given-names></string-name>, <string-name><surname>Guo</surname> <given-names>B</given-names></string-name></person-group>, <article-title>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</article-title> <source>arXiv</source>; <year>2021</year>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van der Maaten</surname> <given-names>L</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>G.</given-names></string-name></person-group> <article-title>Visualizing Data using t-SNE</article-title>. <source>Journal of Machine Learning Research</source>. <year>2008</year>; <volume>9</volume>(<issue>86</issue>):<fpage>2579</fpage>–<lpage>2605</lpage>. <ext-link ext-link-type="uri" xlink:href="http://jmlr.org/papers/v9/vandermaaten08a.html">http://jmlr.org/papers/v9/vandermaaten08a.html</ext-link>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Oquab</surname> <given-names>M</given-names></string-name>, <string-name><surname>Darcet</surname> <given-names>T</given-names></string-name>, <string-name><surname>Moutakanni</surname> <given-names>T</given-names></string-name>, <string-name><surname>Vo</surname> <given-names>H</given-names></string-name>, <string-name><surname>Szafraniec</surname> <given-names>M</given-names></string-name>, <string-name><surname>Khalidov</surname> <given-names>V</given-names></string-name>, <string-name><surname>Fernandez</surname> <given-names>P</given-names></string-name>, <string-name><surname>Haziza</surname> <given-names>D</given-names></string-name>, <string-name><surname>Massa</surname> <given-names>F</given-names></string-name>, <string-name><surname>El-Nouby</surname> <given-names>A</given-names></string-name>, <string-name><surname>Assran</surname> <given-names>M</given-names></string-name>, <string-name><surname>Ballas</surname> <given-names>N</given-names></string-name>, <string-name><surname>Galuba</surname> <given-names>W</given-names></string-name>, <string-name><surname>Howes</surname> <given-names>R</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>PY</given-names></string-name>, <string-name><surname>Li</surname> <given-names>SW</given-names></string-name>, <string-name><surname>Misra</surname> <given-names>I</given-names></string-name>, <string-name><surname>Rabbat</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sharma</surname> <given-names>V</given-names></string-name>, <string-name><surname>Synnaeve</surname> <given-names>G</given-names></string-name>, <etal>et al.</etal></person-group> <source>DINOv2: Learning Robust Visual Features without Supervision</source>.. <year>2024</year>; <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2304.07193">https://arxiv.org/abs/2304.07193</ext-link>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname> <given-names>TD</given-names></string-name>, <string-name><surname>Tabris</surname> <given-names>N</given-names></string-name>, <string-name><surname>Matsliah</surname> <given-names>A</given-names></string-name>, <string-name><surname>Turner</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Li</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ravindranath</surname> <given-names>S</given-names></string-name>, <string-name><surname>Papadoyannis</surname> <given-names>ES</given-names></string-name>, <string-name><surname>Normand</surname> <given-names>E</given-names></string-name>, <string-name><surname>Deutsch</surname> <given-names>DS</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>ZY</given-names></string-name>, <string-name><surname>McKenzie-Smith</surname> <given-names>GC</given-names></string-name>, <string-name><surname>Mitelut</surname> <given-names>CC</given-names></string-name>, <string-name><surname>Castro</surname> <given-names>MD</given-names></string-name>, <string-name><surname>D’Uva</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kislin</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sanes</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Kocher</surname> <given-names>SD</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>SSH</given-names></string-name>, <string-name><surname>Falkner</surname> <given-names>AL</given-names></string-name>, <string-name><surname>Shaevitz</surname> <given-names>JW</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>SLEAP: A deep learning system for multi-animal pose tracking</article-title>. <source>Nature Methods</source>. <year>2022</year> <month>April</month>; <volume>19</volume>(<issue>4</issue>):<fpage>486</fpage>–<lpage>495</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-022-01426-1</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/s41592-022-01426-1</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pérez-Escudero</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vicente-Page</surname> <given-names>J</given-names></string-name>, <string-name><surname>Hinz</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Arganda</surname> <given-names>S</given-names></string-name>, <string-name><surname>De Polavieja</surname> <given-names>GG</given-names></string-name></person-group>. <article-title>idTracker: tracking individuals in a group by automatic identification of unmarked animals</article-title>. <source>Nature methods</source>. <year>2014</year>; <volume>11</volume>(<issue>7</issue>):<fpage>743</fpage>–<lpage>748</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Plum</surname> <given-names>F.</given-names></string-name></person-group> <article-title>OmniTrax: A deep learning-driven multi-animal tracking and pose-estimation add-on for Blender</article-title>. <source>Journal of Open Source Software</source>. <year>2024</year>; <volume>9</volume>(<issue>95</issue>):<fpage>5549</fpage>. <pub-id pub-id-type="doi">10.21105/joss.05549</pub-id>, doi: <pub-id pub-id-type="doi">10.21105/joss.05549</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ristani</surname> <given-names>E</given-names></string-name>, <string-name><surname>Solera</surname> <given-names>F</given-names></string-name>, <string-name><surname>Zou</surname> <given-names>R</given-names></string-name>, <string-name><surname>Cucchiara</surname> <given-names>R</given-names></string-name>, <string-name><surname>Tomasi</surname> <given-names>C.</given-names></string-name></person-group> <chapter-title>Performance Measures and a Data Set for Multi-target, Multi-camera Tracking</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Hua</surname> <given-names>G</given-names></string-name>, <string-name><surname>Jégou</surname> <given-names>H</given-names></string-name></person-group>, editors. <source>Computer Vision – ECCV 2016 Workshops Cham</source>: <publisher-name>Springer International Publishing</publisher-name>; <year>2016</year>. p. <fpage>17</fpage>–<lpage>35</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Robinson</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Chuang</surname> <given-names>CY</given-names></string-name>, <string-name><surname>Sra</surname> <given-names>S</given-names></string-name>, <string-name><surname>Jegelka</surname> <given-names>S.</given-names></string-name></person-group> <article-title>Contrastive Learning with Hard Negative Samples</article-title>. In: <source>International Conference on Learning Representations</source>; <year>2021</year>. https://openreview.net/forum?id=CR1XOQ0UTh-.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Romero-Ferrero</surname> <given-names>F</given-names></string-name>, <string-name><surname>Bergomi</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Hinz</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Heras</surname> <given-names>FJ</given-names></string-name>, <string-name><surname>De Polavieja</surname> <given-names>GG</given-names></string-name></person-group>. <article-title>Idtracker. ai: tracking all individuals in small or large collectives of unmarked animals</article-title>. <source>Nature methods</source>. <year>2019</year>; <volume>16</volume>(<issue>2</issue>):<fpage>179</fpage>–<lpage>182</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Romero-Ferrero</surname> <given-names>F</given-names></string-name>, <string-name><surname>Heras</surname> <given-names>FJ</given-names></string-name>, <string-name><surname>Rance</surname> <given-names>D</given-names></string-name>, <string-name><surname>de Polavieja</surname> <given-names>GG</given-names></string-name></person-group>. <article-title>A study of transfer of information in animal collectives using deep learning tools</article-title>. <source>Philosophical Transactions of the Royal Society B</source>. <year>2023</year>; <volume>378</volume>(<issue>1874</issue>):<fpage>20220073</fpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rousseeuw</surname> <given-names>PJ</given-names></string-name></person-group>. <article-title>Silhouettes: A graphical aid to the interpretation and validation of cluster analysis</article-title>. <source>Journal of Computational and Applied Mathematics</source>. <year>1987</year>; <volume>20</volume>:<fpage>53</fpage>–<lpage>65</lpage>. doi: <pub-id pub-id-type="doi">10.1016/0377-0427(87)90125-7</pub-id>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Rösch</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Oswald</surname> <given-names>N</given-names></string-name>, <string-name><surname>Geierhos</surname> <given-names>M</given-names></string-name>, <string-name><surname>Libovický</surname> <given-names>J</given-names></string-name></person-group>, <source>Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples</source>; <year>2024</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2403.02875">https://arxiv.org/abs/2403.02875</ext-link>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Schroff</surname> <given-names>F</given-names></string-name>, <string-name><surname>Kalenichenko</surname> <given-names>D</given-names></string-name>, <string-name><surname>Philbin</surname> <given-names>J.</given-names></string-name></person-group> <article-title>FaceNet: A unified embedding for face recognition and clustering</article-title>. In: <conf-name>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name> <publisher-name>IEEE</publisher-name>; <year>2015</year>. <pub-id pub-id-type="doi">10.1109/CVPR.2015.7298682</pub-id>, doi: <pub-id pub-id-type="doi">10.1109/cvpr.2015.7298682</pub-id>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Sculley</surname> <given-names>D.</given-names></string-name></person-group> <article-title>Web-scale k-means clustering</article-title>. In: <conf-name>Proceedings of the 19th International Conference on World Wide Web WWW ’10</conf-name>, <publisher-loc>New York, NY, USA</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>; <year>2010</year>. p. <fpage>1177</fpage>–<lpage>1178</lpage>. <pub-id pub-id-type="doi">10.1145/1772690.1772862</pub-id>, doi: <pub-id pub-id-type="doi">10.1145/1772690.1772862</pub-id>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Segalin</surname> <given-names>C</given-names></string-name>, <string-name><surname>Williams</surname> <given-names>J</given-names></string-name>, <string-name><surname>Karigo</surname> <given-names>T</given-names></string-name>, <string-name><surname>Hui</surname> <given-names>M</given-names></string-name>, <string-name><surname>Zelikowsky</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sun</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Perona</surname> <given-names>P</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Kennedy</surname> <given-names>A.</given-names></string-name></person-group> <article-title>The Mouse Action Recognition System (MARS) software pipeline for automated analysis of social behaviors in mice</article-title>. <source>eLife</source>. <year>2021</year> <month>nov</month>; <volume>10</volume>:<elocation-id>e63720</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.63720</pub-id>, doi: <pub-id pub-id-type="doi">10.7554/eLife.63720</pub-id>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Shrivastava</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gupta</surname> <given-names>A</given-names></string-name>, <string-name><surname>Girshick</surname> <given-names>R.</given-names></string-name></person-group> <article-title>Training Region-Based Object Detectors with Online Hard Example Mining</article-title>. In: <conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>; <year>2016</year>. p. <fpage>761</fpage>–<lpage>769</lpage>. doi: <pub-id pub-id-type="doi">10.1109/CVPR.2016.89</pub-id>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Tan</surname> <given-names>M</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>B</given-names></string-name>, <string-name><surname>Pang</surname> <given-names>R</given-names></string-name>, <string-name><surname>Vasudevan</surname> <given-names>V</given-names></string-name>, <string-name><surname>Sandler</surname> <given-names>M</given-names></string-name>, <string-name><surname>Howard</surname> <given-names>A</given-names></string-name>, <string-name><surname>Le</surname> <given-names>QV</given-names></string-name></person-group>, <article-title>MnasNet: Platform-Aware Neural Architecture Search for Mobile</article-title> <source>arXiv</source>; <year>2018</year>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname> <given-names>G</given-names></string-name>, <string-name><surname>Han</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Sun</surname> <given-names>X</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>R</given-names></string-name>, <string-name><surname>Han</surname> <given-names>M</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Wei</surname> <given-names>P.</given-names></string-name></person-group> <article-title>Anti-drift pose tracker (ADPT): A transformer-based network for robust animal pose estimation cross-species</article-title>. <source>eLife</source>. <year>2025</year> <month>Mar</month>; doi: <pub-id pub-id-type="doi">10.7554/elife.95709.2</pub-id>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teh</surname> <given-names>CH</given-names></string-name>, <string-name><surname>Chin</surname> <given-names>RT</given-names></string-name></person-group>. <article-title>On the detection of dominant points on digital curve</article-title>. <source>Pattern Analysis and Machine Intelligence, IEEE Transactions on</source>. <year>1989</year> 09; <volume>11</volume>:<fpage>859</fpage>–<lpage>872</lpage>. doi: <pub-id pub-id-type="doi">10.1109/34.31447</pub-id>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Walter</surname> <given-names>T</given-names></string-name>, <string-name><surname>Couzin</surname> <given-names>ID</given-names></string-name></person-group>. <article-title>TRex, a fast multi-animal tracking system with markerless identification, and 2D estimation of posture and visual fields</article-title>. <source>eLife</source>. <year>2021</year> <month>feb</month>; <volume>10</volume>:<elocation-id>e64000</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.64000</pub-id>, doi: <pub-id pub-id-type="doi">10.7554/eLife.64000</pub-id>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Wang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Zheng</surname> <given-names>L</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Li</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>S.</given-names></string-name></person-group> <article-title>Towards Real-Time Multi-Object Tracking</article-title>. In: <conf-name>Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI</conf-name> <publisher-loc>Berlin, Heidelberg</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>; <year>2020</year>. p. <fpage>107</fpage>–<lpage>122</lpage>. doi: <pub-id pub-id-type="doi">10.1007/978-3-030-58621-8_7</pub-id>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Xing</surname> <given-names>E</given-names></string-name>, <string-name><surname>Jordan</surname> <given-names>M</given-names></string-name>, <string-name><surname>Russell</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Ng</surname> <given-names>A.</given-names></string-name></person-group> <article-title>Distance Metric Learning with Application to Clustering with Side-Information</article-title>. In: <person-group person-group-type="editor"><string-name><surname>Becker</surname> <given-names>S</given-names></string-name>, <string-name><surname>Thrun</surname> <given-names>S</given-names></string-name>, <string-name><surname>Obermayer</surname> <given-names>K</given-names></string-name>, editors</person-group>. <conf-name>Advances in Neural Information Processing Systems</conf-name>, vol. <volume>15</volume> <publisher-name>MIT Press</publisher-name>; <year>2002</year>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper_files/paper/2002/file/c3e4035af2a1cde9f21e1ae1951ac80b-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2002/file/c3e4035af2a1cde9f21e1ae1951ac80b-Paper.pdf</ext-link>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Yang</surname> <given-names>F</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Dang</surname> <given-names>C</given-names></string-name>, <string-name><surname>Zheng</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Sakti</surname> <given-names>S</given-names></string-name>, <string-name><surname>Nakamura</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>Y.</given-names></string-name></person-group> <source>ReMOTS: Self-Supervised Refining Multi-Object Tracking and Segmentation. ArXiv</source>. <year>2020</year>; abs/2007.03200. <ext-link ext-link-type="uri" xlink:href="https://api.semanticscholar.org/CorpusID">https://api.semanticscholar.org/CorpusID</ext-link>: 220404263.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Yi</surname> <given-names>D</given-names></string-name>, <string-name><surname>Lei</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Liao</surname> <given-names>S</given-names></string-name>, <string-name><surname>Li</surname> <given-names>SZ</given-names></string-name></person-group>. <article-title>Deep Metric Learning for Person Re-identification</article-title>. In: <conf-name>2014 22nd International Conference on Pattern Recognition</conf-name>; <year>2014</year>. p. <fpage>34</fpage>–<lpage>39</lpage>. doi: <pub-id pub-id-type="doi">10.1109/ICPR.2014.16</pub-id>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>X</given-names></string-name>, <string-name><surname>Lin</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sun</surname> <given-names>J</given-names></string-name></person-group>, <article-title>ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</article-title>, <source>arXiv</source>; <year>2017</year>.</mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1">
<title>Appendix 1</title>
<sec id="s5">
<title>Computation of tracking accuracy</title>
<p>We used the idtracker.ai Validator tool (see Appendix 4) to manually generate ground-truth trajectories. The input to the Validator were outputs from idtracker.ai v5, and the Validator facilitates that the user manually corrects the mistakes. The ground-truth obtained includes the position and identity of each animal in every frame, along with their classification as either individual or crossing.</p>
<p>Tracking accuracy is quantified using the standard Identification F1 Score (IDF1), defined in <xref ref-type="bibr" rid="c42">Ristani et al. (2016)</xref> as:
<disp-formula id="eqn1">
<graphic xlink:href="657023v2_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here, TP (true positive) refers to predicted positions that match their ground-truth points within a distance <italic>T</italic>. FP (false positive) are predicted positions with no corresponding groundtruth match within a distance <italic>T</italic>, and FN (false negative) are ground-truth positions with no matching prediction within a distance <italic>T</italic>.</p>
<p>An identity switch in a single frame results in two false positives and two false negatives, as both predicted and ground-truth identities become unmatched. False negatives also occur when the software either fails to detect an animal or loses track of its identity in a given frame.</p>
<p>We rely on IDF1 rather than other multi-object tracking metrics such as MOTA or MOTP <xref ref-type="bibr" rid="c4">Bernardin and Stiefelhagen (2008)</xref>, since the central goal of our system is to maintain consistent identities across time. MOTA (Multiple Object Tracking Accuracy) counts an identity switch as a single error regardless of its persistence, thereby underestimating the severity of prolonged misidentifications. MOTP (Multiple Object Tracking Precision) evaluates how well predicted positions or bounding boxes align with ground-truth locations, focusing on spatial localization accuracy. While useful for detection-centric benchmarks, neither MOTA nor MOTP adequately capture the temporal consistency of identities. In contrast, IDF1 directly reflects whether trajectories preserve correct identity assignments throughout the video, which is the relevant criterion for evaluating our system.</p>
<p>We report two types of accuracy: <italic>accuracy with crossings</italic>, which includes all trajectory points; and <italic>accuracy without crossings</italic>, where points labeled as crossings in the groundtruth are excluded from the evaluation.</p>
<p>We present all results using <italic>T</italic> = 1BL with BL being a body length. We also verified that accuracy remains largely unaffected by the value of <italic>T</italic>. For instance, reducing it to <italic>T</italic> = 0.5BL results in a very small change of the mean accuracy (without crossings) across the benchmark in the new idtracker.ai from 99.923% to 99.908%.</p>
<sec id="s5a">
<title>Benchmark of accuracy and tracking time</title>
<p>To evaluate the tracking time and accuracy of version 4 (4.0.12), 5 (5.2.12) and 6 (6.0.9) of idtracker.ai and version 1.1.9 of TRex, we used a set of 33 videos with their corresponding human-validated ground-truth trajectories. Each video is 10 minutes long, has a frame rate between 25 and 60 fps and features one of three species: mice, drosophila, or zebrafish, with the number of individuals ranging from 2 to 100. Crossing blobs in these videos make, on average, the 2.6% of the total amount of blobs (1.1% for zebrafish, 0.7% for drosophila, and 9.4% for mice videos).</p>
<p>Both idtracker.ai and TRex rely on several tracking parameters. Some of them have default values that typically require no adjustment (e.g., minimum blob size, network and training hyperparameters) or have a single correct value (e.g., number of animals). In contrast, variable parameters do not have a unique correct value but rather a valid range, from which the user must select a specific value to run the system. In idtracker.ai, the only variable parameter is intensity_threshold, whereas in TRex two such parameters exist: threshold and <monospace>track_max_speed</monospace>.</p>
<fig id="figA1_1" position="float" fig-type="figure">
<label>Appendix 1—figure 1.</label>
<caption><title>Protocol 2 failure rate.</title>
<p>Probability for the different tracking systems of not tracking the video with Protocol 2 in idtracker.ai (v4 and v5) and in TRex the probability that it fails without generating trajectories.</p></caption>
<graphic xlink:href="657023v2_figA1_1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>For the variable parameters, choosing one value or another within the valid interval can give different tracking results. For some values, previous versions of idtracker.ai (v4 and v5) can stochastically resort to a protocol for tracking that we call protocol 3 (<xref rid="figA1_1" ref-type="fig">Appendix 1—Figure 1</xref>), a method that can take days to process more complex videos. Similarly, TRex can crash without outputting any trajectory in certain videos, leading to missing accuracy outputs (<xref rid="figA1_1" ref-type="fig">Appendix 1—Figure 1</xref>).</p>
<p>For this reason, simulating a user that, for a given video, runs the tracking software a single time can result in extremely slow tracking for previous versions of idtracker.ai and a failed tracking for TRex. This would give an advantage to the new version v6 of idtracker.ai since it does not have the slow protocol 3 and never crashed in our tests. We thus simulated a more realistic user that, out of up to 5 attempts, uses the first run in which there is no use of the slow protocol 3 in idtracker.ai v4 and v5 (or protocol 3 is used if it consumes the 5 runs) and in TRex the first run in which the software does not crash. The tracking times are then the sum of tracking times of the attempts used.</p>
<p>To simulate our user many times, we first prepared the dataset of tracking runs by repeating the tracking for each video and software until reaching 5 successful runs or a maximum of 35 attempts. For the original version of idtracker.ai, this was limited to 3 successful runs or 7 attempts due to significantly longer tracking times. Each new run uses new random values for the variable parameters sampled from a precomputed interval we consider an expert user can chose from. In successful runs, both accuracy and tracking time are recorded. In failed runs, when idtracker.ai switches to protocol 3 or TRex crashes, only the time until failure is recorded.</p>
<p>We then simulated our user up to 10,000 times per software and video by sampling tracking runs from our dataset of tracking runs to obtain robust estimates of the tracking times and accuracies. <xref rid="fig1" ref-type="fig">Figure 1</xref> and <xref rid="figS1_1" ref-type="fig">Figure 1—figure Supplement 1</xref> report the median accuracies, without and with crossings, respectively, and tracking times. <xref rid="tblS1" ref-type="table">Supplementary Table 1</xref>, <xref rid="tblS2" ref-type="table">Supplementary Table 2</xref>, <xref rid="tblS3" ref-type="table">Supplementary Table 3</xref>, and <xref rid="tblS4" ref-type="table">Supplementary Table 4</xref> present the median, mean, and the 20 and 80 percentiles in v4, v5, v6 and TRex respectively.</p>
<p>To ensure a fair comparison, TGrabs (a video pre-processing tool needed by TRex) is included when running TRex, graphical interfaces are always disabled at runtime to maximize performance, output_interpolate_positions is enabled in TRex and posture estimation was not deactivated nor used in TRex.</p>
</sec>
</sec>
</app>
<app id="app2">
<title>Appendix 2</title>
<sec id="s6">
<title>Improvements to the original idtracker.ai in version 5</title>
<p>Following the last publication of idtracker.ai <xref ref-type="bibr" rid="c44">Romero-Ferrero et al. (2019)</xref>, the software underwent continuous maintenance, including feature additions, performance optimizations, and hyperparameter tuning (released via PyPI from March 2023 for v5.0.0 to June 2024 for v5.2.12). These updates improved the implementation and tracking pipeline but did not alter the core algorithm. Significant advancements were made in user experience, tool availability, processing speed, and memory efficiency. Below, we summarize the most notable changes.</p>
<sec id="s6a">
<title>Blob memory optimization</title>
<p>Blobs are defined as collections of connected pixels belonging to one or more animals. In v4, blobs stored pixel indices, causing memory usage to scale quadratically with blob size. In v5, blobs are represented by simplified contours using the Teh-Chin chain approximation <xref ref-type="bibr" rid="c54">Teh and Chin (1989)</xref>, reducing memory usage by 93% in blob instances. This also accelerated blob-related computations (centroid, orientation, area, overlap, identification image creation, etc.).</p>
</sec>
<sec id="s6b">
<title>Efficient image loading</title>
<p>Identification images are now efficiently loaded on demand from HDF5 files, eliminating the need to load all images into memory. This enables training with all images regardless of video length, with minimal memory usage.</p>
</sec>
<sec id="s6c">
<title>Code optimization</title>
<p>The source code was revised to eliminate speed bottlenecks. The most impactful changes include:</p>
<list list-type="bullet">
<list-item><p>Frame segmentation accelerated by 80% through optimized OpenCV usage.</p></list-item>
<list-item><p>Faster blob-to-blob overlap checks by first evaluating bounding boxes before deeper comparisons.</p></list-item>
<list-item><p>Persistent storage of blob overlap checks to avoid redundant computations when reloading data.</p></list-item>
<list-item><p>Efficient disk access for identification images by reading them in sorted batches, minimizing I/O overhead.</p></list-item>
<list-item><p>Reduced bounding box image sizes to the minimum necessary, lowering memory and processing demands.</p></list-item>
<list-item><p>Optimized and parallelized Torch data loaders for more efficient model training.</p></list-item>
<list-item><p>Caching of computationally expensive properties for blobs, fragments, and global fragments.</p></list-item>
<list-item><p>Sorted fragment lists to speed up coexistence detection.</p></list-item>
</list>
</sec>
<sec id="s6d">
<title>Changes to the identification protocol</title>
<p>In v4, identity assignments to high-confidence fragments were fixed and excluded from downstream correction, regardless of later evidence. In v5, this was relaxed for short fragments (fewer than 4 frames), allowing corrections due to their statistical unreliability and frequent image noise.</p>
</sec>
<sec id="s6e">
<title>Expanded idtracker.ai ecosystem</title>
<p>A restructure of the main graphic user interface and the creation of a new validation app for inspecting and correcting tracking results. Direct integration of idmatcher.ai, originally introduced in <xref ref-type="bibr" rid="c45">Romero-Ferrero et al. (2023)</xref>, that allows propagation of consistent identity labels across multiple recordings. See Appendix 4 for a full guide on the current extra features of idtracker.ai.</p>
</sec>
</sec>
</app>
<app id="app3">
<title>Appendix 3</title>
<p>We direct the reader to Appendix B of <xref ref-type="bibr" rid="c44">Romero-Ferrero et al. (2019)</xref> for the formal definitions of the main objects and algorithms used in old idtracker.ai, which are common to versions 4, 5, and 6:</p>
<list list-type="bullet">
<list-item><p><bold>B.2.1</bold> Segmentation</p></list-item>
<list-item><p><bold>B.2.2</bold> Detection of individual and crossing images</p></list-item>
<list-item><p><bold>B.2.3</bold> Fragmentation</p></list-item>
<list-item><p><bold>B.2.5</bold> Residual identification</p></list-item>
<list-item><p><bold>B.2.6</bold> Post-processing</p></list-item>
<list-item><p><bold>B.2.7</bold> Output</p></list-item>
</list>
<p>The following sections describe in detail the new identification algorithm introduced in the new idtracker.ai (v6).</p>
<sec id="s7">
<title>Network Architecture</title>
<p>Training represents 47% of the total tracking time in our benchmark. To identify the fastest architecture in training, we evaluated 31 models from 10 families of state-of-the-art CNNs and vision transformers, including the CNN used in versions 4 and 5 of idtracker.ai.</p>
<fig id="figA3_1" position="float" fig-type="figure">
<label>Appendix 3—figure 1.</label>
<caption><title>Models comparison</title>
<p>Error in image identification as a function of training time for different deep learning models randomly initialized in 6 test videos. For each network we report the multiply-accumulate operations (MAC) in giga operations (G) (for a batch of 1600 images of size 40×40×1) and the number of parameters in the units of million parameters (M). Every 100 training batches, we perform k-means clustering on a randomly selected set of 20,000 images, assigning identities based on clusters. We then compute the Silhouette score and ground-truth error on the same set. The reported error corresponds to the model with the best Silhouette score observed up to that point.</p></caption>
<graphic xlink:href="657023v2_figA3_1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Within each family, we tested different network sizes and dropout values and plotted the learning curves of the best candidates (<xref rid="figA3_1" ref-type="fig">Appendix 3—Figure 1</xref>). The earlier idtracker.ai CNN performed adequately on simpler videos (e.g., <italic>zebrafish_20</italic>) but struggled in more challenging ones (e.g., <italic>zebrafish_100_2</italic>). Vision transformers (ViT <xref ref-type="bibr" rid="c15">Dosovitskiy et al. (2020)</xref>, Swin <xref ref-type="bibr" rid="c36">Liu et al. (2021)</xref>) converged much more slowly, and in some cases could not be trained due to excessive VRAM requirements. Note that image size typically ranges between 20×20 and 100×100 pixels, and each batch consists of 400 positive and 400 negative image pairs (1,600 images per batch).</p>
<p>We also tested more advanced CNN architectures such as SqueezeNet <xref ref-type="bibr" rid="c27">Iandola et al. (2016)</xref>, MobileNet <xref ref-type="bibr" rid="c25">Howard et al. (2017)</xref>, MnasNet <xref ref-type="bibr" rid="c52">Tan et al. (2018)</xref>, ShuffleNet <xref ref-type="bibr" rid="c60">Zhang et al. (2017)</xref>, DenseNet <xref ref-type="bibr" rid="c26">Huang et al. (2016)</xref>, and AlexNet <xref ref-type="bibr" rid="c31">Krizhevsky (2014)</xref>. None achieved the same error levels within comparable training times as our baseline CNN. In contrast, ResNet <xref ref-type="bibr" rid="c22">He et al. (2016a)</xref> consistently outperformed all other models, offering the best balance between training speed and accuracy across videos.</p>
<fig id="figA3_2" position="float" fig-type="figure">
<label>Appendix 3—figure 2.</label>
<caption><title>ResNet models comparison.</title>
<p>Error in image identification as a function of training time for different deep learning models randomly initialized (except <italic>Pre-trained ResNet18</italic>) in 6 test videos. For each network we report the multiply-accumulate operations (MAC) in giga operations (G) (for a batch of 1600 images of size 40×40×1) and the number of parameters in the units of million parameters (M). Every 100 training batches, we perform k-means clustering on a randomly selected set of 20,000 images, assigning identities based on clusters. We then compute the Silhouette score and ground-truth error on the same set. The reported error corresponds to the model with the best Silhouette score observed up to that point.</p></caption>
<graphic xlink:href="657023v2_figA3_2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Within the ResNet family (<xref rid="figA3_2" ref-type="fig">Appendix 3—Figure 2</xref>), the smallest ResNet18 proved optimal: the largest variants (ResNet101, ResNet152) demanded excessive VRAM and the intermediate sizes (ResNet34 and ResNet50) showed slower convergence. Both v1 and v2 versions <xref ref-type="bibr" rid="c23">He et al. (2016b)</xref> performed equivalently. Pre-training on ImageNet conferred no benefit, likely due to the domain mismatch between natural images and video-specific animal crops.</p>
<p>Embedding dimension was another key hyperparameter. Here, too, there is a trade-off between achieving a robust representation of subtle differences between animals and maintaining a compact network size and efficient training speed. Empirically, an embedding dimension of 8 provided a good trade-off with other values performing similarly (<xref rid="figA3_3" ref-type="fig">Appendix 3—Figure 3</xref>).</p>
<fig id="figA3_3" position="float" fig-type="figure">
<label>Appendix 3—figure 3.</label>
<caption><title>Embedding dimensions comparison.</title>
<p>Error in image identification as a function of training time for different embedding dimensions in 6 test videos. Every 100 training batches, we perform k-means clustering on a randomly selected set of 20,000 images, assigning identities based on clusters. We then compute the Silhouette score and ground-truth error on the same set. The reported error corresponds to the model with the best Silhouette score observed up to that point.</p></caption>
<graphic xlink:href="657023v2_figA3_3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The final <italic>contrastive learning</italic> network (<xref rid="fig2" ref-type="fig">Figure 2b</xref>) is a ResNet18 (v1) with a single-channel input for grayscale images and an 8-unit fully connected output layer with no bias and using the identity as activation function. Networks are randomly initialized unless the user indicates to copy the weights from a previous tracking session (see Appendix 4 for a usage example). Training is performed with the Adam optimizer <xref ref-type="bibr" rid="c30">Kingma and Ba (2017)</xref> at a learning rate of 0.001.</p>
</sec>
<sec id="s8">
<title>Loss function</title>
<p>The contrastive loss function operates on pairs of data points, aiming to minimize the distance between positive pairs and maximize the distance for negative pairs. Being <italic>F</italic><sub><italic>i</italic></sub> a fragment with arbitrary identifier <italic>i</italic> and <italic>I</italic><sub><italic>ik</italic></sub> and image in the fragment <italic>F</italic><sub><italic>i</italic></sub> with arbitrary identifier <italic>k</italic>, the contrastive loss ℒ for a pair of images (<italic>I</italic><sub><italic>ik</italic></sub>, <italic>I</italic><sub><italic>jl</italic></sub>) is defined as:
<disp-formula id="eqn2">
<graphic xlink:href="657023v2_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>D</italic><sub><italic>ik, jl</italic></sub> is the Euclidean distance between the embedding of <italic>I</italic><sub><italic>ik</italic></sub> and <italic>I</italic><sub><italic>jl</italic></sub>, <italic>D</italic><sub>neg</sub> is the minimum allowed distance in a negative pair of images (images coming from coexisting fragments), and <italic>D</italic><sub>pos</sub> is the maximum allowed distance in a positive pair of images (images from the same fragment). It is important to emphasize that the network processes one image at a time, obtaining a single independent point in the representational space for each image. The Euclidean distance between the embeddings for the corresponding pairs of images is computed only afterwards.</p>
<p><italic>D</italic><sub>neg</sub> and <italic>D</italic><sub>pos</sub> serve as thresholds to regulate distances in the embedding space. <italic>D</italic><sub>neg</sub> prevents images from negative pairs from being pushed indefinitely far apart, while <italic>D</italic><sub>pos</sub> prevents the collapse of images from positive pairs into a single point.</p>
<p>These thresholds <italic>D</italic><sub>pos</sub> and <italic>D</italic><sub>neg</sub> are crucial in our problem, where we aim to embed images of the same identity in similar regions of the representational space. This is because of the following reason. We cannot compare all possible pairs of images and are instead limited to the fragment structure of the video to obtain the labels <italic>l</italic><sub><italic>ik, jl</italic></sub>. This limitation means that the loss function does not directly pull together embeddings of the same identity, but rather images from the same fragment. Similarly, the loss does not push apart embeddings of different identities but images from coexisting fragments. <italic>D</italic><sub>pos</sub> helps prevent the collapse of all images from the same fragment to a single point, allowing for the creation of a diffuse region in the representational space where fragments from the same identity are clustered together. <italic>D</italic><sub>neg</sub> prevents excessive scattering, ensuring better compression of the representational space and maintaining the integrity of clusters of images from the same identity.</p>
<p>We use <italic>D</italic><sub>pos</sub> = 1 and <italic>D</italic><sub>neg</sub> = 10. These values were determined empirically to provide effective embeddings and were robust for tracking multiple videos across various species and different numbers of animals (<xref rid="figA3_4" ref-type="fig">Appendix 3—Figure 4</xref>).</p>
<fig id="figA3_4" position="float" fig-type="figure">
<label>Appendix 3—figure 4</label>
<caption><title><italic>D</italic><sub>neg</sub> over <italic>D</italic><sub>pos</sub> comparison.</title>
<p>Error in image identification as a function of training time for different ratios of <italic>D</italic><sub>neg</sub>/<italic>D</italic><sub>pos</sub> in 6 test videos. Every 100 training batches, we perform k-means clustering on a randomly selected set of 20,000 images, assigning identities based on clusters. We then compute the Silhouette score and ground-truth error on the same set. The reported error corresponds to the model with the best Silhouette score observed up to that point.</p></caption>
<graphic xlink:href="657023v2_figA3_4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s9">
<title>Sampling strategy</title>
<p>Ideally, we would create two datasets of image pairs: one containing negative pairs and another containing positive pairs. However, the challenge with this approach is that very long videos or those containing a large number of animals can yield trillions of pairs of images, making the process computationally prohibitive. Therefore, we approach the problem with a hierarchical sampling method. For negative pairs, we first select a pair of coexisting fragments, and then we randomly sample an image from each fragment. For a positive pair, we sample two images from the same fragment.</p>
<p>Following this idea, we start by creating two datasets. The first consists of a list of all the fragments in the video, from which we will sample the positive pairs. The second dataset contains all possible pairs of coexisting fragments in the video. From these lists we exclude all fragments smaller than 4 images to reduce possible noisy blobs.</p>
<p>Since the same set of images is used both for training and prediction, and these are already pre-aligned egocentrically, we do not apply data augmentation. Augmenting the data would not introduce new useful information and could, in fact, slow-down the training process. Therefore, the model is trained directly on the unaltered blob images.</p>
<p>Our tests revealed that large and balanced batches, with an equal number of positive and negative pairs, are ideal for our setting of contrastive learning. Specifically, we choose batches consisting of 400 positive pairs of images and 400 negative pairs of images (1600 images in total), as it was the smaller batch size that did not compromise training speed/accuracy (<xref rid="figA3_5" ref-type="fig">Appendix 3—Figure 5</xref>). Intuitively, large batch sizes allow for a good spread of pairs from a significant proportion of the video, thereby forcing the network to learn a global embedding of the video. Since positive pairs tend to diminish the size of the representational space while negative pairs tend to increase it, a good balance between the two forces the network to compress the representational space while respecting the negative relationships <xref ref-type="bibr" rid="c7">Chen et al. (2020a)</xref>. This balance between positive and negative pairs is somewhat surprising, given that several works emphasize the importance of negative examples over positive ones <xref ref-type="bibr" rid="c1">Awasthi et al. (2022)</xref>; <xref ref-type="bibr" rid="c29">Khosla et al. (2021)</xref>. While we do not yet have an explanation for why this balance appears to perform better in our case, we note that it is not possible to compare all images from one class against those of another, as negative pairs of images can only be sampled from coexisting fragments. Additionally, positive pairs that compress the space can only be sampled from the same fragment and not the same identity. Since we cannot compare images freely and are constrained by the fragment structure of the video, we might need more positive pairs to ensure a higher degree of compression of the representational space, such that not only images from the same fragment are close together, but also images from the same identity.</p>
<fig id="figA3_5" position="float" fig-type="figure">
<label>Appendix 3—figure 5.</label>
<caption><title>Batch size comparison.</title>
<p>Error in image identification as a function of training time for different batch sizes of pairs of images in 6 test videos. Every 100 training batches, we perform k-means clustering on a randomly selected set of 20,000 images, assigning identities based on clusters. We then compute the Silhouette score and ground-truth error on the same set. The reported error corresponds to the model with the best Silhouette score observed up to that point.</p></caption>
<graphic xlink:href="657023v2_figA3_5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The hierarchical sampling allows us to address the question of how to select pairs of fragments to optimize the training speed of the network. Since we sample pairs of fragments rather than directly sampling pairs of images, we need to skew the probability of a pair of fragments being sampled to reflect the number of images they contain. More concretely, let <italic>f</italic><sub><italic>i</italic></sub> be the number of images in fragment <italic>F</italic><sub><italic>i</italic></sub>. For negative relations we define <italic>f</italic><sub><italic>i,j</italic></sub> = <italic>f</italic><sub><italic>i</italic></sub> + <italic>f</italic><sub><italic>j</italic></sub> and set the probability of sampling the pair <italic>F</italic><sub><italic>i</italic></sub>, <italic>F</italic><sub><italic>j</italic></sub>, by their size as:
<disp-formula id="eqn3">
<graphic xlink:href="657023v2_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
For positive pairs, the probability of sampling a given fragment <italic>f</italic><sub><italic>i</italic></sub> is:
<disp-formula id="eqn4">
<graphic xlink:href="657023v2_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
By examining the evolution of the clusters during training (<xref rid="fig2" ref-type="fig">Figure 2c</xref>) it is clear that the learning process is not uniform, as some of the identities become separated sooner than others. <xref rid="fig2" ref-type="fig">Figure 2c</xref> top row second and third columns give us a nice illustration of this phenomenon. The images embedded in the pink rectangle of the representational space already satisfy the loss function, meaning that the negative pairwise relationships are already embedded further away than <italic>D</italic><sub>neg</sub>, and images that form positive pairwise relationships are already embedded closer than <italic>D</italic><sub>pos</sub>. Consequently, the loss function for these pairs is effectively zero, and passing them through the network will not alter the weights, merely prolonging the training process. In contrast, the separation of clusters in the orange square is incomplete, indicating that image pairs in this region still contribute to the loss function. These pairs are more pertinent, as they contain information that the network has yet to learn. To bias the sampling of image pairs towards those that still contribute to the loss function, each pair of fragments is assigned a loss score. When a pair of images is sampled for training, if the loss for that pair is not zero, the loss score for the corresponding pair of fragments is incremented by one. This score then undergoes an exponential decay of 2% per batch. More specifically, let <italic>l</italic><sub><italic>s</italic></sub>(<italic>i, j</italic>) be the loss score of the pair of fragments <italic>F</italic><sub><italic>i</italic></sub> and <italic>F</italic><sub><italic>j</italic></sub>, and ℒ (<italic>I</italic><sub><italic>il</italic></sub>, <italic>I</italic><sub><italic>ik</italic></sub>) the loss of the images <italic>I</italic><sub><italic>il</italic></sub> and <italic>I</italic><sub><italic>ik</italic></sub>. If the pair <italic>I</italic><sub><italic>il</italic></sub> and <italic>I</italic><sub><italic>ik</italic></sub> is sampled the loss score is updated by
<disp-formula id="eqn5">
<graphic xlink:href="657023v2_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The exponential decay is always applied independently to every pair of fragments, regardless of whether the pairs of images were sampled from those fragments in the previous batch of images or not. The loss score is converted into a probability distribution over all pairs of fragments by
<disp-formula id="eqn6">
<graphic xlink:href="657023v2_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The final probability of sampling pairs of fragments is given by
<disp-formula id="eqn7">
<graphic xlink:href="657023v2_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
This balance between these two probabilities can be seen as an exploitation versus exploration paradigm. <italic>P</italic><sub><italic>s</italic></sub>(<italic>F</italic><sub><italic>i</italic></sub>, <italic>F</italic><sub><italic>j</italic></sub>) enforces constant exploration, while <inline-formula id="inline-eqn-1"><inline-graphic xlink:href="657023v2_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> exploits the current state of learning by dynamically updating the sampling probability. This ensures that pairs of fragments containing unlearned knowledge are sampled more frequently, while maintaining a baseline of exploration based on fragment size. We tried several values for <italic>α</italic> and found that a value of <italic>α</italic> around <inline-formula id="inline-eqn-2"><inline-graphic xlink:href="657023v2_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> produced the best decrease of the time required to train the network across a large collection of videos (<xref rid="figA3_6" ref-type="fig">Appendix 3—Figure 6</xref>). It is noteworthy that the failure of the <italic>α</italic> = 0 case renders the contrastive protocol ineffective in solving the tracking problem. This failure occurs because the sampling becomes highly biased to-wards specific regions of the representational space, leading to only local solutions for the separation of negative pairs and the compression of positive pairs. In effect, the network experiences catastrophic forgetting by focusing excessively on small groups of fragments at a time, thereby compromising the embeddings of other images.</p>
<fig id="figA3_6" position="float" fig-type="figure">
<label>Appendix 3—figure 6.</label>
<caption><title>Exploration and exploitation comparison.</title>
<p>Error in image identification as a function of training time for different exploration/exploitation weights <italic>α</italic> in 6 test videos. Every 100 training batches, we perform k-means clustering on a randomly selected set of 20,000 images, assigning identities based on clusters. We then compute the Silhouette score and ground-truth error on the same set. The reported error corresponds to the model with the best Silhouette score observed up to that point.</p></caption>
<graphic xlink:href="657023v2_figA3_6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s10">
<title>Clustering and assignment</title>
<p>After training the network using contrastive loss, we pass all images through the network to generate their corresponding embeddings in the learned representational space. These embeddings are then grouped using K-means clustering <xref ref-type="bibr" rid="c49">Sculley (2010)</xref>. Each cluster ideally represents images of the same identity, as the training process has encouraged the network to place similar images close together and dissimilar ones farther apart in the embedding space. Next, we perform single-image classification, assigning each image a label based on the cluster to which its embedding belongs. Afterwards, the assignment method follows two conditions. If more than half of the images in the video can not be identified and there exist global fragments, the accumulation protocol from the original idtracker.ai is run, see 'Fallback accumulation protocol'. Otherwise, we move straight to residual identification, see 'Residual Identification'.</p>
<p>In order to identify fragments, we not only need an identity prediction for each image but also a probability distribution over all the identities. Let <italic>d</italic><sub><italic>j</italic></sub>(<italic>I</italic><sub><italic>ik</italic></sub>) be the distance of image <italic>I</italic><sub><italic>ik</italic></sub> to the center of cluster <italic>j</italic>. We define the probability of image <italic>I</italic><sub><italic>ik</italic></sub> belonging to identity <italic>j</italic>
<disp-formula id="eqn8">
<graphic xlink:href="657023v2_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Equation (8) is used to emphasize differences in distances between points and clusters, creating a more peaked probability distribution that clearly distinguishes closer clusters from farther ones. The exponent of 7 smooths the probability distribution and reduces the influence of distant clusters, making the assignment more discriminative. In higher-dimensional spaces like the 8-dimensional space in the paper, distances are more spread out, and using a high power helps to counteract this dispersion, resulting in more confident cluster assignments.</p>
<p>If we are in a scenario where no global fragments exist, K-means is initialized with Greedy k-means++ <xref ref-type="bibr" rid="c18">Grunau et al. (2022)</xref>. Otherwise, we use the average embedding of the fragment images in one of the global fragments as initial cluster centers. This approach speeds up and stabilizes convergence, allowing us to better compare clusters as training progresses.</p>
</sec>
<sec id="s11">
<title>Differences with previous work in contrastive/metric learning</title>
<p>Recent advances in representation learning have established powerful, scalable tools for learning visual features without labels. This progress has been spearheaded by contrastive methods such as SimCLR/v2 <xref ref-type="bibr" rid="c7">Chen et al. (2020a</xref>,<xref ref-type="bibr" rid="c8">b)</xref>, SimSiam <xref ref-type="bibr" rid="c9">Chen and He (2021)</xref> and Momentum Contrast (MoCo) <xref ref-type="bibr" rid="c21">He et al. (2020)</xref>, and reconstruction/self-distillation techniques like Masked Autoencoders (MAE) <xref ref-type="bibr" rid="c20">He et al. (2022)</xref> and DINOv2 <xref ref-type="bibr" rid="c38">Oquab et al. (2024)</xref>. Collectively, these frameworks have delivered state-of-the-art results in recognition and feature learning. Their success is demonstrated in specialized domains like human and primate facial recognition <xref ref-type="bibr" rid="c48">Schroff et al. (2015)</xref>; <xref ref-type="bibr" rid="c19">Guo et al. (2020)</xref> as well as in setting new standards for general visual representation quality <xref ref-type="bibr" rid="c7">Chen et al. (2020a</xref>,<xref ref-type="bibr" rid="c8">b)</xref>; <xref ref-type="bibr" rid="c21">He et al. (2020)</xref>.</p>
<p>The success of these models has also underpinned foundational progress in Multiple Object Tracking (MOT) via Re-Identification (ReID). Their influence is seen in the evolution of ReID-driven tracking, from early Deep Metric Learning <xref ref-type="bibr" rid="c59">Yi et al. (2014)</xref> and DeepReID <xref ref-type="bibr" rid="c33">Li et al. (2014)</xref> formulations to the sophisticated MOT methods used today <xref ref-type="bibr" rid="c14">Dong and Shen (2018)</xref>; <xref ref-type="bibr" rid="c34">Liang et al. (2020)</xref>; <xref ref-type="bibr" rid="c56">Wang et al. (2020)</xref>; <xref ref-type="bibr" rid="c58">Yang et al. (2020)</xref>.</p>
<p>Against this backdrop, we clarify how our formulation aligns with, and deliberately departs from, these widely adopted variants.</p>
<list list-type="bullet">
<list-item><p><bold>No image augmentations</bold>. We do not apply rotations or other heavy augmentations to generate positives. Positives and negatives are instead sampled from heuristically tracked fragments: same-fragment crops provide positives, and temporally coexisting fragments provide negatives. Because crops are egocentrically pre-aligned, additional invariances are unnecessary and can suppress fine identity cues that are critical in our setting.</p></list-item>
<list-item><p><bold>No projection head</bold>. Unlike SimCLR-style pipelines that add a projection MLP before the contrastive loss <xref ref-type="bibr" rid="c7">Chen et al. (2020a</xref>,<xref ref-type="bibr" rid="c8">b)</xref>), we train a lightweight encoder to output a direct, low-dimensional embedding for the sole downstream task of identity clustering. Since we do not target broad transfer or multi-task robustness, a projection head intended to filter “nuisance” features is not required.</p></list-item>
<list-item><p><bold>No stop-gradient</bold>. BYOL and SimSiam prevent representational collapse via stop-gradient asymmetry <xref ref-type="bibr" rid="c17">Grill et al. (2020)</xref>; <xref ref-type="bibr" rid="c9">Chen and He (2021)</xref>. In our case, fragments naturally yield abundant, high-quality negatives; balanced positive/negative batches and a loss-aware pair sampler maintain training signal without asymmetry. This preserves simplicity while addressing collapse risks that arise when negatives are scarce.</p></list-item>
<list-item><p><bold>Euclidean margins rather than cosine similarity</bold>. While many contrastive methods operate on L2-normalized features with cosine similarity, we optimize Euclidean distances with explicit thresholds <italic>D</italic><sub>pos</sub> and <italic>D</italic><sub>neg</sub>. These margins give direct control over intra- and inter-cluster spacing and align naturally with downstream K-means assignment used to form identities (as opposed to cosine-margin classifiers).</p></list-item>
<list-item><p><bold>Novel pairs sampling strategy</bold>. Our fragment-level, loss-aware sampling is related to online hard example mining (OHEM) and hard negative sampling for contrastive learning <xref ref-type="bibr" rid="c51">Shrivastava et al. (2016)</xref>; <xref ref-type="bibr" rid="c43">Robinson et al. (2021)</xref>, but it differs in two key ways that are specific to our setting. First, selection happens over <italic>fragments</italic> defined by temporal co-existence rather than over labeled instances or memory-bank entries, and hardness is estimated online from a decayed record of recent non-zero loss, with-out curated queues or global rankers. Second, the convex mixture <inline-formula id="inline-eqn-3"><inline-graphic xlink:href="657023v2_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> makes the policy explicitly <italic>exploration–exploitation</italic>: <italic>P</italic><sub><italic>s</italic></sub> guarantees coverage of the fragment graph (preventing sampling starvation), while <inline-formula id="inline-eqn-4"><inline-graphic xlink:href="657023v2_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> focuses on unresolved regions of the embedding. This bandit-style view clarifies our empirical findings, pure exploitation (<italic>α</italic>=0) collapses onto a few regions (catastrophic forgetting), whereas a mid-range <italic>α</italic> maintains global progress while accelerating convergence.</p></list-item>
</list>
</sec>
<sec id="s12">
<title>Stopping criteria</title>
<p>Stopping network training using the loss function directly can be highly variable, as different video conditions, the number of individuals and the sampling method significantly influence this value. To circumvent this, we use the Silhouette score (SS) <xref ref-type="bibr" rid="c46">Rousseeuw (1987)</xref> of the clusters of the embedded images. Let <italic>d</italic>(<italic>I, J</italic>) be the Euclidean distance between the embeddings of image <italic>I</italic> and <italic>J</italic>, for each image <italic>I</italic>, in cluster <italic>C</italic><sub><italic>a</italic></sub> we compute the mean intra-cluster distance
<disp-formula id="eqn9">
<graphic xlink:href="657023v2_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and the mean nearest-cluster distance
<disp-formula id="eqn10">
<graphic xlink:href="657023v2_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The SS is given by
<disp-formula id="eqn11">
<graphic xlink:href="657023v2_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
To determine when to stop training, every <italic>m</italic> batches we compute the SS by clustering the embeddings of a random sample of the images in the video, generating also a checkpoint of the model. <italic>m</italic> was set to be the maximum between 100 and number of animals in a video times 5. We stop training if: 1) there have been 30 consecutive SS evaluations without any improvement (patience of 30), or 2) there have been 2 consecutive SS evaluations without any improvement but the SS already achieved a value of 0.91. After stopping the training, the model with the highest SS is chosen. A threshold of 0.91 was validated empirically (<xref rid="fig2" ref-type="fig">Figure 2d</xref> and <xref rid="fig2" ref-type="fig">Figure 2e</xref>). The number of images used for the computation of the SS is 1000 times the number of animals.</p>
</sec>
<sec id="s13">
<title>Fallback accumulation protocol</title>
<p>The contrastive approach is considered to fail when it can confidently identify only less than half of the images in the video (with CONTRASTIVE_MIN_ACCUMULATION = 0.5, but users can modify this value). In such cases, and only if the video contains global fragments, protocol 2 from the original idtracker.ai is applied. This is a fallback tracking mechanism and, notably, the system never had to apply it in any of the videos of our benchmark.</p>
<p>The way protocol 2 from the original idtracker.ai is applied in this scenario is as follows. Fragments identified by the contrastive algorithm serve as an initial labeled dataset, effectively creating a synthetic initial global fragment in terms of the original idtracker.ai. Using this dataset, the small CNN from the original idtracker.ai is taken as the new identification network, and trained. The trained model then predicts the identities of the remaining unidentified fragments in the video. Then, quality checks are applied to filter out potentially incorrect identity assignments (see Section B.2.4, <italic>Cascade of training/identification protocols</italic>, in Appendix B of <xref ref-type="bibr" rid="c44">Romero-Ferrero et al. (2019)</xref>). Only fragments passing these checks are accumulated into the training dataset. The identification network is then retrained with the expanded dataset, and the prediction-checks-accumulation cycle is repeated until either (i) 99.95% of the images are assigned or (ii) no further fragments pass the quality criteria. In either case, the pipeline proceeds with the 'Residual Identification' step.</p>
</sec>
<sec id="s14">
<title>Residual Identification</title>
<p>Residual identification addresses fragments that did not pass the quality checks or were excluded due to short length. The identification network (ResNet or the small CNN, depending on whether the contrastive algorithm worked well or failed, respectively), predicts the identities of these fragments and a probabilistic method that accounts for the coexistence of already assigned fragments is applied. Assignments are made iteratively in descending order of confidence, with the highest-confidence fragments resolved first (see Section B.2.5, <italic>Residual identification</italic>, in Appendix B of <xref ref-type="bibr" rid="c44">Romero-Ferrero et al. (2019)</xref>).</p>
<p>After the residual identification, the remaining processing stages are the same as in the original idtracker.ai. These include post-processing to correct impossible trajectory jumps, interpolation through crossing events, and the generation of the final output trajectories.</p>
</sec>
</app>
<app id="app4">
<title>Appendix 4</title>
<sec id="s15">
<title>Example Workflow</title>
<p>A typical workflow with idtracker.ai begins when a user records a video of a multi-animal experiment under laboratory conditions. The process starts by launching the Segmentation app (<xref rid="figA4_1" ref-type="fig">Appendix 4—Figure 1</xref>) using the command idtrackerai (see full documentation at <ext-link ext-link-type="uri" xlink:href="https://idtracker.ai/latest/user_guide/segmentation_app.html">https://idtracker.ai/latest/user_guide/segmentation_app.html</ext-link>). This application guides users through setting the essential tracking parameters, such as the number of animals, background subtraction options, and filters for non-animal objects (using regions of interest and minimum blob size). Once configured, tracking runs automatically. Additionally, the state of an already trained ResNet from another video can be used to initialize the current model’s state using the parameter knowledge_transfer_folder, speeding up the training process. Users can save all these parameters to a file and execute tracking from the terminal or a script with <monospace>idtrackerai --load parameters.toml --track</monospace>, which is useful for remote or batch processing.</p>
<fig id="figA4_1" position="float" fig-type="figure">
<label>Appendix 4—figure 1.</label>
<caption><title>Segmentation GUI.</title>
<p>Enables users to set the basic parameters required for running idtracker.ai.</p></caption>
<graphic xlink:href="657023v2_figA4_1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>After tracking, the software outputs <italic>x</italic>-<italic>y</italic> coordinates for each identified animal in every video frame. These trajectories are saved in multiple formats (CSV, HDF5, Numpy) and include metadata such as:</p>
<list list-type="bullet">
<list-item><p>Video properties (height, width, frame rate, average blob size per identity)</p></list-item>
<list-item><p>An overall estimate of tracking accuracy</p></list-item>
<list-item><p>Identification probabilities for each animal in every frame</p></list-item>
<list-item><p>The Silhouette score achieved during training</p></list-item>
</list>
<p>To assess the robustness of the learned representation space, users can generate a t-SNE plot of the embeddings from a sample of animal images by running the command <monospace>idtrackerai_inspect_clusters</monospace>, as shown in <xref rid="fig2" ref-type="fig">Figure 2c</xref> (see documentation at <ext-link ext-link-type="uri" xlink:href="https://idtracker.ai/latest/user_guide/data_analysis.html">https://idtracker.ai/latest/user_guide/data_analysis.html</ext-link>).</p>
<p>Although idtracker.ai achieves over 99% accuracy on well-recorded videos, some frames may still contain missing or mislabeled animals. To address this, users can launch the Validator app with idtrackerai_validate to manually review and correct tracking results (see documentation at <ext-link ext-link-type="uri" xlink:href="https://idtracker.ai/latest/user_guide/validator.html">https://idtracker.ai/latest/user_guide/validator.html</ext-link>). This tool allows to navigate through video frames, inspect the tracked positions and metadata, and detect and correct errors using integrated plugins (<xref rid="figA4_2" ref-type="fig">Appendix 4—Figure 2</xref>).</p>
<fig id="figA4_2" position="float" fig-type="figure">
<label>Appendix 4—figure 2.</label>
<caption><title>Validator GUI.</title>
<p>Enables users to inspect tracking results, correct errors, and access additional tools.</p></caption>
<graphic xlink:href="657023v2_figA4_2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To share the tracking results, the command idtrackerai_video launches a graphical app (<xref rid="figA4_3" ref-type="fig">Appendix 4—Figure 3</xref>) to generate videos with animal trajectories and optional labels overlaid on the original footage (see documentation at <ext-link ext-link-type="uri" xlink:href="https://idtracker.ai/latest/user_guide/video_generators.html">https://idtracker.ai/latest/user_guide/video_generators.html</ext-link>). It can also produce individual videos for each animal, showing only its cropped region over time. These individual videos can be used as input for pose estimation tools such as DeepLabCut <xref ref-type="bibr" rid="c32">Lauer et al. (2022)</xref>, SLEAP <xref ref-type="bibr" rid="c39">Pereira et al. (2022)</xref>, MARS <xref ref-type="bibr" rid="c50">Segalin et al. (2021)</xref>, ADPT <xref ref-type="bibr" rid="c53">Tang et al. (2025)</xref>, and Lightning Pose <xref ref-type="bibr" rid="c5">Biderman et al. (2024)</xref>.</p>
<fig id="figA4_3" position="float" fig-type="figure">
<label>Appendix 4—figure 3.</label>
<caption><title>Video Generator GUI.</title>
<p>Allows users to define parameters for general and individual video generation.</p></caption>
<graphic xlink:href="657023v2_figA4_3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>When experiments involve the same animals across multiple sessions, users can match identities between these sessions using <monospace>idmatcher.ai</monospace> (originally introduced in <xref ref-type="bibr" rid="c45">Romero-Ferrero et al. (2023)</xref> and now integrated into the idtracker.ai ecosystem; see documentation at <ext-link ext-link-type="uri" xlink:href="https://idtracker.ai/latest/user_guide/idmatcherai.html">https://idtracker.ai/latest/user_guide/idmatcherai.html</ext-link> and Appendix 5). This tool ensures consistent identity labeling across independent recordings, supporting multi-session studies.</p>
<p>The resulting trajectories can be loaded and analyzed in any programming language. For this purpose, we developed the Python package <italic>trajectorytools</italic>. It offers utilities for the analysis of 2D trajectory data, basic movements metrics and spatial relationships between the animals, at <ext-link ext-link-type="uri" xlink:href="https://idtracker.ai/latest/user_guide/data_analysis.html#trajectorytools">https://idtracker.ai/latest/user_guide/data_analysis.html#trajectorytools</ext-link>. In the same URL we also provide a set of Jupyter Notebooks to facilitate its usage providing analysis examples.</p>
<p>Additionally, SocialNet <xref ref-type="bibr" rid="c24">Heras et al. (2019)</xref>, a model of collective behavior, can be seam-lessly applied to the trajectories generated by idtracker.ai to extract social interaction rules, as illustrated in <xref rid="figA4_4" ref-type="fig">Appendix 4—Figure 4</xref> (see documentation at <ext-link ext-link-type="uri" xlink:href="https://idtracker.ai/latest/user_guide/socialnet.html">https://idtracker.ai/latest/user_guide/socialnet.html</ext-link>).</p>
<fig id="figA4_4" position="float" fig-type="figure">
<label>Appendix 4—figure 4</label>
<caption><title>SocialNet output example showing learned attraction-repulsion and alignment areas for social interactions around the focal animal.</title></caption>
<graphic xlink:href="657023v2_figA4_4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</app>
<app id="app5">
<title>Appendix 5</title>
<sec id="s16">
<title>Validity of idmatcher.ai in the new idtracker.ai</title>
<p>idmatcher.ai, first introduced in <xref ref-type="bibr" rid="c45">Romero-Ferrero et al. (2023)</xref>, is an integrated tool in id-tracker.ai to match individual identities across videos of the same animals. After tracking two separate videos with an arbitrary order of the identity labels, the identification networks from both are used to cross-identify animal images from the other video. This generates a confusion matrix from which final identities are obtained by solving the assignment problem with a modified Jonker-Volgenant algorithm <xref ref-type="bibr" rid="c13">Crouse (2016)</xref>.</p>
<p>The reliability of idmatcher.ai was already proven for the networks of the previous versions of idtracker.ai (v5 and previous). To confirm its performance also in v6, several videos from the benchmark are split into two non-overlapping parts with a 200 frames gapbetween the them. Both parts are tracked independently and idmatcher.ai is then applied to match identities between them.</p>
<p>This test (with the videos <italic>z_20, z_60_1, z_100_2, z_100_3, z_80_1, z_80_2, z_80_3, z_100 _1, d_60, d_72, d_80, d_100_1, d_100_2</italic>, and <italic>d_100_3</italic>) presents an average image-level accuracy of 89%, enough for a perfect identity matching with zero errors (see <xref rid="figA5_1" ref-type="fig">Appendix 5—Figure 1</xref> for an specific example with <italic>drosophila_80</italic>).</p>
<fig id="figA5_1" position="float" fig-type="figure">
<label>Appendix 5—figure 1</label>
<caption><title>Confusion matrix between the two parts of <italic>drosophila_80</italic> in v6 of idmatcher.ai.</title>
<p>It contains predictions both from the network trained in the first part with images from the second one and the other way around. In this example, the image-level accuracy is 82.9%, enough for a 100% accurate identity matching.</p></caption>
<graphic xlink:href="657023v2_figA5_1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107602.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3588-7820</contrib-id>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study introduces an advance in multi-animal tracking by reframing identity assignment as a self-supervised contrastive representation learning problem. It eliminates the need for segments of video where all animals are simultaneously visible and individually identifiable, and significantly improves tracking speed, accuracy, and robustness with respect to occlusion. This innovation has implications beyond animal tracking, potentially connecting with advances in behavioral analysis and computer vision. The strength of support for these advances is <bold>compelling</bold> overall, although there were some remaining minor methodological concerns.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107602.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This is a strong paper that presents a clear advance in multi-animal tracking. The authors introduce an updated version of idtracker.ai that reframes identity assignment as a contrastive representation learning problem rather than a classification task requiring global fragments. This change leads to substantial gains in speed and accuracy and removes a known bottleneck in the original system. The benchmarking across species is comprehensive, the results are convincing, and the work significant.</p>
<p>Strengths:</p>
<p>The main strengths are the conceptual shift from classification to representation learning, the clear performance gains, and the improved robustness of the new version. Removing the need for global fragments makes the software much more flexible in practice, and the accuracy and speed improvements are well demonstrated across a diverse set of datasets. The authors' response also provides further support for the method's robustness.</p>
<p>The comparison to other methods is now better documented. The authors clarify which features are used, how failures are defined, how parameters are sampled, and how accuracy is assessed against human-validated data. This helps ensure that the evaluation is fair and that readers can understand the assumptions behind the benchmarks.</p>
<p>The software appears thoughtfully implemented, with GUI updates, integration with pose estimators, and tools such as idmatcher.ai for linking identities across videos. The overall presentation has been improved so that the limitations of the original idtracker.ai, the engineering optimizations, and the new contrastive formulation are more clearly separated. This makes the central ideas and contributions easier to follow.</p>
<p>Weaknesses:</p>
<p>I do not have major remaining criticisms. The authors have addressed my earlier concerns about the clarity and fairness of the comparison with prior methods, the benchmark design, and the memory usage analysis by adding methodological detail and clearly explaining their choices. At this point I view these aspects as transparent features of the experimental design that readers can take into account, rather than weaknesses of the work.</p>
<p>Overall, this is a high-quality paper. The improvements to idtracker.ai are well justified and practically significant, and the authors' response addresses the main concerns about clarity and evaluation. The conceptual contribution, thorough empirical validation, and thoughtful software implementation make this a valuable and impactful contribution to multi-animal tracking.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107602.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors propose a new version of idTracker.ai for animal tracking. Specifically, they apply contrastive learning to embed cropped images of animals into a feature space where clusters correspond to individual animal identities. By doing this, they address the requirement for so-called global fragments - segments of the video, in which all entities are visible/detected at the same time. In general, the new method reduces the long tracking times from the previous versions, while also increasing the average accuracy of assigning the identity labels.</p>
<p>Strengths and weaknesses:</p>
<p>The authors have reorganized and rewritten a substantial portion of their manuscript, which has improved the overall clarity and structure to some extent. In particular, omitting the different protocols enhanced readability. However, all technical details are now in appendix which is now referred to more frequently in the manuscript, which was already the case in the initial submission. These frequent references to the appendix - and even to appendices from previous versions - make it difficult to read and fully understand the method and the evaluations in detail. A more self-contained description of the method within the main text would be highly appreciated.</p>
<p>Furthermore, the authors state that they changed their evaluation metric from accuracy to IDF1. However, throughout the manuscript they continue to refer to &quot;accuracy&quot; when evaluating and comparing results. It is unclear which accuracy metric was used or whether the authors are confusing the two metrics. This point needs clarification, as IDF1 is not an &quot;accuracy&quot; measure but rather an F1-score over identity assignments.</p>
<p>The authors compare the speedups of the new version with those of the previous ones by taking the average. However, it appears that there are striking outliers in the tracking performance data (see Supplementary Table 1-4). Therefore, using the average may not be the most appropriate way to compare. The authors should consider using the median or providing more detailed statistics (e.g., boxplots) to better illustrate the distributions.</p>
<p>The authors did not provide any conclusion or discussion section. Including a concise conclusion that summarizes the main findings and their implications would help to convey the message of the manuscript.</p>
<p>The authors report an improvement in the mean accuracy across all benchmarks from 99.49% to 99.82% (with crossings). While this represents a slight improvement, the datasets used for benchmarking seem relatively simple and already largely &quot;solved&quot;. Therefore, the impact of this work on the field may be limited. It would be more informative to evaluate the method on more challenging datasets that include frequent occlusions, crossings, or animals with similar appearances. The accuracy reported in the main text is &quot;without crossings&quot; - this seems like incomplete evaluation, especially that tracking objects that do not cross seems a straightforward task. Information is missing why crossings are a problem and are dealt with separately. There are several videos with a much lower tracking accuracy, explaining what the challenges of these videos are and why the method fails in such cases would help to understand the method's usability and weak points.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107602.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Torrents</surname>
<given-names>Jordi</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0006-6353-4079</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Costa</surname>
<given-names>Tiago</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8538-1345</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>de Polavieja</surname>
<given-names>Gonzalo G</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5359-3426</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Summary</p>
<p>This is a strong paper that presents a clear advance in multi-animal tracking. The authors introduce an updated version of idtracker.ai that reframes identity assignment as a contrastive learning problem rather than a classification task requiring global fragments. This change leads to gains in speed and accuracy. The method eliminates a known bottleneck in the original system, and the benchmarking across species is comprehensive and well executed. I think the results are convincing and the work is significant.</p>
<p>Strengths</p>
<p>The main strengths are the conceptual shift from classification to representation learning, the clear performance gains, and the fact that the new version is more robust. Removing the need for global fragments makes the software more flexible in practice, and the accuracy and speed improvements are well demonstrated. The software appears thoughtfully implemented, with GUI updates and integration with pose estimators.</p>
<p>Weaknesses</p>
<p>I don't have any major criticisms, but I have identified a few points that should be addressed to improve the clarity and accuracy of the claims made in the paper.</p>
<p>(1) The title begins with &quot;New idtracker.ai,&quot; which may not age well and sounds more promotional than scientific. The strength of the work is the conceptual shift to contrastive representation learning, and it might be more helpful to emphasize that in the title rather than branding it as &quot;new.&quot;</p>
</disp-quote>
<p>We considered using “Contrastive idtracker.ai”. However, we thought that readers could then think that we believe they could use both the old idtracker.ai or this contrastive version. But we want to say that the new version is the one to use as it is better in both accuracy and tracking times. We think “New idtracker.ai” communicates better that this version is the version we recommend.</p>
<disp-quote content-type="editor-comment">
<p>(2) Several technical points regarding the comparison between TRex (a system evaluated in the paper) and idtracker.ai should be addressed to ensure the evaluation is fair and readers are fully informed.</p>
<p>(2.1) Lines 158-160: The description of TRex as based on &quot;Protocol 2 of idtracker.ai&quot; overlooks several key additions in TRex, such as posture image normalization, tracklet subsampling, and the use of uniqueness feedback during training. These features are not acknowledged, and it's unclear whether TRex was properly configured - particularly regarding posture estimation, which appears to have been omitted but isn't discussed. Without knowing the actual parameters used to make comparisons, it's difficult to dassess how the method was evaluated.</p>
</disp-quote>
<p>We added the information about the key additions of TRex in the section “The new idtracker.ai uses representation learning”, lines 153-157. Posture estimation in TRex was not explicitly used but neither disabled during the benchmark; we clarified this in the last paragraph of “Benchmark of accuracy and tracking time”, lines 492-495.</p>
<disp-quote content-type="editor-comment">
<p>(2.2) Lines 162-163: The paper implies that TRex gains speed by avoiding Protocol 3, but in practice, idtracker.ai also typically avoids using Protocol 3 due to its extremely long runtime. This part of the framing feels more like a rhetorical contrast than an informative one.</p>
</disp-quote>
<p>We removed this, see new lines 153-157.</p>
<disp-quote content-type="editor-comment">
<p>(2.3) Lines 277-280: The contrastive loss function is written using the label l, but since it refers to a pair of images, it would be clearer and more precise to write it as l_{I,J}. This would help readers unfamiliar with contrastive learning understand the formulation more easily.</p>
</disp-quote>
<p>We added this change in lines 613-620.</p>
<disp-quote content-type="editor-comment">
<p>(2.4) Lines 333-334: The manuscript states that TRex can fail to track certain videos, but this may be inaccurate depending on how the authors classify failures. TRex may return low uniqueness scores if training does not converge well, but this isn't equivalent to tracking failure. Moreover, the metric reported by TRex is uniqueness, not accuracy. Equating the two could mislead readers. If the authors did compare outputs to human-validated data, that should be stated more explicitly.</p>
</disp-quote>
<p>We observed TRex crashing without outputting any trajectories on some occasions (Appendix 1—figure 1), and this is what we labeled as “failure”. These failures happened in the most difficult videos of our benchmark, that’s why we treated them the same way as idtracker.ai going to P3. We clarified this in new lines 464-469.</p>
<p>The accuracy measured in our benchmark is not estimated but it is human-validated (see section Computation of tracking accuracy in Appendix 1). Both softwares report some quality estimators at the end of a tracking (“estimated accuracy” for idtracker.ai and &quot;uniqueness” for TRex) but these were not used in the benchmark.</p>
<disp-quote content-type="editor-comment">
<p>(2.5) Lines 339-341: The evaluation approach defines a &quot;successful run&quot; and then sums the runtime across all attempts up to that point. If success is defined as simply producing any output, this may not reflect how experienced users actually interact with the software, where parameters are iteratively refined to improve quality.</p>
</disp-quote>
<p>Yes, our benchmark was designed to be agnostic to the different experiences of the user. Also, our benchmark was designed for users that do not inspect the trajectories to choose parameters again not to leave room for potential subjectivity.</p>
<disp-quote content-type="editor-comment">
<p>(2.6) Lines 344-346: The simulation process involves sampling tracking parameters 10,000 times and selecting the first &quot;successful&quot; run. If parameter tuning is randomized rather than informed by expert knowledge, this could skew the results in favor of tools that require fewer or simpler adjustments. TRex relies on more tunable behavior, such as longer fragments improving training time, which this approach may not capture.</p>
</disp-quote>
<p>We precisely used the TRex parameter track_max_speed to elongate fragments for optimal tracking. Rather than randomized parameter tuning, we defined the “valid range” for this parameter so that all values in it would produce a decent fragment structure. We used this procedure to avoid worsening those methods that use more parameters.</p>
<disp-quote content-type="editor-comment">
<p>(2.7) Line 354 onward: TRex was evaluated using two varying parameters (threshold and track_max_speed), while idtracker.ai used only one (intensity_threshold). With a fixed number of samples, this asymmetry could bias results against TRex. In addition, users typically set these parameters based on domain knowledge rather than random exploration.</p>
</disp-quote>
<p>idtracker.ai and TRex have several parameters. Some of them have a single correct value (e.g. number of animals) or the default value that the system computes is already good (e.g. minimum blob size). For a second type of parameters, the system finds a value that is in general not as good, so users need to modify them. In general, users find that for this second type of parameter there is a valid interval of possible values, from which they need to choose a single value to run the system. idtracker.ai has intensity_threshold as the only parameter of this second type and TRex has two: threshold and track_max_speed. For these parameters, choosing one value or another within the valid interval can give different tracking results. Therefore, when we model a user that wants to run the system once except if it goes to P3 (idtracker.ai) or except if it crashes (TRex), it is these parameters we sample from within the valid interval to get a different value for each run of the system. We clarify this in lines 452-469 of the section “Benchmark of accuracy and tracking time”.</p>
<p>Note that if we chose to simply run old idtracker.ai (v4 or v5) or TRex a single time, this would benefit the new idtracker.ai (v6). This is because old idtracker.ai can enter the very slow protocol 3 and TRex can fail to track. So running old idtracker.ai or TRex up to 5 times until old idtracker.ai does not use Protocol 3 and TRex does not fail is to make them as good as they can be with respect to the new idtracker.ai.</p>
<disp-quote content-type="editor-comment">
<p>(2.8) Figure 2-figure supplement 3: The memory usage comparison lacks detail. It's unclear whether RAM or VRAM was measured, whether shared or compressed memory was included, or how memory was sampled. Since both tools dynamically adjust to system resources, the relevance of this comparison is questionable without more technical detail.</p>
</disp-quote>
<p>We modified the text in the caption (new Figure 1-figure supplement 2) adding the kind of memory we measured (RAM) and how we measured it. We already have a disclaimer for this plot saying that memory management depends on the machine's available resources. We agree that this is a simple analysis of the usage of computer resources.</p>
<disp-quote content-type="editor-comment">
<p>(3) While the authors cite several key papers on contrastive learning, they do not use the introduction or discussion to effectively situate their approach within related fields where similar strategies have been widely adopted. For example, contrastive embedding methods form the backbone of modern facial recognition and other image similarity systems, where the goal is to map images into a latent space that separates identities or classes through clustering. This connection would help emphasize the conceptual strength of the approach and align the work with well-established applications. Similarly, there is a growing literature on animal re-identification (ReID), which often involves learning identity-preserving representations across time or appearance changes. Referencing these bodies of work would help readers connect the proposed method with adjacent areas using similar ideas, and show that the authors are aware of and building on this wider context.</p>
</disp-quote>
<p>We have now added a new section in Appendix 3, “Differences with previous work in contrastive/metric learning” (lines 792-841) to include references to previous work and a description of what we do differently.</p>
<disp-quote content-type="editor-comment">
<p>(4) Some sections of the Results text (e.g., lines 48-74) read more like extended figure captions than part of the main narrative. They include detailed explanations of figure elements, sorting procedures, and video naming conventions that may be better placed in the actual figure captions or moved to supplementary notes. Streamlining this section in the main text would improve readability and help the central ideas stand out more clear</p>
</disp-quote>
<p>Thank you for pointing this out. We have rewritten the Results, for example streamlining the old lines 48-74 (new lines 42-48)  by moving the comments about names, files and order of videos to the caption of Figure 1.</p>
<disp-quote content-type="editor-comment">
<p>Overall, though, this is a high-quality paper. The improvements to idtracker.ai are well justified and practically significant. Addressing the above comments will strengthen the work, particularly by clarifying the evaluation and comparisons.</p>
</disp-quote>
<p>We thank the reviewer for the detailed suggestions. We believe we have taken all of them into consideration to improve the ms.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Summary:</p>
<p>This work introduces a new version of the state-of-the-art idtracker.ai software for tracking multiple unmarked animals. The authors aimed to solve a critical limitation of their previous software, which relied on the existence of &quot;global fragments&quot; (video segments where all animals are simultaneously visible) to train an identification classifier network, in addition to addressing concerns with runtime speed. To do this, the authors have both re-implemented the backend of their software in PyTorch (in addition to numerous other performance optimizations) as well as moving from a supervised classification framework to a self-supervised, contrastive representation learning approach that no longer requires global fragments to function. By defining positive training pairs as different images from the same fragment and negative pairs as images from any two co-existing fragments, the system cleverly takes advantage of partial (but high-confidence) tracklets to learn a powerful representation of animal identity without direct human supervision. Their formulation of contrastive learning is carefully thought out and comprises a series of empirically validated design choices that are both creative and technically sound. This methodological advance is significant and directly leads to the software's major strengths, including exceptional performance improvements in speed and accuracy and a newfound robustness to occlusion (even in severe cases where no global fragments can be detected). Benchmark comparisons show the new software is, on average, 44 times faster (up to 440 times faster on difficult videos) while also achieving higher accuracy across a range of species and group sizes. This new version of idtracker.ai is shown to consistently outperform the closely related TRex software (Walter &amp; Couzin, 2021\), which, together with the engineering innovations and usability enhancements (e.g., outputs convenient for downstream pose estimation), positions this tool as an advancement on the state-of-the-art for multi-animal tracking, especially for collective behavior studies.</p>
<p>Despite these advances, we note a number of weaknesses and limitations that are not well addressed in the present version of this paper:</p>
<p>Weaknesses</p>
<p>(1) The contrastive representation learning formulation. Contrastive representation learning using deep neural networks has long been used for problems in the multi-object tracking domain, popularized through ReID approaches like DML (Yi et al., 2014) and DeepReID (Li et al., 2014). More recently, contrastive learning has become more popular as an approach for scalable self-supervised representation learning for open-ended vision tasks, as exemplified by approaches like SimCLR (Chen et al., 2020), SimSiam (Chen et al., 2020), and MAE (He et al., 2021) and instantiated in foundation models for image embedding like DINOv2 (Oquab et al., 2023). Given their prevalence, it is useful to contrast the formulation of contrastive learning described here relative to these widely adopted approaches (and why this reviewer feels it is appropriate):</p>
<p>(1.1) No rotations or other image augmentations are performed to generate positive examples. These are not necessary with this approach since the pairs are sampled from heuristically tracked fragments (which produces sufficient training data, though see weaknesses discussed below) and the crops are pre-aligned egocentrically (mitigating the need for rotational invariance).</p>
<p>(1.2) There is no projection head in the architecture, like in SimCLR. Since classification/clustering is the only task that the system is intended to solve, the more general &quot;nuisance&quot; image features that this architectural detail normally affords are not necessary here.</p>
<p>(1.3) There is no stop gradient operator like in BYOL (Grill et al., 2020\) or SimSiam. Since the heuristic tracking implicitly produces plenty of negative pairs from the fragments, there is no need to prevent representational collapse due to class asymmetry. Some care is still needed, but the authors address this well through a pair sampling strategy (discussed below).</p>
<p>(1.4) Euclidean distance is used as the distance metric in the loss rather than cosine similarity as in most contrastive learning works. While cosine similarity coupled with L2-normalized unit hypersphere embeddings has proven to be a successful recipe to deal with the curse of dimensionality (with the added benefit of bounded distance limits), the authors address this through a cleverly constructed loss function that essentially allows direct control over the intra- and inter-cluster distance (D\_pos and D\_neg). This is a clever formulation that aligns well with the use of K-means for the downstream assignment step.</p>
<p>No concerns here, just clarifications for readers who dig into the review. Referencing the above literature would enhance the presentation of the paper to align with the broader computer vision literature.</p>
</disp-quote>
<p>Thank you for this detailed comparison. We have now added a new section in Appendix 3, “Differences with previous work in contrastive/metric learning” (lines 792-841) to include references to previous work and a description of what we do differently, including the points raised by the reviewer.</p>
<disp-quote content-type="editor-comment">
<p>(2) Network architecture for image feature extraction backbone. As most of the computations that drive up processing time happen in the network backbone, the authors explored a variety of architectures to assess speed, accuracy, and memory requirements. They land on ResNet18 due to its empirically determined performance. While the experiments that support this choice are solid, the rationale behind the architecture selection is somewhat weak. The authors state that: &quot;We tested 23 networks from 8 different families of state-of-the-art convolutional neural network architectures, selected for their compatibility with consumer-grade GPUs and ability to handle small input images (20 × 20 to 100 × 100 pixels) typical in collective animal behavior videos.&quot;</p>
<p>(2.1) Most modern architectures have variants that are compatible with consumer-grade GPUs. This is true of, for example, HRNet (Wang et al., 2019), ViT (Dosovitskiy et al., 2020), SwinT (Liu et al., 2021), or ConvNeXt (Liu et al., 2022), all of which report single GPU training and fast runtime speeds through lightweight configuration or subsequent variants, e.g., MobileViT (Mehta et al., 2021). The authors may consider revising that statement or providing additional support for that claim (e.g., empirical experiments) given that these have been reported to outperform ResNet18 across tasks.</p>
</disp-quote>
<p>Following the recommendation of the reviewer, we tested the architectures SwinT, ConvNeXt and ViT. We found out that none of them outperformed ResNet18 since they all showed a slower learning curve. This would result in higher tracking times. These tests are now included in the section “Network architecture” (lines 550-611).</p>
<disp-quote content-type="editor-comment">
<p>(2.2) The compatibility of different architectures with small image sizes is configurable. Most convolutional architectures can be readily adapted to work with smaller image sizes, including 20x20 crops. With their default configuration, they lose feature map resolution through repeated pooling and downsampling steps, but this can be readily mitigated by swapping out standard convolutions with dilated convolutions and/or by setting the stride of pooling layers to 1, preserving feature map resolution across blocks. While these are fairly straightforward modifications (and are even compatible with using pretrained weights), an even more trivial approach is to pad and/or resize the crops to the default image size, which is likely to improve accuracy at a possibly minimal memory and runtime cost. These techniques may even improve the performance with the architectures that the authors did test out.</p>
</disp-quote>
<p>The only two tested architectures that require a minimum image size are AlexNet and DenseNet. DenseNet proved to underperform ResNet18 in the videos where the images are sufficiently large. We have tested AlexNet with padded images to see that it also performs worse than ResNet18 (see Appendix 3—figure 1).</p>
<p>We also tested the initialization of ResNet18 with pre-trained weights from ImageNet (in Appendix 3—figure 2) and it proved to bring no benefit to the training speed (added in lines 591-592).</p>
<disp-quote content-type="editor-comment">
<p>(2.3) The authors do not report whether the architecture experiments were done with pretrained or randomly initialized weights.</p>
</disp-quote>
<p>We adapted the text to make it clear that the networks are always randomly initialized (lines 591-592, lines 608-609 and the captions of Appendix 3—figure 1 and 2).</p>
<disp-quote content-type="editor-comment">
<p>(2.4) The authors do not report some details about their ResNet18 design, specifically whether a global pooling layer is used and whether the output fully connected layer has any activation function. Additionally, they do not report the version of ResNet18 employed here, namely, whether the BatchNorm and ReLU are applied after (v1) or before (v2) the conv layers in the residual path.</p>
</disp-quote>
<p>We use ResNet18 v1 with no activation function nor bias in its last layer (this has been clarified in the lines 606-608). Also, by design, ResNet has a global average pool right before the last fully connected layer which we did not remove. In response to the reviewer, Resnet18 v2 was tested and its performance is the same as that of v1 (see Appendix 3—figure 1 and lines 590-591).</p>
<disp-quote content-type="editor-comment">
<p>(3) Pair sampling strategy. The authors devised a clever approach for sampling positive and negative pairs that is tailored to the nature of the formulation. First, since the positive and negative labels are derived from the co-existence of pretracked fragments, selection has to be done at the level of fragments rather than individual images. This would not be the case if one of the newer approaches for contrastive learning were employed, but it serves as a strength here (assuming that fragment generation/first pass heuristic tracking is achievable and reliable in the dataset). Second, a clever weighted sampling scheme assigns sampling weights to the fragments that are designed to balance &quot;exploration and exploitation&quot;. They weigh samples both by fragment length and by the loss associated with that fragment to bias towards different and more difficult examples.</p>
<p>(3.1) The formulation described here resembles and uses elements of online hard example mining (Shrivastava et al., 2016), hard negative sampling (Robinson et al., 2020\), and curriculum learning more broadly. The authors may consider referencing this literature (particularly Robinson et al., 2020\) for inspiration and to inform the interpretation of the current empirical results on positive/negative balancing.</p>
</disp-quote>
<p>Following this recommendation, we added references of hard negative mining in the new section “Differences with previous work in contrastive/metric learning”, lines 792-841. Regarding curriculum learning, even though in spirit it might have parallels with our sampling method in the sense that there is a guided training of the network, we believe the approach is more similar to an exploration-exploitation paradigm.</p>
<disp-quote content-type="editor-comment">
<p>(4) Speed and accuracy improvements. The authors report considerable improvements in speed and accuracy of the new idTracker (v6) over the original idTracker (v4?) and TRex. It's a bit unclear, however, which of these are attributable to the engineering optimizations (v5?) versus the representation learning formulation.</p>
<p>(4.1) Why is there an improvement in accuracy in idTracker v5 (L77-81)? This is described as a port to PyTorch and improvements largely related to the memory and data loading efficiency. This is particularly notable given that the progression went from 97.52% (v4; original) to 99.58% (v5; engineering enhancements) to 99.92% (v6; representation learning), i.e., most of the new improvement in accuracy owes to the &quot;optimizations&quot; which are not the central emphasis of the systematic evaluations reported in this paper.</p>
</disp-quote>
<p>V5 was a two year-effort designed to improve time efficiency of v4. It was also a surprise to us that accuracy was higher, but that likely comes from the fact that the substituted code from v4 contained some small bug/s. The improvements in v5 are retained in v6 (contrastive learning) and v6 has higher accuracy and shorter tracking times. The difference in v6 for this extra accuracy and shorter tracking times is contrastive learning.</p>
<disp-quote content-type="editor-comment">
<p>(4.2) What about the speed improvements? Relative to the original (v4), the authors report average speed-ups of 13.6x in v5 and 44x in v6. Presumably, the drastic speed-up in v6 comes from a lower Protocol 2 failure rate, but v6 is not evaluated in Figure 2 - figure supplement 2.</p>
</disp-quote>
<p>Idtracker.ai v5 runs an optimized Protocol 2 and, sometimes, the Protocol 3. But v6 doesn’t run either of them. While P2 is still present in v6 as a fallback protocol when contrastive fails, in our v6 benchmark P2 was never needed. So the v6 speedup comes from replacing both P2 and P3 with the contrastive algorithm.</p>
<p>(5) Robustness to occlusion. A major innovation enabled by the contrastive representation learning approach is the ability to tolerate the absence of a global fragment (contiguous frames where all animals are visible) by requiring only co-existing pairs of fragments owing to the paired sampling formulation. While this removes a major limitation of the previous versions of idtracker.ai, its evaluation could be strengthened. The authors describe an ablation experiment where an arc of the arena is masked out to assess the accuracy under artificially difficult conditions. They find that the v6 works robustly up to significant proportions of occlusions, even when doing so eliminates global fragments.</p>
<disp-quote content-type="editor-comment">
<p>(5.1) The experiment setup needs to be more carefully described.</p>
<p>(5.1.1) What does the masking procedure entail? Are the pixels masked out in the original video or are detections removed after segmentation and first pass tracking is done?</p>
</disp-quote>
<p>The mask is defined as a region of interest in the software. This means that it is applied at the segmentation step where the video frame is converted to a foreground-background binary image. The region of interest is applied here, converting to background all pixels not inside of it. We clarified this in the newly added section Occlusion tests, lines 240-244.</p>
<disp-quote content-type="editor-comment">
<p>(5.1.2) What happens at the boundary of the mask? (Partial segmentation masks would throw off the centroids, and doing it after original segmentation does not realistically model the conditions of entering an occlusion area.)</p>
</disp-quote>
<p>Animals at the boundaries of the mask are partially detected. This can change the location of their detected centroid. That’s why, when computing the ground-truth accuracy for these videos, only the groundtruth centroids that were at minimum 15 pixels further from the mask were considered. We clarified this in the newly added section Occlusion tests, lines 248-251.</p>
<disp-quote content-type="editor-comment">
<p>(5.1.3) Are fragments still linked for animals that enter and then exit the mask area?</p>
</disp-quote>
<p>No artificial fragment linking was added in these videos. Detected fragments are linked the usual way. If one animal hides into the mask, the animal disappears so the fragment breaks.  We clarified this in the newly added section Occlusion tests, lines 245-247.</p>
<disp-quote content-type="editor-comment">
<p>(5.1.4) How is the evaluation done? Is it computed with or without the masked region detections?</p>
</disp-quote>
<p>The groundtruth used to validate these videos contains the positions of all animals at all times. But only the positions outside the mask at each frame were considered to compute the tracking accuracy. We clarified this in the newly added section Occlusion tests, lines 248-251.</p>
<disp-quote content-type="editor-comment">
<p>(5.2) The circular masking is perhaps not the most appropriate for the mouse data, which is collected in a rectangular arena.</p>
</disp-quote>
<p>We wanted to show the same proof of concept in different videos. For that reason, we used to cover the arena parametrized by an angle. In the rectangular arena the circular masking uses an external circle, so it is covering the rectangle parametrized by an angle.</p>
<disp-quote content-type="editor-comment">
<p>(5.3) The number of co-existing fragments, which seems to be the main determinant of performance that the authors derive from this experiment, should be reported for these experiments. In particular, a &quot;number of co-existing fragments&quot; vs accuracy plot would support the use of the 0.25(N-1) heuristic and would be especially informative for users seeking to optimize experimental and cage design. Additionally, the number of co-existing fragments can be artificially reduced in other ways other than a fixed occlusion, including random dropout, which would disambiguate it from potential allocentric positional confounds (particularly relevant in arenas where egocentric pose is correlated with allocentric position).</p>
</disp-quote>
<p>We included the requested analysis about the fragment connectivity in Figure 3-figure supplement 1. We agree that there can be additional ways of reducing co-existing fragments, but we think the occlusion tests have the additional value that there are many real experiments similar to this test.</p>
<disp-quote content-type="editor-comment">
<p>(6) Robustness to imaging conditions. The authors state that &quot;the new idtracker.ai can work well with lower resolutions, blur and video compression, and with inhomogeneous light (Figure 2 - figure supplement 4).&quot; (L156). Despite this claim, there are no speed or accuracy results reported for the artificially corrupted data, only examples of these image manipulations in the supplementary figure.</p>
</disp-quote>
<p>We added this information in the same image, new Figure 1 - figure supplement 3.</p>
<disp-quote content-type="editor-comment">
<p>(7) Robustness across longitudinal or multi-session experiments. The authors reference idmatcher.ai as a compatible tool for this use case (matching identities across sessions or long-term monitoring across chunked videos), however, no performance data is presented to support its usage. This is relevant as the innovations described here may interact with this setting. While deep metric learning and contrastive learning for ReID were originally motivated by these types of problems (especially individuals leaving and entering the FOV), it is not clear that the current formulation is ideally suited for this use case. Namely, the design decisions described in point 1 of this review are at times at odds with the idea of learning generalizable representations owing to the feature extractor backbone (less scalable), low-dimensional embedding size (less representational capacity), and Euclidean distance metric without hypersphere embedding (possible sensitivity to drift). It's possible that data to support point 6 can mitigate these concerns through empirical results on variations in illumination, but a stronger experiment would be to artificially split up a longer video into shorter segments and evaluate how generalizable and stable the representations learned in one segment are across contiguous (&quot;longitudinal&quot;) or discontiguous (&quot;multi-session&quot;) segments.</p>
</disp-quote>
<p>We have now added a test to prove the reliability of idmatcher.ai in v6. In this test, 14 videos are taken from the benchmark and split in two non-overlapping parts (with a 200 frames gap in between). idmatcher.ai is run between the two parts presenting a 100% accuracy identity matching across all of them (see section “Validity of idmatcher.ai in the new idtracker.ai”, lines 969-1008).</p>
<p>We thank the reviewer for the detailed suggestions. We believe we have taken all of them into consideration to improve the ms.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public review):</bold></p>
<p>Summary</p>
<p>The authors propose a new version of idTracker.ai for animal tracking. Specifically, they apply contrastive learning to embed cropped images of animals into a feature space where clusters correspond to individual animal identities.</p>
<p>Strengths</p>
<p>By doing this, the new software alleviates the requirement for so-called global fragments - segments of the video, in which all entities are visible/detected at the same time - which was necessary in the previous version of the method. In general, the new method reduces the tracking time compared to the previous versions, while also increasing the average accuracy of assigning the identity labels.</p>
<p>Weaknesses</p>
<p>The general impression of the paper is that, in its current form, it is difficult to disentangle the old from the new method and understand the method in detail. The manuscript would benefit from a major reorganization and rewriting of its parts. There are also certain concerns about the accuracy metric and reducing the computational time.</p>
</disp-quote>
<p>We have made the following modifications in the presentation:</p>
<p>(1) We have added section tiles to the main text so it is clearer what tracking system we are referring to. For example, we now have sections “Limitation of the original idtracker.ai”, “Optimizing idtracker.ai without changes in the learning method” and “The new idtracker.ai uses representation learning”.</p>
<p>(2) We have completely rewritten all the text of the ms until we start with contrastive learning. Old L20-89 is now L20-L66, much shorter and easier to read.</p>
<p>(3) We have rewritten the first 3 paragraphs in the section “The new idtracker.ai uses representation learning” (lines 68-92).</p>
<p>(4) We now expanded Appendix 3 to discuss the details of our approach  (lines 539-897).  It discusses in detail the steps of the algorithm, the network architecture, the loss function, the sampling strategy, the clustering and identity assignment, and the stopping criteria in training</p>
<p>(5) To cite previous work in detail and explain what we do differently, we have now added in Appendix 3 the new section “Differences with previous work in contrastive/metric learning” (lines 792-841).</p>
<p>Regarding accuracy metrics, we have replaced our accuracy metric with the standard metric IDF1. IDF1 is the standard metric that is applied to systems in which the goal is to maintain consistent identities across time. See also the section in Appendix 1 &quot;Computation of tracking accuracy” (lines 414-436) explaining IDF1 and why this is an appropriate metric for our goal.</p>
<p>Using IDF1 we obtain slightly higher accuracies for the idtracker.ai systems. This is the comparison of mean accuracy over all our benchmark for our previous accuracy score and the new one for the full trajectories:</p>
<p>v4:   97.42% -&gt; 98.24%</p>
<p>v5:   99.41% -&gt; 99.49%</p>
<p>v6:   99.74% -&gt; 99.82%</p>
<p>trex: 97.89% -&gt; 97.89%</p>
<p>We thank the reviewer for the suggestions about presentation and about the use of more standard metrics.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewer #2 (Recommendations for the authors):</bold></p>
<p>(1) Figure 1a: A graphical legend inset would make it more readable since there are multiple colors, line styles, and connecting lines to parse out.</p>
</disp-quote>
<p>Following this recommendation, we added a graphical legend in the old Figure 1 (new Figure 2).</p>
<disp-quote content-type="editor-comment">
<p>(2) L46: &quot;have images&quot; → &quot;has images&quot;.</p>
</disp-quote>
<p>We applied this correction. Line 35.</p>
<disp-quote content-type="editor-comment">
<p>(3) L52: &quot;videos start with a letter for the species (z,**f**,m)&quot;, but &quot;d&quot; is used for fly videos.</p>
</disp-quote>
<p>We applied this correction in the caption of Figure 1.</p>
<disp-quote content-type="editor-comment">
<p>(4) L62: &quot;with Protocol 3 a two-step process&quot; → &quot;with Protocol 3 being a two-step process&quot;.</p>
</disp-quote>
<p>We rewrote this paragraph without mentioning Protocol 3, lines 37-41.</p>
<disp-quote content-type="editor-comment">
<p>(5) L82-89: This is the main statement of the problems that are being addressed here (speed and relaxing the need for global fragments). This could be moved up, emphasized, and made clearer without the long preamble and results on the engineering optimizations in v5. This lack of linearity in the narrative is also evident in the fact that after Figure 1a is cited, inline citations skip to Figure 2 before returning to Figure 1 once the contrastive learning is introduced.</p>
</disp-quote>
<p>We have rewritten all the text until the contrastive learning, (old lines 20-89 are now lines 20-66). The text is shorter, more linear and easier to read.</p>
<disp-quote content-type="editor-comment">
<p>(6) L114: &quot;pairs until the distance D_{pos}&quot; → &quot;pairs until the distance approximates D_{pos}&quot;.</p>
</disp-quote>
<p>We rewrote as “ pairs until the distance 𝐷pos (or 𝐷neg) is reached” in line 107.</p>
<disp-quote content-type="editor-comment">
<p>(7) L570: Missing a right parenthesis in the equation.</p>
</disp-quote>
<p>We no longer have this equation in the ms.</p>
<disp-quote content-type="editor-comment">
<p>(8) L705: &quot;In order to identify fragments we, not only need&quot; → &quot;In order to identify fragments, we not only need&quot;.</p>
</disp-quote>
<p>We applied this correction, Line 775.</p>
<disp-quote content-type="editor-comment">
<p>(9) L819: &quot;probably distribution&quot; → &quot;probability distribution&quot;.</p>
</disp-quote>
<p>We applied this correction, Line 776.</p>
<disp-quote content-type="editor-comment">
<p>(10) L833: &quot;produced the best decrease the time required&quot; → &quot;produced the best decrease of the time required&quot;.</p>
</disp-quote>
<p>We applied this correction, Line 746.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations for the authors):</bold></p>
<p>(1) We recommend rewriting and restructuring the manuscript. The paper includes a detailed explanation of the previous approaches (idTracker and idTracker.ai) and their limitations. In contrast, the description of the proposed method is short and unstructured, which makes it difficult to distinguish between the old and new methods as well as to understand the proposed method in general. Here are a few examples illustrating the problem.</p>
<p>(1.1) Only in line 90 do the authors start to describe the work done in this manuscript. The previous 3 pages list limitations of the original method.</p>
</disp-quote>
<p>We have now divided the main text into sections, so it is clearer what is the previous method (“Limitation of the original idtracker.ai”, lines 28-51), the new optimization we did of this method (“Optimizing idtracker.ai without changes in the learning method”, lines 52-66) and the new contrastive approach that also includes the optimizations (“The new idtracker.ai uses representation learning”, lines 66-164). Also, the new text has now been streamlined until the contrastive section, following your suggestion. You can see that in the new writing the three sections are 25 , 15 and 99 lines. The more detailed section is the new system, the other two are needed as reference, to describe which problem we are solving and the extra new optimizations.</p>
<disp-quote content-type="editor-comment">
<p>(1.2) The new method does not have a distinct name, and it is hard to follow which idtracker.ai is a specific part of the text referring to. Not naming the new method makes it difficult to understand.</p>
</disp-quote>
<p>We use the name new idtracker.ai (v6) so it becomes the current default version. v5 is now obsolete, as well as v4. And from the point of view of the end user, no new name is needed since v6 is just an evolution of the same software they have been using. Also, we added sections in the main text to clarify the ideas in there and indicate the version of idtracker.ai we are referring to.</p>
<disp-quote content-type="editor-comment">
<p>(1.3) There are &quot;Protocol 2&quot; and &quot;Protocol 3&quot; mixed with various versions of the software scattered throughout the text, which makes it hard to follow. There should be some systematic naming of approaches and a listing of results introduced.</p>
</disp-quote>
<p>Following this recommendation we no longer talk about the specific protocols of the old version of idtracker.ai in the main text. We rewritten the explanation of these versions in a more clear and straightforward way, lines 29-36.</p>
<disp-quote content-type="editor-comment">
<p>(2) To this end, the authors leave some important concepts either underexplained or only referenced indirectly via prior work. For example, the explanation of how the fragments are created (line 15) is only explained by the &quot;video structure&quot; and the algorithm that is responsible for resolving the identities during crossings is not detailed (see lines 46-47, 149-150). Including summaries of these elements would improve the paper's clarity and accessibility.</p>
</disp-quote>
<p>We listed the specific sections from our previous publication where the reader can find information about the entire tracking pipeline (lines 539-549). This way, we keep the ms clear and focused on the new identification algorithm while indicating where to find such information.</p>
<disp-quote content-type="editor-comment">
<p>(3) Accuracy metrics are not clear. In line 319, the authors define it as based on &quot;proportion of errors in the trajectory&quot;. This proportion is not explained. How is the error calculated if a trajectory is lost or there are identity swaps? Multi-object tracking has a range of accuracy metrics that account for such events but none of those are used by the authors. Estimating metrics that are common for MOT literature, for example, IDF1, MOTA, and MOTP, would allow for better method performance understanding and comparison.</p>
</disp-quote>
<p>In the new ms, we replaced our accuracy metric with the standard metric IDF1. IDF1 is the standard metric that is applied to systems in which the goal is to maintain consistent identities across time. See also the section in Appendix 1 &quot;Computation of tracking accuracy” explaining why IDF1 and not MOTA or MOTP is the adequate metric for a system that wants to give correct tracking by identification in time. See lines 416-436.</p>
<p>Using IDF1 we obtain slightly higher accuracies for the idtracker.ai systems. This is the comparison of mean accuracy four our previous accuracy and the new one for the full trajectories:</p>
<p>v4:   97.42% -&gt; 98.24%</p>
<p>v5:   99.41% -&gt; 99.49%</p>
<p>v6:   99.74% -&gt; 99.82%</p>
<p>trex: 97.89% -&gt; 97.89%</p>
<disp-quote content-type="editor-comment">
<p>(4) Additionally, the authors distinguish between tracking with and without crossings, but do not provide statistics on the frequency of crossings per video. It is also unclear how the crossings are considered for the final output. Including information such as the frame rate of the videos would help to better understand the temporal resolution and the differences between consecutive frames of the videos.</p>
</disp-quote>
<p>We added this information in the Appendix 1 “Benchmark of accuracy and tracking time”, lines 445-451. The framerate in our benchmark videos goes from 25 to 60 fps (average of 37 fps). On average 2.6% of the blobs are crossings (1.1% for zebrafish 0.7% for drosophila 9.4% for mice).</p>
<disp-quote content-type="editor-comment">
<p>(5) In the description of the dataset used for evaluation (lines 349-365), the authors describe the random sampling of parameter values for each tracking run. However, it is unclear whether the same values were used across methods. Without this clarification, comparisons between the proposed method, older versions, and TRex might be biased due to lucky parameter combinations. In addition, the ranges from which the values were randomly sampled were also not described.</p>
</disp-quote>
<p>Only one parameter is shared between idtracker.ai and TRex: intensity_threshold (in idtracker.ai) and threshold (in TRex). Both are conceptually equivalent but differ in their numerical values since they affect different algorithms. V4, v5, and TRex each required the same process of independent expert visual inspection of the segmentation to select the valid value range. Since versions 5 and 6 use exactly the same segmentation algorithm, they share the same parameter ranges.</p>
<p>All the ranges of valid values used in our benchmark are public here <ext-link ext-link-type="uri" xlink:href="https://drive.google.com/drive/folders/1tFxdtFUudl02ICS99vYKrZLeF28TiYpZ">https://drive.google.com/drive/folders/1tFxdtFUudl02ICS99vYKrZLeF28TiYpZ</ext-link> as stated in the section “Data availability”, lines 227-228.</p>
<disp-quote content-type="editor-comment">
<p>(6) Lines 122-123, Figure 1c. &quot;batches&quot; - is an imprecise metric of training time as there is no information about the batch size.</p>
</disp-quote>
<p>We clarified the Figure caption, new Figure 2c.</p>
<disp-quote content-type="editor-comment">
<p>(7) Line 145 - &quot;we run some steps... For example...&quot; leaves the method description somewhat unclear. It would help if you could provide more details about how the assignments are carried out and which metrics are being used.</p>
</disp-quote>
<p>Following this recommendation, we listed the specific sections from our previous publication where the reader can find information about the entire tracking pipeline (lines 539-549). This way, we keep the ms clear and focused on the new identification algorithm while indicating where to find such information.</p>
<disp-quote content-type="editor-comment">
<p>(8) Figure 3. How is tracking accuracy assessed with occlusions? Are the individuals correctly recognized when they reappear from the occluded area?</p>
</disp-quote>
<p>The groundtruth for this video contains the positions of all animals at all times. Only the groundtruth points inside the region of interest are taken into account when computing the accuracy. When the tracking reaches high accuracy, it means that animals are successfully relabeled every time they enter the non-masked region. Note that this software works all the time by identification of animals, so crossings and occlusion are treated the same way. What is new here is that the occlusions are so large that there are no global fragments. We clarified this in the new section “Occlusion tests” in Methods, lines 239-251.</p>
<disp-quote content-type="editor-comment">
<p>(9) Lines 185-187 this part of the sentence is not clear.</p>
</disp-quote>
<p>We rewrote this part in a clearer way, lines 180-182.</p>
<disp-quote content-type="editor-comment">
<p>(10) The authors also highlight the improved runtime performance. However, they do not provide a detailed breakdown of the time spent on each component of the tracking/training pipeline. A timing breakdown would help to compare the training duration with the other components. For example, the calculation of the Silhouette Score alone can be time-consuming and could be a bottleneck in the training process. Including this information would provide a clearer picture of the overall efficiency of the method.</p>
</disp-quote>
<p>We measured that the training of ResNet takes on average in our benchmark 47% of the tracking time (we added this information line 551 section “Network Architecture”). In this training stage the bottleneck becomes the network forward and backward pass, limited by the GPU performance. All other processes happening during training have been deeply optimized and parallelized when needed so their contribution to the training time is minimal. Apart from the training, we also measured 24.4% of the total tracking time spent in reading and segmenting the video files and 11.1% in processing the identification images and detecting crossings.</p>
<disp-quote content-type="editor-comment">
<p>(11) An important part of the computational cost is related to model training. It would be interesting to test whether a model trained on one video of a specific animal type (e.g., zebrafish_5) generalizes to another video of the same type (e.g., zebrafish_7). This would assess the model's generalizability across different videos of the same species and spare a lot of compute. Alternatively, instead of training a model from scratch for each video, the authors could also consider training a base model on a superset of images from different videos and then fine-tuning it with a lower learning rate for each specific video. This could potentially save time and resources while still achieving good performance.</p>
</disp-quote>
<p>Already before v6, there was the possibility for the user to start training the identification network by copying the final weights from another tracking session. This knowledge transfer feature is still present in v6 and it still decreases the training times significatively. This information has been added in Appendix 4, lines 906-909.</p>
<p>We have already begun working on the interesting idea of a general base model but it brings some complex challenges. It could be a very useful new feature for future idtracker.ai releases.</p>
<p>We thank the reviewer for the many suggestions. We have implemented all of them.</p>
</body>
</sub-article>
</article>