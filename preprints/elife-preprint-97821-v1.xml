<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">97821</article-id>
<article-id pub-id-type="doi">10.7554/eLife.97821</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.97821.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Biochemistry and Chemical Biology</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Establishing the foundations for a data-centric AI approach for virtual drug screening through a systematic assessment of the properties of chemical data</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4049-8080</contrib-id>
<name>
<surname>Chong</surname>
<given-names>Allen</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">^</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Phua</surname>
<given-names>Ser-Xian</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Xiao</surname>
<given-names>Yunzhi</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ng</surname>
<given-names>Woon Yee</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Hoi Yeung</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3863-7501</contrib-id>
<name>
<surname>Goh</surname>
<given-names>Wilson Wen Bin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="corresp" rid="cor1">^</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Lee Kong Chian School of Medicine, Nanyang Technological University</institution>, <country>Singapore</country> 636921</aff>
<aff id="a2"><label>2</label><institution>School of Biological Science, Nanyang Technological University</institution>, <country>Singapore</country> 637551</aff>
<aff id="a3"><label>3</label><institution>Center for Biomedical Informatics, Nanyang Technological University</institution>, <country>Singapore</country> 636921</aff>
<aff id="a4"><label>4</label><institution>School of Computer Science and Engineering, Nanyang Technological University</institution>, <country>Singapore</country> 639798</aff>
<aff id="a5"><label>5</label><institution>Center for AI in Medicine, Nanyang Technological University</institution>, <country>Singapore</country> 636921</aff>
<aff id="a6"><label>6</label><institution>Division of Neurology, Department of Brain Sciences, Faculty of Medicine, Imperial College London</institution> London W12 0NN</aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Talevi</surname>
<given-names>Alan</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>National University of La Plata</institution>
</institution-wrap>
<city>La Plata</city>
<country>Argentina</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Walczak</surname>
<given-names>Aleksandra M</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>CNRS</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>^</label>Corresponding authors Goh, W.W.B (<email>wilsongoh@ntu.edu.sg</email>), <ext-link ext-link-type="uri" xlink:href="https://gohwils.github.io/biodatascience/">https://gohwils.github.io/biodatascience/</ext-link>, Chong, A. (<email>chongallen@yahoo.com</email>; <email>kimsanallen.chong@ntu.edu.sg</email>)</corresp>
<fn id="n1" fn-type="equal"><label>*</label><p>These authors contributed equally to this work</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-06-14">
<day>14</day>
<month>06</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP97821</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-03-28">
<day>28</day>
<month>03</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-04-04">
<day>04</day>
<month>04</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.03.28.587184"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Chong et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Chong et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-97821-v1.pdf"/>
<abstract>
<title>Summary</title><p>Researchers have adopted model-centric artificial intelligence (AI) approaches in cheminformatics by using newer, more sophisticated AI methods to take advantage of growing chemical libraries. It has been shown that complex deep learning methods outperform conventional machine learning (ML) methods in QSAR and ligand-based virtual screening<sup><xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c3">3</xref></sup> but such approaches generally lack explanability. Hence, instead of developing more sophisticated AI methods (i.e., pursuing a model-centric approach), we wanted to explore the potential of a data-centric AI paradigm for virtual screening. A data-centric AI is an intelligent system that would automatically identify the right type of data to collect, clean and curate for later use by a predictive AI and this is required given the large volumes of chemical data that exist in chemical databases – PubChem alone has over 100 million unique compounds. However, a systematic assessment of the attributes and properties of suitable data is needed. We show here that it is not the result of deficiencies in current AI algorithms but rather, poor understanding and erroneous use of chemical data that ultimately leads to poor predictive performance. Using a new benchmark dataset of BRAF ligands that we developed, we show that our best performing predictive model can achieve an unprecedented accuracy of 99% with a conventional ML algorithm (SVM) using a merged molecular representation (Extended+ ECFP6 fingerprints), far surpassing past performances of virtual screening platforms using sophisticated deep learning methods. Thus, we demonstrate that it is not necessary to resort to the use of sophisticated deep learning algorithms for virtual screening because conventional ML can perform exceptionally well if given the right data and representation. We also show that the common use of decoys for training leads to high false positive rates and its use for testing will result in an over-optimistic estimation of a model’s predictive performance. Another common practice in virtual screening is defining compounds that are above a certain pharmacological threshold as inactives. Here, we show that the use of these so-called inactive compounds lowers a model’s sensitivity/recall. Considering that some target proteins have a limited number of known ligands, we wanted to also observe how the size and composition of the training data impact predictive performance. We found that an imbalance training dataset where inactives outnumber actives led to a decrease in recall but an increase in precision, regardless of the model or molecular representation used; and overall, we observed a decrease in the model’s accuracy. We highlight in this study some of the considerations that one needs to take into account in future development of data-centric AI for CADD.</p>
</abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>A. Chong, W.W.B. Goh , and H.Y. Li receive funding support from Nanyang Biologics. The authors have no other relevant affiliations or financial involvement with any organisation or entity with a financial interest in or financial conflict with the subject matter or materials discussed in the manuscript apart from those disclosed.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Manuscript revised to acknowledge funding support from the National Research Foundation, Singapore</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The discovery and development of a new drug and subsequently, bringing it onto the market is challenging and a major burden on time and finances. In a study of 63 drugs developed by 47 companies (between 2009 and 2018), it was estimated that the average cost of bringing a new drug to market is USD 1.33 billion<sup><xref ref-type="bibr" rid="c4">4</xref></sup>. To reduce cost and time, attention has shifted to using computer-aided drug design (CADD) methods, which has been relatively successful<sup><xref ref-type="bibr" rid="c5">5</xref></sup>. CADD allows researchers to better focus on drug experiments that have a higher likelihood of success.</p>
<p>In recent years, interest in AI/ML, particularly deep learning methods for CADD has grown. Deep neural networks have been shown to outperform machine learning (ML) algorithms like random forest (RF) and support vector machine (SVM) for quantitative structure-activity relationship (QSAR) and ligand-based virtual screening (LBVS). An example being Dahl et al.’s multi-task deep neural network (MT-DNN) which was the best performing architecture in the Merck Molecular Activity Kaggle Challenge<sup><xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c3">3</xref></sup>. A molecule is normally tested in multiple assays and their MT-DNN was trained so that the multiple output neurons each predicts the activity of the input molecule in a different assay. Following the Merck Challenge, researchers performed a detailed study that specifically compared the performance of deep neural nets (DNN) models to RF models and showed that DNN models routinely made better predictions on a series of large, diverse QSAR datasets generated as part of Merck’s drug discovery efforts<sup><xref ref-type="bibr" rid="c2">2</xref></sup>. However, contrary to findings of the above studies, when using DNN, RF and variable nearest neighbour (one of the simplest ML methods) to predict the molecular activities of 21 <italic>in vivo</italic> and <italic>in vitro</italic> datasets, Liu et al. reported that the overall performance of the three methods were similar<sup><xref ref-type="bibr" rid="c6">6</xref></sup>. This work leads us to question whether this trend to develop more advanced and complex AI methods would really result in any further significant improvements for QSAR/virtual screening.</p>
<p>In 2021, Dr Andrew Ng proposed that AI scientists should attempt to develop data-centric AI approaches instead of the currently pursued model-centric AI approaches<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref></sup>.</p>
<p>A model-centric AI approach involves improving/building on the AI algorithm to improve the model’s predictive performance <sup><xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c10">10</xref></sup>. In contrast, a data-centric AI approach is one where automated methods are developed to improve data quality or to tune the data used by the predictive model – for example, such methods could check for consistency in the labelling of the data and automatically flag mislabelled data – and in so doing, improve the model’s predictive performance.</p>
<p>With some chemical databases like PubChem containing well over 100 million compounds<sup><xref ref-type="bibr" rid="c11">11</xref></sup> and with the urgent need for large, clean datasets for deep learning methods, cheminformaticians are challenged to manually curate clean and consistent benchmark chemical datasets from large chemical libraries. Thus, it makes sense to develop data-centric AI approaches that can automatically do this for CADD. However, before we can develop such an approach, we need to understand what attributes/properties constitute good data for CADD. We believe there are four pillars of cheminformatics data that drives AI performance – namely, data representation, data quality, data quantity and data composition – and we were keen to investigate how each of these pillars contribute to an improved AI performance.</p>
<p>AI research generally begins with the extraction and acquisition of high quality data. But as data sets become larger and more sophisticated, manual data cleaning, restructuring and feature engineering becomes more challenging<sup><xref ref-type="bibr" rid="c12">12</xref>–<xref ref-type="bibr" rid="c14">14</xref></sup>. In a recent study, Northcutt et al.<sup><xref ref-type="bibr" rid="c15">15</xref></sup> found label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio benchmark datasets. On average, they found 3.3% errors across the 10 datasets - for example, the ImageNet validation set contained 6% (over 2,900 errors) label errors. Poor data quality can lead to biased AI models while insufficient data results in non-representative models that are unable to generalize and accurately predict from real-world data. However, in cheminformatics, it would appear that the latter is not an issue as there has been an exponential growth in chemical data: in 2010, PubChem contained 27,443,646 records of unique chemical structure compounds and by 2021, this had grown to around 111 million unique chemical structures (∼four-fold) <sup><xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup>.</p>
<p>Given Northcutt et al.’s findings, we felt that it was important for us to build a new benchmark chemical dataset that we could be confident of and which could be used with an AI to achieve superior performance. Only when we have a superior AI model that gives exceptional performance can we be certain that any change in the AI’s performance is indeed due to perturbations in our data and not because of an imperfect AI model. Thus, we carefully curated a new dataset of BRAF actives and inactives that we used for developing LBVS AI models for BRAF ligands. BRAF ligands are a well-studied class of drugs and there is much interest to develop potent BRAF antagonists that will suppress the actions of the constitutively-active mutant BRAF protein found in cancers like colorectal cancer and lung adenocarcinoma <sup><xref ref-type="bibr" rid="c18">18</xref>–<xref ref-type="bibr" rid="c21">21</xref></sup>. Our results showed that that models trained on our BRAF dataset could attain near-perfect accuracy and so, this allowed us to design a set of experiments to test how each of the four pillars of cheminformatics data impacts on AI performance.</p>
<p>Enhancing data is not just about increasing the size (and representativeness) of the dataset but also about choosing the right data representation for a learning task. We tested the performance of different ML algorithms employing various molecular representations to evaluate how molecular representations affect ML algorithm performances. There have also been studies that have proposed the use of merged molecular representations<sup><xref ref-type="bibr" rid="c22">22</xref>–<xref ref-type="bibr" rid="c24">24</xref></sup>. This multi-representation of molecular information is powerful, and constitutes a form of multi-view learning<sup><xref ref-type="bibr" rid="c25">25</xref></sup>, an emerging direction in AI/ML with implications for improved generalization performance. We systematically tested if paired fingerprint combinations can do a better job of describing a molecule and outperform a single (standalone) fingerprint in LBVS. For this, we tested 10 standalone fingerprints and their 45 paired combinations to shed light on (1) what is the best type of molecular representation for virtual screening (i.e., substructure key-based, topological, path-based or circular fingerprints, and single or merged fingerprints) and (2) how the interplay between molecular representations and different ML algorithms contributes to the changes (if any) in predictive performance. In all, we developed and assessed 1,375 predictive models for LBVS of BRAF ligands.</p>
<p>Next, we investigated the impact that data quality, data quantity and data composition have on predictive performance. Using four top predictive models that utilizes only a single molecular fingerprint, namely, [i] SVM+ECFP6, [ii] RF+ECFP6, [iii] SVM+Daylight-like, and [iv] RF+Extended, we show how current conventional usage of data in CADD can be improved to enhance data quality. For example, a common practice in CADD is the use of DUD-E decoys as inactives but a recent study showed that a hidden bias in these decoys affects predictions for structure-based virtual screening (SBVS) <sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup>. Thus, we wanted to see if this hidden bias also affects LBVS. In addition, we also explored how dataset size and composition impacts the performance of the AI models.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Assessment of machine learning algorithms for ligand-based screening</title>
<p>Five balanced training datasets of BRAF actives and inactives were used to train five conventional machine learners: k-nearest neighbours (kNN), Naïve Bayes (NBayes), gradient-boosted decision tree (GBDT), RF and SVM. We created in total 1,375 predictive models - 5 predictive models for each ML algorithm using one of 55 different molecular representations. For all predictive models, we observe that no real difference between the average accuracy achieved in cross-validation and that achieved on the hold-out test set, for all 55 molecular representations used (Figure 1 and Supplemental Table 1); for example, for the SVM model trained with Estate fingerprint (the worst performing SVM model), cross-validation attained an accuracy of 85.2% while testing gave an accuracy of 85.7% and in some cases, the average accuracy achieved in cross-validation and testing were the same (SVM using either CATS2D [accuracy=97.1%] or Extended [accuracy=98.1%]). This suggest no overfitting, regardless of the ML algorithm or molecular representation used, and that our training and testing sets are representative of the BRAF class of drugs.</p>
<p>The best performance was achieved by an SVM model using the paired (ECFP6+Extended) fingerprints with an average percentage accuracy of 99.05% on the hold-out test set, followed very closely by the RF model using only the ECFP6 fingerprint that gave a 98.48% accuracy. SVM and RF had previously been shown to perform well in virtual screenings for BACE1 inhibitors when compared against other conventional machine learners (RF achieving AUC-ROC of 0.867)<sup><xref ref-type="bibr" rid="c26">26</xref>–<xref ref-type="bibr" rid="c28">28</xref></sup> but to the best of our knowledge, the level of accuracy achieved here for our BRAF ligand dataset is unprecedented for a LBVS platform.</p>
<p>In general, all five ML algorithms tested were able to achieve similar levels of predictive performance, with the best performing models for each ML algorithm all achieving an accuracy of above 97%. We should also highlight that there was no feature selection performed on our training data prior to its use. In fact, the RF models were able to give a superior performance without even needing to tune the default parameters.</p>
<p>Among all predictive models, the worst performing model was the Naïve Bayes model using the Estate fingerprint (accuracy = 59.4%). But this shouldn’t come as a surprise. The conditional independence assumption of Naïve Bayes classifiers means that it can use high-dimensional features with limited training data compared to more sophisticated methods. And here we can see that this is indeed the case with the feature-rich molecular representations, ECFP6 and FCFP6 (each containing 2<sup><xref ref-type="bibr" rid="c32">32</xref></sup> bits), being able to achieve an accuracy of above 98% with Naïve Bayes (comparable to levels attained by the other ML algorithms). Conversely, we should avoid using Naïve Bayes if the feature space for the molecular representation is small or sparse, like Estate (79 bits).</p>
</sec>
<sec id="s2b">
<title>Assessment of molecular representations for ligand-based screening</title>
<p>For all ML algorithms that we tested, the worst performing predictive models were derived from those that had used only the Estate fingerprint to represent the molecules. This is unsurprising considering that the Estate fingerprint is a 79-bit vector which serves as an atom-centred index that describes the arrangement/connectivity and electronic environment of atoms in a molecule<sup><xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c30">30</xref></sup>. However, what is surprising is that a relatively simple 79-bit molecular representation can achieve a predictive performance of above 85% accuracy on all but one ML algorithm – this result is almost on par with some Tox21 challenge winners, for example, dmlab which achieved an AUC-ROC of 0.828 for their screening of androgen receptor agonist and microsomes which achieved an AUC-ROC of 0.827 for their prediction of small molecule agonists of the estrogen receptor alpha (ER-alpha) signaling pathway<sup><xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c32">32</xref></sup>.</p>
<p>For predictive models using only a standalone fingerprint, ECFP6 consistently ranked among the top 3 fingerprints for all ML algorithms (Table 1). All models generated using ECFP6 as a standalone fingerprint managed to achieve an accuracy of above 96% with ECFP6 performing best with RF (98.48%) and SVM (98.93%). Furthermore, in all but one ML algorithm, the best performing models for these machine learners employed ECFP6 either in a paired fingerprint combination or as a standalone fingerprint. Even in the case of GBDT where the best model used a paired combination of AtomPair and Daylight-like fingerprints (97.36% accuracy), it should be noted that a model using a combination of ECFP6 with the AtomPair fingerprint followed closely with a 97.16% accuracy. Overall, the best predictive model was obtained from a paired combination of ECFP6 and Extended fingerprints in a SVM model (99.05%). Considering that ECFP6 occupies a feature space of 2<sup><xref ref-type="bibr" rid="c32">32</xref></sup> bits, this fingerprint obviously provides a very detailed representation of a molecule and so, would undoubtedly be able to describe a molecule accurately and therefore, be useful in a classification task. These results suggest ECFP6 can be an effective molecular representation for LBVS regardless of whether it is used on its own or in combination with another fingerprint and would perform well irrespective of the ML algorithm in a data-driven approach.</p>
<p>So, is there any advantage in using a paired fingerprint combination to train our machines? As we can see with SVM, the pairing of Extended fingerprint with ECFP6 gave a slight improvement in accuracy and in fact, produced the best predictive model (<xref rid="fig1a" ref-type="fig">Figure 1a</xref> and Supplemental Table 1). But more is not always better; there are instances where combining two molecular fingerprints offers no synergistic benefit and in fact, one might even observe a poorer performance from a paired fingerprint compared to the performance obtained by one of the standalone fingerprints that make up the pair. For example, the topological torsion fingerprint can achieve an average of 98.34% accuracy on its own but when combined with PubChem fingerprint, this drops to 97.86%. However, in some cases, this drop in accuracy may, in fact, offer an advantage; using ECFP6 on its own in the RF models, we obtain a 98.48% accuracy but pairing it with the Daylight-like fingerprint, this drops very slightly to 98.44%, but by sacrificing some accuracy, we attain near perfect precision for the model (increasing from 0.9892 to 0.9976) (Supplemental Table 1). For wet lab validation, compromising on accuracy to attain a high level of precision is desirable because it means that the chances of wasting time and resource validating a compound that is likely to be false positive is low.</p>
<fig id="fig1a" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1a</label>
<caption><p>Accuracy of models generated with various single and paired molecular representations using support vector machine (SVM) during cross-validation (purple heatmap) and testing (blue heatmap)</p></caption>
<graphic xlink:href="587184v2_fig1a.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2c">
<title>The role that dataset composition and size plays in performance</title>
<p>Current leading approaches in AI are data-hungry, requiring access to big data to accomplish its training tasks<sup><xref ref-type="bibr" rid="c33">33</xref></sup>. This may not work well in cheminformatics. For LBVS, we rely solely on prior knowledge of known ligands to train our models to predict potential hits, however, some targets have only a limited number of ligands. To illustrate this point, ChEMBL contained information for 6778 human protein targets at the time of writing (April, 2023) but more than half have 500 or less associated ligands<sup><xref ref-type="bibr" rid="c34">34</xref></sup>. Such targets may not benefit from model-centric approaches as the requisite large datasets are unavailable.</p>
<p>Although known ligands for a protein target is limited, the wealth of information in chemical databases like ChEMBL offer an almost unlimited opportunity to mine inactives for use in AI training for quantitative structure-activity relationship (QSAR) and LBVS. We wanted to see how the relative numbers of actives to inactives would impact performance and particularly, if an overabundance of inactives in the training dataset could compensate for a lack of actives on our four chosen predictive models: (i) SVM+ECFP6, (ii) RF+ECFP6, (iii) SVM+Daylight-like, and (iv) RF+Extended. We hypothesized that the use of a large dataset of inactives could improve performance when presented with a finite number of actives.</p>
<p>Model performance was tested in two different scenarios where: (1) the number of inactives increased equally with the number of actives in the training dataset and (2) the ratio of the number of inactives to actives increases for the training dataset. For this latter scenario, the number of inactives were fixed at 3600 while the number of actives were successively decreased from 3600 to 500 actives in the training dataset.</p>
<p>For the SVM models, when the number of actives is fixed, increasing the number of inactives did not have a significant impact on accuracy. With training datasets containing only 500 actives, increasing the number of inactives from 500 to 3600 resulted in a drop in the accuracy drop from 96.42% (<xref rid="tbl2b" ref-type="table">Table 2b</xref>) to 95.73% (<xref rid="tbl2a" ref-type="table">Table 2a</xref>) for the SVM+Daylight-like model - a difference of 0.69% in accuracy.</p>
<table-wrap id="tbl1a" orientation="portrait" position="float">
<label>Table 1a:</label><caption><title>The top performing standalone fingerprints for each of the 5 ML algorithms</title></caption>
<graphic xlink:href="587184v2_tbl1a.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl1b" orientation="portrait" position="float">
<label>Table 1b:</label><caption><title>The best and worst performing models using a merged fingerprint for all 5 ML algorithms</title></caption>
<graphic xlink:href="587184v2_tbl1b.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl2a" orientation="portrait" position="float">
<label>Table 2a</label><caption><title>Accuracy (%) of models trained with an imbalanced training dataset where the number of BRAF actives is decreased but the number of BRAF inactives is maintained at a fixed number (3600)</title></caption>
<graphic xlink:href="587184v2_tbl2a.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl2b" orientation="portrait" position="float">
<label>Table 2b</label><caption><title>Accuracy (%) of models trained with a balanced training dataset where the numbers of BRAF actives and BRAF inactives are both similarly decreased</title></caption>
<graphic xlink:href="587184v2_tbl2b.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>However, for the RF models, the difference in accuracy was conspicuous: the accuracy for the RF+ECFP6 model fell from 98.4% for the full training dataset (3600 actives, 3600 inactives) to 94.71% with the imbalanced (500 actives, 3600 inactives) training dataset. In fact, the performance of the model trained on a balanced (500 actives, 500 inactives) dataset still managed an accuracy of 97.2%. In other words, where the number of actives was fixed at 500, increasing the number of inactives from 500 to 3600 in the RF+ECFP6 model saw a drop in accuracy from 97.2% to 94.71% - a difference of 2.49%.</p>
<p>In short, our study shows that, given a finite number of actives, having a large number of inactives in the training dataset does not improve model accuracy. Furthermore, certain algorithms (like RF) are more sensitive to imbalanced training data than others (SVM) (<xref rid="tbl2a" ref-type="table">Tables 2a</xref> and <xref rid="tbl2b" ref-type="table">2b</xref>) due to stronger affectations on the recall/sensitivity of RF models than SVM models (<xref rid="tbl2c" ref-type="table">Table 2c</xref>).</p>
<table-wrap id="tbl2c" orientation="portrait" position="float">
<label>Table 2c</label><caption><title>Recall and precision (%) of models trained with an imbalanced training dataset where the number of BRAF actives is decreased but the number of BRAF inactives is maintained at a fixed number (3600)</title></caption>
<graphic xlink:href="587184v2_tbl2c.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>Regardless of the ML model and/or molecular representation used, recall/sensitivity decreases as the size of inactives in the training dataset increases (relative to the size of actives) while precision increases (<xref rid="tbl2c" ref-type="table">Tables 2c</xref> and <xref rid="tbl2d" ref-type="table">2d</xref>). In a previous study where the number of actives were kept constant while increasing the number of inactives, Rodríguez-Pérez et al. had also found that precision improves as the ratio of inactives to actives increases <sup><xref ref-type="bibr" rid="c35">35</xref></sup>. They also found that they needed at least 100 actives and 500 inactives to achieve near maximal recall (they achieved a mean recall of 87%) for their SVM model <sup><xref ref-type="bibr" rid="c35">35</xref></sup> but we found that the same cannot be said to be true for RF models – recall for RF models suffered greatly when the number of actives fall below 1000: for the RF+Extended model, recall fell from 93.41% when trained on (1000 actives, 3600 inactives) to 87.82% for the model trained on (500 actives, 3600 inactives) (<xref rid="tbl2c" ref-type="table">Table 2c</xref>).</p>
<p>Although recall fell considerably, all SVM and RF models were able to achieve near-perfect (&gt;99%) or perfect precision when we trained with only 500 actives but provided over seven times the number of inactives (3600 inactives) (<xref rid="tbl2c" ref-type="table">Table 2c</xref>). It is also interesting to note that with a balanced training dataset, there was only a slight change in recall and precision for SVM and RF models even when both the number of actives and inactives in the training dataset drops from 3600 to 500, compared to models trained on the imbalanced datasets – in general, all models trained on a balanced dataset were able to attain &gt;96% for both recall and precision (<xref rid="tbl2c" ref-type="table">Tables 2c</xref> and <xref rid="tbl2d" ref-type="table">2d</xref>).</p>
<table-wrap id="tbl2d" orientation="portrait" position="float">
<label>Table 2d</label><caption><title>Recall and precision (%) of models trained with a balanced training dataset where the numbers of BRAF actives and BRAF inactives are both similarly decreased</title></caption>
<graphic xlink:href="587184v2_tbl2d.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>Despite the drop in recall and accuracy, we recommend that when faced with a limited number of actives for a target, one should increase the number of inactives used in training to lessen false positive predictions. It should be noted that no class weights were introduced when we trained our AI models on the imbalance datasets. Finally, the SVM+ECFP6 model was the most robust among the four models tested as its accuracy, recall and precision were little affected by the change in data composition or size.</p>
</sec>
<sec id="s2d">
<title>Choice of inactives is crucial for training</title>
<sec id="s2d1">
<label>(i)</label>
<title>Less active versus inactive</title>
<p>In many QSAR and LBVS studies, we often see the use of an arbitrary IC50 threshold to divide ligands into 2 classes, actives and inactives – for example, compounds that have an IC50 below 10 μM for a target protein are considered “actives” and those that have an IC50 of 10 μM or more are considered “inactives”<sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c36">36</xref>–<xref ref-type="bibr" rid="c41">41</xref></sup>. We hypothesized that the use of such “inactive” ligands in training could reduce the model’s predictive performance since it is likely that these “inactives” are not really inactive but perhaps, just “less active”. We also recognise that the measurement of IC50 is dependent on the assay and condition under which it is tested. An IC50 curve shifts to the right if the expression/concentration of the protein is high in the cell/assay used to test the drug, thereby making what may be an “active” compound appear to be “inactive”. Here, we set out to see whether the use of these “less actives” affects a model’s predictive performance.</p>
<p>Before proceeding with our comparative studies with the “less actives”, we set out to first establish the baseline accuracy for our 4 predictive models. We created 10 new balanced, hold-out test sets to allow for an unbiased assessment of the 4 predictive models. The accuracy of each model is fairly consistent across all ten test sets, averaging at around 97% for the 4 predictive models (<xref rid="tbl3" ref-type="table">Table 3</xref> and Supplemental Table 2). This suggests that the original test set used was not biased and that the assessment of the ML models in above study (<xref rid="fig1a" ref-type="fig">Figures 1a</xref>-<xref rid="fig1b" ref-type="fig">d</xref>) was valid. These results provide further evidence that the curated BRAF dataset used for training is of high quality and can be used as a reliable benchmark for future virtual screening methods. Finally, these results also form the baseline for the comparative studies below.</p>
<fig id="fig1b" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1b</label>
<caption><p>Accuracy of models generated with various single and paired molecular representations using random forest (RF) during cross-validation (purple heatmap) and testing (blue heatmap)</p></caption>
<graphic xlink:href="587184v2_fig1b.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig1c" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1c</label>
<caption><p>Accuracy of models generated with various single and paired molecular representations using naïve bayes (NBayes) during cross-validation (purple heatmap) and testing (blue heatmap)</p></caption>
<graphic xlink:href="587184v2_fig1c.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig1d" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1d</label>
<caption><p>Accuracy of models generated with various single and paired molecular representations using k-nearest neighbour (kNN) during cross-validation (purple heatmap) and testing (blue heatmap)</p></caption>
<graphic xlink:href="587184v2_fig1d.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig1e" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1e</label>
<caption><p>Accuracy of models generated with various single and paired molecular representations using gradient-boosting decision tree (GBDT) during cross-validation (purple heatmap) and testing (blue heatmap)</p></caption>
<graphic xlink:href="587184v2_fig1e.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3</label>
<caption><title>Average accuracy for the ‘spiked-in’ “less active”-trained models based on testing with 10 balanced BRAF actives and inactives hold-out test sets</title></caption>
<graphic xlink:href="587184v2_tbl3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>To test how “less actives” compounds would affect the predictive performance of the models, we identified 510 BRAF ligands in ChEMBL whose IC50 were more than 10 μM. We used these in a “spike-in” experiment to see if the introduction of these “less actives” in the original training dataset for our ML models would change their performance. We created 3 training datasets in which 5% (180), 10% (360) and 14% (510) of the inactives in the original training dataset were replaced with these “less actives”. In every instance, our results show that the level of accuracy of the model falls as the number of “less actives” in the training dataset increases (<xref rid="tbl3" ref-type="table">Table 3</xref> and Supplemental Table 2). Not surprisingly, this drop in performance is due in large part to a drop in recall, meaning that the models trained on “less actives” are less able to correctly recognise certain actives (true positives) in the test set, probably because some of these actives share certain similarity to the “less actives” which were labelled as “inactive” in the training dataset. This demonstrates that the design and composition of the input data is important.</p>
</sec>
<sec id="s2d2">
<label>(ii)</label>
<title>Using DUD-E decoys as inactives</title>
<p>The DUD-E benchmark dataset is commonly used for the development of SBVS methods<sup><xref ref-type="bibr" rid="c42">42</xref></sup>. Briefly, in DUD-E, a diverse set of actives is built for a target protein. Each active molecule is paired with a set of property matched decoys (PMD). PMDs are selected to be similar to each other and to known actives with respect to certain physico-chemical descriptors (e.g., molecular weight, hydrogen bond donors and acceptors) while being topologically dissimilar. It is believed that topological dissimilarity supports the assumption that the decoys are likely to be inactives because they are chemically different from the actives. The DUD-E database has 9,950 decoys for BRAF. Although originally developed for molecular docking screens, DUD-E’s decoys are also used as inactives in LBVS<sup><xref ref-type="bibr" rid="c43">43</xref>–<xref ref-type="bibr" rid="c48">48</xref></sup>. A recent study showed that the superior enrichment efficiency in convolutional neural network models for SBVS was achieved in the DUD-E dataset because of hidden bias in this dataset rather than a successful generalization of the pattern of protein-ligand interactions<sup><xref ref-type="bibr" rid="c49">49</xref></sup>. Therefore, could this hidden bias also have an impact on LBVS?</p>
<p>As with our spike-in studies with “less actives”, we wanted to see how the introduction of some decoys in training might affect model performance. Therefore, we selected at random 510 BRAF decoys and carried out the same spike-in experiments as described for the “less actives” experiment above. Unlike our models trained on datasets with “less actives” spiked-in, our models trained on datasets with decoys spiked-in do not show much difference in accuracy from our baseline models trained without any decoys. For example, for the RF+ECFP6 model trained on a dataset where 14% of the inactives were replaced with 510 decoys, the average accuracy on the 10 test datasets dropped by a mere 0.09% when compared with the baseline model (<xref rid="tbl4a" ref-type="table">Table 4A</xref>); this is a stark contrast to the nearly 2% drop in accuracy when we used a dataset spiked-in with 510 “less actives” (<xref rid="tbl3" ref-type="table">Table 3</xref>). This is probably because decoys, unlike the “less actives”, are in fact topologically dissimilar from the BRAF actives and so, the use of some decoys to represent inactives does not severely compromise the performance of the model. However, the results for models trained on <underline>only</underline> actives and decoys was surprising.</p>
<table-wrap id="tbl4a" orientation="portrait" position="float">
<label>Table 4A</label><caption><title>Average accuracy for the ‘spiked-in’ decoy-trained models based on testing with 10 balanced BRAF actives and inactives hold-out test sets</title></caption>
<graphic xlink:href="587184v2_tbl4a.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>For ML models trained on 3600 actives plus 3600 decoys, their accuracy when tested on a balanced test dataset of 1000 BRAF actives and decoys is near-perfect (above 99% for all ML models) (<xref rid="tbl4b" ref-type="table">Table 4B</xref>; Supplemental Table 3). However, these models gave an average accuracy of around 94% when we tested it against our 10 balanced test datasets of 1000 BRAF actives and inactives (<xref rid="tbl4a" ref-type="table">Table 4A</xref>; Supplemental Table 3). The drop in accuracy for models trained on 3600 BRAF actives plus 3600 decoys on our 10 test datasets is due to a fall in precision rather than recall. While recall for these models are above 98%, their precision drops by about 5% when compared with their corresponding baseline model. This means that models trained on only actives and decoys tend to give more false positive predictions. Conversely, when we tested our original ML models trained on our training dataset of 3600 BRAF actives plus 3600 BRAF inactives, we found that they gave an accuracy ranging from 96.85 to 97.74% on our 10 test datasets of BRAF actives and inactives but performed even better on the BRAF actives plus decoys test set, achieving near-perfect accuracy of 99% (<xref rid="tbl4a" ref-type="table">Tables 4A</xref> &amp; <xref rid="tbl4b" ref-type="table">4B</xref>; Supplemental Table 3). This is expected when we consider the fact that decoys are topologically dissimilar to BRAF actives, therefore, making it easy for our original ML models to discern between the BRAF actives and decoys. In summary, training with decoys is not recommended because it would result in a model that has a higher false positive rate. Testing with a dataset containing decoys is also not advisable because it may give an over-optimistic assessment of the predictive performance of the model.</p>
<table-wrap id="tbl4b" orientation="portrait" position="float">
<label>Table 4B</label><caption><title>Accuracy for the ‘spiked-in’ decoy-trained models based on testing with a balanced BRAF actives and decoys hold-out test set</title></caption>
<graphic xlink:href="587184v2_tbl4b.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>“<italic>Garbage in, garbage</italic> out” or GIGO, is a concept familiar to all computer scientists denoting the importance and value of data quality in predictive modelling. But despite this wide acknowledgement, prevailing AI approaches tend to be model-centric, focusing more on the sophistication of the AI/ML architecture than the input. Such approaches may not pay enough heed towards resolving issues of data quality, data representativeness (not to be confused for data representations which is investigated in this study), and perhaps even feature-compatibility with the AI/ML models used. By adopting a model-centric AI approach, one hopes to tweak and improve the AI algorithms to improve its predictive performance while also enhancing model explainability or justifiability.</p>
<p>To pursue a data-centric AI approach, Dr Ng advocated developing a systematic approach to mine and clean large volumes of data in order to produce accurately-labelled data which is of high quality, consistent and error-free<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref></sup>. Data-centric AI appear to be gaining traction in the AI community<sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c50">50</xref>,<xref ref-type="bibr" rid="c51">51</xref></sup>. It is also important that we do not confuse data-centric AI for data-driven AI: data-centric AI is an emerging science that studies techniques to improve the quality of datasets (that will be used to train the AI/ML models), whereas, data-driven AI is simply a paradigm that encourages the analysis of large datasets to improve AI performance. Many AI researchers have written to give a general perspective on the principles and critical considerations for the data construction process in data-centric AI <sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c10">10</xref></sup> but to the best of our knowledge, there has been little or no attempts to determine and evaluate the parameters that are necessary for data construction in a data-centric AI approach for CADD. In fact, we can see that cheminformaticians still favour a model-centric AI approach when we look at studies published previously comparing the use of different ML or deep learning methods on different datasets <sup><xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c37">37</xref></sup>. This emphasis on a model-centric AI approach for cheminformatics is also shared by authors of recent reviews surveying the current research landscape of CADD <sup><xref ref-type="bibr" rid="c52">52</xref>–<xref ref-type="bibr" rid="c54">54</xref></sup>.</p>
<p>Whang et al. proposed that data acquisition, data labelling, data augmentation/improvement, data validation, data cleaning and data sanitization as key considerations in the practice of data-centric AI <sup><xref ref-type="bibr" rid="c10">10</xref></sup>. However, we need to recognise that there are other considerations which are unique to chemistry and cheminformatics. A chemical’s property and structure need to be encoded in a meaningful manner so that a predictive AI can learn the salient features of a class of drugs for QSAR and virtual screening and this has given rise to various types of data representations for a molecule (that is, molecular descriptors and fingerprints). Thus, data representation is an important consideration that is somewhat unique to data construction for QSAR and virtual screening.</p>
<p>We believed that good data representation might be equally as important as high-quality data and so, we wanted to see how best to represent a molecule to the machine. In recent years, many newly created molecular representations have been proposed <sup><xref ref-type="bibr" rid="c55">55</xref>–<xref ref-type="bibr" rid="c58">58</xref></sup>. However, instead of reinventing the wheel and creating another novel molecular representation, we tested to see if currently available molecular fingerprints are up to the task for virtual drug screening. Our study shows that certain molecular fingerprints are clearly superior and, in some cases, two molecular fingerprints can be combined to produce a performance which surpasses the performance of the individual molecular fingerprint, as seen when Extended and ECFP6 fingerprints were used with SVM. However, broadly speaking, there is very little difference in performance across different fingerprints. For example, looking at the RF models, most models trained using a single fingerprint could achieve a level of accuracy close to 97% or higher. The most surprising result came from the RF model using EState fingerprint which could achieve an accuracy of ∼92% - it is surprising because EState is only 79 bits but it was outperforming previously developed virtual drug screeners that used a more feature-rich molecular representation with deep learning methods <sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c59">59</xref></sup>. This suggests that current molecular fingerprints are more than up to the task for CADD if one can provide a high-quality dataset for training and testing.</p>
<p>Our study also demonstrates that one cannot pursue a data-centric AI approach to the exclusion of a model-centric approach because there are clearly certain AI models that perform better on certain AI task. Comparing between ML models, we see that although Estate fingerprint paired with RF achieves ∼92% accuracy, SVM with the same fingerprint achieves only 85% accuracy. This, however, does not suggest that SVM is not a good ML model for virtual screening because when given the “right” molecular representation (Extended and ECFP6 fingerprints), SVM can offer a predictive performance that surpasses all other predictive models that we tested. Thus, having the right model is just as important as having the right data and we cannot ignore the importance of a model-centric approach --- both model and data-centric approaches should be seen as complimentary than mutually exclusive. In future, in-depth studies to understand complementariness between models and data should be performed.</p>
<p>Interest in deep learning methods for QSAR and virtual screening have been on the rise following the success of Dahl et al.’s MT-DNN in the Merck Molecular Activity Challenge<sup><xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c3">3</xref></sup> and subsequently, Mayr et al.’s DeepTox deep neural networks in the Tox21 Challenge<sup><xref ref-type="bibr" rid="c59">59</xref></sup>. Studies have shown that deep learning methods consistently outperform conventional ML methods for rational drug discovery<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. For example, DeepTox (the overall winner of the Tox21 Challenge) which used a deep neural network for toxicity prediction could achieve AUC-ROCs of between 0.793-0.942 for 12 nuclear receptor (NR) and stress response (SR) pathway assays<sup><xref ref-type="bibr" rid="c59">59</xref></sup>. The best ligand-based screening model for DeepTox was achieved for ligands of the aryl hydrocarbon receptor (AhR) which gave an AUC-ROC of 0.928. However, we need to recognise certain limitations of deep learning methods. In comparison to conventional ML algorithms, deep neural networks require a much larger amount of data to ensure model generalizability and prevent overfitting. This presents a problem for LBVS because many druggable protein targets have very few known ligands. To illustrate this point, ChEMBL contains information for 6778 human protein targets (as of April, 2023) but more than half (∼3800) have 100 or less compounds/ligands associated with each protein<sup><xref ref-type="bibr" rid="c34">34</xref></sup>. For this reason, we chose to focus only on conventional ML algorithms and we show here that conventional ML models can give superior performance for virtual screening, without the need to resort to more data-hungry deep learning methods. This limitation of deep neural networks also makes it clear that alternative paradigms such as data-centricity, favouring quality over quantity, is more likely to be successful in novel drug discovery using AI/ML.</p>
<p>Until recently, it has been difficult to give a reliable assessment of what constitutes best practice for LBVS since there are no benchmark datasets that can produce ML models capable of giving near-perfect predictive performance. In the absence of a high-quality benchmark dataset, evaluating experiments such as the ones carried out here would be difficult because any observed change in performance could be attributed to the behaviour of an imperfect model rather than a reflection of the use of inappropriate data. We have now generated a high-quality benchmark dataset and were able to show that current practices and use of data lead to serious shortcomings in the state-of-the-art predictive models which, in turn, calls for a reassessment of these model’s true performance. Our work indicates that there is little or no shortcomings with either current predictive AI/ML algorithms or most molecular representations when used in LBVS because the differences in their performance are minimal. Ultimately, the biggest impact on predictive performance comes from data size, composition and poor data quality.</p>
<p>Through this work, we hope to use a single case study to demonstrate the power of data-centric paradigm. As this approach becomes more widespread and better understood across use cases, a foundation for data-centric AI approaches in cheminformatics could eventually be laid. With the exponential growth in chemical databases like ChEMBL and PubChem, it becomes increasingly difficult to manually curate a high-quality dataset when we are dealing with large numbers of compounds/ligands. It is therefore important for us to consider how data-centric AI approaches can help. We propose that pursuing data-centricity require first understanding the nature of our data across 4 pillars: data representation, data quality, data quantity, and data composition on a model’s predictive performance (i.e., what attributes or properties of these 4 pillars contribute to high-quality data for training and testing). If, for example, we do not know what type of data diminishes data quality, then how can we be expected to build a data-centric AI approach for cheminformatics? Our study demonstrates that manipulation of these pillars of data-centricity has affectation on model performance. Pending future generalization studies on more use cases, our work paves the way towards the establishment of a framework for a data-centric AI approach for virtual screening and CADD.</p>
</sec>
<sec id="s4">
<title>Method</title>
<sec id="s4a">
<title>Machine Learning Models</title>
<p>We tested the performance of five machine learning methods representing different strategies. These include, k-nearest neighbours (kNN), Naïve Bayes (NBayes), gradient-boosted decision tree (GBDT), random forest (RF) and support vector machine (SVM) (python ver. 9.1.2 / sklearn ver.1.0.2), using the BRAF dataset described below.</p>
<p>For each of the 55 molecular representations that we tested, we used 5 training datasets on the above machine learning methods. We did not perform any feature scaling or selection. Each training set underwent 10-fold cross-validation to optimize the hyperparameters for the SVM, kNN and NBayes models and to estimate the accuracy of the models. Grid search was used to determine the optimal values of the hyperparameters. For the RF and GBDT models, the default parameters were used with no additional tuning.</p>
<p>Accuracy, precision, and recall were used as performance metrics.</p>
</sec>
<sec id="s4b">
<title>Dataset</title>
<p>Instead of using a readily available benchmark dataset (such as the BACE1 inhibitors dataset by Subramanian et al.<sup><xref ref-type="bibr" rid="c28">28</xref></sup>), we manually curated and built a dataset of BRAF actives and inactives to ensure that we would have a high quality dataset for our study. Actives were defined as validated BRAF ligands with an IC50 of less than 10 µM, while inactives were carefully selected based on the fact that they have no known pharmacological activity against BRAF. In total, we identified 4100 BRAF actives and around 24,000 compounds that we deemed to be inactive against BRAF. From this initial set of compounds, we randomly selected 3600 BRAF actives to be part of the training dataset with the remaining 500 actives becoming a part of the hold-out test set. To avoid training bias from any single curated dataset, we created 5 balanced training datasets, each containing the 3600 BRAF actives with an equal number of inactives and where each training dataset contained a unique set of inactives that was not shared, partially or wholly, with the other 4 training datasets. Unless there were some hidden biases in one or more of the training datasets, we expected the performances of the 5 trained models for each molecular representation to be similar. We also created a single balanced test set with 1000 BRAF actives and inactives that would be used to give a fair and objective comparison of the predictive models. (Please refer to Supplementary data (Part 1) for these training and test datasets)</p>
<p>We chose four top predictive models employing a single molecular fingerprint that had been trained with the third training dataset for the experiments investigating data size, data composition and data quality. The models chosen were: [i] SVM+ECFP6, [ii] SVM+Daylight-like, [iii] RF+ECFP6, and [iv] RF+Extended. The performance of these four original ML models formed the baseline for our comparative analyses and represented the optimal performance achievable for the respective model. Additionally, we generated new predictive models where [i] SVM+ECFP6, [ii] SVM+Daylight-like, [iii] RF+ECFP6, and [iv] RF+Extended were trained on smaller datasets to compare their performance against the baseline performance. The original training dataset was a balanced dataset of 7200 BRAF actives and inactives but the smaller training datasets composed of the following numbers of BRAF actives to inactives: (i) 3000:3600; (ii) 2500:3600; (iii) 2000:3600, (iv) 1500: 3600; (v) 1000:3600; (vi) 500:3600; (vii) 3000:3000; (viii) 2500:2500; (xi) 2000:2000, (x) 1500:1500; (xi) 1000:1000; (xii) 500:500. The actives and inactives in these smaller datasets were randomly selected to avoid any unintentional bias.</p>
<p>For this study, we generated 10 new balanced hold-out test datasets of 1000 BRAF actives and inactives to give us an unbiased gauge of the accuracy that our models can achieve. We define a compound as “inactive” if there are no known pharmacological assays for the said compound on our target, BRAF.</p>
<p>We found 510 BRAF ligands that have an IC50 of more than 10 μM in the ChEMBL database which we labelled as “less active” <sup><xref ref-type="bibr" rid="c34">34</xref></sup>. These “less active” ligands were used for spike-in experiments where we replaced either 180 (5%), 360 (10%) or 510 (14%) inactives in our original training dataset with a similar number of “less active” ligands.</p>
<p>The DUD-E database contains 9,950 decoys for BRAF<sup><xref ref-type="bibr" rid="c42">42</xref></sup>. We randomly selected 3600 decoys to create a training dataset of 7200 BRAF actives and decoys. In addition, we also created a balanced hold-out test set of 1000 BRAF actives and decoys. As with the “less actives” spike-in experiments, we also created 3 separate training datasets that had either 180 (5%), 360 (10%) or 510 (14%) decoys replacing a similar number of inactives in our original training dataset of 3600 actives and 3600 inactives.</p>
<p>Please refer to Supplementary data (Part 2) for training and test datasets used in these studies.</p>
</sec>
<sec id="s4c">
<title>Fingerprint Generation</title>
<p>RDKit and the Chemistry Development Kit (CDK) were used to compute the fingerprints for the molecules in this study<sup><xref ref-type="bibr" rid="c60">60</xref>,<xref ref-type="bibr" rid="c61">61</xref></sup>. The ten standalone molecular fingerprints, namely,
<list list-type="order">
<list-item><p>Estate fingerprints (CDK)<sup><xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c30">30</xref></sup></p></list-item>
<list-item><p>PubChem fingerprints (CDK)<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c62">62</xref></sup></p></list-item>
<list-item><p>Klekota-Roth fingerprints (CDK)<sup><xref ref-type="bibr" rid="c63">63</xref></sup></p></list-item>
<list-item><p>Extended-Connectivity fingerprints 6 (ECFP6) (CDK)<sup><xref ref-type="bibr" rid="c64">64</xref>,<xref ref-type="bibr" rid="c65">65</xref></sup></p></list-item>
<list-item><p>Functional-Class fingerprints 6 (FCFP6) (CDK)<sup><xref ref-type="bibr" rid="c64">64</xref>,<xref ref-type="bibr" rid="c65">65</xref></sup></p></list-item>
<list-item><p>Extended fingerprints (CDK)<sup><xref ref-type="bibr" rid="c66">66</xref></sup></p></list-item>
<list-item><p>Topological Torsion fingerprints (topo_torsion) (RDKit)<sup><xref ref-type="bibr" rid="c67">67</xref></sup></p></list-item>
<list-item><p>Atom Pairs fingerprints (RDKit)<sup><xref ref-type="bibr" rid="c68">68</xref></sup></p></list-item>
<list-item><p>Daylight-like RDKit fingerprints (RDKit)<sup><xref ref-type="bibr" rid="c60">60</xref></sup></p></list-item>
<list-item><p>CATS2D fingerprints (RDKit)<sup><xref ref-type="bibr" rid="c69">69</xref></sup></p></list-item>
</list>
and 45 paired combinations of these were used in this study.</p>
</sec>
</sec>
<sec id="d1e1069" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1165">
<label>Supplemental Table 1</label>
<media xlink:href="supplements/587184_file02.xlsx"/>
</supplementary-material>
<supplementary-material id="d1e1172">
<label>Supplemental Table 2</label>
<media xlink:href="supplements/587184_file03.xlsx"/>
</supplementary-material>
<supplementary-material id="d1e1179">
<label>Supplemental Table 3</label>
<media xlink:href="supplements/587184_file04.xlsx"/>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<title>Acknowledgement</title>
<p>The authors wish to thank Dr. Cameron Osborne for taking time to read our manuscript and offer valuable comments and suggestions to improve its readability.</p>
<p>This research/project is supported by the National Research Foundation, Singapore under its Industry Alignment Fund – Pre-positioning (IAF-PP) Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.</p>
<p>WWBG acknowledges support from a Ministry of Education (MOE), Singapore Tier 1 and SUG grant (Grant No. RS08/21).</p>
</ack>
<sec id="s5">
<title>Financial &amp; competing interests disclosure</title>
<p>A. Chong, W.W.B. Goh, and H.Y. Li receive funding support from Nanyang Biologics. The authors have no other relevant affiliations or financial involvement with any organisation or entity with a financial interest in or financial conflict with the subject matter or materials discussed in the manuscript apart from those disclosed.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Huang</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Xia</surname>, <given-names>M.</given-names></string-name> <article-title>Editorial: Tox21 Challenge to Build Predictive Models of Nuclear Receptor and Stress Response Pathways As Mediated by Exposure to Environmental Toxicants and Drugs</article-title>. <source>Front. Environ. Sci.</source> <volume>5</volume>, (<year>2017</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Ma</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sheridan</surname>, <given-names>R. P.</given-names></string-name>, <string-name><surname>Liaw</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dahl</surname>, <given-names>G. E.</given-names></string-name> &amp; <string-name><surname>Svetnik</surname>, <given-names>V</given-names></string-name>. <article-title>Deep Neural Nets as a Method for Quantitative Structure–Activity Relationships</article-title>. <source>J. Chem. Inf. Model</source>. <volume>55</volume>, <fpage>263</fpage>–<lpage>274</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Dahl</surname>, <given-names>G. E.</given-names></string-name>, <string-name><surname>Jaitly</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Salakhutdinov</surname>, <given-names>R.</given-names></string-name> <article-title>Multi-task Neural Networks for QSAR Predictions</article-title>. <source>arXiv</source> (<year>2014</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Wouters</surname>, <given-names>O. J.</given-names></string-name>, <string-name><surname>McKee</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Luyten</surname>, <given-names>J.</given-names></string-name> <article-title>Estimated Research and Development Investment Needed to Bring a New Medicine to Market, 2009-2018</article-title>. <source>JAMA</source> <volume>323</volume>, <fpage>844</fpage>–<lpage>853</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Sabe</surname>, <given-names>V. T.</given-names></string-name> <etal>et al.</etal> <article-title>Current trends in computer aided drug design and a highlight of drugs discovered via computational techniques: A review</article-title>. <source>Eur. J. Med. Chem</source>. <volume>224</volume>, <fpage>113705</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Liu</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Glover</surname>, <given-names>K. P.</given-names></string-name>, <string-name><surname>Feasel</surname>, <given-names>M. G.</given-names></string-name> &amp; <string-name><surname>Wallqvist</surname>, <given-names>A</given-names></string-name>. <article-title>Dissecting Machine-Learning Prediction of Molecular Activity: Is an Applicability Domain Needed for Quantitative Structure–Activity Relationship Models Based on Deep Neural Networks?</article-title> <source>J. Chem. Inf. Model</source>. <volume>59</volume>, <fpage>117</fpage>–<lpage>126</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="web"><string-name><surname>Ng</surname>, <given-names>A.</given-names></string-name> <article-title>MLOps: From Model-centric to Data-centric AI</article-title>. <ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=06-AZXmwHjo">https://www.youtube.com/watch?v=06-AZXmwHjo</ext-link> (<year>2021</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="web"><string-name><surname>Strickland</surname>, <given-names>E.</given-names></string-name> <article-title>Andrew Ng: Unbiggen AI - IEEE Spectrum</article-title>. <ext-link ext-link-type="uri" xlink:href="https://spectrum.ieee.org/andrew-ng-data-centric-ai">https://spectrum.ieee.org/andrew-ng-data-centric-ai</ext-link> (<year>2022</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Zha</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bhat</surname>, <given-names>Z. P.</given-names></string-name>, <string-name><surname>Lai</surname>, <given-names>K.-H.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Hu</surname>, <given-names>X.</given-names></string-name> <article-title>Data-centric AI: Perspectives and Challenges</article-title>. in <source>Proceedings of the 2023 SIAM International Conference on Data Mining (SDM)</source> <fpage>945</fpage>–<lpage>948</lpage> (Society for Industrial and Applied Mathematics, <year>2023</year>). doi:<pub-id pub-id-type="doi">10.1137/1.9781611977653.ch106</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Whang</surname>, <given-names>S. E.</given-names></string-name>, <string-name><surname>Roh</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Song</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Lee</surname>, <given-names>J.-G</given-names></string-name>. <article-title>Data collection and quality challenges in deep learning: a data-centric AI perspective</article-title>. <source>VLDB J</source>. <volume>32</volume>, <fpage>791</fpage>–<lpage>813</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Kim</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>PubChem 2023 update</article-title>. <source>Nucleic Acids Res</source>. <volume>51</volume>, <fpage>D1373</fpage>–<lpage>D1380</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Aldoseri</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Al-Khalifa</surname>, <given-names>K. N.</given-names></string-name> &amp; <string-name><surname>Hamouda</surname>, <given-names>A. M</given-names></string-name>. <article-title>Re-Thinking Data Strategy and Integration for Artificial Intelligence: Concepts, Opportunities, and Challenges</article-title> <source>Appl. Sci</source>. <volume>13</volume>, <fpage>7082</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Yoon</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Donaldson</surname>, <given-names>D. R</given-names></string-name>. <article-title>Big data curation framework: Curation actions and challenges</article-title>. <source>J. Inf. Sci</source>. (<year>2022</year>) doi:<pub-id pub-id-type="doi">10.1177/01655515221133528</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="book"><string-name><surname>Kalpathy-Cramer</surname>, <given-names>K. C.</given-names></string-name>, <string-name><given-names>Mishka</given-names> <surname>Gidwani</surname></string-name>,. <string-name><given-names>Jay B.</given-names> <surname>Patel</surname></string-name>,. <string-name><given-names>Matthew D.</given-names> <surname>Li</surname></string-name>,. <string-name><surname>Jayashree</surname></string-name>. <chapter-title>Data Curation Challenges for Artificial Intelligence</chapter-title>. in <source>Auto-Segmentation for Radiation Oncology</source> <fpage>201</fpage>–<lpage>216</lpage> (<publisher-name>CRC Press</publisher-name>, <year>2021</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Northcutt</surname>, <given-names>C. G.</given-names></string-name>, <string-name><surname>Athalye</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Mueller</surname>, <given-names>J.</given-names></string-name> <article-title>Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks</article-title>. <source>arXiv</source> <pub-id pub-id-type="doi">10.48550/arXiv.2103.14749</pub-id> (<year>2021</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Xie</surname>, <given-names>X.-Q</given-names></string-name>. <article-title>Exploiting PubChem for Virtual Screening</article-title>. <source>Expert Opin. Drug Discov</source>. <volume>5</volume>, <fpage>1205</fpage>–<lpage>1220</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Kim</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>PubChem in 2021: new data content and improved web interfaces</article-title>. <source>Nucleic Acids Res</source>. <volume>49</volume>, <fpage>D1388</fpage>–<lpage>D1395</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Barras</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal> <article-title>BRAF V600E Mutant Colorectal Cancer Subtypes Based on Gene Expression</article-title>. <source>Clin. Cancer Res</source>. <volume>23</volume>, <fpage>104</fpage>–<lpage>115</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Paik</surname>, <given-names>P. K.</given-names></string-name> <etal>et al.</etal> <article-title>Clinical Characteristics of Patients With Lung Adenocarcinomas Harboring BRAF Mutations</article-title>. <source>J. Clin. Oncol</source>. <volume>29</volume>, <fpage>2046</fpage>–<lpage>2051</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Dain Md Opo</surname>, <given-names>F. A.</given-names></string-name>, <etal>et al.</etal> <article-title>Identification of novel natural drug candidates against BRAF mutated carcinoma; An integrative in-silico structure-based pharmacophore modeling and virtual screening process</article-title>. <source>Front. Chem</source>. <volume>10</volume>, (<year>2022</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Żołek</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Mazurek</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Grudzinski</surname>, <given-names>I. P.</given-names></string-name> <article-title>In Silico Studies of Novel Vemurafenib Derivatives as BRAF Kinase Inhibitors</article-title>. <source>Molecules</source> <volume>28</volume>, <fpage>5273</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Kim</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ahn</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Lee</surname>, <given-names>J. R</given-names></string-name>. <article-title>A merged molecular representation learning for molecular properties prediction with a web-based service</article-title>. <source>Sci. Rep</source>. <volume>11</volume>, <issue>11028</issue> (<year>2021</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Tang</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Nie</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>Q.</given-names></string-name> &amp; <string-name><surname>Chen</surname>, <given-names>W</given-names></string-name>. <article-title>A merged molecular representation deep learning method for blood–brain barrier permeability prediction</article-title>. <source>Brief. Bioinform</source>. <volume>23</volume>, <fpage>bbac357</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Mendolia</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Contino</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>De Simone</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Perricone</surname>, <given-names>U.</given-names></string-name> &amp; <string-name><surname>Pirrone</surname>, <given-names>R.</given-names></string-name> <article-title>EMBER— Embedding Multiple Molecular Fingerprints for Virtual Screening</article-title>. <source>Int. J. Mol. Sci.</source> <volume>23</volume>, <fpage>2156</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Zhao</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Xie</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>X.</given-names></string-name> &amp; <string-name><surname>Sun</surname>, <given-names>S</given-names></string-name>. <article-title>Multi-view learning overview: Recent progress and new challenges</article-title>. <source>Inf. Fusion</source> <volume>38</volume>, <fpage>43</fpage>–<lpage>54</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Jiang</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal> <article-title>Could graph neural networks learn better molecular representation for drug discovery? A comparison study of descriptor-based and graph-based models</article-title> <source>J. Cheminformatics</source> <volume>13</volume>, <issue>12</issue> (<year>2021</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Wu</surname>, <given-names>Z.</given-names></string-name> <etal>et al.</etal> <article-title>MoleculeNet: a benchmark for molecular machine learning</article-title>. <source>Chem. Sci</source>. <volume>9</volume>, <fpage>513</fpage>–<lpage>530</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Subramanian</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Ramsundar</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Pande</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Denny</surname>, <given-names>R. A</given-names></string-name>. <article-title>Computational Modeling of β-Secretase 1 (BACE-1) Inhibitors Using Ligand Based Approaches</article-title>. <source>J. Chem. Inf. Model</source>. <volume>56</volume>, <fpage>1936</fpage>–<lpage>1949</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Hall</surname>, <given-names>L. H.</given-names></string-name> &amp; <string-name><surname>Kier</surname>, <given-names>L. B</given-names></string-name>. <article-title>Electrotopological State Indices for Atom Types: A Novel Combination of Electronic, Topological, and Valence State Information</article-title>. <source>J. Chem. Inf. Comput. Sci</source>. <volume>35</volume>, <fpage>1039</fpage>–<lpage>1045</lpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Roy</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Mitra</surname>, <given-names>I</given-names></string-name>. <article-title>Electrotopological State Atom (E-State) Index in Drug Design, QSAR, Property Prediction and Toxicity Assessment</article-title>. <source>Curr. Comput. Aided Drug Des</source>. <volume>8</volume>, <fpage>135</fpage>–<lpage>158</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Barta</surname>, <given-names>G</given-names></string-name>. <article-title>Identifying Biological Pathway Interrupting Toxins Using Multi-Tree Ensembles. <italic>Front</italic></article-title>. <source>Environ. Sci</source>. <volume>4</volume>, (<year>2016</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Uesawa</surname>, <given-names>Y</given-names></string-name>. <article-title>Rigorous Selection of Random Forest Models for Identifying Compounds that Activate Toxicity-Related Pathways. <italic>Front</italic></article-title>. <source>Environ. Sci</source>. <volume>4</volume>, (<year>2016</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Adadi</surname>, <given-names>A</given-names></string-name>. <article-title>A survey on data-efficient algorithms in big data era</article-title>. <source>J. Big Data</source> <volume>8</volume>, <fpage>24</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Mendez</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal> <article-title>ChEMBL: towards direct deposition of bioassay data</article-title>. <source>Nucleic Acids Res</source>. <volume>47</volume>, <fpage>D930</fpage>–<lpage>D940</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Rodríguez-Pérez</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Vogt</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Bajorath</surname>, <given-names>J</given-names></string-name>. <article-title>Influence of Varying Training Set Composition and Size on Support Vector Machine-Based Prediction of Active Compounds</article-title>. <source>J. Chem. Inf. Model</source>. <volume>57</volume>, <fpage>710</fpage>–<lpage>716</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Srinivas</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Klimovich</surname>, <given-names>P. V.</given-names></string-name> &amp; <string-name><surname>Larson</surname>, <given-names>E. C</given-names></string-name>. <article-title>Implicit-descriptor ligand-based virtual screening by means of collaborative filtering</article-title>. <source>J. Cheminformatics</source> <volume>10</volume>, <issue>56</issue> (<year>2018</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Mayr</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> <article-title>Large-scale comparison of machine learning methods for drug target prediction on ChEMBL</article-title>. <source>Chem. Sci</source>. <volume>9</volume>, <fpage>5441</fpage>–<lpage>5451</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Lee</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kumar</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>S.-Y.</given-names></string-name>, <string-name><surname>Park</surname>, <given-names>S. J.</given-names></string-name> &amp; <string-name><surname>Kim</surname>, <given-names>M</given-names></string-name>. <article-title>Development of Predictive Models for Identifying Potential S100A9 Inhibitors Based on Machine Learning Methods</article-title>. <source>Front. Chem</source>. <volume>7</volume>, (<year>2019</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Lee</surname>, <given-names>A. A.</given-names></string-name> <etal>et al.</etal> <article-title>Ligand biological activity predicted by cleaning positive and negative chemical correlations</article-title>. <source>Proc. Natl. Acad. Sci</source>. <volume>116</volume>, <fpage>3373</fpage>–<lpage>3378</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Yin</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal> <article-title>AFSE: towards improving model generalization of deep graph learning of ligand bioactivities targeting GPCR proteins</article-title>. <source>Brief. Bioinform</source>. <volume>23</volume>, bbac077 (<year>2022</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Siemers</surname>, <given-names>F. M.</given-names></string-name>, <string-name><surname>Feldmann</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Bajorath</surname>, <given-names>J</given-names></string-name>. <article-title>Minimal data requirements for accurate compound activity prediction using machine learning methods of different complexity</article-title>. <source>Cell Rep. Phys. Sci</source>. <volume>3</volume>, <fpage>101113</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Mysinger</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Carchia</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Irwin</surname>, <given-names>John. J.</given-names></string-name> &amp; <string-name><surname>Shoichet</surname>, <given-names>B. K.</given-names></string-name> <article-title>Directory of Useful Decoys, Enhanced (DUD-E): Better Ligands and Decoys for Better Benchmarking</article-title>. <source>J. Med. Chem</source>. <volume>55</volume>, <fpage>6582</fpage>–<lpage>6594</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Riniker</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Landrum</surname>, <given-names>G. A</given-names></string-name>. <article-title>Open-source platform to benchmark fingerprints for ligand-based virtual screening</article-title>. <source>J. Cheminformatics</source> <volume>5</volume>, <issue>26</issue> (<year>2013</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Lee</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kumar</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>S.-Y.</given-names></string-name>, <string-name><surname>Park</surname>, <given-names>S. J.</given-names></string-name> &amp; <string-name><surname>Kim</surname>, <given-names>M</given-names></string-name>. <article-title>Development of Predictive Models for Identifying Potential S100A9 Inhibitors Based on Machine Learning Methods</article-title>. <source>Front. Chem</source>. <volume>7</volume>, (<year>2019</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Khan</surname>, <given-names>M. I.</given-names></string-name> <etal>et al.</etal> <article-title>Development of machine learning models for the screening of potential HSP90 inhibitors</article-title>. <source>Front. Mol. Biosci</source>. <volume>9</volume>, (<year>2022</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Guzelj</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tomašič</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Jakopin</surname>, <given-names>Ž</given-names></string-name>. <article-title>Novel Scaffolds for Modulation of NOD2 Identified by Pharmacophore-Based Virtual Screening</article-title>. <source>Biomolecules</source> <volume>12</volume>, <fpage>1054</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Islam</surname>, <given-names>M. A.</given-names></string-name> <etal>et al.</etal> <article-title>Identification of Potential Cytochrome P450 3A5 Inhibitors: An Extensive Virtual Screening through Molecular Docking, Negative Image-Based Screening, Machine Learning and Molecular Dynamics Simulation Studies</article-title>. <source>Int. J. Mol. Sci.</source> <volume>23</volume>, <fpage>9374</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><surname>Patil</surname>, <given-names>S. P.</given-names></string-name> <etal>et al.</etal> <article-title>Machine-Learning Guided Discovery of Bioactive Inhibitors of PD1-PDL1 Interaction</article-title>. <source>Pharmaceuticals</source> <volume>15</volume>, <fpage>613</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Chen</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal> <article-title>Hidden bias in the DUD-E dataset leads to misleading performance of deep learning in structure-based virtual screening</article-title>. <source>PLOS ONE</source> <volume>14</volume>, <fpage>e0220113</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Polyzotis</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Zaharia</surname>, <given-names>M</given-names></string-name>. <article-title>What can Data-Centric AI Learn from Data and ML Engineering?</article-title> <source>arXiv</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2112.06439">http://arxiv.org/abs/2112.06439</ext-link> (<year>2021</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="web"><string-name><surname>Jarrahi</surname>, <given-names>M. H.</given-names></string-name>, <string-name><surname>Memariani</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Guha</surname>, <given-names>S.</given-names></string-name> <article-title>The Principles of Data-Centric AI (DCAI)</article-title>. Preprint at <pub-id pub-id-type="doi">10.48550/arXiv.2211.14611</pub-id> (<year>2022</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><surname>Tripathi</surname>, <given-names>M. K.</given-names></string-name>, <string-name><surname>Nath</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Singh</surname>, <given-names>T. P.</given-names></string-name>, <string-name><surname>Ethayathulla</surname>, <given-names>A. S.</given-names></string-name> &amp; <string-name><surname>Kaur</surname>, <given-names>P</given-names></string-name>. <article-title>Evolving scenario of big data and Artificial Intelligence (AI) in drug discovery</article-title>. <source>Mol. Divers</source>. <volume>25</volume>, <fpage>1439</fpage>–<lpage>1460</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><surname>Zhao</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Ciallella</surname>, <given-names>H. L.</given-names></string-name>, <string-name><surname>Aleksunes</surname>, <given-names>L. M.</given-names></string-name> &amp; <string-name><surname>Zhu</surname>, <given-names>H</given-names></string-name>. <article-title>Advancing computer-aided drug discovery (CADD) by big data and data-driven machine learning modeling</article-title>. <source>Drug Discov. Today</source> <volume>25</volume>, <fpage>1624</fpage>–<lpage>1638</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><surname>Zhu</surname>, <given-names>H</given-names></string-name>. <article-title>Big Data and Artificial Intelligence Modeling for Drug Discovery</article-title>. <source>Annu. Rev. Pharmacol. Toxicol</source>. <volume>60</volume>, <fpage>573</fpage>–<lpage>589</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><surname>Gómez-Bombarelli</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal> <article-title>Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules</article-title>. <source>ACS Cent. Sci</source>. <volume>4</volume>, <fpage>268</fpage>–<lpage>276</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><string-name><surname>Lin</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Quan</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Z.-J.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Zeng</surname>, <given-names>X</given-names></string-name>. <article-title>A novel molecular representation with BiGRU neural networks for learning atom</article-title>. <source>Brief. Bioinform</source>. <volume>21</volume>, <fpage>2099</fpage>–<lpage>2111</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><string-name><surname>Li</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Jiang</surname>, <given-names>X</given-names></string-name>. <article-title>Mol-BERT: An Effective Molecular Representation with BERT for Molecular Property Prediction</article-title>. <source>Wirel. Commun. Mob. Comput</source>. <volume>2021</volume>, <fpage>e7181815</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><string-name><surname>Mucllari</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Zadorozhnyy</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Ye</surname>, <given-names>Q.</given-names></string-name> &amp; <string-name><surname>Nguyen</surname>, <given-names>D. D</given-names></string-name>. <article-title>Novel Molecular Representations Using Neumann-Cayley Orthogonal Gated Recurrent Unit</article-title>. <source>J. Chem. Inf. Model</source>. <volume>63</volume>, <fpage>2656</fpage>–<lpage>2666</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><string-name><surname>Mayr</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Klambauer</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Unterthiner</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Hochreiter</surname>, <given-names>S</given-names></string-name>. <article-title>DeepTox: Toxicity Prediction using Deep Learning. <italic>Front</italic></article-title>. <source>Environ. Sci</source>. <volume>3</volume>, (<year>2016</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="web"><string-name><surname>Landrum</surname>, <given-names>G.</given-names></string-name> <source>RDKit: Open-source cheminformatics.</source> Available from: <ext-link ext-link-type="uri" xlink:href="https://rdkit.org/">https://rdkit.org/</ext-link>. Zenodo <pub-id pub-id-type="doi">10.5281/zenodo.10398</pub-id> (<year>2014</year>).</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><string-name><surname>Willighagen</surname>, <given-names>E. L.</given-names></string-name> <etal>et al.</etal> <article-title>The Chemistry Development Kit (CDK) v2.0: atom typing, depiction, molecular formulas, and substructure searching</article-title>. <source>J. Cheminformatics</source> <volume>9</volume>, <issue>33</issue> (<year>2017</year>).</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="web"><collab>PubChem.</collab> <source>PubChem Substructure Fingerprint. (2/20/2021).</source> Available at <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/pubchem/specifications/pubchem_fingerprints.pdf">https://ftp.ncbi.nlm.nih.gov/pubchem/specifications/pubchem_fingerprints.pdf</ext-link>. <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/pubchem/specifications/pubchem_fingerprints.pdf">https://ftp.ncbi.nlm.nih.gov/pubchem/specifications/pubchem_fingerprints.pdf</ext-link> (<year>2009</year>).</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><string-name><surname>Klekota</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Roth</surname>, <given-names>F. P</given-names></string-name>. <article-title>Chemical substructures that enrich for biological activity</article-title>. <source>Bioinformatics</source> <volume>24</volume>, <fpage>2518</fpage>–<lpage>2525</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><string-name><surname>Rogers</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Brown</surname>, <given-names>R. D.</given-names></string-name> &amp; <string-name><surname>Hahn</surname>, <given-names>M</given-names></string-name>. <article-title>Using Extended-Connectivity Fingerprints with Laplacian-Modified Bayesian Analysis in High-Throughput Screening Follow-Up</article-title>. <source>SLAS Discov</source>. <volume>10</volume>, <fpage>682</fpage>–<lpage>686</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><string-name><surname>Rogers</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Hahn</surname>, <given-names>M.</given-names></string-name> <article-title>Extended-Connectivity Fingerprints</article-title>. <source>J. Chem. Inf. Model.</source> <volume>50</volume>, <fpage>742</fpage>–<lpage>754</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="web"><collab>The Chemistry Development Kit (CDK): Extended Fingerprint</collab> <ext-link ext-link-type="uri" xlink:href="https://github.com/egonw/cdk/blob/daba2d410cf4fd8462d55a3a4dd6f168db207af3/descriptor/fingerprint/src/main/java/org/openscience/cdk/fingerprint/ExtendedFingerprinter.java">https://github.com/egonw/cdk/blob/daba2d410cf4fd8462d55a3a4dd6f168db207af3/descriptor/fingerprint/src/main/java/org/openscience/cdk/fingerprint/ExtendedFingerprinter.java</ext-link>. (<year>2020</year>).</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><string-name><surname>Nilakantan</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Bauman</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Dixon</surname>, <given-names>J. S.</given-names></string-name> &amp; <string-name><surname>Venkataraghavan</surname>, <given-names>R</given-names></string-name>. <article-title>Topological torsion: a new molecular descriptor for SAR applications. Comparison with other descriptors</article-title>. <source>J. Chem. Inf. Comput. Sci</source>. <volume>27</volume>, <fpage>82</fpage>–<lpage>85</lpage> (<year>1987</year>).</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><string-name><surname>Carhart</surname>, <given-names>R. E.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>D. H.</given-names></string-name> &amp; <string-name><surname>Venkataraghavan</surname>, <given-names>R</given-names></string-name>. <article-title>Atom pairs as molecular features in structure-activity studies: definition and applications</article-title>. <source>J. Chem. Inf. Comput. Sci</source>. <volume>25</volume>, <fpage>64</fpage>–<lpage>73</lpage> (<year>1985</year>).</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><string-name><surname>Reutlinger</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>Chemically Advanced Template Search (CATS) for Scaffold-Hopping and Prospective Target Prediction for ‘Orphan’ Molecules</article-title>. <source>Mol. Inform</source>. <volume>32</volume>, <fpage>133</fpage>–<lpage>138</lpage> (<year>2013</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97821.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Talevi</surname>
<given-names>Alan</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>National University of La Plata</institution>
</institution-wrap>
<city>La Plata</city>
<country>Argentina</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents a <bold>valuable</bold> finding on how data quality and data representation are key to obtain predictive machine learning models, even without resorting to complex machine learning approaches. The evidence supporting the claims of the authors is, however, <bold>incomplete</bold>, as their conclusions are drawn from a single dataset of big size, similarity analysis within and between subsets is lacking, and there are concerns regarding the composition of the training and holdout sets (active:inactive ratio, possible triviality of decoys). If the results were expanded to other quality datasets of different compositions to demonstrate robustness, the manuscript would be of wide interest in the machine learning and drug discovery fields</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97821.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The work provides more evidence of the importance of data quality and representation for ligand-based virtual screening approaches. The authors have applied different machine learning (ML) algorithms and data representation using a new dataset of BRAF ligands. First, the authors evaluate the ML algorithms and demonstrate that independently of the ML algorithm, predictive and robust models can be obtained in this BRAF dataset. Second, the authors investigate how the molecular representations can modify the prediction of the ML algorithm. They found that in this highly curated dataset the different molecule representations are adequate for the ML algorithms since almost all of them obtain high accuracy values, with Estate fingerprints obtaining the worst-performing predictive models and ECFP6 fingerprints producing the best classificatory models. Third, the authors evaluate the performance of the models on subsets of different composition and size of the BRAF dataset. They found that given a finite number of active compounds, increasing the number of inactive compounds worsens the recall and accuracy. Finally, the authors analyze if the use of &quot;less active&quot; molecules affect the model's predictive performance using &quot;less active&quot; molecules taken from ChEMBl Database or using decoys from DUD-E. As results, they found that the accuracy of the model falls as the number of &quot;less active&quot; examples in the training dataset increases while the implementation of decoys in the training set generates results as good as the original models or even better in some cases. However, the use of decoys in the training set worsens the predictive power in the test sets that contain active and inactive molecules.</p>
<p>Strengths:</p>
<p>It is a very interesting topic in medicinal chemistry and drug discovery. This work is very well written and contains up-to-date references. The general structure of the work is adequate, allowing easy reading. The hypotheses are clear and were explored correctly. This work provides new evidence about the importance of inferring models from high-quality data and that, if such a condition is met, it is not necessary to use complex computational methods to obtain predictive models. The generated BRAF dataset is also a valuable benchmark dataset for medicinal chemists working in ligand based virtual screening.</p>
<p>Weaknesses:</p>
<p>Leaving aside the new curated BRAF dataset, the work lacks novelty since it is a topic widely studied in chemoinformatics and medicinal chemistry. Furthermore, the conclusions drawn here correspond to the analysis of only one high-quality dataset where the similarity between the molecules is not quantitatively assessed (maybe active and inactive molecules are very dissimilar and any ML algorithm and fingerprint could obtain good results). To generalize the conclusions, it would be fundamental to repeat the analysis with other high-quality datasets.</p>
<p>Some key tasks are not clearly described, for example, there is no information about the new BRAF dataset (e.g., where the molecules were obtained from or why the inactive molecules provide better results than the &quot;less active&quot; from ChEMBL... what differentiates them?). The defintion of an &quot;inactive&quot; compound is not clear. It is not described if global or balanced accuracy was used in the imbalanced datasets. When using decoys to evaluate the models it is important to consider that decoys were generated to be topologically different from active compounds by the comparison of the ECFP4 fingerprints using the Tanimoto coefficient. Therefore, it is quite obvious that when fingerprints are used to characterize molecules, the models will be able to easily discriminate them. It is important to note that this is not necessarily true for models based on other molecular descriptors, since they are not used in the generation of the decoys. In some cases, the differences between accuracies are very small and there are no statistical analyzes to demonstrate whether they are statistically different or not.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97821.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors explored the importance of data quality and representation for ligand-based virtual screening approaches. I believe the results could be of potential benefit to the drug discovery community, especially to those scientists working in the field of machine learning applied to drug research. The in silico design is comprehensive and adequate for the proposed comparisons.</p>
<p>This manuscript by Chong A. et al describes that it is not necessary to resort to the use of sophisticated deep learning algorithms for virtual screening, since based on their results considering conventional ML may perform exceptionally well if fed by the right data and molecular representations.</p>
<p>The article is interesting and well-written. The overview of the field and the warning about dataset composition are very well thought-out and should be of interest to a broad segment of the AI ​​in drug discovery readership. This article further highlights some of the considerations that need to be taken into consideration for the implementation of data-centric AI for computer-aided drug design methods.</p>
<p>Strengths:</p>
<p>This study contributes significantly to the field of machine learning and data curation in drug discovery. The paper is, in general, well-written and structured. However, in my opinion, there are some suggestions regarding certain aspects of the data analyses.</p>
<p>Weaknesses:</p>
<p>The conclusions drawn in the study are based on the analysis of a single dataset, and I am not sure they can be generalized. Therefore, in my opinion, the conclusions are only partially supported by the data. To generalize the conclusions, it is imperative to conduct a benchmark with diverse datasets, for different molecular targets.</p>
<p>
The conclusion cannot be immediately extended to molecular descriptors or features different from the ones used in this study</p>
<p>
It is advisable to present statistical analyses to ascertain whether the observed differences in metrics hold statistical significance.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97821.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors presented a data-centric ML approach for virtual ligand screening. They used BRAF as an example to demonstrate the predictive power of their approach.</p>
<p>Strengths:</p>
<p>The performance of predictive models in this study is superior (nearly perfect) with respect to exiting methods.</p>
<p>Weaknesses:</p>
<p>I feel the training and testing datasets may not be rigorously constructed. If that is the case, the results would be significantly affected.</p>
<p>I have 3 major comments:</p>
<p>(1) The authors identified ~4100 BRAF actives, then randomly selected 3600 BRAF actives to be part of the training dataset with the remaining 500 actives becoming a part of the hold-out test set. The problem is that, the authors did not evaluate the chemical similarity between the 3600 actives in the training, and the 500 actives in the testing set. If some of them were similar, the testing results would be very good but partially due to information leakage. The authors should carefully examine the chemical similarity between any pairs of their training and testing datasets, before any conclusion is made.</p>
<p>(2) The authors tried to explore the role of dataset size in the performance, in particular, what would happen when the number of actives are reduced. However the minimal number of actives used is 500 while the number of inactives ranges from 500 to 3600. This is quite different from real applications where the number of expected actives in the screening library would be at most 1-2% of the whole database. The authors should further reduced the number of actives (e.g. 125, 25, 5, 1), and evaluate their model's performance.</p>
<p>(3) The authors chose BRAF as example in this study. BRAF is a well studied drug target with thousands of known actives. In real applications, the target may only have a handful of known actives. The authors should try to apply their approach, to a couple other targets that have less known actives than BRAF, to evaluate their method's transferability.</p>
</body>
</sub-article>
</article>