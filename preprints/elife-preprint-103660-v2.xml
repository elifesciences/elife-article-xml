<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">103660</article-id>
<article-id pub-id-type="doi">10.7554/eLife.103660</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.103660.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Neural dynamics of reversal learning in the prefrontal cortex and recurrent neural networks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1322-6207</contrib-id>
<name>
<surname>Kim</surname>
<given-names>Christopher M</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
<email>chrismkkim@gmail.com</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chow</surname>
<given-names>Carson C</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3976-8565</contrib-id>
<name>
<surname>Averbeck</surname>
<given-names>Bruno B</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00adh9b73</institution-id><institution>Laboratory of Biological Modeling, NIDDK/NIH</institution></institution-wrap>, <city>Bethesda</city>, <country country="US">United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id><institution>Laboratory of Neuropsychology, NIMH/NIH</institution></institution-wrap>, <city>Bethesda</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ponte Costa</surname>
<given-names>Rui</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Luo</surname>
<given-names>Huan</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="present-address"><label>*</label><p>Present address: Department of Mathematics, Howard University, Washington, United States</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-12-06">
<day>06</day>
<month>12</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-06-10">
<day>10</day>
<month>06</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP103660</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-10-04">
<day>04</day>
<month>10</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-09-15">
<day>15</day>
<month>09</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.14.613033"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-12-06">
<day>06</day>
<month>12</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103660.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.103660.1.sa3">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.103660.1.sa2">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.103660.1.sa1">Reviewer #2 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.103660.1.sa0">Reviewer #3 (Public review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.103660.1.sa4">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">
<ali:license_ref>https://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref>
<license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-103660-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>In probabilistic reversal learning, the choice option yielding reward with higher probability switches at a random trial. To perform optimally in this task, one has to accumulate evidence across trials to infer the probability that a reversal has occurred. We investigated how this reversal probability is represented in cortical neurons by analyzing the neural activity in the prefrontal cortex of monkeys and recurrent neural networks trained on the task. We found that in a neural subspace encoding reversal probability, its activity represented integration of reward outcomes as in a line attractor model. The reversal probability activity at the start of a trial was stationary, stable and consistent with the attractor dynamics. However, during the trial, the activity was associated with task-related behavior and became non-stationary, thus deviating from the line attractor. Fitting a predictive model to neural data showed that the stationary state at the trial start serves as an initial condition for launching the non-stationary activity. This suggested an extension of the line attractor model with behavior-induced non-stationary dynamics. The non-stationary trajectories were separable indicating that they can represent distinct probabilistic values. Perturbing the reversal probability activity in the recurrent neural networks biased choice outcomes demonstrating its functional significance. In sum, our results show that cortical networks encode reversal probability in stable stationary state at the start of a trial and utilize it to initiate non-stationary dynamics that accommodates task-related behavior while maintaining the reversal information.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Updated interpretation of our results. Details on network model and experimental data included.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>To survive in a dynamically changing world, animals must interact with the environment and learn from their experience to adjust their behavior. Reversal learning has been used for assessing the ability to adapt one’s behavior in such an environment [<xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c6">6</xref>]. For instance, in two-armed bandit tasks with probabilistic reward, the subject learns from initial trials that one option has higher reward probability than the other. When the reward probability of the two options is reversed at a random trial, the subject must learn to reverse its preferred choice to maximize reward outcome. In these tasks, there is uncertainty in when to reverse one’s choice, as reward is received stochastically even when the less favorable option is chosen. Therefore, it is essential that reward outcomes are integrated over multiple trials before the initial choice preference is reversed. Although neural mechanisms for accumulating evidence within a trial have been studied extensively [<xref ref-type="bibr" rid="c7">7</xref>–<xref ref-type="bibr" rid="c10">10</xref>], it remains unclear if a recurrent neural circuit uses a similar neural mechanism for accumulating evidence across multiple trials, while performing task-related intervening behavior during each trial.</p>
<p>In this study, we merged two classes of computational models, i.e., behavioral and neural, to investigate the neural basis of multi-trial evidence accumulation. Behavior models capture subject’s behavioral strategies for performing the reversal learning task. For instance, model-free reinforcement learning (RL) [<xref ref-type="bibr" rid="c11">11</xref>–<xref ref-type="bibr" rid="c13">13</xref>] assumes that the subject learns only from choices and reward outcomes without specific knowledge about task structure. Model-based Bayesian inference [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>], in contrast, assumes that the task structure is known to the subject, and one can infer reversal points statistically, resulting in abrupt switches in choice preference. Model-based and model-free RL models are formal models that do not specify implementation in a network of neurons. On the other hand, neural models implemented with recurrent neural networks (RNNs) can be trained to use recurrent activity to perform the reversal learning task. In particular, attractor dynamics, in which the network state moves towards discrete [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c16">16</xref>] or along continuous [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c17">17</xref>] attractor states, have been studied extensively as a potential neural mechanism for decision-making and evidence accumulation [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c19">19</xref>].</p>
<p>We sought to train continuous time RNNs to mimic the behavioral strategies of monkeys performing the reversal learning task. Previous studies [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c4">4</xref>] have shown that a Bayesian inference model can capture a key aspect of the monkey’s behavioral strategy, i.e., adhere to the preferred choice until the reversal of reward schedule is detected and then switch abruptly. We trained the RNNs to replicate this behavioral strategy by training them on target behaviors generated from the Bayesian model.</p>
<p>We found that the activity in the neural subspace representing reversal probability could be explained by integrating reward outcomes across trials. At the start of a trial, when the subject was holding fixation before cues were shown, the reversal probability activity was stationary and stable. This stationary activity mode was compatible with the line attractor model that accumulates evidence along attracting states. However, during the trial, the neural activity representing reversal probability had substantial dynamics and was associated with task-related behavior, such as making decisions or receiving feedback. The non-stationary activity during the trial made the line attractor model, which requires the network state to stay close to attractor states [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c17">17</xref>], inadequate for explaining the neural activity encoding evidence accumulation in reversal learning.</p>
<p>To better understand how reversal probability is represented in two different activity modes, we investigated the underlying dynamics that link the two. We found that the non-stationary trajectory can be predicted from the stationary activity at the trial start, suggesting that underlying dynamics, associated with task-related behavior, generates the non-stationary activity. In addition, separable points at the initial state remained separable as they propagated in time, allowing distinct probabilistic values to be represented in the trajectories. These findings suggested an extension of the line attractor model where stationary activity on the line attractor provides an initial state from which non-stationary dynamics that preserves separability is initiated. Finally, perturbing reversal probability activity causally affected choice outcomes, demonstrating its functional significance.</p>
<p>Our results show that, in a probabilistic reversal learning task, cortical networks encode the reversal probability by adopting, not only stationary states as in a line attractor, but also separable dynamic trajectories that can represent distinct probabilistic values and accommodate non-stationary dynamics associated with task-related behavior.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<label>1</label>
<title>Trained RNN’s choices are consistent with monkey behavior</title>
<p>We considered a reversal learning task that monkeys were trained to perform in a previous study [<xref ref-type="bibr" rid="c4">4</xref>]. In our study, we trained the RNNs to perform a task similar to the task the monkeys performed. A reversal learning task was completed by executing a block of a fixed number of trials. In each trial, the subject (monkey or RNN) chose one of the two available options, and reward for the choice was delivered stochastically. At the beginning of a block, one option was designated as the high-value option, while the other option was designated as the low-value option. The high-value option was rewarded 70% of the time when chosen, and the low-value option was rewarded 30% of the time when chosen. The reward probability of two options was switched at a random trial near the mid-trial of a block, and the reversed reward schedule was maintained until the end of the block. The reversal of reward schedule occurred only once in our task, differing from other versions of reversal learning where reversal occurs multiple times across trials [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c20">20</xref>]. The subject had to learn to switch its preferred choice to maximize reward. Because reward delivery was stochastic, the subject had to infer the reversal of reward schedule by accumulating evidence that a reversal had occurred.</p>
<p>The sequence of events occurring in a trial was structured similarly in the RNN and the monkey tasks. As described in [<xref ref-type="bibr" rid="c4">4</xref>], in each trial, the monkeys were first shown a signal that required them to fixate for a variable time (400 - 800ms). Then two cues were presented to both sides of the fixation dot simultaneously. The monkeys made a saccade to an option to report their choice. After holding for 500ms, the reward was delivered (see Methods <xref ref-type="sec" rid="s2a">Section 2.1</xref> for details). In the RNNs, they were first stimulated by a cue signaling them to make a choice. After a short delay the RNNs made a choice between two options. Then, the RNNs received feedback that signaled the choice it made and whether the outcome was rewarded (<xref rid="fig1" ref-type="fig">Fig.1A</xref>). This same trial structure was repeated across all trials in a block (see Methods <xref ref-type="sec" rid="s1">Section 1</xref> for details).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Comparison of the behavior of trained RNNs and monkeys.</title>
<p><bold>(A)</bold> Schematic of RNN training setup. In a trial, the network makes a choice in response to a cue. Then, a feedback input, determined by the choice and reward outcome, is injected to the network. This procedure is repeated across trials. The panel on the right shows this sequence of events unfolding in time in a trial. <bold>(B)</bold> Left: Example of a trained RNN’s choice outcomes. Vertical bars show RNN choices in each trial and the reward outcomes (magenta: choice A, blue: choice B, light: rewarded, dark: not rewarded). Horizontal bars on the top show reward schedules (magenta: choice A receiving reward is 70%, choice B receiving reward is 30%; blue: reward schedule is reversed). Black curve shows the RNN output. Green horizontal bars show the posterior of reversal probability at each trial inferred using Bayesian model. Right: Schematic of RNN training scheme. The scheduled reversal indicates the trial at which the reward probabilities of two options switch (color codes for magenta and cyan are the same as the left panel). The inferred reversal is the scheduled reversal trial inferred from the Bayesian model. The behavioral reversal is determined by adding a few delay trials to the inferred reversal trial. The target output, on which the RNNs outputs are trained, switches at the behavioral reversal trial. <bold>(C)</bold> Probability of choosing the initial best (i.e., high-value) option. Relative trial indicates the trial number relative to the behavioral reversal trial inferred from the Bayesian model. Relative trial number 0 is the trial at which the choice was reversed. Shaded region shows the S.E.M (standard error of mean) over blocks in all the sessions (monkeys) or networks (RNNs). <bold>(D)</bold> Fraction of no-reward blocks as a function of relative trial. Dotted lines show 0.3 and 0.7. Shaded region shows the S.E.M (standard error of mean) over blocks in all the sessions (monkeys) or networks (RNNs). <bold>(E)</bold> Distribution of RNN’s and monkey’s reversal trial, relative to the experimentally scheduled reversal trial.</p></caption>
<graphic xlink:href="613033v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The RNN and the monkeys tasks had differences. For the RNNs, there were 36 trials in a block during testing (24 trials were used for training) and, for the monkeys, there were 80 trials in a block. The number of RNN trials was reduced to avoid GPU memory overflow issues when training with backpropagation-through-time. For the RNNs, the reversal of reward schedule occurred on a trial randomly chosen from the 10 middle trials of a block. For the monkeys, the reversal trial was chosen randomly from 20 middle trials of a block. The fixation period was not required in the RNNs since they performed the task without it; however, we examined the effects of having a fixation period on neural dynamics (see <xref rid="fig4" ref-type="fig">Fig. 4D</xref>). In total, 40 successfully trained RNNs were analyzed in our study, where each RNN performed 20 blocks of reversal learning tasks. On the other hand, two monkeys performed in total 8 sessions of experiments, where each session consisted of 24 blocks of reversal learning.</p>
<p>To train RNNs, we set up a reward schedule where the high-value option at the start of a trial was randomly selected from two options. Since the initial high-value option was switched to the other option at a random trial (<xref rid="fig1" ref-type="fig">Fig.1B</xref>, scheduled reversal), the RNNs had to learn to reverse their preferred choice to maximize reward. To learn to reverse, the RNNs were trained to mimic the outputs of a Bayesian inference model that was shown to capture the monkey’s reversal behavior in previous studies [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c4">4</xref>]. We first let the RNNs perform the task by simulating the RNN dynamics starting at a random initial state and providing relevant stimuli, such as cue and feedback, at every trial. Once the RNNs completed a block of trials, the choice and reward outcomes of all trials in a block were fed into the Bayesian model to infer the trial at which reward schedule was reversed, referred to as the inferred scheduled reversal trial (<xref rid="fig1" ref-type="fig">Fig.1B</xref>, inferred reversal). Previous studies have shown that monkeys reverse their preferred choice (Fig.1B, behavioral reversal) a few trials after the scheduled reversal trial [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c4">4</xref>]. Therefore, the target choice outputs (<xref rid="fig1" ref-type="fig">Fig.1B</xref>, target output), on which the RNNs were trained, were set to be the initial high-value option until a few trials after the inferred scheduled reversal trial, followed by an abrupt switch to the other option. The recurrent weights and the output weights of the RNNs were trained via supervised learning to minimize the cross-entropy between the RNN choice outputs and the target choice outputs (See Methods <xref ref-type="sec" rid="s2e">Section 2.5</xref> for details on the RNN training).</p>
<p>After training, in a typical block, a trained RNN selected the initial high-value option, despite occasionally not receiving a reward, but abruptly switched its choice when consecutive no-reward trials persisted (<xref rid="fig1" ref-type="fig">Fig.1B</xref>, left). Such abrupt reversal behavior was expected as the RNNs were trained to mimic the target outputs of the Bayesian inference model (<xref rid="fig1" ref-type="fig">Fig.1B</xref>, right). The intrinsic time scale of the RNN (<italic>τ</italic> = 20ms in <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref> in Methods <xref ref-type="sec" rid="s1">Section 1</xref>) was substantially faster than the duration of a trial (500ms), thus the persistent behavior over multiple trials was a result of learning the task. Analyzing the choice outputs of all the trained RNNs showed that, as in the example discussed, they selected the high-value option with high probability before the behavioral reversal, at which time they abruptly switched their choice (<xref rid="fig1" ref-type="fig">Fig.1C</xref>, black). This abrupt reversal behavior was also found in the monkey’s behavior trained on the same task (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>, orange). The behavioral reversal was preceded by a gradually increasing number of no-reward trials in the RNNs and the monkeys (<xref rid="fig1" ref-type="fig">Fig.1D</xref>). The distribution of behavioral reversal trials (i.e., trial at which preferred choice was reversed) relative to the scheduled reversal trial (i.e., trial at which reward schedule was reversed) was in good agreement with the distribution of monkey’s reversal trials (<xref rid="fig1" ref-type="fig">Fig.1E</xref>).</p>
</sec>
<sec id="s2b">
<label>2</label>
<title>Task-relevant neural activity evolves dynamically</title>
<p>Next, we analyzed the neural activity of neurons in the dorsolateral prefrontal cortex (PFC) of two monkeys and the activation of population of neurons in the RNNs. The spiking activity of PFC neurons was recorded while they performed the reversal learning task using eight microelectrode arrays. In each session, we recorded simultaneously from 573 to 1,023 neurons (n=8 sessions, median population size 706). This neural data was collected and analyzed in a previous manuscript [<xref ref-type="bibr" rid="c4">4</xref>]. For each PFC neuron, we counted the spikes it emitted in a 300ms time window that moved in 20ms increment to analyze its spiking activity over time. For the RNNs, 40 successfully trained RNNs, whose reward rates were close to 70%, were included in the analysis.</p>
<p>We examined the temporal dynamics of task-relevant neural activity, in particular activity encoding the choice and reversal probability. To capture task-relevant neural activity, we first identified population vectors that encoded the task variables using a method called targeted dimensionality reduction [<xref ref-type="bibr" rid="c21">21</xref>]. It regresses the activity of each neuron at each time bin onto task variables across trials and finds the maximal population vector of each task variable. Then, neural activity representing the task variable is obtained by projecting the population activity onto the identified task vectors (see Methods <xref ref-type="sec" rid="s3">Section 3</xref> for details). The spiking activity of PFC neurons was shown to encode the reversal of behavior in a previous study [<xref ref-type="bibr" rid="c4">4</xref>]. Following this line of work, we focused on analyzing the trials around the behavioral reversal point in each block. We referenced the position of each trial to the behavioral reversal trial as a relative trial.</p>
<p>Within each trial, the block-averaged neural activity associated with choices and inferred reversal probability, denoted as <italic>x</italic><sub><italic>choice</italic></sub> and <italic>x</italic><sub><italic>rev</italic></sub>, respectively, produced non-stationary dynamics (<xref rid="fig2" ref-type="fig">Fig.2A</xref>, left). Their activity level reached a maximum around the time of cue onset (white squares in <xref rid="fig2" ref-type="fig">Fig.2A</xref>, left), when the monkey and RNN were about to make a choice. Such rotational neural dynamics were found both in the PFC of monkeys and the RNNs.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Neural trajectories encoding choice and reversal probability variables.</title>
<p><bold>(A)</bold> Neural trajectories of PFC (top) and RNN (bottom) obtained by projecting population activity onto task vectors encoding choice and reversal probability. Trial numbers indicate their relative position to the behavioral reversal trial. Neural trajectories in each trial were averaged over 8 experiment sessions and 23 blocks for the PFC, and 40 networks and 20 blocks for the RNNs. Black square indicates the time of cue onset. <bold>(B-C)</bold> Neural activity encoding reversal probability and choice in PFC (top) and RNN (bottom) at the time of cue onset (black squares in panel A) around the behavioral reversal trial. Shaded region shows the S.E.M over sessions (or networks) and blocks.</p></caption>
<graphic xlink:href="613033v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Across trials, the orientation of rotational trajectories shifted, indicating systematic changes in the choice and reversal probability activity. When the task-relevant activity at cue onset (or fixation) was analyzed, the points in the two-dimensional phase space (<italic>x</italic><sub><italic>rev</italic></sub> and <italic>x</italic><sub><italic>choice</italic></sub>) shifted gradually across trials (<xref rid="fig2" ref-type="fig">Fig.2A</xref>, right). We found that reversal probability activity, <italic>x</italic><sub><italic>rev</italic></sub>, peaked at the reversal trial in the PFC and the RNN (<xref rid="fig2" ref-type="fig">Fig.2B</xref>). Choice activity, <italic>x</italic><sub><italic>choice</italic></sub>, on the other hand, decreased gradually over trials reflecting the changes in choice preference (<xref rid="fig2" ref-type="fig">Fig.2C</xref>). The inverted-V shape of <italic>x</italic><sub><italic>rev</italic></sub> and the monotonic decrease of <italic>x</italic><sub><italic>choice</italic></sub> over trials explained the counter-clockwise shift in the rotational trajectories observed in the two-dimensional phase space (<xref rid="fig2" ref-type="fig">Fig.2A</xref>).</p>
<p>A recent study found that, when a neural network was trained via reinforcement learning to perform a reversal learning task, the first two principal components of the network activity shifted gradually following a shape similar to <italic>x</italic><sub><italic>rev</italic></sub> and <italic>x</italic><sub><italic>choice</italic></sub> (see <xref rid="fig1" ref-type="fig">Fig. 1</xref> in [<xref ref-type="bibr" rid="c20">20</xref>]). These results suggest that the gradual shift in network states across trials (<xref rid="fig2" ref-type="fig">Fig.2B, C</xref>) could be a common feature that emerges in networks performing a reversal learning task, regardless of training methods. One main difference is that, by design, these neural networks lacked within-trial dynamics in contrast to our RNNs [<xref ref-type="bibr" rid="c20">20</xref>]. In the following sections, we characterize the dynamics and properties of the non-stationary neural activity that our RNNs and the PFC of monkeys generated during the trials (<xref rid="fig2" ref-type="fig">Fig.2A</xref>).</p>
</sec>
<sec id="s2c">
<label>3</label>
<title>Integration of reward outcomes drives reversal probability activity</title>
<p>We first asked if there were underlying dynamics that systematically changed the reversal probability activity, <italic>x</italic><sub><italic>rev</italic></sub>. Previous works have shown that accumulation of decision-related evidence can be represented as a line attractor in a stable subspace of network activity [<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>]. We hypothesized that the gradual shift of <italic>x</italic><sub><italic>rev</italic></sub> (<xref rid="fig2" ref-type="fig">Figs.2A, B</xref>) could be characterized similarly by a line attractor model, where <italic>x</italic><sub><italic>rev</italic></sub> is explained by integrating reward outcomes across trials.</p>
<p>To test this idea, we set up a reward integration equation <inline-formula><inline-graphic xlink:href="613033v2_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula> that predicts the next trial’s reversal probability activity at time <italic>t</italic> based on the current trial’s reversal probability activity at time <italic>t</italic> and the reward outcome, therefore predicting across-trial reversal probability by integrating reward outcomes. Here, <italic>t</italic> is a time point within a trial (e.g., <italic>t</italic> = <italic>t</italic><sub><italic>cue</italic></sub> is the time of cue onset), <inline-formula><inline-graphic xlink:href="613033v2_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the reversal probability activity at <italic>t</italic> on trial <italic>k</italic>, and <inline-formula><inline-graphic xlink:href="613033v2_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is an estimate of the shift in reversal probability activity at <italic>t</italic> driven by trial <italic>k</italic>’s reward outcome (+ if rewarded and − if not rewarded. See Methods <xref ref-type="sec" rid="s4">Section 4</xref> for details).</p>
<p>When the reward integration activity at cue onset (<italic>t</italic><sub><italic>cue</italic></sub>) was analyzed, the predicted reversal probability activity was in good agreement with the actual reversal probability activity of PFC and RNN (example blocks shown in <xref rid="fig3" ref-type="fig">Figs.3A,C</xref>; prediction accuracy of all blocks shown in <xref rid="fig3" ref-type="fig">Fig.3E</xref>). In addition, we found that <inline-formula><inline-graphic xlink:href="613033v2_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula> responded to reward outcomes consistently with how reversal probability itself would be updated: no reward increased the reversal probability activity in the next trial (<xref rid="fig3" ref-type="fig">Figs. 3B, D</xref>; no reward), while a reward decreased it (<xref rid="fig3" ref-type="fig">Figs. 3B, D</xref>; reward). At the behavioral reversal trial (<italic>k</italic> = 0), however, the reversal probability activity in the following trial (<italic>k</italic> = 1) decreased regardless of the reward outcome at the reversal trial.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Integration of reward outcomes drives reversal probability activity.</title>
<p><bold>(A)</bold> The reversal probability activity of PFC (orange) and prediction by the reward integration equation (blue) at the time of cue onset across trials around the behavioral reversal trial. Three example blocks are shown. Pearson correlation between the actual and predicted PFC activity is shown on each panel. Relative trial number indicate the trial position relative to the behavioral reversal trial. <bold>(B)</bold> <inline-formula><inline-graphic xlink:href="613033v2_inline68.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of PFC estimated from the reward integration equation at cue onset. <inline-formula><inline-graphic xlink:href="613033v2_inline69.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="613033v2_inline70.gif" mime-subtype="gif" mimetype="image"/></inline-formula> correspond to no-reward (red) and reward trials (blue), respectively. The shaded region shows the S.E.M over blocks and sessions. <bold>(C-D)</bold> Same as in panels (A) and (B) but for trained RNNs. <bold>(E)</bold> Prediction accuracy, quantified with Pearson correlation, of the reward integration equation of all 8 PFC recording sessions and all 40 trained RNNs at cue onset. <bold>(F)</bold> Average prediction accuracy, quantified with Pearson correlation, of the reward integration equation across time. The value at each time point shows the prediction accuracy averaged over all blocks in PFC recording sessions (top) or trained RNNs (bottom).</p></caption>
<graphic xlink:href="613033v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>When the reward integration equation was fitted to other time points in the trial (i.e., estimate <inline-formula><inline-graphic xlink:href="613033v2_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula> at other <italic>t</italic> in the trial), the prediction accuracy remained stable over the trial duration (<xref rid="fig3" ref-type="fig">Fig.3F</xref>). This suggested that a line attractor model might be applicable throughout the trial. However, the reward integration equation is an algebraic relation and does not capture the dynamics of neural activity, such as the non-stationary activity during the trial (e.g., <xref rid="fig2" ref-type="fig">Fig. 2A</xref>). This observation led us to characterize the dynamics of reversal probability activity to assess whether it was compatible with the line attractor model.</p>
</sec>
<sec id="s2d">
<label>4</label>
<title>Augmented attractor model of reversal probability activity</title>
<p>We showed that the reversal probability activity encodes the accumulation of reward outcomes, which resembles a line attractor model (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). However, a direct application of the line attractor dynamics would imply that, when no decision-related evidence is presented within a trial, the reversal probability activity should remain stationary (<xref rid="fig4" ref-type="fig">Fig.4A</xref>, Stationary), which was incompatible with the non-stationary reversal probability activity observed during a trial (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Augmented model for reversal probability activity.</title>
<p><bold>(A)</bold> Schematic of two activity modes of the reversal probability activity. Left: Stationary mode (line attractor) where <italic>x</italic><sub><italic>rev</italic></sub>(<italic>t</italic>) remains constant during a trial, and non-stationary mode where <italic>x</italic><sub><italic>rev</italic></sub>(<italic>t</italic>) is dynamic. Right: Augmentation of stationary and non-stationary activity modes where the stationary mode leads the non-stationary mode in time. The time derivative <italic>dx</italic><sub><italic>rev</italic></sub><italic>/dt</italic> is shown to demonstrate (non-)stationarity of the activity. <bold>(B)</bold> Left: Block-averaged <italic>x</italic><sub><italic>rev</italic></sub><italic>/dt</italic> of PFC across trial and time. Dotted red lines indicate the onset time of fixation (−0.5s), cue (0s) and reward (0.8s); same lines shown on the right. Right: <italic>x</italic><sub><italic>rev</italic></sub><italic>/dt</italic> averaged over all trials (white), together with the trajectories of 5 trials around the reversal trial (colored). <bold>(C)</bold> Left: Contraction factor of <italic>x</italic><sub><italic>rev</italic></sub> of PFC at different time points. Dotted line at 1 indicates the threshold of contraction and expansion. Right: Contraction factor of PFC <italic>x</italic><sub><italic>rev</italic></sub> of individual trials between the time interval -2.5s and -1s. <bold>(D)</bold> Block-averaged <italic>dx</italic><sub><italic>rev</italic></sub><italic>/dt</italic> of RNNs at the pre-reversal (left) and post-reversal (right) trials. Note that the sign of the post-reversal trial trajectories was flipped to match the shape of the pre-reversal trajectories. Dotted red lines indicate the time of fixation, cue off and reward. <bold>(E)</bold> Contraction factor of <italic>x</italic><sub><italic>rev</italic></sub> of RNN. Similar results for RNN as in panel (C). <bold>(F)</bold> Generating PFC non-stationary reversal probability trajectories from the stationary activity using support vector regression (SVR) models. Top: Trajectories generated from SVR compared to the PFC reversal probability trajectories in trials around the reversal trial in an example block. The initial state (green) is the input to the SVR model, which then predicts the rest of the trajectory. The normalized mean-squared-error (MSE) between the SVR trajectory (prediction, red) and the PFC trajectory (data, black) is shown in each trial. Bottom: Trajectories generated from the null SVR compared to the PFC reversal probability trajectories. The initial states of trials in a block were shuffled randomly prior to training the null SVR model. The trajectories predicted from the null SVR model (blue) are compared to the PFC reversal probability trajectories (black). <bold>(G)</bold> The normalized MSE of all trials in the test dataset. <bold>(H)</bold> Difference between the normalized MSE of the SVR and the null models. The difference of normalized MSE between two models was calculated for each trial.</p></caption>
<graphic xlink:href="613033v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To better characterize the dynamics of reversal probability activity, we analyzed how its derivative <italic>dx</italic><sub><italic>rev</italic></sub><italic>/dt</italic> changes throughout a trial. In the PFC of monkeys, we found that the reversal probability activity was stationary at the start of a trial: <italic>dx</italic><sub><italic>rev</italic></sub><italic>/dt</italic> was close to zero before and during the fixation (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>, Stationary). However, the non-stationary activity emerged at cue onset and was associated with task-related behavior, such as making a choice and receiving reward. Specifically, <italic>dx</italic><sub><italic>rev</italic></sub><italic>/dt</italic> increased rapidly at the cue onset, when a decision was made, followed by a sharp fluctuation and slow decay until the reward time (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>, Non-stationary).</p>
<p>We then analyzed the contraction factor of the reversal probability activity to assess whether the activity is contracting or expanding around its mean activity (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>; see Methods <xref ref-type="sec" rid="s5">Section 5</xref> for details). We found that the contraction factor was less than 1 (i.e., contracting) before the fixation period (−2.5s to -1s in <xref rid="fig4" ref-type="fig">Fig. 4C</xref>, left; also see the right panel), became close to 1 (i.e., marginally stable) around fixation (−1s to 0s), and exceeded 1 (i.e., expanding) at cue onset (0s). This showed that the reversal probability activity is a point attractor (i.e., stationary and contracting) at the start of a trial but loses its stability as task-related behavior is executed.</p>
<p>In the RNNs, we found that the dynamics of reversal probability activity were similar to the PFC’s activity when an additional constraint was added to the RNN’s objective function to fix its choice output to zero before making a choice (i.e., fixation period was between fixation and cue-off lines in <xref rid="fig4" ref-type="fig">Fig. 4D</xref>; indicated as Stationary period). With the fixation constraint, the reversal probability activity was stationary during the fixation period (i.e., <italic>dx</italic><sub><italic>rev</italic></sub><italic>/dt</italic> converged and stayed close to zero). This contrasted the RNNs trained without the fixation, which exhibited more dynamics before making a choice, suggesting the role of fixation in promoting stationary dynamics (Supp. Fig. S3). The RNN’s non-stationary dynamics after cue-off was consistent with the PFC’s non-stationary activity regardless of the fixation constraint (<xref rid="fig4" ref-type="fig">Fig. 4D</xref> and <xref rid="figS3" ref-type="fig">Supp. Fig. S3</xref>, Non-stationary).</p>
<p>The contraction factor of reversal probability activity of the RNNs trained with the fixation constraint showed a trend that was similar to the PFC. It was less than 1 during fixation (fixation to cue-off in <xref rid="fig4" ref-type="fig">Fig. 4E</xref>; also see the right panel) and became close to 1 immediately after cue-off when the decision was made (the point next to cue-off in <xref rid="fig4" ref-type="fig">Fig. 4E</xref>). As in the PFC, the contraction factor shows that the RNN’s reversal probability activity is a point attractor at the start of a trial and becomes unstable as the RNN executes decision making.</p>
<p>Thus, analyzing <italic>dx</italic><sub><italic>rev</italic></sub><italic>/dt</italic> across a trial showed that the reversal probability activity consisted of two dynamic modes: a point attractor at the start of a trial, which was consistent with the line attractor model, followed by non-stationary dynamics during the trial, which deviated from the line attractor.</p>
<p>Next, we investigated whether the two activity modes are linked by common dynamics. In the RNNs, the cue applied at the end of the stationary period (<xref rid="fig4" ref-type="fig">Fig. 4D</xref>, cue-on to cue-off) determined the initial state from which the non-stationary activity was generated by the recurrent dynamics. We wondered if the two activity modes of PFC also obeyed the same dynamic relationship, i.e., the non-stationary activity is generated by underlying dynamics with an initial condition given by the stationary state.</p>
<p>To test this hypothesis, we took the PFC activity at the start of fixation period (−0.5s) as the initial state. Then, we trained a support vector regression (SVR) to infer the underlying dynamics, which used the PFC activity at fixation as the input and generated the remaining trajectory until the reward time as the output. The SVR model was trained on neural data from 20 trials around the behavioral reversal in 10 randomly selected blocks from a session and tested on the remaining blocks (approximately 10 blocks) from the same session, thus training separate SVR model for each session. This procedure was repeated 10 times to sample different sets of training blocks (see Methods <xref ref-type="sec" rid="s6">Section 6</xref> for the details). The prediction error was quantified using the normalized mean-squared-error (MSE) between the SVR prediction and the actual reversal probability trajectory normalized by the amplitude of the trajectory.</p>
<p><xref rid="fig4" ref-type="fig">Figure 4F</xref> (top) compares the SVR trajectory (red, prediction) generated from an initial state (green, initial state) and the actual PFC reversal probability trajectory (black, data) in trials around the behavioral reversal. We found that the SVR trajectories were able to capture the overall shape of data trajectories that had a bump shape and shifted up and down with the initial state. The normalized MSE of all the trials in the test dataset was 0.13, i.e., the mean error is 13% of the trajectory amplitude (<xref rid="fig4" ref-type="fig">Fig. 4G</xref>).</p>
<p>To verify the role of initial state in generating trajectories, we trained a null SVR model, in which the initial states of trials in a block were randomly shuffled before training the model. In other words, a reversal probability trajectory was generated not from its own initial state, but from the initial state of a randomly chosen trial. The rest of the training procedure was identical as described above. We found that, although the null trajectory (<xref rid="fig4" ref-type="fig">Fig. 4F</xref>, Null, prediction) resembled the overall shape of the data trajectory (<xref rid="fig4" ref-type="fig">Fig. 4F</xref>, Null, data), i.e., increase towards cue onset and then decrease monotonically, it did not shift together with the initial states (<xref rid="fig4" ref-type="fig">Fig. 4F</xref>, Null, initial state), as did the data trajectories. When the normalized MSE of the SVR and the null trajectories were compared, we found that in 86% of the test trials the SVR error was smaller than the null error (<xref rid="fig4" ref-type="fig">Fig. 4H</xref>).</p>
<p>Together, our results show that the reversal probability activity consists of two activity modes linked by underlying dynamics. The stable stationary state (i.e., point attractor) at the start of a trial determines the initial condition from which the non-stationary dynamics, associated with task-related behavior, is generated. These findings suggest an extension of the standard line attractor model, in which points on the line attractor serve as initial states that launch non-stationary activity necessary to perform a task.</p>
</sec>
<sec id="s2e">
<label>5</label>
<title>Dynamic neural trajectories encoding reversal probability are separable</title>
<p>The previous section showed that the underlying neural dynamics, associated with task-related behavior, generates non-stationary activity during a trial. We next asked how distinct probabilistic values can be encoded in this non-stationary activity. In a stationary state, different positions encode different levels of decision-related evidence. When stationary points evolve through the non-stationary dynamics, as in our augmented model, they must remain separable in order to represent distinct values.</p>
<p>To address this question, we compared trajectories of adjacent trials to examine if the reward outcome drives the next trial’s trajectory away from the current trial’s trajectory, thus separating them. Analysis of PFC activity showed that not receiving a reward increased the next trial’s trajectory<inline-formula><inline-graphic xlink:href="613033v2_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, compared to the current trial’s trajectory<inline-formula><inline-graphic xlink:href="613033v2_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, over the entire trial duration until the next trial’s reward was revealed (<xref rid="fig5" ref-type="fig">Fig.5A</xref>, left; <italic>R</italic><sub>−</sub>). This was shown in the difference of adjacent trials’ trajectories being positive values, when not rewarded through most of the trial (<xref rid="fig5" ref-type="fig">Fig.5A</xref>, right; <italic>R</italic><sub>−</sub>). Moreover, across trials, the same trend was observed in all the trials except at the behavioral reversal trial, at which the reversal probability activity reached its maximum value and decreased in the following trial (<xref rid="fig5" ref-type="fig">Fig.5B</xref>, top; <italic>R</italic><sub>−</sub>). On the other hand, when a reward was received, the next trial’s trajectory was decreased, compared to the current trial’s trajectory over the entire trial duration until the next trial’s reward (<xref rid="fig5" ref-type="fig">Fig.5A</xref>, left; <italic>R</italic><sub>+</sub>). This was shown in the difference of adjacent trials’ trajectories being negative values, when rewarded through most of the trial (<xref rid="fig5" ref-type="fig">Fig.5A</xref>, right; <italic>R</italic><sub>+</sub>). Across trials, the same trend was observed in all the trials except at the trial preceding the behavioral reversal trial, at which the trajectory increased to the maximum value at the reversal trial (<xref rid="fig5" ref-type="fig">Fig.5B</xref>, bottom; <italic>R</italic><sub>+</sub>). Additional analysis on <italic>R</italic><sub>−</sub> and <italic>R</italic><sub>+</sub> beyond the next trial’s reward time can be found in <xref rid="figS1" ref-type="fig">Supp. Figure S1</xref>.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Dynamic neural trajectories encoding reversal probability are separated in response to reward outcomes.</title>
<p><bold>(A)</bold> Left: <italic>x</italic><sub><italic>rev</italic></sub>(<italic>t</italic>) of PFC at current trial (black) is compared to <italic>x</italic><sub><italic>rev</italic></sub>(<italic>t</italic>) in the next trial when reward is received (top, red) and not received (bottom, blue). Right: The difference of <italic>x</italic><sub><italic>rev</italic></sub>(<italic>t</italic>) between current and next trials shown on the left panels. Shaded region shows the S.E.M. across all trials, blocks and sessions. <bold>(B)</bold> Difference of <italic>x</italic><sub><italic>rev</italic></sub> of two adjacent trials when reward is not received (top, <italic>R</italic><sub>−</sub>) or received (bottom, <italic>R</italic><sub>+</sub>). The approximate time of reward outcome is shown. Relative trial number indicate the trial position relative to the behavioral reversal trial. <bold>(C)</bold> Left: <italic>x</italic><sub><italic>rev</italic></sub>(<italic>t</italic>) of PFC of consecutive no reward trials before the behavioral reversal trial (top) and consecutive reward trials after the behavioral reversal (bottom). The initial value was subtracted to compare the ramping rates of <italic>x</italic><sub><italic>rev</italic></sub>(<italic>t</italic>). Right: Difference in the ramping rates of trajectories of adjacent trials, when reward was received (blue) and not received (red). <bold>(D-E)</bold> Same as the right panels in (A) and (C) but for trained RNNs. <bold>(F)</bold> Left, Middle: External (left) and recurrent (middle) inputs to the RNN reversal probability dynamics, when reward was not received (red, magenta) or was received (blue, cyan). Right: Amplification factor shows the ratio of the total input when no reward (or reward) was received to the total input of reference input. The amplification factors for both the external (red, blue) and recurrent (magenta, cyan) inputs are shown. Red and magenta curves and blue and cyan curves overlap.</p></caption>
<graphic xlink:href="613033v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We then examined what type of activity the dynamic trajectories exhibited when separating away from the previous trial’s trajectory. Ramping activity is often observed in cortical neurons of animals engaged in decision-making [<xref ref-type="bibr" rid="c23">23</xref>–<xref ref-type="bibr" rid="c27">27</xref>]. We found that when no rewards were received, trajectories were separated from the previous trial’s trajectory by increasing their ramping rates towards the decision time (<italic>dR</italic><sub>−</sub><italic>/dt &gt;</italic> 0 in <xref rid="fig5" ref-type="fig">Fig.5C</xref>, right). On the other hand, when rewards were received, trajectories were separated by decreasing their ramping rate (<italic>dR</italic><sub>+</sub><italic>/dt &lt;</italic> 0 in <xref rid="fig5" ref-type="fig">Fig.5C</xref>, right). The increase (or decrease) in the ramping rates was observed in consecutive no reward (or reward) trials around the reversal trial (<xref rid="fig5" ref-type="fig">Fig.5C</xref>, left).</p>
<p>Consistent with the PFC activity, the trained RNN exhibited similar activity responses to reward outcomes: neural trajectories encoding reversal probability increased, when not rewarded, and decreased, when rewarded. The shift in trajectories persisted throughout the trial duration (<xref rid="fig5" ref-type="fig">Fig.5D</xref>) and ramping rates changed in agreement with the PFC findings (<xref rid="fig5" ref-type="fig">Fig.5E</xref>).</p>
<p>We examined the circuit dynamic motif of the trained RNN that separates neural trajectories. We projected the differential equation governing the network dynamics onto a one-dimensional subspace encoding reversal probability and analyzed the contribution of external and recurrent inputs to reversal probability dynamics: <inline-formula><inline-graphic xlink:href="613033v2_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (see Methods <xref ref-type="sec" rid="s1">Section 1</xref> for details). We found that the effect of the external input <italic>x</italic><sub><italic>ext</italic></sub> was positive, reflecting that the external feedback input drives the reversal probability. On the other hand, the recurrent input <italic>x</italic><sub><italic>rec</italic></sub> was negative, showing that it curtailed the external input (<xref rid="fig5" ref-type="fig">Fig.5F</xref>, external and recurrent). To analyze the effects of no-reward (or reward), we averaged the reversal probability activity over all trials (<xref rid="fig5" ref-type="fig">Fig.5F</xref>, reference) at which the subsequent trial was not (or was) rewarded. When no reward was received, <italic>x</italic><sub><italic>ext</italic></sub> (<xref rid="fig5" ref-type="fig">Fig.5F</xref>, external, red) and <italic>x</italic><sub><italic>rec</italic></sub> (<xref rid="fig5" ref-type="fig">Fig.5F</xref>, recurrent, magenta) were both amplified, compared to the reference, by approximately the same factor and resulted in increased total input: <inline-formula><inline-graphic xlink:href="613033v2_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula> with <italic>γ</italic><sup><italic>no rew</italic></sup> <italic>&gt;</italic> 1 (<xref rid="fig5" ref-type="fig">Fig.5F</xref>, amplification, red and magenta). On the other hand, when reward was received, <italic>x</italic><sub><italic>ext</italic></sub> (<xref rid="fig5" ref-type="fig">Fig.5F</xref>, external, blue) and <italic>x</italic><sub><italic>rec</italic></sub> (<xref rid="fig5" ref-type="fig">Fig.5F</xref>, recurrent, cyan) were both suppressed, resulting in decreased total input: <inline-formula><inline-graphic xlink:href="613033v2_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula> with <italic>γ</italic><sup><italic>reward</italic></sup> <italic>&lt;</italic> 1 (<xref rid="fig5" ref-type="fig">Fig.5F</xref>, amplification, blue and cyan). This suggests a circuit dynamic motif, where positive external feedback balanced by recurrent inhibition drives the reversal probability dynamics. The total drive is amplified or suppressed, depending on reward outcomes, resulting in a trajectory that separates from the previous trial’s trajectory.</p>
<p>In sum, our findings show that dynamic neural trajectories encoding reversal probability are separated from the previous trial’s trajectory in response to reward outcomes, allowing them to represent distinct values of reversal probability during a trial.</p>
</sec>
<sec id="s2f">
<label>6</label>
<title>Separability of reversal probability trajectories across trials</title>
<p>We investigated if reversal probability trajectories across multiple trials maintained separability. To this end, we analyzed the mean behavior of trajectories in each trial (referred to as mean trajectory of a trial) and analyzed their separability across trials around the behavioral reversal. Since a mean trajectory of a trial was obtained by averaging over all reward outcomes of the previous trial, we compared how reward and no-reward contributed to modifying the next trial’s mean trajectory, which were quantified by <inline-formula><inline-graphic xlink:href="613033v2_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="613033v2_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in <xref rid="fig5" ref-type="fig">Figure 5B</xref>, respectively.</p>
<p>We found that the effect of no-reward was larger than the effect of reward before the behavioral reversal. This is shown as the trace <inline-formula><inline-graphic xlink:href="613033v2_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (no reward) lying above the trace <inline-formula><inline-graphic xlink:href="613033v2_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (reward) during a trial and across all pre-reversal trials (see pre-reversal trials −5 to −1 in <xref rid="fig6" ref-type="fig">Fig.6A</xref>; <inline-formula><inline-graphic xlink:href="613033v2_inline15.gif" mime-subtype="gif" mimetype="image"/></inline-formula>and <inline-formula><inline-graphic xlink:href="613033v2_inline16.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are positive traces since <inline-formula><inline-graphic xlink:href="613033v2_inline17.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is mostly negative). The temporal averages of <inline-formula><inline-graphic xlink:href="613033v2_inline18.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="613033v2_inline19.gif" mime-subtype="gif" mimetype="image"/></inline-formula> captured this systematic differences in the pre-reversal trials (<xref rid="fig6" ref-type="fig">Fig.6B</xref>, bottom), and the sum <inline-formula><inline-graphic xlink:href="613033v2_inline20.gif" mime-subtype="gif" mimetype="image"/></inline-formula> was positive during a trial in 80% of the pre-reversal trials (<xref rid="fig6" ref-type="fig">Fig.6C</xref>, top). The positivity of <inline-formula><inline-graphic xlink:href="613033v2_inline21.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in time and across pre-reversal trials implied that the mean trajectories remained separable by increasing monotonically across the trials (<xref rid="fig6" ref-type="fig">Fig.6C</xref>, bottom). Consistent with this observation, the Spearman rank correlation of pre-reversal trajectories was stable in time (<xref rid="fig6" ref-type="fig">Fig.6E</xref>, pre).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Mean trajectories encoding reversal probability shift monotonically across trials.</title>
<p><bold>(A)</bold> Traces of <inline-formula><inline-graphic xlink:href="613033v2_inline71.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="613033v2_inline72.gif" mime-subtype="gif" mimetype="image"/></inline-formula> around the behavioral reversal trial. Note the sign flip in <inline-formula><inline-graphic xlink:href="613033v2_inline73.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, which was introduced to compare the magnitudes of<inline-formula><inline-graphic xlink:href="613033v2_inline74.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. and <inline-formula><inline-graphic xlink:href="613033v2_inline75.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Relative trial number indicate the trial position relative to the behavioral reversal trial. <bold>(B)</bold> Top: <inline-formula><inline-graphic xlink:href="613033v2_inline76.gif" mime-subtype="gif" mimetype="image"/></inline-formula> across trial and time. Bottom: Temporal averages of <inline-formula><inline-graphic xlink:href="613033v2_inline77.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and<inline-formula><inline-graphic xlink:href="613033v2_inline78.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. over the trial duration. <bold>(C)</bold> Top: Traces of <inline-formula><inline-graphic xlink:href="613033v2_inline79.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of pre-reversal trials (relative trial k = − 5 to − 1), and the fraction of trials at each time point that satisfy<inline-formula><inline-graphic xlink:href="613033v2_inline81.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Bottom: Mean PFC reversal probability trajectories of pre-reversal trials. <bold>(D)</bold> Same as in panel (C), but for post-reversal trials (relative trial k = 0 to 4). <bold>(E)</bold> Spearman rank correlation between trial numbers and the mean PFC reversal probability trajectories across pre-reversal (red) and post-reversal (blue) trials at each time point. For the post-reversal trials, Spearman rank correlation was calculated with the trial numbers in reversed order to capture the descending order. <bold>(F)</bold> <inline-formula><inline-graphic xlink:href="613033v2_inline82.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of trained RNNs across trial and time. (G-I) Trained RNNs’ block-averaged xrev before and after the reversal trial and their average Spearman correlation at each time point.</p></caption>
<graphic xlink:href="613033v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>After the behavioral reversal, the effects of no-reward and reward were the opposite of pre-reversal trials. The trace <inline-formula><inline-graphic xlink:href="613033v2_inline22.gif" mime-subtype="gif" mimetype="image"/></inline-formula> lied above the trace <inline-formula><inline-graphic xlink:href="613033v2_inline23.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (see post-reversal trials 0 to 4 in <xref rid="fig6" ref-type="fig">Fig.6A</xref>), and the temporal average <inline-formula><inline-graphic xlink:href="613033v2_inline24.gif" mime-subtype="gif" mimetype="image"/></inline-formula> was larger than <inline-formula><inline-graphic xlink:href="613033v2_inline25.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref rid="fig6" ref-type="fig">Fig.6B</xref>, bottom). This showed that <inline-formula><inline-graphic xlink:href="613033v2_inline26.gif" mime-subtype="gif" mimetype="image"/></inline-formula> was mostly negative during post-reversal trials. The fraction of trials, for which <inline-formula><inline-graphic xlink:href="613033v2_inline27.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is negative, was close to 80% among the post-reversal trials (<xref rid="fig6" ref-type="fig">Fig.6D</xref>, top). The negativity of <inline-formula><inline-graphic xlink:href="613033v2_inline28.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in time and across post-reversal trials implied that the post-reversal trajectories remained separable by decreasing monotonically across the trials (<xref rid="fig6" ref-type="fig">Fig.6D</xref>, bottom). The order of post-reversal trajectories was stable over the trial duration, similarly to the pre-reversal trials but in the reversed order (<xref rid="fig6" ref-type="fig">Fig.6E</xref>, post).</p>
<p>In the trained RNNs, the effects of reward outcomes on mean trajectories were consistent with the PFC findings: <inline-formula><inline-graphic xlink:href="613033v2_inline29.gif" mime-subtype="gif" mimetype="image"/></inline-formula> was positive and negative before and after the behavioral reversal, respectively (<xref rid="fig6" ref-type="fig">Fig.6F</xref>). Consequently, reversal probability trajectories of the RNNs maintained separability by shifting monotonically as in the PFC (<xref rid="fig6" ref-type="fig">Figs. 6G, H</xref>). The order of trajectories was also stable over the trial duration (<xref rid="fig6" ref-type="fig">Fig.6I</xref>).</p>
<p>These findings show that the mean behavior of reversal probability trajectories is to shift monotonically across trials. It suggests that a family of separable neural trajectories around the reversal trial can represent varying estimates of reversal probability stably in time.</p>
</sec>
<sec id="s2g">
<label>7</label>
<title>Perturbing reversal probability activity biases choice outcomes</title>
<p>Here we turned to the RNN to see if we could perturb activity within the reversal probability space and consequently perturb the network’s choice preference. We defined the reversal probability activity <italic>x</italic><sub><italic>rev</italic></sub> as the activity in a neural subspace correlated to the behavioral reversal probability (Methods <xref ref-type="sec" rid="s3">Section 3</xref>). However, it remains to be shown if the activity within this neural subspace can causally affect network’s behavioral outcomes.</p>
<p>Previous experimental works demonstrated that perturbing neural activity of medial frontal cortex [<xref ref-type="bibr" rid="c28">28</xref>], specific cell types [<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>] or neuromodulators [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c6">6</xref>] affect the performance of reversal learning. In our perturbation experiments in the RNNs, the perturbation was tailored to be within the reversal probability space by applying an external stimulus aligned (<italic>v</italic><sub>+</sub>) or opposite (<italic>v</italic><sub>−</sub>) to the reversal probability vector. An external stimulus in a random direction was also applied as a control (<italic>v</italic><sub><italic>rnd</italic></sub>). All the stimuli were applied before the time of choice at the reversal trial or at preceding trials (<xref rid="fig7" ref-type="fig">Fig.7A</xref>).</p>
<p>We found that the deviation of perturbed reversal probability activity from the unperturbed activity peaked at the end of perturbation duration and decayed gradually (<xref rid="fig7" ref-type="fig">Fig.7B</xref>, red solid). The perturbed choice activity, however, deviated more slowly and peaked during the choice duration (<xref rid="fig7" ref-type="fig">Fig.7B</xref>, black solid). This showed that perturbation of the reversal probability activity had its maximal effect on the choice activity when the choice was made. The strong perturbative effects on the reversal probability and choice activity were not observed in the control (<xref rid="fig7" ref-type="fig">Fig.7B</xref>, dotted).</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7:</label>
<caption><title>Perturbing RNN’s neural activity encoding reversal probability biases choice outcomes.</title>
<p><bold>(A)</bold> RNN perturbation scheme. Three perturbation stimuli were used; <italic>v</italic><sub>+</sub>, population vector encoding the reversal probability; <italic>v</italic><sub>−</sub>, negative of <italic>v</italic><sub>+</sub>; <italic>v</italic><sub><italic>rnd</italic></sub>, control stimulus in random direction. Perturbation stimuli were applied at the reversal (0) and two preceding (−2, -1) trials. <bold>(B)</bold> Deviation of reversal probability activity Δ<italic>x</italic><sub><italic>rev</italic></sub> and choice activity Δ<italic>x</italic><sub><italic>choice</italic></sub> from the unperturbed activity. Perturbation was applied at the reversal trial during a time interval the cue was presented (shaded red). Choice was made after a short delay (shaded gray). Perturbation response along the reversal probability vector <italic>v</italic><sub>+</sub> (solid) and random vector <italic>v</italic><sub><italic>rnd</italic></sub> (dotted) are shown. <bold>(C)</bold> Perturbation of reversal probability activity (left) and choice activity (right) in response to three types of perturbation stimuli. Each dot shows the response of a perturbed network. Two perturbation strengths (multiplicative factor of 3 and 4 shown in panels D and E) were applied to 40 RNNs. Δ<italic>x</italic><sub><italic>rev</italic></sub> shows the activity averaged over the duration of perturbation, and Δ<italic>x</italic><sub><italic>choice</italic></sub> shows the averaged activity over the duration of choice. Δ<italic>x</italic><sub><italic>choice</italic></sub> of <italic>v</italic><sub>+</sub> is significantly smaller than Δ<italic>x</italic><sub><italic>choice</italic></sub> of <italic>v</italic><sub>−</sub> (KS-test, p-value = 0.007). <bold>(D-E)</bold> Fraction of blocks in all 40 trained RNNs that exhibited delayed or accelerated reversal trials in response to perturbations of the reversal probability activity. Perturbations at trial number -1 by three stimulus types are shown on the left panels, and perturbations at all three trials by the stimulus of interest (<italic>v</italic><sub>−</sub> in D and <italic>v</italic><sub>+</sub> in E) are shown on the right panels. A multiplicative factor on the perturbation stimuli is shown as stimulus strength. <bold>(F)</bold> Left: The slope of linear regression model fitted to the residual activity of reversal probability and choice. The residual activity at each trial over the time interval [0, 500]ms was used to fit the linear model. Red dot indicates the slope at trial number -1. Relative trial number indicate the trial position relative to the behavioral reversal trial. Right: Each dot is the residual activity of a block at trial number -1. Red line shows the fitted linear model, and its slope (−0.34) is shown.</p></caption>
<graphic xlink:href="613033v2_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The perturbation in the aligned (<italic>v</italic><sub>+</sub>) and opposite (<italic>v</italic><sub>−</sub>) directions shifted the reversal probability activity along the same direction as the perturbation vector, as expected (<xref rid="fig7" ref-type="fig">Fig.7C</xref>, left). On the other hand, the choice activity was increased when the perturbation was in the opposite direction <italic>v</italic><sub>−</sub>. The perturbation in the aligned direction <italic>v</italic><sub>+</sub> did not decreases the choice activity substantially, but its effect was significantly smaller than the increase seen in <italic>v</italic><sub>−</sub> perturbation (<xref rid="fig7" ref-type="fig">Fig.7C</xref>, right; KS-test, p-value = 0.007).</p>
<p>We further analyzed if perturbing within the reversal probability space could affect the choice out-comes, specifically the behavioral reversal trial. We found that the reversal trial was delayed when <italic>v</italic><sub>−</sub> stimulus was applied to reduce the reversal probability activity (<xref rid="fig7" ref-type="fig">Fig.7D</xref>, left). The effect of <italic>v</italic><sub>−</sub> stimulus increased gradually with the stimulus strength and was significantly stronger than the <italic>v</italic><sub>+</sub> or <italic>v</italic><sub><italic>rnd</italic></sub> stimuli in delaying the reversal trial. Perturbation had the strongest effect when applied to the reversal trial, while perturbations on trials preceding the reversal showed appreciable but reduced effects (<xref rid="fig7" ref-type="fig">Fig.7D</xref>, right). When the <italic>v</italic><sub>+</sub> stimulus was applied to trials preceding the reversal trial, the reversal was accelerated (<xref rid="fig7" ref-type="fig">Fig.7E</xref>, left). The effect of <italic>v</italic><sub>+</sub> stimulus also increased with stimulus strength and was significantly stronger than the <italic>v</italic><sub>−</sub> or <italic>v</italic><sub><italic>rnd</italic></sub> stimuli in accelerating the reversal trial (<xref rid="fig7" ref-type="fig">Fig.7E</xref>, right).</p>
<p>We asked if perturbation of neural activity in PFC could exhibit similar responses. In other words, does increase (or decrease) in reversal probability activity lead to decrease (or increase) in choice activity in PFC? Although PFC activity was not perturbed by external inputs, we considered the residual activity of single trials, i.e., deviation of single trial neural activity around the trial-averaged activity, to be “natural” perturbation responses. We fitted a linear model to the residual activity of reversal probability and choice and found that they were strongly negatively correlated (i.e., negative slope in <xref rid="fig7" ref-type="fig">Fig.7F</xref>)) at the trial preceding the behavioral reversal trial. This analysis demonstrated the correlation between perturbation responses of reversal probability and choice activity. However, it remains to be investigated, through perturbation experiments, whether reversal probability activity is causally linked to choice activity in PFC and, moreover, to animal’s choice outcomes.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<sec id="s3a">
<title>Reversal learning</title>
<p>Reversal learning has been a behavioral framework for investigating how the brain supports flexible behavior [<xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c6">6</xref>] and for elucidating neural mechanisms underlying mental health issues [<xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c32">32</xref>]. It has been shown that multiple brain regions (cortical [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c28">28</xref>–<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c33">33</xref>] and subcortical [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c34">34</xref>]), neuromodulators [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c6">6</xref>] and different inhibitory neuron types [<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>] are involved in reversal learning.</p>
</sec>
<sec id="s3b">
<title>Our results</title>
<p>Despite these recent advances, the dynamics of neural activity in cortical areas during a reversal learning task have not been well characterized. In this study, we investigated how reversal probability is represented in cortical neurons by analyzing neural activity in the prefrontal cortex of monkeys and recurrent neural networks performing the reversal learning task. We found that the activity in a neural subspace encoding reversal probability represented integration of reward outcomes. This reversal probability activity had two activity modes: stable stationary activity at the start of trial, followed by non-stationary activity during the trial. There was underlying dynamics, associated with task-related behavior, that generated the non-stationary activity with an initial condition given by the stationary state. The existence of two activity modes suggested an extension of the standard line attractor model where non-stationary dynamics driven by task-related behavior link the attractor states. The non-stationary trajectories were separable, allowing distinct probabilistic values to be represented in dynamic trajectories. Perturbation experiments in the RNNs demonstrated a potential causal link between reversal the probability activity and choice outcomes.</p>
</sec>
<sec id="s3c">
<title>Attractor dynamics</title>
<p>RNNs with attractor dynamics have been investigated in various contexts as a neural implementation of normative models of decision-making and evidence integration [<xref ref-type="bibr" rid="c35">35</xref>–<xref ref-type="bibr" rid="c39">39</xref>]. One perspective is to consider decision variables as discrete or continuous attractor states of an RNN. Then, the network activity converges to an attracting state as a decision is made. Biologically plausible network models [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c16">16</xref>] and neural recordings in cortical areas have been shown to exhibit discrete [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c40">40</xref>] and continuous [<xref ref-type="bibr" rid="c41">41</xref>] attractor dynamics. Another perspective, more closely related to our study, is to consider evidence integration as a movement of network state along a one-dimensional continuous attractor, as demonstrated in [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c42">42</xref>] (see also continuous attractor dynamics in spatial mapping [<xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c43">43</xref>–<xref ref-type="bibr" rid="c45">45</xref>]).</p>
<p>In most of the studies, decision-related evidence was presented without significant interruption until the decision point [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c40">40</xref>]. However, this was not the case in a reversal learning task with probabilistic rewards, as reward outcomes were revealed intermittently over multiple trials while task-related behavior must be performed within each trial. We showed that such multi-trial evidence integration promoted substantial non-stationary activity in the neural subspace encoding reversal probability. Therefore, the continuous attractor dynamics, in which the network state stays close to the attracting states, did not fully account for the observed neural dynamics. Instead, our findings suggest that separable dynamic trajectories in addition to attractor states could serve as a neural mechanism that represents accumulated evidence and accommodates non-stationary behaviors necessary to perform the task.</p>
</sec>
<sec id="s3d">
<title>Limitations</title>
<p>Our work demonstrated similarities in how reversal probability is represented in the PFC of monkeys and the RNNs. However, our approach, which compares the activity of RNNs trained on the task to the PFC activity, does not directly characterize the dynamic landscape of PFC activity. In particular, our analysis only shows that the PFC activity at the initial state of each trial can be characterized as a point attractor (<xref rid="fig4" ref-type="fig">Fig. 4C, E</xref>). Although this result is compatible with a line attractor model, it does not demonstrate whether the initial states across trials collectively form a line attractor. In order to show the existence of a line attractor, it is necessary to identify the dynamics in the region of the phase space occupied by the set of point attractors.</p>
<p>Alternative approaches that infer the latent dynamics of spiking activity of a neural population [<xref ref-type="bibr" rid="c46">46</xref>– <xref ref-type="bibr" rid="c49">49</xref>] could allow us to address this question by deriving a low-dimensional system of differential equations that generates the PFC activity. If the inferred latent dynamics could generate the two activity modes observed in our data, it would allows us to characterize different aspects of the neural dynamics, such as attractor states, the role of external inputs and the non-stationary dynamics, within a single dynamical system model.</p>
</sec>
<sec id="s3e">
<title>Related work</title>
<p>Recent studies showed that intervening behaviors, such as introducing an intruder [<xref ref-type="bibr" rid="c22">22</xref>] or accumulating reward across trials [<xref ref-type="bibr" rid="c42">42</xref>], could produce neural trajectories that deviate from and retract to a line attractor. These studies are consistent with our finding in that their neural dynamics temporarily deviated from attractor states. However, in our study, we did not thoroughly investigate the neural activity from dynamical systems perspective. It remains as future work to characterize the dynamical landscape of how the separable dynamic trajectories observed in our study are augmented to the continuous attractor model and compare it to the previous works [<xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c42">42</xref>].</p>
<p>A number of relevant studies have trained RNNs to perform various decision-making tasks [<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c50">50</xref>– 52]. In a related work [<xref ref-type="bibr" rid="c50">50</xref>], RNNs were trained to perform a change point detection task designed by the International Brain Laboratory [<xref ref-type="bibr" rid="c53">53</xref>]. They showed that trained RNNs exhibited behavior outputs consistent with an ideal Bayesian observer without explicitly learning from the Bayesian observer. This finding shows that the behavioral strategies of monkeys could emerge by simply learning to do the task, instead of directly mimicking the outputs of Bayesian observer as done in our study.</p>
<p>The trained RNN in their work [<xref ref-type="bibr" rid="c50">50</xref>] exhibited line attractor dynamics in contrast to our observation of stationary and non-stationary dynamics (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>). In another study, a line attractor-like dynamics, where the principal components of network activity moved gradually across trials, was observed in artificial agents trained to perform the reversal learning task via reinforcement learning (see <xref rid="fig1" ref-type="fig">Fig. 1</xref> in [<xref ref-type="bibr" rid="c20">20</xref>]). One possible reason for the lack of non-stationary dynamics within a trial in these other studies is that a trial consisted of only one [<xref ref-type="bibr" rid="c20">20</xref>] or a few time points [<xref ref-type="bibr" rid="c50">50</xref>], which limits the possible range of temporal dynamics RNNs can exhibit during a trial. This suggests that different setup of a task can lead to significantly different dynamics in the trained RNN. Moreover, it needs to be investigated whether such attractor dynamics are present in the neural recordings from mice performing the change point detection task.</p>
<p>The RNNs in our study were trained via supervised learning. However, in real life, animals most likely learn a reversal learning task via reinforcement learning (RL), i.e., learn the task from reward out-comes. Neuromodulators play a key role in mediating RL in the brain. In a recent study, dopamine-based RL was used to train artificial RNNs to conduct reversal learning tasks. It was shown that neural activity in RNNs and mice performing the same tasks were in good agreement [<xref ref-type="bibr" rid="c20">20</xref>]. In addition, projections of serotonin from dorsal raphe nuclei [<xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c54">54</xref>] and norepinephrine from the locus coeruleus [<xref ref-type="bibr" rid="c5">5</xref>] to the cortical areas were shown to be involved in reversal learning. Further studies with biologically plausible network models including neuromodulatory effects [<xref ref-type="bibr" rid="c55">55</xref>, <xref ref-type="bibr" rid="c56">56</xref>] or formal RL theories incorporating neuromodulators [<xref ref-type="bibr" rid="c57">57</xref>] could provide further insights into the role of neuromodulators in reversal learning.</p>
</sec>
<sec id="s4">
<title>Conclusion</title>
<p>Our findings show that, when performing a reversal learning task, a cortical circuit represent reversal probability, not only in stable stationary states as in a line attractor model, but also in dynamic neural trajectories that can accommodate non-stationary task-related behaviors necessary for the task. Such neural mechanism demonstrates the temporal flexibility of cortical computation and opens the opportunity for extending existing neural model for evidence accumulation by augmenting temporal dynamics.</p>
</sec>
</sec>
<sec id="s5">
<title>Methods</title>
<sec id="s5a">
<label>1</label>
<title>Recurrent neural network</title>
<sec id="s5a1">
<title>Network model</title>
<p>For the network model, we considered a continuous time recurrent neural network that operates in a dynamic regime relevant to cortical circuits. A strongly recurrent network with balanced excitation and inhibition has been known to capture canonical features of cortical neural activity, such as fluctuating activity and large trial-to-trial variability [<xref ref-type="bibr" rid="c58">58</xref>–<xref ref-type="bibr" rid="c60">60</xref>]. A standard network model for such balanced state contains both excitatory and inhibitory neurons, and its network dynamics have been investigated extensively [<xref ref-type="bibr" rid="c58">58</xref>, <xref ref-type="bibr" rid="c61">61</xref>, <xref ref-type="bibr" rid="c62">62</xref>].</p>
<p>In our study, we considered a network consisting of recurrently connected inhibitory neurons only. There were no excitatory neurons. Instead, constant excitatory external input was applied to inhibitory neurons to sustain the network activity. A purely inhibitory network can also operate in the balanced regime, where external excitatory input and recurrent inhibitory activity balance each other [<xref ref-type="bibr" rid="c60">60</xref>, <xref ref-type="bibr" rid="c63">63</xref>].</p>
<p>The main reason for adopting a purely inhibitory network was due to the GPU memory issue when training RNNs to perform the reversal learning task. As detailed in Methods <xref ref-type="sec" rid="s2e">Section 2.5</xref> below, the RNNs are first simulated over all the trials in a block and then backpropagation-through-time is applied to update the connection weights. Since a block consists of many trials, the long duration of a block limits the size of network that can be trained. We observed that if connection probability is <italic>p</italic> = 0.1, and the population size of each neuron type is relatively large, then the excitatory-inhibitory network causes GPU memory overflow. This led us to consider a purely inhibitory network that operates in the balanced regime. Throughout network training, the signs of synaptic weights were preserved, resulting in a trained network that had only inhibitory synaptic connections.</p>
<p>The network dynamics were governed by the following equation
<disp-formula id="eqn1">
<graphic xlink:href="613033v2_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and the network readout was
<disp-formula id="eqn2">
<graphic xlink:href="613033v2_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here, <italic>τ</italic> is the intrinsic time constant of the RNN. <bold>u</bold> ∈ ℝ<sup><italic>N</italic></sup> is the neural activity of population of <italic>N</italic> neurons. <italic>W</italic> <sup><italic>rec</italic></sup> is an <italic>N</italic> × <italic>N</italic> recurrent connectivity matrix with inhibitory synaptic weights: <inline-formula><inline-graphic xlink:href="613033v2_inline30.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for all connection from neuron <italic>j</italic> to neuron <italic>i</italic>. The activation function is sigmoidal, <italic>ϕ</italic>(<italic>x</italic>) = 1<italic>/</italic>(1 + exp[−(<italic>ax</italic> + <italic>b</italic>)]), and is applied to <bold>u</bold> elementwise in <italic>ϕ</italic>(<bold>u</bold>). The baseline input <bold>I</bold><sub><italic>base</italic></sub> is constant in time and same for all neurons, the cue <bold>I</bold><sub><italic>cue</italic></sub> is turned on to signal the RNN to make a choice, and the feedback <bold>I</bold><sub><italic>feedback</italic></sub> provides information about the previous trial’s choice and reward outcome (see <xref rid="tbl1" ref-type="table">Table 1</xref>).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>Four types of feedback inputs</title></caption>
<graphic xlink:href="613033v2_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<p>The duration of a trial was <italic>T</italic> = 500ms. The intrinsic time constant <italic>τ</italic> = 20ms was significantly shorter than the trial duration. The feedback <bold>I</bold><sub><italic>feedback</italic></sub> was applied on the time interval [0, <italic>T</italic><sub><italic>feedback</italic></sub>], and the cue <bold>I</bold><sub><italic>cue</italic></sub> was applied on the the time interval <inline-formula><inline-graphic xlink:href="613033v2_inline31.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>T</italic><sub><italic>feedback</italic></sub> = 300ms and<inline-formula><inline-graphic xlink:href="613033v2_inline32.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The feedback and cue overlapped during the cue period. The network choice was defined using the average of the readout <italic>z</italic> on the time interval <inline-formula><inline-graphic xlink:href="613033v2_inline33.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <inline-formula><inline-graphic xlink:href="613033v2_inline34.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="613033v2_inline35.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (see <xref rid="fig1" ref-type="fig">Fig. 1A</xref>). The feedback <bold>I</bold><sub><italic>feedback</italic></sub> and cue <bold>I</bold><sub><italic>cue</italic></sub> were random vectors where each element was sample from Gaussian distribution with mean zero and standard deviation 0.9.</p>
</sec>
<sec id="s5a2">
<title>Reduced model</title>
<p>One-dimensional reduction of the network dynamics in a subspace defined by a task vector, <bold>v</bold>, was derived as follows (see <xref rid="fig5" ref-type="fig">Fig.5</xref>). The projection of network activity onto the task vector was
<disp-formula id="eqn3">
<graphic xlink:href="613033v2_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Then, the dynamics of the projected activity is governed by
<disp-formula id="eqn4">
<graphic xlink:href="613033v2_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where
<disp-formula id="eqn5">
<graphic xlink:href="613033v2_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn6">
<graphic xlink:href="613033v2_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here <italic>x</italic><sub><italic>rec</italic></sub> includes both the decay and recurrent terms, and <italic>x</italic><sub><italic>ext</italic></sub> accounts for all external inputs <bold>I</bold> = <bold>I</bold><sub><italic>base</italic></sub> + <bold>I</bold><sub><italic>cue</italic></sub> + <bold>I</bold><sub><italic>feedback</italic></sub>.</p>
</sec>
</sec>
<sec id="s5b">
<label>2</label>
<title>Reversal learning task</title>
<sec id="s5b1">
<label>2.1</label>
<title>Experiment setup for monkeys</title>
<p>The experimental setup for the animals was reported in a previous work [<xref ref-type="bibr" rid="c4">4</xref>]. Here, we provide a summary of the behavioral task and neural recordings.</p>
<p>Two animals performed the reversal learning task in blocks of trials over 8 sessions. In each trial, the animals were required to fixate centrally for a variable time (400 - 800ms), and, subsequently, two cues were presented to the left and right of fixation dot. The animals made a saccade to select a target, hold sight for 500ms and reward for the choice was delivered stochastically. In What blocks, one image was designated as the high-value option, while the other image was designated as the low-value option at the beginning of a block, regardless of the location of the images. The high-value option was rewarded 70% of the time when chosen, and the low-value option was rewarded 30% of the time when chosen. In Where blocks, one location (e.g., left) was designated as the high-value option, while the other location (e.g., right) was designated as the low-value option, regardless of the actual images at those locations. Each block consisted of 80 trials. The reward probability of two options was switched at a random trial, within 20 trials centered around the mid-trial of a block. The animals explored available option to identify the block types and best options. The animals’ choice (i.e., object location and identity) and the reward outcome (i.e., rewarded or not rewarded) in all the trials were recorded for further analysis.</p>
<p>The extracellular activity of neural populations was recorded in the dorso-lateral prefrontal cortex from both hemi-spheres, using eight multielectrode arrays while the monkeys performed the task. The size of neuronal populations had a range of 573 to 1,023 with a median 706. The recorded neurons were evenly distributed across left and right hemi-spheres. To analyze the spiking activity <italic>y</italic><sub><italic>it</italic></sub>(<italic>k</italic>) of neuron <italic>i</italic> at time <italic>t</italic> and trial <italic>k</italic>, a 300ms-wide time window centered at time <italic>t</italic> was slided forward in time with 20ms increment, as the number of spikes neuron <italic>i</italic> emitted in each time window was counted.</p>
</sec>
<sec id="s5b2">
<label>2.2</label>
<title>Training setup for RNNs</title>
<sec id="s5b2a">
<title>Overview</title>
<p>To train the network, we used a block consisting of <italic>T</italic> = 24 trials. For testing the trained RNNs, as described in the main text, we expanded the number of trials in a block to 36 trials (see <xref rid="fig1" ref-type="fig">Fig. 1</xref> for an example block). The reversal trial <italic>r</italic> was sampled randomly and uniformly from 10 trials around the midtrial:.
<disp-formula id="ueqn1">
<graphic xlink:href="613033v2_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The network made a choice in each trial: A or B. To model which choice was rewarded, we generated a “rewarded” choice for each trial. One of the choices was more likely to be rewarded than the other. The network’s choice was compared to the rewarded choice, and the network received a feedback that signaled its choice and reward outcome (e.g., chose A and received a reward). The option that yielded higher reward prior to the reversal trial was switched to the other option at the reversal trial.</p>
<p>To train the network to reverse its preferred choice, we used the output of an ideal Bayesian observer model as teaching signal. Specifically, we first inferred the scheduled reversal trial (i.e., the trial at which reward probability switched) using the Bayesian model. Then, the network was trained to flip its preferred choice a few trials after the inferred scheduled reversal trial, such that network’s behavioral reversal trial occurred a few trials after the scheduled reversal trial.</p>
<p>Note that, although we refer to “rewarded” choices, there were no actual rewards in our network model. The “rewarded” choices were set up to define feedback inputs that mimic the reward outcomes monkey received.</p>
</sec>
</sec>
<sec id="s5b3">
<label>2.3</label>
<title>Experiment variables</title>
<p>The important variables for training the RNN were network choice, rewarded choice and feedback. <italic>Network choice</italic>. To define network choice, we symmetrized the readout <bold>z</bold><sup><italic>sym</italic></sup> = (<italic>z</italic>, −<italic>z</italic>) and computed its log-softmax <inline-formula><inline-graphic xlink:href="613033v2_inline36.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>s</italic> = <italic>e</italic><sup><italic>z</italic></sup> + <italic>e</italic><sup>−<italic>z</italic></sup>. The network choice was
<disp-formula id="eqn7">
<graphic xlink:href="613033v2_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where
<disp-formula id="eqn8">
<graphic xlink:href="613033v2_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<sec id="s5b3a">
<title>Rewarded choice</title>
<p>To model stochastic rewards, rewarded choices were generated probabilistically for each trial <italic>k</italic>:
<disp-formula id="eqn9">
<graphic xlink:href="613033v2_eqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The reversal of reward schedule was implemented by switching the target probability at the scheduled reversal trial of the block, denoted by <italic>r</italic><sub><italic>sch</italic></sub>.
<disp-formula id="eqn10">
<graphic xlink:href="613033v2_eqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s5b3b">
<title>Feedback</title>
<p>We considered that reward is delivered when the network choice agreed with the rewarded choice, and no reward is delivered when they disagreed. This led to four types of feedback inputs show in <xref rid="tbl1" ref-type="table">Table 1</xref>.</p>
</sec>
</sec>
<sec id="s5b4">
<label>2.4</label>
<title>Bayesian inference model</title>
<p>Here we formulate Bayesian models that infer the scheduled reversal trial or the behavior reversal trial.</p>
<sec id="s5b4a">
<title>Ideal observer model</title>
<p>The ideal observer model, developed previously [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c4">4</xref>], inferred the scheduled reversal trial and assumed that (a) the target probability was known (<xref ref-type="disp-formula" rid="eqn9">Eq. 9</xref>) and (b) it switched at the reversal trial (<xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref>).</p>
<p>The data available to the ideal observer were the choice <italic>y</italic><sub><italic>k</italic></sub> ∈ {<italic>A, B</italic>} and the reward outcome <italic>z</italic><sub><italic>k</italic></sub> ∈ {0, 1} at all the trials <italic>k</italic> ∈ [<xref ref-type="bibr" rid="c1">1</xref>, <italic>T</italic>]. We inferred the posterior distribution of scheduled reversal at trials <italic>k</italic> ∈ [1, <italic>T</italic>]. By Bayes’ rule
<disp-formula id="eqn11">
<graphic xlink:href="613033v2_eqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We evaluated the posterior distribution of <italic>r</italic> when data were available up to any trial <italic>t</italic> ≤ <italic>T</italic>. The likelihood function <italic>f</italic><sub><italic>IO</italic></sub>(<italic>r</italic>) = <italic>p</italic>(<italic>y</italic><sub>1:<italic>t</italic></sub>, <italic>z</italic><sub>1:<italic>t</italic></sub>|<italic>r</italic>) of the ideal observer was defined by
<disp-formula id="ueqn2">
<graphic xlink:href="613033v2_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
For <italic>k &lt; r</italic>,
<disp-formula id="eqn12">
<graphic xlink:href="613033v2_eqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn13">
<graphic xlink:href="613033v2_eqn13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn14">
<graphic xlink:href="613033v2_eqn14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn15">
<graphic xlink:href="613033v2_eqn15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
For <italic>k</italic> ≥ <italic>r</italic>,
<disp-formula id="eqn16">
<graphic xlink:href="613033v2_eqn16.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn17">
<graphic xlink:href="613033v2_eqn17.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn18">
<graphic xlink:href="613033v2_eqn18.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn19">
<graphic xlink:href="613033v2_eqn19.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
To obtain the posterior distribution of <italic>r</italic> (<xref ref-type="disp-formula" rid="eqn11">Eq. 11</xref>), the likelihood function <italic>f</italic><sub><italic>IO</italic></sub>(<italic>r</italic>) was evaluated for all <italic>r</italic> ∈ [<xref ref-type="bibr" rid="c1">1</xref>, <italic>t</italic>], assuming flat prior <italic>p</italic>(<italic>r</italic>) and normalizing by the choice and reward data <italic>p</italic>(<italic>y</italic><sub>1:<italic>t</italic></sub>, <italic>z</italic><sub>1:<italic>t</italic></sub>).</p>
</sec>
<sec id="s5b4b">
<title>Behavioral model</title>
<p>To infer the trial at which choice reversed, i.e., behavior reversal, we used a likeli-hood function that assumed the preferred choice probability switched at the behavior reversal. Here, the reward schedule was not known.</p>
<p>The data available to the behavioral model were the choice <italic>y</italic><sub><italic>k</italic></sub> ∈ {<italic>A, B</italic>} at all the trials <italic>k</italic> ∈ [<xref ref-type="bibr" rid="c1">1</xref>, <italic>T</italic>]. We inferred the posterior distribution of behavior reversal at trials <italic>k</italic> ∈ [1, <italic>T</italic>]. By Bayes’ rule
<disp-formula id="eqn20">
<graphic xlink:href="613033v2_eqn20.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The likelihood function for the behavioral model was
<disp-formula id="ueqn3">
<graphic xlink:href="613033v2_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
For <italic>k &lt; r</italic>,
<disp-formula id="eqn21">
<graphic xlink:href="613033v2_eqn21.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn22">
<graphic xlink:href="613033v2_eqn22.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
For <italic>k</italic> ≥ <italic>r</italic>,
<disp-formula id="eqn23">
<graphic xlink:href="613033v2_eqn23.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn24">
<graphic xlink:href="613033v2_eqn24.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
To obtain the posterior distribution of <italic>r</italic>, we assumed flat prior <italic>p</italic>(<italic>r</italic>), as in the ideal observer, and normalized by the choice data <italic>p</italic>(<italic>y</italic><sub>1:<italic>t</italic></sub>).</p>
</sec>
</sec>
<sec id="s5b5">
<label>2.5</label>
<title>Training scheme</title>
<sec id="s5b5a">
<title>Overview</title>
<p>The ideal observer successfully inferred a scheduled reversal trial, which occurred randomly around the mid-trial. To learn to switch its preferred choice, we trained the network to learn from scheduled reversal trials inferred from the ideal observer. In other words, in a block consisting of <italic>T</italic> trials, the network choices and reward outcomes were fed into the ideal observer model to infer the randomly chosen scheduled reversal trial. Then, the network was trained to switch its preferred choice a few trials after the inferred reversal trial. This delay in the behavior reversal from the scheduled reversal was observed in monkey’s reversal behavior [<xref ref-type="bibr" rid="c4">4</xref>] and a running estimate of the Maximum a Posterior of the reversal probability (see Step 3 below). As the inferred scheduled reversal trial varied across blocks, the network learned to reverse its choice in a block-dependent manner.</p>
<p>Below we described the specific steps taken to train the network.</p>
</sec>
<sec id="s5b5b">
<title>Step 1</title>
<p>Simulate the network starting from a random initial state, apply the external inputs, i.e., cue and feedback inputs, at each trial and store the network choices and reward outcomes at all the trials in a block. The network dynamics is driven by the external inputs applied periodically over the trials.</p>
</sec>
<sec id="s5b5c">
<title>Step 2</title>
<p>Apply the ideal observer model to network’s choice and reward data to infer the scheduled reversal.</p>
</sec>
<sec id="s5b5d">
<title>Step 3</title>
<p>Identify the trial <italic>t</italic><sup>∗</sup> at which network choice should be reversed.</p>
<p>The main observation is that the running estimate of Maximum a Posterior (MAP) of the reversal probability obtained from the ideal observer model converges a few trials past the MAP estimate. In other words, let
<disp-formula id="ueqn4">
<graphic xlink:href="613033v2_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
then,
<disp-formula id="ueqn5">
<graphic xlink:href="613033v2_ueqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the convergence occurs around
<disp-formula id="ueqn6">
<graphic xlink:href="613033v2_ueqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
This observation can be interpreted as follows. If a subject performing the reversal learning task employs the ideal observer model to detect the trial at which reward schedule is reversed, the subject can infer the reversal of reward schedule a few trials past the actual reversal and then switch its preferred choice. This delay in behavioral reversal, relative to the reversal of reward schedule, is consistent with the monkeys switching their preferred choice a few trials after the reversal of reward schedule [<xref ref-type="bibr" rid="c4">4</xref>].</p>
</sec>
<sec id="s5b5e">
<title>Step 4</title>
<p>Construct the choice sequences the network will learn.</p>
<p>We used the observation from Step 3 to define target choice outputs that switch abruptly a few trials after the reversal of reward schedule, denoted as <italic>t</italic><sup>∗</sup> in the following. In each block, the high-value option at the start of a trial was selected randomly between two options. If a block had <italic>A</italic> as its initial high-value option, the target choice outputs were
<disp-formula id="ueqn7">
<graphic xlink:href="613033v2_ueqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
On the other hand, if a block had <italic>B</italic> as its initial high-value option, the target choice outputs were
<disp-formula id="ueqn8">
<graphic xlink:href="613033v2_ueqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
An example of target choice outputs with <italic>A</italic> as its initial high-value option is shown in <xref rid="fig1" ref-type="fig">Fig. 1B</xref>.</p>
</sec>
<sec id="s5b5f">
<title>Step 5</title>
<p>Define the loss function of a block.
<disp-formula id="ueqn9">
<graphic xlink:href="613033v2_ueqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s5b5g">
<title>Step 6</title>
<p>Train the recurrent connectivity weights <italic>W</italic> <sup><italic>rec</italic></sup> and the readout weights <bold>w</bold><sup><italic>out</italic></sup> with backpropagation using Adam optimizer with learning rate 10<sup>−2</sup>. The learning rate was decayed by a factor 0.9 every 3 epochs. The batch size (i.e., the number of networks trained) was 256. The training was continued until the fraction of rewarded trials was close to reward probability <italic>p</italic> of the preferred option.</p>
</sec>
</sec>
</sec>
<sec id="s5c">
<label>3</label>
<title>Targeted dimensionality reduction</title>
<p>Targeted dimensionality reduction (TDR) identifies population vectors that encode task variables explicitly or implicitly utilized in the experiment the subject or RNN performs [<xref ref-type="bibr" rid="c21">21</xref>]. In this study, we were interested in identifying population vectors that encode choice preference and reversal probability. Once those task vectors were identified, we analyzed the neural activity projected to those vectors to investigate neural representation of task variables.</p>
<p>We describe how TDR was performed in our study (see [<xref ref-type="bibr" rid="c21">21</xref>] for the original reference). First we regressed the neural activity of each neuron at each time point onto task variables of interest. Then we used the matrix of regression coefficients (i.e., neuron by time) to identify the task vector. Let <italic>y</italic><sub><italic>it</italic></sub>(<italic>k</italic>) be the spiking rate of neuron <italic>i</italic> at time <italic>t</italic> on trial <italic>k</italic> where we have <italic>N</italic> neurons and <italic>M</italic> time points. We regressed the spiking activity on task variables of interest <italic>z</italic><sup><italic>v</italic></sup>(<italic>k</italic>) where the task variables were <italic>v</italic> ∈ {reversal probability, choice preference, direction, object, block type, reward outcome, trial number}. For each neuron-time pair, (<italic>i, t</italic>), we performed linear regression over all trials <italic>k</italic> ∈ [0, <italic>T</italic>] with a bias:
<disp-formula id="eqn25">
<graphic xlink:href="613033v2_eqn25.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
This regression analysis yielded an <italic>N</italic> ×<italic>M</italic> coefficient matrix <inline-formula><inline-graphic xlink:href="613033v2_inline37.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for each task variable, <italic>v</italic>. We considered this coefficient matrix as a population vector evolving in time: <inline-formula><inline-graphic xlink:href="613033v2_inline38.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Then, a task vector was defined as the population vector <bold>w</bold><sup><italic>v</italic></sup> ∈ ℝ<sup><italic>N</italic></sup> at which the <italic>L</italic><sub>2</sub>-norm <inline-formula><inline-graphic xlink:href="613033v2_inline38a.gif" mime-subtype="gif" mimetype="image"/></inline-formula> achieved its maximum:
<disp-formula id="eqn26">
<graphic xlink:href="613033v2_eqn26.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We performed QR-decomposition on the matrix of task vectors <italic>W</italic> = [<bold>w</bold><sub><italic>rev</italic></sub>, <bold>w</bold><sub><italic>choice</italic></sub>, …] to orthogonalize the task vectors. Then, the population activity was projected onto each (orthogonalized) task vector to obtain the neural activity encoding each task variable:
<disp-formula id="eqn27">
<graphic xlink:href="613033v2_eqn27.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <bold>y</bold><sub><italic>t</italic></sub>(<italic>k</italic>) = (<italic>y</italic><sub>1<italic>t</italic></sub>(<italic>k</italic>), …, <italic>y</italic><sub><italic>Nt</italic></sub>(<italic>k</italic>)) is the population activity at time <italic>t</italic> on trial <italic>k</italic>.</p>
</sec>
<sec id="s5d">
<label>4</label>
<title>Reward integration equation</title>
<p>To derive the reward integration equation shown in <xref rid="fig3" ref-type="fig">Figure 3</xref>, we considered the neural activity in a subspace encoding the reversal probability:
<disp-formula id="eqn28">
<graphic xlink:href="613033v2_eqn28.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We analyzed the neural activity at the time of cue onset <italic>t</italic> = <italic>t</italic><sub><italic>on</italic></sub> and obtained a sequence of reversal probability activity across trials: <inline-formula><inline-graphic xlink:href="613033v2_inline39.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. To set up the reward integration equation
<disp-formula id="eqn29">
<graphic xlink:href="613033v2_eqn29.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
we estimated the update <inline-formula><inline-graphic xlink:href="613033v2_inline40.gif" mime-subtype="gif" mimetype="image"/></inline-formula> driven by reward outcomes at each trial <italic>k</italic>. Specifically, the update term was defined as the block-average of the difference of reversal probability activity at adjacent trials:
<disp-formula id="eqn30">
<graphic xlink:href="613033v2_eqn30.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here, <inline-formula><inline-graphic xlink:href="613033v2_inline41.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denotes all the blocks across sessions (or networks) in which reward was received at trial <italic>k</italic>. Similarly, <inline-formula><inline-graphic xlink:href="613033v2_inline42.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denotes all the blocks in which reward was not received at trial <italic>k</italic>.</p>
<p>To predict <inline-formula><inline-graphic xlink:href="613033v2_inline43.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, we set the initial value <inline-formula><inline-graphic xlink:href="613033v2_inline44.gif" mime-subtype="gif" mimetype="image"/></inline-formula> at trial 0 and sequentially predicted the following trials using <xref ref-type="disp-formula" rid="eqn29">Eq. (29)</xref> with the update term from <xref ref-type="disp-formula" rid="eqn30">Eq. (30)</xref>. The same analysis was performed at different time points <italic>t</italic>. We derived integration equations for each time and assessed its prediction accuracy as shown in <xref rid="fig3" ref-type="fig">Figure 3F</xref>.</p>
<p>To evaluate the contribution of reward and no-reward outcomes on the average responses of <inline-formula><inline-graphic xlink:href="613033v2_inline45.gif" mime-subtype="gif" mimetype="image"/></inline-formula> over blocks, we computed
<disp-formula id="eqn31">
<graphic xlink:href="613033v2_eqn31.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where
<disp-formula id="eqn32">
<graphic xlink:href="613033v2_eqn32.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <inline-formula><inline-graphic xlink:href="613033v2_inline46.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denote the fractions of reward and no-reward blocks at trial <italic>k</italic>. In <xref rid="fig5" ref-type="fig">Figure 5D</xref> and <xref rid="fig6" ref-type="fig">Figure 6A</xref>, the weighted responses, i.e., <inline-formula><inline-graphic xlink:href="613033v2_inline47.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="613033v2_inline48.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, were shown.</p>
</sec>
<sec id="s5e">
<label>5</label>
<title>Contraction factor of reversal probability activity</title>
<p>We defined a contraction factor to quantify whether the reversal probability activity <italic>x</italic><sub><italic>rev</italic></sub>(<italic>t</italic>) contracts to or diverges from its mean activity on a short time interval. The contraction factor on the <italic>n</italic><sup><italic>th</italic></sup> time interval [<italic>nL</italic>, (<italic>n</italic> + 1)<italic>L</italic>] of length <italic>L</italic> was defined to be the coefficient of a one-dimensional autoregressive equation the mean-centered reversal probability activity <inline-formula><inline-graphic xlink:href="613033v2_inline49.gif" mime-subtype="gif" mimetype="image"/></inline-formula> satisfied.
<disp-formula id="ueqn10">
<graphic xlink:href="613033v2_ueqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where
<disp-formula id="ueqn11">
<graphic xlink:href="613033v2_ueqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The contraction factor <italic>a</italic> of a time interval was estimated by performing a scalar linear regression without an intercept given the input data <inline-formula><inline-graphic xlink:href="613033v2_inline50.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and the output data <inline-formula><inline-graphic xlink:href="613033v2_inline51.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
</sec>
<sec id="s5f">
<label>6</label>
<title>Generating the PFC reversal probability trajectories from initial states</title>
<p>We investigated if the PFC reversal probability trajectories <inline-formula><inline-graphic xlink:href="613033v2_inline52.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of trial <italic>k</italic> defined on the trial duration, <italic>t</italic> ∈ [<italic>T</italic><sub>0</sub>, <italic>T</italic><sub><italic>f</italic></sub>], can be generated from their initial states (see <xref rid="fig4" ref-type="fig">Fig.4D</xref>). To test this idea, we trained support vector regression (SVR) model on the spiking activity of PFC neurons. The training data consisted of reversal probability trajectories <inline-formula><inline-graphic xlink:href="613033v2_inline53.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of 20 trials around the behavioral reversal trial in each block. About 50% of the blocks in an experiment session (specifically, 10 blocks) was randomly selected for training, and the remaining 50% of the blocks in the same session was used for testing. This procedure was repeated 10 times, training a different SVR model each time, to test models trained on different sets of blocks. For each experiment session, a different SVR model was trained.</p>
<p>To train an SVR model, reversal probability trajectory at each trial <italic>k</italic> was divided into initial state and remaining trajectory:
<disp-formula id="eqn33">
<graphic xlink:href="613033v2_eqn33.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn34">
<graphic xlink:href="613033v2_eqn34.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>T</italic><sub>0</sub> = −500ms (start of fixation) and <italic>T</italic><sub><italic>i</italic></sub> = −300ms (end of initial state). Then, an SVR model <italic>f</italic> was constructed that takes the initial state <bold>x</bold><sup><italic>k</italic></sup> and a time point <italic>s</italic> as its inputs and produces an approximation of<inline-formula><inline-graphic xlink:href="613033v2_inline54.gif" mime-subtype="gif" mimetype="image"/></inline-formula> :
<disp-formula id="eqn35">
<graphic xlink:href="613033v2_eqn35.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
As described above, we combined 20 trials around the reversal trial (<italic>k</italic> = − 10, …, 10) from 10 randomly selected blocks in a session (about 50% of blocks in a session) to train the SVR model. The radial basis function was used as a kernel, and the optimal hyper-parameters of the SVR model were selected through cross-validation. We used the Python’s sklearn package to implement SVR.</p>
<sec id="s5f1">
<title>Null model</title>
<p>To demonstrate the significance of initial states in generating the reversal probability trajectories, we trained a null SVR model using randomly shuffled initial states. To generate the training and testing data for this null model, we shuffled the initial states <inline-formula><inline-graphic xlink:href="613033v2_inline55.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of 20 trials around the reversal trial in a block, while keeping the remaining trajectories in each trial unchanged. In other words, the null SVR model was trained to generate the reversal probability trajectory of a trial using the initial state from a randomly chosen trial in the same block as the input. As described above, 50% of the blocks in a session was randomly selected for training and the remaining blocks were used for testing. This training and testing procedure was repeated 10 times.</p>
</sec>
</sec>
<sec id="s5g">
<label>7</label>
<title>Perturbation experiments</title>
<sec id="s5g1">
<title>Perturbing RNN</title>
<p>To perturb the activity of an RNN, a perturbation stimulus was applied to the network together with the cue. The perturbation duration was 50ms, identical to the cue duration (<xref rid="fig7" ref-type="fig">Fig. 7B</xref>). The perturbation stimulus was one of <italic>v</italic><sub>+</sub>, <italic>v</italic><sub>−</sub>, <italic>v</italic><sub><italic>rnd</italic></sub> where <italic>v</italic><sub>+</sub> is the reversal probability population vector derived from targeted dimensionality reduction (<xref ref-type="disp-formula" rid="eqn26">Eq. 26</xref>), <italic>v</italic><sub>−</sub> has the opposite sign of <italic>v</italic><sub>+</sub>, and <italic>v</italic><sub><italic>rnd</italic></sub> is a vector with random elements sampled from a Gaussian distribution with mean 0 and standard deviation identical to that of <italic>v</italic><sub>+</sub>. The perturbation stimulus was added to the network dynamic equation (<xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>) as one of the external inputs at either trial number −2, −1 or 0. The strength of perturbation was varied by modulating a multiplicative factor on <italic>v</italic><sub>+</sub>, <italic>v</italic><sub>−</sub>, <italic>v</italic><sub><italic>rnd</italic></sub> from 0.5 to 4.0.</p>
</sec>
<sec id="s5g2">
<title>Residual PFC activity</title>
<p>The residual PFC activity of reversal probability <inline-formula><inline-graphic xlink:href="613033v2_inline56.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and choice <inline-formula><inline-graphic xlink:href="613033v2_inline57.gif" mime-subtype="gif" mimetype="image"/></inline-formula> was defined as the deviation of their individual trial activity from their block-averaged activity at the same trial: <inline-formula><inline-graphic xlink:href="613033v2_inline58.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="613033v2_inline59.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>b</italic><sub><italic>k</italic></sub> denotes trial <italic>k</italic> in all the blocks across sessions. We analyzed the mean residual activity over the time interval [0ms, 500ms], i.e., <inline-formula><inline-graphic xlink:href="613033v2_inline60.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Then, at each trial <italic>k</italic>, we fitted a linear model to characterize the relationship between <inline-formula><inline-graphic xlink:href="613033v2_inline61.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="613033v2_inline62.gif" mime-subtype="gif" mimetype="image"/></inline-formula> by analyzing their values across blocks. <xref rid="fig7" ref-type="fig">Figure 7F</xref> shows the slopes of the linear models at each trial.</p>
</sec>
</sec>
<sec id="s5h">
<label>8</label>
<title>Decoding monkey’s behavioral reversal trial</title>
<p>The PFC activity <inline-formula><inline-graphic xlink:href="613033v2_inline63.gif" mime-subtype="gif" mimetype="image"/></inline-formula> encoding reversal probability was used to decode the behavioral reversal trial at which monkey reversed its preferred choice (see <xref rid="figS2" ref-type="fig">Supp. Fig.S2</xref>). Our analysis is similar to the Linear Discriminant Analysis (LDA) performed in a previous study [<xref ref-type="bibr" rid="c4">4</xref>] at a fixed time point. Here, we applied LDA to time points across a trial.</p>
<p>For training, 90% of the blocks were randomly selected to train the decoder and remaining 10% of the blocks were used for testing. This was repeated 20 times. Input data to LDA was <inline-formula><inline-graphic xlink:href="613033v2_inline64.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of Δ<italic>k</italic> trials around the reverse trial, i.e., <italic>k</italic> ∈ [<italic>k</italic><sub><italic>rev</italic></sub> − Δ<italic>k</italic>, …, <italic>k</italic><sub><italic>rev</italic></sub> + Δ<italic>k</italic>] with Δ<italic>k</italic> = 10. At each trial <italic>k</italic>, we took the activity vector <inline-formula><inline-graphic xlink:href="613033v2_inline65.gif" mime-subtype="gif" mimetype="image"/></inline-formula> around time <italic>t</italic><sub>0</sub> with Δ<italic>t</italic> = 160ms. The target output of LDA was a one-hot vector <bold>y</bold><sup><italic>target</italic></sup>, whose element was 1 at the reversal trial <italic>k</italic><sub><italic>rev</italic></sub> and 0 at other trials. The following input-output shows dataset of a block used for training:
<disp-formula id="eqn36">
<graphic xlink:href="613033v2_eqn36.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn37">
<graphic xlink:href="613033v2_eqn37.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here <inline-formula><inline-graphic xlink:href="613033v2_inline66.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>T</italic><sub><italic>dec</italic></sub> = 2Δ<italic>t/</italic>Δ<italic>h</italic> + 1 denotes the number of time points around <italic>t</italic><sub>0</sub> with time increment Δ<italic>h</italic> = 20ms, and the one-hot vector <inline-formula><inline-graphic xlink:href="613033v2_inline67.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>K</italic><sub><italic>dec</italic></sub> = 2Δ<italic>k</italic> + 1 denotes the number of trials around the reversal trial. As mentioned above, this analysis was repeated for time point <italic>t</italic><sub>0</sub> across a trial.</p>
</sec>
<sec id="s6">
<title>Code availability</title>
<p>The Python code for training RNNs is available in the following Github repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/chrismkkim/LearnToReverse">https://github.com/chrismkkim/LearnToReverse</ext-link></p>
</sec>
</sec>
</body>
<back>
<sec id="s7">
<title>Supplementary figures</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1:</label>
<caption><title>Break down of <italic>R</italic><sup>+</sup>, <italic>R</italic><sup>−</sup> by the reward outcomes of two consecutive trials.</title>
<p><bold>(A)</bold> <italic>R</italic><sup>+</sup> was decomposed into two components <italic>R</italic><sup>+</sup> = <italic>R</italic><sup>++</sup> + <italic>R</italic><sup>+−</sup>, where <italic>R</italic><sup>++</sup> indicates two consecutive reward trials and <italic>R</italic><sup>+−</sup> indicates a reward followed by no reward. Left: <italic>R</italic><sup>++</sup> across trial and time (top). Traces of <italic>R</italic><sup>++</sup> at individual trials and the fraction of trials whose traces are negative (bottom). Middle: Same as the left panel but for <italic>R</italic><sup>+−</sup>. Right: Same as the other panels but for <italic>R</italic><sup>+</sup>. <bold>(B)</bold> <italic>R</italic><sup>−</sup> was decomposed into two components <italic>R</italic><sup>−</sup> = <italic>R</italic><sup>−+</sup> + <italic>R</italic><sup>− −</sup>, where <italic>R</italic><sup>−+</sup> indicates no reward followed by a reward and <italic>R</italic><sup>− −</sup> indicates two consecutive no rewards. Same analysis as in panel (A) was performed.</p></caption>
<graphic xlink:href="613033v2_figS1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2:</label>
<caption><title>Decoding reward outcome and the behavioral reversal trial using neural trajectories encoding reversal probability.</title>
<p><bold>(A)</bold> Left: Decoding the reward outcome (i.e., reward or no reward) of every trial at each time point, given the difference of neural trajectories of two adjacent trials. At each time point, 300ms segment of the trajectories were used for decoding. Right: Decoding accuracy is averaged over all trials shown on the left panel. Red dotted line shows the approximate time of next trial’s reward. Gray dotted line shows the chance level performance. <bold>(B)</bold> Left: Decoding the behavioral reversal trial using neural trajectories of 20 trials around the reversal trial. Decoding error shows the position of predicted reversal trial relative to the actual reverse trial. At each time point, 300ms segment of each trajectory was used for decoding. Black shows the decoding error when single trial trajectories were used, and green shows the result when randomly chosen 5 blocks of trajectories were averaged before decoding. Gray dotted line shows the chance level performance. Right: Distance between trajectories was measured by taking the average of normalized mean-squared-error of adjacent trajectories at all trials. Each dot corresponds to a time point shown on the left panel.</p></caption>
<graphic xlink:href="613033v2_figS2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3:</label>
<caption><title>Comparison of RNNs trained with and without fixation.</title>
<p><bold>(A)</bold> RNNs trained without fixation. Right: The choice output of the RNNs oscillates. Left, Middle: The derivative of reversal probability activity <italic>dx</italic><sub><italic>rev</italic></sub><italic>/dt</italic> does not converge to 0 during the early part of a trial (start to cue-off). As the cue is turned on, <italic>dx</italic><sub><italic>rev</italic></sub><italic>/dt</italic> fluctuates with the cue. The white line shows <italic>dx</italic><sub><italic>rev</italic></sub><italic>/dt</italic> averaged over all pre-reversal (left) and post-reversal (middle) trials. <bold>(B)</bold> RNNs trained with the choice output fixed at 0 before making a choice. Specifically, during the time interval between fixation and cue-off lines shown in the left and middle panels, the choice output was trained to be fixed at 0. Right: The choice output of the RNNs is flat when they are not making choices. Left, Middle: The derivative of reversal probability activity <italic>dx</italic><sub><italic>rev</italic></sub><italic>/dt</italic> converges to 0 during the early part of a trial (fixation to cue-on). As the cue is turned on, <italic>dx</italic><sub><italic>rev</italic></sub><italic>/dt</italic> shows fluctuation milder than RNNs trained without fixation. The white line shows <italic>dx</italic><sub><italic>rev</italic></sub><italic>/dt</italic> averaged over all pre-reversal (left) and post-reversal (middle) trials.</p></caption>
<graphic xlink:href="613033v2_figS3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This research was supported by the Intramural Research Program of the National Institutes of Health: the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK) and the National Institute of Mental Health (NIMH). This work utilized the computational resources of the NIH HPC Biowulf cluster (<ext-link ext-link-type="uri" xlink:href="https://hpc.nih.gov">https://hpc.nih.gov</ext-link>).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Charles M</given-names> <surname>Butter</surname></string-name></person-group>. <article-title>Perseveration in extinction and in discrimination reversal tasks following selective frontal ablations in macaca mulatta</article-title>. <source>Physiology &amp; Behavior</source>, <volume>4</volume>(<issue>2</issue>):<fpage>163</fpage>–<lpage>171</lpage>, <year>1969</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Vincent D</given-names> <surname>Costa</surname></string-name>, <string-name><given-names>Valery L</given-names> <surname>Tran</surname></string-name>, <string-name><given-names>Janita</given-names> <surname>Turchi</surname></string-name>, and <string-name><given-names>Bruno B</given-names> <surname>Averbeck</surname></string-name></person-group>. <article-title>Reversal learning and dopamine: a bayesian perspective</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>6</issue>):<fpage>2407</fpage>–<lpage>2416</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Stephanie M</given-names> <surname>Groman</surname></string-name>, <string-name><given-names>Colby</given-names> <surname>Keistler</surname></string-name>, <string-name><given-names>Alex J</given-names> <surname>Keip</surname></string-name>, <string-name><given-names>Emma</given-names> <surname>Hammarlund</surname></string-name>, <string-name><given-names>Ralph J</given-names> <surname>DiLeone</surname></string-name>, <string-name><given-names>Christopher</given-names> <surname>Pittenger</surname></string-name>, <string-name><given-names>Daeyeol</given-names> <surname>Lee</surname></string-name>, and <string-name><given-names>Jane R</given-names> <surname>Taylor</surname></string-name></person-group>. <article-title>Orbitofrontal circuits control multiple reinforcement-learning processes</article-title>. <source>Neuron</source>, <volume>103</volume>(<issue>4</issue>):<fpage>734</fpage>–<lpage>746</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ramon</given-names> <surname>Bartolo</surname></string-name> and <string-name><given-names>Bruno B</given-names> <surname>Averbeck</surname></string-name></person-group>. <article-title>Prefrontal cortex predicts state switches during reversal learning</article-title>. <source>Neuron</source>, <volume>106</volume>(<issue>6</issue>):<fpage>1044</fpage>–<lpage>1054</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Zhixiao</given-names> <surname>Su</surname></string-name> and <string-name><given-names>Jeremiah Y</given-names> <surname>Cohen</surname></string-name></person-group>. <article-title>Two types of locus coeruleus norepinephrine neurons drive reinforcement learning</article-title>. <source>bioRxiv</source>, pages <elocation-id>2022.12.08.519670</elocation-id>, <year>2022</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Jung Ho</given-names> <surname>Hyun</surname></string-name>, <string-name><given-names>Patrick</given-names> <surname>Hannan</surname></string-name>, <string-name><given-names>Hideki</given-names> <surname>Iwamoto</surname></string-name>, <string-name><given-names>Randy D</given-names> <surname>Blakely</surname></string-name>, and <string-name><given-names>Hyung-Bae</given-names> <surname>Kwon</surname></string-name></person-group>. <article-title>Serotonin in the orbitofrontal cortex enhances cognitive flexibility</article-title>. <source>bioRxiv</source>, pages <elocation-id>2023.03.09.531880.</elocation-id> <year>2023</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Xiao-Jing</given-names> <surname>Wang</surname></string-name></person-group>. <article-title>Probabilistic decision making by slow reverberation in cortical circuits</article-title>. <source>Neuron</source>, <volume>36</volume>(<issue>5</issue>):<fpage>955</fpage>–<lpage>968</lpage>, <year>2002</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Valerio</given-names> <surname>Mante</surname></string-name>, <string-name><given-names>David</given-names> <surname>Sussillo</surname></string-name>, <string-name><given-names>Krishna V</given-names> <surname>Shenoy</surname></string-name>, and <string-name><given-names>William T</given-names> <surname>Newsome</surname></string-name></person-group>. <article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title>. <source>nature</source>, <volume>503</volume>(<issue>7474</issue>):<fpage>78</fpage>–<lpage>84</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Hidehiko K</given-names> <surname>Inagaki</surname></string-name>, <string-name><given-names>Lorenzo</given-names> <surname>Fontolan</surname></string-name>, <string-name><given-names>Sandro</given-names> <surname>Romani</surname></string-name>, and <string-name><given-names>Karel</given-names> <surname>Svoboda</surname></string-name></person-group>. <article-title>Discrete attractor dynamics underlies persistent activity in the frontal cortex</article-title>. <source>Nature</source>, <volume>566</volume>(<issue>7743</issue>):<fpage>212</fpage>–<lpage>217</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Thomas Zhihao</given-names> <surname>Luo</surname></string-name>, <string-name><given-names>Timothy Doyeon</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>Diksha</given-names> <surname>Gupta</surname></string-name>, <string-name><given-names>Adrian G</given-names> <surname>Bondy</surname></string-name>, <string-name><given-names>Charles D</given-names> <surname>Kopec</surname></string-name>, <string-name><given-names>Verity A</given-names> <surname>Elliot</surname></string-name>, <string-name><given-names>Brian</given-names> <surname>DePasquale</surname></string-name>, and <string-name><given-names>Carlos D</given-names> <surname>Brody</surname></string-name></person-group>. <article-title>Transitions in dynamical regime and neural mode underlie perceptual decision-making</article-title>. <source>bioRxiv</source>, pages <elocation-id>2023.10.15.562427</elocation-id>, <year>2023</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Richard S</given-names> <surname>Sutton</surname></string-name></person-group>. <article-title>Learning to predict by the methods of temporal differences</article-title>. <source>Machine learning</source>, <volume>3</volume>:<fpage>9</fpage>–<lpage>44</lpage>, <year>1988</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Rescorla</given-names> <surname>Ra</surname></string-name></person-group>. <chapter-title>A theory of pavlovian conditioning: variations in the effectiveness of reinforcement and nonreinforcement</chapter-title>. <source>Classsical conditioning II: Current research and theory</source>, <publisher-name>Appleton-Century-Crofts</publisher-name> pages <fpage>64</fpage>–<lpage>99</lpage>, <year>1972</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Bruno B</given-names> <surname>Averbeck</surname></string-name></person-group>. <article-title>Amygdala and ventral striatum population codes implement multiple learning rates for reinforcement learning</article-title>. In <conf-name>2017 IEEE Symposium Series on Computational Intelligence (SSCI)</conf-name>, pages <fpage>1</fpage>–<lpage>5</lpage>. <publisher-name>IEEE</publisher-name>, <year>2017</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Anthony I</given-names> <surname>Jang</surname></string-name>, <string-name><given-names>Vincent D</given-names> <surname>Costa</surname></string-name>, <string-name><given-names>Peter H</given-names> <surname>Rudebeck</surname></string-name>, <string-name><given-names>Yogita</given-names> <surname>Chudasama</surname></string-name>, <string-name><given-names>Elisabeth A</given-names> <surname>Murray</surname></string-name>, and <string-name><given-names>Bruno B</given-names> <surname>Averbeck</surname></string-name></person-group>. <article-title>The role of frontal cortical and medial-temporal lobe brain areas in learning a bayesian prior belief on reversals</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>33</issue>):<fpage>11751</fpage>–<lpage>11760</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Robert C</given-names> <surname>Wilson</surname></string-name>, <string-name><given-names>Matthew R</given-names> <surname>Nassar</surname></string-name>, and <string-name><given-names>Joshua I</given-names> <surname>Gold</surname></string-name></person-group>. <article-title>Bayesian online learning of the hazard rate in change-point problems</article-title>. <source>Neural computation</source>, <volume>22</volume>(<issue>9</issue>):<fpage>2452</fpage>–<lpage>2476</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Kong-Fatt</given-names> <surname>Wong</surname></string-name> and <string-name><given-names>Xiao-Jing</given-names> <surname>Wang</surname></string-name></person-group>. <article-title>A recurrent network mechanism of time integration in perceptual decisions</article-title>. <source>Journal of Neuroscience</source>, <volume>26</volume>(<issue>4</issue>):<fpage>1314</fpage>–<lpage>1328</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H Sebastian</given-names> <surname>Seung</surname></string-name></person-group>. <article-title>How the brain keeps the eyes still</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>93</volume>(<issue>23</issue>):<fpage>13339</fpage>–<lpage>13344</lpage>, <year>1996</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Anil</given-names> <surname>Bollimunta</surname></string-name>, <string-name><given-names>Douglas</given-names> <surname>Totten</surname></string-name>, and <string-name><given-names>Jochen</given-names> <surname>Ditterich</surname></string-name></person-group>. <article-title>Neural dynamics of choice: single-trial analysis of decision-related activity in parietal cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>32</volume>(<issue>37</issue>):<fpage>12684</fpage>– <lpage>12701</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Carlos D</given-names> <surname>Brody</surname></string-name> and <string-name><given-names>Timothy D</given-names> <surname>Hanks</surname></string-name></person-group>. <article-title>Neural underpinnings of the evidence accumulator</article-title>. <source>Current opinion in neurobiology</source>, <volume>37</volume>:<fpage>149</fpage>–<lpage>157</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jane X</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Zeb</given-names> <surname>Kurth-Nelson</surname></string-name>, <string-name><given-names>Dharshan</given-names> <surname>Kumaran</surname></string-name>, <string-name><given-names>Dhruva</given-names> <surname>Tirumala</surname></string-name>, <string-name><given-names>Hubert</given-names> <surname>Soyer</surname></string-name>, <string-name><given-names>Joel Z</given-names> <surname>Leibo</surname></string-name>, <string-name><given-names>Demis</given-names> <surname>Hassabis</surname></string-name>, and <string-name><given-names>Matthew</given-names> <surname>Botvinick</surname></string-name></person-group>. <article-title>Prefrontal cortex as a meta-reinforcement learning system</article-title>. <source>Nature neuroscience</source>, <volume>21</volume>(<issue>6</issue>):<fpage>860</fpage>–<lpage>868</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Valerio</given-names> <surname>Mante</surname></string-name>, <string-name><given-names>David</given-names> <surname>Sussillo</surname></string-name>, <string-name><given-names>Krishna V</given-names> <surname>Shenoy</surname></string-name>, and <string-name><given-names>William T</given-names> <surname>Newsome</surname></string-name></person-group>. <article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title>. <source>nature</source>, <volume>503</volume>(<issue>7474</issue>):<fpage>78</fpage>–<lpage>84</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Aditya</given-names> <surname>Nair</surname></string-name>, <string-name><given-names>Tomomi</given-names> <surname>Karigo</surname></string-name>, <string-name><given-names>Bin</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>Surya</given-names> <surname>Ganguli</surname></string-name>, <string-name><given-names>Mark J</given-names> <surname>Schnitzer</surname></string-name>, <string-name><given-names>Scott W</given-names> <surname>Linderman</surname></string-name>, <string-name><given-names>David J</given-names> <surname>Anderson</surname></string-name>, and <string-name><given-names>Ann</given-names> <surname>Kennedy</surname></string-name></person-group>. <article-title>An approximate line attractor in the hypothalamus encodes an aggressive state</article-title>. <source>Cell</source>, <volume>186</volume>(<issue>1</issue>):<fpage>178</fpage>–<lpage>193</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nuo</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Tsai-Wen</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Zengcai V</given-names> <surname>Guo</surname></string-name>, <string-name><given-names>Charles R</given-names> <surname>Gerfen</surname></string-name>, and <string-name><given-names>Karel</given-names> <surname>Svoboda</surname></string-name></person-group>. <article-title>A motor cortex circuit for motor planning and movement</article-title>. <source>Nature</source>, <volume>519</volume>(<issue>7541</issue>):<fpage>51</fpage>–<lpage>56</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nuo</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Kayvon</given-names> <surname>Daie</surname></string-name>, <string-name><given-names>Karel</given-names> <surname>Svoboda</surname></string-name>, and <string-name><given-names>Shaul</given-names> <surname>Druckmann</surname></string-name></person-group>. <article-title>Robust neuronal dynamics in premotor cortex during motor planning</article-title>. <source>Nature</source>, <volume>532</volume>(<issue>7600</issue>):<fpage>459</fpage>–<lpage>464</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Arseny</given-names> <surname>Finkelstein</surname></string-name>, <string-name><given-names>Lorenzo</given-names> <surname>Fontolan</surname></string-name>, <string-name><given-names>Michael N</given-names> <surname>Economo</surname></string-name>, <string-name><given-names>Nuo</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Sandro</given-names> <surname>Romani</surname></string-name>, and <string-name><given-names>Karel</given-names> <surname>Svoboda</surname></string-name></person-group>. <article-title>Attractor dynamics gate cortical information flow during decision-making</article-title>. <source>Nature neuroscience</source>, <volume>24</volume>(<issue>6</issue>):<fpage>843</fpage>–<lpage>850</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Kenneth W</given-names> <surname>Latimer</surname></string-name>, <string-name><given-names>Jacob L</given-names> <surname>Yates</surname></string-name>, <string-name><given-names>Miriam LR</given-names> <surname>Meister</surname></string-name>, <string-name><given-names>Alexander C</given-names> <surname>Huk</surname></string-name>, and <string-name><given-names>Jonathan W</given-names> <surname>Pillow</surname></string-name></person-group>. <article-title>Single-trial spike trains in parietal cortex reveal discrete steps during decision-making</article-title>. <source>Science</source>, <volume>349</volume>(<issue>6244</issue>):<fpage>184</fpage>–<lpage>187</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>David M</given-names> <surname>Zoltowski</surname></string-name>, <string-name><given-names>Kenneth W</given-names> <surname>Latimer</surname></string-name>, <string-name><given-names>Jacob L</given-names> <surname>Yates</surname></string-name>, <string-name><given-names>Alexander C</given-names> <surname>Huk</surname></string-name>, and <string-name><given-names>Jonathan W</given-names> <surname>Pillow</surname></string-name></person-group>. <article-title>Discrete stepping and nonlinear ramping dynamics underlie spiking responses of lip neurons during decision-making</article-title>. <source>Neuron</source>, <volume>102</volume>(<issue>6</issue>):<fpage>1249</fpage>–<lpage>1258</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Huriye</given-names> <surname>Atilgan</surname></string-name>, <string-name><given-names>Cayla E</given-names> <surname>Murphy</surname></string-name>, <string-name><given-names>Hongli</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Heather K</given-names> <surname>Ortega</surname></string-name>, <string-name><given-names>Lucas</given-names> <surname>Pinto</surname></string-name>, and <string-name><given-names>Alex C</given-names> <surname>Kwan</surname></string-name></person-group>. <article-title>Change point estimation by the mouse medial frontal cortex during probabilistic reward learning</article-title>. <source>bioRxiv</source>, pages <elocation-id>2022.05.26.493245</elocation-id>, <year>2022</year>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Jee Hyun</given-names> <surname>Yi</surname></string-name>, <string-name><given-names>Young Ju</given-names> <surname>Yoon</surname></string-name>, <string-name><given-names>Huijeong</given-names> <surname>Jeong</surname></string-name>, <string-name><given-names>Seo Yeon</given-names> <surname>Choe</surname></string-name>, and <string-name><given-names>Min Whan</given-names> <surname>Jung</surname></string-name></person-group>. <article-title>Selective engagement of prefrontal vip neurons in reversal learning</article-title>. <source>bioRxiv</source>, pages <elocation-id>2024.04.03.587891</elocation-id>, <year>2024</year>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Huijeong</given-names> <surname>Jeong</surname></string-name>, <string-name><given-names>Dohoung</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>Min</given-names> <surname>Song</surname></string-name>, <string-name><given-names>Se-Bum</given-names> <surname>Paik</surname></string-name>, and <string-name><given-names>Min Whan</given-names> <surname>Jung</surname></string-name></person-group>. <article-title>Distinct roles of parvalbumin-and somatostatin-expressing neurons in flexible representation of task variables in the prefrontal cortex</article-title>. <source>Progress in Neurobiology</source>, <volume>187</volume>:<fpage>101773</fpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Stephanie M</given-names> <surname>Groman</surname></string-name>, <string-name><given-names>Summer L</given-names> <surname>Thompson</surname></string-name>, <string-name><given-names>Daeyeol</given-names> <surname>Lee</surname></string-name>, and <string-name><given-names>Jane R</given-names> <surname>Taylor</surname></string-name></person-group>. <article-title>Reinforcement learning detuned in addiction: integrative and translational approaches</article-title>. <source>Trends in neurosciences</source>, <volume>45</volume>(<issue>2</issue>):<fpage>96</fpage>–<lpage>105</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Geoffrey</given-names> <surname>Schoenbaum</surname></string-name>, <string-name><given-names>Matthew R</given-names> <surname>Roesch</surname></string-name>, and <string-name><given-names>Thomas A</given-names> <surname>Stalnaker</surname></string-name></person-group>. <article-title>Orbitofrontal cortex, decision-making and drug addiction</article-title>. <source>Trends in neurosciences</source>, <volume>29</volume>(<issue>2</issue>):<fpage>116</fpage>–<lpage>124</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Robert C</given-names> <surname>Wilson</surname></string-name>, <string-name><given-names>Yuji K</given-names> <surname>Takahashi</surname></string-name>, <string-name><given-names>Geoffrey</given-names> <surname>Schoenbaum</surname></string-name>, and <string-name><given-names>Yael</given-names> <surname>Niv</surname></string-name></person-group>. <article-title>Orbitofrontal cortex as a cognitive map of task space</article-title>. <source>Neuron</source>, <volume>81</volume>(<issue>2</issue>):<fpage>267</fpage>–<lpage>279</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Bruno</given-names> <surname>Averbeck</surname></string-name> and <string-name><given-names>John P</given-names> <surname>O’Doherty</surname></string-name></person-group>. <article-title>Reinforcement-learning in fronto-striatal circuits</article-title>. <source>Neuropsychopharmacology</source>, <volume>47</volume>(<issue>1</issue>):<fpage>147</fpage>–<lpage>162</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Roger</given-names> <surname>Ratcliff</surname></string-name></person-group>. <article-title>A theory of memory retrieval</article-title>. <source>Psychological review</source>, <volume>85</volume>(<issue>2</issue>):<fpage>59</fpage>, <year>1978</year>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>John</given-names> <surname>Palmer</surname></string-name>, <string-name><given-names>Alexander C</given-names> <surname>Huk</surname></string-name>, and <string-name><given-names>Michael N</given-names> <surname>Shadlen</surname></string-name></person-group>. <article-title>The effect of stimulus strength on the speed and accuracy of a perceptual decision</article-title>. <source>Journal of vision</source>, <volume>5</volume>(<issue>5</issue>):<fpage>1</fpage>–<lpage>1</lpage>, <year>2005</year>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Michael N</given-names> <surname>Shadlen</surname></string-name> and <string-name><given-names>William T</given-names> <surname>Newsome</surname></string-name></person-group>. <article-title>Neural basis of a perceptual decision in the parietal cortex (area lip) of the rhesus monkey</article-title>. <source>Journal of neurophysiology</source>, <volume>86</volume>(<issue>4</issue>):<fpage>1916</fpage>–<lpage>1936</lpage>, <year>2001</year>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mark E</given-names> <surname>Mazurek</surname></string-name>, <string-name><given-names>Jamie D</given-names> <surname>Roitman</surname></string-name>, <string-name><given-names>Jochen</given-names> <surname>Ditterich</surname></string-name>, and <string-name><given-names>Michael N</given-names> <surname>Shadlen</surname></string-name></person-group>. <article-title>A role for neural integrators in perceptual decision making</article-title>. <source>Cerebral cortex</source>, <volume>13</volume>(<issue>11</issue>):<fpage>1257</fpage>–<lpage>1269</lpage>, <year>2003</year>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Roger</given-names> <surname>Ratcliff</surname></string-name>, <string-name><given-names>Anil</given-names> <surname>Cherian</surname></string-name>, and <string-name><given-names>Mark</given-names> <surname>Segraves</surname></string-name></person-group>. <article-title>A comparison of macaque behavior and superior colliculus neuronal activity to predictions from models of two-choice decisions</article-title>. <source>Journal of neurophysiology</source>, <volume>90</volume>(<issue>3</issue>):<fpage>1392</fpage>–<lpage>1407</lpage>, <year>2003</year>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Mikhail</given-names> <surname>Genkin</surname></string-name>, <string-name><given-names>Krishna V</given-names> <surname>Shenoy</surname></string-name>, <string-name><given-names>Chandramouli</given-names> <surname>Chandrasekaran</surname></string-name>, and <string-name><given-names>Tatiana A</given-names> <surname>Engel</surname></string-name></person-group>. <article-title>The dynamics and geometry of choice in premotor cortex</article-title>. <source>BioRxiv</source>, <year>2023</year>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Klaus</given-names> <surname>Wimmer</surname></string-name>, <string-name><given-names>Duane Q</given-names> <surname>Nykamp</surname></string-name>, <string-name><given-names>Christos</given-names> <surname>Constantinidis</surname></string-name>, and <string-name><given-names>Albert</given-names> <surname>Compte</surname></string-name></person-group>. <article-title>Bump attractor dynamics in prefrontal cortex explains behavioral precision in spatial working memory</article-title>. <source>Nature neuroscience</source>, <volume>17</volume>(<issue>3</issue>):<fpage>431</fpage>–<lpage>439</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Emily L</given-names> <surname>Sylwestrak</surname></string-name>, <string-name><given-names>YoungJu</given-names> <surname>Jo</surname></string-name>, <string-name><given-names>Sam</given-names> <surname>Vesuna</surname></string-name>, <string-name><given-names>Xiao</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Blake</given-names> <surname>Holcomb</surname></string-name>, <string-name><given-names>Rebecca H</given-names> <surname>Tien</surname></string-name>, <string-name><given-names>Doo Kyung</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>Lief</given-names> <surname>Fenno</surname></string-name>, <string-name><given-names>Charu</given-names> <surname>Ramakrishnan</surname></string-name>, <string-name><given-names>William E</given-names> <surname>Allen</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Cell-type-specific population dynamics of diverse reward computations</article-title>. <source>Cell</source>, <volume>185</volume>(<issue>19</issue>):<fpage>3568</fpage>–<lpage>3587</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Richard J</given-names> <surname>Gardner</surname></string-name>, <string-name><given-names>Erik</given-names> <surname>Hermansen</surname></string-name>, <string-name><given-names>Marius</given-names> <surname>Pachitariu</surname></string-name>, <string-name><given-names>Yoram</given-names> <surname>Burak</surname></string-name>, <string-name><given-names>Nils A</given-names> <surname>Baas</surname></string-name>, <string-name><given-names>Benjamin A</given-names> <surname>Dunn</surname></string-name>, <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Toroidal topology of population activity in grid cells</article-title>. <source>Nature</source>, <volume>602</volume>(<issue>7895</issue>):<fpage>123</fpage>–<lpage>128</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ben Sorscher</surname>, <given-names>Gabriel C Mel</given-names></string-name>, <string-name><given-names>Samuel A</given-names> <surname>Ocko</surname></string-name>, <string-name><given-names>Lisa M</given-names> <surname>Giocomo</surname></string-name>, and <string-name><given-names>Surya</given-names> <surname>Ganguli</surname></string-name></person-group>. <article-title>A unified theory for the computational and mechanistic origins of grid cells</article-title>. <source>Neuron</source>, <volume>111</volume>(<issue>1</issue>):<fpage>121</fpage>–<lpage>137</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Brad K</given-names> <surname>Hulse</surname></string-name> and <string-name><given-names>Vivek</given-names> <surname>Jayaraman</surname></string-name></person-group>. <article-title>Mechanisms underlying the neural computation of head direction</article-title>. <source>Annual review of neuroscience</source>, <volume>43</volume>:<fpage>31</fpage>–<lpage>54</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Timothy Doyeon</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>Thomas Zhihao</given-names> <surname>Luo</surname></string-name>, <string-name><given-names>Tankut</given-names> <surname>Can</surname></string-name>, <string-name><given-names>Kamesh</given-names> <surname>Krishnamurthy</surname></string-name>, <string-name><given-names>Jonathan W</given-names> <surname>Pillow</surname></string-name>, and <string-name><given-names>Carlos D</given-names> <surname>Brody</surname></string-name></person-group>. <article-title>Flow-field inference from neural data using deep recurrent networks</article-title>. <source>bioRxiv</source>, <year>2023</year>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Timothy D</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>Thomas Z</given-names> <surname>Luo</surname></string-name>, <string-name><given-names>Jonathan W</given-names> <surname>Pillow</surname></string-name>, and <string-name><given-names>Carlos D</given-names> <surname>Brody</surname></string-name></person-group>. <article-title>Inferring latent dynamics underlying neural population activity via neural differential equations</article-title>. In <conf-name>International Conference on Machine Learning</conf-name>, pages <fpage>5551</fpage>–<lpage>5561</lpage>. <publisher-name>PMLR</publisher-name>, <year>2021</year>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ricky TQ</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Yulia</given-names> <surname>Rubanova</surname></string-name>, <string-name><given-names>Jesse</given-names> <surname>Bettencourt</surname></string-name>, and <string-name><given-names>David K</given-names> <surname>Duvenaud</surname></string-name></person-group>. <article-title>Neural ordinary differential equations</article-title>. <source>Advances in neural information processing systems</source>, <volume>31</volume>, <year>2018</year>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>David</given-names> <surname>Sussillo</surname></string-name>, <string-name><given-names>Rafal</given-names> <surname>Jozefowicz</surname></string-name>, <string-name><given-names>LF</given-names> <surname>Abbott</surname></string-name>, and <string-name><given-names>Chethan</given-names> <surname>Pandarinath</surname></string-name></person-group>. <article-title>Lfads-latent factor analysis via dynamical systems</article-title>. <source>arXiv</source> <pub-id pub-id-type="arxiv">1608.06315</pub-id>, <year>2016</year>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Rylan</given-names> <surname>Schaeffer</surname></string-name>, <string-name><given-names>Mikail</given-names> <surname>Khona</surname></string-name>, <string-name><given-names>Leenoy</given-names> <surname>Meshulam</surname></string-name>, <string-name><given-names>Brain Laboratory</given-names> <surname>International</surname></string-name>, and <string-name><given-names>Ila</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Reverse-engineering recurrent neural network solutions to a hierarchical inference task for mice</article-title>. In <person-group person-group-type="editor"><string-name><given-names>H.</given-names> <surname>Larochelle</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Ranzato</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Hadsell</surname></string-name>, <string-name><given-names>M.F.</given-names> <surname>Balcan</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Lin</surname></string-name></person-group>, editors, <conf-name>Advances in Neural Information Processing Systems, volume 33</conf-name>, pages <fpage>4584</fpage>–<lpage>4596</lpage>. <publisher-name>Curran Associates, Inc</publisher-name>., <year>2020</year>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H Francis</given-names> <surname>Song</surname></string-name>, <string-name><given-names>Guangyu R</given-names> <surname>Yang</surname></string-name>, and <string-name><given-names>Xiao-Jing</given-names> <surname>Wang</surname></string-name></person-group>. <article-title>Reward-based training of recurrent neural networks for cognitive and value-based tasks</article-title>. <source>eLife</source>, <volume>6</volume>:<elocation-id>e21492</elocation-id>, <year>2017</year>. <pub-id pub-id-type="doi">10.7554/eLife.21492</pub-id></mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Manuel</given-names> <surname>Molano-Mazón</surname></string-name>, <string-name><given-names>Yuxiu</given-names> <surname>Shao</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Duque</surname></string-name>, <string-name><given-names>Guangyu Robert</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>Srdjan</given-names> <surname>Ostojic</surname></string-name>, and <string-name><given-names>Jaime De La</given-names> <surname>Rocha</surname></string-name></person-group>. <article-title>Recurrent networks endowed with structural priors explain suboptimal animal behavior</article-title>. <source>Current Biology</source>, <volume>33</volume>(<issue>4</issue>):<fpage>622</fpage>–<lpage>638</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Charles</given-names> <surname>Findling</surname></string-name>, <string-name><given-names>Felix</given-names> <surname>Hubert</surname></string-name>, <collab>International Brain Laboratory</collab>, <string-name><given-names>Luigi</given-names> <surname>Acerbi</surname></string-name>, <string-name><given-names>Brandon</given-names> <surname>Benson</surname></string-name>, <string-name><given-names>Julius</given-names> <surname>Benson</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Birman</surname></string-name>, <string-name><given-names>Niccoló</given-names> <surname>Bonacchi</surname></string-name>, <string-name><given-names>Matteo</given-names> <surname>Carandini</surname></string-name>, <string-name><given-names>Joana A</given-names> <surname>Catarino</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Brainwide representations of prior information in mouse decision-making</article-title>. <source>BioRxiv</source>, pages <elocation-id>2023.07.04.547684</elocation-id>, <year>2023</year>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Sara</given-names> <surname>Matias</surname></string-name>, <string-name><given-names>Eran</given-names> <surname>Lottem</surname></string-name>, <string-name><given-names>Guillaume P</given-names> <surname>Dugué</surname></string-name>, and <string-name><given-names>Zachary F</given-names> <surname>Mainen</surname></string-name></person-group>. <article-title>Activity patterns of serotonin neurons underlying cognitive flexibility</article-title>. <source>eLife</source>, <volume>6</volume>:<elocation-id>e20552</elocation-id>, <year>2017</year>. <pub-id pub-id-type="doi">10.7554/eLife.20552</pub-id></mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Emerson F</given-names> <surname>Harkin</surname></string-name>, <string-name><given-names>Michael B</given-names> <surname>Lynn</surname></string-name>, <string-name><given-names>Alexandre</given-names> <surname>Payeur</surname></string-name>, <string-name><given-names>Jean-François</given-names> <surname>Boucher</surname></string-name>, <string-name><given-names>Léa</given-names> <surname>Caya-Bissonnette</surname></string-name>, <string-name><given-names>Dominic</given-names> <surname>Cyr</surname></string-name>, <string-name><given-names>Chloe</given-names> <surname>Stewart</surname></string-name>, <string-name><surname>André</surname> <given-names>Longtin</given-names></string-name> <string-name><given-names>Richard</given-names> <surname>Naud</surname></string-name>, and <string-name><given-names>Jean-Claude</given-names> <surname>Béëque</surname></string-name></person-group>. <article-title>Temporal derivative computation in the dorsal raphe network revealed by an experimentally driven augmented integrate-and-fire modeling framework</article-title>. <source>eLife</source>, <volume>12</volume>:<elocation-id>e72951</elocation-id>, <year>2023</year>. <pub-id pub-id-type="doi">10.7554/eLife.72951</pub-id></mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Carlos</given-names> <surname>Wert-Carvajal</surname></string-name>, <string-name><given-names>Melissa</given-names> <surname>Reneaux</surname></string-name>, <string-name><given-names>Tatjana</given-names> <surname>Tchumatchenko</surname></string-name>, and <string-name><given-names>Claudia</given-names> <surname>Clopath</surname></string-name></person-group>. <article-title>Dopamine and serotonin interplay for valence-based spatial learning</article-title>. <source>Cell Reports</source>, <volume>39</volume>(<issue>2</issue>), <year>2022</year>.</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Emerson F</given-names> <surname>Harkin</surname></string-name>, <string-name><given-names>Cooper D</given-names> <surname>Grossman</surname></string-name>, <string-name><given-names>Jeremiah Y</given-names> <surname>Cohen</surname></string-name>, <string-name><given-names>Jean-Claude</given-names> <surname>Béëque</surname></string-name>, and <string-name><given-names>Richard</given-names> <surname>Naud</surname></string-name></person-group>. <article-title>Serotonin predictively encodes value</article-title>. <source>bioRxiv</source>, pages <elocation-id>2023.09.19.558526</elocation-id>, <year>2023</year>.</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Carl</given-names> <surname>van Vreeswijk</surname></string-name> and <string-name><given-names>Haim</given-names> <surname>Sompolinsky</surname></string-name></person-group>. <article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title>. <source>Science</source>, <volume>274</volume>(<issue>5293</issue>):<fpage>1724</fpage>, <year>1996</year>.</mixed-citation></ref>
<ref id="c59"><label>[59]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Haim</given-names> <surname>Sompolinsky</surname></string-name>, <string-name><given-names>Andrea</given-names> <surname>Crisanti</surname></string-name>, and <string-name><given-names>Hans-Jurgen</given-names> <surname>Sommers</surname></string-name></person-group>. <article-title>Chaos in random neural net-works</article-title>. <source>Physical review letters</source>, <volume>61</volume>(<issue>3</issue>):<fpage>259</fpage>, <year>1988</year>.</mixed-citation></ref>
<ref id="c60"><label>[60]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jonathan</given-names> <surname>Kadmon</surname></string-name> and <string-name><given-names>Haim</given-names> <surname>Sompolinsky</surname></string-name></person-group>. <article-title>Transition to chaos in random neuronal networks</article-title>. <source>Physical Review X</source>, <volume>5</volume>(<issue>4</issue>):<fpage>041030</fpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c61"><label>[61]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alfonso</given-names> <surname>Renart</surname></string-name>, <string-name><given-names>Jaime De La</given-names> <surname>Rocha</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Bartho</surname></string-name>, <string-name><given-names>Liad</given-names> <surname>Hollender</surname></string-name>, <string-name><given-names>Néstor</given-names> <surname>Parga</surname></string-name>, <string-name><given-names>Alex</given-names> <surname>Reyes</surname></string-name>, and <string-name><given-names>Kenneth D</given-names> <surname>Harris</surname></string-name></person-group>. <article-title>The asynchronous state in cortical circuits</article-title>. <source>science</source>, <volume>327</volume>(<issue>5965</issue>):<fpage>587</fpage>–<lpage>590</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c62"><label>[62]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nicolas</given-names> <surname>Brunel</surname></string-name></person-group>. <article-title>Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title>. <source>Journal of computational neuroscience</source>, <volume>8</volume>:<fpage>183</fpage>–<lpage>208</lpage>, <year>2000</year>.</mixed-citation></ref>
<ref id="c63"><label>[63]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nicolas</given-names> <surname>Brunel</surname></string-name> and <string-name><given-names>David</given-names> <surname>Hansel</surname></string-name></person-group>. <article-title>How noise affects the synchronization properties of recurrent networks of inhibitory neurons</article-title>. <source>Neural computation</source>, <volume>18</volume>(<issue>5</issue>):<fpage>1066</fpage>–<lpage>1110</lpage>, <year>2006</year>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103660.2.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ponte Costa</surname>
<given-names>Rui</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>The findings of this study are <bold>valuable</bold>, offering insights into the neural representation of reversal probability in decision-making tasks, with potential implications for understanding flexible behavior in changing environments. The study contains interesting comparisons between neural data and models, including evidence for partial consistency with line attractor models in this probabilistic reversal learning task. However, it remains <bold>incomplete</bold> due to issues related to how the RNN training and the analysis of its dynamics, which renders the evidence as not complete.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103660.2.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors aimed to investigate how the probability of a reversal in a decision-making task is computed in cortical neurons. They analyzed neural activity in the prefrontal cortex of monkeys and units in recurrent neural networks (RNNs) trained on a similar task. Their goal was to understand how the dynamical systems that implement computation perform a probabilistic reversal learning task in RNNs and nonhuman primates.</p>
<p>Major strengths and weaknesses:</p>
<p>Strengths:</p>
<p>(1) Integrative Approach: The study exemplifies a modern approach by combining empirical data from monkey experiments with computational modeling using RNNs. This integration allows for a more comprehensive understanding of the dynamical systems that implement computation in both biological and artificial neural networks.</p>
<p>
(2) The focus on using perturbations to identify causal relationships in dynamical systems is a good goal. This approach aims to go beyond correlational observations.</p>
<p>
(3) The revised manuscript provides a more nuanced interpretation of the dynamics, reconciling the observations with aspects of line attractor models.</p>
<p>Weaknesses:</p>
<p>(1) The use of targeted dimensionality reduction (TDR) to identify the axis determining reversal probability may not necessarily isolate the dimension along which the RNN computes reversal probability. This should be computed from the RNN update itself rather than through a readout of network variance. Depending on how this is formulated, it could be something like the Jacobian of the state update with respect to inputs at input onset and with respect to the state during relaxation dynamics. This is worth thinking through further. It's important to try to take advantage of access afforded by using RNNs rather than solely relying on analyses available to us in neural data.</p>
<p>Appraisal of aims and conclusions:</p>
<p>The authors have substantially revised their interpretation of the results to reconcile their findings with line attractor models. They now acknowledge that their observation of reward integration explaining reversal probability activity (x_rev) is compatible with line attractor models, which addresses one of my main concerns.</p>
<p>Their expanded analysis now differentiates between two activity modes: (1) substantial non-stationary dynamics during a trial (incompatible with line attractors) and (2) stationary and stable dynamics at trial start (compatible with point attractors and line attractor models). This dual characterization provides a more complete picture of the dynamical system and highlights the composability of dynamical features.</p>
<p>Likely impact and utility:</p>
<p>This work makes a stronger contribution to our understanding of how probabilistic information is represented in neural circuits with intervening behaviors. The augmented model that combines elements of attractor dynamics with non-stationary trajectories offers a more comprehensive framework for understanding neural computations in decision-making tasks.</p>
<p>The data and methods could be useful to the community. While the authors have improved their analysis of network dynamics, additional reverse engineering that takes full advantage of access to the RNN's update equations could further strengthen the work.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103660.2.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this work the authors trained RNN to perform a reversal task also performed by animals while PFC activity is recorded. The authors devised a new method to train RNN on this type of reversal task, which in principle ensures that the behavior of the RNN matches the behavior of the animal. They then performed some analysis of neural activity, both RNN and PFC recording, focusing on the neural representation of the reversal probability and its evolution across trials. Given the analysis presented, it has been difficult for me to asses at which point RNN can reasonably be compared to PFC recordings.</p>
<p>Strengths:</p>
<p>Focusing on a reversal task, the authors address a challenge in RNN training, as they do not use a standard supervised learning procedure where the desired output is available for each trial. They propose a new way of doing that.</p>
<p>They attempt to confront RNN and neural recordings in behaving animals.</p>
<p>Weaknesses:</p>
<p>It would be nice to better articulate the analysis results of the two training set-ups (with and without 0 response during fixation). The dynamical system analysis is confusing, the notions of stationary and non-stationary dynamics and its relationship with attractors are puzzling. Is there a line attractor in one case (with inputs orthogonal to the integration direction being called back to the attractor, and reward input aligned with the stable direction)? In the other case, do we have a cylindrical attracting manifold on which activity circles around and is pushed along the axis of the cylinder by reward inputs? Which case is closest to the PFC recordings?</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103660.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Kim et al. present a study of the neural dynamics underlying reversal learning in monkey PFC and neural networks. Their main finding is that neural activity during fixation resembles a line attractor storing the current belief of the reversal state of the task. This is followed by richer dynamics unfolding throughout the remainder of the trial, which eventually converge to a new point on the line attractor by the start of the next trial. The idea of studying neural dynamics throughout the task (including intervening behaviour) is interesting, and the data provides some insights into the neural dynamics driving reversal learning. The modelling seems to support the analyses, but both the modelling and analyses also leave several open questions.</p>
<p>Strengths:</p>
<p>The paper addresses an interesting topic of the neural dynamics underlying reversal learning in PFC, using a combination of biological and simulated data. Reversal learning has been studied extensively in neuroscience, but this paper takes a step further by analysing neural dynamics throughout the trials instead of focusing on just the evidence integration epoch.</p>
<p>The authors show some close parallels between the experimental data and RNN simulations, both in terms of behaviour and neural dynamics. The analyses of how rewarded and unrewarded trials differentially affect dynamics throughout the trials in RNNs and PFC were particularly interesting. This work has the potential to provide new insights into the neural underpinnings of reversal learning.</p>
<p>Weaknesses:</p>
<p>Data analyses:</p>
<p>While the analyses seem mostly sound, one shortcoming is that they are all aligned to the inferred reversal trial rather than the true experimental reversal trial. For example, the analyses showing that 'x_rev' decays strongly after the reversal trial, irrespective of the reward outcome, seem like they are true essentially by design. The choice to align to the inferred reversal trial also makes this trial seem 'special' (e.g. in Fig 2 &amp; Fig 6A), but it is unclear whether this is a real feature of the data or an artifact of effectively conditioning on a change in behaviour. It would be useful to investigate whether any of these analyses differ when aligned to the true reversal trial. It is also unsurprising that x_rev increases before the reversal and decreases after the reversal (it is hard to imagine a system where this is not the case), yet all of Fig 6 and several other analyses are devoted to this point.</p>
<p>Most of the analyses focus on the dynamics specifically in the x_rev subspace, but a major point of the paper is to say that biological (and artificial) networks may also have to do other things at different times in the trial. If that is the case, it would be interesting to also ask what happens in other subspaces of neural activity, which are not specifically related to evidence integration or choice - are there other subspaces that explain substantial variance? Do they relate to any meaningful features of the experiment?</p>
<p>This is especially important when considering analyses trying to establish the presence (or absence) of attractor dynamics in the circuit. In particular, activity in the x_rev subspace both affects and depends on other subspaces of neural activity, so it is not as meaningful to analyse the dynamics of this subspace in isolation. It would e.g. have been preferable to analyse the early-trial dynamics in the full state space and then possibly projecting onto x_rev, rather than first projecting activity onto x_rev and then fitting a linear autoregressive model.</p>
<p>Modelling:</p>
<p>There are a number of surprising and non-standard modelling choices made in this paper. For example, the choice to only use inhibitory neurons is non-conventional and it is not clear whether and how this impacts the results. The inputs are also provided without any learnable input weights, which makes it harder to interpret the input-driven dynamics during the different phases of a trial.</p>
<p>It is surprising that the RNN is &quot;trained to flip its preferred choice a few trials after the inferred scheduled reversal trial&quot;, with the reversal trial inferred by an ideal Bayesian observer. A more natural approach would be to directly train the RNN to solve the task (by predicting the optimal choice) and then investigating the emergent behaviour &amp; dynamics. If the authors prefer their imitation learning approach, it is also surprising that the network is trained to predict the reversal trial inferred using Bayesian smoothing instead of Bayesian filtering.</p>
<p>Finally, it was surprising that the network is trained and tested with different block lengths (24 &amp; 36 trials, respectively), and it is not mentioned whether or how this affects behaviour.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103660.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kim</surname>
<given-names>Christopher M</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1322-6207</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Chow</surname>
<given-names>Carson C</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Averbeck</surname>
<given-names>Bruno B</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3976-8565</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews</p>
<p>Main revision made to the manuscript</p>
<p>The main revision made to the manuscript is to reconcile our findings with the line attractor model. The revision is based on Reviewer 1’s comment on reinterpreting our results as a superposition of an attractor model with fast timescale dynamics. We expanded our analysis regime to the start of a trial and characterized the overall within-trial dynamics to reinterpret our findings.</p>
<p>We first acknolwedge that our results are not in contradiction with evidence integration on a line attractor. As pointed out by the reviewers, our finding that the integration of reward outcome explains the reversal probability activity x_rev (Figure 3) is compatible with the line attractor model. However, the reward integration equation is an algebraic relation and does not characterize the dynamics of reversal probability activity. So a closer analysis on the neural dynamics is needed to assess the feasibility of line attractor.</p>
<p>In the revised manuscript, we show that x_rev exhibits two different activity modes (Figure 4). First, x_rev has substantial non-stationary dynamics during a trial, and this non-stationary activity is incompatible with the line attractor model, as claimed in the original manuscript. Second, we present new results showing that x_rev is stationary (i.e., constant in time) and stable (i.e., contracting) at the start of a trial. These two properties of x_rev support that it is a point attractor at the start of a trial and is compatible with the line attractor model.</p>
<p>We further analyze how the two activity modes are linked (Figure 4, Support vector regression). We show that the non-stationary activity is predictable from the stationary activity if the underlying dynamics can be inferred. In other words, the non-stationary activity during a trial is generated by an underlying dynamics with the initial condition provided by the stationary state at the start of trial.</p>
<p>These results suggest an extension of the line attractor model where an attractor state at the start of a trial provides an initial condition from which non-stationary activity is generated during a trial by an underlying dynamics associated with task-related behavior (Figure 4, Augmented model).</p>
<p>The separability of non-stationary trajectories (Figure 5 and 6) is a property of the non-stationary dynamics that allows separable points in the initial stationary state to remain separable during a trial, thus making it possible to represent distinct probabilistic values in non-stationary activity.</p>
<p>This revised interpretation of our results (1) retains our original claim that the non-stationary dynamics during a trial is incompatible with the line attractor model and (2) introduces attractor state at the start of a trial which is compatible with the line attractor model. Our anlaysis shows that the two activity modes are linked by an underlying dynamics, and the attractor state serves as initial state to launch the non-stationary activity.</p>
<p><bold>Responses to the Public Reviews:</bold></p>
<p><bold>Reviewer # 1:</bold></p>
<p>(1) To provide better explanation of the reversal learning task and network training method, we added detailed description of RNN and monkey task structure (Result Section 1), included a schematic of target outputs (Figure1B), explained the rationale behind using inhibitory network model (Method Section 1) and explained the supervised RNN training scheme (Result Section 1). This information can also be found in the Methods.</p>
<p>(2) Our understanding is that the augmented model discussed in the previous page is aligned with the model suggested by Reviewer 1: “a curved line attractor, with faster timescale dynamics superimposed on this structure”. It is likely that the “fast” non-stationary activity observed during the trial is driven by task-related behavior, thus is transient. For instance, we do not observe such non-stationary activity in the inter-trial-interval when the task-related behavior is absent. For this reason, the non-stationary trajectories were not considered to be part of the attractor. Instead, they are transient activity generated by the underlying neural dynamics associated with task-related behavior. We believe such characterization of faster timescale dynamics is consistent with Reviewer 1’s view and wanted to clarify that there are two different activity modes.</p>
<p>(3) We appreciate the reviewers (Reviewer 1 and Reviewer 2) comment that TDR may be limited in isolating the neural subspace of interest. Our study presents what could be learned from TDR but is by no means the only way to interpret the neural data. It would be of future work to apply other methods for isolating task-related neural activities.</p>
<p>We would appreciate it if the reviewers could share thoughts on what other alternative methods could better isolate the reversal probability activity.</p>
<p><bold>Reviewer # 2:</bold></p>
<p>(1) (i) We respectfully disagree with Reviewer 2’s comment that “no action is required to be performed by neurons in the RNN”. In our network setup, the output of RNN learns to choose a sign (+ or -), as Reviewer 2 pointed out, to make a choice. This is how the RNN takes an action. It is unclear to us what Reviewer 2 has intended by “action” and how reaching a target value (not just taking a sign) would make a significant difference in how the network performs the task.</p>
<p>(ii)  From Reviewer 2’s comment that “no intervening behavior is thus performed by neurons”, we noticed that the term “intervening behavior” has caused confusion. It refers to task-related behavior, such as making choices or receiving reward, that the subject must perform across trials before reversing its preferred choice. These are the behaviors that intervene the reversal of preferred choice. To clarify its meaning, in the revised manuscript, we changed the term to “task-related behavior” and put them in context. For example, in the Introduction we state that “However, during a trial, task-related behavior, such as making decisions or receiving feedback, produced …”</p>
<p>(iii) As pointed out by Reviewer 2, the lack of fixation period in the RNN could make differences in the neural dynamics of RNN and PFC, especially at the start of a trial. We demonstrate this issue in Result Section 4 where we analyze the stationary activity at the start of a trial. We find that fixating the choice output to zero before making a choice promotes stationary activity and makes the RNN activity more similar to the PFC activity.</p>
<p><bold>Reviewer #3:</bold></p>
<p>(1) (i) In the previous study (Figure 1 in [Bartolo and Averbeck ‘20]), it was shown that neural activity can predict the behavioral reversal trial. This is the reason we examined the neural activity in the trials centered at the behavioral reversal trial. We explained in Result Section 2 that we followed this line of analysis in our study.</p>
<p>(ii) We would like to emphasize that the main point of Figures 4 and 5 is to show the separability of neural trajectories: the entire trajectory shifts without overlapping. It is not obvious that high-dimensional neural population activity from two trials should remain separated when their activities are compressed into a one-dimensional subspace. The onedimensional activities can easily collide since their activities are compressed into a lowdimensional space. We revised the manuscript to bring out these points. We added an opening paragraph that discusses separability of trajectories and revised the main text to bring out the findings on separability.</p>
<p>(iii) We agree with Reviewer 3 that it would be interesting to look at what happens in other subspace of neural activity that are not related to reversal probability and characterize how different neural subspace interact with each. However, the focus of this paper was the reversal probability activity, and we’d consider these questions out of the scope of current paper. We point out that, using the same dataset, neural activity related to other experimental variables were analyzed in other papers [Bartolo and Averbeck ’20; Tang, Bartolo and Averbeck ‘21]</p>
<p>(2) (i) In the revised manuscript, we added explanation on the rational behind choosing inhibitory network as a simplified model for the balanced state. In brief, strong inhibitory recurrent connections with strong excitatory external input operates in the balanced state, as in the standard excitatory-inhibitory network. We included references that studied this inhibitory network. We also explained the technical reason (GPU memory) for choosing the inhibitory model.</p>
<p>(ii) We thank the reviewer for pointing out that the original manuscript did not mention how the feedback and cue were initialized. They were random vectors sample from Gaussian distribution. We added this information in the revised manuscript. In our opinion, it is common to use random external inputs for training RNNs, as it is a priori unclear how to choose them. In fact, it is possible to analyze the effects of random feedback on one-dimensional x_rev dynamics by projecting the random feedback vector to the reversal probability vector. This is shown in Figure 4F.</p>
<p>(iii) We agree that it would be more natural to train the RNN to solve the task without using the Bayesian model. We point out this issue in the Discussion in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewer #1:</bold></p>
<p>(1) My understanding of network training was that a Bayesian ideal observer signaled target output based on previous reward outcomes. However, the authors never mention that networks are trained by supervised learning in the main text until the last paragraph of the discussion. There is no mention that there was an offset in the target based on the behavior of the monkeys in the main text. These are really important things to consider in the context of the network solution after training. I couldn't actually find any figure that presents the target output for the network. Did I miss something key here?</p>
</disp-quote>
<p>In Result Section 1, we added a paragraph that describes in detail how the RNN is trained. We explained that the network is first simulated and then the choice outputs and reward outcomes are fed into the Bayesian model to infer the scheduled reversal trial. A few trials are added to the inferred reversal trial to obtain the behavioral reversal trial, as found in a previous study [Bartolo and Averbeck ‘20]. Then the network weights are updated by backpropagation-through-time via supervised learning.</p>
<p>In the original manuscript, the target output for the network was described in Methods Section 2.5, Step 4. To make this information readily accessible, we added a schematic in Figure 1B that shows the scheduled, inferred and behavioral reversal trials. It also shows how the target choice ouputs are defined. They switch abruptly at the behavioral reversal trial.</p>
<disp-quote content-type="editor-comment">
<p>(2) The role of block structure in the task is an important consideration. What are the statistics of block switches? The authors say on average the reversals are every 36 trials, but also say there are random block switches. The reviewer's notes suggest that both the networks and monkeys may be learning about the typical duration of blocks, which could influence their expectations of reversals. This aspect of the task design should be explained more thoroughly and considered in the context of Figure 1E and 5 results.</p>
</disp-quote>
<p>We provided more detailed description of the reversal learning task in Result Section 1. We clarified that (1) a task is completed by executing a block of fixed number of trials and (2) reversal of reward schedule occurrs at a random trial around the mid-trial in a block. The differences in the number of trials in a block that the RNNs (36) and the monkeys (80) perform are also explained. We also pointed out the differences in how the reversal trial is randomly sampled.</p>
<p>However, it is unclear what Reviewer 1 meant by random block switches. Our reversal learning task is completed when a block of fixed number of trials is executed. Reversal of reward schedule occurs only once on a randomly selected trial in the block, and the reversed reward schedule is maintained until the end of a block. It is different from other versions of reveral learning where the reward schedule switches multiple times across trials. We clarified this point in Result Section 1.</p>
<disp-quote content-type="editor-comment">
<p>(3) The relationship between the supervised learning approach used in the RNNs and reinforcement learning was confused in the discussion. &quot;Although RNNs in our study were trained via supervised learning, animals learn a reversal-learning task from reward feedback, making it into a reinforcement learning (RL) problem.&quot; This is fundamentally not true. In the case of this work, the outcome of the previous trial updates the target output, rather than the trial and error type learning as is typical in reinforcement learning. Networks are not learning by reinforcement learning and this statement is confusing.</p>
</disp-quote>
<p>We agree with Reviewer 1’s comment that the statement in the original manuscript is confusing. Our intention was to point out that our study used supervised learning, and this is different from animals learn by reinforcement learning in rea life. We revised the sentence in Discussion as follows:</p>
<p>“The RNNs in our study were trained via supervised learning. However, in real life, animals learn a reversal learning task via reinforcement learning (RL), i.e., learn the task from reward outcomes.”</p>
<disp-quote content-type="editor-comment">
<p>(4) The distinction between line attractors and the dynamic trajectories described by the authors deserves further investigation. A significant concern arises from the authors' use of targeted dimensionality reduction (TDR), a form of regression, to identify the axis determining reversal probability. While this approach can reveal interesting patterns in the data, it may not necessarily isolate the dimension along which the RNN computes reversal probability. This limitation could lead to misinterpretation of the underlying neural dynamics.</p>
<p>a) This manuscript cites work described in &quot;Prefrontal cortex as a meta-reinforcement learning system,&quot; which examined a similar task. In that study, the authors identified a v-shaped curve in the principal component space of network states, representing the probability of choosing left or right.</p>
<p>Importantly, this curve is topologically equivalent to a line and likely represents a line attractor. However, regressing against reversal probability in such a case would show that a single principal component (PC2) directly correlates with reversal probability.</p>
<p>b) The dynamics observed in the current study bear a striking resemblance to this structure, with the addition of intervening loops in the network state corresponding to within-trial state evolution. Crucially, these observations do not preclude the existence of a line attractor. Instead, they may reflect the network's need to produce fast timescale dynamics within each trial, superimposed on the slower dynamics of the line attractor.</p>
<p>c) This alternative interpretation suggests that reward signals could function as inputs that shift the network state along the line attractor, with information being maintained across trials. The fast &quot;intervening behaviors&quot; observed by the authors could represent faster timescale dynamics occurring on top of the underlying line attractor dynamics, without erasing the accumulated evidence for reversals.</p>
<p>d) Given these considerations, the authors' conclusion that their results are better described by separable dynamic trajectories rather than fixed points on a line attractor may be premature. The observed dynamics could potentially be reconciled with a more nuanced understanding of line attractor models, where the attractor itself may be curved and coexist with faster timescale dynamics.</p>
</disp-quote>
<p>We appreciate the insightful comments on (1) the similarity of the work by Wang et al ’18 with our findings and (2) an alternative interpretation that augments the line attractor with fast timescale dynamics.</p>
<p>(1) We added a discussion of the work by Wang et al ’18 in Result Section 2 to point out the similarity of their findings in the principal component space with ours in the x_rev and x_choice space. We commented that such network dynamics could emerge when learning to perform the reversal learning the task, regardless of the training schemes.</p>
<p>We also mention that the RL approach in Wang et al ’18 does not consider within-trial dynamics, therefore lacks the non-stationary activity observed during the trial in the PFC of monkeys and our trained RNNs.</p>
<p>(2) We revised our original manuscript substantially to reconcile the line attractor model with the nonstationary activity observed during a trial.</p>
<p>Here are the highlights of the revised interpretation of the PFC and the RNN network activity</p>
<p>- The dynamics of x_rev consists of two activity modes, i.e., stationary activity at the start of a trial and non-stationary activity during the trial. Schematic of the augmented model that reconciles two activity modes is shown in Figure 4A. Analysis of the time derivative (dx_reverse / dt) and contractivity of the stationary state are shown in Figure 4B,C to demonstrate two activity modes.</p>
<p>- We discuss in Result Section 4 main text that the stationary activity is consistent with the line attractor model, but the non-stationary activity deviates from the model.</p>
<p>- The two activity modes are linked dynamically. There is an underlying dynamics that can map the stationary state to the non-stationary trajectory. This is shown by predicting the nonstationary trajectory with the stationary state using a support vector regression model. The prediction results are shown in Figure 4D,E,F.</p>
<p>- We discuss in Result Section 4 an extension of the standard line attractor model: points on the line attractor can serve as initial states that launch non-stationary activity associated with taskrelated behavior.</p>
<p>- The separability of neural trajectories presented in Result Section 5 is framed as a property of the non-stationary dynamics associated with task-related behavior.</p>
<disp-quote content-type="editor-comment">
<p>To strengthen their claims, the authors should:</p>
<p>(1) Provide a more detailed description of their RNN training paradigm and task structure, including clear illustrations of target outputs.</p>
<p>(2) Discuss how their findings relate to and potentially extend previous work on similar tasks, particularly addressing the similarities and differences with the v-shaped state organization observed in reinforcement learning contexts. (<ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41593-018-0147-8">https://www.nature.com/articles/s41593-018-0147-8</ext-link> Figure1).</p>
<p>(3) Explore whether their results could be consistent with a curved line attractor model, rather than treating line attractors and dynamic trajectories as mutually exclusive alternatives.</p>
</disp-quote>
<p>Our response to these three comments is described above.</p>
<disp-quote content-type="editor-comment">
<p>Addressing these points would significantly enhance the impact of the study and provide a more nuanced understanding of how reversal probabilities are represented in neural circuits.</p>
<p>In conclusion, while this study provides interesting insights into the neural representation of reversal probability, there are several areas where the methodology and interpretations could be refined.</p>
<p>Additional Minor Concerns:</p>
<p>(1) Network Training and Reversal Timing: The authors mention that the network was trained to switch after a reversal to match animal behavior, stating &quot;Maximum a Posterior (MAP) of the reversal probability converges a few trials past the MAP estimate.&quot; More explanation of how this training strategy relates to actual animal behavior would enhance the reader's understanding of the meaning of the model's similarity to animal behavior in Figure 1.</p>
</disp-quote>
<p>In Method Section 2.5, we described how our observation that the running estimate of MAP converges a few trials after the actual MAP is analogous to the animal’s reversal behavior.</p>
<p>“This observation can be interpreted as follows. If a subject performing the reversal learning task employs the ideal observer model to detect the trial at which reward schedule is reversed, the subject can infer the reversal of reward schedule a few trials past the actual reversal and then switch its preferred choice. This delay in behavioral reversal, relative to the reversal of reward schedule, is analogous to the monkeys switching their preferred choice a few trials after the reversal of reward schedule.”</p>
<p>In Step 4, we also mentioned that the target choice outputs are defined based on our observation in Step 3.</p>
<p>“We used the observation from Step 3 to define target choice outputs that switch abruptly a few trials after the reversal of reward schedule, denoted as $t^*$ in the following. An example of target outputs are shown in Fig.\,\ref{fig_behavior}B.”</p>
<disp-quote content-type="editor-comment">
<p>(2) How is the network simulated in step 1 of training? Is it just randomly initialized? What defines this network structure?</p>
</disp-quote>
<p>The initial state at the start of a block was random. We think the initial state is less relevant as the external inputs (i.e., cue and feedback) are strong and drive the network dynamics. We mentioned these setup and observation in Step 1 of training.</p>
<p>“Step 1. Simulate the network starting from a random initial state, apply the external inputs, i.e., cue and feedback inputs, at each trial and store the network choices and reward outcomes at all the trials in a block. The network dynamics is driven by the external inputs applied periodically over the trials.”</p>
<disp-quote content-type="editor-comment">
<p>(3) Clarification on Learning Approach: More description of the approach in the main text would be beneficial. The statement &quot;Here, we trained RNNs that learned from a Bayesian inference model to mimic the behavioral strategies of monkeys performing the reversal learning task [2, 4]&quot; is somewhat confusing, as the model isn't directly fit to monkey data. A more detailed explanation of how the Bayesian inference model relates to monkey behavior and how it's used in RNN training would improve clarity.</p>
</disp-quote>
<p>We described the learning approach in more detail, but also tried to be concise without going into technical details.</p>
<p>We revised the sentence in Introduction as follows:</p>
<p>“We sought to train RNNs to mimic the behavioral strategies of monkeys performing the reversal learning task. Previous studies \cite{costa2015reversal, bartolo2020prefrontal} have shown that a Bayesian inference model can capture a key aspect of the monkey's behavioral strategy, i.e., adhere to the preferred choice until the reversal of reward is detected and then switch abruptly. We trained the RNNs to replicate this behavioral strategy by training them on target behaviors generated from the Bayesian model.”</p>
<p>We also added a paragraph in Result Section 1 that explains in detail how the training approach works.</p>
<disp-quote content-type="editor-comment">
<p>(4) In Figure 1B, it would be helpful to show the target output.</p>
</disp-quote>
<p>We added a figure in Fig1B that shows a schematic of how the target output is generated.</p>
<disp-quote content-type="editor-comment">
<p>(5) An important point to consider is that a line attractor can be curved while still being topologically equivalent to a line. This nuance makes Figure 4A somewhat difficult to interpret. It might be helpful to discuss how the observed dynamics relate to potentially curved line attractors, which could provide a more nuanced understanding of the neural representations.</p>
</disp-quote>
<p>As discussed above, we interpret the “curved” activity during the trial as non-stationary activity. We do not think this non-stationary activity would be characterized as attractor. Attractor is (1) a minimal set of states that is (2) invariant under the dynamics and (3) attracting when perturbed into its neighborhood [Strogatz, <italic>Nonlinear dynamics and chaos</italic>]. If we consider the autonomous system without the behavior-related external input as the base system, then the non-stationary states could satisfy (2) and (3) but not (1), so they are not part of the attractor. If we include the behavior-related external input to the autonomous dynamics, then it may be possible that the non-stationary trajectories are part of the attractor. We adopted the former interpretation as the behavior-related inputs are external and transient.</p>
<disp-quote content-type="editor-comment">
<p>(6) The results of the perturbation experiments seem to follow necessarily from the way x_rev was defined. It would be valuable to clarify if there's more to these results than what appears to be a direct consequence of the definition, or if there are subtleties in the experimental design or analysis that aren't immediately apparent.</p>
</disp-quote>
<p>The neural activity x_rev is correlated to the reversal probability, but it is unclear if the activity in this neural subspace is causally linked to behavioral variables, such as choice output. We added this explanation at the beginning of Results Section 7 to clarify the reason for performing the perturbation experiments.</p>
<p>“The neural activity $x_{rev}$ is obtained by identifying a neural subspace correlated to reversal probability. However, it remains to be shown if activity within this neural subspace is causally linked to behavioral variables, such as choice output.”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2:</bold></p>
<p>Below is a list of things I have found difficult to understand, and been puzzled/concerned about while reading the manuscript:</p>
<p>(1) It would be nice to say a bit more about the dataset that has been used for PFC analysis, e.g. number of neurons used and in what conditions is Figure 2A obtained (one has to go to supplementary to get the reference).</p>
</disp-quote>
<p>We added information about the PFC dataset in the opening paragraph of Result Section 2 to provide an overview of what type of neural data we’ve analyzed. It includes information about the number of recorded neurons, recording method and spike binning process.</p>
<disp-quote content-type="editor-comment">
<p>(2) It would be nice to give more detail about the monkey task and better explain its trial structure.</p>
</disp-quote>
<p>In Result Section 1 we added a description of the overall task structure (and its difference with other versions of revesal learning task), the RNN / monkey trial structure and differences in RNN and monkey tasks.</p>
<disp-quote content-type="editor-comment">
<p>(3) In the introduction it is mentioned that during the hold period, the probability of reversal is represented. Where does this statement come from?</p>
</disp-quote>
<p>The fact that neural activity during a hold period, i.e., fixation period before presenting the target images, encodes the probability of reversal was demonstrated in a previous study (Bartolo and Averbeck ’20).</p>
<p>We realize that our intention was to state that, during the hold period, the reversal probability activity is stationary as in the line attractor model, instead of focusing on that the probability of reversal is represented during this period. We revised the sentence to convey this message. In addition, we revised the entire paragraph to reinterpret our findings: there are two activity modes where the stationary activity is consistent with the line attractor model but the non-stationary activity deviates from it.</p>
<disp-quote content-type="editor-comment">
<p>(4) &quot;Around the behavioral reversal trial, reversal probabilities were represented by a family of rankordered trajectories that shifted monotonically&quot;. This sentence is confusing and hard to understand.</p>
</disp-quote>
<p>Thank you for point this out. We rewrote the paragraph to reflect our revised interpretation. This sentence was removed, as it can be considered as part of the result on separable trajectories.</p>
<disp-quote content-type="editor-comment">
<p>(5) For clarity, in the first section, when it is written that &quot;The reversal behavior of trained RNNs was similar to the monkey's behavior on the same task&quot; it would be nice to be more precise, that this is to be expected given the strategy used to train the network.</p>
</disp-quote>
<p>We removed this sentence as it makes a blanket statement. Instead, we compared the behavioral outputs of the RNNs and the monkeys one by one.</p>
<p>We added a sentence in Result Section 1 that the RNN’s abrupt behavioral reversal is expected as they are trained to mimic the target choice outputs of the Bayesian model.</p>
<p>“Such abrupt reversal behavior was expected as the RNNs were trained to mimic the target outputs of the Bayesian inference model.”</p>
<disp-quote content-type="editor-comment">
<p>(6) What is the value of tau used in eq (1), and how does it compare to trial duration?</p>
</disp-quote>
<p>We described the value of time constant tau in Eq (1) and also discussed in Result Section 1 that tau=20ms is much faster than trial duration 500ms, thus the persistent behavior seen in trained RNNs is due to learning.</p>
<disp-quote content-type="editor-comment">
<p>(7) It would be nice to expand around the notion of « temporally flexible representation » to help readers grasp what this means.</p>
</disp-quote>
<p>Instead of stating that the separable dynamic trajectories have “temporally flexible representation”, we break down in what sense it is temporally flexible: separable dynamic trajectories can accommodate the effects that task-related behavior have on generating non-stationary neural dynamics.</p>
<p>“In sum, our results show that, in a probabilistic reversal learning task, recurrent neural networks encode reversal probability by adopting, not only stationary states as in a line attractor, but also separable dynamic trajectories that can represent distinct probabilistic values while accommodating non-stationary dynamics associated with task-related behavior.”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3:</bold></p>
<p>(1) Data:</p>
<p>It would be useful to describe the experimental task, recording setup, and analyses in much more detail - both in the text and in the methods. What part of PFC are the recordings from? How many neurons were recorded over how many sessions? Which other papers have they been used in? All of these things are important for the reader to know, but are not listed anywhere. There are also some inconsistencies, with the main text e.g. listing the 'typical block length' as 36 trials, and the methods listing the block length as 24 trials (if this is a difference between the biological data and RNN, that should be more explicit and motivated).</p>
</disp-quote>
<p>We provided more detailed description of the monkey experimental task and PFC recordings in Result Section 1. We also added a new section in Methods 2.1 to describe the monkey experiment.</p>
<disp-quote content-type="editor-comment">
<p>The experimental analyses should be explained in more detail in the methods. There is e.g. no detailed description of the analysis in Figure 6F.</p>
</disp-quote>
<p>We added a new section in Methods 6 to describe how the residual PFC activity is computed. It also describes the RNN perturbation experiments.</p>
<disp-quote content-type="editor-comment">
<p>Finally, it would be useful for more analyses of monkey behaviour and performance, either in the main text or supplementary figures.</p>
</disp-quote>
<p>We did not pursue this comment as it is unclear how additional behavioral analyses would improve the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(2) Model:</p>
<p>When fitting the network, 'step 1' of training in 2.3 seems superfluous. The posterior update from getting a reward at A is the same as that from not getting a reward at B (and vice versa), and it is therefore completely independent of the network choice. The reversal trial can therefore be inferred without ever simulating the network, simply by generating a sample of which trials have the 'good' option being rewarded and which trials have the 'bad' option being rewarded.</p>
</disp-quote>
<p>We respectfully disagree with Reviewer 3’s comment that the reversal trial can be inferred without ever simulating the network. The only way for the network to know about the underlying reward schedule is to perform the task by itself. By simulating the network, it can sample the options and the reward outcomes.</p>
<p>Our understanding is that Review 3 described a strategy that a human would use to perform this task. Our goal was to train the RNN to perform the task.</p>
<disp-quote content-type="editor-comment">
<p>Do the blocks always start with choice A being optimal? Is everything similar if the network is trained with a variable initial rewarded option? E.g. in Fig 6, would you see the appropriate swap in the effect of the perturbation on choice probability if choice B was initially optimal?</p>
</disp-quote>
<p>Thank you for pointing out that the initial high-value option can be random. When setting up the reward schedule, the initial high-value option was chosen randomly from two choice outputs and, at the scheduled reversal, it was switched to the other option. We did not describe this in the original manuscript.</p>
<p>We added a descrption in Training Scheme Step 4 that the the initial high-value option is selected randomly. This is also explained in Result Section 1 when we give an overview of the RNN training procedure.</p>
<disp-quote content-type="editor-comment">
<p>(3) Content:</p>
<p>It is rarely explained what the error bars represent (e.g. Figures 3B, 4C, ...) - this should be clear in all figures.</p>
</disp-quote>
<p>We added that the error bars represent the standard error of mean.</p>
<disp-quote content-type="editor-comment">
<p>Figure 2A: this colour scheme is not great. There are abrupt colour changes both before and after the 'reversal' trial, and both of the extremes are hard to see.</p>
</disp-quote>
<p>We changed the color scheme to contrast pre- and post-reversal trials without the abrupt color change.</p>
<disp-quote content-type="editor-comment">
<p>Figure 3E/F: how is prediction accuracy defined?</p>
</disp-quote>
<p>We added that the prediction accuracy is based on Pearson correlation.</p>
<disp-quote content-type="editor-comment">
<p>Figure 4B: why focus on the derivative of the dynamics? The subsequent plots looking at the actual trajectories are much easier to understand. Also - what is 'relative trial' relative to?</p>
</disp-quote>
<p>The derivative was analyzed to demonstrate stationarity or non-stationarity of the neural activity. We think it will be clearer in the revised manuscript that the derivative allows us to characterize those two activity modes.</p>
<p>Relative trial number indicate the trial position relative to the behavioral reversal trial. We added this description to the figures when “relative trial” is used.</p>
<disp-quote content-type="editor-comment">
<p>Figure 4C: what do these analyses look like if you match the trial numbers for the shift in trajectories? As it is now, there will presumably be more rewarded trials early and late in each block, and more unrewarded trials around the reversal point. Does this introduce biases in the analysis? A related question is (i) why the black lines are different in the top and bottom plots, and (ii) why the ends of the black lines are discontinuous with the beginnings of the red/blue lines.</p>
</disp-quote>
<p>We could not understand what Reviewer 3 was asking in this comment. It’d help if Review 3 could clarify the following question:</p>
<p>“Figure 4C: what do these analyses look like if you match the trial numbers for the shift in trajectories?”</p>
<p>Question (i): We wanted to look at how the trajectory shifts in the subsequent trial if a reward is or is not received in the current trial. The top panel analyzed all the trials in which the subsquent trial did not receive a reward. The bottom panel analyzed all the trials in which the subsequent trial received a reward. So, the trials analyzed in the top and bottom panels are different, and the black lines (x_rev of “current” trial) in the top and bottom panels are different.</p>
<p>Question (ii): Black line is from the preceding trial of the red/blue lines, so if trials are designed to be continuous with the inter-trial-interval, then black and red/blue should be continuous. However, in the monkey experiment, the inter-trial-intervals were variable, so the end of current trial does not match with the start of next trial. The neural trajectories presented in the manuscript did not include the activity in this inter-trial-interval.</p>
<disp-quote content-type="editor-comment">
<p>Figure 6C: are the individual dots different RNNs? Claiming that there is a decrease in Delta x_choice for a v_+ stimulation is very misleading.</p>
</disp-quote>
<p>Yes individual dots are different RNN perturbations. We added explanation about the dots in Figure7C caption.</p>
<p>We agree with the comment that \Delta x_choice did not decrease. This sentence was removed. Instead, we revised the manuscript to state that x_choice for v_+ stimulation was smaller than the x_choice for v_- stimulation. We performed KS-test to confirm statistical significance.</p>
<disp-quote content-type="editor-comment">
<p>Discussion: &quot;...exhibited behaviour consistent with an ideal Bayesian observer, as found in our study&quot;. The RNN was explicitly trained to reproduce an ideal Bayesian observer, so this can only really be considered an assumption (not a result) in the present study.</p>
</disp-quote>
<p>We agree that the statement in the original manuscript is inaccurate. It was revised to reflect that, in the other study, behavior outputs similar to a Bayesian observer emerged by simply learning to do the task, intead of directly mimicking the outputs of Bayesian observer as done in our study.</p>
<p>“Authors showed that trained RNNs exhibited behavior outputs consistent with an ideal Bayesian observer without explicitly learning from the Bayesian observer. This finding shows that the behavioral strategies of monkeys could emerge by simply learning to do the task, instead of directly mimicking the outputs of Bayesian observer as done in our study.”</p>
<disp-quote content-type="editor-comment">
<p>Methods: Would the results differ if your Bayesian observer model used the true prior (i.e. the reversal happens in the middle 10 trials) rather than a uniform prior? Given the extensive literature on prior effects on animal behaviour, it is reasonable to expect that monkeys incorporate some non-uniform prior over the reversal point.</p>
</disp-quote>
<p>Thank you for pointing out the non-uniform prior. We haven’t conducted this analysis, but would guess that the convergence to the posterior distribution would be faster. We’d have to perform further analysis, which is out of the scope of this paper, to investigate whether the posteior distribution would be different from what we obtained from uniform prior.</p>
<disp-quote content-type="editor-comment">
<p>Making the code available would make the work more transparent and useful to the community.</p>
</disp-quote>
<p>The code is available in the following Github repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/chrismkkim/LearnToReverse">https://github.com/chrismkkim/LearnToReverse</ext-link></p>
</body>
</sub-article>
</article>