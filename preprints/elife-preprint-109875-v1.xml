<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">109875</article-id>
<article-id pub-id-type="doi">10.7554/eLife.109875</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.109875.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Deep learning-driven characterization of single cell tuning in primate visual area V4 supports topological organization</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4445-6408</contrib-id>
<name>
<surname>Willeke</surname>
<given-names>Konstantin F</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="aff" rid="a8">8</xref>
<xref ref-type="aff" rid="a9">9</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5754-4883</contrib-id>
<name>
<surname>Restivo</surname>
<given-names>Kelli</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8649-4835</contrib-id>
<name>
<surname>Franke</surname>
<given-names>Katrin</given-names>
</name>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="aff" rid="a8">8</xref>
<xref ref-type="aff" rid="a9">9</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1780-1448</contrib-id>
<name>
<surname>Nix</surname>
<given-names>Arne F</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7508-4443</contrib-id>
<name>
<surname>Cadena</surname>
<given-names>Santiago A</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Shinn</surname>
<given-names>Tori</given-names>
</name>
<xref ref-type="aff" rid="a10">10</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Nealley</surname>
<given-names>Cate</given-names>
</name>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="aff" rid="a8">8</xref>
<xref ref-type="aff" rid="a9">9</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rodriguez</surname>
<given-names>Gabrielle</given-names>
</name>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="aff" rid="a8">8</xref>
<xref ref-type="aff" rid="a9">9</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2386-8940</contrib-id>
<name>
<surname>Patel</surname>
<given-names>Saumil</given-names>
</name>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="aff" rid="a8">8</xref>
<xref ref-type="aff" rid="a9">9</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2392-5105</contrib-id>
<name>
<surname>Ecker</surname>
<given-names>Alexander S</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-1348-9736</contrib-id>
<name>
<surname>Sinz</surname>
<given-names>Fabian H</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="author-notes" rid="n2">†</xref>
<email>sinz@uni-goettingen.de</email>
</contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4305-6376</contrib-id>
<name>
<surname>Tolias</surname>
<given-names>Andreas S</given-names>
</name>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="aff" rid="a8">8</xref>
<xref ref-type="aff" rid="a9">9</xref>
<xref ref-type="author-notes" rid="n2">†</xref>
<email>tolias@stanford.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03a1kwz48</institution-id><institution>Institute for Bioinformatics and Medical Informatics, Tübingen University</institution></institution-wrap>, <city>Tübingen</city>, <country country="DE">Germany</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01y9bpm73</institution-id><institution>Institute of Computer Science and Campus Institute Data Science, University of Göttingen</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02pttbw34</institution-id><institution>Department of Neuroscience, Baylor College of Medicine</institution></institution-wrap>, <city>Houston</city>, <country country="US">United States</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02pttbw34</institution-id><institution>Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine</institution></institution-wrap>, <city>Houston</city>, <country country="US">United States</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0087djs12</institution-id><institution>Max Planck Institute for Dynamics and Self-Organization</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff>
<aff id="a7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Ophthalmology, Byers Eye Institute, Stanford University School of Medicine</institution></institution-wrap>, <city>Stanford</city>, <country country="US">United States</country></aff>
<aff id="a8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford Bio-X, Stanford University</institution></institution-wrap>, <city>Stanford</city>, <country country="US">United States</country></aff>
<aff id="a9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Wu Tsai Neurosciences Institute, Stanford University</institution></institution-wrap>, <city>Stanford</city>, <country country="US">United States</country></aff>
<aff id="a10"><label>10</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02pttbw34</institution-id><institution>Department of Pediatrics; Allergy &amp; Immunology, Baylor College of Medicine</institution></institution-wrap>, <city>Houston</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Moore</surname>
<given-names>Tirin</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University, Howard Hughes Medical Institute</institution>
</institution-wrap>
<city>Stanford</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Moore</surname>
<given-names>Tirin</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University, Howard Hughes Medical Institute</institution>
</institution-wrap>
<city>Stanford</city>
<country country="US">United States</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>*</label><p>Equal contributions</p></fn>
<fn id="n2" fn-type="equal"><label>†</label><p>Equal contributions</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2026-02-06">
<day>06</day>
<month>02</month>
<year>2026</year>
</pub-date>
<volume>15</volume>
<elocation-id>RP109875</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-11-18">
<day>18</day>
<month>11</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-11-05">
<day>05</day>
<month>11</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.05.12.540591"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2026, Willeke et al</copyright-statement>
<copyright-year>2026</copyright-year>
<copyright-holder>Willeke et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-109875-v1.pdf"/>
<abstract>
<p>Deciphering the brain’s structure-function relationship is key to understanding the neuronal mechanisms underlying perception and cognition. The cortical column, a vertical organization of neurons with similar functions, is a classic example of primate neocortex structure-function organization. While columns have been identified in primary sensory areas using parametric stimuli, their prevalence across higher-level cortex is debated, particularly regarding complex tuning in natural image space. However, a key hurdle in identifying columns is characterizing the complex, nonlinear tuning of neurons to high-dimensional sensory inputs. Building on prior findings of topological organization for features like color and orientation, we investigate functional clustering in macaque visual area V4 in non-parametric natural image space, using large-scale recordings and deep learning–based analysis. We combined linear probe recordings with deep learning methods to systematically characterize the tuning of &gt;1,200 V4 neurons using <italic>in silico</italic> synthesis of most exciting images (MEIs), followed by <italic>in vivo</italic> verification. Single V4 neurons exhibited MEIs containing complex features, including textures and shapes, and even high-level attributes with eye-like appearance. Neurons recorded on the same silicon probe, inserted orthogonal to the cortical surface, often exhibited similarities in their spatial feature selectivity, suggesting a degree of functional organization along the cortical depth. We quantified MEI similarity using human psychophysics and distances in a contrastive learning-derived embedding space. Moreover, the selectivity of the V4 neuronal population showed evidence of clustering into functional groups of shared feature selectivity. These functional groups showed parallels with the feature maps of units in artificial vision systems, suggesting potential shared encoding strategies. These results demonstrate the feasibility and scalability of deep learning–based functional characterization of neuronal selectivity in naturalistic visual contexts, offering a framework for quantitatively mapping cortical organization across multiple levels of the visual hierarchy.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Revised the text to improve clarity and precision, incorporating recent literature to strengthen context and interpretation. Adjusted framing throughout to present conclusions more conservatively.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>From the intricate layering of neurons with diverse functions in the retina (e.g. <xref ref-type="bibr" rid="c59">Masland, 2001</xref>) to the topographic maps of the cerebral cortex (e.g. <xref ref-type="bibr" rid="c75">Penfield &amp; Boldrey, 1937</xref>), for decades neuroscientists have been pursing the quest to discover general organizing principles that relate the structure (anatomy) and function (physiology) of the brain. For instance, the concept of functional cortical columns, first described by <xref ref-type="bibr" rid="c65">Mountcastle (1957)</xref> in the somatosensory cortex and later in the primary visual cortex (V1) by <xref ref-type="bibr" rid="c40">Hubel &amp; Wiesel (1968)</xref>, represents one such well-established motif, thought to recur across the primate neocortex (discussed in <xref ref-type="bibr" rid="c39">Horton &amp; Adams, 2005</xref>). In this canonical organization, neurons with similar function are vertically organized across cortical layers. Considering that the connections within the cortex are locally dense and span the cortical layers, this configuration enables neurons with similar response functions to synaptically interact, thus facilitating computations to transform information within and across the layers (e.g. <xref ref-type="bibr" rid="c10">Cadwell et al., 2020</xref>; <xref ref-type="bibr" rid="c12">Campagnola et al., 2022</xref>; <xref ref-type="bibr" rid="c44">Jiang et al., 2015</xref>)</p>
<p>Obtaining a comprehensive understanding of the relationship between anatomy and function requires a thorough characterization of neuronal stimulus selectivity or tuning. The selectivity of neurons in monkey and cat V1 for simple visual features, such as orientation, phase, or spatial frequency (<xref ref-type="bibr" rid="c43">Issa et al., 2000</xref>; <xref ref-type="bibr" rid="c93">Victor et al., 1994</xref>), enables the characterization of the tuning properties of these neurons using well-defined parametric stimuli such as gratings. This has greatly facilitated the identification of general organizing principles of neuronal function in the early visual areas of the cortical hierarchy (e.g. <xref ref-type="bibr" rid="c68">Ohki &amp; Reid, 2014</xref>). However, neurons in higher visual areas prefer more complex visual features found in natural scenes, such as shapes, textures, objects, and faces (e.g. <xref ref-type="bibr" rid="c3">Bashivan et al., 2019</xref>; <xref ref-type="bibr" rid="c46">Kim et al., 2019</xref>; <xref ref-type="bibr" rid="c76">Ponce et al., 2019</xref>; <xref ref-type="bibr" rid="c86">Tang et al., 2020</xref>; <xref ref-type="bibr" rid="c88">Tsao et al., 2003</xref>), which are not easily parameterized. The immense diversity and high dimensionality of the natural image space make it challenging to systematically characterize more complex visual function and link it to an organizing structure.</p>
<p>Although prior studies have described feature selectivity in area V4 (e.g. <xref ref-type="bibr" rid="c3">Bashivan et al., 2019</xref>; <xref ref-type="bibr" rid="c28">Gallant et al., 1993</xref>; <xref ref-type="bibr" rid="c46">Kim et al., 2019</xref>; <xref ref-type="bibr" rid="c72">Pasupathy &amp; Connor, 2002</xref>; <xref ref-type="bibr" rid="c86">Tang et al., 2020</xref>; <xref ref-type="bibr" rid="c96">Wang et al., 2023</xref>), less is known about how this selectivity is organized across cortical depth or whether it exhibits topographic structure akin to columnar motifs observed in early visual cortex. While orientation, curvature and color domains have been reported to exhibit spatial clustering within V4 (<xref ref-type="bibr" rid="c16">Conway &amp; Tsao, 2009</xref>; <xref ref-type="bibr" rid="c29">Ghose &amp; Ts’o, 1997</xref>; <xref ref-type="bibr" rid="c86">Tang et al., 2020</xref>; <xref ref-type="bibr" rid="c87">Tanigawa et al., 2010</xref>; <xref ref-type="bibr" rid="c95">Wang et al., 2024</xref>; <xref ref-type="bibr" rid="c103">Zhang et al., 2023</xref>), other work has suggested a more patchy or irregular distribution of feature selectivity (<xref ref-type="bibr" rid="c51">Kotake et al., 2009</xref>; <xref ref-type="bibr" rid="c66">Namima et al., 2025</xref>), raising debate over whether V4 exhibits consistent columnar or topographic organization. It is therefore unclear whether higher-order tuning, especially under natural image stimulation, follows similar spatial principles. Addressing this question requires scalable, data-driven approaches capable of characterizing neuronal tuning within the rich statistics of natural images, without relying on strong assumptions about feature space.</p>
<p>Advancements in deep learning promise to overcome these challenges. Specifically, recent deep learning functional models of the brain can accurately predict responses to arbitrary stimuli (<xref ref-type="bibr" rid="c3">Bashivan et al., 2019</xref>; <xref ref-type="bibr" rid="c7">Cadena et al., 2019</xref>; <xref ref-type="bibr" rid="c94">Walker et al., 2019</xref>), enabling essentially unlimited <italic>in silico</italic> experiments that complement rather than replace in vivo recordings, and allow exploration of high-dimensional visual feature spaces that are experimentally inaccessible. This can be used for a comprehensive characterization of neuronal tuning function, such as identifying the neurons’ optimal stimuli (<xref ref-type="bibr" rid="c3">Bashivan et al., 2019</xref>; <xref ref-type="bibr" rid="c26">Franke et al., 2022</xref>; <xref ref-type="bibr" rid="c38">Höfling et al., 2022</xref>; <xref ref-type="bibr" rid="c47">Kindel et al., 2019</xref>; <xref ref-type="bibr" rid="c94">Walker et al., 2019</xref>), map their invariances (<xref ref-type="bibr" rid="c20">Ding et al., 2023b</xref>), characterize contextual modulation (<xref ref-type="bibr" rid="c27">Fu et al., 2023</xref>) or characterize how multiple distinct tuning properties or nonlinear contextual effects relate to each other (<xref ref-type="bibr" rid="c89">Ustyuzhaninov et al., 2022</xref>). The predictions derived from these <italic>in silico</italic> analyses can then be verified through <italic>in vivo</italic> closed-loop experiments, known as inception loops, which have been successfully applied to single neurons in mice (<xref ref-type="bibr" rid="c20">Ding et al., 2023b</xref>; <xref ref-type="bibr" rid="c26">Franke et al., 2022</xref>; <xref ref-type="bibr" rid="c27">Fu et al., 2023</xref>; <xref ref-type="bibr" rid="c38">Höfling et al., 2022</xref>; <xref ref-type="bibr" rid="c94">Walker et al., 2019</xref>) and populations of neurons in macaque visual cortex (<xref ref-type="bibr" rid="c3">Bashivan et al., 2019</xref>).</p>
<p>In this study, we adapted the inception loop paradigm for macaque electrophysiological single-unit recordings to systematically map stimulus selectivity and analyze the structure-function organization of neurons in visual area V4. We used deep neural networks to build an accurate model of &gt;1,200 recorded V4 neurons, capable of predicting responses to arbitrary images and used it to synthesize the most exciting image (MEI) for individual neurons, which we subsequently verified <italic>in vivo</italic>. We found that neurons recorded on the same silicon probe orthogonal to the cortical surface appeared to have similar spatial features compared to MEIs of neurons recorded in different locations, and verified this impression with human psychophysics and a non-linear embedding space based on image similarity. Furthermore, the MEIs formed isolated clusters in the non-linear embedding space, indicating that V4 neurons separate into distinct functional groups that are selective for specific complex visual features, such as oriented fur patterns, grid-like motifs, curvatures, or even high-level attributes reminiscent of eyes. Interestingly, these functional groups closely resemble the feature maps of earlyto mid-level units in deep neural networks trained on image classification (<xref ref-type="bibr" rid="c69">Olah et al., 2020</xref>), suggesting that computational principles are shared among biological and artificial visual systems. These findings reveal functional clustering within V4 that is consistent with a topographic organization of feature selectivity and support the notion of a columnar-like organization of natural image selectivity in area V4.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Deep neural network approach captures tuning properties of individual monkey V4 neurons</title>
<p>To systematically study the neuronal tuning properties of monkey V4 neurons in the context of natural scenes, we combined large-scale neuronal recordings with deep neural network modeling. To this end, we presented natural images to awake, head-fixed macaque monkeys and monitored the spiking population activity of V4 neurons using acute electrophysiological recordings with 32-channel linear arrays spanning 1,920 µm in depth, covering the majority of the 2 mm cortical depth (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>; <xref ref-type="bibr" rid="c17">Denfield et al., 2018</xref>). In each recording session, we displayed 9,000–12,075 gray-scale images from the ImageNet database (<xref ref-type="bibr" rid="c18">Deng et al., 2009</xref>) organized in a trial structure, where each trial consisted of 15 images, each presented for 120 ms, followed by a gray screen 1,200 ms inter-trial period. During image presentation, the monkey was trained to maintain fixation on a fixation spot offset from the center of the monitor (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). The spot’s exact location was selected prior to each recording session such that the neurons’ population receptive field (RF), determined by using a sparse random dot stimulus, was centered on the monitor. Post-hoc spike sorting of the neuronal activity recorded across 100 sessions from two monkeys isolated the single-unit visual activity of 1,224 individual V4 neurons (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>), resulting in a large dataset of well-isolated single-unit activity in monkey V4.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1.</label>
<caption><title>Deep neural network approach captures tuning properties of individual monkey V4 neurons</title>
<p><bold>a</bold>, Schematic illustrating experimental setup: Awake, head-fixed macaque monkeys were presented with static natural images after fixating for 300 ms (120 ms presentation time per image, 15 images per trial, 1200 ms inter-trial period), while recording the neuronal activity in V4 using 32-channel probes. Animals were fixating on a fixation spot such that the recorded neurons’ population receptive field was centered on the monitor. Post-hoc spike sorting resulted in single-unit activity of individual V4 neurons. <bold>b</bold>, Schematic illustrating model architecture: The pre-processed stimuli (100 x 100 pixels crop) and neuronal responses were used to train a neuron-specific read-out of a ResNet50 pre-trained on an image classification task. Specifically, we selected the ResNet50 layer with the best V4 predictions and computed the neuronal responses by passing the feature activations to a neuron-specific Gaussian readout and a subsequent non-linearity. Traces on the right show average responses (gray) to 75 test images of two example neurons and their corresponding model predictions (black). <bold>c</bold>, Schematic illustrating 32-channels along the probe used for electrophysiological recordings and number of recording sessions per monkey. In total, we recorded the single-unit activity of n=1,244 neurons. <bold>d</bold>, Explainable variance as a measure of response reliability to natural images plotted versus model prediction performance (correlation between prediction and average neural response to repeated presentations) of all cells. Dotted red line indicates a prediction performance of 0.3 used in subsequent analyses (explainable variance mean <italic>±</italic> s.d. = 0.33 <italic>±</italic> 0.19, correlation to average mean <italic>±</italic> s.d. = 0.43 <italic>±</italic> 0.21) <bold>e</bold>, Schematic illustrating optimization of most exciting images (MEIs). For each <italic>in silico</italic> neuron, we optimized its MEI using gradient ascent over n=100 iterations.The whole gray box (full extent) is 14.82°degrees visual angle in width and height. <bold>f</bold>, MEIs of ten example neurons.</p></caption>
<graphic xlink:href="540591v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To predict the responses of the recorded neurons and characterize the neurons’ tuning properties, we used a deep convolutional neural network (CNN) model (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). Based on previous work in monkey (<xref ref-type="bibr" rid="c3">Bashivan et al., 2019</xref>; <xref ref-type="bibr" rid="c7">Cadena et al., 2019</xref>, <xref ref-type="bibr" rid="c9">2022</xref>), we used a pretrained goal-directed neural network as a non-linear feature space shared across all neurons and fitted only a simple linear-nonlinear neuron-specific readout (<xref ref-type="bibr" rid="c57">Lurz et al., 2020</xref>). Specifically, we chose a robust and high-performing ResNet-50 (<xref ref-type="bibr" rid="c80">Salman et al., 2020</xref>) as goal-directed neural network, trained on an image classification task. We selected one of its intermediate layers (layer 3.0) as non-linear feature space because it resulted in the best response predictions of the recorded V4 neurons. This yielded a correlation between response predictions and mean neuronal responses across repetitions of 0.43 (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>), indicating that the model reliably captured key tuning properties of the V4 population. In the following, we will refer to this measure as model performance.</p>
<p>Treating our CNN model as a functional digital twin of the population of V4 neurons, we synthesized maximally exciting images (MEIs) (<xref ref-type="bibr" rid="c3">Bashivan et al., 2019</xref>; <xref ref-type="bibr" rid="c94">Walker et al., 2019</xref>) for individual V4 neurons <italic>in silico</italic> (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>). To this end, we optimized a contrast-constrained image to produce the highest activation in the model neuron using regularized gradient ascent. The resulting MEI corresponds to the optimal stimulus of a neuron according to the model, depicting the peak of its tuning curve. We found that MEIs strongly differ across neurons, indicating selectivity for distinct stimulus features like texture, curvature and edges (<xref rid="fig1" ref-type="fig">Fig. 1f</xref>), which resemble the features found in the MEIs of V4 multi-unit activity (<xref ref-type="bibr" rid="c3">Bashivan et al., 2019</xref>). Our MEIs were also consistent with tuning properties of macaque V4, such as shape, curvature, and texture selectivity, previously, identified using parametric stimuli (e.g. <xref ref-type="bibr" rid="c46">Kim et al., 2019</xref>; <xref ref-type="bibr" rid="c71">Pasupathy &amp; Connor, 2001</xref>; <xref ref-type="bibr" rid="c73">Pasupathy et al., 2020</xref>). In contrast to these previous studies, our data-driven approach enables characterization of single-neuron selectivity in a natural image context, without requiring prior assumptions about feature space or pre-selection of stimuli.</p>
</sec>
<sec id="s2b">
<title>Closed-loop paradigm verifies model-derived optimal stimuli of single V4 neurons</title>
<p>To demonstrate the model’s accuracy and empirically validate that the synthesized MEIs effectively activated the recorded neurons, we developed a closed-loop paradigm for acute electrophysiological recordings of single neurons (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). Specifically, after fitting the readout for single-unit responses recorded in a “generation” session where natural images were shown, we selected the best six units based on model prediction for <italic>in vivo</italic> verification, generated their MEIs and presented them back to the animal on the same day while recording from the same neurons in a “verification” session. Single units were matched across the generation and verification session using spike waveform similarity and functional consistency of responses to the same natural images (<xref rid="figS1" ref-type="fig">Suppl. Fig. 1</xref>). As a control stimulus for each selected unit, we presented the seven most exciting natural image crops identified by the model by screening 5,000 natural images not used during model training. Each crop was matched to the size, position, and contrast of the MEI of a particular neuron. Control stimuli qualitatively resembled the MEIs (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>), confirming that modelsynthesized features reflect elements commonly found in natural scenes.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig. 2.</label>
<caption><title>Closed-loop paradigm verifies model-derived optimal stimuli of single V4 neurons</title>
<p><bold>a</bold>, Schematic illustrating closed-loop experimental paradigm for acute recordings in monkey V4. In brief, after recording and spike sorting of the “generation session”, we train a model, select neurons for experimental confirmation, generate MEIs and identify the most exciting natural image control stimuli, and present both MEIs and controls back to the animal while recording from the same neurons in the “verification session”. Functional and waveform matching of units across recordings is performed offline. <bold>b</bold>, MEI and the seven most exciting natural image crops, selected from 5,000 natural images, for four example neurons. Natural images were matched in size, location and contrast to the MEI. <bold>c</bold>, Peak-normalized recorded responses of the neurons in (b) to their MEI (orange) and control images (black; mean across n=20 repeats). <bold>d</bold>, Recorded versus predicted neuronal activity of two example neurons to their MEI and control stimuli, as well as to MEIs and control stimuli of other neurons. <bold>e</bold>, Scatter plot of model performance on the test set of natural images and the closed-loop stimuli (as shown in d, but for all neurons). Correlation to average: mean <italic>±</italic> s.d. = 0.61 <italic>±</italic> 0.11; Synthesized and selected stimuli: mean <italic>±</italic> s.d. 0.61 <italic>±</italic> 0.20, <italic>n</italic> = 55 neurons. A paired t-test showed no significant difference <italic>p</italic> = .61. <bold>f</bold>, Distribution of peak-normalized mean responses to each neuron’s MEI and control stimuli, as well as MEIs and control stimuli of other neurons for all closed-loop neurons (<italic>n</italic> = 55 neurons, <italic>n</italic> = 24 sessions, <italic>n</italic> = 1 monkey). P-values for a paired t-test are: MEI-Control, 3.22e-08; MEI-OtherMEIs, 2.57e-14; MEI-OtherControls, 3.06e-19; Control-OtherMEIs, 2.86e-07; Control-OtherControls, 1.62e-19; OtherMEIs-OtherControls, 2.99e-05. P-values were corrected for multiple comparisons with Bonferroni correction.</p></caption>
<graphic xlink:href="540591v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Overall, the model predicted V4 neuronal responses to both full-field natural images and synthesized MEIs with high reliability. Despite the high structural similarity of MEI and control images, the MEI consistently elicited higher neuronal responses than the control images, as well as MEIs and control images of other neurons (<xref rid="fig2" ref-type="fig">Fig. 2c,d,f</xref>), indicating neuron-specific selectivity captured by the model. In addition, the model accurately predicted the neurons’ average responses to their own MEIs, to control stimuli, and to the MEIs and control stimuli of other neurons of the same session (two example neurons in <xref rid="fig2" ref-type="fig">Fig. 2d</xref>). The absolute scale of the firing rate predictions did not always perfectly match the recorded firing rate, likely due to slow drifts in overall firing rates of some neurons (e. g. <xref rid="fig2" ref-type="fig">Fig. 2d</xref>, right). Nevertheless, across neurons, the model trained on the generation session generalized robustly to the verification session, with no significant difference in prediction performance between full-field natural images and synthesized MEIs (<italic>ρ</italic> = 0.61, <xref rid="fig2" ref-type="fig">Fig. 2e</xref>). Moreover, the amplitude of neuronal responses to control stimuli and MEIs of other neurons only slightly differed, suggesting that both sets of stimuli share similar low-level visual statistics despite the neuron-specific optimization of MEIs (<xref rid="fig2" ref-type="fig">Fig. 2f</xref>).</p>
</sec>
<sec id="s2c">
<title>Topographic organization of model-derived feature selectivity in macaque V4</title>
<p>Studying how visual selectivity is organized in a particular brain area has revealed key principles of vision, including the pinwheel of orientation columns in primary visual cortex (<xref ref-type="bibr" rid="c6">Bonhoeffer &amp; Grinvald, 1991</xref>). In monkey V4, neurons are tuned to more complex visual features like shape and texture (<xref ref-type="bibr" rid="c46">Kim et al., 2019</xref>; <xref ref-type="bibr" rid="c71">Pasupathy &amp; Connor, 2001</xref>; <xref ref-type="bibr" rid="c73">Pasupathy et al., 2020</xref>; <xref ref-type="bibr" rid="c83">Srinath et al., 2020</xref>), but it remains unclear whether V4 tuning properties are organized in a columnar manner (<xref ref-type="bibr" rid="c29">Ghose &amp; Ts’o, 1997</xref>; <xref ref-type="bibr" rid="c36">Hatanaka et al., 2022</xref>; <xref ref-type="bibr" rid="c66">Namima et al., 2025</xref>; <xref ref-type="bibr" rid="c86">Tang et al., 2020</xref>). We next asked whether our data-driven approach could reveal topographic patterns in feature selectivity, as inferred from model-derived optimal stimuli.</p>
<p>We noticed that MEIs from individual sessions tended to exhibit higher mutual visual similarity compared to MEIs from other sessions (<xref rid="fig3" ref-type="fig">Fig. 3a,b</xref>). While the range of preferred stimuli we found spanned a large variety from oriented and comb-like patterns, to grid-like motifs and highlevel attributes, the perceived variability within many sessions was much smaller than across sessions. For example, most neurons in an example session preferred oriented and comb-like patterns (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>), while neurons from other example sessions preferred curved edges (session 2 in <xref rid="fig3" ref-type="fig">Fig. 3b</xref>) and grid-like patterns (session 3 in <xref rid="fig3" ref-type="fig">Fig. 3b</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig. 3.</label>
<caption><title>Topographic organization of model-derived feature selectivity in macaque V4</title>
<p><bold>a</bold>, MEIs of 17 neurons recorded in a single experimental session, arranged according to each neuron’s channel location along the recording probe. Numbers indicate channel, with higher channel numbers meaning greater recording depth. <bold>b</bold>, MEIs of varying numbers of neurons for four different sessions (indicated by different colors). <bold>c</bold>, Schematic illustrating paradigm of simple psychophysics experiment. In one trial, subjects were presented with MEIs of 9 neurons recorded within one session (left) or randomly sampled from all neurons except the target session (right), and reported the location (left or right) of the set of MEIs that looked more consistent (i.e. shared the same image features). The experiment included n=50 trials/sessions. <bold>d</bold>, Distribution of fraction of sessions correctly identified across n=25 observers, with change level and observer average indicated by dotted lines. Mean across subjects, = 0.73; subject-variability in s.d., = 0.13; session-variability in s.d., = 0.21.</p></caption>
<graphic xlink:href="540591v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To quantify the perceptual similarity of MEIs within a session, we performed a simple psychophysics experiment, where human observers were presented with two sets of MEIs: (1) a selection of nine neurons recorded within one session and (2) a set of nine neurons randomly sampled across sessions. The two sets were presented side-by-side (as shown in <xref rid="fig3" ref-type="fig">Fig. 3c</xref>, but without the colored frames), with each set being shown on the left or right at random. The observers then had to report in a two-alternative forced-choice paradigm which set of MEIs looked perceptually more similar. On average, the observers classified MEIs of the same session as being more consistent than a random set of MEIs from different sessions for 73% of the sessions (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>), indicating that neurons recorded along the same electrode penetration tend to share similar model-derived feature preferences. Because neurons recorded within a session are arranged roughly along the vertical axis of cortex, these results suggest a local topographic organization of feature selectivity in macaque V4. This pattern is consistent with columnar-like functional clustering but does not constitute direct anatomical evidence for columns.</p>
<p>To further quantify whether tuning properties of V4 neurons are functionally organized in a topographic manner, we performed nonlinear dimensionality reduction to embed the MEIs in a two-dimensional space based on modelderived image feature similarity rather than recording session identity.. In contrast to V1 neurons, whose tuning properties can be compared along clearly defined axes such as orientation or spatial frequency, the complex MEI structure of V4 neurons makes it challenging to quantify the tuning similarity between neurons. To resolve this problem, we used an unsupervised deep learning technique that learns a two-dimensional image-embedding based on mutual similarity of images (<xref ref-type="bibr" rid="c5">Böhm et al., 2023</xref>). The model is trained using a contrastive objective: embeddings of augmented versions of the same image are attracted to each other, while embeddings of distinct images are repelled (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>). The data augmentations used to generate image variants determine which transformations the model learns to ignore as irrelevant differences. By using random rotations, shifts, and scaling as augmentations, we ensured that MEIs differing only by these geometric transformations would cluster together in the embedding space (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>) We chose these augmentations because they generally preserve the identity of many midand high-level image features: for instance, an eye remains an eye even after rotating, shifting or scaling. Moreover, we observed that multiple MEIs of the same neuron, generated by starting from different initial noise images during optimization, often exhibited variations in some or all of these dimensions (<xref rid="figS2" ref-type="fig">Suppl. Fig. 2</xref>). Since the training of the model is exclusively based on image identity, it does not provide any information about which recording session a particular MEI originated from. Therefore, any grouping or clustering of MEIs by session in the learned embedding space reflects only their visual similarity and not any explicit experimental labels</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig. 4.</label>
<caption><title>Contrastive clustering of MEIs confirms topographic organization of V4 visual tuning selectivity</title>
<p><bold>a</bold>, Schematic illustrating contrastive learning approach to quantify MEI similarity. Per neuron, we optimize n=50 MEIs initialized with different random seeds, then select highly activating MEIs, and use one MEI per neuron (n=889) as a training batch. Each MEI is transformed and augmented twice and the model’s objective then is to minimize the distance in a 2D MEI similarity space between different transforms of the same MEI, while maximizing the distance to MEI transforms of other neurons. <bold>b</bold>, Position of all highly activating MEIs (n=19688) of n=889 neurons in a 2D MEI similarity space, with MEIs of five example neurons indicated in different colors. Dots of the same color indicate MEIs optimized from different random seeds of the same neuron. <bold>c</bold>, Schematic illustrating analyses performed on the 2D MEI similarity space. We computed the pairwise 2D distances across all MEIs of one neuron to estimate MEI consistency (left), and all pairwise distances across MEIs of the same recording session to estimate recording session consistency (right). For the latter, we used the distances across a random selection of neurons from other sessions as control. <bold>d</bold>, Distribution of embedding-distances across MEIs of the same neuron. Vertical dotted line indicates mean of the distribution. <bold>e</bold>, Mean distance across neurons from one example session (vertical line), with a null distribution generated by bootstrapping distances across the same number of neurons randomly sampled from all other sessions. Orange shading indicates values &lt;5% percentile. Note that the null distribution depends on how many neurons were recorded in each session, as it estimates the standard error of the mean for each session. <bold>f</bold>, Histogram of session means like in e), but for all sessions. Grand mean across all sessions is indicated by the vertical line. Mean = 423.97 <italic>±</italic> 105.20 s.d. <bold>g</bold>, Mean within-session distance across all sessions from f) along with the mean null distribution across sessions in gray. The population mean significantly deviates from the null distribution (<italic>p &lt;</italic> 4 <italic>×</italic> 10<sup><italic>−</italic>5</sup>; 25,000 bootstrap samples). Orange shading indicates values &lt;5% percentile. <bold>h</bold>, Percentage of sessions with the within-session distance &lt;5% percentile of the null distribution for different numbers of neurons per session (x-axis) and different model predictions thresholds (shades of gray). The percentiles obtained from the embedding space (including all neurons above a prediction threshold of 0.3) were significantly correlated with the observer agreement (percent correct) of the psychophysics experiment (<italic>ρ</italic> = <italic>−</italic>0.33, p=.019, n=50 sessions).</p></caption>
<graphic xlink:href="540591v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The resulting embedding space placed neurons with similar MEIs close to each other (neurons 1 and 3 in <xref rid="fig4" ref-type="fig">Fig. 4b</xref>) and neurons with different MEI features far away (neurons 1 and 2). Similarly, multiple MEIs of the same neuron, generated by starting from different initial noise images during optimization, were placed nearby in the embedding space as well (groups of the same color in <xref rid="fig4" ref-type="fig">Fig. 4b</xref> and <xref rid="fig4" ref-type="fig">Fig. 4c,d</xref>). These observations suggest that the model indeed learned to embed MEIs based on image similarity. We next quantified whether neurons recorded within one session share tuning properties as suggested by the observed MEI similarity within sessions (cf. <xref rid="fig3" ref-type="fig">Fig. 3</xref>). To this end, we computed the mean pairwise distance in the embedding space across neurons from one session and compared it to a null distribution of distances obtained by computing the mean pairwise distance across randomly picked neurons from different sessions (<xref rid="fig4" ref-type="fig">Fig. 4c,e</xref>). While the within-session distance varied across sessions (<xref rid="fig4" ref-type="fig">Fig. 4f</xref>), on a population level, it was significantly smaller than the across-session distance (<xref rid="fig4" ref-type="fig">Fig. 4g</xref>). The percentage of sessions that showed a significantly smaller distance in MEI similarity than the null distribution increased with higher numbers of neurons recorded per session and with higher prediction performance of the model (<xref rid="fig4" ref-type="fig">Fig. 4h</xref>). For more than 12 neurons per session and with the highest performance threshold (correlation to average &gt;0.4), half of the sessions displayed a significantly smaller withinthan across-session distance, indicative for high similarity of MEIs. Importantly, the within-session distances estimated based on the embedding space significantly correlated with the observer agreement (percent of observers who correctly classified a specific session) from the psychophysics results (<italic>ρ</italic> = <italic>−</italic>.33, p=.019), suggesting that MEI distance in the embedding space is informative about MEI perceptual similarity. We additionally confirmed that MEIs of neurons recorded within one session are more similar to each other than to MEIs of neurons recorded in other sessions using an independent similarity metric, namely the representational similarity of MEIs in neuronal response space, which closely mimics perceptual similarity (<xref ref-type="bibr" rid="c52">Kriegeskorte, 2008</xref>) (<xref rid="figS3" ref-type="fig">Suppl. Fig. 3</xref>). Taken together, these analyses demonstrate consistent functional clustering of tuning properties among V4 neurons recorded along the same cortical penetration. This pattern is consistent with a topographic or columnar-like organization of feature selectivity across cortical depth, as inferred from modelderived representations.</p>
</sec>
<sec id="s2d">
<title>V4 neurons cluster into distinct functional groups that resemble feature maps of artificial vision systems</title>
<p>At the level of retinal ganglion cells, neurons cluster into specific functional groups or output channels (<xref ref-type="bibr" rid="c1">Baden et al., 2016</xref>; <xref ref-type="bibr" rid="c31">Goetz et al., 2022</xref>). Whether a similar functional clustering persists for cortical neurons is still an open question. For instance, in V1 it is not clear whether the distinction between simple and complex cells represents two ends of a continuous spectrum or two discrete categories (<xref ref-type="bibr" rid="c61">Mechler &amp; Ringach, 2002</xref>). Previous results in mouse primary visual cortex suggest that neurons cluster according to function, but not in an entirely discrete manner (<xref ref-type="bibr" rid="c90">Ustyuzhaninov et al., 2019</xref>). Motivated by the structure of the MEI embedding space that exhibited isolated “islands” of MEIs (cf. <xref rid="fig4" ref-type="fig">Fig. 4b</xref>), we asked whether tuning properties of V4 neurons fell into similar functional groups, characterized by their preferred stimulus. To address this question, we applied hierarchical clustering (DBSCAN; <xref ref-type="bibr" rid="c60">McInnes et al., 2017</xref>) to the two-dimensional MEI embeddings, yielding 17 distinct functional groups (<xref rid="fig5" ref-type="fig">Fig. 5</xref>). MEIs within each group exhibited high perceptual similarity and shared stimulus preferences—for instance, neurons in group 11 all responded preferentially to patterns resembling eyes. Conversely, neurons in different groups showed markedly different feature selectivity, particularly for groups positioned distantly in embedding space. For example, while group 11 MEIs contained eye-resembling patterns, group 3 and group 8 MEIs displayed grid-like and comb-like textures, respectively</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig. 5.</label>
<caption><title>V4 neurons cluster into distinct response modes that resemble feature maps of artificial vision systems</title>
<p><bold>a</bold>, Position of all highly activating MEIs (n=19,688) of n=889 neurons in the 2D MEI similarity space, color coded based on cluster assignment obtained from the hierarchical clustering algorithm HDBSCAN. For n=12 clusters, we show a random selection of MEIs of different neurons assigned to this cluster. For examples of the other clusters, see <xref ref-type="fig" rid="figS4">Suppl. Fig. 4</xref> and for independent verification of the clusters, see <xref ref-type="fig" rid="figS3">Suppl. Fig. 3</xref>. Light gray dots indicate MEIs that could not be assigned to any of the clusters with high probability. <bold>b</bold>, Feature visualizations of early- to mid-level units in the deep neural network InceptionV1 trained in an image classification task (<xref ref-type="bibr" rid="c69">Olah et al., 2020</xref>). Units are grouped into distinct categories based on (<xref ref-type="bibr" rid="c69">Olah et al., 2020</xref>), with clusters from (a) resembling these categories indicated below. <bold>c</bold>, Example units of the neural network trained on image classification compared with example MEIs exhibiting similar spatial patterns. The resemblance between the two can be used to generate hypotheses, such as to predict color boundary encoding in primate V4 neurons, that can be subsequently tested experimentally.</p></caption>
<graphic xlink:href="540591v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To ensure that the group structure in the embedding space is not an artifact of the contrastive neighborhood embedding method, we additionally used an independent method to compare within-group to across-group similarities. To that end, we computed the representational similarity of the MEIs using the predicted neuronal responses from our model. Specifically, we centered the receptive fields of all neurons in the model and compared the similarity of two MEIs via the cosine of the predicted population response vectors. Importantly, this similarity metric is unrelated to the embedding space used for clustering and, therefore, provides an independent verification of the identified functional groups. We found that MEIs of neurons assigned to the same group were significantly closer to each other in the neuronal response space than MEIs of neurons assigned to different groups (<xref rid="figS3" ref-type="fig">Suppl. Fig. 3</xref>): The withingroup similarity in neuronal response space was significantly higher than the across-group similarity for all of the 17 functional groups.</p>
<p>Interestingly, the MEIs of the identified V4 clusters closely resembled the feature visualizations of single units in modern deep neural networks trained on image recognition tasks. For example, a similar preference for specific complex features can be found in the early layers, specifically layer mixed3a, of the InceptionV1 deep network (<xref ref-type="bibr" rid="c69">Olah et al., 2020</xref>; <xref ref-type="bibr" rid="c84">Szegedy et al., 2015</xref>). This alignment between V4 and deep network features is in line with previous results that found boundary selective units in the AlexNet deep network (<xref ref-type="bibr" rid="c53">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="c77">Pospisil et al., 2018</xref>), similar to boundary neurons in monkey V4 (<xref ref-type="bibr" rid="c72">Pasupathy &amp; Connor, 2002</xref>). <xref ref-type="bibr" rid="c69">Olah et al. (2020)</xref> manually grouped the feature visualizations into different categories like “Oriented fur,” “Eyes/circles/loops” and “Divots/boundaries” (<xref rid="fig5" ref-type="fig">Fig. 5b</xref>). These categories exhibit perceptual similarities to the functional groups of V4 neurons identified through hierarchical clustering, suggesting potential convergence in encoding strategies between primate and artificial vision systems. The correspondence between V4 neuronal selectivity and deep neural network features provides a basis for generating hypotheses about visual tuning properties in primate V4 beyond spatial patterns. For example, given that the artificial vision systems were trained on color images, their feature visualizations also characterize color tuning and may inform predictions about color boundary encoding in monkey V4 functional groups (<xref rid="fig5" ref-type="fig">Fig. 5c</xref>), amenable to verification through <italic>in vivo</italic> experiments. These comparisons highlight how AI models and neuroscience can inform each other, with artificial representations guiding hypotheses while neural data constrain and refine computational models.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Our work provides evidence for a topographic and columnar-like organization of tuning to spatial patterns in visual area V4, derived from detailed single-neuron tuning properties. This organization was revealed using an iterative image synthesis approach based on a deep neural network model of neuronal activity. This model, serving as a digital twin of V4, enabled characterization of spatial pattern tuning without imposing prior parametric assumptions about feature selectivity. By combining this approach with a learned non-linear image-similarity embedding space and human psychophysics, we found that MEIs from neurons recorded along the same cortical penetration tended to be more similar to each other than those from randomly selected neurons, and that V4 neurons formed functionally coherent clusters in the embedding space. Together, these findings indicate that despite the complexity of V4 stimulus preferences, feature selectivity is locally clustered in a manner consistent with columnar-like or topographic organization.</p>
<sec id="s3a">
<title>Are our MEIs consistent with previous findings?</title>
<p>The idea to use neuronal encoding models to synthesize optimal stimuli for the brain is well established (<xref ref-type="bibr" rid="c54">Lehky et al., 1992</xref>) and has already successfully been used in mouse primary visual cortex (<xref ref-type="bibr" rid="c26">Franke et al., 2022</xref>; <xref ref-type="bibr" rid="c94">Walker et al., 2019</xref>), mouse retina (<xref ref-type="bibr" rid="c38">Höfling et al., 2022</xref>), and also macaque V4 (<xref ref-type="bibr" rid="c3">Bashivan et al., 2019</xref>). Additionally, there exist deep learning-based methods for synthesizing optimal stimuli which do not rely on neuronal encoding predictive models. Instead, these methods utilize a genetic algorithm to search through the space of a generative deep neural network of images <bold>?</bold>. Many of our MEIs contain features, such as different types of textures, that qualitatively resembled those found by <xref ref-type="bibr" rid="c3">Bashivan et al. (2019)</xref>. However, some of our MEIs showed shape-like features, including curved strokes, corners, and even higher-level attributes such as individual eye-like stimuli (cluster 7 in <xref rid="fig5" ref-type="fig">Fig. 5a</xref>), which have been less emphasized in previous work.</p>
<p>One distinction between our study and that of <xref ref-type="bibr" rid="c3">Bashivan et al. (2019)</xref> is that we used silicon probes, enabling us to separate spikes from individual neurons, while they employed chronically implanted Utah arrays and optimized stimuli for single ‘sites’ (multi-units) that likely comprise a mix of multiple neurons. Consequently, it remains unclear from their study whether the complexity of the MEIs stems from the mixing of spikes from multiple neurons or if single neurons already prefer such intricate spatial patterns as displayed by the MEIs. Additionally, MEIs for multi-unit activity could average out specific features like shapes. By isolating and verifying MEIs at single-cell resolution, our results demonstrate that the diversity and complexity of optimal stimuli are already present at the level of individual neurons in macaque V4.</p>
<p>Our MEIs are also consistent with previously described tuning properties of V4 neurons. V4 is part of the ventral pathway which plays a major role in object and shape recognition (<xref ref-type="bibr" rid="c25">Felleman &amp; Van Essen, 1991</xref>; <xref ref-type="bibr" rid="c63">Mishkin et al., 1983</xref>). Previous studies have shown that V4 cells are selective for complex shapes (<xref ref-type="bibr" rid="c50">Kobatake &amp; Tanaka, 1994</xref>), and be tuned to convex and concave shapes of object boundaries at specific locations in the visual field (<xref ref-type="bibr" rid="c28">Gallant et al., 1993</xref>; <xref ref-type="bibr" rid="c71">Pasupathy &amp; Connor, 2001</xref>). In addition to shape, V4 neurons are also known to be selective to texture (<xref ref-type="bibr" rid="c50">Kobatake &amp; Tanaka, 1994</xref>). <xref ref-type="bibr" rid="c46">Kim et al. (2019)</xref> found that tuning of single V4 neurons can be placed along a continuum from strong tuning for boundary curvature of shapes to strong tuning for perceptual dimensions of texture. Consistent with these findings, our MEIs captured a wide range of feature preferences. On the one hand, we find MEIs that clearly exhibit curvature elements (e.g. cluster 7 in <xref rid="fig5" ref-type="fig">Fig. 5a</xref>) or ‘eye’-like elements (cluster 1). On the other hand, many MEIs have a texture component such as ‘fur’ (cluster 8), dots (cluster 11), or grid-like elements (cluster 6). Interestingly, generating multiple MEIs from different starting points – known as Diverse Exciting Images (DEIs; <xref ref-type="bibr" rid="c8">Cadena et al., 2018</xref>; <xref ref-type="bibr" rid="c20">Ding et al., 2023b</xref>) – resulted in multiple MEIs that had similar shape and texture features (<xref rid="figS2" ref-type="fig">Suppl. Fig. 2</xref>), indicating that single cells in V4 are neither texturenor shape-invariant. Together, our findings extend previous work by providing a single-cell, data-driven view of selectivity in V4 under naturalistic conditions.</p>
</sec>
<sec id="s3b">
<title>Topological organization of MEIs in V4</title>
<p>One advantage of our recordings is that we can record simultaneously across cortical layers using silicon probes. This enabled us to characterize the vertical organization of the tuning selectivity to complex spatial patterns in area V4 and to assess whether tuning similarity is consistent with columnar or topographic organization. The presence of columns in V4 has been a matter of debate and controversy, predominantly studied in the domain of color (<xref ref-type="bibr" rid="c51">Kotake et al., 2009</xref>) or orientation (<xref ref-type="bibr" rid="c29">Ghose &amp; Ts’o, 1997</xref>) because stimuli for these domains are more accessible to low dimensional parametrization. For instance, in the color domain, some studies has reported columnar organization (<xref ref-type="bibr" rid="c16">Conway &amp; Tsao, 2009</xref>; <xref ref-type="bibr" rid="c51">Kotake et al., 2009</xref>; <xref ref-type="bibr" rid="c87">Tanigawa et al., 2010</xref>), while another found no evidence (<xref ref-type="bibr" rid="c85">Tanaka et al., 1986</xref>). Beyond that, other studies using natural images or parametric stimuli for curvature found local clustering of similarly tuned neurons (<xref ref-type="bibr" rid="c36">Hatanaka et al., 2022</xref>; <xref ref-type="bibr" rid="c86">Tang et al., 2020</xref>; <xref ref-type="bibr" rid="c87">Tanigawa et al., 2010</xref>; <xref ref-type="bibr" rid="c95">Wang et al., 2024</xref>). However, since these studies focused mainly on neurons from superficial layers, it remains unknown whether such topographic clustering extends across cortical depth.</p>
<p>Moreover, it is well-established that V4 neurons exhibit selectivity for complex spatial features (<xref ref-type="bibr" rid="c70">Oliver et al., 2024</xref>; <xref ref-type="bibr" rid="c95">Wang et al., 2024</xref>) that extend beyond those characterized by parametric stimuli defined by orientation, or parameterized curvature (e.g. <xref ref-type="bibr" rid="c86">Tang et al., 2020</xref>). Consequently, in the absence of a detailed identification of the non-parametric optimal stimuli, assessing the hypothesis of functional columns proves to be challenging. For example, if neurons are tuned to similar grid-like textures but with different orientations, using grating stimuli (<xref ref-type="bibr" rid="c29">Ghose &amp; Ts’o, 1997</xref>) or predetermined parameterized curvature stimuli (<xref ref-type="bibr" rid="c86">Tang et al., 2020</xref>) will obscure the true underlying organization. Our deep learning based image synthesis method avoids these challenges by identifying most exciting stimuli in the high-dimensional pixel space.</p>
<p>To compare MEIs regardless of their spatial complexity, we used human psychophysics and deep learning techniques to assess the similarity between them, specifically employing contrastive learning to create a non-linear embedding based on similarity among MEIs (<xref ref-type="bibr" rid="c5">Böhm et al., 2023</xref>). We discovered that neurons across layers recorded in orthogonal penetrations to the cortical surface had MEIs that were perceptually more similar than those from randomly sampled neurons. However, the strength of this clustering effect varied across recording sessions, and was not equally evident in all cases This variability in effect size could have been caused by several factors. First, despite best efforts, our electrode penetrations may not have been perfectly orthogonal to the cortical surface, because the electrodes were aligned relative to the recording chamber. The recording chamber was implanted such that its center was orthogonal to the surface of the cortex. Since the cortex is curved, penetrations further away from the center may not have been perfectly orthogonal to the cortical surface. The brain may also have moved or have been slightly compressed during insertion, potentially resulting in slightly angled penetrations. Second, if there is a topographical organization in V4, it may feature both homogeneous zones and regions with more mixed selectivity, analogous to pinwheels in orientation maps in V1. Thus, we expect a certain fraction of penetrations to be close to such heterogeneous zones. Because extracellular electrodes record the activity of neurons in a roughly cylindrical region around the electrode, we expect a fraction of penetrations to exhibit mixed tuning even if the penetrations were perfectly vertical. Estimating what fraction of penetrations should exhibit consistent tuning is difficult, because the size of the columns and the relationships between them are not yet understood. we estimate that approximately half of the penetrations show a statistically significant bias toward certain feature selectivities, consistent with a topographic organization. While our findings point toward a columnar-like organization of functional selectivity in V4, this interpretation contrasts with recent Neuropixels recordings that revealed sparse, non-columnar clusters for shape and texture tuning (<xref ref-type="bibr" rid="c66">Namima et al., 2025</xref>). One possibility is that such discrepancies arise from differences in stimulus complexity, suggesting that columnar organization in V4 may depend on the specific feature space analyzed.</p>
<p>To more accurately delineate and map the topological organization of spatial form tuning in V4, future studies need to combine recording techniques like two-photon functional imaging with deep learning and inception loop approaches. To this end, a parallel study <xref ref-type="bibr" rid="c95">Wang et al. (2024)</xref> used wide-field and two-photon calcium imaging across the surface of V4 and identified a topographical map of natural stimulus preference. Their 2D map contained distinct (clustered) functional domains preferring a variety of natural image features, ranging from surface-related features such as color and texture to shape-related features such as edge, curvature, and facial features — reflecting the MEI features we identified. Although Wang and colleagues did not study the columnar organization of V4, our combined results provide evidence for the hypothesis that the functional cortical columns we find are meticulously arranged into a cortical map.</p>
</sec>
<sec id="s3c">
<title>Similarity to tuning in deep networks</title>
<p>The MEIs for V4 neurons visually resemble MEIs of units in deep artificial neural networks trained on image recognition tasks. While there are differences between biological and artificial vision (reviewed in <xref ref-type="bibr" rid="c82">Sinz et al., 2019</xref>), deep networks trained on large-scale vision tasks are the closest human engineered system to biological visual system we know. Several previous works have found similarities between the primate visual system and deep network representations of visual stimuli (<xref ref-type="bibr" rid="c32">Güçlü &amp; van Gerven, 2015</xref>; <xref ref-type="bibr" rid="c45">Khaligh-Razavi &amp; Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="c99">Yamins &amp; DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="c100">Yamins et al., 2014</xref>) and this similarity has recently also been shown in mice <xref ref-type="bibr" rid="c2">Bakhtiari et al. (2021)</xref>; <xref ref-type="bibr" rid="c67">Nayebi et al. (2021)</xref>. On a single neuron level, previous work has pointed out similarities between tuning in early vision and selectivities of single units in deep networks (<xref ref-type="bibr" rid="c53">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="c69">Olah et al., 2020</xref>; <xref ref-type="bibr" rid="c102">Zeiler &amp; Fergus, 2014</xref>) and a recent investigation has shown that single units in deep networks can exhibit similar object boundary tuning as V4 neurons (<xref ref-type="bibr" rid="c77">Pospisil et al., 2018</xref>). The striking similarity between our single cell MEIs of V4 neurons and single units in the InceptionV1 architecture (<xref ref-type="bibr" rid="c69">Olah et al., 2020</xref>) provide an even stronger case for similarities in tuning in primate early vision and deep networks. What’s more, by using this similarity we can derive predictions about the color selectivity of V4 neurons despite having shown only grayscale images in the experiment and during model training (<xref rid="fig5" ref-type="fig">Fig. 5b</xref>).</p>
</sec>
<sec id="s3d">
<title>Future directions</title>
<p>Our research highlights the power of applying deep learning to comprehensively characterize neuronal representations and to elucidate the relationships between brain structure and function. The concept of cortical columns offers an appealing framework for understanding cortical computation, as it decomposes complex processing into smaller, repeated building blocks. Building on prior evidence for columnar and clustered organization in sensory cortex, our findings support the hypothesis that groups of functionally similar neurons in the mid-level visual area V4 exhibit local clustering across cortical depth. Because neuronal connections within a cortical area tend to be locally dense <xref ref-type="bibr" rid="c30">Gilbert &amp; Wiesel (1989)</xref>, neighboring neurons within a column—both within and across layers— are much more likely to be interconnected. The purpose of a columnar architecture, therefore, is not to impose strict spatial boundaries between neurons, but rather to enhance synaptic opportunities among neurons that share similar response properties. Such local connectivity facilitates circuit computations that may underlie the emergence of new types of invariances or the construction of increasingly complex feature selectivity.</p>
<p>In this study, we focused on the most activating stimuli and on tuning similarities among neurons recorded along the same probe. However, recent work has extended this approach to include both the most and least activating stimuli, revealing that many visual cortical neurons encode information bidirectionally—as a contrast between two distinct feature types—rather than purely through excitation <bold>?</bold>. This bidirectional framework suggests that structured excitation and suppression jointly define functional micro-circuits, raising the intriguing possibility that such dual-feature coding may also manifest within columnar organizations in higher visual areas.</p>
<p>A comprehensive understanding of local cortical organization will therefore require characterizing full tuning functions—including invariances and the diversity of encoded features among nearby neurons. Future work combining large-scale functional recordings with synapticresolution connectomics <xref ref-type="bibr" rid="c4">Bock et al. (2011)</xref>; <xref ref-type="bibr" rid="c15">Consortium et al. (2021)</xref>; <xref ref-type="bibr" rid="c19">Ding et al. (2023a)</xref>; <xref ref-type="bibr" rid="c78">Reid (2012)</xref> will be essential for clarifying how local circuits implement these computations.</p>
</sec>
</sec>
<sec id="s5">
<title>Materials and Methods</title>
<sec id="s5a" sec-type="ethics-statement">
<title>Ethics statement</title>
<p>Electrophysiological data were gathered from a pair of healthy male rhesus macaque monkeys (<italic>Macaca mulatta</italic>), aged 17 and 19 years, and weighing 16.4 and 10.5 kg, respectively, at the time of the study. The research adhered to NIH guidelines and received approval from the Institutional Animal Care and Use Committee at Baylor College of Medicine (permit number: AN-4367). The monkeys were individually housed in a spacious room near the training facility, in the company of approximately ten other monkeys, allowing for abundant visual, olfactory, and auditory interactions. They were maintained on a 12- hour light/dark cycle.</p>
<p>The Center for Comparative Medicine at Baylor College of Medicine ensured that the monkeys received regular veterinary check-ups, balanced nutrition, and environmental enrichment. Surgical procedures involving the monkeys were performed using general anesthesia and adhering to standard aseptic techniques. Postoperative pain was managed with analgesics for seven days.</p>
</sec>
<sec id="s5b">
<title>Electrophysiological recordings</title>
<p>Non-chronic recordings were conducted using a 32-channel linear silicon probe (NeuroNexus V1x32-Edge-10mm-60-177), with surgical methods and recording protocols previously outlined (<xref ref-type="bibr" rid="c17">Denfield et al., 2018</xref>). In summary, custom titanium recording chambers and head posts were implanted under complete anesthesia and sterile conditions. Initially, the bone remained unaltered, and only before recordings were small trephinations (2 mm) made over lateral V4, with eccentricities spanning from 1.7 to 18.3 degrees of visual angle. Recordings took place after a week of each trephination. A Narishige Microdrive (MO-97) and guide tube were used to carefully lower the probes, penetrating the dura.</p>
</sec>
<sec id="s5c">
<title>Data acquisition and spike sorting</title>
<p>Electrophysiological data were continuously collected as a broadband signal (0.5Hz–16kHz), digitized at 24 bits. The spike sorting methods employed in this study resemble those used in (<xref ref-type="bibr" rid="c7">Cadena et al., 2019</xref>; <xref ref-type="bibr" rid="c17">Denfield et al., 2018</xref>), with the code accessible at <ext-link ext-link-type="uri" xlink:href="https://github.com/aecker/moksm">https://github.com/aecker/moksm</ext-link>. The linear array of 32 channels was divided into 14 groups, each containing six neighboring channels (with a stride of two), which were treated as virtual electrodes for spike detection and sorting. Spikes were identified when channel signals exceeded a threshold equal to five times the standard deviation of the noise.</p>
<p>Following spike alignment, the first three principal components of each channel were extracted, resulting in an 18-dimensional feature space utilized for spike sorting. A Kalman filter mixture model was fitted to monitor waveform drift, which is common in non-chronic recordings (<xref ref-type="bibr" rid="c11">Calabrese &amp; Paninski, 2011</xref>; <xref ref-type="bibr" rid="c81">Shan et al., 2017</xref>). Each cluster’s shape was modeled using a multivariate t-distribution (degrees of freedom = 5) with a ridge-regularized covariance matrix. The cluster count was determined based on a penalized average likelihood with a constant cost for each additional cluster (<xref ref-type="bibr" rid="c21">Ecker et al., 2014</xref>).</p>
<p>Lastly, a custom graphical user interface was used to manually confirm single-unit isolation by evaluating the units’ stability (based on drifts and cell health throughout the session), identifying a refractory period, and inspecting scatter plots of channel principal component pairs.</p>
</sec>
<sec id="s5d">
<title>Visual stimulation and eye tracking</title>
<p>Visual stimuli were generated by a specialized graphics workstation and presented on a 16:9 HD widescreen LCD monitor (23.8”) with a 100 Hz refresh rate and a resolution of 1920 × 1080 pixels, positioned at a 100 cm viewing distance (yielding approximately ∼63<italic>px/</italic><sup><italic>°</italic></sup>). The monitor underwent gamma correction to ensure a linear luminance response profile. A custom-made, camera-based eye tracking system confirmed that monkeys kept their gaze within roughly ∼0.95<italic>°</italic> around a ∼0.15<italic>°</italic>-sized red fixation target. Offline evaluations revealed that the monkeys typically fixated with greater accuracy.</p>
<p>Upon maintaining fixation for 300 ms, a visual stimulus was displayed. If the monkeys sustained their gaze throughout the entire trial duration (1.8 s), they were rewarded with a drop of juice at the end of the trial.</p>
</sec>
<sec id="s5e">
<title>Receptive field mapping and stimulus placing</title>
<p>At the start of each session, we determined receptive fields in relation to a fixation target using a sparse random dot stimulus. A solitary dot, spanning 1<italic>°</italic> of the visual field, was displayed on a uniform gray background, with its location and polarity (black or white) randomly changing every 30 ms. Each fixation trial persisted for two seconds. Multi-unit receptive field profiles for each channel were obtained through reverse correlation. The population receptive field location was estimated by fitting a 2D Gaussian to the spike-triggered average across channels at the time lag that optimized the signal-to-noise ratio.</p>
<p>The natural image stimulus occupied the entire screen. The fixation spot was adjusted so that the mean of the population receptive field was as close to the screen’s center as feasible. Due to the recording sites’ location in both monkeys, this positioning involved placing the fixation spot near the screen’s upper border, shifted to the left.</p>
</sec>
<sec id="s5f">
<title>Natural image stimuli</title>
<p>We selected a collection of 24,075 images from 964 categories (∼25 images per category) from ImageNet (<xref ref-type="bibr" rid="c18">Deng et al., 2009</xref>), transformed them to grayscale, and cropped the central 420 × 420 pixels. For images that were smaller than 420 × 420, a central crop was taken and the resulting image was re-scaled to 420 × 420. Each image had an 8-bit intensity resolution (values ranging from 0 to 255). From this set, we randomly chose 75 images as our <italic>test-set</italic>. Out of the remaining 24,000 images, we designated 20% as <italic>validation-set</italic> at random, leaving 19,200 images in the <italic>train-set</italic>. Natural images were displayed during the standalone generation recordings of 1,244 units and during the generation phase of closed-loop recordings for 82 units. Specifically, <italic>∼</italic> 12k unique <italic>train-set</italic> images were displayed during the standalone generation recordings, and ∼7.5k unique <italic>trainset</italic> images were displayed during the generation phase of closed-loop recordings. Across sessions, train images were randomly sampled from the <italic>train-set</italic> such that the full set was exhausted before cycling back through the same images. The 75 <italic>test-set</italic> images were displayed in every recording session. Note that selecting images from ImageNet means that the pre-trained convolutional network (see below) has likely seen our natural stimuli (but not the neuronal responses) during training on the classification task.</p>
<p>During standalone generation recording sessions, ∼1000 successful trials (∼12k train images and 75 repeated test images) were recorded, whereas 600 successful trials (∼7.5k train images and 75 repeated test images) were recorded during the generation phase of closed-loop experiments. In both instances, each trial involved continuous fixation for 2.4 seconds, which includes 300 ms of a gray screen (intensity 128) at the beginning and end of the trial, as well as 15 consecutive images displayed for 120 ms each without any gaps. Trials contained either training/validation images that were presented only once or test images, which were repeated during the experiment.</p>
<p>Throughout the recording session, all trails were randomly interleaved, with test images being repeated 40-50 times during standalone generation recordings and 20 times during the generation phase of closed-loop recordings. Training and validation images were sampled without replacement, so each image was effectively displayed once or not at all. Images were upscaled using bicubic interpolation to match the screen width (1920 pixels) while maintaining their aspect ratio. The upper and lower 420-pixel bands were cropped out to cover the entire screen, effectively stimulating both classical and beyond classical receptive fields of V4 neurons. After sorting the neurons, spikes associated with each image presentation were counted within a 70-160 ms time window after stimulus onset.</p>
</sec>
<sec id="s6">
<title>Image preprocessing for model training</title>
<p>Starting out from an original image size of 420 × 420 with a resolution of 14px<italic>/</italic><sup><italic>°</italic></sup> we cropped the upper and lower bands to fit the full screen for presentation for a resulting image size of 420 × 236. We then cropped the images so that only the bottom center 200 200 pixels remained. Then, we down-sampled the images to either 80 × 80 or 100 × 100 pixels (5.8px<italic>/</italic><sup><italic>°</italic></sup> or 7px<italic>/</italic><sup><italic>°</italic></sup>), for the closed-loop model training and non closed-loop model training, respectively.</p>
<sec id="s6a">
<title>Model architecture</title>
<p>Our neural predictive model of primate V4 consisted of two main parts: A pretrained <italic>core</italic> that computes nonlinear features of input images, and a <italic>Gaussian readout</italic> (<xref ref-type="bibr" rid="c57">Lurz et al., 2020</xref>) that maps these features to the neuronal responses of the single neurons.</p>
<p>As the core of our model, we used a ResNet50 (<xref ref-type="bibr" rid="c37">He et al., 2016</xref>) which was adversarially trained on ImageNet (<xref ref-type="bibr" rid="c18">Deng et al., 2009</xref>) to have robust visual representations (<xref ref-type="bibr" rid="c80">Salman et al., 2020</xref>), which yields improved transfer-learning performance (<xref ref-type="bibr" rid="c22">Engstrom et al., 2019a</xref>,b; <xref ref-type="bibr" rid="c58">Madry et al., 2017</xref>). Interestingly, it has been previously shown that robust features not only allow for better transfer-learning but they appear to be more similar to biological networks and also improve neural predictivity (<xref ref-type="bibr" rid="c24">Feather et al., 2022</xref>; <xref ref-type="bibr" rid="c33">Guo et al., 2022</xref>; <xref ref-type="bibr" rid="c55">Li et al., 2019</xref>; <xref ref-type="bibr" rid="c79">Safarani et al., 2021</xref>). Building on previous work (<xref ref-type="bibr" rid="c9">Cadena et al., 2022</xref>), we selected the first residual block of layer 3 of the ResNet, layer3.0, to read out from, and found that the adversarially robust training with <italic>ϵ</italic> = 0.1 yielded the highest predictive performance, compared to all other ResNet models and layers. The corresponding size of the output feature map at layer layer3.0 was 1024.</p>
<p>The input images <bold>x</bold> were forwarded through all layers up to a selected layer, to output a tensor of feature maps. Importantly, the parameters of the pretrained network were always kept fixed. We then applied batch-normalization (<xref ref-type="bibr" rid="c42">Ioffe &amp; Szegedy, 2015</xref>). Lastly, we rectified the resulting tensor with a ReLU unit to obtain the final nonlinear feature space Φ(<bold>x</bold>) ∈ ℝ<sup><italic>w×h×c</italic></sup> (<bold>w</bold>idth, <bold>h</bold>eight, <bold>c</bold>hannels) shared by all neurons.</p>
<p>To predict the response of a single neuron from the Φ(<bold>x</bold>) ∈ ℝ<sup><italic>w×h×c</italic></sup> we use a <italic>Gaussian readout</italic> (<xref ref-type="bibr" rid="c57">Lurz et al., 2020</xref>). For each neuron <italic>n</italic>, this readout learns the coordinates (<italic>x</italic><sup>(<italic>n</italic>)</sup>, <italic>y</italic><sup>(<italic>n</italic>)</sup>) of the position of the receptive field on the output tensor and extracts a feature vector <inline-formula id="inline-eqn-1"><inline-graphic xlink:href="540591v2_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> at this location from Φ. To this end, the Gaussian readout learns the parameters of a 2D Gaussian distribution 𝒩 (<italic>µ</italic><sub><italic>n</italic></sub>, Σ<sub><italic>n</italic></sub>) and samples a location in feature space Φ(<bold>x</bold>) during each training step for every neuron <italic>n</italic>. Σ<sub><italic>n</italic></sub> is initialized large enough to ensure that the entire visual field can be covered, and then decreases in size during training to have a more reliable estimate of the mean location <italic>µ</italic><sub><italic>n</italic></sub>. At inference time (i.e. when evaluating our model), the readout is deterministic and uses the fixed position <italic>µ</italic><sub><italic>n</italic></sub>. Although this framework allows for rotated and elongated Gaussian functions, we found that for our data, an isotropic formulation of the covariance – parametrized by a single scalar <inline-formula id="inline-eqn-2"><inline-graphic xlink:href="540591v2_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> – was performing equally well as compared to a fully parametrized Gaussian. Taken together, total number of parameters per neuron of the readout were <italic>c</italic> + 4 (number of channels, bivariate mean, variance, and bias). This extracted feature vector <inline-formula id="inline-eqn-3"><inline-graphic xlink:href="540591v2_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is then used in a linear-nonlinear model to predict the neuronal response. To this end, an affine function of the resulting feature vector at the chosen location was computed, followed by a rectifying nonlinearity <italic>f</italic>, chosen to be an ELU (<xref ref-type="bibr" rid="c14">Clevert et al., 2015</xref>) offset by one (ELU + 1) to make responses positive (<xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>). The weight vector <bold>w</bold><sub><italic>n</italic></sub> ∈ ℝ<sup><italic>c</italic></sup> was <italic>L</italic><sub>1</sub> regularized during training.
<disp-formula id="eqn1">
<graphic xlink:href="540591v2_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s6b">
<title>Model training</title>
<p>We trained our model to minimize the summed Poisson loss across <italic>N</italic> neurons between observed spike counts <italic>r</italic> and the models’ predicted spike counts <inline-formula id="inline-eqn-4"><inline-graphic xlink:href="540591v2_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in addition to the <italic>L</italic><sub>1</sub> regularization of the readout parameters.
<disp-formula id="eqn2">
<graphic xlink:href="540591v2_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We trained the models either on the full dataset of n=100 recording sessions with n=1244 neurons and an image size of 100 by 100 pixels, or on an individual sessions during a closed loop recording with a reduced image size of 80 by 80 pixels in order to save time with model training and stimulus generation. For training on the full dataset, an epoch consisted of the cycling through the whole training set of 19200 images. However, because only <italic>≈</italic>9,000 - 13,000 images were shown in each session, if a particular image was not shown in a session, we simply zeroed out the gradients of all neurons in the sessions that did not contain the image in question. When training models on a single closed loop recording, we cycled through all <italic>≈</italic>9,000 training images that were shown in that session. In all cases, we used a batch size of 64 and after each batch, we updated the weights using the Adam optimizer (<xref ref-type="bibr" rid="c48">Kingma &amp; Ba, 2014</xref>). The initial learning rate was 3 · 10<sup><italic>−</italic>4</sup> with a momentum of 0.1.</p>
<p>After each epoch, we computed the Poisson loss on the entire validation set. Similar to the training set, not all images were shown in all sessions, so that we again zeroed out the loss for the sessions in question. We then used early stopping to decide whether to decay the learning rate or stop the training altogether. We scaled the learning rate by a factor of 0.3 once the validation loss did not improve for five consecutive epochs. Before decaying the learning rate, we restored the weights to the best ones based on the poisson loss on the validation set. After four early stopping steps were completed, we stopped the training. On average, this resulted in <italic>≈</italic>50 training epochs, for a training time of 2 minutes for a closed loop session, and 15 minutes for the entire dataset on a NVIDIA 2080ti GPU.</p>
</sec>
</sec>
<sec id="s7">
<title>Ensemble models</title>
<p>Instead of using a single trained model, we used a model ensemble for all of our analyses and for MEI generation. To predict the neuronal responses to individual images, we trained readout weights for each member of an ensemble of five models initialized with different random seeds and used the average prediction across the ensemble for further analyses (<xref ref-type="bibr" rid="c34">Hansen &amp; Salamon, 1990</xref>). We always trained ten individual models with a different random seed, which determined the model initialization as well as the drawing of training batches. Then, we selected the five models with the highest performance on the validation set to form a model ensemble. The inputs to the ensemble model were passed to each member, and the resulting predictions were averaged to obtain the final model prediction.</p>
<sec id="s7a">
<title>Explainable variance</title>
<p>As a measure of response reliability, we estimated the fraction of the stimulus-driven variability as compared to the overall response variability. More specifically, we computed the ratio between each neurons’ total variance minus the variance of the observation noise, over the total variance (<xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref>).To estimate the variance of the observation noise, we averaged the variance of responses across image repeats for all of the 75 repeated full-field natural image test stimuli <inline-formula id="inline-eqn-5"><inline-graphic xlink:href="540591v2_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where <italic>t</italic> corresponds to the repeats and <italic>x</italic><sub><italic>j</italic></sub> represents a unique image:
<disp-formula id="eqn3">
<graphic xlink:href="540591v2_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s7b">
<title>Model performance measures</title>
<p>To measure the predictive performance of our models, we calculated the correlation to average (<xref ref-type="bibr" rid="c9">Cadena et al., 2022</xref>; <xref ref-type="bibr" rid="c26">Franke et al., 2022</xref>; <xref ref-type="bibr" rid="c98">Willeke et al., 2022</xref>) on the held out test set images. Given a neuron’s response <italic>r</italic><sub><italic>ij</italic></sub> to image <italic>i</italic> and repeat <italic>j</italic> and the model predictions <italic>o</italic><sub><italic>i</italic></sub>, the correlation is computed between the predicted responses and the average neuronal response <inline-formula id="inline-eqn-6"><inline-graphic xlink:href="540591v2_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to the <italic>i</italic><sup><italic>th</italic></sup> test image (averaged across repeated presentations of the same stimulus):
<disp-formula id="eqn4">
<graphic xlink:href="540591v2_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula id="inline-eqn-7"><inline-graphic xlink:href="540591v2_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the average response across <italic>J</italic> repeats, <inline-formula id="inline-eqn-8"><inline-graphic xlink:href="540591v2_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the average response across all repeats and images, and <inline-formula id="inline-eqn-9"><inline-graphic xlink:href="540591v2_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the average prediction across all repeats and images.</p>
</sec>
<sec id="s7c">
<title>Generation of MEIs</title>
<p>We used the trained model to synthesize maximally exciting input images (MEIs) for each neuron using regularized gradient ascent (<xref ref-type="bibr" rid="c3">Bashivan et al., 2019</xref>; <xref ref-type="bibr" rid="c26">Franke et al., 2022</xref>; <xref ref-type="bibr" rid="c94">Walker et al., 2019</xref>). Starting out with a randomly initialized Gaussian white noise image given by <bold>x</bold> ∈ ℝ<sup><italic>h×w</italic></sup>, with height <italic>h</italic> and width <italic>w</italic>, we showed the image to the model and computed the gradients of a single target neuron w.r.t. the image. To avoid high frequency artifacts, after each iteration we applied Gaussian blur with a <italic>σ</italic> of 1 pixel to smoothen the image. Additionally, we constrained the entire image to have a fixed energy budget, which we implemented as a maximum L2 norm of the standardized image, calculated across all pixel intensities. We chose an L2 norm of 25 for all neurons such that the resulting MEIs had minimal and maximal values similar to those found in our training natural image distribution. If the MEI exceeded the allowed norm budget after any iteration, we divided the MEI by factor <italic>f</italic><sub><italic>norm</italic></sub> with <italic>f</italic><sub><italic>norm</italic></sub> = ∥MEI∥<sub>2</sub><italic>/b</italic>. Additionally, enforced that the MEI could not contain values outside of the 8-bit pixel range by clipping the MEI outside of the bounds that correspond to 0 or 255 pixel-intensity. We used the stochastic gradient descent (SGD) optimized with learning rate of 10. We ran each optimization for 1000 iterations, without early stopping.</p>
</sec>
<sec id="s7d">
<title>MEIs with transparency masks</title>
<p>Furthermore, we employed a novel technique of synthesizing MEIs using a transparency channel based on the idea of <xref ref-type="bibr" rid="c64">Mordvintsev et al. (2018)</xref>. Given an MEI optimized with a method as described in the section above, it is difficult to distinguish which of the MEI features are important, i.e. strongly activate the neuron, and which ones are not. Through our inbuilt L2 energy constraint, there only is finite contrast that the model is able to distribute across the image. However, this is still uninformative regarding which features of the MEI are most important. Thus, we adopted a transparency method as a differentiable parametrization (<xref ref-type="bibr" rid="c64">Mordvintsev et al., 2018</xref>), which jointly optimizes the MEI and a transparency mask, with the objective of making the MEI itself as transparent as possible (i.e. uninformative parts of the MEI), while still retaining the high neuronal activation of the resulting image. More specifically, we optimized an image <bold>x</bold> ∈ ℝ<sup><italic>c×h×w</italic></sup>, with channels <italic>c</italic>, height <italic>h</italic> and width <italic>w</italic>. We set the channels <italic>c</italic> = 2, treated the first channel as the MEI <bold>x</bold><sub><italic>mei</italic></sub>, and the second channel as a transparent mask <bold>x</bold><sub><italic>α</italic></sub>, which we optimized jointly as follows: (1) In each iteration, we drew a random image <bold>x</bold><sub><italic>bg</italic></sub> from the training set as a background. (2) We clip the second MEI channel, i.e. the transparent mask, between the values 0–1, with 1 meaning that the MEI is fully opaque, and 0 meaning the the MEI is not visible at all. (3) Then, we blend the background and the MEI according to the transparency mask to get the combined image <bold>x</bold><sub><italic>combined</italic></sub> with
<disp-formula id="eqn5">
<graphic xlink:href="540591v2_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We then showed the combined image to the model to compute the neuronal response of the target neuron. With no additional constraints, the model did in fact learn to set all values of the transparent mask to 1, so that it is necessary to change the MEI objective function <bold>L</bold><sub><italic>old</italic></sub> from simply maximizing the neuronal response <italic>r</italic> to penalize alpha values of zero, as suggested by <xref ref-type="bibr" rid="c64">Mordvintsev et al. (2018)</xref>:
<disp-formula id="eqn6">
<graphic xlink:href="540591v2_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We used the same learning rate and optimizer as for our default gradient descent MEIs. We also applied the same L2 contrast constraint of 25, but only to the MEI image channel <italic>x</italic><sub><italic>mei</italic></sub>. For our closed loop experiments, we ran the initial 16 sessions with the default gradient ascent MEIs, and the final 8 sessions with the transparency method, observing similar success for the <italic>in vivo</italic> verification. Because the transparency method seemed to generate MEIs that were less spread out and had features that were perceptually easier to identify, we used exclusively this method to generate the MEIs for the full dataset. For all the 889 out of 1244 neurons that passed our threshold of correlation to average larger than 0.3, we generated 50 MEIs per neuron for a total of 44,450 MEIs. Each MEI took on average 2 minutes on a NVIDIA 2080ti GPU, resulting in a total compute time of <italic>≈</italic>60 GPU weeks.</p>
</sec>
<sec id="s7e">
<title>Centering of MEIs</title>
<p>In this work, we do not further analyze the transparent MEI masks. However, we use the transparent masks to center the MEIs for the similarity quantification. After MEI optimization, the MEI is located at the approximate RF location of each neuron. To center the MEIs, we find the center of mass of the transparent mask, and move the MEI such that the center of mass is at the center. All MEIs shown throughout this manuscript are shown at the center location for ease of comparison.</p>
</sec>
<sec id="s7f">
<title>Psychophysics</title>
<p>In our psychophysics experiment,the objective was to assess the perceptual similarity between MEIs originating from either a single recording session as compared to a random selection of neurons from different recording sessions. For this purpose, we devised a two-alternative forced choice task in which we showed two sets of MEIs, arranged in 3x3 grids as the basis for the similarity evaluation task. Participants were instructed to simply report which of the MEIs in the two grids looked more similar to each other. No other information apart from the displayed MEIs was provided.</p>
<p>To obtain the images for the task, we randomly chose 50 out of 52 sessions that had a minimum of 9 well-predicted neurons, using the threshold of correlation to average larger than 0.3. From these selected sessions, the top 9 well-predicted neurons were identified, resulting in a total pool of 450 neurons. For each session, participants were presented with images from the top 9 predicted neurons and a randomly selected set of 9 additional neurons from the pool, sampled without replacement. Specifically, for each neuron, we presented five MEIs, generated by starting from a different initial noise image during optimization, as images and compiled in a GIF. Participants were shown the two example sessions which were not included in the study, together with the correct solution. The subjects had unlimited time to make their choice.</p>
<p>Subject recruitment involved lab members that were familiar with the concept of an MEI from all research groups contributing to this article. In total, we recorded the responses of N=25 observers.</p>
</sec>
<sec id="s7g">
<title>Closed-loop procedure</title>
<p>Our closed loop experiments were composed of a generation and verification session. In the generation session, we recorded the electrophysiological responses of V4 neurons to full-field gray-scale natural images for 600 trials, corresponding to 9,000 unique images. After showing of all of these trials, we stopped and promptly restarted the neurophysiological acquisition system to continue recording neuronal activity while we processed the data from the generation session. During this analysis period, we isolated single units and used these units to train several models to predict neuron responses to natural images. We produced an ensemble of the best five models, used this ensemble to select the six highest predicted units and generate ten MEIs per unit. We computed the mask of each MEI by calculating the z-score of each pixel value and setting a threshold (<italic>z</italic> = 0.35) to isolate the area within the mask. We then created a convex hull to close any holes and smoothed the edges with a Gaussian filter (<italic>σ</italic> = 2). We used this masking procedure for our natural image control selection procedure.</p>
<p>Specifically, we screened a distinct set of 5,000 natural images, different from our training, validation, and test set images. This set was selected randomly from a database of 100k images, and we screened this same image set for all neurons across all closed-loop experiments. Each image was masked with the ten MEI masks of each unit and re-normalized to the contrast of the MEIs, resulting in 50,000 total screened images for each unit. We selected the seven highest activating masked natural images of each unit to use as controls for the verification recording.</p>
</sec>
<sec id="s7h">
<title>Closed-loop stimulation paradigm</title>
<p>During the verification session of the closed-loop experiment, we displayed one MEI and seven controls for each of the six units. These stimuli were centered on the RF and repeated 10 times throughout the experiment. We also showed the same full-field test set natural images 20 times each, to enable functional response matching between the generation and verification units. Each trial contained 15 images, composed of either (i) randomly interleaved generated stimuli (i.e. MEIs and masked controls) or (ii) full-field test set natural images, as described previously. Trials were randomly interleaved. In total, 100 trials of full-field images and 180 trials of generated stimuli were shown.</p>
</sec>
<sec id="s7i">
<title>Closed-loop unit matching</title>
<p>To assess the stability of our closed-loop neurons, we developed a procedure for spike waveform matching single units between the generation and verification session recordings. To do this, we performed a <italic>post hoc</italic> spike sorting of the full recording session, which was produced by stitching together the raw data files of the generation and verification recordings. The verification recording includes the analysis period immediately after the generation recording, during which we isolate single units and generate their MEIs. This continuity of recording allows us to more easily track single units throughout the experiment. Furthermore, spike sorting the full session allows us to use a drift correction of the spike sorting model based on a kalman-filter across the entire experiment, which aided the accuracy of our single unit tracking.</p>
<p>We then executed a two-step matching procedure using the spike sorting results of both the generation and full sessions. To determine potentially matched units, we first took the spikes of the generation session units and assigned them to full session units. We then calculated the proportion of spikes that were assigned exclusively to one unit in both the generation and verification sessions. If a unit in the generation session had at least 95% of its spikes assigned to only one single unit in the full session, this was considered a possible match. To confirm true matches, we assigned the full session spikes to the generation session units. If a unit in the full session had at least 95% of its spikes assigned to the potentially matched single unit in the generation session, these units were verified to be a match, and thus certified as stable. We additionally assessed neuron stability by computing the functional consistency of each unit. To compute this measure, we used the test set of 75 full-field natural images that were shown in both generation and verification session and calculated the Pearson correlation to the repeat-averaged responses in both sessions. We set a minimum functional consistency threshold of 0.5. Taken together, we were able to waveform match 82 neurons from 24 closed-loop sessions, with 27 of these failing to meet the functional consistency criterion, resulting in n=55 neurons for the analysis of the closed-loop paradigm.</p>
</sec>
<sec id="s7j">
<title>MEI similarity quantification using contrastive learning</title>
<p>We used a recently proposed method of contrastive learning (<xref ref-type="bibr" rid="c5">Böhm et al., 2023</xref>) to quantify the perceptual similarity of our neurons’ MEIs. The method is based on SimCLR (<xref ref-type="bibr" rid="c13">Chen et al., 2020</xref>), a method of self-supervised contrastive learning of visual representations in which images are embedded in a high-dimensional space. In this space, augmentations of the same image are learned to have small distances while simultaneously training the model to increase the distance to all other images. <xref ref-type="bibr" rid="c5">Böhm et al. (2023)</xref> adapted this procedure for a two-dimensional space by combining the ideas of contrastive learning with 2-d neighbor embeddings as used in t-SNE (<xref ref-type="bibr" rid="c91">van der Maaten &amp; Hinton, 2008</xref>). Their new method, called tSimCNE, achieves this by changing the similarity function between image embeddings from cosine-similarity used in SimCLR, which would constrain the embeddings to the unit circle, to Euclidean distance <italic>d</italic><sub><italic>ij</italic></sub> = ∥<bold>z</bold><sub><italic>i</italic></sub> − <bold>z</bold><sub><italic>j</italic></sub> ∥ between the embeddings <bold>z</bold><sub><italic>i</italic></sub> and <bold>z</bold><sub><italic>j</italic></sub> of two augmentations <italic>i</italic> and <italic>j</italic> of the same image. We use this method to train a model end-to-end to embed our MEIs using the loss function as proposed by <xref ref-type="bibr" rid="c5">Böhm et al. (2023)</xref>:
<disp-formula id="eqn7">
<graphic xlink:href="540591v2_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In our adaptation, <italic>i</italic> and <italic>j</italic> correspond to two data augmentations of the same MEI, and <italic>z</italic> denotes the 2-d embedding, which is the model output. For a given batch size <italic>b</italic>, the resulting batch size is 2<italic>b</italic> because each MEI is augmented twice. As pointed out by the authors of SimCLR (<xref ref-type="bibr" rid="c13">Chen et al., 2020</xref>), this approach requires careful selection of data augmentations, as well as choosing the largest batch size possible.</p>
<p>Taken together, our training procedure consists of these steps: We first select all 889 neurons above the model performance threshold of correlation to average larger than 0.3, as described earlier. For each neuron, from the 50 MEIs that we optimized per neuron, we select only the MEIs that elicited at least 90% of the predicted firing rate of the highest-acivating MEI, which resulted in anywhere from 3 to 42 MEIs per neuron, with a total of n=19,688 MEIs used in this and subsequent analyses. This selection was important as there can be failures during optimizing MEIs, resulting in noisy images which the contrastive learning algorithm was very sensitive to. Next, from all the MEIs that we selected, we assembled a batch of size n=889, by randomly drawing one MEI for each neuron.</p>
<p>Subsequently, we preprocessed the MEIs and applied the data augmentations in the following order: (1) centering the MEIs as described above, (2) center cropping to resize the MEIs from 100×100 to 75x75 pixels, (3) random rotation between 0 and 30 degrees, (4) random rescaling between 32×32 and 75×75 pixels, (5) random cropping from the resulting image to a final size of 32x32 pixels. Steps (2) to (4) were applied twice to each MEI to obtain two augmentations per MEI. The resulting training batch thus consisted of a randomly drawn MEI per neuron, augmented twice, for a total batch size of 889*2=1,778 images, with the model’s objective being to minimize the distance between each pair, and to maximize the distance from all-to- all images which are not pairs. It is important to point out that with this training scheme, the trained model was given no information about recording sessions or which MEIs belong to which neuron. Once the model was fully trained, we obtained the full 2-d embedding of all MEIs from all neurons by applying transformations (1) and (2), i.e. centering and center cropping the MEIs, as well as rescaling the MEIs to 32x32 pixels, and subsequently showing the resulting images to the model. Finally, We obtained a 2-d location of each neuron by taking the mean location over the embedding locations of all MEIs per neuron.</p>
</sec>
<sec id="s7k">
<title>Contrastive learning: model architecture and training</title>
<p>We closely followed the t-simCNE authors (<xref ref-type="bibr" rid="c5">Böhm et al., 2023</xref>) in our choice of model architecture and training paradigm. As a model backbone, we employed a randomly initialized ResNet18 (<xref ref-type="bibr" rid="c37">He et al., 2016</xref>) with an output size of 512 and a reduced kernel size of the first convolutional layer from 7 <italic>×</italic> 7 to 3 <italic>×</italic> 3. Following the authors, we also added one hidden ReLU layer with n=1,024 units followed by a linear output layer of n=128 units, which was reduced to n=2 during the final stage of training. The training consisted of three stages: First, the model was trained for 3,000 epochs with the output layer size of 128. In the second stage, the output layer was disregarded and replaced with a linear output layer of n=2, followed by a training of only this layer for 200 epochs while the rest of the model was frozen. Lastly, in the third stage, the whole model was fine-tuned for another 1,000 epochs with a reduction of the learning rate by a factor of 1,000. In each of the three stages, we used the initial learning rate of 0.03·<italic>b/</italic>256 <italic>≈</italic> 0.1 with b=889, preceded by linear warm-up for ten epochs, followed by cosine annealing (<xref ref-type="bibr" rid="c56">Loshchilov &amp; Hutter, 2016</xref>) with a final learning rate of 0. In each epoch, we accumulated the loss from 10 batches and optimized the model using SGD with a momentum of 0.9.</p>
</sec>
<sec id="s7l">
<title>Contrastive learning: within-neuron MEI distances</title>
<p>As mentioned above, it is crucial that our contrastive learning training scheme did not provide the model with any information about either the recording sessions or the neurons’ identity of each MEI. Therefore, an important sanity-check for the trained model is to analyze the 2-d embedding distances of all MEIs that we optimized for a single neuron. We performed this analysis by simply calculating all pairwise distances and taking the average of all MEIs per neuron.</p>
</sec>
<sec id="s7m">
<title>Contrastive learning: Session distances</title>
<p>We first averaged all pairwise distances across neurons recorded within one session to obtain a within-session distance in the 2-d embedding space. We then compared this withinsession distance to a shuffled control. For this control, we used bootstrapping to sample at random the same number of neurons as in a given session, with the constraint that they cannot be from the session in question and that all randomly drawn neurons originate from different sessions. We then computed the mean pairwise distance for these shuffled neurons, and repeated this process 25,000 times for each session to get the null distribution. We then calculated the percentile of the true within-session distance against each sessions’ null distribution to obtain a p-value.</p>
</sec>
<sec id="s7n">
<title>Contrastive learning: HDBSCAN cluster cutting</title>
<p>After we obtained the 2-d embedding space of MEIs, we clustered this entire two-dimensional space using hierarchical density-based spatial clustering of applications with noise (HDBSCAN <xref ref-type="bibr" rid="c60">McInnes et al., 2017</xref>), the same clustering that was employed by the t-simCNE authors (<xref ref-type="bibr" rid="c5">Böhm et al., 2023</xref>). We searched over the parameter grid minimum cluster size ∈ {100, 125,…, 400} × and minimum samples, which indicates the minimum number of samples to be considered non-noise, 5, ∈ {15,…, 145} with the constraint min-samples ≤ min-cluster-size. The resulting clusterings will create one clustered group labelled -1, for MEIs that were unable to be clustered. We selected the parameters min-cluster-size = 200 and min-samples = 10 which resulted in the lowest number of unassigned MEIs. Using this approach, the MEIs of each neuron get assigned a label, with the possibility that not all MEIs of single neuron were assigned to the same cluster. We assigned a cluster ID to a neuron if more than half of its MEIs had the same cluster ID. Furthermore, we excluded neurons for which the more than half of the MEIs were in the unassigned category -1 of the HDBSCAN algorithm. Based on these two criteria, 828 out of 889 neurons were given a valid clustering ID.</p>
</sec>
<sec id="s7o">
<title>Representational similarity of MEIs</title>
<p>As an independent way to verify the similarity of MEIs, we computed their representational similarity. For this purpose, we used the centered MEIs, and computed the responses of all 889 well-predicted neurons from our neural predictive model to each MEI. We controlled for the different RF positions of each <italic>in silico</italic> neuron by artificially centering the <italic>in silico</italic> neuron’s RF to the center by setting the readout location of the Gaussian readout to the center of the core output. We used this approach for each model ensemble member individually and again took the model ensemble average as the predicted neuronal activity. Consequently, for each MEI, we obtained a response vector of length 889, which we then used for pairwise comparisons using cosine similarity. We then compared the average cosine similarity either within clusters or within recording sessions to the average similarity across sessions/clusters. To obtain a p-value for the similarity within vs. across clusters, we computed the similarity matrix 100 times. In each of the 100 runs, per neuron we drew at random one of the neurons MEIs and computed the population response vector for that MEI. We thus ended up with 100 similarity matrices, for which the pairwise comparisons were thus computed based on different MEIs. For each cluster, we then computed the average similarity across the 100 runs, and compared this similarity. We then took (1) the average cosine similarity within each cluster across the 100 runs, and (2) for each of the 100 runs, for each cluster we average the similarity to all other clusters. We report the p-value as percentile of the within-session similarity to the distribution of across-session similarity.</p>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="s9" sec-type="supplementary">
<title>Supplementary Information</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Supplemental Fig. 1.</label>
<caption><title>Spike waveform and functional matching of single units across recordings.</title>
<p><bold>a</bold>, Schematic illustrating spike sorting of the closed-loop experimental paradigm. The “generation session” is spike sorted directly after the recording, resulting in “Sorting 1”. This data is then used for model training and optimization of MEIs, which are presented back to the animal during the “verification session”. The verification session recording starts immediately after the generation session recording ends, to ensure a continuous monitoring of the recorded units over time. After the experiment, the generation and verification session recordings are concatenated (“full session”) and spike sorted, resulting in “Sorting 2”. <bold>b</bold>, Unit matching based on spike waveforms across Sorting 1 (generation session) and Sorting 2 (full session) for an example session. The left plot shows the percentage of spikes of the Sorting 1 units assigned to the units of Sorting 2. Units were assigned by passing the principal components of each spike, extracted using the Sorting 1 Gaussian Mixture model (GMM), to the Sorting 2 model. For a potential match (orange), at least 95% of the spikes of a single unit of Sorting 1 had to be assigned to an individual unit of Sorting 2. The right plot shows the percentage of spikes of Sorting 2 units assigned to the units of Sorting 1. For a final match (red), at least 95% of the spikes of a single Sorting 2 unit had to be assigned to the potential match of Sorting 1. <bold>c</bold>, Distribution of correlations of mean test set responses for all final matches. We only included matched units into the analysis, if their functional correlation was at least 0.5.</p></caption>
<graphic xlink:href="540591v2_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Supplemental Fig. 2.</label>
<caption><title>Diverse model-derived stimuli of individual monkey V4 neurons.</title>
<p><bold>a</bold>, For a set of 14 example neurons, we show 10 MEIs per neuron, generated from different starting points (random seeds) during the MEI optimization. The different MEIs exhibit the same visual feature but somewhat differ with respect to orientation, scale and position. <bold>b</bold>, Same as (a), for another set of 14 neurons.</p></caption>
<graphic xlink:href="540591v2_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Supplemental Fig. 3.</label>
<caption><title>Similarity of optimal stimuli in neuronal response space.</title>
<p><bold>a</bold>, MEIs of three example neurons. Right shows a schematic illustrating how we compared the similarity of MEIs using representational similarity. In brief, each MEI was presented to the trained CNN model that was used to produce the MEIs to obtain a response vector. The response vectors were then compared using cosine similarity. <bold>b</bold>, Mean cosine similarity of MEIs within a single recording session (diagonal) and across recording sessions for n=88 sessions, peak-normalized for each row. <bold>c</bold>, Distribution of percentiles of within-session similarity. For example, a percentile of 0.05 means that the MEI similarity within the session was larger than the MEI similarity to 95% of the other sessions. For n=30/55 sessions, the percentile was &lt;0.05. <bold>d</bold>, Cosine similarity of MEIs of n=889 neurons, sorted based on cluster assignment (cf. <xref rid="fig5" ref-type="fig">Fig. 5</xref>). <bold>e</bold>, Mean cosine similarity of MEIs within a cluster (diagonal) and across clusters, peak normalized per row. The matrix depicts the mean across n=100 similarity matrices, each generated based on a different random selection of MEIs per neuron. Top shows distribution of cosine similarity within an example cluster (black) and the mean similarity to all other clusters (gray). <bold>f</bold>, Mean within-cluster and across-cluster similarity for all clusters.</p></caption>
<graphic xlink:href="540591v2_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Supplemental Fig. 4.</label>
<caption><title>Overview of optimal stimuli of V4 response modes.</title>
<p><bold>a</bold>, MEIs of example neurons for five response modes not shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>.</p></caption>
<graphic xlink:href="540591v2_figS4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<ack>
<title>Acknowledgements</title>
<p>The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Konstantin Willeke and Arne Nix. The authors would also like to thank Jan Niklas Böhm, Philipp Berens, and Dmitry Kobak for their guidance with using their recently developed t-simCNE model and Greg Horwitz for feedback on the manuscript. The authors also thank Edgar Y. Walker, George Denfield, Christoph Blessing, Mohammad Bashiri, Konstantin-Klemens Lurz, Max Burg, Shanqian Ma, Robert Petrovic, Elena Offenberg and Paul Fahey for technical support and helpful discussions. The research was funded by the Carl-Zeiss-Stiftung (KW, FHS), the Cyber Valley Research Fund (AN, FHS). FHS is further supported by the German Federal Ministry of Education and Research (BMBF) via the Collaborative Research in Computational Neuroscience (CRCNS) (FKZ 01GQ2107), as well as the Collaborative Research Center (SFB 1233, Robust Vision) and the Cluster of Excellence “Machine Learning – New Perspectives for Science” (EXC 2064/1, project number 390727645). ASE received funding for this project from the European Research Council (ERC) under the European Union’s Horizon Europe research and innovation programme (Grant agreement No. 101041669) and Deutsche Forschungsgemeinschaft (DFG, German Research Foundation), project ID 432680300 (SFB 1456, project B05). The work was also supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC) contract numbers D16PC00003, D16PC00004, and D16PC0005. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. We also acknowledge support from the National Institute of Mental Health and National Institute of Neurological Disorders And Stroke under Award Number U19MH114830 and National Eye Institute award numbers R01 EY026927 and Core Grant for Vision Research T32-EY-002520-37 as well as the National Science Foundation Collaborative Research in Computational Neuroscience IIS-2113173. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.</p>
</ack>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
<sec id="s8" sec-type="data-availability">
<title>Data and code availability</title>
<p>The analysis code and data will be publicly available in an online repository latest upon journal publication. Our coding framework uses Pytorch (<xref ref-type="bibr" rid="c74">Paszke et al., 2019</xref>), Numpy (<xref ref-type="bibr" rid="c35">Harris et al., 2020</xref>), scikit-image (<xref ref-type="bibr" rid="c92">Van der Walt et al., 2014</xref>), matplotlib (<xref ref-type="bibr" rid="c41">Hunter, 2007</xref>), seaborn (<xref ref-type="bibr" rid="c97">Waskom et al., 2017</xref>), DataJoint (<xref ref-type="bibr" rid="c101">Yatsenko et al., 2015</xref>), Jupyter (<xref ref-type="bibr" rid="c49">Kluyver et al., 2016</xref>), and Docker (<xref ref-type="bibr" rid="c62">Merkel, 2014</xref>). We also used the following custom libraries and code: neuralpredictors (<ext-link ext-link-type="uri" xlink:href="https://github.com/sinzlab/neuralpredictors">https://github.com/sinzlab/neuralpredictors</ext-link>) for torch-based custom functions for model implementation, nnfabrik (<ext-link ext-link-type="uri" xlink:href="https://github.com/sinzlab/nnfabrik">https://github.com/sinzlab/nnfabrik</ext-link>) for automatic model training pipelines using DataJoint, nnvision (<ext-link ext-link-type="uri" xlink:href="https://github.com/sinzlab/nnvision">https://github.com/sinzlab/nnvision</ext-link>) for specific model definitions, analysis, and figures.</p>
</sec>
<sec id="s4">
<title>Author contributions</title>
<p>
<bold>KFW</bold> Conceptualization, Methodology, Validation, Software, Formal Analysis, Investigation, Writing - Original Draft, Visualization <bold>KR</bold> Conceptualization, Methodology, Validation, Software, Formal Analysis, Investigation, Data acquisition, Data curation, Writing - Original Draft, Visualization; <bold>KF</bold> Conceptualization, Methodology, Investigation, Experimental and analysis design, Writing Original Draft, Writing - Review &amp; Editing, Supervision; <bold>AFN</bold> Methodology, Software, Writing - Review &amp; Editing; <bold>SAC</bold> Conceptualization, Methodology, Software, Investigation, Experimental and analysis design; <bold>TS</bold>,<bold>CN</bold>,<bold>GR</bold>,<bold>SP</bold> Data acquisition, Data curation, Methodology; <bold>ASE</bold> Conceptualization, Methodology, Investigation, Experimental and analysis design, Writing - Review &amp; Editing, Supervision; <bold>FHS</bold> Conceptualization, Methodology, Investigation, Experimental and analysis design, Writing - Original Draft, Writing - Review &amp; Editing, Supervision, Funding Acquisition, Project administration; <bold>AST</bold> Conceptualization, Methodology, Investigation, Experimental and analysis design, Writing - Original Draft, Writing - Review &amp; Editing, Supervision, Funding Acquisition, Project administration;</p>
</sec>
</sec>
<ref-list>
<title>Reference</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baden</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Berens</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Franke</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Román Rosón</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Euler</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2016</year>). <article-title>The functional diversity of retinal ganglion cells in the mouse</article-title>. <source>Nature</source>, <volume>529</volume>(<issue>7586</issue>), <fpage>345</fpage>–<lpage>350</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bakhtiari</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Mineault</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Lillicrap</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Pack</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Richards</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2021</year>). <article-title>The functional specialization of visual cortex emerges from training parallel pathways with self-supervised predictive learning</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>34</volume>, <fpage>25164</fpage>–<lpage>25178</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bashivan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kar</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Neural population control via deep image synthesis</article-title>. <source>Science</source>, <volume>364</volume>(<issue>6439</issue>).</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bock</surname>, <given-names>D. D.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>W.-C. A.</given-names></string-name>, <string-name><surname>Kerlin</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Andermann</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Hood</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Wetzel</surname>, <given-names>A. W.</given-names></string-name>, <string-name><surname>Yurgenson</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Soucy</surname>, <given-names>E. R.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>H. S.</given-names></string-name>, &amp; <string-name><surname>Reid</surname>, <given-names>R. C.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Network anatomy and in vivo physiology of visual cortical neurons</article-title>. <source>Nature</source>, <volume>471</volume>(<issue>7337</issue>), <fpage>177</fpage>–<lpage>182</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Böhm</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Berens</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Kobak</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Unsupervised visualization of image datasets using contrastive learning</article-title>. In <source>The Eleventh International Conference on Learning Representations</source>. URL <ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=nI2HmVA0hvt">https://openreview.net/forum?id=nI2HmVA0hvt</ext-link></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bonhoeffer</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Grinvald</surname>, <given-names>A.</given-names></string-name></person-group> (<year>1991</year>). <article-title>Iso-orientation domains in cat visual cortex are arranged in pinwheel-like patterns</article-title>. <source>Nature</source>, <volume>353</volume>(<issue>6343</issue>), <fpage>429</fpage>–<lpage>431</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cadena</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Denfield</surname>, <given-names>G. H.</given-names></string-name>, <string-name><surname>Walker</surname>, <given-names>E. Y.</given-names></string-name>, <string-name><surname>Gatys</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Deep convolutional models improve predictions of macaque v1 responses to natural images</article-title>. <source>PLoS computational biology</source>, <volume>15</volume>(<issue>4</issue>), <fpage>e1006897</fpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Cadena</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Weis</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Gatys</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Diverse feature visualizations reveal invariances in early layers of deep neural networks</article-title>. In <conf-name>Proceedings of the European Conference on Computer Vision (ECCV)</conf-name>, (pp. <fpage>217</fpage>–<lpage>232</lpage>).</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Cadena</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Willeke</surname>, <given-names>K. F.</given-names></string-name>, <string-name><surname>Restivo</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Denfield</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Sinz</surname>, <given-names>F. H.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name>, &amp; <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Diverse task-driven modeling of macaque V4 reveals functional specialization towards semantic tasks</article-title>. <source>bioRxiv</source>, (p. 2022.05.18.492503).</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cadwell</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Scala</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Fahey</surname>, <given-names>P. G.</given-names></string-name>, <string-name><surname>Kobak</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Mulherkar</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sinz</surname>, <given-names>F. H.</given-names></string-name>, <string-name><surname>Papadopoulos</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tan</surname>, <given-names>Z. H.</given-names></string-name>, <string-name><surname>Johnsson</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Hartmanis</surname>, <given-names>L.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2020</year>). <article-title>Cell type composition and circuit organization of clonally related excitatory neurons in the juvenile mouse neocortex</article-title>. <source>eLife</source>, <volume>9</volume>, <elocation-id>e52951</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.52951</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calabrese</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Paninski</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Kalman filter mixture model for spike sorting of nonstationary data</article-title>. <source>Journal of neuroscience methods</source>, <volume>196</volume>(<issue>1</issue>), <fpage>159</fpage>–<lpage>169</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Campagnola</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Seeman</surname>, <given-names>S. C.</given-names></string-name>, <string-name><surname>Chartrand</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Hoggarth</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gamlin</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Ito</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Trinh</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Davoudian</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Radaelli</surname>, <given-names>C.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2022</year>). <article-title>Local connectivity and synaptic dynamics in mouse and human neocortex</article-title>. <source>Science</source>, <volume>375</volume>(<issue>6585</issue>), <fpage>eabj5861</fpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kornblith</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Norouzi</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Hinton</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2020</year>). <source>A simple framework for contrastive learning of visual representations</source>. URL <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2002.05709">https://arxiv.org/abs/2002.05709</ext-link></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Clevert</surname>, <given-names>D.-A.</given-names></string-name>, <string-name><surname>Unterthiner</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Hochreiter</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Fast and accurate deep network learning by exponential linear units (elus)</article-title>. <source>arXiv</source> <pub-id pub-id-type="doi">10.48550/arXiv.1511.07289</pub-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Consortium</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bae</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Baptiste</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bishop</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Bodor</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Brittain</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Buchanan</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bumbarger</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Castro</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Celii</surname>, <given-names>B.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2021</year>). <article-title>Functional connectomics spanning multiple areas of mouse visual cortex</article-title>. <source>BioRxiv</source>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Conway</surname>, <given-names>B. R.</given-names></string-name>, &amp; <string-name><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Color-tuned neurons are spatially clustered according to color preference within alert macaque posterior inferior temporal cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>106</volume>(<issue>42</issue>), <fpage>18034</fpage>–<lpage>18039</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Denfield</surname>, <given-names>G. H.</given-names></string-name>, <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Shinn</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Attentional fluctuations induce shared variability in macaque primary visual cortex</article-title>. <source>Nature communications</source>, <volume>9</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Deng</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Dong</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Socher</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>L.-J.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Fei-Fei</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Imagenet: A large-scale hierarchical image database</article-title>. In <conf-name>2009 IEEE conference on computer vision and pattern recognition</conf-name>, (pp. <fpage>248</fpage>–<lpage>255</lpage>). Ieee.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Ding</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Fahey</surname>, <given-names>P. G.</given-names></string-name>, <string-name><surname>Papadopoulos</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Celii</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Papadopoulos</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kunin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Fu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>Z.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2023a</year>). <article-title>Functional connectomics reveals general wiring rule in mouse visual cortex</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Ding</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Tran</surname>, <given-names>D. T.</given-names></string-name>, <string-name><surname>Ponder</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Cobos</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Fahey</surname>, <given-names>P. G.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Muhammad</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Fu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Cadena</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Papadopoulos</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Patel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Franke</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Reimer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sinz</surname>, <given-names>F. H.</given-names></string-name>, <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Pitkow</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name></person-group> (<year>2023b</year>). <article-title>Bipartite invariance in mouse primary visual cortex</article-title> <source>bioRxiv</source></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Berens</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Cotton</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Subramaniyan</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Denfield</surname>, <given-names>G. H.</given-names></string-name>, <string-name><surname>Cadwell</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Smirnakis</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name></person-group> (<year>2014</year>). <article-title>State dependence of noise correlations in macaque primary visual cortex</article-title>. <source>Neuron</source>, <volume>82</volume>(<issue>1</issue>), <fpage>235</fpage>–<lpage>248</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Engstrom</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Ilyas</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Salman</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Santurkar</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Tsipras</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2019a</year>). <source>Robustness (python library)</source>. URL <ext-link ext-link-type="uri" xlink:href="https://github.com/MadryLab/robustness">https://github.com/MadryLab/robustness</ext-link></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Engstrom</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Ilyas</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Santurkar</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tsipras</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Tran</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Madry</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2019b</year>). <source>Adversarial robustness as a prior for learned representations</source>. URL <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1906.00945">https://arxiv.org/abs/1906.00945</ext-link></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Feather</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Leclerc</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Mądry</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>McDermott</surname>, <given-names>J. H.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Model metamers illuminate divergences between biological and artificial neural networks</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2022.05.19.492678</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Felleman</surname>, <given-names>D. J.</given-names></string-name>, &amp; <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name></person-group> (<year>1991</year>). <article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title>. <source>Cerebral cortex (New York, NY: 1991)</source>, <volume>1</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>47</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Franke</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Willeke</surname>, <given-names>K. F.</given-names></string-name>, <string-name><surname>Ponder</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Galdamez</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Muhammad</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Patel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Froudarakis</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Reimer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sinz</surname>, <given-names>F. H.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2022</year>). <article-title>State-dependent pupil dilation rapidly shifts visual feature selectivity</article-title>. <source>Nature</source>, <volume>610</volume>(<issue>7930</issue>), <fpage>128</fpage>–<lpage>134</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Fu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Shrinivasan</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ponder</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Muhammad</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Tran</surname>, <given-names>D. T.</given-names></string-name>, <string-name><surname>Fahey</surname>, <given-names>P. G.</given-names></string-name>, <string-name><surname>Papadopoulos</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Patel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Reimer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Pitkow</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Haefner</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Sinz</surname>, <given-names>F. H.</given-names></string-name>, <string-name><surname>Franke</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Pattern completion and disruption characterize contextual modulation in mouse visual cortex</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2023.03.13.532473</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Braun</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name></person-group> (<year>1993</year>). <article-title>Selectivity for polar, hyperbolic, and cartesian gratings in macaque visual cortex</article-title>. <source>Science</source>, <volume>259</volume>(<issue>5091</issue>), <fpage>100</fpage>–<lpage>103</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ghose</surname>, <given-names>G. M.</given-names></string-name>, &amp; <string-name><surname>Ts’o</surname>, <given-names>D. Y.</given-names></string-name></person-group> (<year>1997</year>). <article-title>Form processing modules in primate area V4</article-title>. <source>J. Neurophysiol</source>., <volume>77</volume> (<issue>4</issue>), <fpage>2191</fpage>–<lpage>2196</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gilbert</surname>, <given-names>C. D.</given-names></string-name>, &amp; <string-name><surname>Wiesel</surname>, <given-names>T. N.</given-names></string-name></person-group> (<year>1989</year>). <article-title>Columnar specificity of intrinsic horizontal and corticocortical connections in cat visual cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>9</volume>(<issue>7</issue>), <fpage>2432</fpage>–<lpage>2442</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goetz</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Jessen</surname>, <given-names>Z. F.</given-names></string-name>, <string-name><surname>Jacobi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cooler</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Greer</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kadri</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Segal</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Shekhar</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Sanes</surname>, <given-names>J. R.</given-names></string-name>, &amp; <string-name><surname>Schwartz</surname>, <given-names>G. W.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Unified classification of mouse retinal ganglion cells using function, morphology, and gene expression</article-title>. <source>Cell Rep</source>., <volume>40</volume>(<issue>2</issue>), <fpage>111040</fpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Güçlü</surname>, <given-names>U.</given-names></string-name>, &amp; <string-name><surname>van Gerven</surname>, <given-names>M. A.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>27</issue>), <fpage>10005</fpage>–<lpage>10014</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Guo</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Leclerc</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Dapello</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Rao</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Madry</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2022</year>). <source>Adversarially trained neural representations may already be as robust as corresponding biological neural representations</source>. URL <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2206.11228">https://arxiv.org/abs/2206.11228</ext-link></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hansen</surname>, <given-names>L. K.</given-names></string-name>, &amp; <string-name><surname>Salamon</surname>, <given-names>P.</given-names></string-name></person-group> (<year>1990</year>). <article-title>Neural network ensembles</article-title>. <source>IEEE transactions on pattern analysis and machine intelligence</source>, <volume>12</volume>(<issue>10</issue>), <fpage>993</fpage>–<lpage>1001</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harris</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Millman</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>van der Walt</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Gommers</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Virtanen</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Cournapeau</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Wieser</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Taylor</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Berg</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Kern</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Picus</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hoyer</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>van Kerkwijk</surname>, <given-names>M. H.</given-names></string-name>, <string-name><surname>Brett</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Haldane</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>del R’ıo</surname>, <given-names>J. F.</given-names></string-name>, <string-name><surname>Wiebe</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Peterson</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>G’erard-Marchant</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Sheppard</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Reddy</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Weckesser</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Abbasi</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Gohlke</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Oliphant</surname>, <given-names>T. E.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Array programming with NumPy</article-title>. <source>Nature</source>, <volume>585</volume>(<issue>7825</issue>), <fpage>357</fpage>–<lpage>362</lpage> <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hatanaka</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Inagaki</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Takeuchi</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Nishimoto</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ikezoe</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Fujita</surname>, <given-names>I.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Processing of visual statistics of naturalistic videos in macaque visual areas V1 and V4</article-title>. <source>Brain Struct. Funct</source>., <volume>227</volume> (<issue>4</issue>), <fpage>1385</fpage>–<lpage>1403</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Deep residual learning for image recognition</article-title>. In <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>, (pp. <fpage>770</fpage>–<lpage>778</lpage>).</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Höfling</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Szatko</surname>, <given-names>K. P.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Qiu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Klindt</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Jessen</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Schwartz</surname>, <given-names>G. W.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Berens</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Franke</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name>, &amp; <string-name><surname>Euler</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2022</year>). <article-title>A chromatic feature detector in the retina signals visual context changes</article-title> <source>bioRxiv</source>. <pub-id pub-id-type="doi">10.1101/2022.11.30.518492</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Horton</surname>, <given-names>J. C.</given-names></string-name>, &amp; <string-name><surname>Adams</surname>, <given-names>D. L.</given-names></string-name></person-group> (<year>2005</year>). <article-title>The cortical column: a structure without a function</article-title>. <source>Philos. Trans. R. Soc. Lond. B Biol. Sci</source>., <volume>360</volume>(<issue>1456</issue>), <fpage>837</fpage>–<lpage>862</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hubel</surname>, <given-names>D. H.</given-names></string-name>, &amp; <string-name><surname>Wiesel</surname>, <given-names>T. N.</given-names></string-name></person-group> (<year>1968</year>). <article-title>Receptive fields and functional architecture of monkey striate cortex</article-title>. <source>J. Physiol</source>., <volume>195</volume>(<issue>1</issue>), <fpage>215</fpage>–<lpage>243</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hunter</surname>, <given-names>J. D.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Matplotlib: A 2d graphics environment</article-title>. <source>Computing in Science &amp; Engineering</source>, <volume>9</volume>(<issue>3</issue>), <fpage>90</fpage>–<lpage>95</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Ioffe</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Szegedy</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Batch normalization: Accelerating deep network training by reducing internal covariate shift</article-title>. In <conf-name>International conference on machine learning</conf-name>, (pp. <fpage>448</fpage>–<lpage>456</lpage>).</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Issa</surname>, <given-names>N. P.</given-names></string-name>, <string-name><surname>Trepel</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Stryker</surname>, <given-names>M. P.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Spatial frequency maps in cat visual cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>20</volume>(<issue>22</issue>), <fpage>8504</fpage>–<lpage>8514</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jiang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Shen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Cadwell</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Berens</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Sinz</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Patel</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Principles of connectivity among morphologically defined cell types in adult neocortex</article-title>. <source>Science</source>, <volume>350</volume>(<issue>6264</issue>), <fpage>aac9462</fpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khaligh-Razavi</surname>, <given-names>S.-M.</given-names></string-name>, &amp; <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Deep supervised, but not unsupervised, models may explain it cortical representation</article-title>. <source>PLoS computational biology</source>, <volume>10</volume>(<issue>11</issue>), <fpage>e1003915</fpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Bair</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Pasupathy</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Neural coding for shape and texture in macaque area v4</article-title>. <source>Journal of Neuroscience</source>, <volume>39</volume>(<issue>24</issue>), <fpage>4760</fpage>–<lpage>4774</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kindel</surname>, <given-names>W. F.</given-names></string-name>, <string-name><surname>Christensen</surname>, <given-names>E. D.</given-names></string-name>, &amp; <string-name><surname>Zylberberg</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Using deep learning to probe the neural code for images in primary visual cortex</article-title>. <source>Journal of vision</source>, <volume>19</volume>(<issue>4</issue>), <fpage>29</fpage>–<lpage>29</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Kingma</surname>, <given-names>D. P.</given-names></string-name>, &amp; <string-name><surname>Ba</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Adam: A method for stochastic optimization</article-title>. <source>arXiv</source>:<pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kluyver</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Ragan-Kelley</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Pérez</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Granger</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Bussonnier</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Frederic</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kelley</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Hamrick</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Grout</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Corlay</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ivanov</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Avila</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Abdalla</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Willing</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2016</year>). <chapter-title>Jupyter notebooks – a publishing format for reproducible computational workflows</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>F.</given-names> <surname>Loizides</surname></string-name>, &amp; <string-name><given-names>B.</given-names> <surname>Schmidt</surname></string-name></person-group> (Eds.) <source>Positioning and Power in Academic Publishing: Players, Agents and Agendas</source>, (pp. <fpage>87</fpage>–<lpage>90</lpage>). <publisher-name>IOS Press</publisher-name>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kobatake</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Tanaka</surname>, <given-names>K.</given-names></string-name></person-group> (<year>1994</year>). <article-title>Neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex</article-title>. <source>J. Neurophysiol</source>., <volume>71</volume>(<issue>3</issue>), <fpage>856</fpage>–<lpage>867</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kotake</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Morimoto</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Okazaki</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Fujita</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Tamura</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Organization of colorselective neurons in macaque visual area V4</article-title>. <source>J. Neurophysiol</source>., <volume>102</volume>(<issue>1</issue>), <fpage>15</fpage>–<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Representational similarity analysis – connecting the branches of systems neuroscience</article-title>. <source>Frontiers in Systems Neuroscience</source>. <volume>2</volume> <pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Imagenet classification with deep convolutional neural networks</article-title>. <source>Advances in neural information processing systems</source>, <volume>25</volume>, <fpage>1097</fpage>–<lpage>1105</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lehky</surname>, <given-names>S. R. R. S. R. R.</given-names></string-name>, <string-name><surname>Sejnowski</surname>, <given-names>T. J. J.</given-names></string-name>, &amp; <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name></person-group> (<year>1992</year>). <article-title>Predicting responses of nonlinear neurons in monkey striate cortex to complex patterns</article-title>. <source>J. Neurosci</source>., <volume>12</volume>(<issue>9</issue>), <fpage>3568</fpage>–<lpage>3581</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Brendel</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Walker</surname>, <given-names>E. Y.</given-names></string-name>, <string-name><surname>Cobos</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Muhammad</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Reimer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sinz</surname>, <given-names>F. H.</given-names></string-name>, <string-name><surname>Pitkow</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name></person-group> (<year>2019</year>). <source>Learning from brains how to regularize machines</source>. URL <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1911.05072">https://arxiv.org/abs/1911.05072</ext-link></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Loshchilov</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Hutter</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2016</year>). <source>Sgdr: Stochastic gradient descent with warm restarts</source>. URL <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1608.03983">https://arxiv.org/abs/1608.03983</ext-link></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Lurz</surname>, <given-names>K.-K.</given-names></string-name>, <string-name><surname>Bashiri</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Willeke</surname>, <given-names>K. F.</given-names></string-name>, <string-name><surname>Jagadish</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Walker</surname>, <given-names>E. Y.</given-names></string-name>, <string-name><surname>Cadena</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Muhammad</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Cobos</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Tolias</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2020</year>). <article-title>Generalization in data-driven models of primary visual cortex</article-title>. <source>bioRxiv</source> .</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Madry</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Makelov</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Tsipras</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Vladu</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2017</year>). <source>Towards deep learning models resistant to adversarial attacks</source>. URL <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1706.06083">https://arxiv.org/abs/1706.06083</ext-link></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Masland</surname>, <given-names>R. H.</given-names></string-name></person-group> (<year>2001</year>). <article-title>The fundamental plan of the retina</article-title>. <source>Nat. Neurosci</source>., <volume>4</volume>(<issue>9</issue>), <fpage>877</fpage>–<lpage>886</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McInnes</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Healy</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Astels</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2017</year>). <article-title>hdbscan: Hierarchical density based clustering</article-title>. <source>Journal of Open Source Software</source>, <volume>2</volume>(<issue>11</issue>), <fpage>205</fpage>. <pub-id pub-id-type="doi">10.21105/joss.00205</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mechler</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Ringach</surname>, <given-names>D. L.</given-names></string-name></person-group> (<year>2002</year>). <article-title>On the classification of simple and complex cells</article-title>. <source>Vision research</source>, <volume>42</volume>(<issue>8</issue>), <fpage>1017</fpage>–<lpage>1033</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Merkel</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Docker: lightweight linux containers for consistent development and deployment</article-title>. <source>Linux journal</source>, <volume>2014</volume>(<issue>239</issue>), <fpage>2</fpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mishkin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ungerleider</surname>, <given-names>L. G.</given-names></string-name>, &amp; <string-name><surname>Macko</surname>, <given-names>K. A.</given-names></string-name></person-group> (<year>1983</year>). <article-title>Object vision and spatial vision: two cortical pathways</article-title>. <source>Trends in neurosciences</source>, <volume>6</volume>, <fpage>414</fpage>–<lpage>417</lpage>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Mordvintsev</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pezzotti</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Schubert</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Olah</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Differentiable image parameterizations</article-title>. <source>Distill</source>. <ext-link ext-link-type="uri" xlink:href="https://distill.pub/2018/differentiable-parameterizations">https://distill.pub/2018/differentiable-parameterizations</ext-link>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mountcastle</surname>, <given-names>V. B.</given-names></string-name></person-group> (<year>1957</year>). <article-title>Modality and topographic properties of single neurons of cat’s somatic sensory cortex</article-title>. <source>J. Neurophysiol</source>., <volume>20</volume>(<issue>4</issue>), <fpage>408</fpage>–<lpage>434</lpage>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Namima</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kempkes</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Zamarashkina</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Owen</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Pasupathy</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2025</year>). <article-title>High-density recording reveals sparse clusters (but not columns) for shape and texture encoding in macaque V4</article-title>. <source>J. Neurosci</source>., <volume>45</volume>(<issue>5</issue>), <fpage>e1893232024</fpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Nayebi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kong</surname>, <given-names>N. C.</given-names></string-name>, <string-name><surname>Zhuang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gardner</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Norcia</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name><surname>Yamins</surname>, <given-names>D. L.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Shallow unsupervised models best predict neural responses in mouse visual cortex</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ohki</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Reid</surname>, <given-names>R. C.</given-names></string-name></person-group> (<year>2014</year>). <article-title>In vivo two-photon calcium imaging in the visual system</article-title>. <source>Cold Spring Harbor Protocols</source>, <volume>2014</volume>(<issue>4</issue>), <elocation-id>pdb–prot081455</elocation-id>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Olah</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Cammarata</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Schubert</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Goh</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Petrov</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Carter</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2020</year>). <article-title>An overview of early vision in inceptionv1</article-title>. <source>Distill</source>. <ext-link ext-link-type="uri" xlink:href="https://distill.pub/2020/circuits/early-vision">https://distill.pub/2020/circuits/early-vision</ext-link>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Oliver</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Winter</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dupréla Tour</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Eickenberg</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name></person-group> (<year>2024</year>). <article-title>A biologicallyinspired hierarchical convolutional energy model predicts V4 responses to natural videos</article-title>. <source>bioRxiv</source>, (p. 2024.12.16.628781).</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pasupathy</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Connor</surname>, <given-names>C. E.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Shape representation in area v4: position-specific tuning for boundary conformation</article-title>. <source>Journal of neurophysiology</source>, <volume>86</volume>(<issue>5</issue>), <fpage>2505</fpage>–<lpage>2519</lpage>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pasupathy</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Connor</surname>, <given-names>C. E.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Population coding of shape in area V4</article-title>. <source>Nat. Neurosci</source>., <volume>5</volume>(<issue>12</issue>), <fpage>1332</fpage>–<lpage>1338</lpage>.</mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pasupathy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Popovkina</surname>, <given-names>D. V.</given-names></string-name>, &amp; <string-name><surname>Kim</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Visual functions of primate area V4</article-title>. <source>Annu Rev Vis Sci</source>, <volume>6</volume>, <fpage>363</fpage>–<lpage>385</lpage>.</mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Paszke</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gross</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Massa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Lerer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bradbury</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chanan</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Killeen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Gimelshein</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Antiga</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Desmaison</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kopf</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>DeVito</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Raison</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Tejani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chilamkurthy</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Steiner</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Fang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Bai</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Chintala</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>. In <conf-name>Advances in Neural Information Processing Systems 32</conf-name>, (pp. <fpage>8024</fpage>–<lpage>8035</lpage>). <publisher-name>Curran Associates, Inc</publisher-name>.</mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Penfield</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Boldrey</surname>, <given-names>E.</given-names></string-name></person-group> (<year>1937</year>). <article-title>Somatic motor and sensory representation in the cerebral cortex of man as studied by electrical stimulation</article-title>. <source>Brain</source>, <volume>60</volume>, <fpage>389</fpage>–<lpage>443</lpage>.</mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ponce</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Xiao</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Schade</surname>, <given-names>P. F.</given-names></string-name>, <string-name><surname>Hartmann</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Kreiman</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Livingstone</surname>, <given-names>M. S.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Evolving images for visual neurons using a deep generative network reveals coding principles and neuronal preferences</article-title>. <source>Cell</source>, <volume>177</volume> (<issue>4</issue>), <fpage>999</fpage>–<lpage>1009</lpage>.</mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pospisil</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Pasupathy</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Bair</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2018</year>). <article-title>‘artiphysiology’ reveals v4-like shape tuning in a deep network trained for image classification</article-title>. <source>eLife</source>, <volume>7</volume>.</mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reid</surname>, <given-names>R. C.</given-names></string-name></person-group> (<year>2012</year>). <article-title>From functional architecture to functional connectomics</article-title>. <source>Neuron</source>, <volume>75</volume>(<issue>2</issue>), <fpage>209</fpage>–<lpage>217</lpage>.</mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Safarani</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Nix</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Willeke</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Cadena</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Restivo</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Denfield</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name>, &amp; <string-name><surname>Sinz</surname>, <given-names>F. H.</given-names></string-name></person-group> (<year>2021</year>). <source>Towards robust vision by multi-task learning on monkey visual cortex</source>. URL <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2107.14344">https://arxiv.org/abs/2107.14344</ext-link></mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Salman</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ilyas</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Engstrom</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kapoor</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Madry</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Do adversarially robust imagenet models transfer better?</article-title> <source>arXiv</source>:<pub-id pub-id-type="doi">10.48550/arXiv.2007.08489</pub-id>.</mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shan</surname>, <given-names>K. Q.</given-names></string-name>, <string-name><surname>Lubenov</surname>, <given-names>E. V.</given-names></string-name>, &amp; <string-name><surname>Siapas</surname>, <given-names>A. G.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Model-based spike sorting with a mixture of drifting t-distributions</article-title>. <source>Journal of neuroscience methods</source>, <volume>288</volume>, <fpage>82</fpage>–<lpage>98</lpage>.</mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sinz</surname>, <given-names>F. H.</given-names></string-name>, <string-name><surname>Pitkow</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Reimer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Engineering a less artificial intelligence</article-title>. <source>Neuron</source>, <volume>103</volume>(<issue>6</issue>), <fpage>967</fpage>–<lpage>979</lpage>.</mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Srinath</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Emonds</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Lempel</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Dunn-Weiss</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Connor</surname>, <given-names>C. E.</given-names></string-name>, &amp; <string-name><surname>Nielsen</surname>, <given-names>K. J.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Early emergence of solid shape coding in natural and deep network vision</article-title>. <source>Curr. Biol</source>. <volume>31</volume>:<fpage>51</fpage>–<lpage>65.e5</lpage>.</mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Szegedy</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Jia</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Sermanet</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Reed</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Anguelov</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Erhan</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Vanhoucke</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Rabinovich</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Going deeper with convolutions</article-title>. In <conf-name>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>, (pp. <fpage>1</fpage>–<lpage>9</lpage>).</mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tanaka</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Weber</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Creutzfeldt</surname>, <given-names>O.</given-names></string-name></person-group> (<year>1986</year>). <article-title>Visual properties and spatial distribution of neurones in the visual association area on the prelunate gyrus of the awake monkey</article-title>. <source>Experimental Brain Research</source>, <volume>65</volume>, <fpage>11</fpage>–<lpage>37</lpage>.</mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Song</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Cai</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Lu</surname>, <given-names>H. D.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Curvature-processing domains in primate V4</article-title>. <source>eLife</source>, <volume>9</volume>.</mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tanigawa</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>H. D.</given-names></string-name>, &amp; <string-name><surname>Roe</surname>, <given-names>A. W.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Functional organization for color and orientation in macaque V4</article-title>. <source>Nat. Neurosci</source>., <volume>13</volume>(<issue>12</issue>), <fpage>1542</fpage>–<lpage>1548</lpage>.</mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name>, <string-name><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name>, <string-name><surname>Knutsen</surname>, <given-names>T. A.</given-names></string-name>, <string-name><surname>Mandeville</surname>, <given-names>J. B.</given-names></string-name>, &amp; <string-name><surname>Tootell</surname>, <given-names>R. B.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Faces and objects in macaque cerebral cortex</article-title>. <source>Nature neuroscience</source>, <volume>6</volume>(<issue>9</issue>), <fpage>989</fpage>–<lpage>995</lpage>.</mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Ustyuzhaninov</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Burg</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Cadena</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Fu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Muhammad</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Ponder</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Froudarakis</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name>, &amp; <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Digital twin reveals combinatorial code of non-linear computations in the mouse primary visual cortex</article-title>. <source>bioRxiv</source></mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ustyuzhaninov</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Cadena</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Froudarakis</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Fahey</surname>, <given-names>P. G.</given-names></string-name>, <string-name><surname>Walker</surname>, <given-names>E. Y.</given-names></string-name>, <string-name><surname>Cobos</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Reimer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sinz</surname>, <given-names>F. H.</given-names></string-name>, <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name></person-group> (<year>2019</year>). <source>Rotation-invariant clustering of neuronal responses in primary visual cortex</source>.</mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van der Maaten</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Hinton</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Visualizing data using t-sne</article-title>. <source>Journal of Machine Learning Research</source>, <volume>9</volume>(<issue>86</issue>), <fpage>2579</fpage>–<lpage>2605</lpage>.</mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van der Walt</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Schönberger</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Nunez-Iglesias</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Boulogne</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Warner</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Yager</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Gouillart</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Yu</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2014</year>). <article-title>scikit-image: image processing in python</article-title>. <source>PeerJ</source>, <volume>2</volume>, <fpage>e453</fpage>.</mixed-citation></ref>
<ref id="c93"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Victor</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Purpura</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Katz</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Mao</surname>, <given-names>B.</given-names></string-name></person-group> (<year>1994</year>). <article-title>Population encoding of spatial frequency, orientation, and color in macaque v1</article-title>. <source>Journal of neurophysiology</source>, <volume>72</volume>(<issue>5</issue>), <fpage>2151</fpage>–<lpage>2166</lpage>.</mixed-citation></ref>
<ref id="c94"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Walker</surname>, <given-names>E. Y.</given-names></string-name>, <string-name><surname>Sinz</surname>, <given-names>F. H.</given-names></string-name>, <string-name><surname>Cobos</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Muhammad</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Froudarakis</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Fahey</surname>, <given-names>P. G.</given-names></string-name>, <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Reimer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Pitkow</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Inception loops discover what excites neurons most using deep predictive models</article-title>. <source>Nat. Neurosci</source>. <volume>22</volume>:<fpage>2060</fpage>–<lpage>2065</lpage>.</mixed-citation></ref>
<ref id="c95"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Yao</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Hong</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Jiang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Andolina</surname>, <given-names>I. M.</given-names></string-name>, &amp; <string-name><surname>Tang</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Large-scale calcium imaging reveals a systematic V4 map for encoding natural scenes</article-title>. <source>Nat. Commun</source>., <volume>15</volume>(<issue>1</issue>), <fpage>6401</fpage>.</mixed-citation></ref>
<ref id="c96"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Yao</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Hong</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Jiang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Andolina</surname>, <given-names>I. M.</given-names></string-name>, &amp; <string-name><surname>Tang</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2023</year>). <article-title>A calcium imaging large dataset reveals novel functional organization in macaque v4</article-title>. <source>arXiv</source>:<pub-id pub-id-type="doi">10.48550/arXiv.2307.00932</pub-id>.</mixed-citation></ref>
<ref id="c97"><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Waskom</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Botvinnik</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>O’Kane</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hobson</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Lukauskas</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gemperline</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Augspurger</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Halchenko</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Cole</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Warmenhoven</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>de Ruiter</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Pye</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Hoyer</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Vanderplas</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Villalba</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kunter</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Quintero</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Bachant</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Martin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Meyer</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Miles</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Ram</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Yarkoni</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Evans</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Fitzgerald</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Brian Fonnesbeck</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Qalieh</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2017</year>). <article-title>mwaskom/seaborn: v0.8.1</article-title> <source>Zenodo</source> <pub-id pub-id-type="doi">10.5281/zenodo.883859</pub-id></mixed-citation></ref>
<ref id="c98"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Willeke</surname>, <given-names>K. F.</given-names></string-name>, <string-name><surname>Fahey</surname>, <given-names>P. G.</given-names></string-name>, <string-name><surname>Bashiri</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pede</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Burg</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Blessing</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Cadena</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Lurz</surname>, <given-names>K.-K.</given-names></string-name>, <string-name><surname>Ponder</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Muhammad</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Patel</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name>, &amp; <string-name><surname>Sinz</surname>, <given-names>F. H.</given-names></string-name></person-group> (<year>2022</year>). <source>The sensorium competition on predicting large-scale mouse primary visual cortex activity</source>. URL <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2206.08666">https://arxiv.org/abs/2206.08666</ext-link></mixed-citation></ref>
<ref id="c99"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamins</surname>, <given-names>D. L.</given-names></string-name>, &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title>. <source>Nature neuroscience</source>, <volume>19</volume>(<issue>3</issue>), <fpage>356</fpage>–<lpage>365</lpage>.</mixed-citation></ref>
<ref id="c100"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamins</surname>, <given-names>D. L.</given-names></string-name>, <string-name><surname>Hong</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Cadieu</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Solomon</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Seibert</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proceedings of the national academy of sciences</source>, <volume>111</volume>(<issue>23</issue>), <fpage>8619</fpage>–<lpage>8624</lpage>.</mixed-citation></ref>
<ref id="c101"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Yatsenko</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Reimer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Walker</surname>, <given-names>E. Y.</given-names></string-name>, <string-name><surname>Sinz</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Berens</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Hoenselaar</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cotton</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Siapas</surname>, <given-names>A. S.</given-names></string-name>, &amp; <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Datajoint: managing big scientific data using matlab or python</article-title>. <source>BioRxiv</source>, (p. 031658).</mixed-citation></ref>
<ref id="c102"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeiler</surname>, <given-names>M. D.</given-names></string-name>, &amp; <string-name><surname>Fergus</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Visualizing and understanding convolutional networks</article-title>. In <source>Computer Vision–ECCV 2014</source>, vol. <volume>8689</volume>, (pp. <fpage>818</fpage>–<lpage>833</lpage>).</mixed-citation></ref>
<ref id="c103"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Schriver</surname>, <given-names>K. E.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>Roe</surname>, <given-names>A. W.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Spatial frequency representation in V2 and V4 of macaque monkey</article-title>. <source>eLife</source>, <volume>12</volume>(<issue>e81794</issue>), <elocation-id>e81794</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.81794</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109875.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Moore</surname>
<given-names>Tirin</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University, Howard Hughes Medical Institute</institution>
</institution-wrap>
<city>Stanford</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study addresses a key, long-standing question about how visual feature selectivity is organized in mid-level visual cortex, using an ambitious combination of large-scale neural recordings and image synthesis. It provides <bold>important</bold> insights into the complexity of single-neuron selectivity and suggests a structured organization across cortical depth. While the evidence is generally <bold>solid</bold> and technically impressive, several key claims would be strengthened by additional controls, particularly regarding the sources of similarity across neurons and the dependence of the results on modeling choices.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109875.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Willeke et al. hypothesize that macaque V4, like other visual areas, may exhibit a topographic functional organization. One challenge to studying the functional (tuning) organization of V4 is that neurons in V4 are selective for complex visual stimuli that are hard to parameterize. Thus, the authors leverage an approach comprising digital twins and most exciting stimuli (MEIs) that they have pioneered. This data-driven, deep-learning framework can effectively handle the difficulty of parametrizing relevant stimuli. They verify that the model-synthesized MEIs indeed drive V4 neurons more effectively than matched natural image controls. They then performed psychophysics experiments (on humans) along with the application of contrastive learning to illustrate that anatomically neighboring neurons often care about similar stimuli. Importantly, the weaknesses of the approach are clearly appreciated and discussed.</p>
<p>Comments:</p>
<p>(1) The correlation between predictions and data is 0.43. I'd agree with the authors that this is &quot;reliable&quot; and would recommend that they discuss how the fact that performance is not saturated influences the results.</p>
<p>(2) Modeling V4 using a CNN and claiming that the identified functional groups look like those found in artificial vision systems may be a bit circular.</p>
<p>(3) No architecture other than ResNet-50 was tested. This might be a major drawback, since the MEIs could very well be reflections of the architecture and also the statistics of the dataset, rather than intrinsic biological properties. Do the authors find the same result with different architectures as the basis of the goal-driven model?</p>
<p>(4) The closed-loop analysis seems to be using a much smaller sample of the recorded neurons - &quot;resulting in n=55 neurons for the analysis of the closed-loop paradigm&quot;.</p>
<p>(5) A discussion on adversarial machine learning and the adversarial training that was used is lacking.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109875.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This is an ambitious and technically powerful study, investigating a long-standing question about the functional organization of area V4. The project combined large-scale single-unit electrophysiology in macaque V4 with deep learning-based activation maximization to characterize neuronal tuning in natural image space. The authors built predictive encoding models for V4 neurons and used these models to synthesize most exciting images (MEIs), which are subsequently validated in vivo using a closed-loop experimental paradigm.</p>
<p>Overall, the manuscript advances three main claims:</p>
<p>(1) Individual V4 neurons showed complex and highly structured selectivity for naturalistic visual features, including textures, curvatures, repeating patterns, and apparently eye-like motifs.</p>
<p>(2) Neurons recorded along the same linear probe penetration tended to have more similar MEIs than neurons recorded at different cortical locations (this similarity was supported by human psychophysics and by distances in a learned, contrastive image embedding space).</p>
<p>(3) MEIs clustered into a limited number of functional groups that resembled feature visualizations observed in deep convolutional neural networks.</p>
<p>Strengths:</p>
<p>(1) The study is important in that it is the first to apply activation maximization to neurons sampled at such fine spatial resolution. The authors used 32-channel linear silicon probes, spanning approximately 2 mm of cortical depth, with inter-contact spacing of roughly 60 µm. This enabled fine sampling across most of the cortical thickness of V4, substantially finer resolution than prior Utah-array or surface-biased approaches.</p>
<p>(2) A key strength is the direct in vivo validation of model-derived synthetic images by stimulating the same neurons used to build the models, a critical step often absent in other neural network-based encoding studies.</p>
<p>(3) More broadly, the study highlights the value of probing neuronal selectivity with rich, naturalistic stimulus spaces rather than relying exclusively on oversimplified stimuli such as Gabors.</p>
<p>Weaknesses:</p>
<p>(1) A central claim is that neurons sampled within the same penetration shared MEI tuning properties compared to neurons sampled in different penetrations because of functional organization. I am concerned about technical correlations in activity due to technical or methodology-related approaches (for example, shared reference or grounding) instead of functional organization alone. These recordings were obtained with linear silicon probes, and there have been observations that neuronal activity along this type of probe (including neuropixels probes) may be correlated above what prior work showed, using manually advanced single electrodes. For example, Fujita et al. (1992) showed finer micro-domains and systematic changes in selectivity along a cortical penetration, and it is not clear if that is true or detectable here. I think that the manuscript would be strengthened by a more thorough and explicit characterization of lower-level response correlations (at the neuronal electrophysiology level) prior to starting with fitting models. In particular, the authors could examine noise correlations along the electrode shaft (using the repeated test images, for example), as well as signal correlations in tuning, both within and across sessions. It would also be helpful to clarify whether these correlations depended on penetration day, recording chamber hole (how many were used?), or spatial separation between penetrations, and whether repeated use of the same hole yielded stable or changing correlations. Illustrations of the peristimulus time histogram changes across the shaft and across penetrations would also help. All of this would help us understand if the reports of clustering were technically inevitable due to the technique.</p>
<p>(2) It is difficult to understand a story of visual cortex neurons without more information about their receptive field locations and widths, particularly given that the stimulus was full-screen. I understand that there was a sparse random dot stimulus used to find the population RF, so it should be possible to visualize the individual and population RFs. Also, the investigators inferred the locations of the important patches using a masking algorithm, but where were those masks relative to the retinal image, and how distributed were they as a function of the shaft location? This would help us understand how similar each contact was.</p>
<p>(3) A major claim is that V4 MEIs formed groups that were comparable to those produced by artificial vision systems, &quot;suggesting potential shared encoding strategies.&quot; The issue is that the &quot;shared encoding strategy&quot; might be the authors' use of this same class of models in the first place. It would be useful to know if different functional groups arise as a function of other encoding neural network models, beyond the robust-trained ResNet-50. I am unsure to what extent the reported clustering, depth-wise similarity, and correspondence to artificial features depended on architectural and training bias. It would substantially strengthen the manuscript to test whether a similar organizational structure would emerge using alternative encoding models, such as attention-based vision transformers, self-supervised visual representations, or other non-convolutional architectures. Another important point of contrast would be to examine the functional groups encoded by the ResNet architecture before its activations were fit to V4 neuronal activity: put simply, is ResNet just re-stating what it already knows?</p>
<p>(4) Several comparisons to prior work are presented largely at a qualitative level, without quantitative support. For example, the authors state that their MEIs are consistent with known tuning properties of macaque V4, such as selectivity for shape, curvature, and texture. However, this claim is not supported by explicit image analyses or metrics that would substantiate these correspondences beyond appeal to visual inspection. Incorporating quantitative analyses, for instance, measures of curvature, texture statistics, or comparisons to established stimulus sets, would strengthen these links to prior literature and clarify the relationship between the synthesized MEIs and previously characterized V4 tuning properties.</p>
</body>
</sub-article>
</article>