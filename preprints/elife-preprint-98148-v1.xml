<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">98148</article-id>
<article-id pub-id-type="doi">10.7554/eLife.98148</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98148.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The transformation of sensory to perceptual braille letter representations in the visually deprived brain</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1683-8679</contrib-id>
<name>
<surname>Haupt</surname>
<given-names>Marleen</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">¶</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Graumann</surname>
<given-names>Monika</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">¶</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Teng</surname>
<given-names>Santani</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kaltenbach</surname>
<given-names>Carina</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cichy</surname>
<given-names>Radoslaw M.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Education and Psychology, Freie Universität Berlin</institution>, Berlin, <country>Germany</country></aff>
<aff id="a2"><label>2</label><institution>Berlin School of Mind and Brain, Faculty of Philosophy, Humboldt-Universität zu Berlin</institution>, Berlin, <country>Germany</country></aff>
<aff id="a3"><label>3</label><institution>Smith-Kettlewell Eye Research Institute</institution>, San Francisco, <country>USA</country></aff>
<aff id="a4"><label>4</label><institution>Bernstein Center for Computational Neuroscience Berlin</institution>, Berlin, <country>Germany</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>SP</surname>
<given-names>Arun</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>correspondence to: <email>marleen.haupt@gmail.com</email></corresp>
<fn id="n1" fn-type="equal"><label>¶</label><p>these authors contributed equally</p></fn>
<fn fn-type="supported-by"><p><bold>Funding:</bold> The study was supported by German Research Council grants (CI241/1-1, CI241/3-1, CI241/7-1) and European Research Council grants (ERC-StG-2018-803370) to R.M.C. The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.</p></fn>
<fn fn-type="con"><p><bold>Author contributions:</bold> M.G., S.T., and R.M.C. designed research. M.G. and C.K. collected data. M.H. and M.G. analyzed data. M.H. and R.M.C. wrote the manuscript. M.G. and S.T. provided feedback on the manuscript. R.M.C acquired funding.</p></fn>
<fn fn-type="others"><p><bold>Competing interests:</bold> The authors declare no competing interests.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-06-28">
<day>28</day>
<month>06</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP98148</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-04-10">
<day>10</day>
<month>04</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-03-22">
<day>22</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.02.12.579923"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Haupt et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Haupt et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-98148-v1.pdf"/>
<abstract>
<label>1.</label>
<title>Abstract</title>
<p>Experience-based plasticity of the human cortex mediates the influence of individual experience on cognition and behavior. The complete loss of a sensory modality is among the most extreme such experiences. Investigating such a selective, yet extreme change in experience allows for the characterization of experience-based plasticity at its boundaries.</p>
<p>Here, we investigated information processing in individuals who lost vision at birth or early in life by probing the processing of braille letter information. We characterized the transformation of braille letter information from sensory representations depending on the reading hand to perceptual representations that are independent of the reading hand.</p>
<p>Using a multivariate analysis framework in combination with fMRI, EEG and behavioral assessment, we tracked cortical braille representations in space and time, and probed their behavioral relevance.</p>
<p>We located sensory representations in tactile processing areas and perceptual representations in sighted reading areas, with the lateral occipital complex as a connecting “hinge” region. This elucidates the plasticity of the visually deprived brain in terms of information processing.</p>
<p>Regarding information processing in time, we found that sensory representations emerge before perceptual representations. This indicates that even extreme cases of brain plasticity adhere to a common temporal scheme in the progression from sensory to perceptual transformations.</p>
<p>Ascertaining behavioral relevance through perceived similarity ratings, we found that perceptual representations in sighted reading areas, but not sensory representations in tactile processing areas are suitably formatted to guide behavior.</p>
<p>Together, our results reveal a nuanced picture of both the potentials and limits of experience-dependent plasticity in the visually deprived brain.</p>
</abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Separate section on code availability added; Github link added</p></fn>
</fn-group>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds004956">https://openneuro.org/datasets/ds004956</ext-link>
</p></fn>
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds004951">https://openneuro.org/datasets/ds004951</ext-link>
</p></fn>
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://osf.io/a64hp">https://osf.io/a64hp</ext-link>
</p></fn>
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/marleenhaupt/BrailleLetterRepresentations">https://github.com/marleenhaupt/BrailleLetterRepresentations</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>2.</label>
<title>Introduction</title>
<p>Human brains vary due to individual experiences. This so-called experience-based plasticity of the human cortex mediates cognitive and behavioral adaptation to changes in the environment <sup><xref ref-type="bibr" rid="c2">2</xref></sup>. Typically, plasticity reflects learning from species-typical experiences. However, plasticity also results from species-atypical changes to experience like the loss of a sensory modality.</p>
<p>Sensory loss constitutes a selective, yet large-scale change in experience that offers a unique experimental opportunity to study cortical plasticity at its boundaries <sup><xref ref-type="bibr" rid="c3">3</xref></sup>. One deeply investigated case of sensory loss is blindness, i.e., the lack of visual input to the brain. Previous research has shown that cortical structures most strongly activated by visual input in sighted brains are activated by a plethora of other cognitive functions in visually deprived brains <sup><xref ref-type="bibr" rid="c4">4</xref></sup>, including braille reading <sup><xref ref-type="bibr" rid="c5">5</xref>–<xref ref-type="bibr" rid="c11">11</xref></sup>. However, overlapping functional responses alone cannot inform us about the nature of the observed activations, i.e., what kind of information they represent and thus what role they play in cognitive processing.</p>
<p>To elucidate the nature of information processing in the visually deprived brain, we investigate the tactile braille system in individuals who lost vision at birth or early in life (hereafter blind participants). Braille readers commonly use both hands, requiring their brain to transform sensory tactile input into a hand-independent perceptual format. We made use of this practical everyday requirement to experimentally characterize the transformation of sensory to perceptual braille letter representations. We operationalize sensory braille letter representations as representations coding information specific to the hand that was reading (hand-dependent). In contrast, we operationalize perceptual braille letter representations as representations coding information independent of which hand was reading (hand-independent).</p>
<p>Combining this operationalization with fMRI and EEG in a multivariate analysis framework <sup><xref ref-type="bibr" rid="c12">12</xref>–<xref ref-type="bibr" rid="c15">15</xref></sup>, we determine the cortical location and temporal emergence of sensory and perceptual representations. Lastly, to ascertain the functional role of the identified representations, we relate them to behavioral similarity ratings <sup><xref ref-type="bibr" rid="c16">16</xref>–<xref ref-type="bibr" rid="c19">19</xref></sup>.</p>
</sec>
<sec id="s2">
<label>3.</label>
<title>Results</title>
<p>We recorded fMRI (N = 15) and EEG (N = 11) data while blind participants (see Supplementary Table 1) read braille letters with their left or right index finger. We delivered the braille stimuli using single piezo-electric refreshable cells. This allowed participants to read braille letters without moving their finger, thus avoiding finger motion artifacts in the brain signal and analyses.</p>
<p>We used a common experimental paradigm for fMRI and EEG that was adapted to the specifics of each imaging modality. The common stimulus set consisted of ten different braille letters (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). Eight letters entered the main analysis. Two letters (E and O) served as vigilance targets to which participants responded with their foot; these trials were excluded from all analyses. The stimuli were presented in random order, with each trial consisting of a 500ms stimulus presentation to either the right or left hand. In fMRI, all trials were followed by an inter-stimulus interval (ISI) of 2,500ms to account for the sluggishness of the BOLD response (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). In EEG, standard trials had an ISI of 500ms while catch trials had an ISI of 1,100ms in order to avoid movement contaminations.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Stimuli, experimental design, pattern extraction and multivariate analysis framework.</title>
<p>A) We presented eight braille letters (B,C,D,L,M,N,V,Z) to participants on braille cells. Two additional letters (E and O) served as catch trials and were excluded from all analyses.</p>
<p>B) Top: During the fMRI session, braille letters were presented for 500ms with an inter-stimulus interval (ISI) of 2500ms. Participants were instructed to respond to catch trials by pressing a button with their foot. Bottom: During a separate EEG session, braille letters were presented for 500ms. The ISI following regular trials lasted 500ms, the ISI following catch trials lasted 1,100ms to avoid movement confounds. Participants were instructed to respond to catch trials by pressing a foot pedal.</p>
<p>C) In fMRI, we extracted voxel-wise activations for every region of interest (ROI). In EEG, we extracted channel-wise activations for every time point. In both cases, this resulted in one response vector per letter and per experimental run.</p>
<p>D) For both fMRI and EEG, we divided pattern vectors into training (4 pseudo-runs) and test (1 pseudo-run) sets. For every pair of braille letters (e.g., B and V), we trained a support vector machine (SVM) to classify between pattern vectors related to the presentation of both letters read with the same hand. We then tested the SVM on the left-out pattern vectors related to the presentation of the same two letters read with the same hand (within-hand classification) or with the other hand (across-hand classification). The resulting pairwise decoding accuracies were aggregated in a decoding accuracy matrix that is symmetric along the diagonal, with the diagonal itself being undefined. We interpret the within-hand matrix (black) as a measure of sensory and perceptual braille letter representations. We interpret the across-hand matrix (green) as a measure of perceptual braille letter representations. We derive the measure of sensory braille letter representations (blue) by subtracting one matrix from the other.</p></caption>
<graphic xlink:href="579923v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The common experimental paradigm for fMRI and EEG allowed us to use an equivalent multivariate classification scheme to track the transformation of sensory to perceptual representations. We assessed fMRI voxel patterns to reveal where sensory braille letter representations are located in the cortex. Likewise, we assessed EEG electrode patterns to reveal the temporal dynamics of braille letter representations (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>).</p>
<p>We operationalized sensory versus perceptual representations as hand-dependent versus hand-independent braille letter representations, respectively. To measure perceptual representations, we trained classifiers on brain data recorded during stimulation of one hand and tested the classifiers on data recorded during the stimulation of the other hand (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>). We refer to this analysis as across-hand classification. It reveals perceptual braille letter representations that are independent of the specific hand being stimulated.</p>
<p>To assess sensory representations, we used a two-step procedure. In a first step, we trained and tested classifiers on brain data recorded during stimulation of the same hand (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>). We refer to this analysis as within-hand classification. It reveals both sensory and perceptual braille letter representations. Thus, to further isolate sensory representations from perceptual representations, in a second step, we subtracted across-hand classification from within-hand classification results.</p>
<sec id="s2a">
<label>3.1</label>
<title>Spatial dynamics of braille letter representations</title>
<p>We started the analyses by determining the locations of sensory and perceptual braille letter representations in the visually deprived brain using fMRI. We focused our investigation on two sets of cortical regions based on previous literature: tactile processing areas and sighted reading areas (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). Given the tactile nature of braille, we expected braille letters to be represented in the tactile processing stream encompassing somatosensory cortices (S1 and S2), intra-parietal cortex (IPS), and insula <sup><xref ref-type="bibr" rid="c20">20</xref></sup>. Given that visual reading information is processed in a ventral processing stream <sup><xref ref-type="bibr" rid="c21">21</xref>–<xref ref-type="bibr" rid="c23">23</xref></sup>, and that braille reading has been observed to elicit activations along those nodes <sup><xref ref-type="bibr" rid="c5">5</xref>–<xref ref-type="bibr" rid="c9">9</xref></sup>, we investigated the sighted processing stream ranging from early visual cortex (EVC) <sup><xref ref-type="bibr" rid="c24">24</xref></sup> over V4 <sup><xref ref-type="bibr" rid="c25">25</xref></sup> and the lateral occipital complex (LOC) <sup><xref ref-type="bibr" rid="c26">26</xref></sup> to the letter form area (LFA) <sup><xref ref-type="bibr" rid="c27">27</xref></sup> and visual word form area (VWFA) <sup><xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup>.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Spatial dynamics of braille letter representations.</title>
<p>A) Rendering of regions of interests associated with tactile processing (S1: primary somatosensory cortex, S2: secondary somatosensory cortex, aIPS: anterior intraparietal sulcus, pIPS: posterior intraparietal sulcus, insula) and sighted reading (EVC: early visual cortex, V4: visual area 4, LOC: lateral occipital complex, LFA: letter form area, VWFA: visual word form area).</p>
<p>B) Sensory and perceptual (left), sensory (middle) and perceptual (right) braille letters representations in tactile processing and sighted reading ROIs (N = 15, two-tailed Wilcoxon signed-rank test, P &lt; 0.05, FDR corrected; Stars below bars indicate significance above chance. Error bars represent 95% confidence intervals. Dots represent single subject data.</p>
<p>C) fMRI searchlight results for sensory (blue) and perceptual (green) braille letter representations (N = 15, height threshold P&lt; 0.001, cluster-level FWE corrected P &lt; 0.05, colored voxels indicate significance). Results for combined sensory and perceptual representations are in Supplementary Figure 1.</p></caption>
<graphic xlink:href="579923v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We hypothesized (H1) that sensory braille letter information is represented in tactile processing areas (H1.1), while perceptual braille letter representations are located in sighted reading areas (H1.2). To test H1, we conducted within-hand and across-hand classification of braille letters in the above-mentioned areas for tactile processing and sighted reading (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>) in a region-of-interest (ROI) analysis.</p>
<p>We found that within-hand classification of braille letters was significantly above chance in regions associated with both tactile processing (S1, S2, aIPS, pIPS, insula) and sighted reading (EVC, LOC, LFA, VWFA) (<xref rid="fig2" ref-type="fig">Fig. 2b</xref> left; N = 15, one-tailed Wilcoxon signed-rank test, P &lt; 0.05, FDR corrected). As expected, this reveals both tactile and sighted reading areas as potential candidate regions housing sensory and perceptual braille letter representations.</p>
<p>To pinpoint the loci of sensory representations we subtracted the results of the across-hand classification from the within-hand classification. We found the difference to be significant in tactile processing areas (S1, S2, aIPS and pIPS) and in LOC, but not elsewhere (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>, middle). This confirms H1.1 in that sensory braille letter representations are located in tactile areas.</p>
<p>To determine the location of perceptual representations, we assessed the results of across-hand classification of braille letters. We found significant information (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>, right) only in sighted reading areas (EVC, LOC, VWFA), with the exception of the insula. This confirms H1.2 in that perceptual braille letter representations emerge predominantly in sighted reading areas. The surprising finding of the insula can possibly be explained by the insula’s heterogeneous functions <sup><xref ref-type="bibr" rid="c30">30</xref></sup> beyond tactile processing.</p>
<p>Notably, LOC is the only region that contained both sensory and perceptual braille letter representations. This suggests a “hinge” function in the transformation from sensory to perceptual braille letter representations.</p>
<p>To ascertain whether other areas beyond our hypothesized ROIs contained braille letter representations, we conducted a spatially unbiased fMRI searchlight classification analysis. The results confirmed that braille letter representations are located in the assessed ROIs without revealing any additional regions (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>, N = 15, height threshold P&lt; 0.001, cluster-level FWE corrected P &lt; 0.05).</p>
<p>Together, our fMRI results revealed sensory braille letter representations in tactile processing areas and perceptual braille letter representations in sighted reading areas, with LOC serving as a “hinge” region between them.</p>
</sec>
<sec id="s2b">
<label>3.2</label>
<title>Temporal dynamics of braille letter representations</title>
<p>We next determined the temporal dynamics with which braille letter representations emerge using EEG.</p>
<p>We hypothesized (H2) that sensory braille letter representations emerge in time before perceptual braille letter representations, analogous to the sequential processing of sensory representations before perceptual representations in the visual <sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c31">31</xref>–<xref ref-type="bibr" rid="c33">33</xref></sup> and auditory <sup><xref ref-type="bibr" rid="c34">34</xref></sup> domain.</p>
<p>To test H2, we conducted time-resolved within- and across-hand classification on EEG data. We determined the time point at which representations emerge by finding the first time point with respect to the onset of the braille stimulation where the classification effects are significant (fifty consecutive significant time point criteria, 95% confidence intervals reported in brackets).</p>
<p>The EEG classification analyses revealed significant and reliable results for both within- and across- hand classification of braille letters, as well as their difference (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>; N = 11, 1,000 bootstraps, one-tailed Wilcoxon signed-rank test, P &lt; 0.05, FDR corrected). We found that within-hand classification became significant at 62ms (29-111ms) (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>, blue curve). To isolate sensory representations, we subtracted the results of the across-hand classification from the within-hand classification. This difference became significant at 77ms (45-138ms) (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>, black curve). In contrast, the across-hand classification, indicating perceptual representations, became significant later at 184ms (127-230ms) (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>, green curve).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Temporal dynamics of braille letter representations.</title>
<p>A) EEG results for sensory and perceptual (black), sensory (blue) and perceptual (green) braille letter representations in time. Shaded areas around curves indicate standard error of the mean. Significance onsets and their 95% confidence intervals are indicated by dots and horizontal lines below curves (color-coded as curves, N = 11, 1,000 bootstraps, one-tailed Wilcoxon signed-rank test, P &lt; 0.05, FDR corrected).</p>
<p>B) EEG searchlight results for sensory and perceptual (top), sensory (middle) and perceptual (bottom) braille letter representations in EEG channel space (sampled down to 10ms resolution) in 100ms intervals. Significant electrodes are marked with black circles (N = 11, one-tailed Wilcoxon signed-rank test, P &lt; 0.05, FDR corrected across electrodes and time points).</p></caption>
<graphic xlink:href="579923v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Importantly, the temporal dynamics of sensory and perceptual representations differed significantly. Compared to sensory representations, the significance onset of perceptual representations was delayed by 107ms (21-167ms) (N = 11, 1,000 bootstraps, one-tailed bootstrap test against zero, P= 0.012).</p>
<p>To approximate the sources of the temporal signals, we complemented the EEG classification analysis with a searchlight classification analysis in EEG sensor space. In the time window of highest decodability (∼200-300ms), sensory braille letter information was decodable from widespread electrodes across the scalp. In contrast, perceptual braille letter information was decodable later and from overall fewer electrodes which were located over right frontal, central, left parietal and left temporal areas (<xref rid="fig3" ref-type="fig">Figure 3b</xref>; N = 11, one-tailed Wilcoxon signed-rank test, P &lt; 0.05, FDR corrected across electrodes and time points). These additional results reinforce that sensory braille letter information is represented in more widespread brain areas than perceptual braille letter information, corroborating our ROI classification results.</p>
<p>In sum, our EEG results characterized the temporal dynamics with which braille letter representations emerge as a temporal sequence of sensory before perceptual representations.</p>
</sec>
<sec id="s2c">
<label>3.3</label>
<title>Relating representations of braille letters to behavior</title>
<p>The ultimate goal of perception is to provide an organism with representations enabling adaptive behavior. The analyses across space and time described above identified potential candidates for such braille letter representations in the sense that the representations distinguished between braille letters. However, not all of these representations have to be used by the brain to guide behavior; some of these representations might be epiphenomenal and only available to the experimenter <sup><xref ref-type="bibr" rid="c35">35</xref>–<xref ref-type="bibr" rid="c37">37</xref></sup>.</p>
<p>Therefore, we tested the hypothesis (H3) that the sensory and perceptual braille letter representations identified in space (H1) and time (H2) are in a suitable format to be behaviorally relevant. We used perceived similarity as a proxy for behavioral relevance. The idea is that if two stimuli are perceived to be similar, they will also elicit similar actions <sup><xref ref-type="bibr" rid="c16">16</xref>–<xref ref-type="bibr" rid="c19">19</xref></sup>. For this, we acquired perceived similarity ratings from blind participants (N=19) in a separate behavioral experiment (<xref rid="fig4" ref-type="fig">Fig. 4a</xref> middle) in which participants verbally rated the similarity of each pair of braille letters from the stimulus set on a scale.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Representational similarity of braille letters in neural and behavioral measures.</title>
<p>A) For fMRI (top) and EEG (bottom), we formed representational dissimilarity matrices (RDMs) from pairwise decoding accuracies. In the behavioral experiment (middle), we presented two braille letters (e.g., B and V) on two braille cells. For all combinations, we asked participants to read both braille letters with the same hand (e.g., right) and verbally rate their similarity on a scale from 1-7. We formed the behavioral RDM from those perceptual similarity judgements for every letter pair. We then correlated the behavioral RDM (averaged over participants) with neural RDMs (subject specific) using Spearman’s R.</p>
<p>B) Results of RSA relating fMRI and behavior in ROIs that showed significant sensory (left) and perceptual (right) braille letter representations in fMRI (see. <xref ref-type="fig" rid="fig2">Fig. 2</xref>) (N = 15, two-tailed Wilcoxon signed-rank test, P &lt; 0.05, FDR corrected). Stars below bars indicate significance above chance. Error bars represent 95% confidence intervals. Dots represent single subject data.</p>
<p>C) Results of RSA relating EEG and behavior for sensory (blue) and perceptual (green) representations. Shaded areas around curves indicate standard error of the mean. Significance onsets and their 95% confidence intervals are indicated by dots and lines below curves (N = 11, 1,000 bootstraps, one-tailed Wilcoxon signed-rank test, P &lt; 0.05, FDR corrected, color-coded as result curves).</p></caption>
<graphic xlink:href="579923v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To relate perceived similarity ratings to neural representations, we used representational similarity analysis (RSA) <sup><xref ref-type="bibr" rid="c38">38</xref></sup>. RSA relates different measurement spaces (such as EEG, fMRI and behavior) by abstracting them into a common representational similarity space. We sorted behavioral similarity ratings into representational dissimilarity matrices (RDMs) indexed in rows and columns by the 8 braille letters used in experimental conditions (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>). For neural data, we sorted decoding accuracies from the previous analyses as a dissimilarity measure <sup><xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c39">39</xref></sup>. Thus, by arranging the classification results of the EEG and fMRI results we obtained EEG RDMs for every time point and fMRI RDMs for each ROI (for the within-hand and the across-hand analyses separately). To finally relate behavioral and neural data in the common RDM space, we correlated the behavioral RDM with each fMRI ROI RDM and each EEG time point RDM.</p>
<p>Considering braille letter representations in space through fMRI (<xref rid="fig4" ref-type="fig">Fig. 4b</xref>, N = 15, one-tailed Wilcoxon signed-rank test, P &lt; 0.05, FDR corrected), we found that previously identified perceptual representations (i.e., identified by the across-hand analysis) in EVC, LOC, VWFA, and insula showed significant correlations with behavior. In contrast, sensory representations (i.e., identified by the differences between within and across-hand analyses) in S1, S2, aIPS, pIPS and LOC were not significantly correlated with behavior. This indicates that perceptual braille letter representations in sighted reading areas are suitably formatted to guide behavior.</p>
<p>Considering braille letter representations in time through EEG (<xref rid="fig4" ref-type="fig">Fig. 4c</xref>; N = 11, 1,000 bootstraps, one-tailed Wilcoxon signed-rank test, P &lt; 0.05, FDR corrected), we found significant relationships with behavior for both sensory and perceptual representations. The temporal dynamics mirrored those of the EEG classification analysis (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>), in that the results related to sensory representations emerged earlier at 220ms (167-567ms) than the results related to perceptual representations at 466ms (249-877ms). The onset latency differences of 240ms (−81-636ms) (N = 11, 1,000 bootstrap, one-tailed bootstrap test against zero, P = 0.046) was significant. This indicates that both earlier sensory representations and later perceptual representations of braille letters are suitable formatted to guide behavior.</p>
<p>In sum, our RSA results highlighted that perceptual representations in sighted reading areas, as well as initial sensory and later perceptual representations in time, are suitably formatted to guide behavior.</p>
</sec>
</sec>
<sec id="s3">
<label>4.</label>
<title>Discussion</title>
<p>We assessed experience-based brain plasticity at its boundaries by investigating the nature of braille information processing in the visually deprived brain. For this, we assessed the transformation of sensory to perceptual braille letter representations in blind participants. Our experimental strategy combining fMRI, EEG, and behavioral assessment yielded three key findings about spatial distribution, temporal emergence and behavioral relevance. First, concerning the spatial distribution of braille letter representations, we found that sensory braille letter representations are located in tactile processing areas while perceptual braille letter representations are located in sighted reading areas. Second, concerning the temporal emergence of braille letter representations, we found that sensory braille letter representations emerge before perceptual braille letter representations. Third, concerning the behavioral relevance of representations, we found that perceptual representations identified in sighted reading areas, as well as sensory and perceptual representations identified in time, are suitably formatted to guide behavior.</p>
<sec id="s3a">
<label>4.1</label>
<title>The topography of sensory and perceptual braille letter representations</title>
<p>Previous research has identified the regions activated during braille reading in high detail <sup><xref ref-type="bibr" rid="c5">5</xref>–<xref ref-type="bibr" rid="c11">11</xref></sup>. However, activation in a brain region alone does not indicate its functional role or the kind of information it represents <sup><xref ref-type="bibr" rid="c40">40</xref></sup>. Here, we characterize the information represented in a region by distinguishing between sensory and perceptual representations of single braille letters. Our findings extend our understanding of the cortical regions processing braille letters in the visually deprived brain in five ways.</p>
<p>First, we clarified the role of EVC activations in braille reading <sup><xref ref-type="bibr" rid="c5">5</xref>–<xref ref-type="bibr" rid="c11">11</xref></sup> by showing that EVC harbors representations of single braille letters. More specifically, our finding that EVC represents perceptual rather than sensory braille letter information indicates that EVC representations are formatted at a higher perceptual level rather than a tactile input level. Previous studies also found that EVC of blind participants processes other higher-level information such as natural sounds <sup><xref ref-type="bibr" rid="c41">41</xref>,<xref ref-type="bibr" rid="c42">42</xref></sup> and language <sup><xref ref-type="bibr" rid="c43">43</xref>–<xref ref-type="bibr" rid="c52">52</xref></sup>. A parsimonious view is that EVC in the visually deprived brain engages in higher-level computations shared across domains, rather than performing multiple distinct lower-level sensory computations. Importantly, higher-level computations are not limited to the EVC in visually deprived brains. Natural sound representations <sup><xref ref-type="bibr" rid="c41">41</xref></sup> and language activations <sup><xref ref-type="bibr" rid="c53">53</xref></sup> are also located in EVC of sighted participants. This suggests that EVC, in general, has the capacity to process higher-level information <sup><xref ref-type="bibr" rid="c54">54</xref></sup>. Thus, EVC in the visually deprived brain might not be undergoing fundamental changes in brain organization <sup><xref ref-type="bibr" rid="c53">53</xref></sup>. This promotes a view of brain plasticity in which the cortex is capable of dynamic adjustments within pre-existing computational capacity limits <sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c53">53</xref>–<xref ref-type="bibr" rid="c55">55</xref></sup>.</p>
<p>Second, we found that VWFA contains perceptual braille letter representations. By clarifying the representational format of language representations in VWFA, our results support previous findings of the VWFA being functionally selective for letter and word stimuli in the visually deprived brain <sup><xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c56">56</xref>,<xref ref-type="bibr" rid="c57">57</xref></sup>.</p>
<p>Third, LOC represented hand-dependent and -independent braille letter information, suggesting a “hinge” function between sensory and perceptual braille letter representations. We stipulate that shape serves as an intermediate level representational format in between lower-level properties such as specific location of tactile stimulation or dot number in braille letters and higher-level perceptual letter features <sup><xref ref-type="bibr" rid="c58">58</xref></sup>.</p>
<p>Fourth, the finding of letter representations of tactile origin in both VWFA and LOC indicate that the functional organization of both regions is multimodal, contributing to the debate on how experience from vision or other sensory modalities shapes representations along the ventral stream <sup><xref ref-type="bibr" rid="c58">58</xref>,<xref ref-type="bibr" rid="c59">59</xref></sup>.</p>
<p>Fifth, we observed that the somatosensory cortices and intra-parietal sulci represent hand-dependent but not hand-independent braille letter representations. This is consistent with previous studies reporting that the primary somatosensory cortex represents the location of tactile stimulation <sup><xref ref-type="bibr" rid="c60">60</xref></sup>, but not the identity of braille words <sup><xref ref-type="bibr" rid="c57">57</xref></sup>. Taken together, these findings suggest that these tactile processing areas represent sensory rather than higher-level features of tactile inputs in visually deprived brains.</p>
<p>The involvement of the insula in processing braille letter information is more difficult to interpret. Based on previous studies in the sighted brain, the insula plays a role in tactile memory <sup><xref ref-type="bibr" rid="c61">61</xref>–<xref ref-type="bibr" rid="c63">63</xref></sup> and multisensory integration <sup><xref ref-type="bibr" rid="c64">64</xref>–<xref ref-type="bibr" rid="c67">67</xref></sup>. Both aspects could have contributed to our findings as braille letters are retrievable from long-term memory but are also inherently nameable and linked to auditory experiences. A future study could disambiguate the contributions of tactile memory and multisensory integration by presenting meaningless dot arrays, that are either unnamable or paired with invented names. Insular representations of trained, unnamable stimuli but not novel, unnamable stimuli would align with memory requirements. Insular representations of trained, namable stimuli but not trained, unnamable stimuli would favor audio-tactile integration.</p>
</sec>
<sec id="s3b">
<label>4.2</label>
<title>Sensory representations emerge before perceptual representations</title>
<p>Using time-resolved multivariate analysis of EEG data <sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup>, we showed that hand-dependent, sensory braille letter representations emerge in time before hand-independent, perceptual representations. Such sequential multi-step processing in time is a general principle of information processing in the human brain, also known in the visual <sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c32">32</xref></sup> and auditory <sup><xref ref-type="bibr" rid="c34">34</xref></sup> domain. Together, these findings suggest that the human brain, even in extreme instances of species-atypical cortical plasticity, honors this principle.</p>
<p>While braille letter reading follows the same temporal processing sequence as its visual counterpart, it operates on a different time scale. Our results indicate that braille letter classification peaks substantially later in time (∼200ms for hand-dependent and ∼390ms for hand-independent representations) than previously reported classification of visually presented words, letters, objects, and object categories (e.g., ∼125ms for location-dependent and ∼175ms for location-independent representations) <sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c68">68</xref>,<xref ref-type="bibr" rid="c69">69</xref></sup>. This discrepancy raises the question which factors could limit the speed of processing braille letters. Importantly, this delay is not a consequence of slower cortical processing in the tactile domain <sup><xref ref-type="bibr" rid="c70">70</xref></sup>. We find that tactile information reaches the cortex fast: we can classify which hand was stimulated as early as 35ms after stimulation onset (Supplementary Figure 2). Thus, the delay relates directly to the identification of braille letters.</p>
<p>A compelling explanation for the temporal processing properties of braille letter information are the underlying reading mechanics. Braille reading is slower than print reading <sup><xref ref-type="bibr" rid="c71">71</xref>,<xref ref-type="bibr" rid="c72">72</xref></sup> even if participants are fluent braille readers. This slowing is specific to braille reading and does not translate to other types of information intake in the visually deprived brain, e.g., auditory information. Blind participants have higher listening rates <sup><xref ref-type="bibr" rid="c73">73</xref></sup> and better auditory discrimination skills <sup><xref ref-type="bibr" rid="c74">74</xref></sup> than sighted participants, indicating more efficient auditory processing <sup><xref ref-type="bibr" rid="c75">75</xref>,<xref ref-type="bibr" rid="c76">76</xref></sup>. Together, this result pattern suggests that the temporal dynamics with which braille letter representations emerge are limited by the particular efficiency of the braille letter system, rather than the capacity of the brain. To test this idea, future studies could compare the temporal dynamics of braille letter and haptic object representations: a temporal processing delay specific to braille letters would support the hypothesis.</p>
</sec>
<sec id="s3c">
<label>4.3</label>
<title>Representations identified in space and time guide behavior</title>
<p>Our results clarified that perceptual rather than sensory braille letter representations identified in space are suitably formatted to guide behavior. However, we acknowledge that this finding is task-dependent. Arguably, general similarity ratings of braille letters depend more on intake-independent (e.g., dot arrangements or linguistic similarities such as pronunciation) than intake-dependent features (similarities in stimulation location on finger). Future behavioral assessments could ask participants to assess similarity separately based on only stimulation location or linguistic features. We would predict that similarity ratings based on stimulation location are related to sensory representations while similarity ratings based on linguistic features are related to perceptual representations of braille letters.</p>
<p>Concerning temporal dynamics, our results reveal that sensory representations of braille letters are relevant for behavior earlier than perceptual ones. Interestingly, the similarity between perceptual braille letter representations and behavioral similarity ratings emerges in a time window in which transcranial magnetic stimulation (TMS) over the VWFA affects braille letter reading in sighted braille readers <sup><xref ref-type="bibr" rid="c77">77</xref></sup>. This implies that around 320-420ms after the onset of reading braille, visually deprived and sighted brains utilize braille letter representations for performing tasks such as letter identification. Applying a comparable TMS protocol not only to sighted but also non-sighted braille readers would elucidate whether this time window of behavioral relevance can be generalized to braille reading, independent of visual experience.</p>
</sec>
<sec id="s3d">
<label>4.4</label>
<title>Conclusions</title>
<p>Our investigation of experience-based plasticity at its boundaries due to the loss of the visual modality reveals a nuanced picture of its potential and limits. On the one hand, our findings emphasize how plastic the brain is by showing that regions typically processing visual information adapt to represent perceptual braille letter information. On the other hand, our findings illustrate inherent limits of brain plasticity. Brain areas represent information from atypical inputs within the boundaries of their pre-existing computational capacity and the progression from sensory to perceptual transformations adheres to a common temporal scheme and functional role.</p>
</sec>
</sec>
<sec id="s5">
<label>5.</label>
<title>Methods</title>
<sec id="s5a">
<label>5.1</label>
<title>Participants</title>
<p>We conducted three separate experiments with partially overlapping participants: an fMRI, an EEG, and a behavioral experiment. All experiments were approved by the ethics committee of the Department of Education and Psychology of the Freie Universität Berlin and were conducted in accordance with the Declaration of Helsinki. 16 participants completed the fMRI experiment. One person was excluded due to technical problems during the recording, leaving a total of 15 participants in the fMRI experiment (mean age 39 years, <italic>SD</italic>=10, 9 female). 11 participants participated in the EEG experiment (<italic>N</italic>=11, mean age 44 years, <italic>SD</italic>=10, 8 female). The participant pools of the EEG and fMRI experiments overlapped by five participants. Out of a total of 21 participants, 19 participants (excluding one fMRI and one EEG participant) completed an additional behavioral task in which they rated the perceived similarity of braille letter pairs. All participants were blind since birth or early childhood (≤ 3 years, for details see Supplementary Table 1). All participants provided informed consent prior to the studies and received a monetary reward for their participation.</p>
</sec>
<sec id="s5b">
<label>5.2</label>
<title>Experimental stimuli and design</title>
<p>In all experiments, we presented braille letters (B, C, D, L, M, N, V, Z; <xref rid="fig2" ref-type="fig">Figure 2a</xref>) to the left and right index fingers of participants using piezo-actuated refreshable braille cells (<ext-link ext-link-type="uri" xlink:href="https://metec-ag.de/index.php">https://metec-ag.de/index.php</ext-link>) with two modules of 8 pins each. We only used the top 6 pins from each module to present letters from the braille alphabet. The modules were taped to the clothes of a participant for the fMRI experiment and on the table for the EEG and behavioral experiment. This way, participants could read in a comfortable position with their index fingers resting on the braille cells to avoid motion confounds. We instructed participants to read letters regardless of whether the pins stimulated their right or left index finger. We presented all eight letters to both hands, resulting in 16 experimental conditions (8 letters × 2 hands). In addition, two braille letters (E, O) were included as catch stimuli and participants were instructed to respond to them by pressing a button (fMRI) or pedal (EEG) with their foot. Catch trials were excluded from further analysis due to confounding motor and sensory signals.</p>
</sec>
<sec id="s5c">
<label>5.3</label>
<title>Experimental procedures</title>
<sec id="s5c1">
<label>5.3.1</label>
<title>fMRI experiment</title>
<p>The fMRI experiment consisted of two sessions. 15 participants completed the first fMRI session, during which we recorded a structural image (∼4 min.), a localizer run (7 min) and 10 runs of the main experiment (56 minutes). The total duration of the first session was 67 minutes excluding breaks. Eight of these 15 subjects completed a second fMRI recording session, in which we recorded an additional 15 runs of the main experiment (85 minutes). We did not record any structural images or localizer runs in the second session, resulting in a total duration of 85 minutes excluding breaks.</p>
<sec id="s5c1a">
<label>5.3.1.1</label>
<title>fMRI main experiment</title>
<p>During the fMRI main experiment, we presented participants with letters on braille cells and asked them to respond to occasionally appearing catch letters. We presented letters for 500ms, with a 2500ms inter-stimulus-interval (ISI; see <xref rid="fig2" ref-type="fig">Figure 2b</xref> top). Each regular trial - belonging to one of the 16 experimental conditions - was repeated 5 times per run (run duration: 337 s) in random order. Regular trials were interspersed every ∼20 trials with a catch trial, such that a catch trial occurred about once per minute. In addition, every 3<sup>rd</sup> to 5<sup>th</sup> trial (equally probable) was a null trial where no stimulation was given. In total, one run consisted of 80 regular trials, 5 catch trials and 22 null trials, amounting to a total of 107 trials per run.</p>
<p>To ensure that participants were able to read letters with both hands and understood the task instructions, participants first completed an experimental run outside the scanner.</p>
</sec>
<sec id="s5c1b">
<label>5.3.1.2</label>
<title>fMRI localizer experiment</title>
<p>To define regions-of-interest (ROIs), we performed a separate localizer experiment prior to the main fMRI experiment with tactile stimuli in four experimental conditions: braille letters read with the left hand, braille letters read with the right hand, fake letters read with the left hand and fake letters read with the right hand. The letters presented in the braille conditions were 16 letters from the alphabet excluding the letters used in the main experiment. The stimuli in the fake letter conditions were 16 tactile stimuli that were each composed of 8 dots, deviating from the standard 6-dot configuration in the braille alphabet.</p>
<p>The localizer experiment consisted of a single run lasting 432s, comprising 5 blocks of presentation of braille letters left, braille letters right, fake letters left, fake letters right and blank blocks as baseline. Each stimulation block was 14.4s long, consisting of 18 different letters presentations (500ms on, 300ms off) including two one-back repetitions that participants were instructed to respond to by pressing a button with their foot. We presented stimulation blocks in random order and regularly interspersed them with blank blocks.</p>
</sec>
</sec>
<sec id="s5c2">
<label>5.3.2</label>
<title>EEG experiment</title>
<p>The EEG experiment consisted of two sessions. 11 participants completed the first session and 8 of these participants completed a second session. The total duration of each session was 59 minutes excluding breaks.</p>
<p>The experimental setup was similar to that for fMRI but adapted to the specifics of EEG. We presented braille letters for 500ms with a 500ms ISI on regular trials. In catch trials, the letters were presented for 500ms with a 1100ms ISI to avoid contamination of movement on subsequent trials (see <xref rid="fig2" ref-type="fig">Figure 2b</xref> bottom). Each of the 16 experimental conditions was presented 170 times per session. Regular trials were interspersed every 5<sup>th</sup> to 7<sup>th</sup> trial (equally probable) with a catch trial. In total, one EEG recording session consisted of 2720 regular trials and 541 catch trials, amounting to a total of 3261 trials. 2 participants completed additional trials due to technical problems leading to a total of 190 and 180 repetitions per stimulus, accordingly.</p>
<p>Prior to the experiment, participants completed a short screening task during which each letter of the alphabet was presented for 500ms to each hand in random order. Participants were asked to verbally report the letter they had perceived to assess their reading capabilities with both hands using the same presentation time as in the experiment. The average performance for the left hand was 89% correct (SD = 10) and for the right hand it was 88% correct (SD = 13).</p>
</sec>
<sec id="s5c3">
<label>5.3.3</label>
<title>Behavioral letter similarity ratings</title>
<p>In a separate behavioral experiment, participants judged the perceived similarity of the braille letters used in the neuroimaging experiments. For this task, participants sat at a desk and were presented with two braille cells next to each other. Each pair of letters was presented once, and participants compared them with the same finger. The rating was without time constraints, meaning participants decided when they rated the stimuli. Participants were asked to verbally rate the similarity of each pair of braille letters on a scale from 1 = very similar to 7 = very different and the experimenter noted down their responses.</p>
</sec>
</sec>
<sec id="s5d">
<label>5.4</label>
<title>fMRI data acquisition, preprocessing and preparation</title>
<sec id="s5d1">
<label>5.4.1</label>
<title>fMRI acquisition</title>
<p>We acquired MRI data on a 3-T Siemens Tim Trio scanner with a 12-channel head coil. We obtained structural images using a T1-weighted sequence (magnetization-prepared rapid gradient-echo, 1 mm<sup><xref ref-type="bibr" rid="c3">3</xref></sup> voxel size). For the main experiment and the localizer run, we obtained functional images covering the entire brain using a T2*-weighted gradient-echo planar sequence (TR=2s, TE=30ms, flip angle=70°, 3 mm<sup><xref ref-type="bibr" rid="c3">3</xref></sup> voxel size, 37 slices, FOV=192mm, matrix size=64×64, interleaved acquisition).</p>
</sec>
<sec id="s5d2">
<label>5.4.2</label>
<title>fMRI preprocessing</title>
<p>For fMRI preprocessing, we used tools from FMRIB’s Software Library (FSL, <ext-link ext-link-type="uri" xlink:href="http://www.fmrib.ox.ac.uk/fsl">www.fmrib.ox.ac.uk/fsl</ext-link>). We excluded non-brain tissue from analysis using the Brain Extraction Tool (BET)<sup><xref ref-type="bibr" rid="c78">78</xref></sup> and motion corrected the data using MCFLIRT <sup><xref ref-type="bibr" rid="c79">79</xref></sup>. We did not apply high or low-pass temporal filters. We spatially smoothed fMRI localizer data with an 8mm FWHM Gaussian kernel. We registered functional images to the high-resolution structural scans and to the MNI standard template using FLIRT <sup><xref ref-type="bibr" rid="c80">80</xref></sup>. We carried out all further fMRI analyses in MATLAB R2021a (<ext-link ext-link-type="uri" xlink:href="http://www.mathworks.com">www.mathworks.com</ext-link>).</p>
</sec>
<sec id="s5d3">
<label>5.4.3</label>
<title>Univariate fMRI analysis</title>
<p>For all univariate fMRI analyses, we used SPM12 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/">http://www.fil.ion.ucl.ac.uk/spm/</ext-link>). For the main experiment, we modelled the fMRI responses to the 16 experimental conditions for each run using a general linear model (GLM). The onsets and durations of each image presentation entered the GLM as regressors and were convolved with a hemodynamic response function (hrf). Six movement parameters (pitch, yaw, roll, x-, y-, z-translation) entered the GLM as nuisance regressors. For each of the 16 conditions we converted GLM parameter estimates into <italic>t</italic>-values by contrasting each parameter estimate against the implicit baseline. This resulted in 16 condition specific <italic>t</italic>-value maps per run and participant.</p>
<p>For the localizer experiment, we modelled the fMRI response to the 5 experimental conditions entering block onsets and durations as regressors of interest and movement parameters as nuisance regressors before convolving with the hrf. From the resulting three parameter estimates we generated two contrasts. The first contrast served to localize activations in primary (S1) and secondary (S2) somatosensory cortex and was defined as letters &amp; fake letters &gt; baseline. The second contrast served to localize activations in early visual cortex (EVC), V4, lateral occipital complex (LOC), letter form area (LFA), visual word form area (VWFA), anterior intra-parietal sulcus (aIPS), posterior intra-parietal sulcus (pIPS) and insula and was defined as letters &gt; fake letters. In sum, this resulted in two <italic>t</italic>-value maps for the localizer run per participant.</p>
</sec>
<sec id="s5d4">
<label>5.4.4</label>
<title>Definition of regions-of-interest</title>
<p>To identify regions along the sighted reading and tactile processing pathway, we defined regions of interest (ROIs) in a two-step procedure. We first constrained ROIs by anatomical masks using brain atlases, in each case combining regions across both hemispheres. We included 5 ROIs from the sighted reading pathway: EVC (merging the anatomical masks of V1, V2 and V3), V4, LOC, LFA and the VWFA. We also included 5 ROIs from the tactile processing pathway: S1, S2, aIPS (merging the anatomical masks of IPS3, IPS4, IPS5), pIPS (merging the anatomical masks of IPS0, IPS1 and IPS2) and the insula. For EVC, V4, LOC, aIPS and pIPS, we used masks from the probabilistic Wang atlas <sup><xref ref-type="bibr" rid="c81">81</xref></sup>. For LFA, we defined the mask using the MarsBaR Toolbox (<ext-link ext-link-type="uri" xlink:href="https://marsbar-toolbox.github.io/">https://marsbar-toolbox.github.io/</ext-link>) with a 10 mm radius around the center voxel at MNI coordinates X=-40; Y=-78 and Z=-18 <sup><xref ref-type="bibr" rid="c27">27</xref></sup>. We also defined the VWFA mask using the MarsBaR Toolbox with a 10 mm radius around the center voxel at MNI coordinates X=-44; Y=-57 and Z=-13 <sup><xref ref-type="bibr" rid="c25">25</xref></sup> and converted from Talairach to MNI space using the MNI&lt;-&gt;Talairach Tool (<ext-link ext-link-type="uri" xlink:href="https://bioimagesuiteweb.github.io/bisweb-manual/tools/mni2tal.html">https://bioimagesuiteweb.github.io/bisweb-manual/tools/mni2tal.html</ext-link>). We created the mask for S1 by merging the sub-masks of BA1, BA2 and BA3 from the WFU PickAtlas (<ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/wfu_pickatlas/">https://www.nitrc.org/projects/wfu_pickatlas/</ext-link>) and the mask for S2 by merging the sub-masks operculum 1-4 from the Anatomy Toolbox <sup><xref ref-type="bibr" rid="c82">82</xref></sup>. Lastly, we extracted the mask for the insula from the WFU PickAtlas. The smallest mask included 321 voxels. Therefore, in a second step, we selected the 321 most activated voxels of the participant-specific localizer results within each of the masks, using the letters &amp; fake letters &gt; baseline contrast for S1 and S2 and the letters &gt; fake letters for the remaining ROIs. This yielded participant-specific definitions for all ROIs.</p>
</sec>
</sec>
<sec id="s5e">
<label>5.5</label>
<title>EEG data acquisition and preprocessing</title>
<p>We recorded EEG data using an EASYCAP 64-channel system and a Brainvision actiCHamp amplifier at a sampling rate of 1,000 Hz. The electrodes were placed according to the standard 10-10 system. The data was filtered online between 0.03 and 100 Hz and re-referenced online to FCz.</p>
<p>We preprocessed data offline using the EEGLAB toolbox version 14 <sup><xref ref-type="bibr" rid="c83">83</xref></sup>. We incorporated a low-pass filter with a cut-off at 50 Hz and epoched trials between -100 ms and 999 ms with respect to stimulus onset, resulting in 1,100 1ms data points per epoch. We baseline-corrected the epochs by subtracting the mean of the 100ms prestimulus time window from the epoch. We re-referenced the data offline to the average reference. To clean the data from artifacts such as eye blinks, eye movements and muscular contractions, we used independent component analysis as implemented in the EEGLAB toolbox. We used SASICA <sup><xref ref-type="bibr" rid="c84">84</xref></sup> to guide the visual inspection of components for removal. We identified components related to horizontal eye movements using two lateral frontal electrodes (F7-F8). During five recordings (1 participant first session, 4 participants second session), additional external electrodes were available that allowed for the direct recording of the horizontal electro-oculogram to identify and remove components related to horizontal eye movements. For blink artifact detection based on the vertical electro-oculogram, we used two frontal electrodes (Fp1 and Fp2). As a final step, we applied multivariate noise normalization to improve the signal-to-noise ratio (SNR) and reliability of the data <sup><xref ref-type="bibr" rid="c39">39</xref></sup>, resulting in subject-specific trial-based time courses of electrode activity.</p>
</sec>
<sec id="s5f">
<label>5.6</label>
<title>Braille letter classification from brain measurements</title>
<p>To determine the amount of information about braille letter identity present in brain measurements, we used a multivariate classification scheme <sup><xref ref-type="bibr" rid="c12">12</xref>–<xref ref-type="bibr" rid="c15">15</xref></sup>. We conducted subject-specific braille letter classification in two ways. First, we classified between letter pairs presented to one reading hand, i.e., we trained and tested a classifier on brain data recorded during the presentation of braille stimuli to the same hand (either the right or the left hand). This yields a measure of hand-dependent braille letter information in neural measurements. We refer to this analysis as within-hand classification. Second, we classified between letter pairs presented to different hands in that we trained a classifier on brain data recorded during the presentation of stimuli to one hand (e.g., right), and tested it on data related to the other hand (e.g., left). This yields a measure of hand-independent braille letter information in neural measurements. We refer to this analysis as across-hand classification.</p>
<p>All classification analyses were carried out in MATLAB R2021a (<ext-link ext-link-type="uri" xlink:href="http://www.mathworks.com">www.mathworks.com</ext-link>) and relied on binary c-support vector classification (C-SVC) with a linear kernel as implemented in the libsvm toolbox <sup><xref ref-type="bibr" rid="c85">85</xref></sup> (<ext-link ext-link-type="uri" xlink:href="https://www.csie.ntu.edu.tw/cjlin/libsvm">https://www.csie.ntu.edu.tw/cjlin/libsvm</ext-link>). Furthermore, all analyses were conducted in a participant specific manner. The next section describes the multivariate fMRI and EEG analyses in more detail.</p>
<sec id="s5f1">
<label>5.6.1</label>
<title>Spatially resolved multivariate fMRI analysis</title>
<p>We conducted both a ROI-based and a spatially unbiased volumetric searchlight procedure <sup><xref ref-type="bibr" rid="c86">86</xref></sup>. For each ROI included in the ROI-based analysis, we extracted and arranged <italic>t</italic>-values into pattern vectors for each of the 16 conditions and experimental runs. If participants completed only one session, the analysis was conducted on 10 runs. If participants completed both sessions, the 10 runs from session 1 and 15 runs from session 2 were pooled and the analysis was conducted across 25 runs. To increase the SNR, we randomly assigned run-wise pattern vectors into bins and averaged them into pseudo-runs. For participants with one session, the bin size was 2 runs, resulting in 5 pseudo-runs. If participants completed 2 sessions and thus had 25 runs, the bin size was 5 runs resulting in 5 pseudo-runs. Thus, in both cases, each participant ended up with five pseudo-run pattern vectors that entered the classification analysis. We then performed 5-fold leave-one-pseudo-run-out-cross validation, training on 4 and testing on one pseudo-trial per classification iteration.</p>
<p>We will first describe the classification procedure for braille letters within-hand and then for the classification of braille letters across-hand.</p>
</sec>
<sec id="s5f2">
<label>5.6.2</label>
<title>fMRI ROI-based classification of braille Letters within-hand</title>
<p>For the classification of braille letters within-hand, we assigned four pseudo-trials corresponding to the data from two braille letters of the same hand (e.g., right) to the training set. We then tested the SVM on the remaining, fifth pseudo-trial corresponding to data from the same two braille letters of the same hand (e.g., right) as in the training set but using held-out data for the testing set. This yielded percent classification accuracy (50% chance level) as output. Equivalent SVM training and testing was repeated for all combinations of letter pairs within each hand.</p>
<p>With 8 letters that were all classified pairwise once per hand, this resulted in 28 pairwise classification accuracies per hand. We averaged accuracies across condition pairs and hands, yielding a measure of hand-dependent braille letter information for each ROI and participant separately</p>
</sec>
<sec id="s5f3">
<label>5.6.3</label>
<title>fMRI ROI-based classification of braille Letters across-hand</title>
<p>The classification procedure of braille letters across reading hands was identical to the classification procedure within-hand with the important difference that the training data always came from one hand (e.g., right) and the testing data from the other hand (e.g., left).</p>
<p>With 8 letters that were all classified pairwise once across two hands, this resulted again in 28 pairwise classification accuracies across-hand per training-testing direction (i.e., train left, test right and vice versa). We averaged accuracies across condition pairs and training-testing directions, yielding a measure of hand-independent braille letter information for each ROI and participant separately.</p>
</sec>
<sec id="s5f4">
<label>5.6.4</label>
<title>fMRI searchlight classification of braille Letters</title>
<p>The searchlight procedure was conceptually equivalent to the ROI-based analysis. For each voxel vi in the 3D <italic>t</italic>-value maps, we defined a sphere with a radius of 4 voxels centered around voxel vi. For each condition and run, we extracted and arranged the <italic>t</italic>-values for each voxel of the sphere into pattern vectors. Classification of braille letters across-hand proceeded as described above. This resulted in one average classification accuracy for voxel vi. Iterated across all voxels this yielded a 3D volume of classification accuracies across the brain for each participant separately.</p>
</sec>
<sec id="s5f5">
<label>5.6.5</label>
<title>Time-resolved classification of braille letters within-hand from EEG data</title>
<p>To determine the timing with which braille letter information emerges in the brain, we conducted time-resolved EEG classification <sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup>. This procedure was conceptually equivalent to the fMRI braille letter classification in that it classified letter pairs either within or across-hand and was conducted separately for each participant.</p>
<p>For each time point of the epoched EEG data, we extracted 63 EEG channel activations and arranged them into pattern vectors for each of the 16 conditions. Participants who completed one session had 170 trials per condition and participants who completed two sessions had 340 trials per condition. To increase the SNR, we randomly assigned the trials into bins and averaged them into new pseudo-trials. For participants with one session, the bin size was 34 trials, resulting in 5 pseudo-trials. If participants completed 2 sessions and thus had 340 trials, the bin size was 68 trials resulting in 5 pseudo-trials. In both cases, each participant ended up with five pseudo-run pattern vectors that entered the classification analysis. We then performed 5-fold leave-one-pseudo-run-out-cross validation, training on 4 and testing on one pseudo-trial per classification iteration. This procedure was repeated 100 times with random assignment of trials to pseudo-trials, and across all combinations of letter pairs and hands. We averaged results across condition pairs, folds, iterations and hands, yielding a decoding accuracy time course reflecting how much hand-dependent braille letter information was present at each time point in each participant.</p>
</sec>
<sec id="s5f6">
<label>5.6.6</label>
<title>Time-resolved classification of braille letters across-hand from EEG data</title>
<p>The classification procedure for braille letters across-hand was identical to the classification of braille letters within-hand with the crucial difference that training and testing data always came from separate hands and results were averaged across condition pairs, folds, iterations and training-testing directions. Averaging results yielded a decoding accuracy time course reflecting how much hand-independent braille letter information was present at each time point in each participant.</p>
</sec>
<sec id="s5f7">
<label>5.6.7</label>
<title>Time-resolved EEG searchlight in sensor space</title>
<p>We conducted an EEG searchlight analysis resolved in time and sensor space (i.e., across 63 EEG channels) to gain insights into which EEG channels contributed to the results of the time-resolved analysis described above. For the EEG searchlight, we conducted the time-resolved EEG classification as described above with the following difference: For each EEG channel ci, we conducted the classification procedure on the four closest channels surrounding c. The classification accuracy was stored at the position of c. After iterating across all channels and down-sampling the time points to a 10ms resolution, this yielded a classification accuracy map across all channels and time points in 10ms steps for each participant.</p>
</sec>
</sec>
<sec id="s5g">
<label>5.7</label>
<title>Representational similarity analysis of brain data and behavioral letter similarity ratings</title>
<p>To determine the subset of neural braille letter representations identified that is relevant for behavior <sup><xref ref-type="bibr" rid="c16">16</xref>–<xref ref-type="bibr" rid="c19">19</xref></sup>, we compared perceptual letter similarity ratings to braille letter representations identified from EEG and fMRI signals using representational similarity analysis (RSA) <sup><xref ref-type="bibr" rid="c38">38</xref></sup>. RSA characterizes the representational space of a measurement space (e.g., fMRI or EEG data) with a representational dissimilarity matrix (RDM). RDMs aggregate pairwise distances between responses to all experimental conditions, thereby abstracting from the activity patterns of measurement units (e.g., fMRI voxels or EEG channels) to between-condition dissimilarities. The rationale of the approach is that neural measures and behavior are linked if their RDMs are similar.</p>
<p>We constructed RDMs for behavior, fMRI and EEG as follows.</p>
<p>For behavior, we arranged the perceptual similarity judgments averaged across participants (indicated by participants on a scale from 1 = very similar to 7 = very different) into an RDM format. All RDMs were averaged over both hands and had the dimensions 8 letters × 8 letters.</p>
<p>For both fMRI and EEG, we used the classification results from the conducted within-hand and across-hand classifications as a measure of (dis-)similarity relations between braille letters. Classification accuracies can be interpreted as a measure of dissimilarity because two conditions have a higher classification accuracy when they are more dissimilar<sup><xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c39">39</xref></sup>. Thus, we assembled participant-specific RDMs for each fMRI ROI and EEG time point in the time course from decoding accuracies.</p>
<p>In a final step we correlated (Spearman’s <italic>R</italic>) the lower triangular part of the respective RDMs (without the diagonal <sup><xref ref-type="bibr" rid="c84">84</xref></sup>). For fMRI, this resulted in one correlation value per participant and ROI. For EEG, this analysis resulted in one correlation time course per participant.</p>
</sec>
<sec id="s5h">
<label>5.8</label>
<title>Statistical testing</title>
<sec id="s5h1">
<label>5.8.1</label>
<title>Wilcoxon signed-rank test</title>
<p>We performed non-parametric one-tailed Wilcoxon signed-rank tests to test for above-chance classification accuracy for ROIs in the fMRI classification, for time points in the EEG classification, for time points and channels in the EEG searchlight, and for ROI and time courses in the RSA. In each case the null hypothesis was that the observed parameter (i.e., classification accuracy, correlation) came from a distribution with a median of chance level performance (i.e., 50% for pairwise classification; 0 correlation). The resulting <italic>P</italic>-values were corrected for multiple comparisons using the false discovery rate (FDR) <sup><xref ref-type="bibr" rid="c87">87</xref></sup> at 5% level if more than one test was conducted. This was done a) across ROIs in the ROI classification and fMRI-behavior RSA, b) across time points in the EEG classification and EEG-behavior RSA, and c) across time points and channels in the EEG searchlight.</p>
</sec>
<sec id="s5h2">
<label>5.8.2</label>
<title>Bootstrap tests</title>
<p>We used bootstrapping to compute 95% confidence intervals for onset latencies (the first 50 consecutive significant timepoints after trial onset) of EEG time courses as well as for determining the significance of onset latencies. In each case we sampled the participant pool 1,000 times with replacement calculated the statistic of interest for each sample.</p>
<p>For the EEG onset latency differences, we bootstrapped the latency difference between the onsets of the time courses of hand-dependent or hand-independent letter representations. This yielded an empirical distribution that could be compared to zero. To determine whether onset latencies differences in the EEG time courses were significantly different from zero, we computed the proportion of values that were equal to or smaller than zero and corrected them for multiple comparisons using FDR at <italic>P</italic>=0.05.</p>
</sec>
<sec id="s5h3">
<label>5.8.3</label>
<title>Other statistical tests</title>
<p>For the fMRI searchlight classification results, we applied a voxel-wise height threshold of P= 0.001. The resulting <italic>P</italic>-values were corrected for multiple comparisons using the family-wise error (FWE) at the 5% level.</p>
</sec>
</sec>
</sec>
<sec id="d1e1402" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1490">
<label>Supplementary Material</label>
<media xlink:href="supplements/579923_file02.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank all of our participants for taking part in our experiments. We also thank Agnessa Karapetian, Johannes Singer and Siying Xie for their valuable comments on the manuscript. We acquired EEG and fMRI data at the Center for Cognitive Neuroscience (CCNB), Freie Universität Berlin, and we thank the HPC Service of ZEDAT, Freie Universität Berlin, for computing time <sup><xref ref-type="bibr" rid="c1">1</xref></sup>.</p>
</ack>
<sec id="s6">
<label>5.9</label>
<title>Data availability</title>
<p>The raw fMRI and EEG data are available on OpenNeuro via <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds004956">https://openneuro.org/datasets/ds004956</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds004951/">https://openneuro.org/datasets/ds004951/</ext-link>. The preprocessed fMRI, EEG and behavioral data as well as the results of the ROI classification, time classification and RSA can be accessed on OSF via <ext-link ext-link-type="uri" xlink:href="https://osf.io/a64hp/">https://osf.io/a64hp/</ext-link>.</p>
</sec>
<sec id="s7">
<label>5.10</label>
<title>Code availability</title>
<p>The code used in this study is available on Github via <ext-link ext-link-type="uri" xlink:href="https://github.com/marleenhaupt/BrailleLetterRepresentations/">https://github.com/marleenhaupt/BrailleLetterRepresentations/</ext-link>.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="software"><string-name><surname>Bennett</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Melchers</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Proppe</surname>, <given-names>B.</given-names></string-name> <article-title>Curta: A General-purpose High-Performance Computer at ZEDAT</article-title>, <source>Freie Universität Berlin</source>. (<year>2020</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Pascual-Leone</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Amedi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Fregni</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Merabet</surname>, <given-names>L. B.</given-names></string-name> <article-title>The plastic human brain cortex</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>28</volume>, <fpage>377</fpage>–<lpage>401</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Ricciardi</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Bottari</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ptito</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Röder</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Pietrini</surname>, <given-names>P.</given-names></string-name> <article-title>The sensory-deprived brain as a unique tool to understand brain development and function</article-title>. <source>Neurosci. Biobehav. Rev</source>. <volume>108</volume>, <fpage>78</fpage>–<lpage>82</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Bedny</surname>, <given-names>M.</given-names></string-name> <article-title>Evidence from Blindness for a Cognitively Pluripotent Cortex</article-title>. <source>Trends Cogn. Sci</source>. <volume>21</volume>, <fpage>637</fpage>–<lpage>648</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Büchel</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Price</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name> <article-title>A multimodal language region in the ventral visual pathway</article-title>. <source>Nature</source> <volume>394</volume>, <fpage>274</fpage>–<lpage>277</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Burton</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Sinclair</surname>, <given-names>R. J.</given-names></string-name> &amp; <string-name><surname>Agato</surname>, <given-names>A.</given-names></string-name> <article-title>Recognition memory for Braille or spoken words: An fMRI study in early blind</article-title>. <source>Brain Res</source>. <volume>1438</volume>, <fpage>22</fpage>–<lpage>34</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Raczy</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal> <article-title>Orthographic Priming in Braille Reading as Evidence for Task-specific Reorganization in the Ventral Visual Cortex of the Congenitally Blind</article-title>. <source>J. Cogn. Neurosci</source>. <volume>31</volume>, <fpage>1065</fpage>–<lpage>1078</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Reich</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Szwed</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Cohen</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Amedi</surname>, <given-names>A.</given-names></string-name> <article-title>A ventral visual stream reading center independent of visual experience</article-title>. <source>Curr. Biol</source>. <volume>21</volume>, <fpage>363</fpage>–<lpage>368</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Sadato</surname>, <given-names>N.</given-names></string-name> <article-title>Neural networks for Braille reading by the blind</article-title>. <source>Brain</source> <volume>121</volume>, <fpage>1213</fpage>–<lpage>1229</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Uhl</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Franzen</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Lindinger</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Lang</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Deecke</surname>, <given-names>L.</given-names></string-name> <article-title>On the functionality of the visually deprived occipital cortex in early blind persons</article-title>. <source>Neurosci. Lett</source>. <volume>124</volume>, <fpage>256</fpage>–<lpage>259</lpage> (<year>1991</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Sadato</surname>, <given-names>N.</given-names></string-name> <etal>et al.</etal> <article-title>Activation of V1 by Braille reading in blind subjects</article-title>. <source>Nature</source> <volume>380</volume>, <fpage>526</fpage>–<lpage>528</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name> <etal>et al.</etal> <article-title>Probing principles of large-scale object representation: Category preference and location encoding</article-title>. <source>Hum. Brain Mapp</source>. <volume>34</volume>, <fpage>1636</fpage>–<lpage>1651</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Haynes</surname>, <given-names>J. D.</given-names></string-name> <article-title>Encoding the identity and location of objects in human LOC</article-title>. <source>Neuroimage</source> <volume>54</volume>, <fpage>2297</fpage>–<lpage>2307</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Carlson</surname>, <given-names>T. A.</given-names></string-name>, <string-name><surname>Hogendoorn</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Fonteijn</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Verstraten</surname>, <given-names>F. A.</given-names></string-name> <article-title>Spatial coding and invariance in object-selective cortex</article-title>. <source>Cortex</source> <volume>47</volume>, <fpage>14</fpage>–<lpage>22</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Isik</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Meyers</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Leibo</surname>, <given-names>J. Z.</given-names></string-name> &amp; <string-name><surname>Poggio</surname>, <given-names>T.</given-names></string-name> <article-title>The dynamics of invariant object recognition in the human visual system</article-title>. <source>J. Neurophysiol</source>. <volume>111</volume>, <fpage>91</fpage>–<lpage>102</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Jozwik</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>van den Bosch</surname>, <given-names>J. J. F.</given-names></string-name> &amp; <string-name><surname>Charest</surname>, <given-names>I.</given-names></string-name> <article-title>The spatiotemporal neural dynamics underlying perceived similarity for real-world objects</article-title>. <source>Neuroimage</source> <volume>194</volume>, <fpage>12</fpage>–<lpage>24</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Bankson</surname>, <given-names>B. B.</given-names></string-name>, <string-name><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name>, <string-name><surname>Groen</surname>, <given-names>I. I. A.</given-names></string-name> &amp; <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name> <article-title>The temporal evolution of conceptual object representations revealed through models of behavior, semantics and deep neural networks</article-title>. <source>Neuroimage</source> <volume>178</volume>, <fpage>172</fpage>–<lpage>182</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>Human object-similarity judgments reflect and transcend the primate-IT object representation</article-title>. <source>Front. Psychol</source>. <volume>4</volume>, <fpage>1</fpage>–<lpage>22</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Charest</surname>, <given-names>I.</given-names></string-name> <etal>et al.</etal> <article-title>Unique semantic space in the brain of each beholder predicts perceived similarity</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source>. <volume>111</volume>, <fpage>14565</fpage>–<lpage>14570</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Dijkerman</surname>, <given-names>H. C.</given-names></string-name> &amp; <string-name><surname>de Haan</surname>, <given-names>E. H. F.</given-names></string-name> <article-title>Somatosensory processes subserving perception and action</article-title>. <source>Behav. Brain Sci</source>. <volume>30</volume>, <fpage>189</fpage>–<lpage>201</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name> &amp; <string-name><surname>Cox</surname>, <given-names>D. D.</given-names></string-name> <article-title>Untangling invariant object recognition</article-title>. <source>Trends Cogn. Sci</source>. <volume>11</volume>, <fpage>333</fpage>–<lpage>341</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Zoccolan</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Rust</surname>, <given-names>N. C.</given-names></string-name> <article-title>How does the brain solve visual object recognition?</article-title> <source>Neuron</source> <volume>73</volume>, <fpage>415</fpage>–<lpage>434</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Goodale</surname>, <given-names>M. A.</given-names></string-name> &amp; <string-name><surname>Milner</surname>, <given-names>A. D.</given-names></string-name> <article-title>Separate visual pathways for perception and action</article-title>. <source>Trends Neurosci</source>. <volume>15</volume>, <fpage>20</fpage>–<lpage>25</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Cohen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sigman</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Vinckier</surname>, <given-names>F.</given-names></string-name> <article-title>The neural code for written words: A proposal</article-title>. <source>Trends Cogn. Sci</source>. <volume>9</volume>, <fpage>335</fpage>–<lpage>341</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Cohen</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal> <article-title>The visual word form area: spatial and temporal characterization of an initial stage of reading in normal subjects and posterior split-brain patients</article-title>. <source>Brain</source> <volume>123</volume>, <fpage>291</fpage>–<lpage>307</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Neudorf</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Gould</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Mickleborough</surname>, <given-names>M. J. S.</given-names></string-name>, <string-name><surname>Ekstrand</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Borowsky</surname>, <given-names>R.</given-names></string-name> <article-title>Unique, Shared, and Dominant Brain Activation in Visual Word Form Area and Lateral Occipital Complex during Reading and Picture Naming</article-title>. <source>Neuroscience</source> <volume>481</volume>, <fpage>178</fpage>–<lpage>196</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Thesen</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal> <article-title>Sequential then interactive processing of letters and words in the left fusiform gyrus</article-title>. <source>Nat. Commun</source>. <volume>3</volume>, <fpage>1</fpage>–<lpage>8</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>McCandliss</surname>, <given-names>B. D.</given-names></string-name>, <string-name><surname>Cohen</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name> <article-title>The visual word form area: Expertise for reading in the fusiform gyrus</article-title>. <source>Trends Cogn. Sci</source>. <volume>7</volume>, <fpage>293</fpage>–<lpage>299</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Cohen</surname>, <given-names>L.</given-names></string-name> <article-title>The unique role of the visual word form area in reading</article-title>. <source>Trends Cogn. Sci</source>. <volume>15</volume>, <fpage>254</fpage>–<lpage>262</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Uddin</surname>, <given-names>L. Q.</given-names></string-name>, <string-name><surname>Nomi</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Hébert-Seropian</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ghaziri</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Boucher</surname>, <given-names>O.</given-names></string-name> <article-title>Structure and Function of the Human Insula</article-title>. <source>J. Clin. Neurophysiol</source>. <volume>34</volume>, <fpage>300</fpage>–<lpage>306</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Carlson</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Tovar</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Alink</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> <article-title>Representational dynamics of object vision: The first 1000 ms</article-title>. <source>J. Vis</source>. <volume>13</volume>, <fpage>1</fpage>–<lpage>19</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Pantazis</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Oliva</surname>, <given-names>A.</given-names></string-name> <article-title>Resolving human object recognition in space and time</article-title>. <source>Nat. Neurosci</source>. <volume>17</volume>, <fpage>455</fpage>–<lpage>462</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Carlson</surname>, <given-names>T. A.</given-names></string-name>, <string-name><surname>Hogendoorn</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Kanai</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Mesik</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Turret</surname>, <given-names>J.</given-names></string-name> <article-title>High temporal resolution decoding of object position and category</article-title>. <source>J. Vis</source>. <volume>11</volume>, <fpage>1</fpage>–<lpage>17</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Lowe</surname>, <given-names>M. X.</given-names></string-name> <etal>et al.</etal> <article-title>Cochlea to categories: The spatiotemporal dynamics of semantic auditory representations</article-title>. <source>Cogn. Neuropsychol</source>. <volume>38</volume>, <fpage>468</fpage>–<lpage>489</lpage> (<year>2021</year>) doi:<pub-id pub-id-type="doi">10.1080/02643294.2022.2085085</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>de-Wit</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Alexander</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ekroll</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Wagemans</surname>, <given-names>J.</given-names></string-name> <article-title>Is neuroimaging measuring information in the brain?</article-title> <source>Psychon. Bull. Rev</source>. <volume>23</volume>, <fpage>1415</fpage>–<lpage>1428</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Reddy</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name> <article-title>Category Selectivity in the Ventral Visual Pathway Confers Robustness to Clutter and Diverted Attention</article-title>. <source>Curr. Biol</source>. <volume>17</volume>, <fpage>2067</fpage>–<lpage>2072</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Williams</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Dang</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Kanwisher</surname>, <given-names>N. G.</given-names></string-name> <article-title>Only some spatial patterns of fMRI response are read out in task performance</article-title>. <source>Nat. Neurosci</source>. <volume>10</volume>, <fpage>685</fpage>–<lpage>686</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Bandettini</surname>, <given-names>P.</given-names></string-name> <article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title>. <source>Front. Syst. Neurosci</source>. <volume>2</volume>, <fpage>1</fpage>–<lpage>28</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Guggenmos</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sterzer</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name> <article-title>Multivariate pattern analysis for MEG: A comparison of dissimilarity measures</article-title>. <source>Neuroimage</source> <volume>173</volume>, <fpage>434</fpage>–<lpage>447</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Haynes</surname>, <given-names>J. D.</given-names></string-name> &amp; <string-name><surname>Rees</surname>, <given-names>G.</given-names></string-name> <article-title>Decoding mental states from brain activity in humans</article-title>. <source>Nat. Rev. Neurosci</source>. <volume>7</volume>, <fpage>523</fpage>–<lpage>534</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Vetter</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal> <article-title>Decoding Natural Sounds in Early “Visual” Cortex of Congenitally Blind Individuals</article-title>. <source>Curr. Biol</source>. <volume>30</volume>, <fpage>3039</fpage>-<lpage>3044</lpage>.e2 (<year>2020</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Mattioni</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>Categorical representation from sound and sight in the ventral occipito-temporal cortex of sighted and blind</article-title>. <source>Elife</source> <volume>9</volume>, <fpage>1</fpage>–<lpage>33</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Burton</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>A. Z.</given-names></string-name>, <string-name><surname>Diamond</surname>, <given-names>J. B.</given-names></string-name> &amp; <string-name><surname>Raichle</surname>, <given-names>M. E.</given-names></string-name> <article-title>Adaptive changes in early and late blind: A fMRI study of verb generation to heard nouns</article-title>. <source>J. Neurophysiol</source>. <volume>88</volume>, <fpage>3359</fpage>–<lpage>3371</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Burton</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Diamond</surname>, <given-names>J. B.</given-names></string-name> &amp; <string-name><surname>McDermott</surname>, <given-names>K. B.</given-names></string-name> <article-title>Dissociating cortical regions activated by semantic and phonological tasks: A fMRI study in blind and sighted people</article-title>. <source>J. Neurophysiol</source>. <volume>90</volume>, <fpage>1965</fpage>–<lpage>1982</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Röder</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Stock</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Bien</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Neville</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Rösler</surname>, <given-names>F.</given-names></string-name> <article-title>Speech processing activates visual cortex in congenitally blind humans</article-title>. <source>Eur. J. Neurosci</source>. <volume>16</volume>, <fpage>930</fpage>–<lpage>936</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Bedny</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pascual-Leone</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dodell-Feder</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Fedorenko</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Saxe</surname>, <given-names>R.</given-names></string-name> <article-title>Language processing in the occipital cortex of congenitally blind adults</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source>. <volume>108</volume>, <fpage>4429</fpage>–<lpage>4434</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Lane</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kanjlia</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Omaki</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Bedny</surname>, <given-names>M.</given-names></string-name> <article-title>“Visual” cortex of congenitally blind adults responds to syntactic movement</article-title>. <source>J. Neurosci</source>. <volume>35</volume>, <fpage>12859</fpage>–<lpage>12868</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><surname>Dietrich</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hertrich</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Ackermann</surname>, <given-names>H.</given-names></string-name> <article-title>Ultra-fast speech comprehension in blind subjects engages primary visual cortex, fusiform gyrus, and pulvinar - a functional magnetic resonance imaging (fMRI) study</article-title>. <source>BMC Neurosci</source>. <volume>14</volume>, (<year>2013</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Watkins</surname>, <given-names>K. E.</given-names></string-name>, <string-name><surname>Coullon</surname>, <given-names>G. S. L.</given-names></string-name> &amp; <string-name><surname>Bridge</surname>, <given-names>H.</given-names></string-name> <article-title>Language and nonverbal auditory processing in the occipital cortex of individuals who are congenitally blind due to anophthalmia</article-title>. <source>Neuropsychologia</source> <volume>173</volume>, <fpage>108304</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Abboud</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Engemann</surname>, <given-names>D. A.</given-names></string-name> &amp; <string-name><surname>Cohen</surname>, <given-names>L.</given-names></string-name> <article-title>Semantic coding in the occipital cortex of early blind individuals</article-title>. <source>bioRxiv</source> <volume>539437</volume> (<year>2019</year>) doi:<pub-id pub-id-type="doi">10.1101/539437</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>Amedi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Raz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Pianka</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Malach</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Zohary</surname>, <given-names>E.</given-names></string-name> <article-title>Early ‘visual’ cortex activation correlates with superior verbal memory performance in the blind</article-title>. <source>Nat. Neurosci</source>. <volume>6</volume>, <fpage>758</fpage>–<lpage>766</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><surname>Burton</surname>, <given-names>H.</given-names></string-name> <article-title>Visual Cortex Activity in Early and Late Blind People</article-title>. <source>J. Neurosci</source>. <volume>23</volume>, <fpage>4005</fpage>–<lpage>4011</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><surname>Seydell-Greenwald</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Newport</surname>, <given-names>E. L.</given-names></string-name>, <string-name><surname>Bi</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Striem-Amit</surname>, <given-names>E.</given-names></string-name> <article-title>Spoken language processing activates the primary visual cortex</article-title>. <source>PLoS One</source> <volume>18</volume>, <fpage>1</fpage>–<lpage>22</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><surname>Pascual-Leone</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Hamilton</surname>, <given-names>R.</given-names></string-name> <article-title>The metamodal organization of the brain</article-title>. <source>Prog. Brain Res</source>. <volume>134</volume>, <fpage>427</fpage>–<lpage>445</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><surname>Makin</surname>, <given-names>T. R.</given-names></string-name> &amp; <string-name><surname>Krakauer</surname>, <given-names>J. W.</given-names></string-name> <article-title>Against cortical reorganisation</article-title>. <source>eLife</source>. <volume>12</volume>, <fpage>e84716</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><string-name><surname>Striem-Amit</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Dakwar</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Reich</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Amedi</surname>, <given-names>A.</given-names></string-name> <article-title>The large-scale organization of ‘visual’ streams emerges without visual experience</article-title>. <source>Cereb. Cortex</source> <volume>22</volume>, <fpage>1698</fpage>–<lpage>1709</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><string-name><surname>Liu</surname>, <given-names>Y.-F.</given-names></string-name>, <string-name><surname>Rapp</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Bedny</surname>, <given-names>M.</given-names></string-name> <article-title>Reading Braille by Touch Recruits Posterior Parietal Cortex</article-title>. <source>J. Cogn. Neurosci</source>. <volume>35</volume>, <fpage>1593</fpage>–<lpage>1616</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><string-name><surname>Amedi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Malach</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Hendler</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Peled</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Zohary</surname>, <given-names>E.</given-names></string-name> <article-title>Visuo-haptic object-related activation in the ventral visual pathway</article-title>. <source>Nat. Neurosci</source>. <volume>4</volume>, <fpage>324</fpage>–<lpage>330</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="other"><string-name><surname>Bedny</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>How does visual experience shape representations and transformations along the ventral stream?</article-title> <source>CCN Gener. Advers. Collab</source>. (<year>2021</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><string-name><surname>Sanchez-Panchuelo</surname>, <given-names>R. M.</given-names></string-name> <etal>et al.</etal> <article-title>Within-digit functional parcellation of brodmann areas of the human primary somatosensory cortex using functional magnetic resonance imaging at 7 tesla</article-title>. <source>J. Neurosci</source>. <volume>32</volume>, <fpage>15815</fpage>–<lpage>15822</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><string-name><surname>Bonda</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Petrides</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Evans</surname>, <given-names>A.</given-names></string-name> <article-title>Neural systems for tactual memories</article-title>. <source>J. Neurophysiol</source>. <volume>75</volume>, <fpage>1730</fpage>–<lpage>1737</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><string-name><surname>Burton</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Sinclair</surname>, <given-names>R. J.</given-names></string-name> <article-title>Attending to and Remembering Tactile Stimuli</article-title>. <source>J. Clin. Neurophysiol</source>. <volume>17</volume>, <fpage>575</fpage>–<lpage>591</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><string-name><surname>Reed</surname>, <given-names>C. L.</given-names></string-name>, <string-name><surname>Shoham</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Halgren</surname>, <given-names>E.</given-names></string-name> <article-title>Neural Substrates of Tactile Object Recognition: An fMRI Study</article-title>. <source>Hum. Brain Mapp</source>. <volume>21</volume>, <fpage>236</fpage>–<lpage>246</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><string-name><surname>Banati</surname>, <given-names>R.</given-names></string-name> <article-title>The functional anatomy of visual-tactile integration in man: a study using positron emission tomography</article-title>. <source>Neuropsychologia</source> <volume>38</volume>, <fpage>115</fpage>–<lpage>124</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><string-name><surname>Hadjikhani</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Roland</surname>, <given-names>P. E.</given-names></string-name> <article-title>Cross-modal transfer of information between the tactile and the visual representations in the human brain: A positron emission tomographic study</article-title>. <source>J. Neurosci</source>. <volume>18</volume>, <fpage>1072</fpage>–<lpage>1084</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><string-name><surname>Prather</surname>, <given-names>S. C.</given-names></string-name>, <string-name><surname>Votaw</surname>, <given-names>J. R.</given-names></string-name> &amp; <string-name><surname>Sathian</surname>, <given-names>K.</given-names></string-name> <article-title>Task-specific recruitment of dorsal and ventral visual areas during tactile perception</article-title>. <source>Neuropsychologia</source> <volume>42</volume>, <fpage>1079</fpage>–<lpage>1087</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><string-name><surname>Naghavi</surname>, <given-names>H. R.</given-names></string-name>, <string-name><surname>Eriksson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Larsson</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Nyberg</surname>, <given-names>L.</given-names></string-name> <article-title>The claustrum/insula region integrates conceptually related sounds and pictures</article-title>. <source>Neurosci. Lett</source>. <volume>422</volume>, <fpage>77</fpage>–<lpage>80</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><string-name><surname>Ling</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>A. C. H.</given-names></string-name>, <string-name><surname>Armstrong</surname>, <given-names>B. C.</given-names></string-name> &amp; <string-name><surname>Nestor</surname>, <given-names>A.</given-names></string-name> <article-title>How are visual words represented? Insights from EEG-based visual word decoding, feature derivation and image reconstruction</article-title>. <source>Hum. Brain Mapp</source>. <volume>40</volume>, <fpage>5056</fpage>–<lpage>5068</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><string-name><surname>Teng</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Cichy</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Pantazis</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Oliva</surname>, <given-names>A.</given-names></string-name> <article-title>The neural dynamics of letter perception in blind and sighted readers</article-title>. <source>J. Vis</source>. <volume>15</volume>, <fpage>126</fpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><string-name><surname>Müller</surname>, <given-names>F.</given-names></string-name> <etal>et al.</etal> <article-title>A thalamocortical pathway for fast rerouting of tactile information to occipital cortex in congenital blindness</article-title>. <source>Nat. Commun</source>. <volume>10</volume>, <fpage>1</fpage>–<lpage>9</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><string-name><surname>Bola</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal> <article-title>Braille in the sighted: Teaching tactile reading to sighted adults</article-title>. <source>PLoS One</source> <volume>11</volume>, <fpage>1</fpage>–<lpage>13</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><string-name><surname>Brysbaert</surname>, <given-names>M.</given-names></string-name> <article-title>How many words do we read per minute? A review and meta-analysis of reading rate</article-title>. <source>J. Mem. Lang</source>. <volume>109</volume>, <fpage>104047</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="other"><string-name><surname>Bragg</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bennett</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Reinecke</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Ladner</surname>, <given-names>R.</given-names></string-name> <article-title>A large inclusive study of human listening rates</article-title>. <source>Conf. Hum. Factors Comput. Syst. - Proc</source>. 2018-April, <fpage>1</fpage>–<lpage>12</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><string-name><surname>Hötting</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Röder</surname>, <given-names>B.</given-names></string-name> <article-title>Auditory and auditory-tactile processing in congenitally blind humans</article-title>. <source>Hear. Res</source>. <volume>258</volume>, <fpage>165</fpage>–<lpage>174</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><string-name><surname>Röder</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Rösler</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Hennighausen</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Näcker</surname>, <given-names>F.</given-names></string-name> <article-title>Event-related potentials during auditory and somatosensory discrimination in sighted and blind human subjects</article-title>. <source>Cogn. Brain Res</source>. <volume>4</volume>, <fpage>77</fpage>–<lpage>93</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><string-name><surname>Muchnik</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Efrati</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nemeth</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Malin</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Hildesheimer</surname>, <given-names>M.</given-names></string-name> <article-title>Central Auditory Skills In Blind And Sighted Subjects</article-title>. <source>Scand. Audiol</source>. <volume>20</volume>, <fpage>19</fpage>–<lpage>23</lpage> (<year>1991</year>).</mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="journal"><string-name><surname>Bola</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal> <article-title>Functional hierarchy for tactile processing in the visual cortex of sighted adults</article-title>. <source>Neuroimage</source> <volume>202</volume>, (<year>2019</year>).</mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name> <article-title>Fast robust automated brain extraction</article-title>. <source>Hum. Brain Mapp</source>. <volume>17</volume>, <fpage>143</fpage>–<lpage>155</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="journal"><string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bannister</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Brady</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Smith</surname>, <given-names>S.</given-names></string-name> <article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title>. <source>Neuroimage</source> <volume>17</volume>, <fpage>825</fpage>–<lpage>841</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Smith</surname>, <given-names>S.</given-names></string-name> <article-title>A global optimisation method for robust affine registration of brain images</article-title>. <source>Med. Image Anal</source>. <volume>5</volume>, <fpage>143</fpage>–<lpage>156</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c81"><label>81.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Mruczek</surname>, <given-names>R. E. B.</given-names></string-name>, <string-name><surname>Arcaro</surname>, <given-names>M. J.</given-names></string-name> &amp; <string-name><surname>Kastner</surname>, <given-names>S.</given-names></string-name> <article-title>Probabilistic maps of visual topography in human cortex</article-title>. <source>Cereb. Cortex</source> <volume>25</volume>, <fpage>3911</fpage>–<lpage>3931</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c82"><label>82.</label><mixed-citation publication-type="journal"><string-name><surname>Eickhoff</surname>, <given-names>S. B.</given-names></string-name> <etal>et al.</etal> <article-title>A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data</article-title>. <source>Neuroimage</source> <volume>25</volume>, <fpage>1325</fpage>–<lpage>1335</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c83"><label>83.</label><mixed-citation publication-type="journal"><string-name><surname>Delorme</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Makeig</surname>, <given-names>S.</given-names></string-name> <article-title>EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title>. <source>J. Neurosci. Methods</source> <volume>134</volume>, <fpage>9</fpage>–<lpage>21</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c84"><label>84.</label><mixed-citation publication-type="journal"><string-name><surname>Chaumon</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bishop</surname>, <given-names>D. V. M.</given-names></string-name> &amp; <string-name><surname>Busch</surname>, <given-names>N. A.</given-names></string-name> <article-title>A practical guide to the selection of independent components of the electroencephalogram for artifact correction</article-title>. <source>J. Neurosci. Methods</source> <volume>250</volume>, <fpage>47</fpage>–<lpage>63</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c85"><label>85.</label><mixed-citation publication-type="journal"><string-name><surname>Chang</surname>, <given-names>C.-C.</given-names></string-name> &amp; <string-name><surname>Lin</surname>, <given-names>C.-J.</given-names></string-name> <article-title>Libsvm: A library for support vector machines</article-title>. <source>ACM Trans. Intell. Syst. Technol</source>. <volume>2</volume>, <fpage>1</fpage>–<lpage>27</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c86"><label>86.</label><mixed-citation publication-type="journal"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Goebel</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Bandettini</surname>, <given-names>P.</given-names></string-name> <article-title>Information-based functional brain mapping</article-title>. <source>Proc. Natl. Acad. Sci</source>. <volume>103</volume>, <fpage>3863</fpage>–<lpage>3868</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c87"><label>87.</label><mixed-citation publication-type="journal"><string-name><surname>Benjamini</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Hochberg</surname>, <given-names>Y.</given-names></string-name> <article-title>Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing</article-title>. <source>J. R. Stat. Soc. Ser. B</source> <volume>57</volume>, <fpage>289</fpage>–<lpage>300</lpage> (<year>1995</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98148.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>SP</surname>
<given-names>Arun</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study investigates the brain representations of Braille letters in blind participants and provides <bold>convincing</bold> evidence using EEG and fMRI that the decoding of letter identity across the reading hand takes place in the visual cortex. The evidence supporting the claims of the authors is <bold>solid</bold>, although the inclusion of a sighted control group and additional analyses would have strengthened the study. The work will be of interest to neuroscientists working on brain plasticity.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98148.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The researchers examined how individuals who were born blind or lost their vision early in life process information, specifically focusing on the decoding of Braille characters. They explored the transition of Braille character information from tactile sensory inputs, based on which hand was used for reading, to perceptual representations that are not dependent on the reading hand.</p>
<p>They identified tactile sensory representations in areas responsible for touch processing and perceptual representations in brain regions typically involved in visual reading, with the lateral occipital complex serving as a pivotal &quot;hinge&quot; region between them.</p>
<p>In terms of temporal information processing, they discovered that tactile sensory representations occur prior to cognitive-perceptual representations. The researchers suggest that this pattern indicates that even in situations of significant brain adaptability, there is a consistent chronological progression from sensory to cognitive processing.</p>
<p>Strengths:</p>
<p>By combining fMRI and EEG, and focusing on the diagnostic case of Braille reading, the paper provides an integrated view of the transformation processing from sensation to perception in the visually deprived brain. Such a multimodal approach is still rare in the study of human brain plasticity and allows us to discern the nature of information processing in blind people's early visual cortex, as well as the time course of information processing in a situation of significant brain adaptability.</p>
<p>Weaknesses:</p>
<p>The lack of a sighted control group limits the interpretations of the results in terms of profound cortical reorganization, or simple unmasking of the architectural potentials already present in the normally developing brain. Moreover, the conclusions regarding the behavioral relevance of the sensory and perceptual representations in the putatively reorganized brain are limited due to the behavioral measurements adopted.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98148.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Haupt and colleagues performed a well-designed study to test the spatial and temporal gradient of perceiving braille letters in blind individuals. Using cross-hand decoding of the read letters, and comparing it to the decoding of the read letter for each hand, they defined perceptual and sensory responses. Then they compared where (using fMRI) and when (using EEG) these were decodable. Using fMRI, they showed that low-level tactile responses specific to each hand are decodable from the primary and secondary somatosensory cortex as well as from IPS subregions, the insula, and LOC. In contrast, more abstract representations of the braille letter independent from the reading hand were decodable from several visual ROIs, LOC, VWFA, and surprisingly also EVC. Using a parallel EEG design, they showed that sensory hand-specific responses emerge in time before perceptual braille letter representations. Last, they used RSA to show that the behavioral similarity of the letter pairs correlates to the neural signal of both fMRI (for the perceptual decoding, in visual and ventral ROIs) and EEG (for both sensory and perceptual decoding).</p>
<p>Strengths:</p>
<p>This is a very well-designed study and it is analyzed well. The writing clearly describes the analyses and results. Overall, the study provides convincing evidence from EEG and fMRI that the decoding of letter identity across the reading hand occurs in the visual cortex in blindness. Further, it addresses important questions about the visual cortex hierarchy in blindness (whether it parallels that of the sighted brain or is inverted) and its link to braille reading.</p>
<p>Weaknesses:</p>
<p>Although I have some comments and requests for clarification about the details of the methods, my main comment is that the manuscript could benefit from expanding its discussion. Specifically, I'd appreciate the authors drawing clearer theoretical conclusions about what this data suggests about the direction of information flow in the reorganized visual system in blindness, the role VWFA plays in blindness (revised from the original sighted role or similar to it?), how information arrives to the visual cortex, and what the authors' predictions would be if a parallel experiment would be carried out in sighted people (is this a multisensory recruitment or reorganization?). The data has the potential to speak to a lot of questions about the scope of brain plasticity, and that would interest broad audiences.</p>
<p>To aid in drawing even more concrete conclusions about the flow of information, I suggest that the authors also add at least another early visual ROI to plot more clearly whether EVC's response to braille letters arrives there through an inverted cortical hierarchy, intermediate stages from VWFA, or directly, as found in the sighted brain for spoken language.</p>
<p>Similarly, it may be informative to look specifically at the occipital electrodes' time differences between decoding for the different parameters and their correlation to behavior.</p>
<p>Regarding the methods, further detail on the ability to read with both hands equally and any residual vision of the participants would be helpful.</p>
</body>
</sub-article>
</article>