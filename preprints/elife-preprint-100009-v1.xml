<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">100009</article-id>
<article-id pub-id-type="doi">10.7554/eLife.100009</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.100009.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A scene with an invisible wall - navigational experience shapes visual scene representation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8336-594X</contrib-id>
<name>
<surname>Li</surname>
<given-names>Shi Pui Donald</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Shao</surname>
<given-names>Jiayu</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lu</surname>
<given-names>Zhengang</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>McCloskey</surname>
<given-names>Michael</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Park</surname>
<given-names>Soojin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Cognitive Science, Johns Hopkins University</institution>, Baltimore, MD, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>Laboratory of Brain and Cognition, National Institute of Mental Health</institution>, Bethesda, MD, <country>USA</country></aff>
<aff id="a3"><label>3</label><institution>Department of Psychology, New York University</institution>, New York City, NY, <country>USA</country></aff>
<aff id="a4"><label>4</label><institution>Department of Psychology, Yonsei University</institution>, Seoul, <country>Republic of Korea</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>SP</surname>
<given-names>Arun</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label><bold>Please address correspondence to:</bold> Soojin Park, Department of Psychology, Yonsei University, Seoul, Republic of Korea, Email: <email>soojin.park@yonsei.ac.kr</email></corresp>
<fn fn-type="con"><p>Author Contributions: SPDL, ZL, MM and SP designed research. SPDL and JS performed experiments. SPDL and SP analyzed data. SPDL, JS, ZL, MM &amp; SP wrote the manuscript.</p></fn>
<fn fn-type="conflict"><p>Conflicts of interest: nothing to declare</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-08-13">
<day>13</day>
<month>08</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP100009</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-07-03">
<day>03</day>
<month>07</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-07-05">
<day>05</day>
<month>07</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.07.03.601933"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Li et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Li et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-100009-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Human navigation heavily relies on visual information. Although many previous studies have investigated how navigational information is inferred from visual features of scenes, little is understood about the impact of navigational experience on visual scene representation. In this study, we examined how navigational experience influences both the behavioral and neural responses to a visual scene. During training, participants navigated in the virtual reality (VR) environments which we manipulated navigational experience while holding the visual properties of scenes constant. Half of the environments allowed free navigation (navigable), while the other half featured an ‘invisible wall’ preventing the participants to continue forward even though the scene was visually navigable (non-navigable). During testing, participants viewed scene images from the VR environment while completing either a behavioral perceptual identification task (Experiment1) or an fMRI scan (Experiment2). Behaviorally, we found that participants judged a scene pair to be significantly more visually different if their prior navigational experience varied, even after accounting for visual similarities between the scene pairs. Neurally, multi-voxel pattern of the parahippocampal place area (PPA) distinguished visual scenes based on prior navigational experience alone. These results suggest that the human visual scene cortex represents information about navigability obtained through prior experience, beyond those computable from the visual properties of the scene. Taken together, these results suggest that scene representation is modulated by prior navigational experience to help us construct a functionally meaningful visual environment.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>scene perception</kwd>
<kwd>navigation</kwd>
<kwd>virtual reality</kwd>
<kwd>fMRI</kwd>
<kwd>scene-selective cortex</kwd>
</kwd-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Imagine traveling to the Harry Potter’s world through the Platform Nine and Three Quarters. Harry Potter, the young wizard, initially regards the ordinary-looking wall at King’s Cross station as an impenetrable obstacle, then finds out that the wall is actually traversable and serve as a pathway to a train platform. How he represents the wall after passing through was never the same. This episode vividly illustrates how the human navigation initially leverages on visually available features of scenes such as the spatial layout (<xref ref-type="bibr" rid="c17">Henriksson et al., 2019</xref>; Kravitz et al., 2011; <xref rid="c11" ref-type="bibr">Park et al., 2011</xref>), height of a boundary (<xref rid="c11" ref-type="bibr">Ferrara &amp; Park, 2016</xref>), and rectilinear corners (Lescroart &amp; Gallant, 2019; <xref ref-type="bibr" rid="c3">Bonner &amp; Epstein, 2017</xref>; <xref ref-type="bibr" rid="c17">Henriksson et al., 2019</xref>), but a novel navigational experience such as discovering the trasversable wall, may alter the visual representation. Scenes possess a unique characteristic: the viewer’s presence within the space directly influences their experiences, which in turn shape how they perceive and represent the scene (Dilks et al., 2022; Cheng et al., 2021; Chaisilprungraung &amp; Park, 2021). However, there remains a significant gap in research regarding the extent to which the viewer’s immersive navigational experience influence their visual perception (Nau et al., 2018). Here, we aim to bridge this gap by creating novel navigational experiences in VR and investigating the influence of previous navigational experiences on both behavioral and neural responses to visual scene.</p>
<p>To ask whether the navigational experience, rather than the immediate visual features of a scene, can affect visual representation, one must consider a unique situation. In the real world, the navigational affordance of a scene from visually computed features is strongly confounded with the actual navigational experience of a scene. Thus, to disentangle the two, we used Unreal Engine, a virtual reality (VR) software, to create different navigational experiences while controlling the visual properties of scenes. Specifically, participants navigated in an environment with an ‘invisible wall’ blocking navigation, experiencing a non-navigable environment that contained the same visual information as in the navigable environment. We investigated whether these experiences are powerful enough to alter the perceptual representation of scenes in a simple behavioral scene identification task (Experiment 1), and the neural representation in a scene-selective visual cortex (Experiment 2).</p>
<sec id="s1a">
<title>Experiment 1: Navigational experience influences visual scene perception</title>
<p>We designed environments where the visual appearance did not accurately show if a scene was navigable or not. In half of the environments (navigable environments), participants could continue walking through the scene after performing the task. However, in the other half of the environments (non-navigable), participants could not continue forward, as if an ‘invisible wall’ blocked navigation, even though the scene was visually navigable and contained the same visual navigational information as in the navigable condition (see <xref rid="fig1" ref-type="fig">Figure 1</xref>). The visual environments were the same in both conditions, but the navigational affordances were either present or absent, requiring participants to rely on their experience to determine if they could move through a scene.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Virtual environments used in training.</title>
<p><bold>A</bold>: Screenshots of six different outdoor environments that used in training. These are also images used in both the behavioral and fMRI testing. <bold>B</bold>: Structure of the virtual maze used in the training phase. First, participants followed the path to the outdoor scene, and performed an object detection task at a designated location. In the navigable condition, participants were able to navigate forward after the task to reach the destination. In the non-navigable condition, an invisible wall was placed in front of the designated location. When the participant tried to navigate forward, they would not be able to as there was an invisible wall. They would have to turn back and use another path to reach the destination.</p></caption>
<graphic xlink:href="601933v1_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>After the participants explored the virtual environments, they were presented with pairs of previously shown scenes and asked to judge whether the pair was perceptually ‘same or different’ by pressing buttons (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). The participants would only respond “same” if the two images were identical; otherwise, they would respond with “different”. This simple same-different judgement task probed the representational differences between scene pairs contingent on previous navigational experience (<xref ref-type="bibr" rid="c1">Bamber, 1969</xref>; <xref ref-type="bibr" rid="c7">Defever et al., 2012</xref>). In the “different” trials, the pair could originate from the same navigational experience condition during the VR training (e.g., both images drawn from navigable environments or both from non-navigable environments), which we refer to as matched navigability condition. Alternatively, the pair could consist of one image drawn from the navigable environment and the other from the non-navigable environment, which we refer to as mis-matched navigability condition. The critical question was whether the response time to judge ‘different’ differed between the matched and mismatched navigability conditions. We hypothesized that if the navigability of scenes learned through prior experience in the VR Training influences the representational similarity of scenes, the ‘different’ response for a scene pair that has matched navigability will take longer (similar navigational experience), compared to the ‘different’ response for a scene pair that has mismatched navigability (dissimilar navigational experience) (see <xref rid="fig2" ref-type="fig">Figure 2A</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Behavioral testing showed navigational experience affect visual scene representation.</title>
<p><bold>A</bold>: The top panel shows the visual same/different task paradigm used in the testing. The second and third panel demonstrates an example of condition assignment. In the analysis, only trials with different response were divided into matched navigability and mismatched navigability condition. When a participants respond ‘different’ to a scene pair, a slower response time is expected in the matched condition compared to the mismatched condition if the navigational experience affect visual scene representation. <bold>B</bold>: Matched navigability condition (Mean=685ms, 95% CI: [637, 732]) showed a slower response time compared to the mismatched condition (Mean=712ms, 95% CI: [662, 762]) in the experimental group but not in the control group. These results suggest navigational experience affect visual scene representation even in visual task that does not involve navigation. ** indicates p&lt;0.01.</p></caption>
<graphic xlink:href="601933v1_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We observed a robust impact of navigational experience on perceptual judgements of scenes. A paired t-test on the log-transformed RT showed a significant difference between the experimental conditions (matched-navigability or mismatched-navigability within the ‘different’ response) (t(24)=3.05, p=0.006, Cohen’s d =0.23). To further confirm that the main effect was due to the prior navigation experience, we recruited additional participants for a yoked control group where participants had no prior VR Training experience. In the control group, we did not find any significant difference in RT between the experimental conditions (t(24)=1.2, p=0.23) (see <xref rid="fig2" ref-type="fig">Figure 2B</xref>). We further conducted a linear mixed effects modelling to predict log-transformed RT and found an interaction between groups (i.e., experimental group vs. control group) and the experimental conditions (<italic>F</italic>(1,5484) = 3.72, <italic>P</italic> = 0.05) (see Methods for details).</p>
<p>To understand how the RT is affected by navigational experience and visual similarity between scenes, a separate linear-mixed effects model was used to analyze the log-transformed RT of the experimental group, with experimental conditions, visual similarity, and the interaction of the two factors as fixed factors (see Methods for details). Most critically, we observed a significant main effect of experimental conditions, with significantly slower RT on the matched-navigability trials than on the mismatched-navigability trials (<italic>F</italic>(1,2762) = 4.38, <italic>P</italic> = <italic>0</italic>.<italic>0</italic>37). We also observed a significant main effect of the visual similarity, <italic>F</italic>(1,2762) = 355, <italic>P</italic> &lt;<italic>0</italic>.<italic>00</italic>1), suggesting visual features play a role in the visual same-different task. Previous study (<xref ref-type="bibr" rid="c22">Negen et al., 2020</xref>) found that when no physical boundary is present, the visual system may use navigational experience to further estimate the functional constraints of the scene. However, when a physical boundary is visible, this visual cue may provide strong enough information about the environment’s functional constraint, making it difficult to alter with experience. Similarly, the interaction between experimental conditions and visual similarity in our experiment (<italic>F</italic>(1,2762) = 8.84, <italic>P</italic> = 0.003) suggests the effect of navigational experience was greater when visual similarity of scenes was higher, implying that the navigational experience may play a more prominent role when visual cues are relatively weak. This result further confirms that the RT difference observed in the experimental group depends on prior navigational experience, and such effect is robust across analyses.</p>
<p>This evidence hints that top-down information, such as those gained through navigational experience, may alter visual scene representation in a task without any navigational component. Next, we sought to examine whether the prior navigational experience is powerful enough to alter the neural scene representation in areas that process visual cues in scenes.</p>
</sec>
<sec id="s1b">
<title>Experiment 2: Navigational experience influences the neural representation of visual scenes in the scene-selective visual cortex</title>
<p>In this experiment, we performed fMRI scanning while participants were viewing scenes that were presented during the VR Training. If prior navigational experience altered the neural representation of scenes, then we would expect a high fMRI multi-voxel pattern decoding accuracy on the navigability of scenes, despite being visually equated. We found a significantly above chance classification in the PPA (t(19) = 2.2, p=0.02 (one-tailed), Cohen’s d =0.49), suggesting that top-down navigational experience information could be decoded in this scene-selective area. This result is consistent with the specialized role of the PPA in scene identification and suggests that navigational experience may be an important attribute in identifying and classifying scenes. Previous studies found that high-level visual areas can represent scene familiarity (<xref ref-type="bibr" rid="c9">Epstein et al., 2007</xref>), the viewer’s expertise (<xref ref-type="bibr" rid="c12">Gauthier et al., 1999</xref>; <xref ref-type="bibr" rid="c14">Grill-Spector et al., 2004</xref>), and learned knowledge and associations (<xref ref-type="bibr" rid="c19">Kastner &amp; Ungerleider, 2001</xref>; <xref ref-type="bibr" rid="c23">Summerfield &amp; Egner, 2009</xref>) that are not solely based on physical features of the image. We propose that PPA utilizes navigational experience to carve up and differentiate otherwise similar scene environments. The experience of navigation provides a meaningful additional dimension that allows for classification and identification of scenes that are otherwise confusable. This process aligns with how perceptual learning theories suggest differentiation is achieved after experience, for example, learning to perceptually separate two stimuli that once appeared fused together (<xref ref-type="bibr" rid="c2">Biederman &amp; Shiffrar, 1987</xref>; <xref ref-type="bibr" rid="c13">Goldstone, 1998</xref>). Prior experience with scene environments may be a critical key to flexibly stretch and compress the representational space of how different instances of scenes are represented, thus facilitate scene categorization in the PPA not only by separating visually similar scenes into different categories, but also unitizing different visual scenes into the same class. In contrast, no other scene-selective areas showed significant classification accuracies (OPA: t(19)=-0.54, p=0.70; RSC: t(19)=-0.11, p=0.50), suggesting that such top-down effect was unique to the PPA (see <xref rid="fig3" ref-type="fig">Figure 3B</xref>). We also found that navigational experience information was not decodable from EVC (t(19)=-0.16, p=0.56). This result confirms that our visual stimuli were appropriately controlled and counterbalanced for low-level visual information, indicating that navigational experience information is encoded at a higher-level of processing.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Navigational experience information can be decoded from PPA.</title>
<p><bold>A</bold>: Participants performed a same/different color dot task while viewing scene images in the fMRI scanner. A SVM classifier is trained to classify whether the neural pattern of visually controlled scenes are distinguishable based on navigable experience or not. A leave-one-trial-out cross validation approach is used to test the performance of the classifier. <bold>B</bold>: Scene-selective region PPA shows an above chance classification accuracy suggesting its representation contains navigational experience information. * indicates p&lt;0.05.</p></caption>
<graphic xlink:href="601933v1_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Our finding that navigational experience information is decoded even in a task that is not directly related to navigation has important theoretical implications. It suggests that the neural representation of scenes in the brain is not only based on bottom-up visual information, but also on top-down information such as prior experience in this case. This finding is consistent with the idea that the brain actively constructs perceptions rather than simply passively receiving and processing sensory inputs. These results altogether highlight that the representation of scenes in the PPA does not solely rely on visually computable bottom-up visual information such as layout, openness, etc., but also incorporates the viewer’s prior navigational experience and learned knowledge of the visual scene.</p>
</sec>
</sec>
<sec id="s2">
<title>Conclusion</title>
<p>Our study underscores the significant impact of prior navigational experience on visual scene representation. By manipulating navigational experiences in VR environments that disentangle visual features and navigability of scenes, we showed that prior experiences are powerful enough to override visually computed scene information. We also demonstrated navigational experience can significantly influence the neural representations in PPA, vital for identifying scenes. In essence, our findings highlight the flexible nature of scene representations in the brain, continually shaped by our experiences and knowledge.</p>
</sec>
</body>
<back>
<sec id="s3">
<title>Materials and methods</title>
<sec id="s3a">
<title>Experiment 1</title>
<sec id="s3a1">
<title>Participants</title>
<p>Twenty-five experimental group participants (18 females; ages: 18-29 years; 2 left-handed) and twenty-five separate group of control group participants (12 females; ages: 18-25 years; 2 left-handed) were recruited from the Johns Hopkins University community for financial compensation. The sample size was determined apriori based on other studies that used same-different tasks, and we estimated the effect size to be a medium one (i.e., Cohen’s <italic>d</italic> = 0.55). Together with an alpha level of 0.05 and the standard desired level of power at 0.8, a power analysis revealed that a minimum of 22 participants were needed to detect the effect that we set out to test in Experiment 1. We ended up recruiting 25 participants in anticipation of participant attrition. All participants had normal or corrected-to-normal vision. This study was approved by the Institutional Review Board of the Johns Hopkins University School of Medicine, and written informed consent was obtained for every participant.</p>
</sec>
<sec id="s3a2">
<title>Visual Stimuli</title>
<p>Eight virtual environments (Six for the main experiment; two for the practice session) were created using game developing software Unreal Engine 4.16 (Epic Games, Inc, Cary, NC) and were presented using a 27 inch LED monitor (1920 × 1080 pixels, 60-Hz frame rate). Each virtual environment contained a simple maze (see <xref rid="fig1" ref-type="fig">Figure 1B</xref>) in which participants navigated. All eight environments consisted of identical architecture: each contained the same underground tunnel and different outdoor environment, which differed in terms of landscape, plantation and ground materials. For the Testing Phase, a single view of an image was taken from the object detection task point in the virtual environment, which resulted in six different scene images (1.8 meters height at the object detection task location of the scene with 90° field of view, 2084×1182 pixels). In the Testing Phase, two scene images were presented side by side. All test images were presented using Psychtoolbox, Matlab R2017A (<xref ref-type="bibr" rid="c20">Kleiner, 2010</xref>).</p>
</sec>
<sec id="s3a3">
<title>Experimental Design and Procedure</title>
<p>For each participant, three virtual environments were randomly assigned as the Navigable condition, and the other three were assigned as the Non-Navigable condition. Participants were not informed about the navigability of each environment. The experiment consisted of two phases: a training phase in which participants navigated in the virtual experiment, and a testing phase in which participants performed a same-different judgment on scenes. Before the training phase, there were three practice trials to familiarize participants with movement control in a virtual environment.</p>
</sec>
<sec id="s3a4">
<title>Training Phase</title>
<p>The training phase contained 2 blocks, each with 12 trials in a pseudo-randomized order with a 5s ITI. Each environment was presented for four times during the training phase. In each training phase trial, participants navigated the virtual environment, starting from a point at an underground tunnel, then navigating through the outdoor environment to reach to a destination, which was a point at an underground tunnel just behind the original starting point, separated by a glass wall (see <xref rid="fig1" ref-type="fig">Figure 1B</xref>). Participants were instructed to move forward by pressing the forward button until they reached the end of the tunnel, then they made a left turn by controlling with a mouse to open a left automatic door to the outdoor environment. Participants continued navigating in the outdoor environment along the path until they reached a task point (indicated by a floor mat), where they would stop navigating and perform an object detection task displayed on a scene view. In the object detection task, a monkey head statue appeared in the view and participants were asked to press a button as soon as they detected it (see <xref rid="fig1" ref-type="fig">Figure 1B</xref>). The object appeared at a random location for four times while participants were standing at the task point, which lasted for at least 10s. The goal of the object detection task was to ensure that the participant spent time to fully perceive the particular view of an environment, which, unbeknownst to participants, served as stimuli in the testing phase. Participants kept moving forward after the object detection task.</p>
<p>A critical manipulation in the training phase was whether the scene that participants moved forward to after the object detection task was truly navigable or not. In the Navigable condition, participants could proceed forward and follow the path in a scene to enter the underground tunnel again to reach the final destination. On the contrary, in the Non-Navigable condition, participants were not able to proceed forward and navigate along the path, because an invisible wall was placed in the location of the task mat. Critically, the invisible wall was ‘<italic>invisible</italic>’, so the scene itself was visually navigable and visually equal to those presented in the Navigable condition, but functionally inhibited the participant’s navigation. In the Non-Navigable condition, instead of proceeding forward to enter the underground tunnel, participants made a turnaround to return back to the tunnel, use the right automatic door to access another tunnel to reach the final destination. (Note: the right automatic door can only be opened after the participant finished the object detection task and return back to the tunnel in the Non-Navigable condition)</p>
</sec>
<sec id="s3a5">
<title>Testing Phase</title>
<p>In the testing phase, participants were presented with a pair of scenes and asked to judge, as quickly and accurately as possible, whether the pair of images were visually the same or different by pressing one of the two buttons. There were three blocks in the testing phase, each block with 80 trials, totalling 240 randomized trials. In half of the trials, identical scenes were presented side by side, eliciting a “same” response. Six identical scene pairs were repeated 20 times. In the other half of the trials, two different scenes were presented side by side, eliciting a “different” response. Fifteen different scene pairs were repeated eight times. Within each trial, 1s blank gray screen with fixation cross appeared, then a pair of scene images were presented on a gray background that remained on the screen until response or 1.5s had elapsed.</p>
<p>Performance in the “different” response trials was the critical dependent variable of the Test Phase. The “different” response trials were divided into two conditions: 48 trials (six pairs repeated eight times) were the matched-navigability condition, in which both images were drawn from environments that participants experienced the matched navigability (i.e., both navigable or both non-navigable); 72 trials (nine pairs repeated eight times) were the different-navigability condition, in which one image was drawn from the navigable environment while the other was drawn from the non-navigable environment, having a different navigability between a scene pair. It is important to note that, regardless of whether the two scenes had the matched or mismatched navigability, participants answered ‘different’ to scenes if they were visually different from each other.</p>
<p>We hypothesized that the RT to respond “different” in the different response trials will change depending on the representational similarity of the two scene pairs shaped through the navigational experience in the training phase. Simply, if the two scenes are represented similarly in the brain, it will take a longer time to respond ‘different’ to a visually different, but representationally similar scenes, than responding ‘different’ to two visually different scenes that are represented less similar in the brain (<xref ref-type="bibr" rid="c1">Bamber, 1969</xref>; <xref ref-type="bibr" rid="c7">Defever et al., 2012</xref>). Specifically, we predicted that, if the navigability of scenes learned through prior experience in the Training Phase is represented, the ‘different’ response for a scene pair that has the matched navigability will take longer, compared to the ‘different’ response for a scene pair that has mismatched navigability (see <xref rid="fig2" ref-type="fig">Figure 2A</xref>).</p>
</sec>
<sec id="s3a6">
<title>Control group</title>
<p>To control for the intrinsic visual similarity between scene pairs, a separate group of control participants were tested in a yoked design, matching one-on-one with the experimental group for the experimental condition assignment. There was no Training Phase for the control group, and the participants were only tested in the Testing Phase, performing the same/different judgement on scenes. All the procedures, stimuli, and condition assignments of stimuli were yoked to the experimental group, except for the lack of Training Phase. Since the participants did not experience the scenes under any navigational context, any RT differences across conditions in the same/different task will reflect intrinsic visual similarity of scenes.</p>
</sec>
<sec id="s3a7">
<title>Analysis</title>
<p>In the preprocessing, only the correct responses for the “different” trials in the same/different judgement task were included to probe how the similarity in navigational experience changes the RTs for two different, visually similar scenes. Trials with RTs faster or slower than 3 Standard Deviations from the mean RT in each participant were removed.</p>
<p>In the first set of analysis, we focus on answering the question on whether there is a navigational experience effect in the same/different judgement task. In the group analysis, paired t-tests were conducted on the RT between the matched-navigability and the mismatched-navigability conditions for both the experimental and the control group. To further investigate the difference of experimental effect between the experimental and the control group, a linear-mixed effects model was used to analyze the log-transformed RTs (in ms). In the model, experimental conditions (matched-navigability or mismatched-navigability), groups (experimental or control) and the interaction between the experimental condition and group were included as fixed factor. Block number and trial number were also added as fixed factor to remove any linear trend (i.e., practice effect) of RT across time. Subjects and items were included as random intercepts and random slopes of experimental conditions, block number and trial number by subjects were also included in the model.</p>
<p>In the second set of analysis, we investigate the relationship between visual similarity and navigational experience using linear-mixed effect model. Linear-mixed effect model was constructed to model the experimental group’s RT for the different trials. Control group’s average RTs for each pair of different scenes were calculated and used as a proxy for the visual similarity of pairs of scenes. Similar to the first model, experimental conditions (matched-navigability or mismatched-navigability), visual similarity (i.e. the mean RT of an image pair measured from the control group), the interaction between the experimental condition and visual similarity, block number and trial number were included as fixed factors, with random intercepts by subjects and items. Random slopes of experimental conditions, block number and trial number by subjects are also included in the model. In all the linear-mixed effect models, p values were computed using ANOVA marginal test of the linear-mixed effects models.</p>
<p>The accuracy of the same/different task is 93% (SD=4%) and 88% (SD=13%) respectively for the experimental and control group. Within both the experimental and the control group, there is no significant difference in accuracy between the matched navigability condition and the mismatched navigability condition (experimental group: t(24)=0.6, p=0.55; control group: t(24)=0.37, p=0.71). Therefore, there was not a speed-accuracy trade-off in the same/different task.</p>
</sec>
</sec>
<sec id="s3b">
<title>Experiment 2</title>
<sec id="s3b1">
<title>Participants</title>
<p>Twenty participants (16 females; ages: 19-31 years; 1 left-handed) were recruited from the Johns Hopkins University community for financial compensation. The sample size was determined apriori based on similar fMRI studies that used a region-of-interest approach in scene-selective regions. The power was determined with Cohen’s <italic>d</italic> of 0.6 based on medium effect size, an alpha level of 0.05 and the standard desired level of power at 0.8, which revealed that a minimum of 19 participants, thus we scanned 20 participants which is more than the anticipated sample size. All participants had normal or corrected-to-normal vision. This study was approved by the Institutional Review Board of the Johns Hopkins University School of Medicine, and written informed consent was obtained for every participant.</p>
</sec>
<sec id="s3b2">
<title>Stimuli</title>
<p>All visual stimuli, including virtual environments used in the training phase and single views of images used in the testing phase, were identical to those used in Experiment 1.</p>
</sec>
<sec id="s3b3">
<title>Procedure</title>
<p>The experiment was conducted in two phases. First, outside the scanner, participants participated in the behavioral training identical to the training phase of Experiment 1. Between subjects counterbalancing was used to assign the navigability of the environments, so that visual similarities between scenes were fully counterbalanced. After the training phase, participants participated in the fMRI testing phase, which includes four fMRI experimental runs, two functional localizer runs and one T1 anatomical run. Each experimental run included 8 trials of each image captured from six virtual environments, with a total of 48 trials. Stimulus sequences were presented with a 6-12s ITI in a pseudo-randomized order using optimal sequencing (optseq; (<xref ref-type="bibr" rid="c6">Dale et al., 1999</xref>), which allowed the deconvolution of fMRI signal time locked to the rapidly presented stimuli. Each stimulus trial lasted for 2s: a scene image was presented for 2s with two color-dots overlaid on the image after the first second. Participants were asked to perform a color-dot same/different task and press a button as rapidly as possible to indicate whether the two color-dots had the same or different colors (see <xref rid="fig3" ref-type="fig">Figure 3A</xref>). The average accuracy across subjects is 87% (SD = 7.5%) and every subject performance is within +/-2.5 SD.</p>
<p>In the functional localizer run, participants performed a one-back repetition task while they viewed 16s blocks of images of places, single objects without backgrounds or grid-scrambled objects. Twenty images were presented in each block, with 600ms image presentation time with a 200ms ITI. Each block was followed by 16s fixation-only resting period.</p>
</sec>
<sec id="s3b4">
<title>MRI acquisition and preprocessing</title>
<p>fMRI data were acquired with a 3-T Philips fMRI scanner with a 32-channel Sense head coil at the F.M. Kirby Research Center for Functional Neuroimaging at Johns Hopkins University. Stimuli images were rear-projected in 1000 × 530-pixel resolution (∼7.5<sup>°</sup> × 4<sup>°</sup> visual angle) onto a screen positioned at the rear of the scanner using an Epson PowerLite 7350 projector (type: XGA, resolution: 1024 × 768; brightness: 1600 ANSI lumens). A T1-weighted MPRAGE scan were acquired for each fMRI session (voxel size = 1 × 1 × 1 mm). T2*-weighted functional images sensitive to BOLD contrasts were acquired using a gradient echo planar imaging (EPI) sequence (TR = 2000ms; TE = 30 ms; 2.5 × 2.5 × 2.5 mm voxels; flip angle = 70°; 36 axial 2.5-mm sliced (0.5-mm gap); acquired parallel to the anterior commissure-posterior commissure line).</p>
<p>All functional data were preprocessed using fMRIPrep (<xref ref-type="bibr" rid="c10">Esteban et al., 2019</xref>). Preprocessing included slice time correction, 3D motion correction, linear trend removal and normalization to MNI space. Spatial smoothing was performed using Gaussian kernel with 4 mm FWHM, and the data were analyzed in MNI space. In the experimental runs, to obtain a representative trial-by trial estimates (betas) with higher signal-to-noise ratios in the event-related experiment, we obtained each trial’s estimate through a general linear model including a regressor for that trial as well as another regressor for all other trials using Analysis of Functional NeuroImages (<xref ref-type="bibr" rid="c5">Cox, 1996</xref>) function 3dLSS (<xref ref-type="bibr" rid="c21">Mumford et al., 2012</xref>).</p>
</sec>
<sec id="s3b5">
<title>Region of interest (ROI) identification</title>
<p>Function region of interest is defined independently before the multi-voxel pattern analysis using independent localizer runs. Preprocessed localizer run fMRI data were fitted in a general linear model to localize regions of interests using AFNI (<xref ref-type="bibr" rid="c5">Cox, 1996</xref>). All scene-selective regions, which include occipital place area (OPA), parahippocampal place area (PPA) and retrosplenial cortex (RSC) were identified in each subject using a contrast of scenes &gt; objects and an anatomical constraint. Object-selective area, lateral occipital complex (LOC) was identified in each subject using a contrast of objects &gt; scrambled objects. Early visual cortex (EVC) was also identified in each individual subject using a contrast of scrambled objects &gt; objects. A group-based anatomical map of ROIs derived from a large number of subjects in MNI space was used as anatomical constraints (<xref ref-type="bibr" rid="c18">Julian et al., 2012</xref>). Bilateral ROIs were identified within the anatomical constraints in each hemisphere using the top 100 voxels from the localizer contrast.</p>
</sec>
<sec id="s3b6">
<title>Multi-voxel pattern analysis (MVPA)</title>
<p>To determine whether multi-voxel patterns within each ROI encoded information about the navigability of scenes, we implemented a linear support vector machine (linear Nu-SVM) classification technique in which multi-voxel patterns were compared across each trial. Using PyMVPA, trial’s estimates were extracted for each trial at each voxel in a given ROI, and SVM classification was performed on these estimates. We asked whether the navigability of scenes, which depends on the navigational experience during the training phase, could be classified from multi-voxel patterns for the scenes during the fMRI testing phase. We used a leave-1-trial out cross-validation approach. Classification accuracy was averaged across trials for a given ROI and tested against random chance (i.e., 0.5) using a one-tailed t-test.</p>
</sec>
</sec>
</sec>
<ack>
<title>Acknowledgments</title>
<p>This work is supported by a National Eye Institute Grant (R01EY026042) to M. M and National Research Foundation of Korea (NRF-2023R1A2C1006673) to S. P. We thank the F.M. Kirby Research Center for Functional Brain Imaging in the Kennedy Krieger Institute and the Maryland Advanced Research Computing Center (MARCC).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Bamber</surname>, <given-names>D.</given-names></string-name> (<year>1969</year>). <article-title>Reaction times and error rates for “same”-”different” judgments of multidimensional stimull</article-title>. <source>Perception &amp; Psychophysics</source>, <volume>6</volume>(<issue>3</issue>), <fpage>169</fpage>–<lpage>174</lpage>. <pub-id pub-id-type="doi">10.3758/BF03210087</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Biederman</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Shiffrar</surname>, <given-names>M. M.</given-names></string-name> (<year>1987</year>). <article-title>Sexing day-old chicks: A case study and expert systems analysis of a difficult perceptual-learning task</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>, <volume>13</volume>, <fpage>640</fpage>–<lpage>645</lpage>. <pub-id pub-id-type="doi">10.1037/0278-7393.13.4.640</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Bonner</surname>, <given-names>M. F.</given-names></string-name>, &amp; <string-name><surname>Epstein</surname>, <given-names>R. A.</given-names></string-name> (<year>2017</year>). <article-title>Coding of navigational affordances in the human visual system</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>114</volume>(<issue>18</issue>), <fpage>4793</fpage>–<lpage>4798</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1618228114</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Brady</surname>, <given-names>T. F.</given-names></string-name>, <string-name><surname>Shafer-Skelton</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Alvarez</surname>, <given-names>G. A.</given-names></string-name> (<year>2017</year>). <article-title>Global ensemble texture representations are critical to rapid scene perception</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>43</volume>(<issue>6</issue>), <fpage>1160</fpage>–<lpage>1176</lpage>. <pub-id pub-id-type="doi">10.1037/xhp0000399</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Cox</surname>, <given-names>R. W.</given-names></string-name> (<year>1996</year>). <article-title>AFNI: Software for Analysis and Visualization of Functional Magnetic Resonance Neuroimages</article-title>. <source>Computers and Biomedical Research</source>, <volume>3</volume>(<issue>29</issue>), <fpage>162</fpage>–<lpage>173</lpage>. <pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Dale</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Greve</surname>, <given-names>D. N.</given-names></string-name>, &amp; <string-name><surname>Burock</surname>, <given-names>M. A.</given-names></string-name> (<year>1999</year>). <article-title>Optimal stimulus sequences for event-related fMRI</article-title>. <source>NeuroImage</source>, <volume>9</volume>, <fpage>S33</fpage>–<lpage>S33</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Defever</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Sasanguie</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Vandewaetere</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Reynvoet</surname>, <given-names>B.</given-names></string-name> (<year>2012</year>). <article-title>What can the same– different task tell us about the development of magnitude representations?</article-title> <source>Acta Psychologica</source>, <volume>140</volume>(<issue>1</issue>), <fpage>35</fpage>–<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1016/j.actpsy.2012.02.005</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Epstein</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name> (<year>2019</year>). <article-title>Scene Perception in the Human Brain</article-title>. <source>Annual Review of Vision Science</source>, <volume>5</volume>(<issue>1</issue>), <fpage>373</fpage>–<lpage>397</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-vision-091718-014809</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Epstein</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Higgins</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Jablonski</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Feiler</surname>, <given-names>A. M.</given-names></string-name> (<year>2007</year>). <article-title>Visual Scene Processing in Familiar and Unfamiliar Environments</article-title>. <source>Journal of Neurophysiology</source>, <volume>97</volume>(<issue>5</issue>), <fpage>3670</fpage>–<lpage>3683</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00003.2007</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Esteban</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Markiewicz</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Blair</surname>, <given-names>R. W.</given-names></string-name>, <string-name><surname>Moodie</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Isik</surname>, <given-names>A. I.</given-names></string-name>, <string-name><surname>Erramuzpe</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kent</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Goncalves</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>DuPre</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Oya</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ghosh</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Wright</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Durnez</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Poldrack</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name><surname>Gorgolewski</surname>, <given-names>K. J.</given-names></string-name> (<year>2019</year>). <article-title>fMRIPrep: A robust preprocessing pipeline for functional MRI</article-title>. <source>Nature Methods</source>, <volume>16</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Ferrara</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Park</surname>, <given-names>S.</given-names></string-name> (<year>2016</year>). <article-title>Neural representation of scene boundaries</article-title>. <source>Neuropsychologia</source>, <volume>89</volume>, <fpage>180</fpage>–<lpage>190</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2016.05.012</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Gauthier</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>A. W.</given-names></string-name>, <string-name><surname>Skudlarski</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Gore</surname>, <given-names>J. C.</given-names></string-name> (<year>1999</year>). <article-title>Activation of the middle fusiform ‘face area’ increases with expertise in recognizing novel objects</article-title>. <source>Nature Neuroscience</source>, <volume>2</volume>(<issue>6</issue>). <pub-id pub-id-type="doi">10.1038/9224</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Goldstone</surname>, <given-names>R. L.</given-names></string-name> (<year>1998</year>). <article-title>Perceptual Learning</article-title>. <source>Annual Review of Psychology</source>, <volume>49</volume>(<issue>1</issue>), <fpage>585</fpage>–<lpage>612</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.psych.49.1.585</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Knouf</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name> (<year>2004</year>). <article-title>The fusiform face area subserves face perception, not generic within-category identification</article-title>. <source>Nature Neuroscience</source>, <volume>7</volume>(<issue>5</issue>). <pub-id pub-id-type="doi">10.1038/nn1224</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Groen</surname>, <given-names>I. I. A.</given-names></string-name>, <string-name><surname>Ghebreab</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Prins</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Lamme</surname>, <given-names>V. A. F.</given-names></string-name>, &amp; <string-name><surname>Scholte</surname>, <given-names>H. S.</given-names></string-name> (<year>2013</year>). <article-title>From Image Statistics to Scene Gist: Evoked Neural Activity Reveals Transition from Low-Level Natural Image Structure to Scene Category</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume>(<issue>48</issue>), <fpage>18814</fpage>–<lpage>18824</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3128-13.2013</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Groen</surname>, <given-names>I. I. A.</given-names></string-name>, <string-name><surname>Silson</surname>, <given-names>E. H.</given-names></string-name>, &amp; <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name> (<year>2017</year>). <article-title>Contributions of low- and high-level properties to neural processing of visual scenes in the human brain</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>372</volume>(<issue>1714</issue>), <fpage>20160102</fpage>. <pub-id pub-id-type="doi">10.1098/rstb.2016.0102</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Henriksson</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> (<year>2019</year>). <article-title>Rapid Invariant Encoding of Scene Layout in Human OPA</article-title>. <source>Neuron</source>, <volume>103</volume>(<issue>1</issue>), <fpage>161</fpage>-<lpage>171.e3.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2019.04.014</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Julian</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Fedorenko</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Webster</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name> (<year>2012</year>). <article-title>An algorithmic method for functionally defining regions of interest in the ventral visual pathway</article-title>. <source>NeuroImage</source>, <volume>60</volume>(<issue>4</issue>), <fpage>2357</fpage>–<lpage>2364</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.055</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Kastner</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Ungerleider</surname>, <given-names>L. G.</given-names></string-name> (<year>2001</year>). <article-title>The neural basis of biased competition in human visual cortex</article-title>. <source>Neuropsychologia</source>, <volume>39</volume>(<issue>12</issue>), <fpage>1263</fpage>–<lpage>1276</lpage>. <pub-id pub-id-type="doi">10.1016/S0028-3932(01)00116-6</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Kleiner</surname>, <given-names>M.</given-names></string-name> (<year>2010</year>). <article-title>ECVP 2010 Abstracts: Visual stimulus timing precision in Psychtoolbox-3: Tests, pitfalls and solutions</article-title>. <source>Personality Science</source>, <volume>39</volume>(<issue>1</issue>), <fpage>189</fpage>. <pub-id pub-id-type="doi">10.1177/03010066100390S101</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Mumford</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>B. O.</given-names></string-name>, <string-name><surname>Ashby</surname>, <given-names>F. G.</given-names></string-name>, &amp; <string-name><surname>Poldrack</surname>, <given-names>R. A.</given-names></string-name> (<year>2012</year>). <article-title>Deconvolving BOLD activation in event-related designs for multivoxel pattern classification analyses</article-title>. <source>NeuroImage</source>, <volume>59</volume>(<issue>3</issue>), <fpage>2636</fpage>–<lpage>2643</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.08.076</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Negen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sandri</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>S. A.</given-names></string-name>, &amp; <string-name><surname>Nardini</surname>, <given-names>M.</given-names></string-name> (<year>2020</year>). <article-title>Boundaries in spatial cognition: Looking like a boundary is more important than being a boundary</article-title>. <source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source>, <volume>46</volume>(<issue>6</issue>), <fpage>1007</fpage>–<lpage>1021</lpage>. <pub-id pub-id-type="doi">10.1037/xlm0000760</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Egner</surname>, <given-names>T.</given-names></string-name> (<year>2009</year>). <article-title>Expectation (and attention) in visual cognition</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>13</volume>(<issue>9</issue>), <fpage>403</fpage>–<lpage>409</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2009.06.003</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100009.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>SP</surname>
<given-names>Arun</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study provides novel evidence that navigational experiences can shape perceptual scene representations. The evidence presented is <bold>incomplete</bold> and would benefit from clearer explanations of the experiment design and careful discussion of alternative interpretations such as contextual associations or familiarity. The work will be of interest to cognitive psychologists and neuroscientists working on perception and navigation.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100009.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this study, Li et al. aim to determine the effect of navigational experience on visual representations of scenes. Participants first learn to navigate within simple virtual environments where navigation is either unrestricted or restricted by an invisible wall. Environments are matched in terms of their spatial layout and instead differ primarily in terms of their background visual features. In a later same/different task, participants are slower to distinguish between pairs of scenes taken from the same navigation condition (i.e. both restricted or both unrestricted) than different navigation conditions. Neural response patterns in the PPA also discriminate between scenes from different navigation conditions. These results suggest that navigational experience influences perceptual representations of scenes. This is an interesting study, and the results and conclusions are clearly explained and easy to follow. There are a few points that I think would benefit from further consideration or elaboration from the authors, which I detail below.</p>
<p>First, I am a little sceptical of the extent to which the tasks are able to measure navigational or perceptual experience with the scenes. The training procedure seems like it wouldn't require obtaining substantial navigational experience as the environments are all relatively simple and only require participants to follow basic paths, rather than encouraging more active exploration of a more complex environment. Furthermore, in the same/different task, all images show the same view of the environment (meaning they show the exact same image in the &quot;same environment&quot; condition). The task is therefore really a simple image-matching task and doesn't require participants to meaningfully extract the perceptual or spatial features of the scenes. An alternative would have been to present different views of the scenes, which would have prevented the use of image-matching and encouraged further engagement with the scenes themselves. Ultimately, the authors do still find a response time difference between the navigation conditions, but the effect does appear quite small. I wonder if the design choices could be obscuring larger effects, which might have been better evident if the navigational and perceptual tasks had encouraged greater encoding of the spatial and perceptual features of the environment. I think it would be helpful for the authors to explain their reasons for not employing such designs, or to at least give some consideration to alternative designs.</p>
<p>Figure 1B illustrates that the non-navigable condition includes a more complicated environment than the navigable condition, and requires following a longer path with more turns in it. I guess this is a necessary consequence of the experiment design, as the non-navigable condition requires participants to turn around and find an alternative route. Still, this does introduce spatial and perceptual differences between the two navigation conditions, which could be a confounding factor. What do the response times for the &quot;matched&quot; condition in the same/different task look like if they are broken down by the navigable and non-navigable environments? If there is a substantial difference between them, it could be that this is driving the difference between the matched and mismatched conditions, rather than the matching/mismatching experience itself.</p>
<p>In both experiments, the authors determined their sample sizes via a priori power analyses. This is good, but a bit more detail on these analyses would be helpful. How were the effect sizes estimated? The authors say it was based on other studies with similar methodologies - does this mean the effect sizes were obtained from a literature search? If so, it would be good to give some details of the studies included in this search, and how the effect size was obtained from these (e.g., it is generally recommended to take a lower bound over studies). Or is the effect size based on standard guidelines (e.g., Cohen's d ≈ 0.5 is a medium effect size)? If so, why are the effect sizes different for the two studies?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100009.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Li and colleagues applied virtual reality (VR) based training to create different navigational experiences for a set of visually similar scenes. They found that participants were better at visually discriminating scenes with different navigational experiences compared to scenes with similar navigational experiences. Moreover, this experience-based effect was also reflected in the fMRI data, with the PPA showing higher discriminability for scenes with different navigational experiences. Together, their results suggest that previous navigational experiences shape visual scene representation.</p>
<p>Strengths:</p>
<p>(1) The work has theoretical value as it provides novel evidence to the ongoing debate between visual and non-visual contributions to scene representation. While the idea that visual scene representation can encode navigational affordances is not new (e.g., Bonner &amp; Epstein, 2017, PNAS), this study is one of the first to demonstrate that navigational experiences can causally shape visual scene representation. Thus, it serves as a strong test for the hypothesis that our visual scene representations involve encoding top-down navigational information.</p>
<p>(2) The training paradigm with VR is novel and has the potential to be used by the broader community to explore the impact of experience on other categorical visual representations.</p>
<p>(3) The converging evidence from behavioral and fMRI experiments consolidates the work's conclusion.</p>
<p>Weaknesses:</p>
<p>(1) While this work attempts to demonstrate the effect of navigational experience on visual scene representation, it's not immediately clear to what extent such an effect necessarily reflects altered visual representations. Given that scenes in the navigable condition were more explored and had distinct contextual associations than scenes in the non-navigable condition (where participants simply turned around), could the shorter response time for a scene pair with mismatched navigability be explained by the facilitation of different contextual associations or scene familiarities, rather than changes in perceptual representations? Especially when the visual similarity of the scenes was high and different visual cues might not have been immediately available to participants, the different contextual associations and/or familiarity could serve as indirect cues to facilitate participants' judgment, even if perceptual representations remained intact.</p>
<p>(2) Similarly, the above-chance fMRI classification results in the PPA could also be explained by the different contextual associations and/or scene familiarities between navigable and non-navigable scenes, rather than different perceptual processes related to scene identification.</p>
<p>(3) For the fMRI results, the specificity of the experience effect on the PPA is not strictly established, making the statement &quot;such top-down effect was unique to the PPA&quot; groundless. A significant interaction between navigational conditions and ROIs would be required to make such a claim.</p>
<p>(4) For the behavioral results, the p-value of the interaction between groups and the navigational conditions was 0.05. I think this is not a convincing p-value to rule out visual confounding for the training group. Moreover, from Figure 2B, there appears to be an outlier participant in the control group who deviates dramatically from the rest of the participants. If this outlier is excluded, will the interaction become even less significant?</p>
<p>(5) Experiment 1 only consists of 25 participants in each group. This is quite a small sample size for behavioral studies when there's no replication. It would be more convincing if an independent pre-registered replication study with a larger sample size could be conducted.</p>
</body>
</sub-article>
</article>