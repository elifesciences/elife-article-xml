<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">101036</article-id>
<article-id pub-id-type="doi">10.7554/eLife.101036</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.101036.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Age-dependent predictors of effective reinforcement motor learning across childhood</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9710-0291</contrib-id>
<name>
<surname>Hill</surname>
<given-names>Nayo M</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Tripp</surname>
<given-names>Haley M</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2011-2790</contrib-id>
<name>
<surname>Wolpert</surname>
<given-names>Daniel M</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Malone</surname>
<given-names>Laura A</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<name>
<surname>Bastian</surname>
<given-names>Amy J</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
<email>bastian@kennedykrieger.org</email>
</contrib>
    <aff id="a1"><label>1</label><institution>Kennedy Krieger Institute</institution>, <addr-line>Baltimore, MD</addr-line>, <country>United States</country></aff>
    <aff id="a2"><label>2</label><institution>Department of Neuroscience, Johns Hopkins School of Medicine</institution>, <addr-line>Baltimore, MD</addr-line>, <country>United States</country></aff>
    <aff id="a3"><label>3</label><institution>Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University</institution>, <addr-line>New York, NY</addr-line>, <country>United States</country></aff>
    <aff id="a4"><label>4</label><institution>Department of Neuroscience, Columbia University</institution>, <addr-line>New York, NY</addr-line>, <country>United States</country></aff>
    <aff id="a5"><label>5</label><institution>Department of Neurology, Johns Hopkins School of Medicine</institution>, <addr-line>Baltimore, MD</addr-line>, <country>United States</country></aff>
    <aff id="a6"><label>6</label><institution>Department of Physical Medicine and Rehabilitation, Johns Hopkins School of Medicine</institution>, <addr-line>Baltimore, MD</addr-line>, <country>United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Dekker</surname>
<given-names>Tessa</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Makin</surname>
<given-names>Tamar R</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Cambridge</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>*</label><p>These authors contributed equally to this work</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-09-30">
<day>30</day>
<month>09</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP101036</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-07-09">
<day>09</day>
<month>07</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-07-09">
<day>09</day>
<month>07</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.07.09.602665"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Hill et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Hill et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-101036-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Across development, children must learn motor skills such as eating with a spoon and drawing with a crayon. Reinforcement learning, driven by success and failure, is fundamental to such sensorimotor learning. It typically requires a child to explore movement options along a continuum (grip location on a crayon) and learn from probabilistic rewards (whether the crayon draws or breaks). Here, we studied the development of reinforcement motor learning using online motor tasks to engage children aged 3 to 17 and adults (cross-sectional sample, N=385). Participants moved a cartoon penguin across a scene and were rewarded (animated cartoon clip) based on their final movement position. Learning followed a clear developmental trajectory when participants could choose to move anywhere along a continuum and the reward probability depended on final movement position. Learning was incomplete or absent in 3 to 8-year-olds and gradually improved to adult-like levels by adolescence. A reinforcement learning model fit to each participant identified three age-dependent factors underlying improvement: amount of exploration after a failed movement, learning rate, and level of motor noise. We predicted, and confirmed, that switching to discrete targets and deterministic reward would improve 3 to 8-year-olds’ learning to adult-like levels by increasing exploration after failed movements. Overall, we show a robust developmental trajectory of reinforcement motor learning abilities under ecologically relevant conditions i.e., continuous movement options mapped to probabilistic reward. This learning appears to be limited by immature spatial processing and probabilistic reasoning abilities in young children and can be rescued by reducing the demands in these domains.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>In the game of Poohsticks, invented by A. A. Milne and described in <italic>The House at Pooh Corner</italic>, two children each drop a stick into a stream from the upstream side of a bridge [<xref ref-type="bibr" rid="c1">1</xref>]. They then race to the downstream side to see whose stick appears first, with the winner scoring a point. The game is repeated with each child trying to find the sweet spot to drop their stick in to win. Given the capricious nature of streams with their turbulent flow, dropping both sticks in exactly the same spot on two successive games can lead to different outcomes. To be an expert, a child must use probabilistic success and failure feedback to select a location from the infinite options available (the continuous span of the bridge) to drop their stick to maximize reward. Despite the complexity of this probabilistic reinforcement task, the current world champion is 9 years old. Here we examine how children develop the ability to learn such tasks from reinforcement feedback alone.</p>
<p>Reinforcement learning is essential for successful movement. Unlike error-based learning that uses a vector error, reinforcement learning relies on reward feedback that is indicated by a scalar value. Reward feedback can be simple binary feedback, such as success or failure in hitting the space bar on a keyboard, or continuous, such as the height achieved on a swing. Critically, the learner is not told what to do but must discover which movement or behavior produces reward by trying different options [<xref ref-type="bibr" rid="c2">2</xref>]. Therefore, a key component of reinforcement learning is exploring and evaluating feedback to maximize reward.</p>
<p>The basic capacity for reinforcement learning emerges early in life. For example, a nine-week-old infant will increase kicking frequency when a ribbon connects their foot to an overhead mobile that moves with their kicks [<xref ref-type="bibr" rid="c3">3</xref>]. Three-month-olds can learn to produce a specific kick height to move a mobile [<xref ref-type="bibr" rid="c4">4</xref>]. Both tasks have a deterministic relationship between the action and the outcome; a correct kick is always rewarded. In a more complex probabilistic reaching task, children aged 3- to 9-years old showed different sensitives to reward probability [<xref ref-type="bibr" rid="c5">5</xref>]. The youngest aged children were more likely to stick on a rewarded target even if the reward rate was low (e.g., 33%), whereas older children explored other targets. This suggests that younger children are less likely to explore new options and are more willing to accept lower reward rates.</p>
<p>Reinforcement learning has been studied in cognitive tasks that require children to select from discrete options to receive reward. In a relatively simple task, two-year old children could accurately select a reinforced visual image from two choices [<xref ref-type="bibr" rid="c6">6</xref>]. More complex tasks have been studied in older children and adolescents identifying age-related changes in learning ability [<xref ref-type="bibr" rid="c7">7</xref>–<xref ref-type="bibr" rid="c11">11</xref>]. In one task, participants selected from two options in a probabilistic reward environment with hidden factors that could change the outcome. Unbeknownst to the participants, agents (e.g., millionaires, robbers, or sheriffs) occasionally intervened to generate positive, negative, or random outcomes [<xref ref-type="bibr" rid="c8">8</xref>]. Both children and adolescents could identify the factors in the environment that changed the outcome. However, children under 12 years old did not use this information; younger children’s reinforcement learning mechanism was unable to use declarative information to optimize their choices. Similarly, Decker et al. used a sequential learning task with probabilistic rewards to show that probabilistic reasoning improves with age in a cohort of participants aged 8 to 25 years [<xref ref-type="bibr" rid="c9">9</xref>]. As a whole, this previous work identifies differences between younger and older children on choice-based selection tasks that require integration of reward feedback to learn successfully.</p>
<p>Although motor learning tasks have cognitive aspects [<xref ref-type="bibr" rid="c12">12</xref>], they also have features which are not present in such cognitive tasks. In motor learning, movement choice options are typically continuous (the choice of direction to kick a soccer ball), corrupted by motor noise, and there can be spatial gradient of reward (the probability of scoring as a function of kick direction). In contrast, cognitive tasks tend to be discrete with arbitrary assignment of reward probability to options. Here, we examine motor learning under reinforcement in which we control two key experimental factors. First, rewards can either be deterministic, the same action leads to the same outcome (e.g., pressing the space bar on a keyboard), or probabilistic, the outcome is stochastic (e.g., the path of a soccer ball depends on the wind and surface on which it is kicked). Second, action options can be discrete (e.g., the space or shift key on a keyboard) or continuous (e.g., the direction of a soccer kick). We report on a series of experiments in which we control both factors, reinforcement feedback (deterministic vs. probabilistic) and the action options (discrete vs. continuous targets) to examine the development of reinforcement learning across childhood. Our study builds on previous work in healthy adults examining reaching movements under binary reward feedback [<xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c14">14</xref>].</p>
<p>We developed a remote video game paradigm for a cross-sectional study of 298 children aged 3 to 17 years and 87 adults from across the USA (locations and demographics shown in <xref rid="figS1" ref-type="fig">Supp. Fig. 1</xref> and <xref rid="tbl1" ref-type="table">Table 1</xref>). Focusing on children’s reinforcement learning abilities in different tasks, we examined the developmental trajectory of exploration variability, learning rate, and motor noise. We found that younger children (3 to 8 years) fail to learn with a continuous target and probabilistic reward feedback. Reinforcement learning improved with age enabling older children to find the optimal reward region. Using a mechanistic model, we show that this improvement is due to a developmental gradient of increasing exploration, increasing learning rate, and reducing motor noise with age. Importantly, we then show that use of deterministic feedback and discrete targets can dramatically improve learning abilities of younger children.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Participant Demographics</title>
<p>Ethnicity and race classifications were self-reported by participant/parent. Participants who identified as two or more categories of race (Black, White, and/or Asian) were classified as Multiple. Participants who specified Asian (Indian) or South Asian were classified as Asian. Participants who identified as one or more races other than Black, White, or Asian were classified as <italic>Other. Abbreviations: Cont</italic>., <italic>continuous; Det</italic>., <italic>deterministic; Dis</italic>., <italic>discrete; n, number; Prob, probabilistic; RH, right-handed; std, standard deviation; yrs, years</italic>.</p></caption>
<graphic xlink:href="602665v1_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s2">
<title>Results</title>
<p>In the first experiment (continuous probabilistic task), we studied 111 children and adolescents, aged 3 to 17 years, and 33 adults as they attempted to learn to produce movements that maximized reward. For the key learning phase of the experiment, we created a probabilistic reward landscape in which the probability of reward after each movement depended on its endpoint. To implement this in a task that was engaging to children, we created a computer game that was played at home. Participants were required to move a cartoon penguin from a starting position to join a group of penguins arranged horizontally across the screen (<xref rid="fig1" ref-type="fig">Fig. 1a</xref> - continuous). Participants were told that there was a slippery ice patch just before the group (dark blue area, <xref rid="fig1" ref-type="fig">Fig. 1a</xref> - continuous) and that they should try to make the penguin cross at the location where they would not slip. In the instructions, participants were told that there was a location in the ice where they would never slip. The reward landscape (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> top left) determined the probability of success on each trial and was not known to the participant. There was a small 100% reward zone where the penguin would never slip with the reward probability decreasing away from this zone. Successful trials led to a short Disney cartoon clip playing whereas a failure trial led to the penguin falling over and the appearance of a static image of a sad penguin (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig 1.</label>
<caption><title>Game Environment.</title>
<p><bold>a</bold>. Screenshot of game environment and sample movement path (large text, arrows, and movement path were not displayed to participants). During the learning block, participants either experienced a continuous target (continuous groups) or seven discrete targets (discrete groups). <bold>b</bold>. Reward landscape for the learning blocks for the different task paradigms. Continuous probabilistic: continuous target with position-based reward probability gradient; discrete probabilistic: discrete targets with target specific reward probabilities; continuous deterministic: continuous target with a single 100% rewarded zone; discrete deterministic: discrete targets with a single target giving 100% reward. <bold>c</bold>. Outcome feedback for continuous probabilistic task. Success (top), movie clip and pleasant sound plays, and video screen is outlined in blue. Failure (bottom), movie clip does not play, the penguin falls over and red text “You slipped!” appears with a sad face image.</p></caption>
<graphic xlink:href="602665v1_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Participants performed 5 blocks of trials (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). In block 1, participants made 20 baseline movements to a series of small, discrete targets that appeared at different locations (sample paths are shown in <xref rid="figS2" ref-type="fig">Supp. Figs. 2</xref> and <xref rid="figS3" ref-type="fig">3</xref>). The targets were presented in a randomized order but each participant received the same 20 target locations. This allowed the participants to experience reaching to many locations in the workspace as well as assessed their accuracy to hit discrete targets. <xref rid="fig2" ref-type="fig">Fig. 2b</xref> shows examples from different aged participants. All performed well in the baseline block of the task (trials 1–20). On average, all but one participant could accurately hit targets (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>) although younger children tended to make less straight movements (<xref rid="figS4" ref-type="fig">Supp. Fig. 4</xref>) and took longer to initiate and start trials (<xref rid="figS5" ref-type="fig">Supp. Fig. 5</xref>). Participants then began the learning block (described above), where they could move to any endpoint location on the screen. <xref rid="fig2" ref-type="fig">Fig. 2b</xref> shows representative examples of endpoint locations across the experiment for participants of different ages. The 5- and 7-year-old children moved to many endpoint locations (i.e., showed exploration) receiving reward based on the probability landscape. Interestingly, their exploration appeared to show little sensitivity to previous failure (open circles) versus success (filled circles) and endpoints did not converge on the 100% reward zone (gray area) by the end of the block. The older children, aged 11, 12, and 15, performed similarly to the adult, exploring early in the block, and then focusing movements on the unseen 100% reward zone. This indicates that they were able to explore successfully after failures and exploit the 100% reward zone.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig 2.</label>
<caption><title>Paradigm, example behavior, and target accuracy in the continuous probabilistic task.</title>
<p><bold>a</bold>. Experimental design with baseline: single discrete target presented in randomized locations across the screen; learning: learning block with reward determined by endpoint position; success clamp: feedback clamped to 100% success independent of endpoint position; fail clamp: feedback clamped to 100% failure independent of endpoint position; single target: single discrete target presented in the middle of the screen. <bold>b</bold>. Representative endpoint time series from various aged participants. Gray shaded zones indicate positions in the workspace where a reward is given 100% of the time (thin gray lines are for discrete targets). Green filled circles indicate rewarded trials while open circles indicate unrewarded trials. The horizontal colored bar on the x-axis indicates the trials corresponding with the experimental blocks outlined in <bold>a</bold>. In the learning block (trials 21–120), rewards were given based on the continuous probabilistic landscape. <bold>c</bold>. Mean baseline accuracy (average reach deviation from the discrete targets) by age. Adult data are averaged and plotted to the right with standard error of the mean. The gray region shows the width of a discrete target. <bold>d</bold>. Same as <bold>c</bold> for the single target in block 5. In <bold>c</bold> and <bold>d</bold>, participants who completed the task in person (in lab) are indicated in white circles.</p></caption>
<graphic xlink:href="602665v1_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig 3.</label>
<caption><title>Continuous probabilistic task learning block time series.</title>
<p>Data and model (red, smooth curves) for each trial of the learning block grouped into age ranges. Data shows mean (solid line) and standard error of mean (shading) of participants’ endpoint. Model in red shows mean (dashed lines) and standard deviation (shading) from the model simulations. The gray region shows 100% reward zone.</p></caption>
<graphic xlink:href="602665v1_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig 4.</label>
<caption><title>Variability and learning in the continuous probabilistic task.</title>
<p><bold>a</bold>. Baseline variability by age. Average adult variability shown for comparison. <bold>b</bold>. Learning block performance (absolute distance from 100% reward zone) by age. <bold>c</bold>. Endpoint variability in the success clamp by age. <bold>d</bold>. Endpoint variability in the failure clamp by age. For <bold>a</bold> - <bold>d</bold>, regression line with 95% confidence limits shown for children and error bars show standard error of the mean for adults. <bold>e</bold>. Predicted vs. observed performance from the multiple regression of learning as a function of age, baseline variability and fail clamp variability. <bold>f</bold>. Mediation analysis (see Methods for details). Top pathway shows the direct relationship between age and learning. Bottom pathway shows the indirect relationship between age and learning when mediated by baseline variability and fail clamp variability. Note that in our measure of learning, smaller distances from the 100% reward zone reflect better learning, which explains the negative relationships in this analysis (e.g., increasing age is associated with decreased distances from the reward zone). Age is coded in months. Participants who completed the task in person (in lab) are indicated in white symbols.</p></caption>
<graphic xlink:href="602665v1_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig 5.</label>
<caption><title>Reinforcement learning model for the continuous probabilistic task.</title>
<p><bold>a</bold>. Model schematic. The participant maintains an estimate of desired reach which they can update across the experiment. The actual reach on the current trial (pink box) depends on whether the previous trial (yellow box) was a failure (top) or success (bottom). After failure (top) the actual reach is the desired reach with the addition of exploration and motor noise (draws from zero mean Gaussian distributions with standard deviations <italic>σ</italic><sub><italic>e</italic></sub> and <italic>σ</italic><sub><italic>m</italic></sub>, respectively). In contrast, if the previous trial was a success (bottom), the participant does not explore so that the actual reach is the desired reach with only motor noise. The actual reach determines the probability of whether the current trial is a failure or a success. If the current trial is a success the desired reach is updated for the next trial (blue box) by the exploration (if any), modulated by a learning rate <italic>η</italic>. <bold>b</bold>. Model fit parameters {<italic>σ</italic><sub><italic>m</italic></sub>, <italic>σ</italic><sub><italic>e</italic></sub>, <italic>η</italic>}, by age for the continuous probabilistic group. The solid thick line is a regression fit to the data for participants less than 18 years old and the thin line is a running mean <italic>±</italic>3 years with the standard error of the mean. The correlation and p-value for each regression are shown in the bottom left corner of each plot (and exclude the adult data). Average adult parameters are shown on the right with standard error of the mean. <bold>c</bold>. Predicted vs. actual variability in the success (left column) and fail (right column) clamp blocks. Correlations and p-vales are shown above each plot (the plots and statistics exclude the adult data).</p></caption>
<graphic xlink:href="602665v1_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>These patterns were also observed in group data (<xref rid="fig3" ref-type="fig">Fig. 3</xref>, binned across 6 age groups) where, on average, the 3 to 5 and 6 to 8-year-olds did not find the 100% reward zone, but the older age groups did. After learning, participants transitioned seamlessly (no break or change in instructions) into two short blocks with clamped feedback to examine behavior in response to repeated success or repeated failure. The success clamp always rewarded movements and the fail clamp never rewarded movements. The 5- and 7-year-olds explored similarly in both clamped blocks whereas the older children showed greater exploration in the fail clamp compared to the success clamp (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>). In the final block, participants moved to a single discrete target in the center of the workspace (examples <xref rid="fig2" ref-type="fig">Fig. 2b</xref>; group data <xref rid="fig2" ref-type="fig">Fig. 2d</xref>). For participants age 3 to 17 years, an ANOVA of accuracy (distance from discrete target center) by block (first vs. last) and age (5 bins) shows no effect of block (<italic>F</italic><sub>1</sub>,<sub>106</sub> = 0.282, p = 0.597) and no interaction between age and block (<italic>F</italic><sub>4</sub>,<sub>106</sub> = 1.219, p = 0.307), indicating that accuracy of movement was maintained throughout the experiment even for the younger children.</p>
<p>We assessed key features of behavior in the first four blocks of the experiment that could contribute to the learning patterns observed across age (<xref rid="fig4" ref-type="fig">Fig. 4a-d</xref>, with the average of adult performance for comparison). In the Baseline block, although all participants were able to hit the targets, the endpoint variability relative to target center decreased significantly with age (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>; regression <italic>R</italic><sup>2</sup> = 0.153, <italic>F</italic><sub>1</sub>,<sub>109</sub> = 19.7, p &lt; 0.001). This is consistent with known developmental changes in motor variability [<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c16">16</xref>] and may represent motor noise which is distinct from exploration.</p>
<p>We quantified final performance in the learning block as the distance of the reaches from the center of the 100% reward zone averaged over the last 10 trials of the block. In children, the distance significantly reduced with age indicating better learning (<xref rid="fig4" ref-type="fig">Fig. 4b</xref>; regression <italic>R</italic><sup>2</sup> = 0.256, <italic>F</italic><sub>1</sub>,<sub>109</sub> = 37.4, p &lt; 0.001). Younger children rarely reached the 100% reward zone whereas children over 9 years old often did.</p>
<p>To be successful in the task, we would expect participants to behave differently after a failure versus after a success. That is, participants need to explore to find the 100% reward zone when their reward rate is low (i.e., after failure) and reduce exploration when their reward rate is high (i.e., after success). We used the clamp blocks to assess how participants responded to repeated success versus repeated failure, and if this contributed to developing adult-like learning patterns. We calculated the standard deviation of movement endpoints in each clamp condition as a measure of movement variability (<xref rid="fig4" ref-type="fig">Fig. 4c &amp; d</xref>). Adults showed high variability after failure and low variability after success, as expected. In children, variability after failure was low in younger children and increased significantly with age (regression <italic>R</italic><sup>2</sup> = 0.17, <italic>F</italic><sub>1</sub>,<sub>109</sub> = 22.3, p &lt; 0.001) to reach adult levels. Regression shows that variability after failure doubles across the age range.This likely reflects an increase in active exploration in an attempt to find the reward zone. In contrast, variability after success was relatively stable across ages (regression <italic>R</italic><sup>2</sup> = 0.012, <italic>F</italic><sub>1</sub>,<sub>109</sub> = 1.34, p = 0.249). Overall, these results suggest that younger children do not explore as much as older children after failures, which is essential to finding the 100% reward zone.</p>
<p>We hypothesized that a combination of age and the variability in the baseline, success clamp, and fail clamp blocks could be predictors of final performance. Using multiple linear regression, we found that three of the four factors were significant predictors of learning (<italic>R</italic><sup>2</sup> = 0.537, <italic>F</italic><sub>4</sub>,<sub>106</sub> = 30.7, p &lt; 0.001). Variability after failure was the strongest predictor of learning (<italic>β</italic> = -0.613, p &lt; 0.001) followed by baseline variability (<italic>β</italic> = 0.706, p = 0.001) and age (<italic>β</italic> = - 0.019, p = 0.022). Variability after success was not significant (<italic>β</italic> = 0.244, p = 0.067). <xref rid="fig4" ref-type="fig">Fig. 4e</xref> shows that a model with the three significant regressors predicts the observed behavior well (<italic>R</italic><sup>2</sup> = 0.703, p &lt; 0.001). We then performed a mediation analysis to determine if variability in the baseline and fail clamp blocks mediated the effect of age on learning (<xref rid="tblS1" ref-type="table">Supplementary Table 1</xref>). Both mediators significantly reduced, but did not eliminate, the direct effect of age on learning (<xref rid="fig4" ref-type="fig">Fig. 4f</xref>, <italic>c</italic><sup><italic>′</italic></sup> was reduced compared to <italic>c</italic> in the mediation, but still remained significant. See Methods for details). Thus, baseline variability and variability after failure can explain part of the effect of age on learning, but age still has a significant direct effect.</p>
<p>Up to this point, we have considered variability in the baseline and clamp blocks as a combination of sensorimotor noise and exploration. To separate motor noise from exploration and to understand how experience changes behavior, we developed a reinforcement learning model of the task. To determine the structure of the model, we first examined how many recent trials affected the current trial. Specifically, we considered how failures up to three trials back affected how participants change their current endpoint (See Methods for additional details). This analysis (<xref rid="figS6" ref-type="fig">Supp. Fig. 6</xref>) showed that for the majority of participants only the just previous failure trial impacted the change in endpoint on the current trial and that failures further back did not affect current behavior. Given this, the reinforcement learning model assumes that exploration only depends on the outcome of the previous trial.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Fig 6.</label>
<caption><title>Comparison of the four tasks for the three- to eight-year-old children.</title>
<p><bold>a</bold>. Learning block performance for the continuous probabilistic, discrete probabilistic, continuous deterministic, and discrete deterministic tasks in the same format as <xref rid="fig3" ref-type="fig">Fig. 3</xref>. <bold>b</bold>. Comparative performance between tasks for learning distance and variability in baseline, success clamp, and fail clamp. Learning significantly improved with discrete targets and deterministic reward feedback. Baseline variability was not statistically different between tasks. Statistically significant pairwise comparisons indicated as follows: * = p &lt; 0.05, + = p &lt; 0.01, and Δ = p &lt; 0.001. Abbreviations: CD = Continuous Deterministic; CP = Continuous Probabilistic; DD = Discrete Deterministic; DP = Discrete Probabilistic</p></caption>
<graphic xlink:href="602665v1_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We modeled each participant as a learner who maintains an estimate of their desired reach location which can be updated. If the previous trial was a failure (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>, top), the current reach is the desired reach with the addition of motor noise and exploration (draws from zero mean Gaussian distributions with standard deviations <italic>σ</italic><sub><italic>e</italic></sub> and <italic>σ</italic><sub><italic>m</italic></sub>, respectively). The desired reach for the next trial is updated only if the current reach is successful. The update is a proportion (<italic>η</italic>, termed the learning rate) of the exploration that led to success. In contrast, if the previous reach was a success (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>, bottom) the participant does not explore so that the current reach is the desired reach with motor noise. As there is no exploration, the desired reach will remain the same for the next trial whatever the outcome (see Methods for details).</p>
<p>We fit the model to the 100 learning trials for each child and adult participant (see Methods for details), so as not to contaminate the fits by behavior in the clamp blocks. We first examined how well our model fitting procedure could recover parameters from synthetic data. This showed that all three parameters (<italic>σ</italic><sub><italic>m</italic></sub>, <italic>σ</italic><sub><italic>e</italic></sub>, <italic>η</italic>) were well recovered with correlations with the true parameters of at least 0.99 (<xref rid="figS7" ref-type="fig">Supp. Fig. 7</xref>). We examined how the three parameters fit to the actual data varied with age (<xref rid="fig5" ref-type="fig">Fig. 5b</xref>). This showed that all three parameters varied significantly with age for the children (all p &lt; 0.01). Motor noise decreased by about 40% within the age range tested consistent with our measure of baseline variability which also decreased significantly with age. Exploration variability increased with age, approximately doubling on average from the youngest to the oldest child. While this is similar to the variability findings in the fail clamp, the model allows us to examine exploration variability in the learning setting and separate it from motor noise. In addition to exploration increasing with age, the learning rate also increased across the age span from about 0.5 to around 0.8 by 17 years. Overall, this suggests that the increased motor noise, reduced exploration, and reduced learning rate limits the ability of the younger children to learn efficiently.</p>
<p>The simulations of the model with the fit parameters accounted for the differences in learning across the age group bins (<xref rid="fig3" ref-type="fig">Fig. 3</xref> red dotted line and shading show model simulations). The model simulated data performed similarly to the actual data from children age 6 and older as well as the adults. Note that the model simulated data performed slightly better than the youngest children. The model also makes a prediction of the reach variability expected in the two clamp trial blocks. In the success clamp block the variability (s.d.) only arises from motor noise and should be <italic>σ</italic><sub><italic>m</italic></sub>. For the fail clamp block it arises from both motor noise and exploration and should be <inline-formula><inline-graphic xlink:href="602665v1_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. <xref rid="fig5" ref-type="fig">Fig. 5c</xref> shows the variability expected from the model against the measured variability of the children in the clamp blocks (left and right columns for success and fail clamps). These out-of-sample predictions showed significant positive correlations for both the success and fail clamp blocks (p &lt; 0.001). This was despite having only 9 trials to calculate each clamp block s.d., suggesting that the model fit to learning can predict differences in exploration.</p>
<p>Poor or absent learning in the youngest age group could arise from these participants not understanding the task. However, we can rule out task comprehension as an explanation for lack of learning, at least for the majority of younger participants. First, 16 participants completed the task in the lab, and this sample included 5 participants in the 3- to 8-year-old age range. The task was administered in the same way as the online version, without additional instructions or interaction with the experimenter during the task. However, after completion, the experimenter asked these children to explain what they did during the task as an informal comprehension check and it was clear that this group understood the task. Performance for all participants tested in the lab was not significantly different from online completion and these data points are indicated in <xref rid="fig2" ref-type="fig">Fig. 2</xref> and <xref rid="fig4" ref-type="fig">Fig. 4</xref> with white symbols. Wilcoxon signed-rank tests showed no significant difference between in lab and online participants in baseline mean (Z = 1.238, p = 0.216), baseline variability (Z = -0.626, p = 0.532), learning distance (Z = -0.348, p = 0.728), success variability (Z = -1.952, p = 0.051), fail clamp variability (Z = 0.281, p = 0.779), or single target mean (Z = -0.399, p = 0.690).</p>
<p>Second, we used the reinforcement learning model to assess whether individual’s performance is consistent with understanding the task. In the case of a child who does not understand the task, we expect that they simply have motor noise on their reach, but crucially that they would not explore more after failure, nor update their reach after success. Therefore, we fit both a reduced model (motor noise only) as well as the full model (including exploration and learning) to each participant. We used a likelihood ratio test to examine whether the full model was significantly better at explaining each participant’s data compared to the reduced, non-learning model. Importantly, even if there is no learning on average, the full model can capture aspects of the data. For example, increased exploration after failure with a learning rate of zero would give no learning but the full model could explain the data better as it would capture the changes in variance as a function of trial success or failure. Even with a non-zero learning rate the model can show little overall learning but capture aspects of the child’s behavior over the learning block. This analysis shows that only 19 of the 144 participants were better fit with the noise only model with the remaining participants best fit with the full model. Importantly, 39 out of 48 of the younger participants (3 to 8 years) were better fit by the full model, indicating non-zero exploration after failure and typically a non-zero learning rate. <xref rid="figS8" ref-type="fig">Supp. Fig. 8</xref> shows averages for those fit best by the motor noise model (left) and those fit best by the full model (right). The younger children who are best fit by the full model show minimal learning, similar to what we report for the entire group (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). Therefore, our results are robust to only including participants whose data is best accounted for by including exploration and learning.</p>
<p>Thus far, we identified features that contribute to the younger children’s inability to learn within a continuous probabilistic landscape. Specifically, less variability after failure, more baseline variability, and younger age. In a second experiment, we asked if segmenting the continuous target into seven discrete targets could improve learning (<xref rid="fig1" ref-type="fig">Fig. 1a</xref> - discrete). In this new task, participants were not able to move between targets but were required to definitively select a single target on each trial in order for the experiment to advance. We hypothesized that discrete targets could increase exploration by encouraging children to move to a different target after failure. We studied a new cohort of 106 children and 33 adults as they learned the discrete probabilistic task (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> top right). <xref rid="figS9" ref-type="fig">Supp. Fig. 9</xref> shows individual example data. Some of our participants tended to move to a new target after failure (e.g., the 6 and 16 year old). Group data (<xref rid="figS10" ref-type="fig">Supp. Fig. 10</xref>) showed modest benefit in the 3 to 5 and 6 to 8 year-old participants, with all other age groups performing similarly compared to the continuous task. <xref rid="figS11" ref-type="fig">Supp. Fig. 11</xref> shows variability and learning measures which are similar to the continuous group. Learning distance decreased with age (regression <italic>R</italic><sup>2</sup> = 0.074, <italic>F</italic><sub>1</sub>,<sub>104</sub> = 8.31, p = 0.0048) indicating better learning performance. Baseline endpoint variability and variability after success decreased with age (baseline: regression <italic>R</italic><sup>2</sup> = 0.150, <italic>F</italic><sub>1</sub>,<sub>104</sub> = 18.4, p &lt; 0.001; success clamp: regression <italic>R</italic><sup>2</sup> = 0.048, <italic>F</italic><sub>1</sub>,<sub>104</sub> = 5.25, p = 0.024) while variability after failure increased with age (regression <italic>R</italic><sup>2</sup> = 0.162, <italic>F</italic><sub>1</sub>,<sub>104</sub> = 20.1, p &lt; 0.001).</p>
<p>We performed multiple regression, as for the continuous task, which showed that only variability after failure (<italic>β</italic> = -0.343, p &lt; 0.001) and after success (<italic>β</italic> = 1.171, p &lt; 0.001) but not age (<italic>β</italic> = -0.002, p = 0.722) or baseline variability (<italic>β</italic> = -0.210, p = 0.070) were significant predictors of learning (<italic>R</italic><sup>2</sup> = 0.764, <italic>F</italic><sub>4</sub>,<sub>101</sub> = 81.9, p &lt; 0.001). <xref rid="figS11" ref-type="fig">Supp. Fig. 11e</xref> shows that the model with only the significant regressors predicts the learning well (<italic>R</italic><sup>2</sup> = 0.686, p &lt; 0.001). In contrast to the continuous task, baseline variability did not affect learning. This is likely because in the discrete task, as long as you can reach to your target of choice, motor noise will not affect the outcome. In order to understand why variability after success and failure, but not age, were significant regressors, we performed a mediation analysis (<xref rid="figS11" ref-type="fig">Supp. Fig. 11f</xref>). This showed that although age alone was a predictor of learning, it was fully mediated by variability after success and failure (<xref rid="tblS2" ref-type="table">Supplementary Table 2</xref>).</p>
<p>We used the reinforcement model to fit data in the discrete probabilistic task. The model is fit to the reach endpoint locations and rewards the participants received (that is we do not explicitly represent discrete targets in the model). Compared to the continuous task, the parameters did not change significantly with age (<xref rid="figS12" ref-type="fig">Supp. Fig. 12</xref>). To compare the fit parameter by age across the two tasks, we compared the linear regression. This showed that the exploration and learning rate had higher intercepts for the discrete task (both p &lt; 0.05) but there was no significant difference in motor noise. This suggests that the discrete nature of the task encouraged all participants to explore more and update their desired reach more after success.</p>
<p>In both experiments considered so far, the reward was probabilistic. While this provides gradient information that should guide the participant to the target, younger children may not use the same strategy for this reward landscape. In the final two tasks, we studied new cohorts of 3–8-year-old children since they showed poorest learning in the continuous and discrete probabilistic tasks. We assessed the effect of a deterministic reward landscape (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>, bottom row) on learning with the continuous and discrete targets. <xref rid="fig6" ref-type="fig">Fig. 6a</xref> compares the time courses for the 3–8 year olds across all four tasks. There was not a significant difference in mean age between the four tasks (<italic>F</italic><sub>3</sub>,<sub>168</sub> = 1.072, p = 0.362) nor was there a significant interaction between task and sex on learning ability (<italic>F</italic><sub>3</sub>,<sub>164</sub> = 0.97, p = 0.409). Learning was worst under the continuous probabilistic task, followed by the discrete probabilistic task and continuous deterministic task. The 3–8 year olds performed best on the discrete deterministic task and in contrast to the other three tasks, show a similar time course of learning as adults for the same task (<xref rid="figS13" ref-type="fig">Supp. Fig. 13</xref>). ANOVA of final learning (<xref rid="fig6" ref-type="fig">Fig. 6b</xref> left panel) showed a significant effect of Target type (discrete better than continuous; <italic>F</italic><sub>1</sub>,<sub>168</sub> = 12.87, p &lt; 0.001) and Reward landscape (deterministic better than probabilistic; <italic>F</italic><sub>1</sub>,<sub>168</sub> = 43.66, p &lt; 0.001) with no interaction (<italic>F</italic><sub>1</sub>,<sub>168</sub> = 0.24, p = 0.628). This shows that making the task discrete or deterministic improves learning and that these factors were additive. This was not due to significant differences in baseline variability (<xref rid="fig6" ref-type="fig">Fig. 6b</xref> 2nd panel; Target type: <italic>F</italic><sub>1</sub>,<sub>168</sub> = 0.16, p = 0.69, Reward landscape: <italic>F</italic><sub>1</sub>,<sub>168</sub> = 0.56, p = 0.455, interaction: <italic>F</italic><sub>1</sub>,<sub>168</sub> = 1.71, p = 0.192).</p>
<p>We examined variability in the success and fail clamp blocks as a function of Target type and Reward landscape. For the success clamp block, there was no main effect of Target type (<italic>F</italic><sub>1</sub>,<sub>168</sub> = 0.07, p = 0.79), but a main effect of Reward landscape (<italic>F</italic><sub>1</sub>,<sub>168</sub> = 6.71, p = 0.01; less variability with a deterministic landscape) with no interaction (<italic>F</italic><sub>1</sub>,<sub>168</sub> = 3.58, p = 0.06). This is consistent with there being no advantage to explore after success under a deterministic reward landscape, whereas there is under a probabilistic landscape (exploration can lead to locations that have higher probability of reward). For the fail clamp block there was a main effect of both Target type (<italic>F</italic><sub>1</sub>,<sub>168</sub> = 29.93, p &lt; 0.001; greater variability for discrete targets) and Reward landscape (<italic>F</italic><sub>1</sub>,<sub>168</sub> = 9.26, p = 0.003; greater variability with a deterministic landscape) with no interaction (<italic>F</italic><sub>1</sub>,<sub>168</sub> = 1.01, p = 0.316). The increased variability for discrete targets is likely because participants must move to a new target to explore resulting in a larger position change. Increased variability after failure in the deterministic landscape is likely because a failure at one location predicts there will never be success at that location (in contrast to the probabilistic tasks) thereby encouraging exploration. These show that even young children choose their exploration in a rational way based on tasks features.</p>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We studied a large, cross-sectional cohort of 3 to 17-year-old children and adults performing reinforcement-based motor learning tasks. Binary feedback—success or failure—was the only learning signal provided as participants moved to continuous or discrete targets under probabilistic or deterministic reward landscapes. The continuous target gave them unlimited movement choices across a target zone, whereas discrete targets constrained choices. Reward probability mapped to movement endpoint position, with the probabilistic landscape varying from 25% to a small 100% region, and the deterministic landscape with a single, small 100% reward zone. We found a developmental trajectory of reinforcement learning in the continuous probabilistic task, with older children learning more than younger children. This was paralleled by age-dependent increases in exploration after a failed trial which is essential for learning and decreases in baseline movement variability. A mechanistic model of the task revealed that in addition to the increase in exploration with age, there was a beneficial reduction in motor noise with age. The model also indicated that the learning rate increased with age. The learning rate was a proportion of the exploration (that led to success) that was incorporated into the estimate of where participants should move in future trials; higher learning rates meant that they updated more.</p>
<p>In contrast to the children in the continuous probabilistic group, children in the discrete targets or deterministic reward groups showed better learning. Moreover, these effects appeared to be additive—the 3to 8-year-old children learned best with discrete targets <italic>and</italic> in a deterministic reward landscape. Thus, the youngest children had the fundamental mechanisms for interpreting binary reward signals to drive reinforcement learning of movement—this is not surprising given that this ability has been shown in infants [<xref ref-type="bibr" rid="c3">3</xref>]. However, children aged 3 to 8 did not effectively utilize reward signals to learn in situations where they had to respond to probabilistic environments or where there were no spatial cues specifying distinct targets.</p>
<p>Our data suggest that the developmental trajectory we identified was not due to poor motor accuracy or lack of understanding of the task in the younger children. We designed the baseline block to ensure that children could accurately hit individual targets presented in different locations across the screen. The width of these targets was the same as that of the hidden 100% reward zone within the learning block and children of all ages could hit these targets accurately. The youngest children could also learn similarly to adults in the discrete deterministic task. This shows that children were able to understand the concept of the task and how to be successful.</p>
<p>Movement variability in our task can be due to motor noise (unintentional) as well as active exploration (intentional). While experimentally, it is challenging to separate the two sources of variability, our reinforcement learning model allows us to individually characterize motor noise and exploration. Simulations with our fit parameters approximated the observed learning behavior suggesting that these two components of variability are the drivers of learning performance. Our model suggests that with age, motor noise decreases and exploration variability increases making one more able to learn effectively within a probabilistic learning environment.</p>
<p>Why do discrete targets improve learning performance? In our tasks, a child needs to remember their previous endpoint position and whether it was rewarded to decide where to move at the start of the next trial. We observed that tasks with the continuous target were harder for younger participants to learn, and suspect this is because spatial working memory is not fully developed [<xref ref-type="bibr" rid="c17">17</xref>], particularly in those under 10 years of age [<xref ref-type="bibr" rid="c18">18</xref>–<xref ref-type="bibr" rid="c20">20</xref>]. Younger children may benefit from clear markers in the environment to differentiate various spatial positions along the target. This is consistent with tasks that have assessed children’s preferences for reporting information; they find it easier to select from discrete choices on a Likert scale versus using a continuous visual analog scale [<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>]. Finally, it is also possible that older children had the ability to use different working memory encoding mechanisms. It is known that phonological encoding using verbal labels develops later than visual encoding and both can be used to hold information in working memory (e.g., my last movement was near the left edge of the screen) [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c19">19</xref>]. Future work could explore whether providing an explicit verbal strategy to younger children could improve their ability to explore more effectively with a continuous target, highlighting the interplay of cognitive and motor domains in reinforcement learning.</p>
<p>Why did deterministic reward feedback improve learning? The deterministic landscape is less ambiguous than the probabilistic landscape by design—participants always fail if they are outside the 100% reward zone. The need to track and evaluate the reward frequency across target zones is eliminated, making the task less complex. Younger children have been reported to take longer to choose a reward maximization strategy in a probabilistic environment where all choices have the possibility of being rewarded. Plate et al. showed that when children and adults choose between eight options, they initially probability match (i.e., the frequency of selection closely matches the frequency of reward on the various task options). However, adults switch over to a maximizing strategy (i.e., sticking to the highest reward option) more quickly than children [<xref ref-type="bibr" rid="c23">23</xref>]. In a deterministic landscape, probability matching would result in the same behavior as reward maximizing and therefore the younger children’s behavior would appear nearly the same as adults. Young children’s behavior may also stem from a fundamental need to prioritize hypothesis testing and gathering information about the world [<xref ref-type="bibr" rid="c24">24</xref>–<xref ref-type="bibr" rid="c26">26</xref>], a discovery process that facilitates increased knowledge about the environment’s causal structure [<xref ref-type="bibr" rid="c27">27</xref>].</p>
<p>Our interpretation is that poorer learning performance in tasks besides discrete deterministic was due to inability to effectively utilize probabilistic reward signals or find the high reward location without clearly delineated spatial cues. This has significant ramifications as most objects in the world do not have delineated targets on them; we learn which location on an object leads to reward by exploring different options (e.g., the best location to grab a mug or push a heavy door open). The world is also not deterministic, as it is rare that the same action will always give the same result. Movement outcomes are probabilistic due to both environmental variation and motor noise (e.g., the direction of a soccer ball when kicked on different fields or the location of a thrown dart on a dartboard). Eventually, children must learn how to move successfully to interact with their environments using such probabilistic signals.</p>
<p>The differential ability to incorporate reward signals into changes in behavior across childhood may stem from maturation of the reward centers in the brain. Structures important for these processes, such as the basal ganglia reward centers, dorsal lateral prefrontal cortex, posterior parietal cortex, and the anterior cingulate cortex develop throughout childhood [<xref ref-type="bibr" rid="c28">28</xref>–<xref ref-type="bibr" rid="c30">30</xref>]. As a main underlying neural structure of reinforcement learning, the pallidum reaches peak volume at 9.5 years for females and 7.7 years for males while it takes the thalamus until 13.8 years for females and 17.4 years for males to reach peak volume [<xref ref-type="bibr" rid="c28">28</xref>]. Older children have more mature brain circuits and may be better able to take advantage of strategies in probabilistic environments that younger children cannot [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>]. For example, older children might know to avoid a location as soon as a failure occurs, even if that location was previously rewarded. Younger children might continue sampling those locations, perhaps due to immature signaling in the brain. Indeed, it has been shown that brain activity in younger children can be similar after positive and negative rewards whereas in older children and adults it is more distinct [<xref ref-type="bibr" rid="c31">31</xref>–<xref ref-type="bibr" rid="c33">33</xref>].</p>
<p>Online studies such as ours inevitably have some limitations. Given that this task was conducted remotely, we did not have control of the computer operating quality, internet speed, or testing environment for our participants (see Methods for exclusion criteria). As such, we were not able to control or analyze timing parameters of movement which can be done more easily in in-person experimentation. However, our key analyses compare across large groups which likely factor out these uncontrolled variables. Our results (comparison of in lab vs. online and fit of a noise model vs. full reinforcement learning model) also confirm that even the youngest age group understood the task.</p>
<p>Our findings in typical development lay a foundation for better understanding of behavior during childhood and could help inform what function may be lost or impaired after injury. This underscores the need to consider not only disease process for interventions but also age as there are developmental differences in motor learning capacity in individuals at different ages. Knowing how the sensorimotor system works at different ages can guide decisions on how to intervene or alter an environment and give children the best opportunity to use reinforcement learning mechanisms for rehabilitation outcomes.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>Children and adults without neurological impairment or developmental delay were recruited to one of four experiments as outlined in <xref rid="tbl1" ref-type="table">Table 1</xref>. A total of 385 complete datasets were included in the analysis. The continuous probabilistic task was designed first and the other three tasks, discrete probabilistic, continuous deterministic, and discrete deterministic, were conducted to expand upon the results of the first task and further identify factors contributing to learning by reinforcement across the developmental spectrum. Participants were recruited from the Johns Hopkins University community through an online announcement portal, the greater Baltimore Maryland area through Research Match, and nationwide through the online platform Lookit which merged with Children Helping Science in 2023 [<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>]. Our sample includes participants from 38 states out of the 50 United States of America (<xref rid="figS1" ref-type="fig">Supp. Fig. 1</xref>). Each participant was screened to rule out neurological impairment, developmental delay, and other developmental disorders that could affect motor and cognitive development. This study was approved by the Johns Hopkins School of Medicine Institutional Review board and all participants, or their parent/legal guardian, provided informed consent prior to participation.</p>
</sec>
<sec id="s4b">
<title>Task Platform</title>
<p>All four tasks were completed on a web-based platform built with Javascript as previously reported by [<xref ref-type="bibr" rid="c36">36</xref>] with modifications to the game environment and feedback type. This platform allowed creativity in the game environment design to be kid friendly and engaging to young participants to foster sustained attention and increase likelihood of completing the full task. The task platform allowed remote completion of the experiment by participants on their home computer or tablet. A small subset of participants in the continuous probabilistic task (n = 16; 3 to 5yo n = 2, 6 to 8yo n = 3, 9 to 11yo n = 5, 12 to 14yo n = 5, 15 to 17yo n = 1) completed the task in person in the research laboratory and the remainder of participants completed the task remotely. Participants used a mouse, trackpad, or touchscreen input to control a cartoon penguin game piece and move across the game environment. Movement trajectories were sampled at the polling rate of the selected input device and data from each trial were uploaded and saved to Google Firebase Realtime Database at the end of each trial. Due to the remote data collection nature of this experiment, we were not able to control data sampling rates. Each trial was manually inspected and sampling rates of input devices had a mean of 37.363 <italic>±</italic> 4.184 Hz.</p>
</sec>
<sec id="s4c">
<title>Procedure</title>
<p>The game environment is an immersive icy landscape with penguins. The overall goal of the task was to move a penguin from the starting position on one side of the ice (at the bottom of the computer or tablet screen closest to the participant) to a distance of 24 game units (GU) into the game (at the far edge of the screen away from the participant). If successful, a pleasant sound would play, the video screen above the ice would be outlined in blue, and a Disney video clip (different on each trial; gifs hosted on <ext-link ext-link-type="uri" xlink:href="https://giphy.com">https://giphy.com</ext-link>) would play. Multiple signals of reward were provided to ensure that participants associated their behavior with the feedback provided and could clearly differentiate between a successful trial and a failure trial. The penguin game piece started as a blue color to indicate that it was not active. To activate the penguin, the participant had to click the left mouse button (mouse or trackpad) or touch and hold the penguin and it would turn white to indicate that it was movable. Then the participant made a reaching movement to move the penguin across the ice. The trial would end when the Y position of the penguin exceeded 24 GU. To account for variability in input device sampling rates, the final X position was computed as an interpolation between the data points immediately before and after Y = 24 GU such that every trial had a calculated X position at Y = 24 GU.</p>
<p>Rewards were determined based upon the interpolated final X position of the game piece at the back edge of the ice and a task specific reward landscape. All tasks included five blocks as outlined in <xref rid="fig2" ref-type="fig">Fig. 2a</xref>. Baseline (20 trials): a single, discrete target (image of three dancing penguins) was presented at a randomized X position at the far edge of the ice. Participants were instructed to move accurately to the target. Learning (100 trials): participants were exposed to one of four learning environments with variations in the target type (continuous vs. discrete, <xref rid="fig1" ref-type="fig">Fig. 1a</xref>) and the reward feedback (probabilistic vs. deterministic, <xref rid="fig1" ref-type="fig">Fig. 1b</xref>). In the continuous conditions, participants could choose to move the penguin to any location on a continuous horizontal target (<xref rid="fig1" ref-type="fig">Fig. 1a</xref> – continuous). In the discrete conditions, participants could move to one of seven targets spread horizontally (<xref rid="fig1" ref-type="fig">Fig. 1a</xref> – discrete). For probabilistic feedback, the reward was determined by an unseen position-based probability gradient with a small 100% reward zone away from which reward probabilities decreased linearly to a baseline (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> – continuous probabilistic and discrete probabilistic). For deterministic feedback, reward was always given within a reward zone but not elsewhere (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> – continuous deterministic and discrete deterministic). Success Clamp (10 trials): every trial in this block was rewarded regardless of the final X position of the game piece. Fail Clamp (10 trials): no trials in this block were rewarded. Single Target (10 trials): the same single, discrete target as in baseline was presented in the center of the far edge of the ice and participants were cued to move accurately towards it. A break for a new set of instructions was provided between Baseline and Learning as well as Fail Clamp and Single Target. Participants were unaware of the reward criteria transitions between Learning and Success Clamp and Success Clamp and Fail Clamp. The ideal/mature behavior in these tasks was to explore at the beginning of the learning block to find the area of most frequent reward and then exploit this behavior to maximize reward for the remaining trials of the block. Moreover, if the 100% reward zone has been found successfully, continuing to move to this reinforced position during the success clamp and then exploring again in the fail clamp are indicators of mature sensitivity to reward and failure.</p>
</sec>
<sec id="s4d">
<title>Continuous Probabilistic</title>
<p>In the Learning block for the continuous probabilistic task, participants were presented with a continuous target and probabilistic reward feedback (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> – continuous probabilistic). The probability reward landscape is defined by setting reward percentage probabilities at 5 specific <italic>x</italic> locations with reward between these locations being linearly interpolated (and constant outside the range). We set X values of {−24, −14.5, −9.5, 1.1875} to reward probabilities {33, 100, 100, 25}.</p>
<p>Participants were warned that some of the ice was starting to melt which would cause the penguin to slip and were told that in some places they would slip a lot of the time, some places they would slip some of the time, and in some places they would never slip. They were instructed to move the penguin across the ice without slipping to get the movie clip to play as often as possible. If they were not successful, the penguin would fall over, and they would see a sad face penguin image before the game reset for the next trial <xref rid="fig1" ref-type="fig">Fig. 1c</xref>. This task design builds from Cashaback et al. where participants were asked to reach to any location on a continuous line, with the probability of being rewarded dependent on the reach end location. In their task, there was a small (hidden) 100% reward zone with reward probability decreasing on either side away from this zone. They found that adult participants could learn from a probabilistic reward landscape and find the rewarding zone [<xref ref-type="bibr" rid="c13">13</xref>]. We explored a similar task design in participants across development.</p>
</sec>
<sec id="s4e">
<title>Discrete Probabilistic</title>
<p>In the Learning block for the discrete probabilistic task, participants were presented with a set of targets, each associated with a specific probability of reward (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> – discrete probabilistic). We set the center of seven the targets at X values of {−18, −12, −6, 0, 6, 12, 18} with target width of 5 and with reward percentage probabilities of the {66, 100, 66, 33, 25, 25, 25} (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> bottom left).</p>
<p>The discrete targets were visually the same as those used in the Baseline and Single Target blocks, however all seven were presented at the same time in equally spaced positions across the edge of the ice. Participants were instructed to find the group of penguins that made the video clip play all the time by moving their penguin game piece across the ice. They were told to move to one group of penguins on each trial and that some penguins would make the movie clip play some of the time but there was a group of penguins that played the clip all the time. If they were not successful, they would see a red X on the video screen and the video clip would not play before the game reset for the next trial. To ensure that participants could accurately distinguish the feedback associated with each target, there was a visible space between each target. If the participant moved between targets, the participant would receive a message to try again, and the trial would reset until one target was accurately hit.</p>
</sec>
<sec id="s4f">
<title>Continuous Deterministic</title>
<p>In the Learning block for the continuous deterministic task, participants were presented with a continuous target and deterministic reward feedback (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> – continuous deterministic). Participants were warned that some of the ice was starting to melt which would cause the penguin to slip and were told that in some places they would slip all of the time and in some places they would never slip. They were instructed to move the penguin across the ice without slipping to get the movie clip to play as often as possible. If they were not successful, the penguin would fall over, and they would see a sad face penguin image before the game reset for the next trial. This task was completed by a subset of participants aged three to eight years and adults.</p>
</sec>
<sec id="s4g">
<title>Discrete Deterministic</title>
<p>In the Learning block for the discrete deterministic task, participants were presented with a set of seven discrete targets and deterministic reward feedback (<xref rid="fig1" ref-type="fig">Fig. 1b</xref> – discrete deterministic). Participants were instructed to move across the ice to one of the groups of penguins on each trial to get the movie clip to play. They were told that one group of penguins would make the video clip play all of the time. If they were not successful, they would see a red X on the video screen and the video clip would not play before the game reset for the next trial. As in the discrete probabilistic task, to ensure that participants could accurately distinguish the feedback associated with each target, there was a space between each target. If the participant moved between targets, the trial would reset until one target was accurately hit. This task was completed by a subset of participants aged three to eight years and adults.</p>
</sec>
<sec id="s4h">
<title>Demo Versions of Tasks</title>
<p>Shortened versions of each task without data collection are provided at the following links. In these versions, there are 5 trials in baseline, 10 trials in learning, 2 trials in each clamp, and 3 trials of single target. To proceed beyond the information input screen, use an arbitrary 6-digit code for the subjectID and participant information to sample the game environment.</p>
<p>Continuous Probabilistic: <ext-link ext-link-type="uri" xlink:href="https://kidmotorlearning.github.io/PenguinsDemo_Continuous-Probabilistic/">https://kidmotorlearning.github.io/PenguinsDemo_Continuous-Probabilistic/</ext-link></p>
<p>Discrete Probabilistic: <ext-link ext-link-type="uri" xlink:href="https://kidmotorlearning.github.io/PenguinsDemo_Discrete-Probabilistic/">https://kidmotorlearning.github.io/PenguinsDemo_Discrete-Probabilistic/</ext-link></p>
<p>Continuous Deterministic: <ext-link ext-link-type="uri" xlink:href="https://kidmotorlearning.github.io/PenguinsDemo_Continuous-Deterministic/">https://kidmotorlearning.github.io/PenguinsDemo_Continuous-Deterministic/</ext-link></p>
<p>Discrete Deterministic: <ext-link ext-link-type="uri" xlink:href="https://kidmotorlearning.github.io/PenguinsDemo_Discrete-Deterministic/">https://kidmotorlearning.github.io/PenguinsDemo_Discrete-Deterministic/</ext-link></p>
</sec>
<sec id="s4i">
<title>Measurements and Analysis</title>
<p>We used several metrics to analyze performance and variability in different blocks and evaluate reinforcement learning over childhood. Baseline performance was defined as the distance from the target center averaged over each of the 20 baseline trials. Baseline variability was defined as the standard deviation of the baseline performance. Learning performance (<italic>Distance</italic>) is defined as the absolute value of the interpolated X position distance from the center of 100% reward zone (located at X = -12 in all tasks) averaged over the last 10 trials of the learning block. A <italic>Distance</italic> value closer to 0 indicates better learning. To quantify variability after success, we used the standard deviation of the interpolated X position in trials 2 through 10 of the success clamp block. To quantify variability after failure, we used the standard deviation of the interpolated X position in trials 2 through 10 of the fail clamp block.</p>
<p>Participant characteristics (sex and game play handedness) and game play parameters (device and browser) were analyzed in one way ANOVAs with dependent variable of <italic>Distance</italic> to determine whether these parameters influenced learning ability. To determine whether there was a differential effect of sex, device, or handedness on learning performance in the four different tasks, additional two way ANOVAs with dependent variable of <italic>Distance</italic> were used. There was not a significant interaction between task and sex (reported in Results), task and device (<italic>F</italic><sub>3</sub>,<sub>168</sub> = 0.62, p = 0.717), or task and handedness (<italic>F</italic><sub>3</sub>,<sub>168</sub> = 1.2, p = 0.312). Each trial was divided into three phases to describe the reaction, stationary, and movement times. Reaction time is defined as the time from the appearance of the penguin (start of the trial) to the time the penguin is clicked and activated. Stationary time is defined as the time from penguin click to movement onset (first non-zero position). Movement time is defined as movement onset to end of the trial when the penguin moves across the back edge of the ice. Trial timings for each phase of movement were extracted and averaged for each participant across the whole experiment and then averaged within individual age bins to evaluate across ages. Total game play time was also extracted and averaged by age bins. Path length ratios were calculated as the actual path length from the start to end of the trajectory divided by the ideal straight-line path from the first position in the trajectory to the last position in the trajectory. The path length ratio for all trials were averaged for each participant and then averaged within age bins for comparison between ages.</p>
<p>We used linear regression, mediation analysis, and one and two-way ANOVAs to evaluate effects of age and other outcome variables on learning as well as compare performance between tasks. Significance level of 0.05 was used for all statistical tests. All raw data and statistical analyses were completed using custom scripts in MATLAB (version R2023a). We regressed Age with each performance or variability metric independently as well as in a multiple linear regression model. To compare performance by age bin for the children (5 bins of 3 years each spanning age 3 to 17 years) between the baseline block of discrete targets and the final block of discrete targets, we used a repeated measures ANOVA with a grouping variable of age bin. We used a mediation analysis model with Age as the independent variable (X) and <italic>Distance</italic> as the dependent variable (Y). For the continuous probabilistic task, the mediators (M1 and M2) were Baseline Variability and Variability after Failure. For the discrete probabilistic task, the mediators (M1 and M2) were Variability after Success and Variability after Failure. For each model, the five path coefficients were calculated describing the relationship of X to M (path <italic>a</italic>), M to Y (path <italic>b</italic>), X to Y (path <italic>c</italic>), X to Y mediated by M (path <italic>c</italic><sup><italic>′</italic></sup>) and the mediation effect (path <italic>ab</italic>). Mediation analyses were completed using a MATLAB Mediation Toolbox (<ext-link ext-link-type="uri" xlink:href="https://github.com/canlab/MediationToolbox">https://github.com/canlab/MediationToolbox</ext-link>; original access date June 1, 2023).</p>
<p>To investigate the influence of a previous failure on selection of the next movement, we used a regression analysis. <xref ref-type="disp-formula" rid="eqn1">Equation 1</xref> describes the relationship between change in position from the immediately previous trial weighted by experiencing a failure on up to three recent trials.
<disp-formula id="eqn1">
<graphic xlink:href="602665v1_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where:</p>
<p>Δ<italic>x</italic><sub><italic>t</italic></sub> = <italic>x</italic><sub><italic>t</italic></sub> − <italic>x</italic><sub><italic>t</italic>−1</sub>; change in position from the previous trial</p>
<p><italic>w</italic><sub>0</sub> = inherent exploration if no failures on previous 3 trials</p>
<p><italic>w</italic><sub><italic>i</italic></sub> = weight of failure on the t-i<sup><italic>th</italic></sup> trial</p>
<p><italic>F</italic><sub><italic>t</italic>−<italic>i</italic></sub> = failure status of the t-i<sup><italic>th</italic></sup> trial; 1 = fail, 0 = success</p>
</sec>
<sec id="s4j">
<title>Reinforcement learning model</title>
<p>To account for the reaching behavior, we developed a probabilistic model that incorporates both exploration variability and motor noise. The data consists of the observed reach locations, <italic>s</italic><sub><italic>t</italic></sub>, on trial <italic>t</italic> and the reward <italic>r</italic><sub><italic>t</italic></sub> (1 or 0 depending on whether the reach was rewarded or not). Our model builds from Therrien et al. who examined how participants used binary deterministic reinforcement of reaching movements to find a rewarded endpoint location that moved over time. Using a mechanistic model, they showed that the ability to learn depended upon appropriate exploration variability relative to their motor noise (i.e., variability that is not under the control of the subject) [<xref ref-type="bibr" rid="c14">14</xref>].</p>
<p>We assume that on each trial the participant has an internal estimate of the desired reach location, <italic>x</italic><sub><italic>t</italic></sub>, which they can update as the experiment proceeds. On each trial we include variability in a participant’s produced reach location from two potential sources – motor noise (<italic>m</italic><sub><italic>t</italic></sub>) and exploration variability (<italic>e</italic><sub><italic>t</italic></sub>). The key difference between these two sources of variability is that we assume participants can use the exploration variability, but not their motor noise, to update their desired reach location. On each trial, each source of variability is modeled as a draw from a zero-mean Gaussian distribution. For motor noise the standard deviation of the Gaussian is <italic>σ</italic><sub><italic>m</italic></sub> which is constant across trials. We assume that subjects only explore after a unrewarded trial and that the exploration variability is a draw from a zero-mean Gaussian distribution with standard deviation <italic>σ</italic><sub><italic>e</italic></sub>.
<disp-formula id="eqn2">
<graphic xlink:href="602665v1_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The produced reach, <italic>s</italic><sub><italic>t</italic></sub>, is then the desired reach with the addition of the variability from motor noise and exploration (if any), given by the <italic>output</italic> equation:
<disp-formula id="eqn3">
<graphic xlink:href="602665v1_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The probability <italic>p</italic> of reward received, <italic>r</italic><sub><italic>t</italic></sub>, depends on the the actual reach location <italic>s</italic><sub><italic>t</italic></sub> and the particular reward regime used such that the <italic>reward</italic> equation is
<disp-formula id="eqn4">
<graphic xlink:href="602665v1_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>f</italic> () can represent different functions such as the continuous or discrete probabilistic reward regimes.</p>
<p>After each reach, the participant updates their desired reach location only if they were successful. They update the reach to incorporate part of any exploration so that the <italic>update</italic> equation is
<disp-formula id="eqn5">
<graphic xlink:href="602665v1_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>η</italic> is a learning rate between 0 and 1.</p>
<sec id="s4j1">
<title>Model fitting</title>
<p>To fit this stochastic model to each participant’s data, we used a Kalman filter. The model has 3 parameters: <italic>θ</italic> = {<italic>σ</italic><sub><italic>m</italic></sub>, <italic>σ</italic><sub><italic>e</italic></sub>, <italic>η</italic>}. We can reformulate the equations above into a standard Kalman filter. The update equation is given by
<disp-formula id="eqn6">
<graphic xlink:href="602665v1_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>v</italic><sub><italic>t</italic></sub> ∼ <italic>𝒩</italic> (0, 1). The output equation is given by
<disp-formula id="eqn7">
<graphic xlink:href="602665v1_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>w</italic><sub><italic>t</italic></sub> ∼ <italic>𝒩</italic> (0, 1). We initialized the desired state as the median of the first five reach locations and the state covariance as
<disp-formula id="eqn8">
<graphic xlink:href="602665v1_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We used Matlab fminsearchbnd to fit the parameters to maximize the likelihood over the 100 learning trials for each participant. This ensured that our parameter fits were not contaminated by behavior in the clamp block and also allowed us to try and predict the out-of-sample clamp behavior. In addition, we also examined parameter recovery by generating simulated data (for each participant using their best fit parameters) and comparing the fit values with the true values used to simulate the data. We also fit data from the discrete probabilistic task in the same way as the continuous (so that the model does not know about the discrete nature of the task).</p>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="s5">
<title>Data Availability</title>
<p>Data are available upon reasonable request to the corresponding author.</p>
</sec>
<ack>
<title>Acknowledgments</title>
<p>This work was supported by the following funding sources: grants from the National Institutes of Health T32 HD007414 and R35 NS122266 to AJB. DMW is a consultant to CTRL-Labs Inc., in the Reality Labs Division of Meta. This entity did not support or influence this work.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Milne</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Shepard</surname>, <given-names>E.</given-names></string-name></person-group> <source>The House at Pooh Corner</source> <ext-link ext-link-type="uri" xlink:href="https://books.google.com/books?id=1R3hAAAAMAAJ">https://books.google.com/books?id=1R3hAAAAMAAJ</ext-link> (<publisher-name>E.P. Dutton &amp; Company</publisher-name>, <year>1928</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sutton</surname>, <given-names>R. S.</given-names></string-name> &amp; <string-name><surname>Barto</surname>, <given-names>A. G.</given-names></string-name></person-group> <source>Reinforcement learning: an introduction</source> <edition>Second</edition>. <volume>526</volume> pp. ISBN: <isbn>978-0-262-03924-6</isbn> (<publisher-name>The MIT Press</publisher-name>, <publisher-loc>Cambridge, Massachusetts</publisher-loc>, <year>2018</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rovee</surname>, <given-names>C. K.</given-names></string-name> &amp; <string-name><surname>Rovee</surname>, <given-names>D. T.</given-names></string-name></person-group> <article-title>Conjugate reinforcement of infant exploratory behavior</article-title>. <source>Journal of Experimental Child Psychology</source> <volume>8</volume>, <fpage>33</fpage>–<lpage>39</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/0022096569900253">https://www.sciencedirect.com/science/article/pii/0022096569900253</ext-link> (<year>1969</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sargent</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Schweighofer</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Kubo</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Fetters</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>Infant Exploratory Learning: Influence on Leg Joint Coordination</article-title>. <source>PLoS ONE</source> <volume>9</volume> <fpage>e91500</fpage>. <ext-link ext-link-type="uri" xlink:href="https://dx.plos.org/10.1371/journal.pone.0091500">https://dx.plos.org/10.1371/journal.pone.0091500</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stevenson</surname>, <given-names>H. W.</given-names></string-name> &amp; <string-name><surname>Weir</surname>, <given-names>M. W.</given-names></string-name></person-group> <article-title>Variables affecting children’s performance in a probability learning task</article-title>. <source>Journal of experimental psychology</source> <volume>57</volume>, <fpage>403</fpage>–<lpage>412</lpage>. (<year>1959</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Minto de Sousa</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Gil</surname>, <given-names>M. S. C. d. A.</given-names></string-name> &amp; <string-name><surname>McIlvane</surname>, <given-names>W. J.</given-names></string-name></person-group> <article-title>Discrimination and Reversal Learning by Toddlers Aged 15-23 Months</article-title>. <source>The Psychological Record</source> <volume>65</volume>, <fpage>41</fpage>–<lpage>47</lpage>. <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.1007/s40732-014-0084-1">http://link.springer.com/10.1007/s40732-014-0084-1</ext-link> (<year>2022</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xia</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Modeling changes in probabilistic reinforcement learning during adolescence</article-title>. <source>PLOS Computational Biology</source> <volume>17</volume>, <fpage>e1008524</fpage>. <ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008524">https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008524</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname>, <given-names>A. O.</given-names></string-name>, <string-name><surname>Nussenbaum</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Dorfman</surname>, <given-names>H. M.</given-names></string-name>, <string-name><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name> &amp; <string-name><surname>Hartley</surname>, <given-names>C. A.</given-names></string-name></person-group> <article-title>The rational use of causal inference to guide reinforcement learning strengthens with age</article-title>. <source>Science of Learning</source> <volume>5</volume>, <fpage>16</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41539-020-00075-3">https://www.nature.com/articles/s41539-020-00075-3</ext-link> (<year>2023</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Decker</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Otto</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Daw</surname>, <given-names>N. D.</given-names></string-name> &amp; <string-name><surname>Hartley</surname>, <given-names>C. A.</given-names></string-name></person-group> <article-title>From Creatures of Habit to Goal-Directed Learners: Tracking the Developmental Emergence of Model-Based Reinforcement Learning</article-title>. <source>Psychological Science</source> <volume>27</volume>, <fpage>848</fpage>–<lpage>858</lpage>. <ext-link ext-link-type="uri" xlink:href="http://journals.sagepub.com/doi/10.1177/0956797616639301">http://journals.sagepub.com/doi/10.1177/0956797616639301</ext-link> (<year>2022</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mayor-Dubois</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Zesiger</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Van der Linden</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Roulet-Perez</surname>, <given-names>E.</given-names></string-name></person-group> <article-title>Procedural learning: A developmental study of motor sequence learning and probabilistic classification learning in school-aged children</article-title>. <source>Child Neuropsychology</source> <volume>22</volume>, <fpage>718</fpage>–<lpage>734</lpage>. ISSN: <issn>0929-7049</issn>. <pub-id pub-id-type="doi">10.1080/09297049.2015.1058347</pub-id> (<year>2016</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Master</surname>, <given-names>S. L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Disentangling the systems contributing to changes in learning during adolescence</article-title>. <source>Developmental Cognitive Neuroscience</source> <volume>41</volume>, <fpage>100732</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1878929319303196">https://www.sciencedirect.com/science/article/pii/S1878929319303196</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDougle</surname>, <given-names>S. D.</given-names></string-name>, <string-name><surname>Ivry</surname>, <given-names>R. B.</given-names></string-name> &amp; <string-name><surname>Taylor</surname>, <given-names>J. A.</given-names></string-name></person-group> <article-title>Taking Aim at the Cognitive Side of Learning in Sensorimotor Adaptation Tasks</article-title>. <source>Trends in Cognitive Sciences</source> <volume>20</volume>, <fpage>535</fpage>–<lpage>544</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1364661316300419">https://www.sciencedirect.com/science/article/pii/S1364661316300419</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cashaback</surname>, <given-names>J. G. A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The gradient of the reinforcement landscape influences sensorimotor learning</article-title>. <source>PLOS Computational Biology</source> <volume>15</volume> <fpage>e1006839</fpage>. <ext-link ext-link-type="uri" xlink:href="https://dx.plos.org/10.1371/journal.pcbi.1006839">https://dx.plos.org/10.1371/journal.pcbi.1006839</ext-link> (<year>2023</year>) (<month>Mar</month>. 4, 2019).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Therrien</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name> &amp; <string-name><surname>Bastian</surname>, <given-names>A. J.</given-names></string-name></person-group> <article-title>Effective reinforcement learning following cerebellar damage requires a balance between exploration and motor noise</article-title>. <source>Brain</source> <volume>139</volume>, <fpage>101</fpage>–<lpage>114</lpage>. <ext-link ext-link-type="uri" xlink:href="https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/awv329">https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/awv329</ext-link> (<year>2022</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Takahashi</surname>, <given-names>C. D.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Neuromotor Noise Limits Motor Performance, But Not Motor Adaptation, in Children</article-title>. <source>Journal of Neurophysiology</source> <volume>90</volume>. <pub-id pub-id-type="doi">10.1152/jn.01173.2002</pub-id>, <fpage>703</fpage>–<lpage>711</lpage>. (<year>2003</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deutsch</surname>, <given-names>K. M.</given-names></string-name> &amp; <string-name><surname>Newell</surname>, <given-names>K. M.</given-names></string-name></person-group> <article-title>Noise, variability, and the development of children’s perceptual-motor skills</article-title>. <source>Developmental Review</source> <volume>25</volume>, <fpage>155</fpage>–<lpage>180</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0273229704000498">https://www.sciencedirect.com/science/article/pii/S0273229704000498</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Asselen</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Brain areas involved in spatial working memory</article-title>. <source>Neuropsychologia</source> <volume>44</volume>, <fpage>1185</fpage>–<lpage>1194</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0028393205003180">https://www.sciencedirect.com/science/article/pii/S0028393205003180</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hitch</surname>, <given-names>G. J.</given-names></string-name>, <string-name><surname>Halliday</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Schaafstal</surname>, <given-names>A. M.</given-names></string-name> &amp; <string-name><surname>Schraagen</surname>, <given-names>J. M. C.</given-names></string-name></person-group> <article-title>Visual working memory in young children</article-title>. <source>Memory &amp; Cognition</source> <volume>16</volume>, <fpage>120</fpage>–<lpage>132</lpage>. <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.3758/BF03213479">http://link.springer.com/10.3758/BF03213479</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Pickering</surname>, <given-names>S. J.</given-names></string-name></person-group> <chapter-title>The development of visuo-spatial working memory</chapter-title>. <source>Memory</source>. <volume>9</volume>: <pub-id pub-id-type="doi">10.1080/09658210143000182</pub-id>, <fpage>423</fpage>–<lpage>432</lpage>.(<year>2024</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsujii</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Yamamoto</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Masuda</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Watanabe</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Longitudinal study of spatial working memory development in young children</article-title>. <source>NeuroReport</source> <volume>20</volume>, <fpage>759</fpage>–<lpage>763</lpage>. <ext-link ext-link-type="uri" xlink:href="https://journals.lww.com/00001756-200905270-00005">https://journals.lww.com/00001756-200905270-00005</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Laerhoven</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>van der Zaag-Loonen</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Derkx</surname>, <given-names>B.</given-names></string-name></person-group> <article-title>A comparison of Likert scale and visual analogue scales as response options in children’s questionnaires</article-title>. <source>Acta Paediatrica</source> <volume>93</volume>, <fpage>830</fpage>–<lpage>835</lpage>. <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1651-2227.2004.tb03026.x">https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1651-2227.2004.tb03026.x</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shields</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Palermo</surname>, <given-names>T. M.</given-names></string-name>, <string-name><surname>Powers</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Grewe</surname>, <given-names>S. D.</given-names></string-name> &amp; <string-name><surname>Smith</surname>, <given-names>G. A.</given-names></string-name></person-group> <article-title>Predictors of a child’s ability to use a visual analogue scale</article-title>. <source>Child: Care, Health and Development</source> <volume>29</volume>, <fpage>281</fpage>–<lpage>290</lpage>. <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/abs/10.1046/j.1365-2214.2003.00343.x">https://onlinelibrary.wiley.com/doi/abs/10.1046/j.1365-2214.2003.00343.x</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Plate</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Fulvio</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Shutts</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Green</surname>, <given-names>C. S.</given-names></string-name> &amp; <string-name><surname>Pollak</surname>, <given-names>S. D.</given-names></string-name></person-group> <article-title>Probability Learning: Changes in Behavior Across Time and Development</article-title>. <source>Child Development</source> <volume>89</volume>, <fpage>205</fpage>–<lpage>218</lpage>. (<year>2018</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schulz</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Ruggeri</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Meder</surname>, <given-names>B.</given-names></string-name></person-group> <article-title>Searching for Rewards Like a Child Means Less Generalization and More Directed Exploration</article-title>. <source>Psychological Science</source> <volume>30</volume>, <fpage>1561</fpage>–<lpage>1572</lpage>. <pub-id pub-id-type="doi">10.1177/0956797619863663</pub-id> (<year>2024</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liquin</surname>, <given-names>E. G.</given-names></string-name> &amp; <string-name><surname>Gopnik</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Children are more exploratory and learn more than adults in an approach-avoid task</article-title>. <source>Cognition</source> <volume>218</volume>, <fpage>104940</fpage>. (<year>2022</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nussenbaum</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Hartley</surname>, <given-names>C. A.</given-names></string-name></person-group> <article-title>Reinforcement learning across development: What insights can we draw from a decade of research?</article-title> <source>Developmental Cognitive Neuroscience</source> <volume>40</volume>, <fpage>100733</fpage>. (<year>2019</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sobel</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Sommerville</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>The Importance of Discovery in Children’s Causal Learning from Interventions</article-title>. <source>Frontiers in Psychology</source> <volume>1</volume>. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2010.00176/full">https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2010.00176/full</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raznahan</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Longitudinal four-dimensional mapping of subcortical anatomy in human development</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>111</volume>, <fpage>1592</fpage>–<lpage>1597</lpage>. <ext-link ext-link-type="uri" xlink:href="https://pnas.org/doi/full/10.1073/pnas.1316911111">https://pnas.org/doi/full/10.1073/pnas.1316911111</ext-link> (<year>2022</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nelson</surname>, <given-names>C. A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Functional neuroanatomy of spatial working memory in children</article-title>. <source>Developmental Psychology</source> <volume>36</volume>, <fpage>109</fpage>–<lpage>116</lpage>. (<year>2000</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Schultz</surname>, <given-names>W.</given-names></string-name></person-group> <chapter-title>Predictive Reward Signal of Dopamine Neurons</chapter-title>. <source>Journal of Neurophysiology</source> <volume>80</volume> <fpage>1</fpage>–<lpage>27</lpage>. <ext-link ext-link-type="uri" xlink:href="https://journals.physiology.org/doi/full/10.1152/jn.1998.80.1.1">https://journals.physiology.org/doi/full/10.1152/jn.1998.80.1.1</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mai</surname>, <given-names>X.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Brain Activity Elicited by Positive and Negative Feedback in Preschool-Aged Children</article-title>. <source>PLoS ONE</source> <volume>6</volume> <fpage>e18774</fpage>. <ext-link ext-link-type="uri" xlink:href="https://dx.plos.org/10.1371/journal.pone.0018774">https://dx.plos.org/10.1371/journal.pone.0018774</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eppinger</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Mock</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Kray</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Developmental differences in learning and error processing: evidence from ERPs</article-title>. <source>Psychophysiology</source> <volume>46</volume>, <fpage>1043</fpage>–<lpage>1053</lpage>. ISSN: <issn>1540-5958</issn> (<year>2009</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Meel</surname>, <given-names>C. S.</given-names></string-name>, <string-name><surname>Oosterlaan</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Heslenfeld</surname>, <given-names>D. J.</given-names></string-name> &amp; <string-name><surname>Sergeant</surname>, <given-names>J. A.</given-names></string-name></person-group> <article-title>Telling good from bad news: ADHD differentially affects processing of positive and negative feedback during guessing</article-title>. <source>Neuropsychologia</source> <volume>43</volume>, <fpage>1946</fpage>–<lpage>1954</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0028393205001260">https://www.sciencedirect.com/science/article/pii/S0028393205001260</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sheskin</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <chapter-title>Online Developmental Science to Foster Innovation, Access, and Impact</chapter-title>. <source>Trends in Cognitive Sciences 24</source>. Publisher: <publisher-name>Elsevier</publisher-name>, <fpage>675</fpage>–<lpage>678</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2020.06.004</pub-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scott</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Schulz</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>Lookit (Part 1): A New Online Platform for Developmental Research</article-title>. <source>Open Mind</source> <volume>1</volume>, <fpage>4</fpage>–<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1162/OPMI_a_00002</pub-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Malone</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Hill</surname>, <given-names>N. M.</given-names></string-name>, <string-name><surname>Tripp</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name> &amp; <string-name><surname>Bastian</surname>, <given-names>A. J.</given-names></string-name></person-group> <article-title>A novel video game for remote studies of motor adaptation in children</article-title>. <source>Physiological Reports</source> <volume>11</volume>, <fpage>e15764</fpage>. <ext-link ext-link-type="uri" xlink:href="https://physoc.onlinelibrary.wiley.com/doi/10.14814/phy2.15764">https://physoc.onlinelibrary.wiley.com/doi/10.14814/phy2.15764</ext-link> (<year>2023</year>).</mixed-citation></ref>
</ref-list>
<sec id="s6">
<title>Supplementary Information</title>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Supp. Tab. 1.</label>
<caption><title>Mediation analysis for continuous probabilistic task.</title>
<p>Results of the effect of age on learning mediated by baseline variability and variability after failure. Baseline variability and variability after failure together partially mediate the effect of age on learning. Significant effects are in bold. <italic>β</italic>: regression coefficient, SE: standard error</p></caption>
<graphic xlink:href="602665v1_tblS1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>Supp. Tab. 2.</label>
<caption><title>Mediation analysis for discrete probabilistic task.</title>
<p>Results of the effect of age on learning mediated by variability after success and after failure. Together, variability after success and after failure completely mediate the effect of age on learning. Significant effects are in bold. <italic>β</italic>: regression coefficient, SE: standard error</p></caption>
<graphic xlink:href="602665v1_tblS2.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s7">
<title>Supplementary figures</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Supp. Fig. 1.</label>
<caption><title>Map of participant locations.</title>
<p>Thirty-eight states of the United States of America are represented in this dataset. The map was generated in Excel on Microsoft 365.</p></caption>
<graphic xlink:href="602665v1_figS1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Supp. Fig. 2.</label>
<caption><title>Example baseline paths for participants ages 3 to 11 years old.</title>
<p>Each trajectory begins at (0,0) and ends at Y = 24 when the penguin crosses the back edge of the ice. The final X position of each trajectory corresponds to the interpolated final position of the movement (see Methods for additional details). As available, a sample for each age bin from each input device type is provided. Note that trajectories tend to be straighter for touchscreen input compared to other devices. The twenty squares represent the target centers. Note that the full reward zone is not shown due to overlap between targets. Unrewarded and reward paths are shown as dashed and solid lines, respectively.</p></caption>
<graphic xlink:href="602665v1_figS2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Supp. Fig. 3.</label>
<caption><title>Example baseline paths for participants for participants ages 12 to 17 years old and adults.</title>
<p>Same format as <xref rid="figS2" ref-type="fig">Supp. Fig. 2</xref>.</p></caption>
<graphic xlink:href="602665v1_figS3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Supp. Fig. 4.</label>
<caption><title>Path length ratios.</title>
<p>The path length ratio is a measure of path curvature (path length divided by distance from first to last point of movement) for the four tasks. Significant pairwise comparisons between age bins indicated above plots as follow: * = p &lt; 0.05, + = p &lt; 0.01, and Δ = p &lt; 0.001. Bars show mean and standard error of the mean.</p></caption>
<graphic xlink:href="602665v1_figS4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Supp. Fig. 5.</label>
<caption><title>Timing information.</title>
<p>Reaction time (time from when penguin appeared until the participant clicked on the penguin to start the trial), stationary time (time from click to start of movement), movement time (time from start to end of movement) and game time (time to complete the whole task in minutes) for the 4 tasks split by age bins. Significant pairwise comparisons between age bins indicated above plots as follow: * p &lt; 0.05, + p &lt; 0.01, Δ p &lt; 0.001. Bars show mean and standard error of the mean.</p></caption>
<graphic xlink:href="602665v1_figS5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Supp. Fig. 6.</label>
<caption><title>History dependence of change in reach as a function of reward history for the continuous probabilistic task.</title>
<p>We performed a regression analysis on the change in absolute reach direction (Δ<italic>X</italic><sub><italic>t</italic></sub> = <italic>X</italic><sub><italic>t</italic></sub> − <italic>X</italic><sub><italic>t</italic>−1</sub>) as a function of whether the last three trials were successes or failures. That is, we fit Δ<italic>X</italic><sub><italic>t</italic></sub> = <italic>w</italic><sub>0</sub> +<italic>w</italic><sub>1</sub><italic>f</italic><sub><italic>t</italic>−1</sub> +<italic>w</italic><sub>2</sub><italic>f</italic><sub><italic>t</italic>−2</sub> +<italic>w</italic><sub>3</sub><italic>f</italic><sub><italic>t</italic>−3</sub>, where <italic>f</italic><sub><italic>t</italic></sub> is 1 if trial <italic>t</italic> was a failure and 0 for success. Note that <italic>w</italic><sub>0</sub> reflects the change in reach when there were no failures in the previous three trials. This decreased with age and may represent decreasing sensorimotor noise (<italic>w</italic><sub>0</sub>: 100 out of 111 participants significant at p=0.05; correlation with age <italic>R</italic><sup>2</sup> = 0.122, F = 15.2, p &lt; 0.001). <italic>w</italic><sub>1</sub>, <italic>w</italic><sub>2</sub> and <italic>w</italic><sub>3</sub> reflect the contribution of failing on the previous trial, two and three trials ago, respectively to the change in reach. The change in reach after one failure increased with age (<italic>w</italic><sub>1</sub>: n = 66 out of 111 participants significant; correlation with age <italic>R</italic><sup>2</sup> = 0.139, F = 17.6, p &lt; 0.001). The effect of failure for two and three trials back were mostly not significant (<italic>w</italic><sub>2</sub>: 20 out of 111 participants significant; correlation with age <italic>R</italic><sup>2</sup> = 0.0166, F = 1.83, p = 0.178; <italic>w</italic><sub>3</sub>: 17 out of 111 participants significant; correlation with age <italic>R</italic><sup>2</sup> = 0.119, F = 14.7, p &lt; 0.001). For the adults, the average of all of the data points is plotted as a horizontal line.</p></caption>
<graphic xlink:href="602665v1_figS6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS7" position="float" fig-type="figure">
<label>Supp. Fig. 7.</label>
<caption><title>Model parameter recovery.</title>
<p>The recovered vs. true parameters for synthetic data generated by the model (for 100 learning trials) and then fit. Correlations are shown above the plots.</p></caption>
<graphic xlink:href="602665v1_figS7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS8" position="float" fig-type="figure">
<label>Supp. Fig. 8.</label>
<caption><title>Comparison of significant models.</title>
<p>Some participants in each age bin were best fit with the noise only model (left) compared to the full model (right). When removing the participants who were best fit with the noise only model, the same age related trends in learning remained as depicted in <xref rid="fig3" ref-type="fig">Fig. 3</xref>. Children age 3 to 8 years old show poor learning compared to older participants.</p></caption>
<graphic xlink:href="602665v1_figS8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS9" position="float" fig-type="figure">
<label>Supp. Fig. 9.</label>
<caption><title>Example behavior, and discrete target performance in the discrete probabilistic task.</title>
<p>Same format as <xref rid="fig2" ref-type="fig">Fig. 2</xref>.</p></caption>
<graphic xlink:href="602665v1_figS9.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS10" position="float" fig-type="figure">
<label>Supp. Fig. 10.</label>
<caption><title>Discrete probabilistic task learning block time series.</title>
<p>Same format as <xref rid="fig3" ref-type="fig">Fig. 3</xref>.</p></caption>
<graphic xlink:href="602665v1_figS10.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS11" position="float" fig-type="figure">
<label>Supp. Fig. 11.</label>
<caption><title>Variability and learning in the discrete probabilistic task.</title>
<p>Same format as <xref rid="fig4" ref-type="fig">Fig. 4</xref>.</p></caption>
<graphic xlink:href="602665v1_figS11.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS12" position="float" fig-type="figure">
<label>Supp. Fig. 12.</label>
<caption><title>Reinforcement learning model for the discrete probabilistic task.</title>
<p>Panels in same format as <xref rid="fig5" ref-type="fig">Fig. 5b</xref>.</p></caption>
<graphic xlink:href="602665v1_figS12.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS13" position="float" fig-type="figure">
<label>Supp. Fig. 13.</label>
<caption><title>Adult and 3 to 8-year-old children performance on all four tasks.</title>
<p>Panels in same format as <xref rid="fig6" ref-type="fig">Fig. 6a</xref>.</p></caption>
<graphic xlink:href="602665v1_figS13.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101036.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Dekker</surname>
<given-names>Tessa</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This work by Hill and colleagues offers <bold>valuable</bold> insights into the development of learning abilities involved in action control from toddlerhood to adulthood. Data across 4 experiments provide <bold>solid</bold> evidence that in a task involving noisy but continuous action, the ability to learn reward probability develops gradually and may be limited by spatial processing and probabilistic reward reasoning. Questions remain about whether the task truly measures motor learning or more generic cognitive capacities, and whether a proposed model of reinforcement-based motor learning adequately captures the data.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101036.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Here the authors address how reinforcement-based sensorimotor adaptation changes throughout development. To address this question, they collected many participants in ages that ranged from small children (3 years old) to adulthood (18+ years old). The authors used four experiments to manipulate whether binary and positive reinforcement was provided probabilistically (e.g., 30 or 50%) versus deterministically (e.g.,100%), and continuous (infinite possible locations) versus discrete (binned possible locations) when the probability of reinforcement varied along the span of a large redundant target. The authors found that both movement variability and the extent of adaptation changed with age.</p>
<p>Strengths:</p>
<p>The major strength of the paper is the number of participants collected (n = 385). The authors also answer their primary question, that reinforcement-based sensorimotor adaptation changes throughout development, which was shown by utilizing established experimental designs and computational modelling.</p>
<p>Weaknesses:</p>
<p>Potential concerns involve inconsistent findings with secondary analyses, current assumptions that impact both interpretation and computational modelling, and a lack of clearly stated hypotheses.</p>
<p>(1) Multiple regression and Mediation Analyses.</p>
<p>The challenge with these secondary analyses is that:</p>
<p>
(a) The results are inconsistent between Experiments 1 and 2, and the analysis was not performed for Experiments 3 and 4,</p>
<p>
(b) The authors used a two-stage procedure of using multiple regression to determine what variables to use for the mediation analysis, and</p>
<p>
(c) The authors already have a trial-by-trial model that is arguably more insightful.</p>
<p>Given this, some suggested changes are to:</p>
<p>
(a) Perform the mediation analysis with all the possible variables (i.e., not informed by multiple regression) to see if the results are consistent.</p>
<p>
(b) Move the regression/mediation analysis to Supplementary, since it is slightly distracting given current inconsistencies and that the trial-by-trial model is arguably more insightful.</p>
<p>(2) Variability for different phases and model assumptions:</p>
<p>A nice feature of the experimental design is the use of success and failure clamps. These clamped phases, along with baseline, are useful because they can provide insights into the partitioning of motor and exploratory noise. Based on the assumptions of the model, the success clamp would only reflect variability due to motor noise (excludes variability due to exploratory noise and any variability due to updates in reach aim). Thus, it is reasonable to expect that the success clamps would have lower variability than the failure clamps (which it obviously does in Figure 6), and presumably baseline (which provides success and failure feedback, thus would contain motor noise and likely some exploratory noise).</p>
<p>However, in Figure 6, one visually observes greater variability during the success clamp (where it is assumed variability only comes from motor noise) compared to baseline (where variability would come from:</p>
<p>
(a) Motor noise.</p>
<p>
(b) Likely some exploratory noise since there were some failures.</p>
<p>
(c) Updates in reach aim.</p>
<p>Given the comment above, can the authors please:</p>
<p>
(a) Statistically compare movement variability between the baseline, success clamp, and failure clamp phases.</p>
<p>
(b) The authors have examined how their model predicts variability during success clamps and failure clamps, but can they also please show predictions for baseline (similar to that of Cashaback et al., 2019; Supplementary B, which alternatively used a no feedback baseline)?</p>
<p>
(c) Can the authors show whether participants updated their aim towards their last successful reach during the success clamp? This would be a particularly insightful analysis of model assumptions.</p>
<p>
(d) Different sources of movement variability have been proposed in the literature, as have different related models. One possibility is that the nervous system has knowledge of 'planned (noise)' movement variability that is always present, irrespective of success (van Beers, R. J. (2009). Motor learning is optimally tuned to the properties of motor noise. Neuron, 63(3), 406-417). The authors have used slightly different variations of their model in the past. Roth et al (2023) directly compared several different plausible models with various combinations of motor, planned, and exploratory noise (Roth A, 2023, &quot;Reinforcement-based processes actively regulate motor exploration along redundant solution manifolds.&quot; Proceedings of the Royal Society B 290: 20231475: see Supplemental). Their best-fit model seems similar to the one the authors propose here, but the current paper has the added benefit of the success and failure clamps to tease the different potential models apart. In light of the results of a), b), and c), the authors are encouraged to provide a paragraph on how their model relates to the various sources of movement variability and other models proposed in the literature.</p>
<p>
(e) line 155. Why would the success clamp be composed of both motor and exploratory noise? Please clarify in the text</p>
<p>(3) Hypotheses:</p>
<p>The introduction did not have any hypotheses of development and reinforcement, despite the discussion above setting up potential hypotheses. Did the authors have any hypotheses related to why they might expect age to change motor noise, exploratory noise, and learning rates? If so, what would the experimental behaviour look like to confirm these hypotheses? Currently, the manuscript reads more as an exploratory study, which is certainly fine if true, it should just be explicitly stated in the introduction. Note: on line 144, this is a prediction, not a hypothesis. Line 225: this idea could be sharpened. I believe the authors are speaking to the idea of having more explicit knowledge of action-target pairings changing behaviour.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101036.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this study, Hill and colleagues use a novel reinforcement-based motor learning task (&quot;RML&quot;), asking how aspects of RML change over the course of development from toddler years through adolescence. Multiple versions of the RML task were used in different samples, which varied on two dimensions: whether the reward probability of a given hand movement direction was deterministic or probabilistic, and whether the solution space had continuous reach targets or discrete reach targets. Using analyses of both raw behavioral data and model fits, the authors report four main results: First, developmental improvements reflected 3 clear changes, including increases in exploration, an increase in the RL learning rate, and a reduction of intrinsic motor noise. Second, changes to the task that made it discrete and/or deterministic both rescued performance in the youngest age groups, suggesting that observed deficits could be linked to continuous/probabilistic learning settings. Overall, the results shed light on how RML changes throughout human development, and the modeling characterizes the specific learning deficits seen in the youngest ages.</p>
<p>Strengths:</p>
<p>(1) This impressive work addresses an understudied subfield of motor control/psychology - the developmental trajectory of motor learning. It is thus timely and will interest many researchers.</p>
<p>(2) The task, analysis, and modeling methods are very strong. The empirical findings are rather clear and compelling, and the analysis approaches are convincing. Thus, at the empirical level, this study has very few weaknesses.</p>
<p>(3) The large sample sizes and in-lab replications further reflect the laudable rigor of the study.</p>
<p>(4) The main and supplemental figures are clear and concise.</p>
<p>Weaknesses:</p>
<p>(1) Framing.</p>
<p>
One weakness of the current paper is the framing, namely w/r/t what can be considered &quot;cognitive&quot; versus &quot;non-cognitive&quot; (&quot;procedural?&quot;) here. In the Intro, for example, it is stated that there are specific features of RML tasks that deviate from cognitive tasks. This is of course true in terms of having a continuous choice space and motor noise, but spatially correlated reward functions are not a unique feature of motor learning (see e.g. Giron et al., 2023, NHB). Given the result here that simplifying the spatial memory demands of the task greatly improved learning for the youngest cohort, it is hard to say whether the task is truly getting at a motor learning process or more generic cognitive capacities for spatial learning, working memory, and hypothesis testing. This is not a logical problem with the design, as spatial reasoning and working memory are intrinsically tied to motor learning. However, I think the framing of the study could be revised to focus in on what the authors truly think is motor about the task versus more general psychological mechanisms. Indeed, it may be the case that deficits in motor learning in young children are mostly about cognitive factors, which is still an interesting result!</p>
<p>(2) Links to other scholarship.</p>
<p>
If I'm not mistaken a common observation in studies of the development of reinforcement learning is a decrease in exploration over-development (e.g., Nussenbaum and Hartley, 2019; Giron et al., 2023; Schulz et al., 2019); this contrasts with the current results which instead show an increase. It would be nice to see a more direct discussion of previous findings showing decreases in exploration over development, and why the current study deviates from that. It could also be useful for the authors to bring in concepts of different types of exploration (e.g. &quot;directed&quot; vs &quot;random&quot;), in their interpretations and potentially in their modeling.</p>
<p>(3) Modeling.</p>
<p>
First, I may have missed something, but it is unclear to me if the model is actually accounting for the gradient of rewards (e.g., if I get a probabilistic reward moving at 45˚, but then don't get one at 40˚, I should be more likely to try 50˚ next then 35˚). I couldn't tell from the current equations if this was the case, or if exploration was essentially &quot;unsigned,&quot; nor if the multiple-trials-back regression analysis would truly capture signed behavior. If the model is sensitive to the gradient, it would be nice if this was more clear in the Methods. If not, it would be interesting to have a model that does &quot;function approximation&quot; of the task space, and see if that improves the fit or explains developmental changes. Second, I am curious if the current modeling approach could incorporate a kind of &quot;action hysteresis&quot; (aka perseveration), such that regardless of previous outcomes, the same action is biased to be repeated (or, based on parameter settings, avoided).</p>
<p>(4) Psychological mechanisms.</p>
<p>
There is a line of work that shows that when children and adults perform RL tasks they use a combination of working memory and trial-by-trial incremental learning processes (e.g., Master et al., 2020; Collins and Frank 2012). Thus, the observed increase in the learning rate over development could in theory reflect improvements in instrumental learning, working memory, or both. Could it be that older participants are better at remembering their recent movements in short-term memory (Hadjiosif et al., 2023; Hillman et al., 2024)?</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101036.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The study investigates reinforcement learning across the lifespan with a large sample of participants recruited for an online game. It finds that children gradually develop their abilities to learn reward probability, possibly hindered by their immature spatial processing and probabilistic reasoning abilities. Motor noise, reinforcement learning rate, and exploration after a failure all contribute to children's subpar performance.</p>
<p>Strengths:</p>
<p>(1) The paradigm is novel because it requires continuous movement to indicate people's choices, as opposed to discrete actions in previous studies.</p>
<p>(2) A large sample of participants were recruited.</p>
<p>(3) The model-based analysis provides further insights into the development of reinforcement learning ability.</p>
<p>Weaknesses:</p>
<p>(1) The adequacy of model-based analysis is questionable, given the current presentation and some inconsistency in the results.</p>
<p>(2) The task should not be labeled as reinforcement motor learning, as it is not about learning a motor skill or adapting to sensorimotor perturbations. It is a classical reinforcement learning paradigm.</p>
</body>
</sub-article>
</article>