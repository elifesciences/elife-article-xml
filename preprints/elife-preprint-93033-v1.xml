<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">93033</article-id>
<article-id pub-id-type="doi">10.7554/eLife.93033</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.93033.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Visual homogeneity computations in the brain enable solving generic visual tasks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8262-0155</contrib-id>
<name>
<surname>Jacob</surname>
<given-names>Georgin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5933-7893</contrib-id>
<name>
<surname>Pramod</surname>
<given-names>R. T.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9602-5066</contrib-id>
<name>
<surname>Arun</surname>
<given-names>S. P.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Electrical Communication Engineering</institution></aff>
<aff id="a2"><label>2</label><institution>Centre for Neuroscience Indian Institute of Science</institution>, Bangalore 560012</aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Kok</surname>
<given-names>Peter</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Moore</surname>
<given-names>Tirin</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University, Howard Hughes Medical Institute</institution>
</institution-wrap>
<city>Stanford</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Correspondence to <email>sparun@iisc.ac.in</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-02-05">
<day>05</day>
<month>02</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP93033</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-11-02">
<day>02</day>
<month>11</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-11-05">
<day>05</day>
<month>11</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.12.03.518965"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Jacob et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Jacob et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-93033-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Many visual tasks involve looking for specific object features. But we also often perform generic tasks where we look for specific property, such as finding an odd item, deciding if two items are same, or if an object has symmetry. How do we solve such tasks? Using simple neural rules, we show that displays with repeating elements can be distinguished from heterogeneous displays using a property we define as visual homogeneity. In behavior, visual homogeneity predicted response times on visual search and symmetry tasks. Brain imaging during these tasks revealed that visual homogeneity in both tasks is localized to a region in the object-selective cortex. Thus, a novel image property, visual homogeneity, is encoded in a localized brain region, to solve generic visual tasks.</p>
</abstract>
<abstract abstract-type="teaser">
<title>Significance statement</title>
<p>Most visual tasks involve looking for specific features, like finding a face in a crowd. But we often also perform generic tasks where we look for a particular image property – such as finding an odd item, deciding if two items are same, or judging if an object is symmetric. Precisely how we solve such disparate tasks is unclear. Here, we show that these tasks can be solved using a simple property we define as visual homogeneity. In behavior, visual homogeneity predicted response times on visual search and symmetry tasks. In brain imaging, it was localized to a region near the object-selective cortex. Thus, a novel image property, visual homogeneity, is computed by the brain to solve generic visual tasks.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Minor text edits</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Many visual tasks involve looking for specific objects or features, such as a friend in a crowd or selecting vegetables in the market. In such tasks, which have been studied extensively, we form a template in our brain that helps guide eye movements and locate the target (<xref ref-type="bibr" rid="c27">Peelen and Kastner, 2014</xref>). However, we also easily perform tasks that do not involve any specific feature but finding a property or relation between items. Examples of these generic tasks include finding an odd item, deciding if two items are same and judging if an object is symmetric. While machine vision algorithms are extremely successful in solving feature-based tasks like object categorization (<xref ref-type="bibr" rid="c39">Serre, 2019</xref>), they struggle to solve these generic tasks (<xref ref-type="bibr" rid="c18">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="c35">Ricci et al., 2021</xref>).</p>
<p>At first glance, these tasks appear completely different. Indeed, visual search (<xref ref-type="bibr" rid="c51">Verghese, 2001</xref>; <xref ref-type="bibr" rid="c55">Wolfe and Horowitz, 2017</xref>), same-different judgments (<xref ref-type="bibr" rid="c25">Nickerson, 1969</xref>; <xref ref-type="bibr" rid="c29">Petrov, 2009</xref>) and symmetry detection (<xref ref-type="bibr" rid="c52">Wagemans, 1997</xref>; <xref ref-type="bibr" rid="c6">Bertamini and Makin, 2014</xref>) have all been studied extensively, but always separately. However, at a deeper level, these tasks are similar because they all involve discriminating between items with repeating features from those without repeating features. We reasoned that if images with repeating features are somehow represented differently in the brain, this difference could be used to solve all these tasks without requiring separate computations for each task. Here we provide evidence for this hypothesis through behavioural and brain imaging experiments on humans.</p>
<p>Our key predictions are depicted in <xref rid="fig1" ref-type="fig">Figure 1</xref>. Consider a visual search task where participants have to indicate if a display contains an oddball target (<xref rid="fig1" ref-type="fig">Figure 1A</xref>) or contains no oddball targets (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). According the well-known principle of divisive normalization in high-level visual cortex (<xref ref-type="bibr" rid="c57">Zoccolan et al., 2005</xref>; <xref ref-type="bibr" rid="c1">Agrawal et al., 2020</xref>; <xref ref-type="bibr" rid="c16">Katti and Arun, 2022</xref>), the neural response to multiple objects is the average of the single object responses. Accordingly, the response to an array of identical items will be the same as the response to the single item. Moreover, the response to an array containing a target among distractors would lie along the line joining the target and distractor in the (neural) representational space. These possibilities are shown for all possible arrays made from three objects in <xref rid="fig1" ref-type="fig">Figure 1C</xref>. It can be seen that the homogeneous (target-absent) arrays stand apart since they contain repeating items, whereas the heterogeneous (target-present) arrays come closer since they contain a mixture of items. Since displays with repeating items are further away from the center of this space, this distance can be used to discriminate them from heterogeneous displays (<xref rid="fig1" ref-type="fig">Figure 1C</xref>, <italic>inset</italic>). We note here that this decision process does not capture many other complex forms of visual search, but may guide the initial selection process towards possible targets.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Solving visual search and symmetry tasks using visual homogeneity.</title><p>(A) Example target-present search display, containing a single oddball target (horse) among identical distractors (dog). Participants in such tasks have to indicate whether the display contains an oddball or not, without knowing the features of the target or distractor. This means they have to perform this task by detecting some property of each display rather than some feature contained in it.</p><p>(B) Example target-absent search display containing no oddball target.</p><p>(C) Hypothesized neural computation for target present/absent judgements. According to multiple object normalization, the response to multiple items is an average of the responses to the individual items. Thus, the response to a target-absent array will be identical to the individual items, whereas the response to a target-present array will lie along the line joining the corresponding target-absent arrays. This causes the target-absent arrays to stay apart (<italic>red lines</italic>), and the target-present arrays to come closer due to mixing (<italic>blue lines</italic>). If we calculate the distance (VH, for visual homogeneity) for each display, then target-absent arrays will have a larger distance to the center (VH<sub>a</sub>) compared to target-present arrays (VH<sub>p</sub>), and this distance can be used to distinguish between them. <italic>Inset:</italic> Schematic distance from center for target-absent arrays (red) and target-present arrays (blue). Note that this approach might only reflect the initial target selection process involved in oddball visual search but does not capture all forms of visual search.</p><p>(D) Example asymmetric object in a symmetry detection task. Here too, participants have to indicate if the display contains a symmetric object or not, without knowing the features of the object itself. This means they have to perform this task by detecting some property in the display.</p><p>(E) Example symmetric object in a symmetry detection task.</p><p>(F) Hypothesized neural computations for symmetry detection. Following multiple object normalization, the response to an object containing repeated parts is equal the response to the individual part, whereas the response to an object containing two different parts will lie along the line joining the objects with the two parts repeating. This causes symmetric objects to stand apart (red lines) and asymmetric objects to come closer due to mixing (<italic>blue lines</italic>). Thus, the visual homogeneity for symmetric objects (VH<sub>s</sub>) will be larger than for asymmetric objects (VH<sub>a</sub>). <italic>Inset:</italic> Schematic distance from center for symmetric objects (red) and asymmetric objects (blue).</p><p>(G) <italic>Behavioral predictions for VH.</italic> If visual homogeneity (VH) is a decision variable in visual search and symmetry detection tasks, then response times (RT) must be largest for displays with VH close to the decision boundary. This predicts opposite correlations between response time and VH for the present/absent or symmetry/asymmetry judgements. It also predicts zero overall correlation between VH and RT.</p><p>(H) <italic>Neural predictions for VH. Left:</italic> Correlation between brain activations and VH for two hypothetical brain regions. In the VH-encoding region, brain activations should be positively correlated with VH. In any region that encodes task difficulty as indexed by response time, brain activity should show no correlation since VH itself is uncorrelated with RT (see Panel G). <italic>Right:</italic> Correlation between brain activations and RT. Since VH is uncorrelated with RT overall, the region VH should show little or no correlation, whereas the regions encoding task difficulty would show a positive correlation.</p></caption>
<graphic xlink:href="518965v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We reasoned similarly for symmetry detection: here, participants have to decide if an object is asymmetric (<xref rid="fig1" ref-type="fig">Figure 1D</xref>) or symmetric (<xref rid="fig1" ref-type="fig">Figure 1E</xref>). According to multiple object normalization, objects with two different parts would lie along the line joining objects containing the two repeated parts (<xref rid="fig1" ref-type="fig">Figure 1F</xref>). Indeed, both symmetric and asymmetric objects show part summation in their neural responses (<xref ref-type="bibr" rid="c32">Pramod and Arun, 2018</xref>). Consequently, symmetric objects will be further away from the centre of this space compared to asymmetric objects, and this can be the basis for discriminating them (<xref rid="fig1" ref-type="fig">Figure 1F</xref>, <italic>inset</italic>).</p>
<p>We define this distance from the center for each image as its <italic>visual homogeneity</italic> (VH). We made two key experimental predictions to test in behavioural and brain imaging experiments. First, if VH is being used to solve visual search and symmetry detection tasks, then responses should be slowest for displays with VH close to the decision boundary and faster for displays with VH far away (<xref rid="fig1" ref-type="fig">Figure 1G</xref>). This predicts opposite correlations between response time and VH: for target-present arrays and asymmetric objects, the response time should be positively correlated with VH. By contrast, for target-absent arrays and symmetric objects, response time should be negatively correlated with VH. Importantly, because response times can be positively or negatively correlated with VH, the net correlation between response time and VH will be close to zero. Second, if VH is encoded by a dedicated brain region, then brain activity in that region will be positively correlated with VH (<xref rid="fig1" ref-type="fig">Figure 1H</xref>). Such a positive correlation cannot be explained easily by cognitive processes linked to response time such as attention or task difficulty, since response times will have zero correlation with the mean activity of this region.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>In Experiments 1-2, we investigated whether visual homogeneity computations could explain decisions about targets being present or absent in an array. Since visual homogeneity requires measuring distance in perceptual space, we set out to first characterize the underlying representation of a set of natural objects using measurements of perceptual dissimilarity.</p>
<sec id="s2a">
<title>Measuring perceptual space for natural objects</title>
<p>In Experiment 1, 16 human participants viewed arrays made from a set of 32 grayscale natural objects, with an oddball on the left or right (<xref rid="fig2" ref-type="fig">Figure 2A</xref>), and had to indicate the side on which the oddball appeared using a key press. Participants were highly accurate and consistent in their responses during this task (accuracy, mean ± sd: 98.8 ± 0.9%; correlation between mean response times of even- and odd-numbered participants: r = 0.91, p &lt; 0.0001 across all <sup>32</sup>C<sub>2</sub> = 496 object pairs). The reciprocal of response time is a measure of perceptual distance (or dissimilarity) between the two images (<xref ref-type="bibr" rid="c2">Arun, 2012</xref>). To visualize the underlying object representation, we performed a multidimensional scaling analysis, which embeds objects in a multidimensional space such that their pairwise dissimilarities match the experimentally observed dissimilarities (see Methods). The resulting two-dimensional embedding of all objects is shown in <xref rid="fig2" ref-type="fig">Figure 2B</xref>. In the resulting plot, nearby objects correspond to hard searches, and far away objects correspond to easy searches. Such representations reconstructed from behavioural data closely match population neural responses in high-level visual areas (<xref ref-type="bibr" rid="c26">Op de Beeck et al., 2001</xref>; <xref ref-type="bibr" rid="c40">Sripati and Olson, 2010</xref>). To capture the object representation accurately, we took the multidimensional embedding of all objects and treated the values along each dimension as the responses of an individual artificial neuron. We selected the number of dimensions in the multidimensional embedding so that the correlation between the observed and embedding dissimilarities matches the noise ceiling in the data. Subsequently, we averaged these single object responses to obtain responses to larger visual search arrays, as detailed below.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Visual homogeneity predicts target present/absent responses</title><p>(A) Example search array in an oddball search task (Experiment 1). Participants viewed an array containing identical items except for an oddball present either on the left or right side, and had to indicate using a key press which side the oddball appeared. The reciprocal of average search time was taken as the perceptual distance between the target and distractor items. We measured all possible pairwise distances for 32 grayscale natural objects in this manner.</p><p>(B) Perceptual space reconstructed using multidimensional scaling performed on the pairwise perceptual dissimilarities. In the resulting plot, nearby objects represent hard searches, and far away objects represent easy searches. Some images are shown at a small size due to space constraints; in the actual experiment, all objects were equated to have the same longer dimension. The correlation on the top right indicates the match between the distances in the 2D plot with the observed pairwise distances (**** is p &lt; 0.00005).</p><p>(C) Example display from Experiment 2. Participants performed this task inside the scanner. On each trial, they had to indicate whether an oddball target is present or absent using a key press.</p><p>(D) Predicted response to target-present and target-absent arrays, using the principle that the neural response to multiple items is the average of the individual item responses. This predicts that target-present arrays become similar due to mixing of responses, whereas target-absent arrays stand apart. Consequently, these two types of displays can be distinguished using their distance to a central point in this space. We define this distance as visual homogeneity, and it is obtained by finding the optimum center that maximizes the difference in correlations with response times (see Methods).</p><p>(E) Mean visual homogeneity relative to the optimum center for target-present and target-absent displays. Error bars represent s.e.m across all displays. Asterisks represent statistical significance (**** is p &lt; 0.00005, unpaired rank-sum test comparing visual homogeneity for 32 target-absent and 32 target-present arrays).</p><p>(F) Response time for target-present searches in Experiment 2 plotted against visual homogeneity calculated from Experiment 1. Asterisks represent statistical significance of the correlation (**** is p &lt; 0.00005). Note that a single model is fit to find the optimum center in representational space that predicts the response times for both target-present and target-absent searches.</p><p>(G) Response time for target-absent searches in Experiment 2 plotted against visual homogeneity calculated from Experiment 1. Asterisks represent statistical significance of the correlation (**** is p &lt; 0.00005).</p></caption>
<graphic xlink:href="518965v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<title>Visual homogeneity predicts target present/absent judgments (Experiments 1-2)</title>
<p>Having characterized the underlying perceptual representation for single objects, we set out to investigate whether target present/absent responses during visual search can be explained using this representation. In Experiment 2, 16 human participants viewed an array of items on each trial, and indicated using a key press whether there was an oddball target present or not (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). This task was performed inside an MRI scanner to simultaneously observe both brain activity and behaviour. Participants were highly accurate and consistent in their responses (accuracy, mean ± sd: 95 ± 3%; correlation between average response times of even- and odd-numbered participants: r = 0.86, p &lt; 0.0001 across 32 target-present searches, r = 0.63, p &lt; 0.001 across 32 target-absent searches).</p>
<p>Next we set out to predict the responses to target-present and target-absent search displays containing these objects. We first took the object coordinates returned by multidimensional scaling in Experiment 1 as neural responses of multiple neurons. We then used a well-known principle of object representations in high-level visual areas: the response to multiple objects is the average of the single object responses (<xref ref-type="bibr" rid="c57">Zoccolan et al., 2005</xref>; <xref ref-type="bibr" rid="c1">Agrawal et al., 2020</xref>). Thus, we took the response vector for a target-present array to be the average of the response vectors of the target and distractor (<xref rid="fig2" ref-type="fig">Figure 2D</xref>). Likewise, we took the response vector for a target-absent array to be equal to the response vector of the single item. We then asked if there is any point in this multidimensional representation such that distances from this point to the target-present and target-absent response vectors can accurately predict the target-present and target-absent response times with a positive and negative correlation respectively (see Methods). Note that this model has as many free parameters as the coordinates of this unknown point or center in multidimensional space, and this model can simultaneously predict both target-present and target-absent judgments. We used nonlinear optimization to find the coordinates of the center to best match the data (see Methods).</p>
<p>We denoted the distance of each display to the optimized center as the visual homogeneity. As expected, the visual homogeneity of target-present arrays was significantly smaller than target-absent arrays (<xref rid="fig2" ref-type="fig">Figure 2E</xref>). The resulting model predictions are shown in <xref rid="fig2" ref-type="fig">Figure 2F-G</xref>. The response times for target-present searches were positively correlated with visual homogeneity (r = 0.68, p &lt; 0.0001; <xref rid="fig2" ref-type="fig">Figure 2F</xref>). By contrast, the response times for target-absent searches were negatively correlated with visual homogeneity (r = -0.71, p &lt; 0.0001; <xref rid="fig2" ref-type="fig">Figure 2G</xref>). This is exactly as predicted if visual homogeneity is the underlying decision variable (<xref rid="fig1" ref-type="fig">Figure 1G</xref>). We note that the range of visual homogeneity values for target-present and target-absent searches do overlap, suggesting that visual homogeneity contributes but does not fully determine task performance. Rather, we suggest that visual homogeneity provides a useful and initial first guess at the presence or absence of a target, which can be refined further through detailed scrutiny.</p>
<p>To confirm that the above model fits are not due to overfitting, we performed a leave-one-out cross validation analysis, where we left out all target-present and target-absent searches involving a particular image, and then predicted these searches by calculating visual homogeneity. This too yielded similar correlations (r = 0.63, p &lt; 0.0001 for target-present, r = -0.63, p &lt; 0.001 for target-absent).</p>
<p>These findings are non-trivial for several reasons. First, it suggests that there are highly specific computations that can be performed on perceptual space to solve oddball tasks. This result is by no means straightforward from the mere measurement of perceptual dissimilarities. Second, while target-present response times are known to be driven by target-distractor similarity, target-absent response times are known to vary systematically but the reasons have been unclear. To the best of our knowledge our model provides the first unified mechanistic explanation for the systematic variations in both target-present and target-absent responses.</p>
<p>We performed several additional analyses to validate these results and confirm their generality. First, if heterogeneous displays elicit similar neural responses due to mixing, then their average distance to other objects must be related to their visual homogeneity. We confirmed that this was indeed the case, suggesting that the average distance of an object from all other objects is an useful estimate of visual homogeneity (Section S1). Second, the above analysis was based on taking the neural response to oddball arrays to be the average of the target and distractor responses. To confirm that averaging was indeed optimal, we repeated the above analysis by assuming a range of relative weights between the target and distractor. The best correlation was obtained for almost equal weights in the lateral occipital (LO) region, consistent with averaging and its role in the underlying perceptual representation (Section S1). Third, we performed several additional experiments on a larger set of natural objects as well as on silhouette shapes. In all cases, present/absent responses were explained using visual homogeneity (Section S2).</p>
<p>In sum, we conclude that visual homogeneity can explain oddball target present/absent judgements during visual search.</p>
</sec>
<sec id="s2c">
<title>Visual homogeneity predicts same/different responses</title>
<p>We have proposed that visual homogeneity can be used to solve any task that requires discriminating between homogeneous and heterogeneous displays. In Experiments 1-2, we have shown that visual homogeneity predicts target present/absent responses in visual search. We performed an additional experiment to assess whether visual homogeneity can be used to solve an entirely different task, namely a same-different task. In this task, participants have to indicate whether two items are the same or different. We note that instructions to participants for the same/different task (“you have to indicate if the two items are same or different”) are quite different from the visual search task (“you have to indicate whether an oddball target is present or absent”). Yet both tasks involve discriminating between homogeneous and heterogeneous displays. We therefore predicted that “same” responses would be correlated with target-absent judgements and “different” responses would be correlated with target-present judgements. Remarkably, this was indeed the case (Section S3), demonstrating that same/different responses can also be predicted using visual homogeneity.</p>
</sec>
<sec id="s2d">
<title>Visual homogeneity is independent of experimental context</title>
<p>In the above analyses, visual homogeneity was calculated for each display as its distance from an optimum center in perceptual space. This raises the possibility that visual homogeneity could be modified depending on experimental context since it could depend on the set of objects relative to which the visual homogeneity is computed. We performed a number of experiments to evaluate this possibility: we found that target-absent response times, which index visual homogeneity, are unaffected by a variety of experimental context manipulations (Section S4).</p>
<p>We therefore propose that visual homogeneity is an image-computable property that remains stable across tasks. Moreover, it can be empirically estimated as the reciprocal of the target-absent response times in a visual search task.</p>
</sec>
<sec id="s2e">
<title>A localized brain region encodes visual homogeneity (Experiment 2)</title>
<p>So far, we have found that target present/absent response times had opposite correlations with visual homogeneity (<xref rid="fig2" ref-type="fig">Figure 2F-G</xref>), suggesting that visual homogeneity is a possible decision variable for this task. Therefore, we reasoned that visual homogeneity may be localized to specific brain regions, such as in the visual or prefrontal cortices. Since the task in Experiment 2 was performed by participants inside an MRI scanner, we set out to investigate this issue by analyzing their brain activations.</p>
<p>We estimated brain activations in each voxel for individual target-present and target-absent search arrays (see Methods). To identify the brain regions whose activations correlated with visual homogeneity, we performed a whole-brain searchlight analysis. For each voxel, we calculated the mean activity in a 3×3×3 volume centered on that voxel (averaged across voxels and participants) for each present/absent search display, and calculated its correlation with visual homogeneity predictions derived from behavior (see Methods). The resulting map is shown in <xref rid="fig3" ref-type="fig">Figure 3A</xref>. Visual homogeneity was encoded in a highly localized region just anterior of the lateral occipital (LO) region, with additional weak activations in the parietal and frontal regions. To compare these trends across key visual regions, we calculated the correlation between mean activation and visual homogeneity for each region. This revealed visual homogeneity to be encoded strongly in this region VH, and only weakly in other visual regions (<xref rid="fig3" ref-type="fig">Figure 3D</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>A localized brain region encodes visual homogeneity</title><p>A. Searchlight map showing the correlation between mean activation in each 3×3×3 voxel neighborhood and visual homogeneity.</p><p>B. Searchlight map showing the correlation between neural dissimilarity in each 3×3×3 voxel neighborhood and perceptual dissimilarity measured in visual search.</p><p>C. Key visual regions identified using standard anatomical masks: early visual cortex (EVC), area V4, lateral occipital (LO) region. The visual homogeneity (VH) region was identified using the searchlight map in Panel A.</p><p>D. Correlation between the mean activation and visual homogeneity in key visual regions EVC, V4, LO and VH. Error bars represent standard deviation of the correlation obtained using a boostrap process, by repeatedly sampling participants with replacement for 10,000 times. Asterisks represent statistical significance, estimated by calculating the fraction of bootstrap samples in which the observed trend was violated (* is p &lt; 0.05, ** is p&lt; 0.01, **** is p &lt; 0.0001).</p><p>E. Correlation between neural dissimilarity in key visual regions with perceptual dissimilarity. Error bars represent the standard deviation of correlation obtained using a bootstrap process, by repeatedly sampling participants with replacement 10,000 times. Asterisks represent statistical significance, estimated by calculating the fraction of bootstrap samples in which the observed trend was violated (** is p &lt; 0.001).</p></caption>
<graphic xlink:href="518965v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To ensure that the high match between visual homogeneity and neural activations in the VH region is not due to an artefact of voxel selection, we performed subject-level analysis (Section S5). We repeated the searchlight analysis for each subject and defined VH region for each subject. We find this VH region consistently anterior to the LO region in each subject. Next, we divided participants into two groups, and repeated the brain-wide searchlight analysis. Importantly, the match between mean activation and visual homogeneity remained significant even when the VH region was defined using one group of participants and the correlation was calculated using the mean activations of the other group (Section S5).</p>
<p>To confirm that neural activations in VH region are not driven by other cognitive processes linked to response time, such as attention, we performed a whole-brain searchlight analysis using response times across both target-present and target-absent searches. Proceeding as before, we calculated the correlation between mean activations to the target-present, target-absent and all displays with the respective response times. The resulting maps show that mean activations in the VH region are uncorrelated with response times overall (Section S5). By contrast, activations in EVC and LO are negatively correlated with response times, suggesting that faster responses are driven by higher activation of these areas. Finally, mean activation of parietal and prefrontal regions were strongly correlated with response times, consistent with their role in attentional modulation (Section S5).</p>
</sec>
<sec id="s2f">
<title>Object representations in LO match with visual search dissimilarities</title>
<p>To investigate the neural space on which visual homogeneity is being computed, we performed a dissimilarity analysis. Since target-absent displays contain multiple instances of a single item, we took the neural response to target-absent displays as a proxy for the response to single items. For each pair of objects, we took the neural activations in a 3×3×3 neighborhood centered around a given voxel and calculated the Euclidean distance between the two 27-dimensional response vectors (averaged across participants). In this manner, we calculated the neural dissimilarity for all <sup>32</sup>C<sub>2</sub> = 496 pairs of objects used in the experiment, and calculated the correlation between the neural dissimilarity in each local neighborhood and the perceptual dissimilarities for the same objects measured using oddball search in Experiment 1. The resulting map is shown in <xref rid="fig3" ref-type="fig">Figure 3B</xref>. It can be seen that perceptual dissimilarities from visual search are best correlated in the lateral occipital region, consistent with previous studies (<xref rid="fig3" ref-type="fig">Figure 3E</xref>). To compare these trends across key visual regions, we performed this analysis for early visual cortex (EVC), area V4, LO and for the newly identified region VH (average MNI coordinates (x, y, z): (-48, -59, -6) with 111 voxels in the left hemisphere; (49, -56, -7) with 60 voxels in the right hemisphere). Perceptual dissimilarities matched best with neural dissimilarities in LO compared to the other visual regions (<xref rid="fig3" ref-type="fig">Figure 3E</xref>). We conclude that neural representations in LO match with perceptual space. This is concordant with many previous studies (<xref ref-type="bibr" rid="c13">Haushofer et al., 2008</xref>; <xref ref-type="bibr" rid="c19">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="c1">Agrawal et al., 2020</xref>; <xref ref-type="bibr" rid="c42">Storrs et al., 2021</xref>; <xref ref-type="bibr" rid="c4">Ayzenberg et al., 2022</xref>).</p>
</sec>
<sec id="s2g">
<title>Equal weights for target and distractor in target-present array responses</title>
<p>In the preceding sections, visual homogeneity was calculated using behavioural experiments assuming a neural representation that gives equal weights to the target and distractor. We tested this assumption experimentally by asking whether neural responses to target-present displays can be predicted using the response to the target and distractor items separately. The resulting maps revealed that target-present arrays were accurately predicted as a linear sum of the constituent items, with roughly equal weights for the target and distractor (Section S5).</p>
</sec>
<sec id="s2h">
<title>Visual homogeneity predicts symmetry perception (Experiments 3-4)</title>
<p>The preceding sections show that visual homogeneity predicts target present/absent responses as well same/different responses. We have proposed that visual homogeneity can be used to solve any task that involves discriminating homogeneous and heterogeneous displays. In Experiments 3 &amp; 4, we extend the generality of these findings to an entirely different task, namely symmetry perception. Here, asymmetric objects are akin to heterogeneous displays whereas symmetric objects are homogeneous displays. In Experiment 3, we measured perceptual dissimilarities for a set of 64 objects (32 symmetric, 32 asymmetric objects) made from a common set of parts. On each trial, participants viewed a search array with identical items except for one oddball, and had to indicate the side (left/right) on which the oddball appeared using a key press. An example search array is shown in <xref rid="fig4" ref-type="fig">Figure 4A</xref>. Participants performed searches involving all possible <sup>64</sup>C<sub>2</sub> = 2,016 pairs of objects. Participants made highly accurate and consistent responses on this task (accuracy, mean ± sd: 98.5 ± 1.33%; correlation between average response times from even- and odd-numbered subjects: r = 0.88, p &lt; 0.0001 across 2,016 searches). As before, we took the perceptual dissimilarity between each pair of objects to be the reciprocal of the average response time for displays with either item as target and the other as distractor. To visualize the underlying object representation, we performed a multidimensional scaling analysis, which embeds objects in a multidimensional space such that their pairwise dissimilarities match the experimentally observed dissimilarities. The resulting plot for two dimensions is shown in <xref rid="fig4" ref-type="fig">Figure 4B</xref>, where nearby objects correspond to similar searches. It can be seen that symmetric objects are generally more spread apart than asymmetric objects, suggesting that visual homogeneity could discriminate between symmetric and asymmetric objects.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Visual homogeneity predicts symmetry perception</title><p>(A) Example search array in Experiment 3. Participants viewed an array containing identical items except for an oddball present either on the left or right side, and had to indicate using a key press which side the oddball appeared. The reciprocal of average search time was taken as the perceptual distance between the target and distractor items. We measured all possible pairwise distances for 64 objects (32 symmetric, 32 asymmetric) in this manner.</p><p>(B) Perceptual space reconstructed using multidimensional scaling performed on the pairwise perceptual dissimilarities. In the resulting plot, nearby objects represent hard searches, and far away objects represent easy searches. Some images are shown at a small size due to space constraints; in the actual experiment, all objects were equated to have the same longer dimension. The correlation on the <italic>top right</italic> indicates the match between the distances in the 2D plot with the observed pairwise distances (**** is p &lt; 0.00005).</p><p>(C) Two example displays from Experiment 4. Participants had to indicate whether the object is symmetric or asymmetric using a key press.</p><p>(D) Using the perceptual representation of symmetric and asymmetric objects from Experiment 3, we reasoned that they can be distinguished using their distance to a center in perceptual space. The coordinates of this center are optimized to maximize the match to the observed symmetry detection times.</p><p>(E) Visual homogeneity relative to the optimum center for asymmetric and symmetric objects. Error bar represents s.e.m. across images. Asterisks represent statistical significance (* is p &lt; 0.05, unpaired rank-sum test comparing visual homogeneity for 32 symmetric and 32 asymmetric objects).</p><p>(F) Response time for asymmetric objects in Experiment 4 plotted against visual homogeneity calculated from Experiment 3. Asterisks represent statistical significance of the correlation (** is p &lt; 0.001).</p><p>(G) Response time for symmetric objects in Experiment 4 plotted against visual homogeneity calculated from Experiment 3. Asterisks represent statistical significance of the correlation (* is p &lt; 0.05).</p></caption>
<graphic xlink:href="518965v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In Experiment 4, we tested this prediction experimentally using a symmetry detection task that was performed by participants inside an MRI scanner. On each trial, participants viewed a briefly presented object, and had to indicate whether the object was symmetric or asymmetric using a key press (<xref rid="fig4" ref-type="fig">Figure 4C</xref>). Participants made accurate and consistent responses in this task (accuracy, mean ± sd: 97.7 ± 1.7%; correlation between response times of odd- and even-numbered participants: r = 0.47, p &lt; 0.0001).</p>
<p>We next wondered whether visual homogeneity can be used to predict symmetry judgments. To this end, we took the embedding of all objects from Experiment 3, and asked whether there was a center in this multidimensional space such that the distance of each object to this center would be oppositely correlated with response times for symmetric and asymmetric objects (see Methods). Model predictions are shown in <xref rid="fig4" ref-type="fig">Figure 4E-G</xref>. As predicted, visual homogeneity was significantly larger for symmetric compared to asymmetric objects (visual homogeneity, mean ± sd: 0.60 ± 0.24 s<sup>-1</sup> for asymmetric objects; 0.76 ± 0.29 s<sup>-1</sup> for symmetric objects; p &lt; 0.05, rank-sum test; <xref rid="fig4" ref-type="fig">Figure 4E</xref>). For asymmetric objects, symmetry detection response times were positively correlated with visual homogeneity (r = 0.56, p &lt; 0.001; <xref rid="fig4" ref-type="fig">Figure 4F</xref>). By contrast, for symmetric objects, response times were negatively correlated with visual homogeneity (r = -0.39, p &lt; 0.05; <xref rid="fig4" ref-type="fig">Figure 4G</xref>). These patterns are exactly as expected if visual homogeneity was the underlying decision variable for symmetry detection. However, we note that the range of visual homogeneity values for asymmetric and symmetric objects do overlap, suggesting that visual homogeneity contributes but does not fully determine task performance. Rather, we suggest that visual homogeneity provides a useful and initial first guess at symmetry in an image, which can be refined further through detailed scrutiny.</p>
<p>To confirm that these model fits are not due to overfitting, we performed a leave-one-out cross validation analysis, where we left out one object at a time, and then calculated its visual homogeneity. This too yielded similar correlations (r = 0.44 for asymmetric, r = -0.39 for symmetric objects, p &lt; 0.05 in both cases).</p>
<p>In sum, we conclude that visual homogeneity can predict symmetry perception. Taken together, these experiments demonstrate that the same computation (distance from a center) explains two disparate generic visual tasks: symmetry perception and visual search.</p>
</sec>
<sec id="s2i">
<title>Visual homogeneity is encoded by the VH region during symmetry detection</title>
<p>If visual homogeneity is a decision variable for symmetry detection, it could be localized to specific regions in the brain. To investigate this issue, we analyzed the brain activations of participants in Experiment 4.</p>
<p>To investigate the neural substrates of visual homogeneity, we performed a searchlight analysis. For each voxel, we calculated the correlation between mean activations in a 3×3×3 voxel neighborhood and visual homogeneity. This revealed a localized region in the visual cortex as well as some parietal regions where this correlation attained a maximum (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). This VH region (average MNI coordinates (x, y, z): (-57, -56, -8) with 93 voxels in the left hemisphere; (58, -50, -8) with 73 voxels in the right hemisphere) overlaps with VH region defined during visual search in Experiment 3 (for a detailed comparison, see Section S7). We note that it is not straightforward to interpret the activation differences here, since the objects in this experiment were present foveally whereas the visual search arrays contained no central item with items exclusively in the periphery.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Brain region encoding visual homogeneity during symmetry detection</title><p>(A) Searchlight map showing the correlation between mean activation in each 3×3×3 voxel neighborhood and visual homogeneity.</p><p>(B) Searchlight map showing the correlation between neural dissimilarity in each 3×3×3 voxel neighborhood and perceptual dissimilarity measured in visual search.</p><p>(C) Key visual regions identified using standard anatomical masks: early visual cortex (EVC), area V4, Lateral occipital (LO) region. The visual homogeneity (VH) region was identified using searchlight map in Panel A.</p><p>(D) Correlation between the mean activation and visual homogeneity in key visual regions EVC, V4, LO and VH. Error bars represent standard deviation of the correlation obtained using a boostrap process, by repeatedly sampling participants with replacement for 10,000 times. Asterisks represent statistical significance, estimated by calculating the fraction of bootstrap samples in which the observed trend was violated (* is p &lt; 0.05, ** is p&lt; 0.01, **** is p &lt; 0.0001).</p><p>(E) Correlation between neural dissimilarity in key visual regions with perceptual dissimilarity. Error bars represent the standard deviation of correlation obtained using a bootstrap process, by repeatedly sampling participants with replacement 10,000 times. Asterisks represent statistical significance, estimated by calculating the fraction of bootstrap samples in which the observed trend was violated (** is p &lt; 0.001).</p></caption>
<graphic xlink:href="518965v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To confirm that neural activations in VH region are not driven by other cognitive processes linked to response time, such as attention, we performed a whole-brain searchlight analysis using response times across both symmetric and asymmetric objects. This revealed that mean activations in the VH region were poorly correlated with response times overall, whereas other parietal and prefrontal regions strongly correlated with response times consistent with their role in attentional modulation (Section S6).</p>
<p>To investigate the perceptual representation that is being used for visual homogeneity computations, we performed a neural dissimilarity analysis. For each pair of objects, we took the neural activations in a 3×3×3 neighborhood centered around a given voxel and calculated the Euclidean distance between the two 27-dimensional response vectors. In this manner, we calculated the neural dissimilarity for all <sup>64</sup>C<sub>2</sub> = 2,016 pairs of objects used in the experiment, and calculated the correlation between the neural dissimilarity in each local neighborhood and the perceptual dissimilarities for the same objects measured using oddball search in Experiment 3. The resulting map is shown in <xref rid="fig5" ref-type="fig">Figure 5B</xref>. The match between neural and perceptual dissimilarity was maximum in the lateral occipital region (<xref rid="fig5" ref-type="fig">Figure 5B</xref>).</p>
<p>To compare these trends for key visual regions, we repeated this analysis for anatomically defined regions of interest in the visual cortex: early visual cortex (EVC), area V4, the lateral occipital (LO) region, and the VH region defined based on the searchlight map in <xref rid="fig5" ref-type="fig">Figure 5A</xref>. These regions are depicted in <xref rid="fig5" ref-type="fig">Figure 5C</xref>. We then asked how mean activations in each of these regions is correlated with visual homogeneity. This revealed that the match with visual homogeneity is best in the VH region compared to the other regions (<xref rid="fig5" ref-type="fig">Figure 5D</xref>). To further confirm that visual homogeneity is encoded in a localized region in the symmetry task, we repeated the searchlight analysis on two independent subgroups of participants. This revealed highly similar regions in both groups (Section S6).</p>
<p>Finally, we compared neural dissimilarities and perceptual dissimilarities in each region as before. This revealed that perceptual dissimilarities (measured from Experiment 3, during visual search) matched best with the LO region (<xref rid="fig5" ref-type="fig">Figure 5E</xref>), suggesting that object representations in LO are the basis for visual homogeneity computations in the VH region.</p>
<p>In sum, our results suggest that visual homogeneity is encoded by the VH region, using object representations present in the adjoining LO region.</p>
</sec>
<sec id="s2j">
<title>Target-absent responses predict symmetry detection</title>
<p>So far, we have shown that visual homogeneity predicts target present/absent responses in visual search as well as symmetry detection responses. These results suggest a direct empirical link between these two tasks. Specifically, since target-absent response time is inversely correlated with visual homogeneity, we can take its reciprocal as an estimate of visual homogeneity. This in turn predicts opposite correlations between symmetry detection times and reciprocal of target-absent response time: in other words, we should see a positive correlation for asymmetric objects, and a negative correlation for symmetric objects. We confirmed these predictions using additional experiments (Section S8). These results reconfirm that a common decision variable, visual homogeneity, drives both target present/absent and symmetry judgements.</p>
</sec>
<sec id="s2k">
<title>Visual homogeneity explains animate categorization</title>
<p>Since visual homogeneity is always calculated relative to a location in perceptual space, we reasoned that shifting this center towards a particular object category would make it a decision variable for object categorization. To test this prediction, we reanalyzed data from a previous study in which participants had to categorize images as belonging to three hierarchical categories: animals, dogs or Labradors (<xref ref-type="bibr" rid="c22">Mohan and Arun, 2012</xref>). By adjusting the center of the perceptual space measured using visual search, we were able to predict categorization responses for all three categories (Section S9). We further reasoned that, if the optimum center for animal/dog/Labrador categorization is close to the default center in perceptual space that predicts target present/absent judgements, then even the default visual homogeneity, as indexed by the reciprocal of target-absent search time, should predict categorization responses. Interestingly, this was indeed the case (Section S9). We conclude that, at least for the categories tested, visual homogeneity computations can serve as a decision variable for object categorization.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Here, we investigated three disparate visual tasks: detecting whether an oddball is present in a search array, deciding if two items are same or different, and judging whether an object is symmetric/asymmetric. Although these tasks are superficially quite different, they all involve discriminating between homogeneous and heterogeneous displays. We defined a new image property computable from the underlying perceptual representation, namely visual homogeneity, that can be used to solve these tasks. We found that visual homogeneity can predict response times in all three tasks. Finally we have found that visual homogeneity is encoded in a highly localized brain region in both visual search and symmetry tasks. Below we discuss these findings in relation to the existing literature.</p>
<sec id="s3a">
<title>Visual homogeneity unifies visual search, same-different and symmetry tasks</title>
<p>Our main finding, that a single decision variable (visual homogeneity) can be used to solve three disparate visual tasks (visual search, same/different and symmetry detection) is novel to the best of our knowledge. This finding is interesting and important because it establishes a close correspondence between all three tasks, and explains some unresolved puzzles in each of these tasks, as detailed below.</p>
<p>First, with regards to visual search, theoretical accounts of search are based on signal detection theory (<xref ref-type="bibr" rid="c51">Verghese, 2001</xref>; <xref ref-type="bibr" rid="c55">Wolfe and Horowitz, 2017</xref>), but define the signal only for specific target-distractor pairs. By contrast, the task of detecting whether an oddball item is present requires a more general decision rule that has not been identified. Our results suggest that visual homogeneity is the underlying decision variable. In visual search, target-present search times are widely believed to be driven by target-distractor similarity (<xref ref-type="bibr" rid="c10">Duncan and Humphreys, 1989</xref>; <xref ref-type="bibr" rid="c54">Wolfe and Horowitz, 2004</xref>). But target-absent search times also vary systematically for reasons that have not been elucidated previously. The slope of target-absent search times as a function of set size are typically twice the slope of target present searches (<xref ref-type="bibr" rid="c53">Wolfe, 1998</xref>). However this observation is based on averaging across many target-present searches. Since there is only one unique item in a target-absent search array, there must be some image property that causes systematic variations. Our results resolve this puzzle by showing that this systematic variation is driven by visual homogeneity. Finally, our findings also help explain why we sometimes know a target is present without knowing its exact location – this is because the underlying decision variable, visual homogeneity, arises in high-level visual areas with relatively coarse spatial information. However, we note that visual homogeneity computations described in this study do not completely explain all the variance observed in oddball search times in our study – rather they offer an important quantitative model that could explain the initial phase of target selection. We speculate that this initial phase could be shared by all forms of visual search (e.g. searching among non-identical distractors, memory-guided search, conjunction search), and these would be interesting possibilities for future work.</p>
<p>Second, with regards to same-different tasks, most theoretical accounts use signal detection theory but usually with reference to specific stimulus pairs (<xref ref-type="bibr" rid="c25">Nickerson, 1969</xref>; <xref ref-type="bibr" rid="c29">Petrov, 2009</xref>). It has long been observed that “different” responses become faster with increasing target-distractor dissimilarity but this trend logically predicts that “same” responses, which have zero difference, should be the slowest (<xref ref-type="bibr" rid="c24">Nickerson, 1967</xref>, <xref ref-type="bibr" rid="c25">1969</xref>). But in fact, “same” responses are faster than “different” responses. This puzzle has been resolved by assuming a separate decision rule for “same” judgements, making the overall decision process more complex (<xref ref-type="bibr" rid="c29">Petrov, 2009</xref>; <xref ref-type="bibr" rid="c12">Goulet, 2020</xref>). Our findings resolve this puzzle by identifying a novel variable, visual homogeneity, which can be used to implement a simple decision rule for making same/different responses. Our findings also explain why some images elicit faster “same” responses than others: this is due to image-to-image differences in visual homogeneity.</p>
<p>Third, with regard to symmetry detection, most theoretical accounts assume that symmetry is explicitly detected using symmetry detectors along particular axes (<xref ref-type="bibr" rid="c52">Wagemans, 1997</xref>; <xref ref-type="bibr" rid="c6">Bertamini and Makin, 2014</xref>). By contrast, our findings indicate an indirect mechanism for symmetry detection that does not invoke any special symmetry computations. We show that visual homogeneity computations can easily discriminate between symmetric and asymmetric objects. This is because symmetric objects have high visual homogeneity since they have repeated parts, whereas asymmetric objects have low visual homogeneity since they have disparate parts (<xref ref-type="bibr" rid="c32">Pramod and Arun, 2018</xref>). In a recent study, symmetry detection was explained by the average distance of objects relative to other objects (<xref ref-type="bibr" rid="c32">Pramod and Arun, 2018</xref>). This finding is consistent with ours since visual homogeneity is correlated with the average distance to other objects (Section S1). However, there is an important distinction between these two quantities. Visual homogeneity is an intrinsic image property, whereas the average distance of an object to other objects depends on the set of other objects on which the average is being computed. Indeed, we have confirmed through additional experiments that visual homogeneity is independent of experimental context (Section S4). We speculate that visual homogeneity can explain many other aspects of symmetry perception, such as the relative strength of symmetries.</p>
</sec>
<sec id="s3b">
<title>Visual homogeneity in other visual tasks</title>
<p>Our finding that visual homogeneity explains generic visual tasks has several important implications for visual tasks in general. First, we note that visual homogeneity can be easily extended to explain other generic tasks such as delayed match-to-sample tasks or n-back tasks, by taking the response to the test stimulus as being averaged with the sample-related information in working memory. In such tasks, visual homogeneity will be larger for sequences with repeated compared to non-repeated stimuli, and can easily be used to solve the task. Testing these possibilities will require comparing systematic variations in response times in these tasks across images, and measurements of perceptual space for calculating visual homogeneity.</p>
<p>Second, we note that visual homogeneity can also be extended to explain object categorization, if one assumes that the center in perceptual space for calculating visual homogeneity can be temporarily shifted to the center of an object category. In such tasks, visual homogeneity relative to the category center will be small for objects belonging to a category and large for objects outside the category, and can be used as a decision variable to solve categorization tasks. This idea is consistent with prevalent accounts of object categorization (<xref ref-type="bibr" rid="c41">Stewart and Morin, 2007</xref>; <xref ref-type="bibr" rid="c3">Ashby and Maddox, 2011</xref>; <xref ref-type="bibr" rid="c22">Mohan and Arun, 2012</xref>). Indeed, categorization response times can be explained using perceptual distances to category and non-category items (<xref ref-type="bibr" rid="c22">Mohan and Arun, 2012</xref>). By reanalyzing data from this study, we have found that, at least for the animate categories tested, visual homogeneity can explain categorization responses (Section S9). However, this remains to be tested in a more general fashion across multiple object categories.</p>
</sec>
<sec id="s3c">
<title>Neural encoding of visual homogeneity</title>
<p>We have found that visual homogeneity is encoded in a specific region of the brain, which we denote as region VH, in both visual search and symmetry detection tasks (<xref rid="fig3" ref-type="fig">Figure 3D</xref>, 5D). This finding is consistent with observations of norm-based encoding in IT neurons (<xref ref-type="bibr" rid="c20">Leopold et al., 2006</xref>) and in face recognition (<xref ref-type="bibr" rid="c47">Valentine, 1991</xref>; <xref ref-type="bibr" rid="c34">Rhodes and Jeffery, 2006</xref>; <xref ref-type="bibr" rid="c9">Carlin and Kriegeskorte, 2017</xref>). However, our findings are significant because they reveal a dedicated region in high-level visual cortex for solving generic visual tasks.</p>
<p>We have found that the VH region is located just anterior to the lateral occipital (LO) region, where neural dissimilarities match closely with perceptual dissimilarities (<xref rid="fig3" ref-type="fig">Figure 3E</xref>, <xref rid="fig5" ref-type="fig">5E</xref>). Based on this proximity, we speculate that visual homogeneity computations are based on object representations in LO. However, confirming this prediction will require fine-grained recordings of neural activity in VH and LO. An interesting possibility for future studies would be to causally perturb brain activity separately in VH or LO using magnetic or electrical stimulation, if at all possible. A simple prediction would be that perturbing LO would distort the underlying representation, whereas perturbing VH would distort the underlying decision process. We caution however that the results might not be so easily interpretable if visual homogeneity computations in VH are based on object representations in LO.</p>
<p>Recent observations from neural recordings in monkeys suggest that perceptual dissimilarities and visual homogeneity need not be encoded in separate regions. For instance, the overall magnitude of the population neural response of monkey inferior temporal (IT) cortex neurons was found to correlate with memorability (<xref ref-type="bibr" rid="c15">Jaegle et al., 2019</xref>). These results are consistent with encoding of visual homogeneity in these regions. However, we note that neural responses in IT cortex also predict perceptual dissimilarities (<xref ref-type="bibr" rid="c26">Op de Beeck et al., 2001</xref>; <xref ref-type="bibr" rid="c40">Sripati and Olson, 2010</xref>; <xref ref-type="bibr" rid="c56">Zhivago and Arun, 2014</xref>; <xref ref-type="bibr" rid="c1">Agrawal et al., 2020</xref>). Taken together, these findings suggest that visual homogeneity computations and the underlying perceptual representation could be interleaved within a single neural population, unlike in humans where we found separate regions. Indeed, in our study, the mean activations of the LO region were also correlated with visual homogeneity for symmetry detection (<xref rid="fig5" ref-type="fig">Figure 5A</xref>), but not for target present/absent search (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). We speculate that perhaps visual homogeneity might be intermingled into the object representation in monkeys but separated into a dedicated region in humans. These are interesting possibilities for future work.</p>
<p>Although many previous studies have reported brain activations in the vicinity of the VH region, we are unaware of any study that has ascribed a specific function to this region. The localized activations in our study match closely with the location of the recently reported ventral stream attention module in both humans and monkeys (<xref ref-type="bibr" rid="c37">Sani et al., 2021</xref>). Previous studies have observed important differences in brain activations in this region, which we can be explained using visual homogeneity, as detailed below.</p>
<p>First, previous studies have observed larger brain activations for animate compared to inanimate objects in higher visual areas which have typically included our newly defined VH region (<xref ref-type="bibr" rid="c7">Bracci and Op de Beeck, 2015</xref>; <xref ref-type="bibr" rid="c33">Proklova et al., 2016</xref>; <xref ref-type="bibr" rid="c45">Thorat et al., 2019</xref>). These observations would be consistent with our findings if visual homogeneity is smaller for animate compared to inanimate objects, which would result in weaker activations for animate objects in region VH. Indeed, visual homogeneity, as indexed by the reciprocal of target-absent search time, is smaller for animate objects compared to inanimate objects (Section S9). Likewise, brain activations were weaker for animate objects compared to inanimate objects in region VH (average VH activations, mean ± sd across participants: 0.50 ± 0.61 for animate target-absent displays, 0.64 ± 0.59 for inanimate target-absent displays, p &lt; 0.05, sign-rank test across participants). Based on this we speculate that visual homogeneity may partially explain the animacy organization of human ventral temporal cortex. Establishing this will require controlling animate/inanimate stimuli not only for shape but also for visual homogeneity.</p>
<p>Second, previous studies have reported larger brain activations for symmetric objects compared to asymmetric objects in the vicinity of this region (<xref ref-type="bibr" rid="c38">Sasaki et al., 2005</xref>; <xref ref-type="bibr" rid="c50">Van Meel et al., 2019</xref>). This can be explained by our finding that symmetric objects have larger visual homogeneity (<xref rid="fig4" ref-type="fig">Figure 4E</xref>), leading to activation of the VH region (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). But the increased activations in previous studies were located in the V4 &amp; LO regions, whereas we have found greater activations more anteriorly in the VH region. This difference could be due to the stimulus-related differences: both previous studies used dot patterns, which could appear more object-like when symmetric, leading to more widespread differences in brain activation due to other visual processes like figure-ground segmentation (<xref ref-type="bibr" rid="c50">Van Meel et al., 2019</xref>). By contrast, both symmetric and asymmetric objects in our study are equally object-like. Resolving these discrepancies will require measuring visual homogeneity as well as behavioural performance during symmetry detection for dot patterns.</p>
<p>Finally, our results are consistent with the greater activity observed for objects with shared features observed in ventral temporal cortex during naming tasks (<xref ref-type="bibr" rid="c44">Taylor et al., 2012</xref>; <xref ref-type="bibr" rid="c46">Tyler et al., 2013</xref>). Our study extends these observations by demonstrating an empirical measure for shared feature (target-absent times in visual search) and encoding of this empirical measure into a localized region in object selective cortex across many tasks. We speculate that visual homogeneity may at least partially explain semantic concepts such as those described in these studies.</p>
</sec>
<sec id="s3d">
<title>Relation to image memorability</title>
<p>We have defined a novel image property, visual homogeneity, which refers to the distance of a visual image to a central point in the underlying perceptual representation. It can be reliably estimated for each image as the inverse of the target-absent response time in a visual search task (<xref rid="fig2" ref-type="fig">Figure 2</xref>) and is independent of the immediate experimental context (Section S4).</p>
<p>The above observations raise the question of whether and how visual homogeneity might be related to image memorability. It has long been noted that faces that are rated as being distinctive or unusual are also easier to remember (<xref ref-type="bibr" rid="c23">Murdock, 1960</xref>; <xref ref-type="bibr" rid="c48">Valentine and Bruce, 1986a</xref>, <xref ref-type="bibr" rid="c49">1986b</xref>; <xref ref-type="bibr" rid="c47">Valentine, 1991</xref>). Recent studies have elucidated this observation by showing that there are specific image properties that predict image memorability (<xref ref-type="bibr" rid="c5">Bainbridge et al., 2017</xref>; <xref ref-type="bibr" rid="c21">Lukavský and Děchtěrenko, 2017</xref>; <xref ref-type="bibr" rid="c36">Rust and Mehrpour, 2020</xref>). However, image memorability, as elegantly summarized in a recent review (<xref ref-type="bibr" rid="c36">Rust and Mehrpour, 2020</xref>), could be driven by a number of both intrinsic and extrinsic factors. It would therefore be interesting to characterize how well visual homogeneity, empirically measured using target-absent visual search, can predict image memorability.</p>
</sec>
<sec id="s3e">
<title>Conclusions</title>
<p>Taken together, our results show that a variety of generic visual tasks that require detecting feature-independent image properties, can be solved using visual homogeneity as a decision variable, which is localized to a specific region anterior to the lateral occipital region. While this does not explain all possible variations of these tasks, our study represents an important first step in terms of demonstrating a quantitative, falsifiable model and a localized neural substrate. These can serve as a basis to elucidate whether/how visual homogeneity computations might contribute to a variety of visual tasks.</p>
</sec>
</sec>
<sec id="s4">
<title>Materials and methods</title>
<p>All participants had a normal or corrected-to-normal vision and gave informed consent to an experimental protocol approved by the Institutional Human Ethics Committee of the Indian Institute of Science (IHEC # 6-15092017). Participants provided written informed consent before each experiment and were monetarily compensated.</p>
<sec id="s4a">
<title>Experiment 1. Oddball detection for perceptual space (natural objects)</title>
<sec id="s4a1">
<title>Participants</title>
<p>A total of 16 participants (8 males, 22 ± 2.8 years) participated in this experiment.</p>
</sec>
<sec id="s4a2">
<title>Stimuli</title>
<p>The stimulus set comprised a set of 32 grayscale natural objects (16 animate, 16 inanimate) presented against a black background.</p>
</sec>
<sec id="s4a3">
<title>Procedure</title>
<p>Participants performed an oddball detection task with a block of practice trials involving unrelated stimuli followed by the main task. Each trial began with a red fixation cross (diameter 0.5°) for 500 ms, followed by a 4 x 4 search array measuring 30° x 30° for 5 seconds or until a response was made. The search array always contained one oddball target and 15 identical distractors, with the target appearing equally often on the left or right. A vertical red line divided the screen equally into two halves to facilitate responses. Participants were asked to respond as quickly and as accurately as possible using a key press to indicate the side of the screen containing the target (’Z’ for left, M’ for right). Incorrect trials were repeated later after a random number of other trials. Each participant completed 992 correct trials (<sup>32</sup>C<sub>2</sub> object pairs x 2 repetitions with either image as target). The experiment was created using PsychoPy (<xref ref-type="bibr" rid="c28">Peirce et al., 2019</xref>) and ported to the online platform Pavlovia for collecting data.</p>
<p>Since stimulus size could vary with the monitor used by the online participants, we equated the stimulus size across participants using the ScreenScale function (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/8FHQK">https://doi.org/10.17605/OSF.IO/8FHQK</ext-link>). Each participant adjusted the size of a rectangle on the screen such that its size matched the physical dimensions of a credit card. All the stimuli presented were then scaled with the estimated scaling function to obtain the desired size in degrees of visual angle, assuming an average distance to screen of 60 cm.</p>
</sec>
<sec id="s4a4">
<title>Data Analysis</title>
<p>Response times faster than 0.3 seconds or slower than 3 seconds were removed from the data. This step removed only 1.25% of the data and improved the overall response time consistency, but did not qualitatively alter the results.</p>
</sec>
<sec id="s4a5">
<title>Characterizing perceptual space using multidimensional scaling</title>
<p>To characterize the perceptual space on which present/absent decisions are made, we took the inverse of the average response times (across trials and participants) for each image pair. This inverse of response time (i.e. 1/RT) represents the dissimilarity between the target and distractor (<xref ref-type="bibr" rid="c2">Arun, 2012</xref>), indexes the underlying salience signal in visual search (<xref ref-type="bibr" rid="c43">Sunder and Arun, 2016</xref>) and combines linearly across a variety of factors (<xref ref-type="bibr" rid="c30">Pramod and Arun, 2014</xref>, <xref ref-type="bibr" rid="c31">2016</xref>; <xref ref-type="bibr" rid="c14">Jacob and Arun, 2020</xref>). Since there were 32 natural objects in the experiment and all possible (<sup>32</sup>C<sub>2</sub> = 496) pairwise searches in the experiment, we obtained 496 pairwise dissimilarities overall. To calculate target-present and target-absent array responses, we embedded these objects into a multidimensional space using multidimensional scaling analysis (<italic>mdscale</italic> function; MATLAB 2019). This analysis finds the n-dimensional coordinates for each object such that pairwise distances between objects best matches with the experimentally observed pairwise distances. We then treated the activations of objects along each dimension as the responses of a single artificial neuron, so that the response to target-present arrays could be computed as the average of the target and distractor responses.</p>
</sec>
</sec>
<sec id="s4b">
<title>Experiment 2. Target present-absent search during fMRI</title>
<sec id="s4b1">
<title>Participants</title>
<p>A total of 16 subjects (11 males; age, mean ± sd: 25 ± 2.9 years) participated in this experiment. Participants with history of neurological or psychiatric disorders, or with metal implants or claustrophobia were excluded through screening questionnaires.</p>
</sec>
<sec id="s4b2">
<title>Procedure</title>
<p>Inside the scanner, participants performed a single run of a one-back task for functional localizers (block design, object vs scrambled objects), eight runs of the present-absent search task (event-related design), and an anatomical scan. The experiment was deployed using custom MATLAB scripts written using Psychophysics Toolbox (<xref ref-type="bibr" rid="c8">Brainard, 1997</xref>).</p>
</sec>
<sec id="s4b3">
<title>Functional localizer runs</title>
<p>Participants had to view a series of images against a black background and press a response button whenever an item was repeated. On each trial, 16 images were presented (0.8 s on, 0.2 s off), containing one repeat of an image that could occur at random. Trials were combined into blocks of 16 trials each containing either only objects or only scrambled objects. A single run of the functional localizers contained 12 such blocks (6 object blocks &amp; 6 scrambled-object blocks). Stimuli in each block were chosen randomly from a larger pool of 80 naturalistic objects with the corresponding phase-scrambled objects (created by taking the 2D Fourier transform of each image, randomly shuffling the Fourier phase, and performing the Fourier inverse transform). This is a widely used method for functional localization of object-selective cortex. In practice, however, we observed no qualitative differences in our results upon using voxels activated during these functional localizer runs to further narrow down the voxels selected using anatomical masks. As a result, we did not use the functional localizer data, and all the analyses presented here are based on anatomical masks only.</p>
</sec>
<sec id="s4b4">
<title>Visual search task</title>
<p>In the present-absent search task, participants reported the presence or absence of an oddball target by pressing one of two buttons using their right hand. The response buttons were fixed for a given participant and counterbalanced across participants. Each search array had eight items, measuring 1.5° along the longer dimension, arranged in a 3 x 3 grid, with no central element to avoid fixation biases (as shown in <xref rid="fig2" ref-type="fig">Figure 2C</xref>). The entire search array measured 6.5°, with an average inter-item spacing of 2.5°. Item positions were jittered randomly on each trial according to a uniform distribution with range ± 0.2°. Each trial lasted 4 s (1 s ON time and 3 s OFF time), and participants had to respond within 4 s. Each run had 64 unique searches (32 present, 32 absent) presented in random order, using the natural objects from Experiment 1. Target-present searches were chosen randomly from all possible searches such that all 32 images appeared equally often. Target-absent searches included all 32 objects. The location of the target in the target-present searches was chosen such that all eight locations were sampled equally often. In this manner, participants performed 8 such runs of 64 trials each.</p>
</sec>
<sec id="s4b5">
<title>Data acquisition</title>
<p>Participants viewed images projected on a screen through a mirror placed above their eyes. Functional MRI (fMRI) data were acquired using a 32-channel head coil on a 3T Skyra (Siemens, Mumbai, India) at the HealthCare Global Hospital, Bengaluru. Functional scans were performed using a T2*-weighted gradient-echo-planar imaging sequence with the following parameters: repetition time (TR) = 2s, echo time (TE) = 28 ms, flip angle = 79°, voxel size = 3 × 3 × 3 mm<sup>3</sup>, field of view = 192 × 192 mm<sup>2</sup>, and 33 axial-oblique slices for whole-brain coverage. Anatomical scans were performed using T1-weighted images with the following parameters: TR = 2.30 s, TE = 1.99 ms, flip angle = 9°, voxel size = 1 × 1 × 1 mm<sup>3</sup>, field of view = 256 × 256 × 176 mm<sup>3</sup>.</p>
</sec>
<sec id="s4b6">
<title>Data preprocessing</title>
<p>The raw fMRI data were preprocessed using Statistical Parametric Mapping (SPM) software (Version12; Welcome Center for Human Neuroimaging; <ext-link ext-link-type="uri" xlink:href="https://www.fil.ion.ucl.ac.uk/spm/software/spm12/">https://www.fil.ion.ucl.ac.uk/spm/software/spm12/</ext-link>), running on MATLAB 2019b. Raw images were realigned, slice-time corrected, co-registered to the anatomical image, segmented, and normalized to the Montreal Neurological Institute (MNI) 305 anatomical template. Repeating the key analyses with voxel activations estimated from individual subjects yielded qualitatively similar results. Smoothing was performed only on the functional localizer blocks using a Gaussian kernel with a full-width half-maximum of 5 mm. Default SPM parameters were used, and voxel size after normalization was kept at 3×3×3 mm<sup>3</sup>. The data were further processed using GLMdenoise (<xref ref-type="bibr" rid="c17">Kay et al., 2013</xref>). GLMdenoise improves the signal-to-noise ratio in the data by regressing out the noise estimated from task-unrelated voxels. The denoised time-series data were modeled using generalized linear modeling in SPM after removing low-frequency drift using a high-pass filter with a cutoff of 128 s. In the main experiment, the activity of each voxel was modeled using 83 regressors (68 stimuli + 1 fixation + 6 motion regressors + 8 runs). In the localizer block, each voxel was modeled using 14 regressors (6 stimuli + 1 fixation + 6 motion regressors + 1 run).</p>
</sec>
<sec id="s4b7">
<title>ROI definitions</title>
<p>The regions of interest (ROI) of Early Visual Cortex (EVC) and Lateral Occipital (LO) regions were defined using anatomical masks from the SPM anatomy toolbox (<xref ref-type="bibr" rid="c11">Eickhoff et al., 2005</xref>). All brain maps were visualized on the inflated brain using Freesurfer (<ext-link ext-link-type="uri" xlink:href="https://surfer.nmr.mgh.harvard.edu/fswiki/">https://surfer.nmr.mgh.harvard.edu/fswiki/</ext-link>).</p>
</sec>
<sec id="s4b8">
<title>Behavioral data analysis</title>
<p>Response times faster than 0.3 seconds or slower than 3 seconds were removed from the data. This step removed only 0.75% of the data and improved the overall response time consistency, but did not qualitatively alter the results.</p>
</sec>
<sec id="s4b9">
<title>Model fitting for visual homogeneity</title>
<p>We took the multi-dimensional embedding returned by the perceptual space experiment (Experiment 1) in 5 dimensions as the responses of 5 artificial neurons to the entire set of objects. For each target-present array, we calculated the neural response as the average of the responses elicited by these 5 neurons to the target and distractor items. Likewise, for target-absent search arrays, the neural response was simply the response elicited by these 5 neurons to the distractor item in the search array. To estimate the visual homogeneity of the target-present and target-absent search arrays, we calculated the distance of each of these arrays from a single point in the multidimensional representation. We then calculated the correlation between the visual homogeneity calculated relative to this point and the response times for the target-present and target-absent search arrays. The 5 coordinates of this center point was adjusted using constrained nonlinear optimization to maximize the difference between correlations with the target-present &amp; target-absent response times respectively. This optimum center remained stable across many random starting points, and our results were qualitatively similar upon varying the number of embedding dimensions.</p>
<p>Additionally, we performed a leave-one-out cross-validation analysis to validate the number of dimensions or neurons used for the multidimensional scaling analysis in the visual homogeneity model fits. For each choice of number of dimensions, we estimated the optimal centre for visual homogeneity calculations while leaving out all searches involving a single image. We then calculated the visual homogeneity for all the target-present and target-absent searches involving the left-out image. Compiling these predictions by leaving out all images by turn results in a leave-one-out predicted visual homogeneity, which we correlated with the target-present and target-absent response times. We found that the absolute sum of the correlations between visual homogeneity and present/absent reaction times increased monotonically from 1 to 5 neurons, remained at a steady level from 5 to 9 neurons and decreased beyond 9 neurons. Furthermore, the visual homogeneity using the optimal center is highly correlated for 5-9 neurons. We therefore selected 5 neurons or dimensions for reporting visual homogeneity computations.</p>
</sec>
<sec id="s4b10">
<title>Searchlight maps for mean activation (<italic><xref rid="fig3" ref-type="fig">Figure 3A</xref></italic>, <italic><xref rid="fig5" ref-type="fig">Figure 5A</xref></italic>)</title>
<p>To characterize the brain regions that encode visual homogeneity, we performed a whole-brain searchlight analysis. For each voxel, we took the voxels in a 3×3×3 neighborhood and calculated the mean activations across these voxels across all participants. To avoid overall activation level differences between target-present and target-absent searches, we z-scored the mean activations separately across target-present and target-absent searches. Similarly, we calculated the visual homogeneity model predictions from behaviour, and z-scored the visual homogeneity values for target-present and target-absent searches separately. We then calculated the correlation between the normalized mean activations and the normalized visual homogeneity for each voxel, and displayed this as a colormap on the inflated MNI brain template in <xref rid="fig3" ref-type="fig">Figures 3A</xref> &amp; <xref rid="fig5" ref-type="fig">5A</xref>.</p>
<p>Note that the z-scoring of mean activations and visual homogeneity removes any artefactual correlation between mean activation and visual homogeneity arising simply due to overall level differences in mean activation or visual homogeneity itself, but does not alter the overall positive correlation between the visual homogeneity and mean activation across individual search displays.</p>
</sec>
<sec id="s4b11">
<title>Searchlight maps for neural and behavioural dissimilarity (<italic><xref rid="fig3" ref-type="fig">Figure 3B</xref></italic>, <italic><xref rid="fig5" ref-type="fig">Figure 5B</xref></italic>)</title>
<p>To characterize the brain regions that encode perceptual dissimilarity, we performed a whole-brain searchlight analysis. For each voxel, we took the voxel activations in a 3×3×3 neighborhood to target-absent displays as a proxy for the neural response to the single image. For each image pair, we calculated the pair-wise Euclidean distance between the 27-dimensional voxel activations evoked by the two images, and averaged this distance across participants to get a single average distance. For 32 target-absent displays in the experiment, taking all possible pairwise distances results in <sup>32</sup>C<sub>2</sub> = 496 pairwise distances. Similarly, we obtained the same 496 pairwise perceptual dissimilarities between these items from the oddball detection task (Experiment 1). We then calculated the correlation between the mean neural dissimilarities at each voxel with perceptual dissimilarities, and displayed this as a colormap on the flattened MNI brain template in <xref rid="fig3" ref-type="fig">Figures 3B</xref> &amp; <xref rid="fig5" ref-type="fig">5B</xref>.</p>
</sec>
</sec>
<sec id="s4c">
<title>Experiment 3. Oddball detection for perceptual space (Symmetric/Asymmetric objects)</title>
<sec id="s4c1">
<title>Participants</title>
<p>A total of 15 participants (11 males, 22.8 ± 4.3 years) participated in this experiment.</p>
</sec>
<sec id="s4c2">
<title>Paradigm</title>
<p>Participants performed an oddball visual search task. Participants completed 4032 correct trials (<sup>64</sup>C<sub>2</sub> shape pairs x 2 repetitions) as two sessions in two days. We used a total of 64 baton shapes (32 symmetric and 32 asymmetric), and all shapes were presented against a black background. We created 32 unique parts with the vertical line as part of the contour. We created 32 symmetric by joining the part and its mirror-filled version, and 32 asymmetric objects were created by randomly pairing the left part and mirror flipped version of another left part. All parts were occurring equally likely. All other task details are the same as Experiment 1.</p>
</sec>
</sec>
<sec id="s4d">
<title>Experiment 4 Symmetry judgment task (fMRI &amp; behavior)</title>
<sec id="s4d1">
<title>Participants</title>
<p>A total of 15 subjects participated in this study. Participants had normal or corrected to normal vision. Participants had no history of neurological or psychiatric impairment. We excluded participants with metal implants or claustrophobia from the study.</p>
</sec>
<sec id="s4d2">
<title>Paradigm</title>
<p>Inside the scanner, participants performed two runs of one-back identity task (functional localizer), eight runs of symmetry judgment task (event-related design), and one anatomical scan. We excluded the data from one participant due to poor accuracy and long response times.</p>
</sec>
<sec id="s4d3">
<title>Symmetry Task</title>
<p>On each trial, participants had to report whether a briefly presented object was symmetric or not using a keypress. Objects measured 4° and were presented against a black background. Response keys were counterbalanced across participants. Each trial lasted 4 s, with the object displayed for 200 ms followed by a blank period of 3800 ms. Participants could respond at any time following appearance of the object, up to a time out of 4 s after which the next trial began. Each run had 64 unique conditions (32 symmetric and 32 asymmetric).</p>
</sec>
<sec id="s4d4">
<title>1-back task for functional localizers</title>
<p>Stimuli were presented as blocks, and participants reported repeated stimuli with a keypress. Each run had blocks of either silhouettes (asymmetric/symmetric), dot patterns (asymmetric/symmetric), combination of baton and dot patterns (asymmetric/symmetric) and natural objects (intact/scrambled).</p>
</sec>
</sec>
<sec id="s4e">
<title>Data Analysis</title>
<sec id="s4e1">
<title>Noise Removal in RT</title>
<p>Very fast (&lt; 100 ms) reaction times were removed. We also discarded all reaction times to an object if participant’s accuracy was less than 80%. This process removed 3.6% of RT data.</p>
</sec>
<sec id="s4e2">
<title>Model fitting for visual homogeneity</title>
<p>We proceeded as before by embedding the oddball detection response times into multidimensional space with 3 dimensions. For each image, the visual homogeneity was defined as its distance from an optimum center. We then calculated the correlation between the visual homogeneity calculated relative to this optimum center and the response times for the target-present and target-absent search arrays separately. This optimum center was estimated using a constrained nonlinear optimization to maximize the difference between the correlations for asymmetric object response times and symmetric object response times. Other details were the same as in Experiment-2.</p>
</sec>
</sec>
</sec>
<sec id="d1e1281" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1363">
<label>Supplementary Text</label>
<media xlink:href="supplements/518965_file02.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Agrawal</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hari</surname> <given-names>K</given-names></string-name>, <string-name><surname>Arun</surname> <given-names>SP</given-names></string-name> (<year>2020</year>) <article-title>A compositional neural code in high-level visual cortex can explain jumbled word reading</article-title>. <source>Elife</source> <volume>9</volume>:<fpage>e54846</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/32369017">http://www.ncbi.nlm.nih.gov/pubmed/32369017</ext-link>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Arun</surname> <given-names>SP</given-names></string-name> (<year>2012</year>) <article-title>Turning visual search time on its head</article-title>. <source>Vision Res</source> <volume>74</volume>:<fpage>86</fpage>–<lpage>92</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Ashby</surname> <given-names>FG</given-names></string-name>, <string-name><surname>Maddox</surname> <given-names>WT</given-names></string-name> (<year>2011</year>) <article-title>Human category learning 2.0</article-title>. <source>Ann N Y Acad Sci</source> <volume>1224</volume>:<fpage>147</fpage>–<lpage>161</lpage> Available at: <pub-id pub-id-type="doi">10.1111/j.1749-6632.2010.05874.x</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Ayzenberg</surname> <given-names>V</given-names></string-name>, <string-name><surname>Kamps</surname> <given-names>FS</given-names></string-name>, <string-name><surname>Dilks</surname> <given-names>DD</given-names></string-name>, <string-name><surname>Lourenco</surname> <given-names>SF</given-names></string-name> (<year>2022</year>) <article-title>Skeletal representations of shape in the human visual cortex</article-title>. <source>Neuropsychologia</source> <volume>164</volume>:<fpage>108092</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/34801519">http://www.ncbi.nlm.nih.gov/pubmed/34801519</ext-link>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Bainbridge</surname> <given-names>WA</given-names></string-name>, <string-name><surname>Dilks</surname> <given-names>DD</given-names></string-name>, <string-name><surname>Oliva</surname> <given-names>A</given-names></string-name> (<year>2017</year>) <article-title>Memorability: A stimulus-driven perceptual neural signature distinctive from memory</article-title>. <source>Neuroimage</source> <volume>149</volume>:<fpage>141</fpage>–<lpage>152</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/28132932">http://www.ncbi.nlm.nih.gov/pubmed/28132932</ext-link>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Bertamini</surname> <given-names>M</given-names></string-name>, <string-name><surname>Makin</surname> <given-names>ADJ</given-names></string-name> (<year>2014</year>) <article-title>Brain Activity in Response to Visual Symmetry</article-title>. <source>Symmetry (Basel)</source> <volume>6</volume>:<fpage>975</fpage>–<lpage>996</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.mdpi.com/2073-8994/6/4/975">http://www.mdpi.com/2073-8994/6/4/975</ext-link>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Bracci</surname> <given-names>S</given-names></string-name>, <string-name><given-names>Op</given-names> <surname>de Beeck H</surname></string-name> (<year>2015</year>) <article-title>Dissociations and associations between shape and category representations in the two visual pathways</article-title>. <source>J Neurosci</source> <volume>36</volume>:<fpage>432</fpage>–<lpage>444</lpage> Available at: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2314-15.2016</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Brainard</surname> <given-names>DH</given-names></string-name> (<year>1997</year>) <article-title>The Psychophysics Toolbox</article-title>. <source>Spat Vis</source> <volume>10</volume>:<fpage>433</fpage>–<lpage>436</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Carlin</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name> (<year>2017</year>) <article-title>Adjudicating between face-coding models with individual-face fMRI responses</article-title>. <source>PLoS Comput Biol</source> <volume>13</volume>:<fpage>e1005604</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/28746335">http://www.ncbi.nlm.nih.gov/pubmed/28746335</ext-link> [Accessed <date-in-citation content-type="access-date">September 9, 2017</date-in-citation>].</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Duncan</surname> <given-names>J</given-names></string-name>, <string-name><surname>Humphreys</surname> <given-names>GW</given-names></string-name> (<year>1989</year>) <article-title>Visual search and stimulus similarity</article-title>. <source>Psychol Rev</source> <volume>96</volume>:<fpage>433</fpage>–<lpage>458</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.421.4379%7B&amp;%7Drep=rep1%7B&amp;%7Dtype=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.421.4379%7B&amp;%7Drep=rep1%7B&amp;%7Dtype=pdf</ext-link>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Eickhoff</surname> <given-names>SB</given-names></string-name>, <string-name><surname>Stephan</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Mohlberg</surname> <given-names>H</given-names></string-name>, <string-name><surname>Grefkes</surname> <given-names>C</given-names></string-name>, <string-name><surname>Fink</surname> <given-names>GR</given-names></string-name>, <string-name><surname>Amunts</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zilles</surname> <given-names>K</given-names></string-name> (<year>2005</year>) <article-title>A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data</article-title>. <source>Neuroimage</source> <volume>25</volume>:<fpage>1325</fpage>–<lpage>1335</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Goulet</surname> <given-names>M-A</given-names></string-name> (<year>2020</year>) <article-title>Investigation of the Cognitive Mechanisms of Same and Different Judgments</article-title>. <source>Available at</source>: <pub-id pub-id-type="doi">10.20381/ruor-24882</pub-id>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Haushofer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Livingstone</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name> (<year>2008</year>) <article-title>Multivariate patterns in object-selective cortex dissociate perceptual and physical shape similarity</article-title>. <source>PLoS Biol</source> <volume>6</volume>:<fpage>e187</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/18666833">http://www.ncbi.nlm.nih.gov/pubmed/18666833</ext-link>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Jacob</surname> <given-names>G</given-names></string-name>, <string-name><surname>Arun</surname> <given-names>SP</given-names></string-name> (<year>2020</year>) <article-title>How the forest interacts with the trees: Multiscale shape integration explains global and local processing</article-title>. <source>J Vis</source> <volume>20</volume>:<fpage>20</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/33107916">http://www.ncbi.nlm.nih.gov/pubmed/33107916</ext-link>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Jaegle</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mehrpour</surname> <given-names>V</given-names></string-name>, <string-name><surname>Mohsenzadeh</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Meyer</surname> <given-names>T</given-names></string-name>, <string-name><surname>Oliva</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rust</surname> <given-names>N</given-names></string-name> (<year>2019</year>) <article-title>Population response magnitude variation in inferotemporal cortex predicts image memorability</article-title>. <source>Elife</source> <volume>8</volume>:<fpage>e47596</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/31464687">http://www.ncbi.nlm.nih.gov/pubmed/31464687</ext-link>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Katti</surname> <given-names>H</given-names></string-name>, <string-name><surname>Arun</surname> <given-names>SP</given-names></string-name> (<year>2022</year>) <article-title>A separable neural code in monkey IT enables perfect CAPTCHA decoding</article-title>. <source>J Neurophysiol</source> <volume>127</volume>:<fpage>869</fpage>–<lpage>884</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/35196158">http://www.ncbi.nlm.nih.gov/pubmed/35196158</ext-link>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Kay</surname> <given-names>KN</given-names></string-name>, <string-name><surname>Rokem</surname> <given-names>A</given-names></string-name>, <string-name><surname>Winawer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Dougherty</surname> <given-names>RF</given-names></string-name>, <string-name><surname>Wandell</surname> <given-names>BA</given-names></string-name> (<year>2013</year>) <article-title>GLMdenoise: A fast, automated technique for denoising task-based fMRI data</article-title>. <source>Front Neurosci</source> <volume>7</volume>:<fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Kim</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ricci</surname> <given-names>M</given-names></string-name>, <string-name><surname>Serre</surname> <given-names>T</given-names></string-name> (<year>2018</year>) <article-title>Not-So-CLEVR: Learning same-different relations strains feedforward neural networks</article-title>. <source>Interface Focus</source> <volume>8</volume>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name>, <string-name><surname>Mur</surname> <given-names>M</given-names></string-name>, <string-name><surname>Ruff</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Kiani</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bodurka</surname> <given-names>J</given-names></string-name>, <string-name><surname>Esteky</surname> <given-names>H</given-names></string-name>, <string-name><surname>Tanaka</surname> <given-names>K</given-names></string-name>, <string-name><surname>Bandettini</surname> <given-names>PA</given-names></string-name> (<year>2008</year>) <article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title>. <source>Neuron</source> <volume>60</volume>:<fpage>1126</fpage>–<lpage>1141</lpage> Available at: <pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Leopold</surname> <given-names>D a</given-names></string-name>, <string-name><surname>Bondar</surname> <given-names>I V</given-names></string-name>, <string-name><surname>Giese</surname> <given-names>M a</given-names></string-name> (<year>2006</year>) <article-title>Norm-based face encoding by single neurons in the monkey inferotemporal cortex</article-title>. <source>Nature</source> <volume>442</volume>:<fpage>572</fpage>–<lpage>575</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Lukavský</surname> <given-names>J</given-names></string-name>, <string-name><surname>Děchtěrenko</surname> <given-names>F</given-names></string-name> (<year>2017</year>) <article-title>Visual properties and memorising scenes: Effects of image-space sparseness and uniformity. Attention, Perception</article-title>, <source>Psychophys</source> <volume>79</volume>:<fpage>2044</fpage>–<lpage>2054</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Mohan</surname> <given-names>K</given-names></string-name>, <string-name><surname>Arun</surname> <given-names>SP</given-names></string-name> (<year>2012</year>) <article-title>Similarity relations in visual search predict rapid visual categorization</article-title>. <source>J Vis</source> <volume>12</volume>:<fpage>19</fpage>–<lpage>19</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Murdock</surname> <given-names>BB</given-names></string-name> (<year>1960</year>) <article-title>The distinctiveness of stimuli</article-title>. <source>Psychol Rev</source> <volume>67</volume>:<fpage>16</fpage>–<lpage>31</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Nickerson</surname> <given-names>RS</given-names></string-name> (<year>1967</year>) <article-title>“Same”-“different” response times with multi-attribute stimulus differences</article-title>. <source>Percept Mot Skills</source> <volume>24</volume>:<fpage>543</fpage>–<lpage>554</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/6040229">http://www.ncbi.nlm.nih.gov/pubmed/6040229</ext-link>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Nickerson</surname> <given-names>RS</given-names></string-name> (<year>1969</year>) <article-title>‘Same’-‘different’ response times: A model and a preliminary test</article-title>. <source>Acta Psychol (Amst)</source> <volume>30</volume>:<fpage>257</fpage>–<lpage>275</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/0001691869900547">https://linkinghub.elsevier.com/retrieve/pii/0001691869900547</ext-link>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Op de Beeck</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wagemans</surname> <given-names>J</given-names></string-name>, <string-name><surname>Vogels</surname> <given-names>R</given-names></string-name> (<year>2001</year>) <article-title>Inferotemporal neurons represent low-dimensional configurations of parameterized shapes</article-title>. <source>Nat Neurosci</source> <volume>4</volume>:<fpage>1244</fpage>–<lpage>1252</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Peelen</surname> <given-names>M V.</given-names></string-name>, <string-name><surname>Kastner</surname> <given-names>S</given-names></string-name> (<year>2014</year>) <article-title>Attention in the real world: Toward understanding its neural basis</article-title>. <source>Trends Cogn Sci</source> <volume>18</volume>:<fpage>242</fpage>–<lpage>250</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Peirce</surname> <given-names>J</given-names></string-name>, <string-name><surname>Gray</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Simpson</surname> <given-names>S</given-names></string-name>, <string-name><surname>MacAskill</surname> <given-names>M</given-names></string-name>, <string-name><surname>Höchenberger</surname> <given-names>R</given-names></string-name>, <string-name><surname>Sogo</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kastman</surname> <given-names>E</given-names></string-name>, <string-name><surname>Lindeløv</surname> <given-names>JK</given-names></string-name> (<year>2019</year>) <article-title>PsychoPy2: Experiments in behavior made easy</article-title>. <source>Behav Res Methods</source> <volume>51</volume>:<fpage>195</fpage>–<lpage>203</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Petrov</surname> <given-names>AA</given-names></string-name> (<year>2009</year>) <article-title>Symmetry-based methodology for decision-rule identification in same-different experiments</article-title>. <source>Psychon Bull Rev</source> <volume>16</volume>:<fpage>1011</fpage>–<lpage>1025</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Pramod</surname> <given-names>RT</given-names></string-name>, <string-name><surname>Arun</surname> <given-names>SP</given-names></string-name> (<year>2014</year>) <article-title>Features in visual search combine linearly</article-title>. <source>J Vis</source> <volume>14</volume>:<fpage>1</fpage>–<lpage>20</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3980647&amp;tool=pmcentrez&amp;rendertype=abstract">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3980647&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Pramod</surname> <given-names>RT</given-names></string-name>, <string-name><surname>Arun</surname> <given-names>SP</given-names></string-name> (<year>2016</year>) <article-title>Object attributes combine additively in visual search</article-title>. <source>J Vis</source> <volume>16</volume>:<fpage>8</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/26967014">http://www.ncbi.nlm.nih.gov/pubmed/26967014</ext-link>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Pramod</surname> <given-names>RT</given-names></string-name>, <string-name><surname>Arun</surname> <given-names>SP</given-names></string-name> (<year>2018</year>) <article-title>Symmetric Objects Become Special in Perception Because of Generic Computations in Neurons</article-title>. <source>Psychol Sci</source> <volume>29</volume>:<fpage>95</fpage>–<lpage>109</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://journals.sagepub.com/doi/10.1177/0956797617729808">http://journals.sagepub.com/doi/10.1177/0956797617729808</ext-link> [Accessed <date-in-citation content-type="access-date">December 12, 2017</date-in-citation>].</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Proklova</surname> <given-names>D</given-names></string-name>, <string-name><surname>Kaiser</surname> <given-names>D</given-names></string-name>, <string-name><surname>Peelen M</surname> <given-names>V</given-names></string-name> (<year>2016</year>) <article-title>Disentangling Representations of Object Shape and Object Category in Human Visual Cortex: The Animate-Inanimate Distinction</article-title>. <source>J Cogn Neurosci</source> <volume>28</volume>:<fpage>680</fpage>–<lpage>692</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/26765944">http://www.ncbi.nlm.nih.gov/pubmed/26765944</ext-link>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Rhodes</surname> <given-names>G</given-names></string-name>, <string-name><surname>Jeffery</surname> <given-names>L</given-names></string-name> (<year>2006</year>) <article-title>Adaptive norm-based coding of facial identity</article-title>. <source>Vision Res</source> <volume>46</volume>:<fpage>2977</fpage>–<lpage>2987</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Ricci</surname> <given-names>M</given-names></string-name>, <string-name><surname>Cadène</surname> <given-names>R</given-names></string-name>, <string-name><surname>Serre</surname> <given-names>T</given-names></string-name> (<year>2021</year>) <article-title>Same-different conceptualization: a machine vision perspective</article-title>. <source>Curr Opin Behav Sci</source> <volume>37</volume>:<fpage>47</fpage>–<lpage>55</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Rust</surname> <given-names>NC</given-names></string-name>, <string-name><surname>Mehrpour</surname> <given-names>V</given-names></string-name> (<year>2020</year>) <article-title>Understanding Image Memorability</article-title>. <source>Trends Cogn Sci</source> <volume>24</volume>:<fpage>557</fpage>–<lpage>568</lpage> Available at: <pub-id pub-id-type="doi">10.1016/j.tics.2020.04.001</pub-id>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Sani</surname> <given-names>I</given-names></string-name>, <string-name><surname>Stemmann</surname> <given-names>H</given-names></string-name>, <string-name><surname>Caron</surname> <given-names>B</given-names></string-name>, <string-name><surname>Bullock</surname> <given-names>D</given-names></string-name>, <string-name><surname>Stemmler</surname> <given-names>T</given-names></string-name>, <string-name><surname>Fahle</surname> <given-names>M</given-names></string-name>, <string-name><surname>Pestilli</surname> <given-names>F</given-names></string-name>, <string-name><surname>Freiwald</surname> <given-names>WA</given-names></string-name> (<year>2021</year>) <article-title>The human endogenous attentional control network includes a ventro-temporal cortical node</article-title>. <source>Nat Commun</source> <volume>12</volume>:<fpage>360</fpage> Available at: <pub-id pub-id-type="doi">10.1038/s41467-020-20583-5</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Sasaki</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Vanduffel</surname> <given-names>W</given-names></string-name>, <string-name><surname>Knutsen</surname> <given-names>T</given-names></string-name>, <string-name><surname>Tyler</surname> <given-names>C</given-names></string-name>, <string-name><surname>Tootell</surname> <given-names>R</given-names></string-name> (<year>2005</year>) <article-title>Symmetry activates extrastriate visual cortex in human and nonhuman primates</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>102</volume>:<fpage>3159</fpage>–<lpage>3163</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/15710884">http://www.ncbi.nlm.nih.gov/pubmed/15710884</ext-link>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Serre</surname> <given-names>T</given-names></string-name> (<year>2019</year>) <article-title>Deep Learning: The Good, the Bad, and the Ugly</article-title>. <source>Annu Rev Vis Sci</source> <volume>5</volume>:<fpage>399</fpage>–<lpage>426</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="https://www.annualreviews.org/doi/10.1146/annurev-vision-091718-014951">https://www.annualreviews.org/doi/10.1146/annurev-vision-091718-014951</ext-link>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Sripati</surname> <given-names>AP</given-names></string-name>, <string-name><surname>Olson</surname> <given-names>CR</given-names></string-name> (<year>2010</year>) <article-title>Global Image Dissimilarity in Macaque Inferotemporal Cortex Predicts Human Visual Search Efficiency</article-title>. <source>J Neurosci</source> <volume>30</volume>:<fpage>1258</fpage>–<lpage>1269</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.1908-09.2010">http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.1908-09.2010</ext-link>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Stewart</surname> <given-names>N</given-names></string-name>, <string-name><surname>Morin</surname> <given-names>C</given-names></string-name> (<year>2007</year>) <article-title>Dissimilarity is used as evidence of category membership in multidimensional perceptual categorization: a test of the similarity-dissimilarity generalized context model</article-title>. <source>Q J Exp Psychol (Hove)</source> <volume>60</volume>:<fpage>1337</fpage>–<lpage>1346</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.tandfonline.com/doi/abs/10.1080/17470210701480444%7B%25%7D5Cnpapers3://publication/doi/10.1080/17470210701480444">http://www.tandfonline.com/doi/abs/10.1080/17470210701480444%7B%25%7D5Cnpapers3://publication/doi/10.1080/17470210701480444</ext-link>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Storrs</surname> <given-names>KR</given-names></string-name>, <string-name><surname>Kietzmann</surname> <given-names>TC</given-names></string-name>, <string-name><surname>Walther</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mehrer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name> (<year>2021</year>) <article-title>Diverse Deep Neural Networks All Predict Human Inferior Temporal Cortex Well, After Training and Fitting</article-title>. <source>J Cogn Neurosci</source> <volume>33</volume>:<fpage>2044</fpage>–<lpage>2064</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/34272948">http://www.ncbi.nlm.nih.gov/pubmed/34272948</ext-link>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Sunder</surname> <given-names>S</given-names></string-name>, <string-name><surname>Arun</surname> <given-names>SP</given-names></string-name> (<year>2016</year>) <article-title>Look before you seek: Preview adds a fixed benefit to all searches</article-title>. <source>J Vis</source> <volume>16</volume>:<fpage>3</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/27919099">http://www.ncbi.nlm.nih.gov/pubmed/27919099</ext-link>%5Cn <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5142796">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5142796</ext-link>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Taylor</surname> <given-names>KI</given-names></string-name>, <string-name><surname>Devereux</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Acres</surname> <given-names>K</given-names></string-name>, <string-name><surname>Randall</surname> <given-names>B</given-names></string-name>, <string-name><surname>Tyler</surname> <given-names>LK</given-names></string-name> (<year>2012</year>) <article-title>Contrasting effects of feature-based statistics on the categorisation and basic-level identification of visual objects</article-title>. <source>Cognition</source> <volume>122</volume>:<fpage>363</fpage>–<lpage>374</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/22137770">http://www.ncbi.nlm.nih.gov/pubmed/22137770</ext-link>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Thorat</surname> <given-names>S</given-names></string-name>, <string-name><surname>Proklova</surname> <given-names>D</given-names></string-name>, <string-name><surname>Peelen M</surname> <given-names>V</given-names></string-name> (<year>2019</year>) <article-title>The nature of the animacy organization in human ventral temporal cortex</article-title>. <source>Elife</source> <volume>8</volume> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/31496518">http://www.ncbi.nlm.nih.gov/pubmed/31496518</ext-link>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Tyler</surname> <given-names>LK</given-names></string-name>, <string-name><surname>Chiu</surname> <given-names>S</given-names></string-name>, <string-name><surname>Zhuang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Randall</surname> <given-names>B</given-names></string-name>, <string-name><surname>Devereux</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Wright</surname> <given-names>P</given-names></string-name>, <string-name><surname>Clarke</surname> <given-names>A</given-names></string-name>, <string-name><surname>Taylor</surname> <given-names>KI</given-names></string-name> (<year>2013</year>) <article-title>Objects and categories: feature statistics and object processing in the ventral stream</article-title>. <source>J Cogn Neurosci</source> <volume>25</volume>:<fpage>1723</fpage>–<lpage>1735</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/23662861">http://www.ncbi.nlm.nih.gov/pubmed/23662861</ext-link>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Valentine</surname> <given-names>T</given-names></string-name> (<year>1991</year>) <article-title>A Unified Account of the Effects of Distinctiveness, Inversion, and Race in Face Recognition</article-title>. <source>Q J Exp Psychol Sect A</source> <volume>43</volume>:<fpage>161</fpage>–<lpage>204</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Valentine</surname> <given-names>T</given-names></string-name>, <string-name><surname>Bruce</surname> <given-names>V</given-names></string-name> (<year>1986a</year>) <article-title>Recognizing familiar faces: The role of distinctiveness and familiarity</article-title>. <source>Can J Psychol Can Psychol</source> <volume>40</volume>:<fpage>300</fpage>–<lpage>305</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Valentine</surname> <given-names>T</given-names></string-name>, <string-name><surname>Bruce</surname> <given-names>V</given-names></string-name> (<year>1986b</year>) <article-title>The Effects of Distinctiveness in Recognising and Classifying Faces</article-title>. <source>Perception</source> <volume>15</volume>:<fpage>525</fpage>–<lpage>535</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="web"><string-name><surname>Van Meel</surname> <given-names>C</given-names></string-name>, <string-name><surname>Baeck</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gillebert</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Wagemans</surname> <given-names>J</given-names></string-name>, <string-name><given-names>Op</given-names> <surname>de Beeck HP</surname></string-name> (<year>2019</year>) <article-title>The representation of symmetry in multi-voxel response patterns and functional connectivity throughout the ventral visual stream</article-title>. <source>Neuroimage</source> Available at: <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1053811919301247">https://linkinghub.elsevier.com/retrieve/pii/S1053811919301247</ext-link>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Verghese</surname> <given-names>P</given-names></string-name> (<year>2001</year>) <article-title>Visual search and attention: A signal detection theory approach</article-title>. <source>Neuron</source> <volume>31</volume>:<fpage>523</fpage>–<lpage>535</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Wagemans</surname> <given-names>J</given-names></string-name> (<year>1997</year>) <article-title>Characteristics and models of human symmetry detection</article-title>. <source>Trends Cogn Sci</source> <volume>1</volume>:<fpage>346</fpage>–<lpage>352</lpage> Available at: <pub-id pub-id-type="doi">10.1016/S1364-6613(97)01105-4</pub-id>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Wolfe</surname> <given-names>JM</given-names></string-name> (<year>1998</year>) <article-title>What Can 1 Million Trials Tell Us About Visual Search?</article-title> <source>Psychol Sci</source> <volume>9</volume>:<fpage>33</fpage>–<lpage>39</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://pss.sagepub.com/content/9/1/33.abstract">http://pss.sagepub.com/content/9/1/33.abstract</ext-link>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Wolfe</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Horowitz</surname> <given-names>TS</given-names></string-name> (<year>2004</year>) <article-title>What attributes guide the deployment of visual attention and how do they do it?</article-title> <source>Nat Rev Neurosci</source> <volume>5</volume>:<fpage>495</fpage>–<lpage>501</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Wolfe</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Horowitz</surname> <given-names>TS</given-names></string-name> (<year>2017</year>) <article-title>Five factors that guide attention in visual search</article-title>. <source>Nat Hum Behav</source> <volume>1</volume>:<fpage>0058</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.nature.com/articles/s41562-017-0058">http://www.nature.com/articles/s41562-017-0058</ext-link>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Zhivago</surname> <given-names>KA</given-names></string-name>, <string-name><surname>Arun</surname> <given-names>SP</given-names></string-name> (<year>2014</year>) <article-title>Texture discriminability in monkey inferotemporal cortex predicts human texture perception</article-title>. <source>J Neurophysiol</source> <volume>112</volume>:<fpage>2745</fpage>–<lpage>2755</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4254883&amp;tool=pmcentrez&amp;rendertype=abstract">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4254883&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Zoccolan</surname> <given-names>D</given-names></string-name>, <string-name><surname>Cox</surname> <given-names>DD</given-names></string-name>, <string-name><surname>DiCarlo</surname> <given-names>JJ</given-names></string-name> (<year>2005</year>) <article-title>Multiple Object Response Normalization in Monkey Inferotemporal Cortex</article-title>. <source>J Neurosci</source> <volume>25</volume>:<fpage>8150</fpage>–<lpage>8164</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.2058-05.2005">http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.2058-05.2005</ext-link>.</mixed-citation></ref>
</ref-list>
<sec id="s5">
<title>Data availability</title>
<p>All data and code required to reproduce the results will be made available on OSF.</p>
</sec>
<sec id="s6">
<title>Author contributions</title>
<p>GJ and SPA designed the visual search experiments; GJ collected behavioral and fMRI data; GJ &amp; SPA analyzed and interpreted the data; PRT &amp; SPA designed the symmetry experiments; PRT collected fMRI data; GJ, PRT &amp; SPA analyzed and interpreted the data; GJ and SPA wrote the manuscript with inputs from PRT.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank Divya Gulati for help with data collection of Experiments S4 &amp; S5. This research was supported by the DBT/Wellcome Trust India Alliance Senior Fellowship awarded to SPA (Grant# IA/S/17/1/503081). GJ was supported by a Senior Research Fellowship from MHRD, Government of India.</p>
</ack>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93033.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kok</surname>
<given-names>Peter</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Inadequate</kwd>
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study uses carefully designed experiments to generate a <bold>useful</bold> behavioural and neuroimaging dataset on visual cognition. The results provide <bold>solid</bold> evidence for the involvement of higher-order visual cortex in processing visual oddballs and asymmetry. However, the evidence provided for the very strong claims of homogeneity as a novel concept in vision science, separable from existing concepts such as target saliency, is <bold>inadequate</bold>.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93033.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors define a new metric for visual displays, derived from psychophysical response times, called visual homogeneity (VH). They attempt to show that VH is explanatory of response times across multiple visual tasks. They use fMRI to find visual cortex regions with VH-correlated activity. On this basis, they declare a new visual region in the human brain, area VH, whose purpose is to represent VH for the purpose of visual search and symmetry tasks.</p>
<p>Strengths:</p>
<p>The authors present carefully designed experiments, combining multiple types of visual judgments and multiple types of visual stimuli with concurrent fMRI measurements. This is a rich dataset with many possibilities for analysis and interpretation.</p>
<p>Weaknesses:</p>
<p>The datasets presented here should provide a rich basis for analysis. However, in this version of the manuscript, I believe that there are major problems with the logic underlying the authors' new theory of visual homogeneity (VH), with the specific methods they used to calculate VH, and with their interpretation of psychophysical results using these methods. These problems with the coherency of VH as a theoretical construct and metric value make it hard to interpret the fMRI results based on searchlight analysis of neural activity correlated with VH. In addition, the large regions of VH correlations identified in Experiments 1 and 2 vs. Experiments 3 and 4 are barely overlapping. This undermines the claim that VH is a universal quantity, represented in a newly discovered area of the visual cortex, that underlies a wide variety of visual tasks and functions.</p>
<p>Maybe I have missed something, or there is some flaw in my logic. But, absent that, I think the authors should radically reconsider their theory, analyses, and interpretations, in light of the detailed comments below, to make the best use of their extensive and valuable datasets combining behavior and fMRI. I think doing so could lead to a much more coherent and convincing paper, albeit possibly supporting less novel conclusions.</p>
<p>THEORY AND ANALYSIS OF VH</p>
<p>1. VH is an unnecessary, complex proxy for response time and target-distractor similarity.</p>
<p>VH is defined as a novel visual quality, calculable for both arrays of objects (as studied in Experiments 1-3) and individual objects (as studied in Experiment 4). It is derived from a center-to-distance calculation in a perceptual space. That space in turn is derived from the multi-dimensional scaling of response times for target-distractor pairs in an oddball detection task (Experiments 1 and 2) or in a same-different task (Experiments 3 and 4). Proximity of objects in the space is inversely proportional to response times for arrays in which they were paired. These response times are higher for more similar objects. Hence, proximity is proportional to similarity. This is visible in Fig. 2B as the close clustering of complex, confusable animal shapes.</p>
<p>VH, i.e. distance-to-center, for target-present arrays, is calculated as shown in Fig. 1C, based on a point on the line connecting the target and distractors. The authors justify this idea with previous findings that responses to multiple stimuli are an average of responses to the constituent individual stimuli. The distance of the connecting line to the center is inversely proportional to the distance between the two stimuli in the pair, as shown in Fig. 2D. As a result, VH is inversely proportional to the distance between the stimuli and thus to stimulus similarity and response times. But this just makes VH a highly derived, unnecessarily complex proxy for target-distractor similarity and response time. The original response times on which the perceptual space is based are far more simple and direct measures of similarity for predicting response times.</p>
<p>1. The use of VH derived from Experiment 1 to predict response times in Experiment 2 is circular and does not validate the VH theory.</p>
<p>The use of VH, a response time proxy, to predict response times in other, similar tasks, using the same stimuli, is circular. In effect, response times are being used to predict response times across two similar experiments using the same stimuli. Experiment 1 and the target present condition of Experiment 2 involve the same essential task of oddball detection. The results of Experiment 1 are converted into VH values as described above, and these are used to predict response times in Experiment 2 (Fig. 2F). Since VH is a derived proxy for response values in Experiment 1, this prediction is circular, and the observed correlation shows only consistency between two oddball detection tasks in two experiments using the same stimuli.</p>
<p>1. The negative correlation of target-absent response times with VH as it is defined for target-absent arrays, based on the distance of a single stimulus from the center, is uninterpretable without understanding the effects of center-fitting. Most likely, center-fitting and the different VH metrics for target-absent trials produce an inverse correlation of VH with target-distractor similarity.</p>
<p>The construction of the VH perceptual space also involves fitting a &quot;center&quot; point such that distances to center predict response times as closely as possible. The effect of this fitting process on distance-to-center values for individual objects or clusters of objects is unknowable from what is presented here. These effects would depend on the residual errors after fitting response times with the connecting line distances. The center point location and its effects on the distance-to-center of single objects and object clusters are not discussed or reported here.</p>
<p>Yet, this uninterpretable distance-to-center of single objects is chosen as the metric for VH of target-absent displays (VHabsent). This is justified by the idea that arrays of a single stimulus will produce an average response equal to one stimulus of the same kind. However, it is not logically clear why response strength to a stimulus should be a metric for homogeneity of arrays constructed from that stimulus, or even what homogeneity could mean for a single stimulus from this set. It is not clear how this VHabsent metric based on single stimuli can be equated to the connecting line VH metric for stimulus pairs, i.e. VHpresent, or how both could be plotted on a single continuum.</p>
<p>It is clear, however, what *should* be correlated with difficulty and response time in the target-absent trials, and that is the complexity of the stimuli and the numerosity of similar distractors in the overall stimulus set. The complexity of the target, similarity with potential distractors, and the number of such similar distractors all make ruling out distractor presence more difficult. The correlation seen in Fig. 2G must reflect these kinds of effects, with higher response times for complex animal shapes with lots of similar distractors and lower response times for simpler round shapes with fewer similar distractors.</p>
<p>The example points in Fig. 2G seem to bear this out, with higher response times for the deer stimulus (complex, many close distractors in the Fig. 2B perceptual space) and lower response times for the coffee cup (simple, few close distractors in the perceptual space). While the meaning of the VH scale in Fig. 2G, and its relationship to the scale in Fig. 2F, are unknown, it seems like the Fig. 2G scale has an inverse relationship to stimulus complexity, in contrast to the expected positive relationship for Fig. 2F. This is presumably what creates the observed negative correlation in Fig. 2G.</p>
<p>Taken together, points 1-3 suggest that VHpresent and VHabsent are complex, unnecessary, and disconnected metrics for understanding target detection response times. The standard, simple explanation should stand. Task difficulty and response time in target detection tasks, in both present and absent trials, are positively correlated with target-distractor similarity.</p>
<p>I think my interpretations apply to Experiments 3 and 4 as well, although I find the analysis in Fig. 4 especially hard to understand. The VH space in this case is based on Experiment 3 oddball detection in a stimulus set that included both symmetric and asymmetric objects. However, the response times for a very different task in Experiment 4, a symmetric/asymmetric judgment, are plotted against the axes derived from Experiment 3 (Fig. 4F and 4G). It is not clear to me why a measure based on oddball detection that requires no use of symmetry information should be predictive of within-stimulus symmetry detection response times. If it is, that requires a theoretical explanation not provided here.</p>
<p>1. Contrary to the VH theory, same/different tasks are unlikely to depend on a decision boundary in the middle of a similarity or homogeneity continuum.</p>
<p>The authors interpret the inverse relationship of response times with VHpresent and VHabsent, described above, as evidence for their theory. They hypothesize, in Fig. 1G, that VHpresent and VHabsent occupy a single scale, with maximum VHpresent falling at the same point as minimum VHabsent. This is not borne out by their analysis, since the VHpresent and VHabsent value scales are mainly overlapping, not only in Experiments 1 and 2 but also in Experiments 3 and 4. The authors dismiss this problem by saying that their analyses are a first pass that will require future refinement. Instead, the failure to conform to this basic part of the theory should be a red flag calling for revision of the theory.</p>
<p>The reason for this single scale is that the authors think of target detection as a boundary decision task, along a single scale, with a decision boundary somewhere in the middle, separating present and absent. This model makes sense for decision dimensions or spaces where there are two categories (right/left motion; cats vs. dogs), separated by an inherent boundary (equal left/right motion; training-defined cat/dog boundary). In these cases, there is less information near the boundary, leading to reduced speed/accuracy and producing a pattern like that shown in Fig. 1G.</p>
<p>This logic does not hold for target detection tasks. There is no inherent middle point boundary between target present and target absent. Instead, in both types of trials, maximum information is present when the target and distractors are most dissimilar, and minimum information is present when the target and distractors are most similar. The point of greatest similarity occurs at the limit of any metric for similarity. Correspondingly, there is no middle point dip in information that would produce greater difficulty and higher response times. Instead, task difficulty and response times increase monotonically with the similarity between targets and distractors, for both target present and target absent decisions. Thus, in Figs. 2F and 2G, response times appear to be highest for animals, which share the largest numbers of closely similar distractors.</p>
<p>DEFINITION OF AREA VH USING fMRI</p>
<p>1. The area VH boundaries from different experiments are nearly completely non-overlapping.</p>
<p>In line with their theory that VH is a single continuum with a decision boundary somewhere in the middle, the authors use fMRI searchlight to find an area whose responses positively correlate with homogeneity, as calculated across all of their target present and target absent arrays. They report VH-correlated activity in regions anterior to LO. However, the VH defined by symmetry Experiments 3 and 4 (VHsymmetry) is substantially anterior to LO, while the VH defined by target detection Experiments 1 and 2 (VHdetection) is almost immediately adjacent to LO. Fig. S13 shows that VHsymmetry and VHdetection are nearly non-overlapping. This is a fundamental problem with the claim of discovering a new area that represents a new quantity that explains response times across multiple visual tasks. In addition, it is hard to understand why VHsymmetry does not show up in a straightforward subtraction between symmetric and asymmetric objects, which should show a clear difference in homogeneity.</p>
<p>1. It is hard to understand how neural responses can be correlated with both VHpresent and VHabsent.</p>
<p>The main paper results for VHdetection are based on both target-present and target-absent trials, considered together. It is hard to interpret the observed correlations, since the VHpresent and VHabsent metrics are calculated in such different ways and have opposite correlations with target similarity, task difficulty, and response times (see above). It may be that one or the other dominates the observed correlations. It would be clarifying to analyze correlations for target-present and target-absent trials separately, to see if they are both positive and correlated with each other.</p>
<p>1. The definition of the boundaries and purpose of a new visual area in the brain requires circumspection, abundant and convergent evidence, and careful controls.</p>
<p>Even if the VH metric, as defined and calculated by the authors here, is a meaningful quantity, it is a bold claim that a large cortical area just anterior to LO is devoted to calculating this metric as its major task. Vision involves much more than target detection and symmetry detection. The cortex anterior to LO is bound to perform a much wider range of visual functionalities. If the reported correlations can be clarified and supported, it would be more circumspect to treat them as one byproduct of unknown visual processing in the cortex anterior to LO, rather than treating them as the defining purpose for a large area of the visual cortex.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93033.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study proposes visual homogeneity as a novel visual property that enables observers perform to several seemingly disparate visual tasks, such as finding an odd item, deciding if two items are the same, or judging if an object is symmetric. In Experiment 1, the reaction times on several objects were measured in human subjects. In Experiment 2, the visual homogeneity of each object was calculated based on the reaction time data. The visual homogeneity scores predicted reaction times. This value was also correlated with the BOLD signals in a specific region anterior to LO. Similar methods were used to analyze reaction time and fMRI data in a symmetry detection task. It is concluded that visual homogeneity is an important feature that enables observers to solve these two tasks.</p>
<p>Strengths:</p>
<p>1. The writing is very clear. The presentation of the study is informative.</p>
<p>2. This study includes several behavioral and fMRI experiments. I appreciate the scientific rigor of the authors.</p>
<p>Weaknesses:</p>
<p>1. My main concern with this paper is the way visual homogeneity is computed. On page 10, lines 188-192, it says: &quot;we then asked if there is any point in this multidimensional representation such that distances from this point to the target-present and target-absent response vectors can accurately predict the target-present and target-absent response times with a positive and negative correlation respectively (see Methods)&quot;. This is also true for the symmetry detection task. If I understand correctly, the reference point in this perceptual space was found by deliberating satisfying the negative and positive correlations in response times. And then on page 10, lines 200-205, it shows that the positive and negative correlations actually exist. This logic is confusing. The positive and negative correlations emerge only because this method is optimized to do so. It seems more reasonable to identify the reference point of this perceptual space independently, without using the reaction time data. Otherwise, the inference process sounds circular. A simple way is to just use the mean point of all objects in Exp 1, without any optimization towards reaction time data.</p>
<p>2. On page 11, lines 214-221. It says: &quot;these findings are non-trivial for several reasons&quot;. However, the first reason is confusing. It is unclear to me why &quot;it suggests that there are highly specific computations that can be performed on perceptual space to solve oddball tasks&quot;. In fact, these two sentences provide no specific explanation for the results.</p>
<p>3. The second reason is interesting. Reaction times in target-present trials can be easily explained by target-distractor similarity. But why does reaction time vary substantially across target-absent stimuli? One possible explanation is that the objects that are distant from the feature distribution elicit shorter reaction times. Here, all objects constitute a statistical distribution in the feature (perceptual) space. There is certainly a mean of this distribution. Some objects look like outliers and these outliers elicit shorter reaction times in the target-absent trials because outlier detection is very salient.</p>
<p>One might argue that the above account is merely a rephrasing of the idea of visual homogeneity proposed in this study. If so, feature saliency is not a new account. In other words, the idea of visual homogeneity is another way of reiterating the old feature saliency theory.</p>
<p>1. One way to reject the feature saliency theory is to compare the reaction times of the objects that are very different from other objects (i.e., no surrounding objects in the perceptual space, e.g., the wheel in the lower right corner of Fig. 2B) with the objects that are surrounded by several similar objects (e.g., the horse in the upper part of Fig. 2B). Also, please choose the two objects with similar distance from the reference point. I predict that the latter will elicit longer reaction times because they can be easily confounded by surrounding similar objects (i.e., four-legged horses can be easily confounded by four-legged dogs). If the density of object distribution per se influences the visual homogeneity score, I would say that the &quot;visual homogeneity&quot; is essentially another way of describing the distributional density of the perceptual space.</p>
<p>2. The searchlight analysis looks strange to me. One can easily perform a parametric modulation by setting visual homogeneity as the trial-by-trial parametric modulator and reaction times as a covariate. This parametric modulation produces a brain map with the correlation of every voxel in the brain. On page 17 lines 340-343, it is unclear to me what the &quot;mean activation&quot; is.</p>
<p>Minor points:</p>
<p>1. In the intro, it says: &quot;using simple neural rules...&quot; actually it is very confusing what &quot;neural rules&quot; are here. Better to change it to &quot;computational principles&quot; or &quot;neural network models&quot;??</p>
<p>2. In the intro, it says: &quot;while machine vision algorithms are extremely successful in solving feature-based tasks like object categorization (Serre, 2019), they struggle to solve these generic tasks (Kim et al., 2018; Ricci et al. 2021). These are not generic tasks. They are just a specific type of visual task-judging relationship between multiple objects. Moreover, a large number of studies in machine vision have shown that DNNs are capable of solving these tasks and even more difficult tasks. Two survey papers are listed here.</p>
<p>Wu, Q., Teney, D., Wang, P., Shen, C., Dick, A., &amp; Van Den Hengel, A. (2017). Visual question answering: A survey of methods and datasets. Computer Vision and Image Understanding, 163, 21-40.</p>
<p>Małkiński, M., &amp; Mańdziuk, J. (2022). Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven's Progressive Matrices. arXiv preprint arXiv:2201.12382.</p>
</body>
</sub-article>
</article>