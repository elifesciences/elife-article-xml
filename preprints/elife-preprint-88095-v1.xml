<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">88095</article-id>
<article-id pub-id-type="doi">10.7554/eLife.88095</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.88095.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>An allocentric human odometer for perceiving distances on the ground plane</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhou</surname>
<given-names>Liu</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wei</surname>
<given-names>Wei</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Ooi</surname>
<given-names>Teng Leng</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6313-9016</contrib-id>
<name>
<surname>He</surname>
<given-names>Zijiang J.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Psychological and Brain Sciences, University of Louisville</institution>, Louisville, Kentucky 40292, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>College of Optometry, The Ohio State University</institution>, Columbus, Ohio 43210, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Meng</surname>
<given-names>Ming</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>South China Normal University</institution>
</institution-wrap>
<city>Guangzhou</city>
<country>China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>E-mail: <email>ooi.22@osu.edu</email>; <email>zjhe@louisville.edu</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-07-17">
<day>17</day>
<month>07</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP88095</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-04-21">
<day>21</day>
<month>04</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-03-24">
<day>24</day>
<month>03</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.22.533725"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Zhou et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Zhou et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-88095-v1.pdf"/>
<abstract>
<title>Abstract</title><p>We reliably judge locations of static objects when we walk despite the retinal images of these objects moving with every step we take. Here, we showed our brains solve this optical illusion by adopting an allocentric spatial reference frame. We measured perceived target location after the observer walked a short distance from the home base. Supporting the allocentric coding scheme, we found the intrinsic bias<sup><xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref></sup>, which acts as a spatial reference frame for perceiving location of a dimly lit target in the dark, remained grounded at the home base rather than traveled along with the observer. The path-integration mechanism responsible for this can utilize both active and passive (vestibular) translational motion signals, but only along the horizontal direction. This anisotropic path-integration finding in human visual space perception is reminiscent of the anisotropic spatial memory finding in desert ants<sup><xref ref-type="bibr" rid="c3">3</xref></sup>, pointing to nature’s wondrous and logically simple design for terrestrial creatures.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>When viewing natural scenes, we readily appreciate the visual space expanse that “fills” the scene being enveloped by the sky above and terrain below our feet (<xref rid="fig1" ref-type="fig">figure 1a</xref>). Notably, within this phenomenological visual space, our visual system employs the prevalent ground surface, where creatures and objects frequently interact, as a reference frame for coding spatial locations<sup><xref ref-type="bibr" rid="c4">4</xref>–<xref ref-type="bibr" rid="c6">6</xref></sup>. Empirical findings have revealed the ground-based spatial coding scheme accurately localizes objects when the horizontal ground surface is continuous and carries rich depth cues<sup><xref ref-type="bibr" rid="c6">6</xref>–<xref ref-type="bibr" rid="c14">14</xref></sup>. Attesting to the significant role of the ground surface, it has been found that object localization becomes inaccurate when the ground surface is disrupted by a gap or an occluding box<sup><xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c13">13</xref></sup>. Similarly, object localization is inaccurate in the dark when the ground is not visible<sup><xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c15">15</xref></sup>. In fact, research conducted in total darkness to prevent the visual system from obtaining depth information from the ground reveals the visual system defaults to using an implicit curved surface representation that we refer to as the <italic><underline>intrinsic bias</underline></italic> to localize objects<sup><xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c16">16</xref></sup>. <xref rid="fig1" ref-type="fig">Figure 1b</xref> illustrates the intrinsic bias’s curved representation in the dark (dashed white curve) that leads to an observer perceiving a dimly lit target (green ring) at the intersection (green disc) between its projection line from the eye and the intrinsic bias. Furthermore, when a parallel row of texture elements on the ground serves as the depth cue (<xref rid="fig1" ref-type="fig">figure 1c</xref>), object localization becomes more accurate. It is as if the objects are now located along a curved reference (solid white curve), which is less slanted than the intrinsic bias. This suggests the intrinsic bias, acting as the prototype spatial reference of the ground surface when visual cues are not visible, integrates with the visible depth cue to form a new ground surface representation<sup><xref ref-type="bibr" rid="c17">17</xref></sup>.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>The ground-based spatial coding scheme. a. Natural scenes are largely enveloped by the prevalent sky above us and the ground on which we stand. Evidence suggests target locations are judged with respect to the ground surface, which is our terrestrial niche. <bold>b.</bold> The visual system uses the intrinsic bias, an implicit curved surface representation in the dark (illustrated as dashed white curve), to locate a dimly-lit target (illustrated as unfilled green disc). The target is perceived (filled green disc) at the intersection between the projection line from the eye to the target and the intrinsic bias. Essentially, the intrinsic bias acts like the visual system’s internal model of the ground surface. <bold>c.</bold> When the ground becomes visible, the intrinsic bias integrates with the visible depth cues to form a ground surface representation, which serves as a reference frame to code target location. For example, the parallel rows of texture elements on the ground (illustrated as filled red circles) provide the depth cue for the visual system to construct a ground surface representation (solid white curve) that is less slanted than the intrinsic bias. This leads to a more accurate target localization than in the dark (b).</p></caption>
<graphic xlink:href="533725v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Nevertheless, current understanding of the ground-based spatial coding scheme is limited to situations where the tested observer stood still. Namely, just as in <xref rid="fig1" ref-type="fig">figure 1b</xref>, now redrawn in <xref rid="fig2" ref-type="fig">figure 2a</xref> and referred to as the <italic>baseline-stationary</italic> condition, the observer is instructed to stand still at one location during the test, which anchors his/her intrinsic bias on the ground under his/her feet. In this way, object location in visual space can be computed relative to the intrinsic bias. However, because retinal images of the stationary environment move when one moves (e.g., walk), it is unclear what additional computation the ground-based spatial coding scheme requires to construct a stable perceptual space. We proposed that during translational movements, such as walking, the visual system anchors the intrinsic bias at one location on the ground surface using a <italic><underline>path integration</underline></italic> mechanism<sup><xref ref-type="bibr" rid="c18">18</xref>–<xref ref-type="bibr" rid="c25">25</xref></sup>. By doing so, the visual system can obtain a stable ground surface representation. Accordingly, the ground-based reference frame can use an allocentric, i.e., world-centered, coordinate system to code locations in visual space. This allocentric hypothesis predicts that during self-motion, the intrinsic bias is fixed to the ground location before the motion begins (home base) and remains at the same ground location during self-motion. This hypothesis stands in contrast to an alternative egocentric coordinate system hypothesis, which predicts the intrinsic bias moves along with the observer’s body. To distinguish between these two hypotheses, we began by measuring the intrinsic bias when the observer walked in the dark. Consider the scenario where an observer walks from his/her home base (blue cross added in figure for illustrative purpose) in the dark and stops at a new location (<xref rid="fig2" ref-type="fig">figure 2b</xref>; red cross added for illustration). According to the allocentric hypothesis, the intrinsic bias (blue) would remain at the home base. Thus, when he/she stops and is presented with a dimly lit test target (illustrated by the green ring), the observer would perceive the target to be nearer and higher (<xref rid="fig2" ref-type="fig">figure 2b</xref>) than if he/she had not walked from the home base (<xref rid="fig2" ref-type="fig">figure 2a</xref>; we shall dub this the <italic>baseline-stationary</italic> condition). In contrast, if the intrinsic bias had moved along with the observer to the new location (illustrated as the grey curve in <xref rid="fig2" ref-type="fig">figure 2c</xref>), as predicted by the alternative egocentric hypothesis, the observer would perceive the target at the same location as that in the <italic>baseline-stationary</italic> condition (<xref rid="fig2" ref-type="fig">figure 2a</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Hypotheses and predictions of Experiment 1. <bold>a.</bold> <italic>Baseline-stationary</italic> condition. A non-moving, static observer perceives the dimly lit target (illustrated as unfilled green disc) at the intersection (illustrated as filled red disc) between the projection line from the eye to the target and the intrinsic bias (dashed white curve). <bold>b.</bold> The allocentric hypothesis predicts when the observer walks forward from the home base (illustrated as blue cross) to a new location (illustrated as red cross), the visual system relies on the path-integration mechanism to keep the intrinsic bias (illustrated as dashed blue curve) at the home base while monitoring the homing error vector (illustrated as white arrow). From the new location, he/she perceives the target (illustrated as unfilled green circle) at the intersection (illustrated as red disk) between the intrinsic bias anchored at the home base and the projection line from the eye to the target. <bold>c.</bold> The egocentric hypothesis predicts the intrinsic bias tags along with the observer’s body (illustrated as gray intrinsic bias) when he/she walks to the new location (illustrated as red cross). From the new location, the target (illustrated as unfilled green circle) is perceived (illustrated as red disk) at the same location as in the <italic>baseline-stationary</italic> condition (in <bold>a</bold>).</p></caption>
<graphic xlink:href="533725v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We have premised the allocentric hypothesis on the ground-based spatial coding scheme employing the path-integration (or spatial updating) system to maintain the intrinsic bias at the home base (blue cross in <xref rid="fig2" ref-type="fig">figure 2b</xref>). Previous studies have found that when performing spatial memory tasks, in conditions where visual cues are not visible in the dark, the path integration mechanism computes the traveled distance based on (non-visual) idiothetic distance information, such as from the vestibular, proprioceptive, and efference copy of motor control signals<sup><xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c25">25</xref></sup>. Thus, we extended this study to characterize the behaviors of the path integration mechanism in a visual spatial perception task, to investigate if the path integration mechanism behaved in a similar manner as in the spatial memory task.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Experiment 1: Testing Allocentric vs. Egocentric Hypothesis</title>
<p>We first verified the prediction of the allocentric hypothesis (<xref rid="fig2" ref-type="fig">figure 2</xref>) by testing a <italic>baseline-stationary</italic> condition (<xref rid="fig3" ref-type="fig">figure 3a</xref>) and a <italic>walking</italic> condition (<xref rid="fig3" ref-type="fig">figure 3b</xref>). In both conditions, the observer readied for the experiment by sitting on a chair in the waiting area, which was illuminated by a dimly lit LED light from the ceiling. To ready for a test trial, he/she waited for an audio tone. Upon hearing the tone, he/she stood up and aligned his/her feet at the start point (home base) that faced a black curtain in the direction of the testing area. About 30 sec later, the experimenter turned off the LED light in the waiting area and the observer drew the curtain open in the dark to begin the trial.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Experiment 1. <bold>a.</bold> <italic>Baseline-stationary</italic> condition: The observer stood at the home base (illustrated as blue cross) and judged the location of a briefly presented dimly-lit target. <bold>b</bold>. <italic>Walking</italic> condition: The observer walked blindly from the home base (illustrated as blue cross) for 1.5 m to the new location (illustrated as red cross). After a short delay (12 or 60 sec), the dimly-lit target was presented for him/her to judge the location. <bold>c.</bold> Graph plotting the average (n=8) judged target locations from Experiment 1. The black plus symbols, here and in other graphs, represent the physical target locations. The filled and open green triangle symbols represent the results for the 12 sec and 60 sec waiting periods, respectively, in the <italic>walking</italic> condition. The red circle symbols represent the <italic>baseline-stationary</italic> condition. With the 12 sec waiting period, judged locations (filled green triangles) in the <italic>walking</italic> condition were significantly nearer than in the <italic>baseline-stationary</italic> (filled red circles) condition. With the 60 sec waiting period, judged locations (open triangles) had a much smaller separation from the <italic>baseline-stationary</italic> condition. The blue and gray curves with the same shape are the intrinsic bias fitted to the data by eye. Their horizontal shift is about 1.35m. <bold>d</bold>. Graph plotting the average (n=8) judged target locations from Experiment 2. The green triangle and red circle symbols represent results from the <italic>divided-attention-walking</italic> and <italic>baseline-stationary</italic> conditions, respectively. Judged locations from both conditions were similar and are fitted by the same intrinsic bias curve. Error bars represent the standard errors of the mean.</p></caption>
<graphic xlink:href="533725v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In the <italic>baseline-stationary</italic> condition, after further waiting at the home base (blue cross) for either 12 or 60 sec, the observer saw a briefly presented (1 sec) dimly lit target. He/she was instructed to judge the target location and to respond by walking blindly to the remembered target location and gesturing its height after reaching the walked destination<sup><xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>, 7–<xref ref-type="bibr" rid="c10">10</xref></sup>. (While walking, the observer’s right hand glided along a horizontal guidance rope, drawn as the yellow line in <xref rid="fig3" ref-type="fig">figure 3a</xref>). Effectively, this blind walking-gesturing task reveals the perceived target location (x: walked distance; y: gestured height).</p>
<p>In the <italic>walking</italic> condition (<xref rid="fig3" ref-type="fig">figure 3b</xref>), the observer walked blindly from the home base (blue cross) to a new location (red cross). While walking, the observer’s right hand glided along a guidance rope and stopped at the new location when he/she made contact with a soft plastic wrap on the rope (1.5m from the home base). After waiting at the new location for either 12 or 60 sec, he/she saw a briefly presented (1 sec) dimly lit target. He/she then performed the blind walking-gesturing task to indicate the perceived target location.</p>
<p><xref rid="fig3" ref-type="fig">Figure 3c</xref> shows the average results of the two conditions. With the 12 sec waiting period, judged locations in the <italic>walking</italic> condition (filled green triangles) were significantly nearer than the <italic>baseline-stationary</italic> (filled red circles) condition (<italic>P</italic>&lt;0.0001; please refer to the supplement for details of all statistical analyses). The two sets of data points are fitted by the same shaped intrinsic bias curve with about 1.35 m horizontal separation, which is close to the initially walked distance from the home base (1.5 m; in the <italic>walking</italic> condition). This result thus confirms the prediction of the allocentric hypothesis (<xref rid="fig2" ref-type="fig">figure 2b</xref>). With the 60 sec waiting period, judged locations (open triangles) in the <italic>walking</italic> condition had a smaller, though statistically significant, separation from the <italic>baseline-stationary</italic> condition (open circles) (<italic>P</italic>&lt;0.01). This suggests if the waiting period was sufficiently long, suggesting the visual memory of the home base decays over time<sup><xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c19">19</xref></sup>, the visual system automatically resets the intrinsic bias to the observer’s (new/current) location, i.e., making it the updated home base.</p>
<p>The results above demonstrate the action of the path integration mechanism. It tracks the observer’s location with respect to the external world during locomotion and plays a critical role in maintaining an allocentric reference frame. In the <italic>walking</italic> condition, the path-integration mechanism computes the traveled distance relative to the home base. Experiments 2-4 below explored the characteristics of the path-integration mechanism.</p>
</sec>
<sec id="s2b">
<title>Experiment 2: Path Integration Affected by Cognitive Load</title>
<p>Since in the <italic>walking</italic> condition (<xref rid="fig3" ref-type="fig">figure 3b</xref>) observers were simply attentive to the task at hand but otherwise neutral and not subjected to excessive cognitive demands, one might assume that the path-integration mechanism operates automatically. To test this assumption, we investigated whether the path-integration mechanism requires some attentional resources to function normally<sup><xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref></sup>. We thus tested a new, <italic>divided attention walking</italic> condition, where the observer continuously performed a cognitive task, counting number backward, while walking from the home base to the new location. We predicted this would reduce the attention resources available for path-integration. For comparison, we also tested the <italic>baseline-stationary</italic> condition as in Experiment 1 (<xref rid="fig3" ref-type="fig">figure 3a</xref>).</p>
<p>The average results in <xref rid="fig3" ref-type="fig">figure 3d</xref> reveal a small, though significant, difference between the <italic>divided attention walking</italic> condition and the <italic>baseline-stationary</italic> conditions (<italic>P</italic>=0.022). Noticeably, the distance underestimation in the <italic>divided attention walking</italic> condition (<xref rid="fig3" ref-type="fig">figure 3d</xref>) was much smaller than that found in Experiment 1 (<xref rid="fig3" ref-type="fig">figure 3c</xref>). This suggests path-integration is less effective when observers were engaged in another mental task that distracted their spatial attention.</p>
</sec>
<sec id="s2c">
<title>Experiment 3: Path-integration from Vestibular Input</title>
<p>We addressed two issues regarding the idiothetic distance cues used by the path-integration mechanism. First, does it operate only when the observer’s movement is self-initiated? Second, does it operate when observer moves in the backward direction? To answer these questions, we moved the observers passively to stimulate their vestibular system<sup><xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c28">28</xref>–<xref ref-type="bibr" rid="c29">29</xref></sup>. That is, instead of instructing the observer to walk in the dark from the home base, he/she stood upright on a rolling platform that was moved by the experimenter (insets in <xref rid="fig4" ref-type="fig">figure 4</xref>). Doing so negated the contributions of the proprioception and motor efference copy information to path-integration while keeping the vestibular information intact.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Results of Experiment 3 (<bold>a, b</bold>) and Experiment 4 (<bold>c</bold>). <bold>a.</bold> <italic>Vestibular-forward</italic> condition: The blue square and green triangle symbols represent the <italic>vestibular-forward</italic> and <italic>walking</italic> conditions, respectively, and they show that judged target locations were similar (n=8). These judged target locations were nearer than those from the <italic>baseline-stationary</italic> condition (red circles). <bold>b.</bold> <italic>Vestibular-backward</italic> condition: Backward translation caused judged target locations (blue squares) to be farther than those from the <italic>baseline-stationary</italic> condition (red circles) (n=8). <bold>c.</bold> Experiment 4. Verbally reported results of <italic>vestibular-forward</italic> condition show shorter reported eye-to-target distances (blue squares) than in the baseline-stationary (red disc) condition (n=8). Error bars represent standard errors of the mean.</p></caption>
<graphic xlink:href="533725v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We first tested a new <italic>vestibular-forward</italic> condition in which the experimenter pushed the observer on the platform forward by 1.5 m to the new location. Once at the new location, the observer stepped down from the platform and stood on the floor to wait for the dimly lit test target to be presented. He/she responded with the blind walking-gesturing task. For comparison, we also tested the <italic>walking</italic> (<xref rid="fig3" ref-type="fig">figure 3b</xref>) and <italic>baseline-stationary</italic> (<xref rid="fig3" ref-type="fig">figure 3a</xref>) conditions as in Experiment 1. <xref rid="fig4" ref-type="fig">Figure 4a</xref> reveals the judged target locations in the <italic>vestibular-forward condition</italic> (blue squares) were similar to the <italic>walking</italic> condition (green triangles; <italic>P</italic>=0.718), and were significantly nearer than the <italic>baseline-stationary</italic> condition (red circles; <italic>P</italic>&lt;0.0001). This indicates the path-integration mechanism can function during passive movements that stimulate the vestibular system.</p>
<p>To answer the second question, we then tested a complementary condition wherein the experimenter pulled the rolling platform supporting the observer backward by 1.5 m from the home base (<italic>vestibular-backward</italic> condition). As expected, the backward displacement caused judged target locations to be farther than the <italic>baseline-stationary</italic> condition (<italic>P</italic>&lt;0.0001; <xref rid="fig4" ref-type="fig">figure 4b</xref>). Taken together, the experiment reveals the path-integration mechanism can sufficiently utilize the vestibular cue in both forward and backward moving directions.</p>
</sec>
<sec id="s2d">
<title>Experiment 4: The Allocentric Principle Is Not Task Specific</title>
<p>Previous studies of space perception with stationary observers not undergoing self-motion measured with different response tasks have shown a concordance in finding. Specifically, the perceptual effects found were similar between an action-based task (blind walking-gesturing) and a perception-based task (e.g., verbal reports and perceptual matching) <sup><xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c30">30</xref></sup>. To confirm that this is also true for observers undergoing self-motion, we repeated the <italic>vestibular-forward</italic> and the <italic>baseline-stationary</italic> conditions in Experiment 3 and employed the verbal report task, wherein the observer verbally reported the perceived eye-to-target distances in feet or meters. We found that consistent with the results from the blind walking-gesturing task in Experiment 3 (<xref rid="fig4" ref-type="fig">figure 4a</xref>), verbally reported distances were significantly shorter in the <italic>vestibular-forward</italic> than <italic>the baseline-stationary</italic> condition (<italic>P</italic>&lt;0.0001, <xref rid="fig4" ref-type="fig">figure 4c</xref>).</p>
</sec>
<sec id="s2e">
<title>Experiment 5: Anisotropic Path-integration – A Bias for the Ground Surface</title>
<p>An assumption underlying the allocentric hypothesis is that adoption of the ground-based spatial coding scheme is fitting for our terrestrial existence, where everyday activities, including navigation, are performed with respect to the horizontal ground surface<sup><xref ref-type="bibr" rid="c4">4</xref></sup>. This dependence on the ground surface, i.e., an ecological constraint, predicts when the observer travels on a sloping surface in the dark, the visual system would not be able to simultaneously maintain the intrinsic bias at the home base in the vertical and horizontal directions. To investigate this prediction of anisotropic path-integration, we tested a new, <italic>stepladder</italic> condition, where the observer descended a stepladder in the dark (<xref rid="fig5" ref-type="fig">figure 5a</xref>). As the self-motion here consisted of both horizontal forward and vertical downward vectors, we predicted the intrinsic bias would be spatially updated in the horizontal but not in the vertical direction. This prediction is illustrated in <xref rid="fig5" ref-type="fig">figure 5a</xref> where the horizontal coordinate of the intrinsic bias (blue curve) remains at the home base location on the floor while the vertical coordinate travels along with the observer. Thus, upon stepping down from the stepladder and standing on the floor, the observer underestimates the horizontal distance of a dimly lit target. We expected that the target underestimation will be similar to that of a <italic>horizontal-walking</italic> condition (<xref rid="fig5" ref-type="fig">figure 5b</xref>). In contrast, if the intrinsic bias is spatially updated in both the horizontal and vertical dimensions, the intrinsic bias will remain at the top of the staircase (orange curve, <xref rid="fig5" ref-type="fig">figure 5a</xref>). The perceived target location will then be dramatically different from the predicted horizontal-only-updating strategy (<xref rid="fig5" ref-type="fig">figure 5b</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Experiment 5: Predictions (<bold>a &amp; b</bold>) and average results (<bold>c</bold>). <bold>a.</bold> <italic>Stepladder</italic> condition where the observer descends from the stepladder in the dark. If the path-integration process only gauges the horizontally traveled distance, the intrinsic bias would be path integrated in the horizontal but not vertical direction. As such, the horizontal coordinate of the intrinsic bias (blue curve) is kept at the home base location on the floor while the vertical coordinate travels along with the observer. Thus, upon stepping down from the stepladder and standing on the floor, the observer underestimates the horizontal distance of the target. The target underestimation would be similar to that of the <italic>horizontal-walking</italic> condition depicted in <bold>b</bold>. In contrast, if the path-integration process integrates distance in both the horizontal and vertical directions, the intrinsic bias will remain at the top of the staircase (orange curve). The perceived target location will then be dramatically different from that in the <italic>horizontal-walking</italic> condition. <bold>b.</bold> <italic>Horizontal-walking</italic> condition. <bold>c</bold>. The average results of the <italic>baseline-stationary</italic>, <italic>horizontal-walking</italic> and <italic>stepladder</italic> conditions, respectively represented by the red circle, green square and blue triangle symbols (n=9). The judged horizontal distances are significantly shorter in the <italic>horizontal-walking</italic> condition than in the <italic>baseline-stationary</italic> conditions. The two sets of data points are fitted by the same intrinsic bias profile with a horizontal separation of 1.0 m, which is close to the walked distance (1.06 m) from the home base to the new location in the <italic>horizontal-walking</italic> condition. Of significance, the judged locations in the <italic>stepladder</italic> condition (blue triangles) are similar to that in the <italic>horizontal-walking</italic> condition (green squares). This confirms that the path-integration process mainly gauges the horizontal (ground) distance travelled.</p></caption>
<graphic xlink:href="533725v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><xref rid="fig5" ref-type="fig">Figure 5c</xref> depicts the average judged target locations (n=9) from the <italic>stepladder</italic> (blue triangles), <italic>horizontal-walking</italic> (green square) and <italic>baseline-stationary</italic> (red circles) conditions (the <italic>baseline-stationary</italic> condition’s setup was the same as in <xref rid="fig3" ref-type="fig">figure 3a</xref>). A comparison between the <italic>horizontal-walking</italic> and the <italic>baseline-stationary</italic> conditions reveals the judged horizontal distances were significantly shorter in the <italic>horizontal-walking</italic> condition (P&lt;0.001). The two sets of data are fitted by the same intrinsic bias profile with a horizontal separation of 1.0 m, which is close to the walked distance (1.06 m) in the <italic>horizontal-walking</italic> condition.</p>
<p>The judged horizontal distances in the <italic>stepladder</italic> condition were significantly shorter than that in the <italic>baseline-stationary</italic> condition (P&lt;0.001). This confirms the prediction that while descending the stepladder, the visual system spatially updated the horizontal but not the vertical vector of the intrinsic bias (blue intrinsic bias, <xref rid="fig5" ref-type="fig">figure 5a</xref>). Further supporting this, we found the data from the <italic>stepladder</italic> and the <italic>horizontal-walking</italic> conditions overlap substantially.</p>
<p>Taken together, our experiment revealed when stepping down from a stepladder, the horizontal coordinate of the intrinsic bias is kept at the home base on the floor while the vertical coordinate moves downward with the body. This suggests human’s path-integration mechanism, responsible for allocentric spatial coding, operates much more efficiently along the horizontal ground plane than in the vertical direction.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We constantly move relative to the environment, which causes our retinal images of the environment to also be in constant motion. Yet, we reliably localize objects as we navigate our environment. This raises the question of how our visual system creates a stable visual space for us to operate and plan future actions, such as walking, despite the inadequate retinal inputs. While it was known the visual system relies predominantly on the ground surface to accurately localize objects, it was unclear what coordinate system it uses for coding visual space. In this study, we proposed the visual system uses an allocentric, or world-centered, coordinate system for the ground-based reference. In this way, the visual system could alleviate the challenge of constant retinal motion during self-motion by streamlining online location computation to only the moving observer and the object of interest. The locations of surrounding objects are anchored to a fixed position on the ground negating excessive computation, thereby reducing coding redundancy. Our main finding provides support for the allocentric hypothesis. We showed during self-motion, the intrinsic bias that acts as the spatial reference of the ground surface when visual cues are not visible, is fixed to the ground location before the motion begins (home base) and remains at the same ground location during self-motion.</p>
<p>Previous studies mainly tested location judgments of observers who stood stationary at one location on the ground while viewing a stationary target (similar to our <italic>baseline-stationary</italic> condition). Because the observers need not generate significant self-motion in those studies, it was not possible to investigate the allocentric hypothesis. In principle, without body motion, the allocentric coordinate system does not have an advantage over the egocentric coordinate system for processing efficiency. In fact, previous findings with stationary observers led to the widely accepted notion that absolute distance judgment relies on the egocentric coordinate system.</p>
<p>The visual system relies on the path-integration mechanism to ground the intrinsic bias at the home base. The path-integration mechanism can function when the observer’s motion is passive, suggesting the traveled distance can be based on the vestibular signals alone. Also, the path-integration mechanism fails to operate efficiently when the observer’s attention is distracted to an unrelated cognitive demand during walking. This suggests some degree of attentional effort is required to reliably path integrate an observer’s body location relative to the environment. Lastly, we showed the path-integration mechanism is more efficient in the horizontal than the vertical direction (anisotropic with a ground-bias). This observation further highlights one of the most important design principles underlying our sensory and motor systems, namely, fitting our terrestrial niche. We inhabit a land-based environment and most of our daily activities are structured relative to the ground. The current finding on the vertical-horizontal anisotropy of the path integration mechanism for visual space perception is consistent with previous studies on spatial memory, which revealed the path-integration systems of land-dwelling animals are more efficient in estimating horizontally traveled distance<sup><xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c31">31</xref>–<xref ref-type="bibr" rid="c34">34</xref></sup>. In particular, our observation is reminiscent of the behavioral studies of the desert ants, Cataglyphis fortis. Wohlgemuth et al (2001) found desert ants used path integration to return from foraging excursions on a shortcut way to their nests (also see Ronacher, 2020). Specifically, it was revealed when the ants were crawling over a slope surface with uphill and downhill trajectory, their path integration system computed only the horizontal component of the traveled distance. We note, however, further research is needed to investigate whether the same path-integration mechanism is employed for a spatial memory task (previous studies by others) and a spatial perception task (current study).</p>
<p>Overall, our study suggests the visual system, by anchoring its spatial reference frame to the ground during navigation can create a world-centered visual space. One advantage of adopting this space coding strategy is that when we travel, the spatial representations of most surrounding objects, which are static relative to the ground, will remain constant (static) despite the retinal image motion. Accordingly, our visual system only needs to dynamically update the spatial representations of our bodies and the few objects that are moving relative to an allocentric origin on the ground (within a world-centered context). A consequential byproduct of operating within the framework of an allocentric, world-centered visual space is the perceived stability of our visual environment during locomotion.</p>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Observers</title>
<p>Fourteen observers (age=24.47 ± 1.45 years old; eye height= 1.60 ± 0.02 m; 7 males and 7 females) participated in Experiments 1-4 and nine observers (age=22.78 ± 1.22 years old; eye height= 1.58 ± 0.04 cm; 6 males and 3 males) participated in Experiment 5. They were naïve to the purpose of the study and signed the informed consent form at the start of the study. All observers had normal, or corrected-to-normal, visual acuity (at least 20/20) and a stereoscopic resolution of a 20 arc sec or better. They viewed the visual scene binocularly. A within-subject experimental design was used. The study protocol was approved by the Institutional Review Board and followed the tenets of the Declaration of Helsinki.</p>
</sec>
<sec id="s4b">
<title>General stimulus and testing environment</title>
<p>All experiments 1-4 were performed in a dark room whose layout and dimensions were unknown to the observers. One end of the room, just before the testing area, served as the waiting area (~3 m<sup>2</sup>) for the observer. The waiting area had a chair facing the wall for the observer to sit in between trials so that his/her back faced the testing area. Two white LED lights on the ceiling provided ambient illumination while the observer waited in between trials. The testing and waiting areas were separated by a black curtain. A long guidance rope (0.8 m above the floor) was tied to both ends of the room and served to guide the observer while walking blindly. A plastic wrap was tied to the guidance rope on the part of the rope located in the waiting area near the curtain to mark the start point (home base). To ready for a trial, the observer walked to this start point and faced the test area while holding onto the plastic wrap on the rope and called out “ready”. The curtain was then drawn open for the trial to begin.</p>
<p>The dimly lit test target used in all experiments was a diffused green LED (0.16 cd m<sup>-2</sup>). The LED was placed in the center of a ping-pong ball that was encased in a small opaque box. An adjustable iris-diaphragm aperture was placed at the front of the ping-pong ball to keep its visual angular size constant at 0.22° when measured at the eye level. During testing, the target was displayed with a 5 Hz flicker for 1 sec. Music was played aloud during the entire experimental session to mask extraneous auditory information during the experiments.</p>
</sec>
<sec id="s4c">
<title>Observer’s response tasks</title>
<p>The main task used in all experiments was the <italic>blind walking-gesturing task</italic> <sup>35, 36</sup>. For each trial, the observer stood by the guidance rope in the dark and judged the location of the 1 sec flickering target. After which he/she then put on the blindfold and called out “ready to walk”. This signaled the experimenter to quickly remove the target and shook the guidance rope to indicate it was safe to walk. The observer walked while sliding his/her right hand along the guidance rope until he/she reached the remembered target location. Once there, he/she indicated the remembered target height with his/her left hand and called out “done”. The experimenter turned on the flashlight, marked the observer’s feet location, measured the gestured height, and asked the observer to turn around and walk back to the start point using the guidance rope. When the observer arrived at the start point, the experimenter turned on the ceiling LED lights in the waiting area. The observer then removed the blindfold, sat down, and waited for the next trial.</p>
<p>An additional task used, in Experiment 4, was the <italic>verbal reporting task</italic> <sup>35, 37</sup>. Here, the observer stood next to the guidance rope in the dark and viewed the target for 5 sec to judge its absolute distance between the target and his/her eyes. Once the target was turned off, an audio tone was presented to signal the observer to immediately report the estimated distance either in meters or feet. For both this and the blind walking-gesturing task, the observer was provided five practice trials before each test session. No feedback regarding performance was provided to the observer during the practice or test session.</p>
</sec>
<sec id="s4d">
<title>Experiment 1</title>
<sec id="s4d1">
<title>Design</title>
<p>Two viewing conditions were tested: <italic>walking</italic> and <italic>baseline-stationary</italic>. Each condition was tested separately with a 12 sec and a 60 sec waiting period. The target was placed at one of six locations in both conditions. Four locations were on the floor at 3, 4.25, 5.25, or 6.5 m from the observer, the fifth location was 0.5 m above the floor at 6.5 m from the observer, and the sixth location was at the observer’s eye level and 6.5 m from the observer. Testing of the 4.25, 5.25, and 6.5 m targets on the floor were repeated three times while testing of the remaining targets was repeated twice. A total of 60 trials were run over two days. The order of stimulus presentation was randomized.</p>
</sec>
<sec id="s4d2">
<title>Procedure</title>
<p>While the observer sat at the waiting area before each trial, he/she was informed of the upcoming test condition (<italic>baseline-stationary</italic> or <italic>walking</italic>) and of the waiting period (12 sec or 60 sec). After that, an audio tone was presented to signal to the observer to walk to the start point (home base) and face the black curtain in the direction of the testing area. About 30 sec later, the experimenter turned off the ambient LED lights in the waiting area and the observer drew the curtain open in the dark.</p>
<p>For the <italic>baseline-stationary</italic> condition, the observer stood in the dark at the start point (home base) over the predetermined waiting duration (12 or 60 sec). He/she was instructed to stand upright with minimal head motion during the waiting period, and to expect hearing a pure tone at the end of the waiting period. Roughly, two sec after hearing the tone, the test target was presented at one of the six predetermined locations. The observer’s task was to judge its location and perform the blind walking-gesturing task. For the <italic>walking</italic> trial, the observer stood at the start point until a white noise (instead of pure tone) was heard. He/she then walked forward until his/her right hand touched a plastic wrap tied on the guidance rope at the new location (1.5m from the start point). He/she then stopped walking, called out “ready” and waited there for either 12 sec or 60 sec before the test target was presented. The remaining procedural steps were the same as in the <italic>baseline-stationary</italic> condition.</p>
</sec>
</sec>
<sec id="s4e">
<title>Experiment 2</title>
<sec id="s4e1">
<title>Design and Procedure</title>
<p>Both the <italic>baseline-stationary</italic> and <italic>walking</italic> conditions with the 12 sec waiting period were tested but with one modification. The modification was that during the 12 second waiting period at the start point (home base) (for <italic>baseline-stationary</italic> condition), or while walking to, and waiting at the new location (for <italic>walking</italic> condition), the observer performed a counting task. The experimenter would provide a random number between 50 and 99 for the observer to count backward. The same six target locations tested in Experiment 1 were tested here. A total of 30 trials were run. The order of stimulus presentation was randomized.</p>
</sec>
</sec>
<sec id="s4f">
<title>Experiment 3</title>
<sec id="s4f1">
<label>3.1</label><title>Vestibular-forward condition</title>
<sec id="s4f1a">
<title>Design</title>
<p>The two conditions from Experiment 1 (<italic>walking</italic> and <italic>baseline-stationary</italic> conditions with the 12 seconds waiting period) and a new, <italic>vestibular-forward</italic> condition were tested. The latter <italic>vestibular-forward</italic> condition was the same as in the <italic>walking</italic> condition, except that the observer now stood on a rolling-platform that was pushed forward by the experimenter for 1.5 m. The rolling-platform had four wheels that was 0.30 m above the floor. The same six target locations tested in Experiment 1 were tested here. There were 60 trials in total with 30 trials tested per day. The order of stimulus presentation was randomized.</p>
</sec>
<sec id="s4f1b">
<title>Procedure</title>
<p>Two experimenters (A and B) conducted the experiment. Before each trial, experimenter A instructed the observer of the upcoming test condition and to prepare for an audio signal. Once the signal was heard, the observer stepped onto the rolling-platform at the start point (home base) behind the curtain with eyes opened and facing the test area. He/she then called out “ready”. A second audio signal was presented after 30 seconds. A pure tone indicated to the observer <underline>to stay</underline> (<italic>baseline-stationary</italic> condition), while a white noise indicated <underline>to walk</underline> (<italic>walking</italic> condition) or <underline>to be pushed forward</underline> (<italic>vestibular-forward</italic> condition). At the same time, experimenter A turned off the LED lights in the waiting area, and the observer drew the curtain open. For the <italic>baseline-stationary</italic> condition trial, the observer stepped down from the rolling-platform, stood at the start point, and called out “ready to view”. For the <italic>walking</italic> condition trial, the observer stepped down from the rolling-platform, and walked forward until he/she touched the plastic wrap tied onto the guidance rope at the new location (1.5 m), and then stopped and called out “ready to view”. For the <italic>vestibular-forward</italic> condition trial, the observer kept standing on the rolling-platform. Experimenter B verbally informed the observer the trial was starting and pushed the rolling platform forward by 1.5 m to the new location. Upon arrival, Experiment B instructed the observer to step down from the rolling platform and to call out “ready to view”. For all three trial types, after the observer called out “ready to view”, Experimenter A turned on the test target after a 12 sec waiting period for the observer to judge its location. He/she then responded by performing the blind walking-gesturing task.</p>
</sec>
</sec>
<sec id="s4f2">
<label>3.2</label><title>Vestibular-backward condition</title>
<sec id="s4f2a">
<title>Design and procedure</title>
<p>The <italic>baseline-stationary</italic> condition was the same as in 3.1 above. The <italic>vestibular-backward</italic> condition was modified from the <italic>vestibular-forward</italic> in 3.1 above, by pulling the rolling-platform backward for 1.5 m. The same six target locations as in 3.1 were tested. A total 30 of trials were tested in one session. The order of stimulus presentation was randomized.</p>
</sec>
</sec>
</sec>
<sec id="s4g">
<title>Experiment 4</title>
<sec id="s4g1">
<title>Design</title>
<p>The two conditions tested were the <italic>vestibular-forward</italic> and <italic>baseline-stationary</italic> conditions as in 3.1 above. But instead of the six test target locations, only five target locations on the floor were tested. The target location at 6.5 m distance and 0.5m above the ground was not tested. All targets were presented for 5 sec. Twenty-eight trials in total were tested in one session. The order of stimulus presentation was randomized.</p>
</sec>
<sec id="s4g2">
<title>Procedure</title>
<p>The procedure was the same as in 3.1 above, except the observers performed the verbal report task. To do so, the observer judged the absolute distance between the target and his/her eyes. Immediately after the target was turned off, an audio signal was presented to prompt the observer to report the estimated distance aloud either in meters or feet.</p>
</sec>
</sec>
<sec id="s4h">
<title>Experiment 5</title>
<sec id="s4h1">
<title>Design</title>
<p>Three conditions (<italic>baseline-stationary</italic>, <italic>horizontal-walking</italic> and <italic>stepladder</italic>) were tested in two different blocks. The <italic>horizontal-walking</italic> and <italic>baseline-stationary</italic> conditions were mixed in test block-A, while the <italic>stepladder</italic> and <italic>baseline-stationary</italic> conditions in test block-B. Each block consisted of 20 test trials. The order of test conditions within each block was randomized. Each block was tested in a daily session and ran twice. The testing order of the four blocks was alternated between observers.</p>
</sec>
<sec id="s4h2">
<title>Procedure</title>
<p>Before each trial, the experimenter informed the observer of the condition to be tested. The <italic>baseline-stationary</italic> and <italic>horizontal-walking</italic> conditions were conducted as in the main experiment (Experiment 1). For the <italic>stepladder</italic> condition, the observer first ascended the stepladder, and waited for an audio tone that signaled to descend the stepladder with eyes opened and looking at the invisible horizon (in the dark). Upon reaching the foot of the stepladder, he/she held onto the guidance rope and walked forward until he/she felt the plastic wrap on the rope at the new location. The observer then stood still and waited for 12 s before the test target was presented. After judging the target’s location, he/she put on the blindfold and called out “ready” to begin the blindfolded walking-gesturing task.</p>
</sec>
<sec id="s4h3">
<title>Statistical tests</title>
<p>Data were analyzed using analysis of variance with repeated measures. The Mauchly’s test was applied to verify the assumption of sphericity.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Ooi</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>He</surname>, <given-names>Z. J.</given-names></string-name> <article-title>Distance determined by the angular declination below the horizon</article-title>. <source>Nature</source> <volume>414</volume>, <fpage>197</fpage>–<lpage>200</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Ooi</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>He</surname>, <given-names>Z. J.</given-names></string-name> <article-title>Perceptual space in the dark affected by the intrinsic bias of the visual system</article-title>. <source>Perception</source> <volume>35</volume>, <fpage>605</fpage>–<lpage>624</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Wohlgemuth</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ronacher</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Wehner</surname>, <given-names>R.</given-names></string-name> <article-title>Ant odometry in the third dimension</article-title>. <source>Nature</source> <volume>411</volume>, <fpage>795</fpage>–<lpage>798</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="book"><string-name><surname>Gibson</surname>, <given-names>J. J.</given-names></string-name> <source>The Perception of the Visual World</source> (<publisher-name>Houghton Mifflin</publisher-name>, <year>1950</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="book"><string-name><surname>Sedgwick</surname>, <given-names>H. A.</given-names></string-name> <chapter-title>Space perception</chapter-title>. in <source>Handbook of Perception and Human Performance</source> (eds <person-group person-group-type="editor"><string-name><surname>Boff</surname>, <given-names>K. R.</given-names></string-name></person-group>, <etal>et al.</etal>) <fpage>21.1</fpage>–<lpage>21.57</lpage> (<publisher-loc>New York</publisher-loc> <publisher-name>Wiley</publisher-name>, <year>1986)</year>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Sinai</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Ooi</surname>, <given-names>T. L.</given-names></string-name> &amp; <string-name><surname>He</surname>, <given-names>Z. J.</given-names></string-name> <article-title>Terrain influences the accurate judgement of distance</article-title>. <source>Nature</source> <volume>395</volume>, <fpage>497</fpage>–<lpage>500</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Thomson</surname>, <given-names>J. A.</given-names></string-name> <article-title>Is continuous visual monitoring necessary in visually guided locomotion?</article-title> <source>J. Exp. Psychol. Hum. Percept. Perform</source> <volume>9</volume>, <fpage>427</fpage>–<lpage>443</lpage> (<year>1983</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Rieser</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Ashmead</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Talor</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Youngquist</surname>, <given-names>G.</given-names></string-name> <article-title>Visual perception and the guidance of locomotion without vision to previously seen targets</article-title>. <source>Perception</source> <volume>19</volume>, <fpage>675</fpage>– <lpage>689</lpage> (<year>1990</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Loomis</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Da Silva</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Fujita</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Fukusima</surname>, <given-names>S. S.</given-names></string-name> <article-title>Visual space perception and visually directed action</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source> <volume>18</volume>, <fpage>906</fpage>–<lpage>921</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Loomis</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>DaSilva</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Philbeck</surname>, <given-names>J. W.</given-names></string-name> &amp; <string-name><surname>Fukusima</surname>, <given-names>S. S.</given-names></string-name> <article-title>Visual perception of location and distance</article-title>. <source>Curr. Dir. Psychol. Sci</source> <volume>5</volume>, <fpage>72</fpage>–<lpage>77</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Meng</surname>, <given-names>J. C.</given-names></string-name> &amp; <string-name><surname>Sedgwick</surname>, <given-names>H. A.</given-names></string-name> <article-title>Distance perception mediated through nested contact relations among surfaces</article-title>. <source>Percept. Psychophys</source>. <volume>63</volume>, <fpage>1</fpage>–<lpage>15</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Wu</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ooi</surname>, <given-names>T. L.</given-names></string-name> &amp; <string-name><surname>He</surname>, <given-names>Z. J.</given-names></string-name> <article-title>Perceiving distance accurately by a directional process of integrating ground information</article-title>. <source>Nature</source> <volume>428</volume>, <fpage>73</fpage>–<lpage>77</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>He</surname>, <given-names>Z. J.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ooi</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Yarbrough</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Wu</surname>, <given-names>J.</given-names></string-name> <article-title>Judging egocentric distance on the ground: Occlusion and surface integration</article-title>. <source>Perception</source>, <volume>33</volume>, <fpage>789</fpage>–<lpage>806</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Bian</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Braunstein</surname>, <given-names>M. L.</given-names></string-name> &amp; <string-name><surname>Andersen</surname>, <given-names>G. J.</given-names></string-name> <article-title>The ground dominance effect in the perception of 3-D layout</article-title>. <source>Percept. Psychophys</source> <volume>67</volume>, <fpage>802</fpage>–<lpage>815</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Philbeck</surname>, <given-names>J. W.</given-names></string-name> &amp; <string-name><surname>Loomis</surname>, <given-names>J. M.</given-names></string-name> <article-title>Comparison of two indicators of perceived egocentric distance under full-cue and reduced-cue conditions</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>23</volume>, <fpage>72</fpage>–<lpage>85</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Gogel</surname>, <given-names>W. C.</given-names></string-name> &amp; <string-name><surname>Tietz</surname>, <given-names>J. D.</given-names></string-name> <article-title>A comparison of oculomotor and motion parallax cues of egocentric distance</article-title>. <source>Vision Res</source>. <volume>19</volume>, <fpage>1161</fpage>–<lpage>1170</lpage> (<year>1979</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Wu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Shi</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>He</surname>, <given-names>Z. J.</given-names></string-name> &amp; <string-name><surname>Ooi</surname>, <given-names>T. L.</given-names></string-name> <article-title>The visible ground surface as a reference frame for scaling binocular depth of a target in midair</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>41</volume>, <fpage>111</fpage>–<lpage>126</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Mittelstaedt</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Mittelstaedt</surname> <given-names>H.</given-names></string-name> <article-title>Homing by path integration in a mammal</article-title>. <source>Naturwissenschaften</source>. <volume>67</volume>, <fpage>566</fpage>–<lpage>567</lpage> (<year>1980</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Wehner</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Srinivasan</surname>, <given-names>M. V.</given-names></string-name> <article-title>Searching behavior of desert ants, genusCataglyphis (Formicidae</article-title>, <source>Hymenoptera). J. Comp. Physiol</source>. <volume>142</volume>, <fpage>315</fpage>–<lpage>318</lpage> (<year>1981</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Rieser</surname>, <given-names>J. J.</given-names></string-name> <article-title>Access to knowledge of spatial structure at novel points of observation</article-title>. <source>J. Exp. Psychol. Learn. Mem. Cogn</source>. <volume>15</volume>, <fpage>1157</fpage>–<lpage>1165</lpage> (<year>1989</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Etienne</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Maurer</surname> <given-names>R</given-names></string-name>, <string-name><surname>Séguinot</surname> <given-names>V.</given-names></string-name> <article-title>Path integration in mammals and its interaction with visual landmarks</article-title>. <source>J Exper Biol</source>. <volume>199</volume>, <fpage>201</fpage>–<lpage>209</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>McNaughton</surname>, <given-names>B. L.</given-names></string-name> <etal>et al.</etal> <article-title>Deciphering the hippocampal polyglot: the hippocampus as a path integration system</article-title>. <source>J. Exp. Biol</source>. <volume>199</volume>, <fpage>173</fpage>–<lpage>185</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="book"><string-name><surname>Loomis</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Klatzky</surname>, <given-names>R. L.</given-names></string-name>, <string-name><surname>Golledge</surname>, <given-names>R. G.</given-names></string-name> &amp; <string-name><surname>Philbeck</surname>, <given-names>J. W.</given-names></string-name> <source>Human navigation by path integration. In Wayfinding: Cognitive mapping and other spatial processes</source> (eds <person-group person-group-type="editor"><string-name><surname>Golledge</surname>, <given-names>R. G</given-names></string-name></person-group>) <fpage>125</fpage>–<lpage>151</lpage>, (<publisher-name>Baltimore Johns Hopkins</publisher-name>, <year>1999</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Mittelstaedt</surname>, <given-names>M. L.</given-names></string-name> &amp; <string-name><surname>Mittelstaedt</surname>, <given-names>H.</given-names></string-name> <article-title>Idiothetic navigation in humans: estimation of path length</article-title>. <source>Exp. Brain Res</source>. <volume>139</volume>, <fpage>318</fpage>–<lpage>332</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="book"><string-name><surname>Loomis</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Klatzky</surname>, <given-names>R. L.</given-names></string-name> &amp; <string-name><surname>Giudice</surname>, <given-names>N. A.</given-names></string-name> <chapter-title>Representing 3D space in working memory: Spatial images from vision, hearing, touch, and language</chapter-title>., in <source>Multisensory imagery</source> (eds <person-group person-group-type="editor"><string-name><surname>Lacey</surname>, <given-names>S.</given-names></string-name></person-group> &amp; <person-group person-group-type="editor"><string-name><surname>Lawson</surname>, <given-names>R.</given-names></string-name></person-group>) <fpage>131</fpage>–<lpage>155</lpage> (<publisher-loc>New York</publisher-loc> <publisher-name>Springer</publisher-name>, <year>2013</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Amorim</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Glasauer</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Corpinot</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Berthoz</surname>, <given-names>A.</given-names></string-name> <article-title>Updating an object’s orientation and location during nonvisual navigation: A comparison between two processing modes</article-title>. <source>Percept. Psychophys</source>. <volume>59</volume>, <fpage>404</fpage>–<lpage>418</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Cavanagh</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Hunt</surname> <given-names>A. R</given-names></string-name>, <string-name><surname>Afraz</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Rolfs</surname>, <given-names>M.</given-names></string-name> <article-title>Visual stability based on remapping of attention pointers</article-title>. <source>Trends Cogn Sci</source>.<volume>14</volume>, <fpage>147</fpage>–<lpage>153</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Israël</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Berthoz</surname>, <given-names>A.</given-names></string-name> <article-title>Contribution of the otoliths to the calculation of linear displacement</article-title>. <source>J. Neurophysiol</source>. <volume>62</volume>, <fpage>247</fpage>–<lpage>263</lpage> (<year>1989</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Cohen</surname>, <given-names>H. S.</given-names></string-name> <article-title>Vestibular disorders and impaired path integration along a linear trajectory</article-title>. <source>J. Vestib. Res</source>. <volume>10</volume>, <fpage>7</fpage>–<lpage>15</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Zhou</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>He</surname>, <given-names>Z. J.</given-names></string-name> &amp; <string-name><surname>Ooi</surname>, <given-names>T. L.</given-names></string-name> <article-title>The visual system’s intrinsic bias and knowledge of size mediate perceived size and location in the dark</article-title>. <source>J. Exp. Psychol. Learn. Mem. Cogn</source>. <volume>39</volume>, <fpage>1930</fpage>–<lpage>1942</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Jovalekic</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hayman</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Becares</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Reid</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Thomas</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>J. J.</given-names></string-name>, <etal>et al.</etal> <article-title>Horizontal biases in rats’ use of three-dimensional space</article-title>. <source>Behavioural Brain Research</source>, <volume>222</volume>(<issue>2</issue>), <fpage>279</fpage>–<lpage>288</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Hayman</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Verriotis</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Jovalekic</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Fenton</surname>, <given-names>A. A.</given-names></string-name>, &amp; <string-name><surname>Jeffery</surname>, <given-names>K. J.</given-names></string-name> <article-title>Anisotropic encoding of three-dimensional space by place cells and grid cells</article-title>. <source>Nature Neuroscience</source>, <volume>14</volume>(<issue>9</issue>), <fpage>1182</fpage>–<lpage>1188</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Zwergal</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schoberl</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Xiong</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Pradhan</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Covic</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Werner</surname>, <given-names>P.</given-names></string-name>, <etal>et al.</etal> <article-title>Anisotropy of human horizontal and vertical navigation in real space: Behavioral and PET correlates</article-title>. <source>Cerebral Cortex</source>, <volume>26</volume>(<issue>11</issue>), <fpage>4392</fpage>–<lpage>4404</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Ronacher</surname>, <given-names>B.</given-names></string-name> <article-title>Path integration in a three-dimensional world: the case of desert ants</article-title>. <source>J Comp Physiol A</source>, <volume>206</volume>, <fpage>379</fpage>–<lpage>387</lpage> (<year>2020</year>).</mixed-citation></ref>
</ref-list>
<sec id="s5">
<title>Funding</title>
<p>The work was supported by a grant from the National Institutes of Health (EY033190) to Z.J.H. and T.L.O. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
</sec>
<sec id="s6">
<title>Author contributions</title>
<p>T.L.O. and Z.J.H. conceived of the project. L.Z, T.L.O. and Z.J.H. designed experiments and analyses. L.Z. and W.W. performed the experiments and analyzed the data. L.Z. contributed to the manuscript writing; T.L.O. and Z.J.H. wrote the manuscript.</p>
</sec>
<sec id="s7">
<title>Competing interests</title>
<p>The authors declare no competing interests.</p>
</sec>
<sec id="s8">
<title>Data and materials availability</title>
<p>All data are available in the main text or the supplementary materials. The data sets generated are available from the corresponding authors upon reasonable request.</p>
<p><bold>Supplementary information</bold> is available for this paper.</p>
</sec>
<sec id="s9">
<title>Supplementary Information</title>
<sec id="s9a">
<title>Statistical Analysis of Data</title>
<sec id="s9a1">
<title>Experiment 1 (<xref rid="fig3" ref-type="fig">figure 3c</xref>)</title>
<sec id="s9a1a">
<label>1.</label><title>Baseline-stationary condition vs. walking condition with a 12 sec waiting period</title>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 6 horizontal distances) to the walked horizontal distance data. The analysis reveals:
<list list-type="bullet">
<list-item><p>Main effect of test condition (baseline-stationary vs. walking): F(1,7)=439.133, p&lt;0.001</p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(5, 35)=158.676, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: F(5, 35)=13.255, p&lt;0.001</p></list-item>
</list>
</p>
</sec>
<sec id="s9a1b">
<label>2.</label><title>Baseline-stationary condition vs. walking condition with a 60 sec waiting period</title>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 6 horizontal distances) to the walked horizontal distance data. The analysis reveals:
<list list-type="bullet">
<list-item><p>Main effect of test condition (baseline-stationary vs. walking): F(1,7)=13.406, p&lt;0.01</p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(5, 35)=133.358, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: F(5, 35)=1.555, p=0.198</p></list-item>
</list>
</p>
</sec>
</sec>
<sec id="s9a2">
<title>Experiment 2 (<xref rid="fig3" ref-type="fig">figure 3d</xref>)</title>
<sec id="s9a2a">
<title>Baseline-stationary condition vs. divided attention walking condition</title>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 6 horizontal distances) to the walked horizontal distance data. The analysis reveals:
<list list-type="bullet">
<list-item><p>Main effect of test condition (baseline-stationary vs. divided attention walking): F(1,7)=8.529, p=0.022</p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(5, 35)=63.780, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: F(5, 35)=1.072, p=0.392</p></list-item>
</list>
</p>
</sec>
</sec>
<sec id="s9a3">
<title>Experiment 3 (<xref rid="fig4" ref-type="fig">figure 4</xref>)</title>
<sec id="s9a3a">
<label>1.</label><title>Vestibular-forward condition vs. walking condition (<xref rid="fig4" ref-type="fig">figure 4a</xref>)</title>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 6 horizontal distances) to the walked horizontal distance data. The analysis reveals:
<list list-type="bullet">
<list-item><p>Main effect of test condition (vestibular-forward vs. walking): F(1,7)=0.142, p=0.718</p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(5, 35)=99.788, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: F(5, 35)=0.714, p=0.617</p></list-item>
</list>
</p>
</sec>
<sec id="s9a3b">
<label>2.</label><title>Vestibular-forward condition vs. baseline-stationary condition (<xref rid="fig4" ref-type="fig">figure 4a</xref>)</title>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 6 horizontal distances) to the walked horizontal distance data. The analysis reveals:
<list list-type="bullet">
<list-item><p>Main effect of test condition (baseline-stationary vs. vestibular-forward): F(1,7)=598.551, p&lt;0.001</p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(5, 35)=168.855, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: F(5, 35)=19.422, p&lt;0.001</p></list-item>
</list>
</p>
</sec>
<sec id="s9a3c">
<label>3.</label><title>Baseline-stationary condition vs. walking condition (<xref rid="fig4" ref-type="fig">figure 4a</xref>)</title>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 6 horizontal distances) to the walked horizontal distance data. The analysis reveals:
<list list-type="bullet">
<list-item><p>Main effect of test condition (baseline-stationary vs. walking): F(1,7)=299.648, p&lt;0.001</p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(5, 35)=160.078, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: F(5, 35)=12.607, p&lt;0.001</p></list-item>
</list>
</p>
</sec>
<sec id="s9a3d">
<label>4.</label><title>Vestibular-backward condition vs. baseline-stationary condition (<xref rid="fig4" ref-type="fig">figure 4b</xref>)</title>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 6 horizontal distances) to the walked horizontal distance data. The analysis reveals:
<list list-type="bullet">
<list-item><p>Main effect of test condition (vestibular-backward vs. baseline-stationary): F(1,7)=651.863, p&lt;0.001</p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(5, 35)=51.558, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: F(5, 35)=10.848, p&lt;0.001</p></list-item>
</list>
</p>
</sec>
</sec>
<sec id="s9a4">
<title>Experiment 4 (<xref rid="fig4" ref-type="fig">figure 4c</xref>)</title>
<sec id="s9a4a">
<title>Vestibular-forward condition vs. baseline-stationary condition (verbal reports of eye-to-target distances)</title>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 5 horizontal distances) to the verbally reported distance data. The analysis reveals:
<list list-type="bullet">
<list-item><p>Main effect of test condition (vestibular-forward vs. baseline-stationary): F(1,7)=217.749, p&lt;0.001</p></list-item>
<list-item><p>Main effect of distance: <italic>F(4, 28)=36.945, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: F(4, 28)=8.973, p&lt;0.001</p></list-item>
</list>
</p>
</sec>
</sec>
<sec id="s9a5">
<title>Experiment 5 (<xref rid="fig5" ref-type="fig">figure 5c</xref>)</title>
<sec id="s9a5a">
<label>1.</label><title>Baseline-stationary condition vs. stepladder condition</title>
<sec id="s9a5a1">
<title>Horizontal distances</title>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 5 horizontal distances) to the walked horizontal distance data. The analysis reveals:
<list list-type="bullet">
<list-item><p>Main effect of test condition (baseline-stationary vs. stepladder): F(1,8)=153.541, p=0.000</p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(4, 32)=35.954, p=0.000</italic></p></list-item>
<list-item><p>Interaction effect: F(2.58, 20.69)= 3.89, p=0.028</p></list-item>
</list>
</p>
</sec>
<sec id="s9a5a2">
<title>Heights</title>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 5 heights) to the judged height data. The analysis reveals:
<list list-type="bullet">
<list-item><p>Main effect of test condition (baseline-stationary vs stepladder): F(1,8)=0.112, p=0.747</p></list-item>
<list-item><p>Main effect of height: <italic>F(4, 32)=181.666, p=0.000</italic></p></list-item>
<list-item><p>Interaction effect: F(4, 32)= 0.288, p=0.883</p></list-item>
</list>
</p>
</sec>
</sec>
<sec id="s9a5b">
<label>2.</label><title>Baseline-stationary condition vs. horizontal-walking condition</title>
<sec id="s9a5b1">
<title>Horizontal distances</title>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 5 horizontal distances) to the walked horizontal distance data. The analysis reveals:
<list list-type="bullet">
<list-item><p>Main effect of test condition (baseline-stationary vs. horizontal-walking): F(1,8)=104.198, p=0.000</p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(4, 32)=45.768, p=0.000</italic></p></list-item>
<list-item><p>Interaction effect: F(1.892, 15.137)= 8.461, p=0.004</p></list-item>
</list>
</p>
</sec>
<sec id="s9a5b2">
<title>Heights</title>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 5 heights) to the judged height data. The analysis reveals:
<list list-type="bullet">
<list-item><p>Main effect of test condition (baseline-stationary vs. horizontal-walking): F(1,8)=7.695, p=0.024</p></list-item>
<list-item><p>Main effect of height distance: <italic>F(4, 32)=236.530, p=0.000</italic></p></list-item>
<list-item><p>Interaction effect: F(2.636, 21.088)= 2.472, p=0.096</p></list-item>
</list>
</p>
</sec>
</sec>
<sec id="s9a5c">
<label>3.</label><title>Stepladder condition vs. horizontal-walking condition</title>
<sec id="s9a5c1">
<title>Horizontal distances</title>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 5 horizontal distances) to the walked horizontal distance data. The analysis reveals:
<list list-type="bullet">
<list-item><p>Main effect of test condition (stepladder vs. horizontal-walking): F(1,8)=0.532, p=0.487</p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(4, 32)=32.372, p=0.000</italic></p></list-item>
<list-item><p>Interaction effect: F(2.967, 23.737)=0.874, p=0.467</p></list-item>
</list>
</p>
</sec>
<sec id="s9a5c2">
<title>Heights</title>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 5 heights) to the judged height data. The analysis reveals:
<list list-type="bullet">
<list-item><p>Main effect of test condition (stepladder vs. horizontal-walking): F(1,8)=0.864, p=0.380</p></list-item>
<list-item><p>Main effect of height: <italic>F(4, 32)=185.248, p=0.000</italic></p></list-item>
<list-item><p>Interaction effect: F(2.864, 22.908)=0.829, p=0.487</p></list-item>
</list>
</p>
</sec>
</sec>
</sec>
</sec>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88095.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Meng</surname>
<given-names>Ming</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>South China Normal University</institution>
</institution-wrap>
<city>Guangzhou</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study reveals the use of an allocentric spatial reference frame in how the perception of the location of a dimly lit target is updated during locomotion. The evidence supporting this claim is <bold>convincing</bold>, based on a series of cleverly and carefully designed behavioral experiments. The results will be of interest not only to scientists who study perception, action and cognition, but also to engineers who work on developing visually guided robots and self-driving vehicles.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88095.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study conducted a series of experiments to comprehensively support the allocentric rather than egocentric visual spatial reference updating for the path-integration mechanism in the control of target-oriented locomotion. Authors firstly manipulated the waiting time before walking to tease apart the influence from spatial working memory in guiding locomotion. They demonstrated that the intrinsic bias in perceiving distance remained constant during walking and that the establishment of a new spatial layout in the brain took a relatively longer time beyond the visual-spatial working memory. In the following experiments, the authors then uncovered that the strength of the intrinsic bias in distance perception along the horizontal direction is reduced when participants' attention is distracted, implying that world-centered path integration requires attentional effort. This study also revealed horizontal-vertical asymmetry in a spatial coding scheme that bears a resemblance to the locomotion control in other animal species such as desert ants.</p>
<p>The overall design of the behavioral experiments is elegant and statistics are well performed to support the authors' viewpoint in the allocentric rather than egocentric visual spatial coding scheme for distance perception along the horizontal line.</p>
<p>It is however worth noting the statement from Gibson in 1979 that for egocentric distances, tangible information arises from the effort required to walk a distance, thus, effort becomes associated through experience with visual distance cues. Accordingly, visual information alone is insufficient to support the awareness of distance. Perceived distance is rather specified by an invariant relationship between distal extent and a persons' potential to perform gross motion actions such as walking. This view is supported later by Proffitt et al. (2003) in which participants wore backpacks and their perceived distance increased compared with the baseline condition. Authors need to acknowledge the physical effort in addition to visual information for the spatial coding and may consider the manipulation of physical efforts in the future to support the robustness of constant intrinsic bias in ground-based spatial coding during walking.</p>
<p>Furthermore, it would be more comprehensive and fit into the Neuroscience Section if the authors can add in current understandings of the spatial reference frames in neuroscience in the introduction and discussion, and provide explanations on how the findings of this study supplement the physiological evidence that supports our spatial perception as well. For instance, world-centered representations of the environment, or cognitive maps, are associated with hippocampal formation while self-centered spatial relationships, or image spaces, are associated with the parietal cortex (see Bottini, R., &amp; Doeller, C. F. (2020). Knowledge Across Reference Frames: Cognitive Maps and Image Spaces. Trends in Cognitive Sciences, 24(8), 606-619. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2020.05.008">https://doi.org/10.1016/j.tics.2020.05.008</ext-link> for details)</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88095.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The study provides a valuable contribution by demonstrating the use of an allocentric spatial reference frame in the perception of the location of a dimly lit target in the dark. While the evidence presented in support of the authors' claims is solid and convincing, it would be beneficial for the study to address potential limitations, such as its ecological validity.</p>
<p>Strengths:</p>
<p>
Unlike previous research where observers were stationary during a visual-spatial perception task, this recent study expanded upon prior findings by incorporating bodily movements for the observers. This study is a valuable addition to the literature as it not only discovered that the intrinsic bias is grounded on the home base, but also identified several key characteristics through a series of follow-up experiments. The findings suggest that this &quot;allocentric&quot; spatial coding decays over time, requires attentional resources, can be based solely on vestibular signals, and is most effective in the horizontal direction. In general, this study is interesting, clearly presented, well-thought-out and executed. The results confirmed the conclusions and the study's comprehensive approach offers valuable insights into the nature of intrinsic bias in spatial perception.</p>
<p>The counter-intuitive results presented in the manuscript are intriguing and add to the study's overall appeal. Moreover, the manuscript draws an interesting parallel between human spatial navigation and that of desert ants. This comparison helps to underscore the importance of understanding spatial coding mechanisms across different species and highlights potential avenues for future research.</p>
<p>One aspect I particularly valued about this study was the authors' thorough description of the experimental methods. This level of detail not only highlights the rigor of the research but also enhances the reproducibility of the study, making it more accessible for future researchers.</p>
<p>Weaknesses:</p>
<p>
While the current study provides valuable insights into the nature of intrinsic bias in spatial perception, there is a concern regarding its ecological validity. The experimental design involved stringent precautions, such as a very dark room and a small target, to minimize the presence of depth cues. This is in contrast to the real world, where depth information is readily available from the ground and surrounding objects, aiding in our perception of space and depth. As a result, it is unclear to what extent this &quot;allocentric&quot; intrinsic bias is involved in our everyday spatial perception. To provide more context for the general audience, it would be beneficial for the authors to address this issue in their discussion.</p>
<p>The current findings on the &quot;allocentric&quot; coding scheme raise some intriguing questions as to why such a mechanism would be developed and how it could be beneficial. The finding that the &quot;allocentric&quot; coding scheme results in less accurate object localization and requires attentional resources seems counterintuitive and raises questions about its usefulness. However, this observation presents an opportunity for the manuscript to discuss the potential evolutionary advantages or trade-offs associated with this coding mechanism.</p>
<p>The manuscript lacks a thorough description of the data analysis process, particularly regarding the fitting of the intrinsic bias curve (e.g., the blue and gray dashed curve in Figure 3c) and the calculation of the horizontal separation between the curves. It would be beneficial for the authors to provide more detailed information on the specific function and parameters used in the fitting process and the formula used for the separation calculation to ensure the transparency and reproducibility of the study's results.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88095.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study investigated what kind of reference (allocentric or egocentric) frame we used for perception in darkness. This question is essential and was not addressed much before. The authors compared the perception in the walking condition with that in the stationary condition, which successfully separated the contribution of self-movement to the spatial representation. In addition, the authors also carefully manipulated the contribution of the waiting period, attentional load, vestibular input, testing task, and walking direction (forward or backward) to examine the nature of the reference frame in darkness systematically.</p>
<p>I am a bit confused by Figure 2b. Allocentric coordinate refers to the representation of the distance and direction of an object relative to other objects but not relative to the observer. In Figure 2, however, the authors assumed that the perceived target was located on the interception between the intrinsic bias curve and the viewing line from the NEW eye position to the target. This suggests that the perceived object depends on the observer's new location, which seems odd with the allocentric coordinate hypothesis.</p>
<p>According to Fig 2b, the perceived size should be left-shifted and lifted up in the walking condition compared to that in the stationary condition. However, in Figure 3C and Fig 4, the perceived size was the same height as that in the baseline condition.</p>
<p>Is the left-shifted perceived distance possibly reflecting a kind of compensation mechanism? Participants could not see the target's location but knew they had moved forward. Therefore, their brain automatically compensates for this self-movement when judging the location of a target. This would perfectly predict the left-shifted but not upward-shifted data in Fig 3C. A similar compensation mechanism exists for size constancy in which we tend to compensate for distance in computing object size.</p>
<p>According to Fig 2a, the target, perceived target, and eye should be aligned in one straight line. This means that connecting the physical targets and the corresponding perceived target results in straight lines that converge at the eye position. This seems, however, unlikely in Figure 3c.</p>
</body>
</sub-article>
</article>