<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">90378</article-id>
<article-id pub-id-type="doi">10.7554/eLife.90378</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.90378.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Boosting of neural circuit chaos at the onset of collective oscillations</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Palmigiano</surname>
<given-names>Agostina</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
<xref ref-type="aff" rid="a2">b</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Engelken</surname>
<given-names>Rainer</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
<xref ref-type="aff" rid="a2">b</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Wolf</surname>
<given-names>Fred</given-names>
</name>
<xref ref-type="aff" rid="a2">b</xref>
<xref ref-type="aff" rid="a3">c</xref>
<xref ref-type="aff" rid="a4">d</xref>
<xref ref-type="aff" rid="a5">e</xref>
<xref ref-type="aff" rid="a6">f</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>a</label><institution>Center for Theoretical Neuroscience, Zuckerman Institute, Columbia University</institution>, New York, <country>USA</country></aff>
<aff id="a2"><label>b</label><institution>Göttingen Campus Institute for Dynamics of Biological Networks</institution>, Göttingen, <country>Germany</country></aff>
<aff id="a3"><label>c</label><institution>Max Planck Institute for Dynamics and Self-Organization</institution>, Göttingen, <country>Germany</country></aff>
<aff id="a4"><label>d</label><institution>Bernstein Center for Computational Neuroscience</institution>, Göttingen, <country>Germany</country></aff>
<aff id="a5"><label>e</label><institution>Institute for Dynamics of Complex Systems, Georg-August University</institution>, Göttingen, <country>Germany</country></aff>
<aff id="a6"><label>f</label><institution>Max Planck Institute for Multidisciplinary Sciences</institution>, Göttingen, <country>Germany</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Gjorgjieva</surname>
<given-names>Julijana</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Technical University of Munich</institution>
</institution-wrap>
<city>Freising</city>
<country>Germany</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>To whom correspondence should be addressed. E-mail:<email>fred.wolf@ds.mpg.de</email></corresp>
<fn fn-type="others"><p>AP and FW designed research; AP performed the delayed-network related research while RE performed the input-driven network related research. AP wrote the paper with input from FW and RE.</p></fn>
<fn fn-type="conflict"><p>The authors declare no conflict of interest</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-11-28">
<day>28</day>
<month>11</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP90378</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-07-17">
<day>17</day>
<month>07</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-06-21">
<day>21</day>
<month>06</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.08.28.505598"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Palmigiano et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Palmigiano et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-90378-v1.pdf"/>
<abstract>
<p>Neuronal spiking activity in cortical circuits is often temporally structured by collective rhythms. Rhythmic activity has been hypothesized to regulate temporal coding and to mediate the flexible routing of information flow across the cortex. Spiking neuronal circuits, however, are non-linear systems that, through chaotic dynamics, can amplify insignificant microscopic fluctuations into network-scale response variability. In nonlinear systems in general, rhythmic oscillatory drive can induce chaotic behavior or boost the intensity of chaos. Thus, neuronal oscillations could rather disrupt than facilitate cortical coding functions by flooding the finite population bandwidth with chaotically-boosted noise. Here we tackle a fundamental mathematical challenge to characterize the dynamics on the attractor of effectively delayed network models. We find that delays introduce a transition to collective oscillations, below which ergodic theory measures have a stereotypical dependence on the delay so far only described in scalar systems and low-dimensional maps. We demonstrate that the emergence of internally generated oscillations induces a complete dynamical reconfiguration, by increasing the dimensionality of the chaotic attractor, the speed at which nearby trajectories separate from one another, and the rate at which the network produces entropy. We find that periodic input drive leads to a dramatic increase of chaotic measures at a the resonance frequency of the recurrent network. However, transient oscillatory input only has a moderate role on the collective dynamics. Our results suggest that simple temporal dynamics of the mean activity can have a profound effect on the structure of the spiking patterns and therefore on the information processing capability of neuronal networks.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Abstract and introduction for re-submission</p></fn>
</fn-group>
</notes>
</front>
<body>
<p>Cortical spiking patterns are unreliable and vary from trial to trial in response to identical stimuli (<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>). Multiple factors contribute to this noise entropy – variability unrelated to the stimulus or to the network state – from network-based deterministic chaos (<xref ref-type="bibr" rid="c3">3</xref>–<xref ref-type="bibr" rid="c6">6</xref>) to stochasticity in the action potential generation (<xref ref-type="bibr" rid="c7">7</xref>) and transmission (<xref ref-type="bibr" rid="c8">8</xref>). Noise entropy necessarily interferes with the robust representation of spiking patterns and code-word based information flow, and poses the question of which possible mechanisms cortical networks implement to represent and transmit information in the presence of this undesired variability. One possibility is that cortex integrates each-cell spikes in a few ms window to represent stimuli. In this rate code, spurious changes in spiking patters contribute to asynchronous irregular dynamics with low correlations. However, if precisely-timed spike patterns are the code used by cortical circuits, spiking variability is an undesired feature, and other network mechanisms should exist to control it.</p>
<p>Neuronal oscillations have been proposed to be a backbone organizing neuronal firing (<xref ref-type="bibr" rid="c9">9</xref>) that presumably quenches spikepattern variability by reducing the number of possible network configurations. Transient oscillatory (but not strongly synchronous) activity can aid the flexible gating information flow (<xref ref-type="bibr" rid="c10">10</xref>), but whether any type of oscillatory activity is able to tame noise entropy is still an unanswered question. In fact, from a theoretical perspective, whether noise entropy due to chaotic dynamics is intensified or quenched by collective oscillations is not easily predictable. First, the internallygenerated population oscillation can be thought of as a common harmonic drive to all the cells in the network. Generally, whether a harmonic drive will intensify or quench chaos in a given system is not possible to know a priory, and both effects have been observed (<xref ref-type="bibr" rid="c11">11</xref>–<xref ref-type="bibr" rid="c14">14</xref>). Second, oscillatory activity has been theoretically (<xref ref-type="bibr" rid="c15">15</xref>–<xref ref-type="bibr" rid="c20">20</xref>) and experimentally (<xref ref-type="bibr" rid="c21">21</xref>–<xref ref-type="bibr" rid="c24">24</xref>) traced back to delayed recurrent inhibition, whose effect is also difficult to predict: On the one hand, delayed interactions can induce highly synchronized and therefore highly structured activity (<xref ref-type="bibr" rid="c25">25</xref>–<xref ref-type="bibr" rid="c27">27</xref>), possibly shrinking the dimensionality of the dynamics and quenching variability. On the other hand, the dimensionality of the chaotic attractor increases linearly with increasing delays (<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c29">29</xref>) in simple systems, and it is possible that similar dependencies occur in more complex systems than the ones studied so far.</p>
<p>Here we investigate whether the emergence of collective oscillations at the population level quenches noise entropy by synchronizing neuronal firing or contributes to an expansion of the number of the possible spike-pattern configurations explored by the circuit by intensifying chaos. To do so, we advance the tractability of large neuronal spiking networks of exactly solvable neuronal models by developing a strategy that allows analyzing the dynamics of delayed spiking networks in a system with fixed and finite degrees of freedom. By building a two-stage model of the action potential generation and transmission, we map the infinite-dimensional system to a finite one and semi-analytically compute the entire spectrum of Lyapunov exponents whose positive sum is a direct proxy for the network’s noise entropy (<xref ref-type="bibr" rid="c6">6</xref>). We find that when delays are small, the dimensionality of the attractor increases linearly with the delay while keeping noise entropy constant, analogously to what has been reported in simplified delayed systems (<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c29">29</xref>). Larger delays destabilize the asynchronous irregular dynamics of the spiking network, giving rise to population oscillations in the beta frequency range (∼ 20 Hz). We show that this transition is achieved via Hopf bifurcation of the mean activity at a critical set of parameters that can be computed exactly, and is analogous to what is reported in network models without a dynamical mechanism for action potential generation (<xref ref-type="bibr" rid="c15">15</xref>–<xref ref-type="bibr" rid="c18">18</xref>). Unexpectedly, the noise entropy, together with the dimensionality of the chaotic attractor, is boosted at the oscillatory instability. To investigate the mechanism that underlies this effect, we drive a network with a periodic external input, to first order, analogous to the one that the network receives at the oscillatory transition. We find that the external periodic drive dramatically boosts noise entropy in the neighborhood of the resonant frequency of the network, and quenches it otherwise. We find that a realistic drive, in which collective activity has short transients of oscillatory activity with drifting frequencies, leads to only moderate changes in the noise entropy. Our results indicate that simple dynamics of the mean activity can have a profound effect on the chaotic dynamics of the network and that realistic transient synchrony signals could, beyond flexibly gating information transmission, prevent boosting of undesired variability in the cortex.</p>
<sec id="s1">
<title>Results</title>
<sec id="s1a">
<title>Delayed networks with a finite-dimensional phase space</title>
<p>Spiking networks have an innate source of instability, the generation of the action potential, which leads to chaotic dynamics even when the statistics of the population firing rate are independent of time (<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c6">6</xref>). The noise entropy generated by these networks, present even in the absence of any stochastic source of variability, can be computed through the spectrum of Lyapunov exponents (<xref ref-type="bibr" rid="c6">6</xref>) (see <italic>Methods</italic>). Therefore, given a group of control parameters that control the transition to oscillations in the network we can ask: does the onset of collective rhythms imply a reduction of the intensity of chaos and a more robust representation of the network’s inputs? Or does it imply the opposite?</p>
<p>One well-known parameter controlling the transition to collective oscillations in brain-like inhibitory-dominated networks is the synaptic delay (<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c31">31</xref>) (<xref rid="fig1" ref-type="fig">Fig. 1 <bold>a-b</bold></xref>). This parameter, despite being inescapable in biology, introduces a new degree of complexity: even simple scalar differential equations (<xref rid="fig1" ref-type="fig">Fig. 1<bold>c</bold></xref>) are transformed into an infinite-dimensional system when incorporating delayed feedback (<xref rid="fig1" ref-type="fig">Fig. 1<bold>d</bold></xref>). Whether a discrete spectrum of Lyapunov exponents generally exists in delayed systems, and whether it can be computed exactly, remains poorly understood. The most general strategy proposed so far suggests binning the delay period and expanding the dimensionality of the system by the number of bins considered (<xref ref-type="bibr" rid="c32">32</xref>). However, this procedure becomes arbitrarily costly with increasing resolution, even in scalar equations.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1.</label>
<caption><title>Fundamental challenges of the study of oscillations due to delayed interactions.</title>
<p><bold>(a)</bold> Neuronal network without synaptic delays. Whenever the unit with dynamical variable <italic>ϕ</italic><sub><italic>j</italic></sub> spikes, its spike is immediately relayed. <bold>(b)</bold>. Delayed network. Each time a neuron <italic>j</italic> is active, it takes a time <italic>δ</italic> for the spike to propagate to the postsynaptic neurons. In the case of delayed pulse-coupled oscillators, this network can be mapped to a finite size system, with variable dimension (<xref ref-type="bibr" rid="c33">33</xref>). <bold>(c)</bold> An <italic>N</italic> dimensional deterministic dynamical system is defined by <italic>N</italic> scalar initial conditions.<bold>(d)</bold> In a delayed dynamical system, each of the <italic>N</italic> variables will be initialized by a history function. In this case, the system will be infinite-dimensional.</p></caption>
<graphic xlink:href="505598v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We developed a strategy that allows for studying the impact of collective oscillations on the properties of the network’s chaos using delayed recurrent inhibition as the control parameter in a numerically exact and efficient way. We include delays dynamically by designing a two-stage singleneuron model composed of a soma, modeled as a quadratic integrate-and-fire neuron (<italic>ϕ</italic>), and a single compartment axon (SCA, <italic>ξ</italic>, see <xref rid="fig2" ref-type="fig">Fig. 2<bold>a</bold></xref>, <italic>Methods</italic> and <xref rid="figS1" ref-type="fig">Fig. S1</xref> of the <italic>Supplementary Information</italic>). The SCA is modeled as an excitable spiking unit, resting in a steady state, which depolarizes at spike arrival. Post-synaptic delays are introduced as the time it takes for the axon to spike and relay its input. This strategy allows mapping the infinite-dimensional system to a system with 2<italic>N</italic> dimensions, under the assumption that the interspike interval is much larger than the delay <italic>δ</italic>. Numericallyexact event-based simulations of a network of inhibitory neurons with short delays (see below for E-I networks) exhibit the characteristic dynamics of non-delayed balanced state networks, with asynchronous irregular spiking patterns (<xref rid="fig2" ref-type="fig">Fig. 2<bold>b</bold></xref>), broad firing rate distributions (<xref rid="fig2" ref-type="fig">Fig. 2<bold>c</bold></xref>) and a large coefficient of variation of the inter spike intervals (<xref rid="fig2" ref-type="fig">Fig. 2<bold>d</bold></xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig. 2.</label>
<caption><title>Extensive chaos in delayed balanced circuits.</title>
<p><bold>(a)</bold> Circuit diagram of the two-stage neuronal model. Each neuron (brown) is composed of a soma (orange) linked to its single-compartment-axon (SCA, green). The link weight <italic>J</italic><sub><italic>lj</italic></sub> is such that when the soma spikes (i.e. <italic>ϕ</italic><sub><italic>j</italic></sub> reaches threshold), the SCA (<italic>ξ</italic><sub><italic>l</italic></sub>) is released from its fixed-point steadystate, relaying its input after a delay <italic>δ</italic> exactly defined by their connection weight (see <xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref> <italic>Methods</italic>). Each soma receives on average the activity of <italic>K</italic> neighbors with weight <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline92.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and external inputs of order <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline93.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. <bold>(b)</bold> Raster plot of network’s activity. <bold>(c)</bold> Firing rate distribution (left) and <bold>(d)</bold> CV distribution (right) for a small (1 ms) delay. <bold>(e)</bold> Terms involved in the calculation of the single-spike Jacobian. When an SCA (soma) spikes, the state of the postsynaptic soma (SCA) is propagated via the function <italic>f</italic> (<italic>η</italic>) from the previous spike time <italic>t</italic><sub><italic>s</italic></sub> to the new spike time <italic>τ</italic><sub><italic>s</italic>+1</sub>, then updated via <italic>g</italic> (<italic>γ</italic>), and then propagated up to the next spike time. Then the limit of <italic>τ</italic><sub><italic>s</italic>+1</sub> → <italic>t</italic><sub><italic>s</italic>+1</sub> is taken (see <italic>Methods</italic>). <bold>(f)</bold> Chaos descriptors: KaplanYork dimension (top) and the metric entropy (bottom). <bold>(g)</bold> Spectrum of Lyapunov exponents. Shaded area are the positive exponents. <bold>(h)</bold> Spectrum for different network sizes as a function of the normalized index of the exponent. A system-size invariant Lyapunov spectrum indicates extensive chaos <bold>(i)</bold> First Lyapunov exponent <bold>(j)</bold> Attractor dimension <bold>(k)</bold> and Metric entropy as a function of system size (N).</p></caption>
<graphic xlink:href="505598v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We compute the entire spectrum of Lyapunov exponents by deriving an analytical expression of the Jacobian of the effectively delayed network that is evaluated numerically at each spike time. The Jacobian’s non-trivial elements will characterize either the change in the state of a post-synaptic soma to an infinitesimal change in the state of the pre-synaptic SCA (<xref rid="fig2" ref-type="fig">Fig. 2<bold>e</bold></xref> top) or the change in the state of an SCA to an infinitesimal change in the state of the pre-synaptic soma (<xref rid="fig2" ref-type="fig">Fig. 2<bold>e</bold></xref> bottom, see <italic>Methods</italic> and <italic>Supplementary Information</italic> for an in-depth description).</p>
<p>The spectrum of exponents is shown in <xref rid="fig2" ref-type="fig">Figure 2<bold>g</bold></xref> (see <xref rid="figS2" ref-type="fig">Fig. S2</xref> of the <italic>Supplementary Information</italic> for different parameter choices of the SCA and S3 for convergence). The spectrum is invariant of network size (N), hinting at a continuous density of Lyapunov exponents for infinitely large networks (<xref rid="fig2" ref-type="fig">Fig. 2<bold>h</bold></xref>), and a network-size independent first Lyapunov exponent (<xref rid="fig2" ref-type="fig">Fig. 2<bold>i</bold></xref>). The attractor dimension (<xref rid="fig2" ref-type="fig">Fig. 2<bold>j</bold></xref>) and the metric entropy, a proxy for the noise entropy in the network (<xref ref-type="bibr" rid="c6">6</xref>) (<xref rid="fig2" ref-type="fig">Fig. 2<bold>j</bold></xref>), grow linearly with network size corresponding to a network-size invariant spectrum, which invites defining <italic>intensive</italic> measures of dimension <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline1.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and metric entropy <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline2.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the latter in units of bits per second.</p>
</sec>
<sec id="s1b">
<title>Boosting and collapse of chaos</title>
<p>As expected, sufficiently long delays destabilize the asynchronous irregular state in balanced-state networks of QIF neurons and lead to a state of collective oscillations in which the irregular dynamics at the single cell level is preserved (<xref rid="fig3" ref-type="fig">Fig. 3<bold>a</bold></xref>). The mean square deviation (MSD, see <italic>Methods</italic>) of the mean firing rate has a threshold-linear dependence on the delay, which is the signature of a Hopf bifurcation at the level of the population mean, and is consistent with previously described oscillatory transitions to analogous states in related systems (15, 18, 19, 31, 34). The critical delay <italic>δ</italic><sub><italic>MSD</italic></sub> that indicates the onset of oscillatory activity depends on the parameters of the network, decreasing with increasing connection strength and target mean firing rate, and becoming arbitrarily small for large in-degree (see below). A second instability, now to full network synchrony, occurs at a second critical delay. The collective oscillation, in the beta range up to that point, collapses to a fully synchronous state with a frequency defined by the network’s target mean firing rate of 5 Hz (<xref rid="fig3" ref-type="fig">Fig. 3<bold>c</bold></xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig. 3.</label>
<caption><title>Boosting and collapse of chaos with increasing delayed inhibition.</title>
<p><bold>(a)</bold> Diagram of an inhibitory network with dynamic delays, each neuron is as in <xref ref-type="fig" rid="fig2">Fig. 2a</xref>. The mean firing rate of the network is kept constant at 5 Hz by adjusting the homogeneous external input that each neuron receives. <bold>(b)</bold> Left: Mean squared deviation from the mean population firing rate (MSD, vanishing values indicates an asynchronous state) as a function of the synaptic delay for different values of the average in-degree <italic>K</italic>. The larger <italic>K</italic>, the smaller the delay <italic>δ</italic> needed to destabilize the asynchronous irregular state. Numbered circles indicate the points for which raster plots are shown. <bold>(c)</bold> Frequency of the oscillation is in the beta range and monotonically decreases with the delay. After a <italic>K</italic>-dependent critical delay <italic>δ</italic><sub><italic>c</italic></sub>, the network collapses to a limit cycle (complete synchrony), and the network oscillates at the target firing rate <italic>ν</italic> (see <italic>Methods</italic>) <bold>(d)</bold> Maximum Lyapunov exponent as a function of <italic>δ</italic> for different values of <italic>K</italic>. <bold>(e)</bold> A third of the spectrum of Lyapunov exponents as a function of the delay for <italic>K</italic> = 75. The blue line in panel (d) is the bottom row of this panel. <bold>(f)</bold> Intensive metric entropy (<italic>h</italic> = <italic>H/N</italic>) divided by the mean firing rate <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline94.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, giving units of bits per spike, and <bold>(g)</bold> attractor dimension <italic>d</italic> = <italic>D/N</italic>, as a function of the delay. Both the metric entropy and the attractor dimension have a non-monotonic dependence with the delay prior to the collapse to a limit-cycle.</p></caption>
<graphic xlink:href="505598v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Surprisingly, we find that contrary to the intuition of increased regularity in oscillatory networks, chaos is boosted at the (first) oscillatory transition. The maximum Lyapunov exponent (LE) decreases monotonically with increasing delays up to the oscillatory instability (<xref rid="fig3" ref-type="fig">Fig. 3<bold>d</bold></xref>) together with around 6% of the LEs (<xref rid="fig3" ref-type="fig">Fig. 3<bold>e</bold></xref>, see also <xref rid="figS4" ref-type="fig">Fig. S4</xref> of the <italic>Supplementary Information</italic>). Simultaneously, the number of positive LEs increases monotonically (<xref rid="figS4" ref-type="fig">Fig. S4<bold>a</bold></xref>). These competing effects contribute to the remarkable fact that the metric entropy remains independent of the delay up to until the oscillatory instability (<xref rid="fig3" ref-type="fig">Fig. 3<bold>f</bold></xref>, compare to dashed line). The attractor dimension, on the other hand, is dominated by the number of positive LEs and increases monotonically up to the transition (<xref rid="fig3" ref-type="fig">Fig. 3<bold>g</bold></xref>). Interestingly, up to the oscillatory instability the ergodic properties of the pulse-coupled highdimensional network studied here match the dependencies reported for low-dimensional systems (and also the dependences of rate networks, see below); with increasing delay the maximum Lyapunov exponent decreases, the attractor dimension increases, while the metric entropy is independent of the delay (<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c36">36</xref>).</p>
<p>At the critical delay, all chaotic measures intensify. Looking at a fraction of the Lyapunov spectrum as a function of the delay (<xref rid="fig3" ref-type="fig">Fig. 3<bold>e</bold></xref>) reveals that i) all the positive LEs increase in value after the transition and ii) the total number of positive LEs initially increases monotonically after the first oscillatory transition but later decreases (<xref rid="figS4" ref-type="fig">Fig. S4<bold>a</bold></xref>). The entropy and the attractor dimension at the transition can only increase given the increase in the magnitude and the amount of positive LEs (<xref rid="fig3" ref-type="fig">Fig. 3<bold>f</bold>,<bold>g</bold></xref>). Although the positive LEs grow larger in magnitude, their number decreases soon after the transition; the uneven contribution of these opposing effects gives rise to the non-monotonic behavior of the entropy and the attractor dimension. These dependencies are not a finite size effect (see <xref rid="figS3" ref-type="fig">Fig. S3<bold>d</bold></xref>) and we have not observed signs of bistability (<xref ref-type="bibr" rid="c37">37</xref>).</p>
<p>We compare the original, infinite-dimensional delayed system with the equivalent finite system studied here by selfconsistently computing the instability periphery in parameter space (following Ref. (<xref ref-type="bibr" rid="c38">38</xref>), see <italic>Methods</italic>). We compute the critical value of the delay at which the mean firing rate (in the original, delayed system) undergoes a Hopf bifurcation, and we compare it to the one obtained numerically by estimating the critical delay from the MSD’s dependency on the delay, <italic>δ</italic><sub><italic>MSD</italic></sub> (<xref rid="fig4" ref-type="fig">Fig. 4<bold>c</bold></xref>) the first LE, <italic>δ</italic><sub><italic>LE</italic></sub>, (<xref rid="fig4" ref-type="fig">Fig. 4<bold>d</bold></xref>) and the metric entropy, <italic>δ</italic><sub><italic>H</italic></sub>, (<xref rid="fig4" ref-type="fig">Fig. 4<bold>e</bold></xref>). Concretely, we compute the delay value at which the derivative of the maximum LE with respect to the delay changes from negative to positive, the delay at which the derivative of the metric entropy departs from zero, and the delay at which the derivative of the attractor dimension departs from a constant value. The accuracy of the match in panels (<italic>c-e</italic>) depends on both the estimation of the critical parameters from ergodic theory quantities, which are sensitive to simulation size (see <xref rid="fig4" ref-type="fig">Fig. 4</xref>), and details of the implementation of the self-consistent estimation of the critical delay in the original system (see also <xref rid="figS5" ref-type="fig">Fig. S5</xref> in the <italic>Supplementary Information</italic>). Despite the assumption of fixed in-degree implicit in the semi-analytic method used here to compute the critical delay (<xref ref-type="bibr" rid="c38">38</xref>), broken in our simulations, we find good agreement.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig. 4.</label>
<caption><title>Chaos is boosted at the oscillatory instability</title>
<p><bold>(a)</bold> We compare the infinite-dimensional system (original delayed system) with the system with fixed and finite degrees of freedom studied here. <bold>(b)</bold> Phase diagram (computed semi-analytically following (<xref ref-type="bibr" rid="c38">38</xref>)) showing the critical delay at which the asynchronous irregular state loses stability to collective oscillations, as a function of the mean degree K and the target mean firing rate. <bold>(c)</bold> Comparison between the critical delay estimated from simulations as when the MSD departs from zero (<italic>δ</italic><sub><italic>MSD</italic></sub>) and the one from the true delayed system (<italic>δ</italic><sub><italic>A</italic></sub>) <bold>(d)</bold> Same as c) but for the critical delay estimated in simulations is estimated from when the first Lyapunov exponent increases <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline95.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> <bold>(e)</bold> Same as c) but for the critical delay estimated from the increase in the metric entropy (<italic>δ</italic><sub><italic>H</italic></sub>)</p></caption>
<graphic xlink:href="505598v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Does the emergence of collective oscillations always lead to boosting of chaos? We test two other mechanisms that induce oscillations in the network. First, as suggested in <xref rid="fig4" ref-type="fig">Fig. 4</xref>, increasing the average degree (K) can induce oscillations in this network even in the absence of delayed interactions. <italic>Supplementary Figure</italic> S6 demonstrates that all indicators of chaos (<italic>λ</italic><sub>1</sub>, <italic>d</italic> and <italic>h</italic>) also increase at the oscillatory transition as a function of the mean degree K. Second, we find that in non-delayed excitatory-inhibitory networks, there is a transition to a balanced oscillatory regime with an increasing inhibitory time constant, in agreement to what is reported in binary networks (<xref ref-type="bibr" rid="c39">39</xref>). We find that, right after the transition to collective oscillations, all indicators of chaos are intensified. Taken together, our results suggest that the transition from an asynchronous irregular activity to collective oscillations, although only slightly changing the single neuron spiking statistics, induces a full reconfiguration of the network dynamics by simultaneously increasing the maximum Lyapunov exponent, the metric entropy and the dimension of the chaotic attractor.</p>
</sec>
<sec id="s1c">
<title>Single-cell heterogeneity perpetuates chaos</title>
<p>In a network in which all the neurons are the same and receive the same inputs, the only source of disorder is from the sparse connectivity. Increasing delays in these types of networks pushes neurons to align their activity to the ongoing rhythm until the dynamics of the network collapses to a limit cycle (<xref rid="fig3" ref-type="fig">Fig 3</xref>, <italic>δ</italic> ≈ 2.1 ms).</p>
<p>How robust is this pathological state of full synchrony? Would it be observable in cortex where cell and cell-type heterogeneity are abundant? We find that the inclusion of even moderate amounts of single-cell heterogeneity not only curbs the synchronization capability of the network (<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c40">40</xref>, <xref ref-type="bibr" rid="c41">41</xref>) (<xref rid="fig5" ref-type="fig">Fig. 5<bold>a</bold></xref>), but also safeguards a biophysically plausible regime by eliminating the second oscillatory instability leading to a pathological full-synchrony state (<xref ref-type="fig" rid="fig5">Fig.5<bold>b</bold></xref>). In this case, the maximum LE (and also the coefficient of variation, see <xref rid="figS9" ref-type="fig">Fig. S9</xref> and <xref ref-type="fig" rid="figS10">S10</xref> of the <italic>Supplementary Information</italic>) peaks at the delay that would have led to a full-synchrony state in the absence of heterogeneity. The network, instead of collapsing to a limit cycle, retains the irregular firing patterns and the microscopic chaos associated with it for delays as long as the membrane time constant (<xref ref-type="fig" rid="fig5">Fig.5<bold>b-h</bold></xref>, see also <xref rid="figS8" ref-type="fig">Fig. S8</xref> in the <italic>Supplementary Information</italic>). The ghost of the second oscillatory transition marks a regime in which increasing delays can only further align the neurons to the ongoing rhythm, resulting in a vanishingly small metric entropy for large delays (<xref ref-type="fig" rid="fig5">Fig.5<bold>f</bold></xref>). Interestingly, for large delays the maximum LE can have the same value as with no delay while the metric entropy undergoes a ten-fold decrease, indicating that simple numerical estimates of the maximum LE could not reveal the full extent of the dynamical reconfiguration the network undergoes after the second oscillatory transition. These results are robust at all network parameters (the connection strength, the target mean firing rate, and the average degree K) as shown in <xref rid="figS9" ref-type="fig">Figures S9</xref> and <xref ref-type="fig" rid="figS10">S10</xref> of the <italic>Supplementary Information</italic>.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig. 5.</label>
<caption><title>Single cell and cell-type heterogeneity perpetuate chaos.</title>
<p><bold>(a)</bold> Network of heterogeneous inhibitory neurons. <bold>(b)</bold> Maximum Lyapunov exponent <bold>(c)</bold>, network-size intensive metric entropy normalized by the firing rate and <bold>(d)</bold> intensive attractor dimension as a function of the delay for different values of heterogeneity. The homogeneous case (of <xref ref-type="fig" rid="fig3">Fig. 3</xref>) is shown for reference. <bold>(e)</bold> Spiking patterns at different values of the synaptic delays. Notice how despite the seemingly strong synchronization for large delays, the network remains strongly chaotic. <bold>(f)</bold> Diagram of a network of <italic>N</italic> neurons, <italic>N</italic><sub><italic>E</italic></sub> = 0.8<italic>N</italic> excitatory and <italic>N</italic><sub><italic>I</italic></sub> = 0.2<italic>N</italic> inhibitory, in the balanced state. The connectivity matrix is proportional to <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline96.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> with entries <italic>J</italic><sub><italic>EE</italic></sub> ∝ <italic>ϵη, J</italic><sub><italic>IE</italic></sub> ∝ <italic>ϵη</italic>, <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline97.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline98.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and <italic>η</italic>=0.9 <bold>(g)</bold> Maximum LE as a function of the delay for different strength of the excitatory to inhibition connectivity <italic>ϵ</italic>. Dashed dark gray is a network of <italic>N</italic> inhibitory neurons. <bold>(h)</bold> Intensive metric entropy as a function of the delay for different values of <italic>ϵ</italic> and for a network of purely inhibitory neurons of size <italic>N</italic> (dashed dark gray) and <italic>N</italic><sub><italic>I</italic></sub> (dashed light gray). <bold>(i)</bold> Intensive attractor dimension as a function of the delay for different values of <italic>ϵ</italic> and for a network of purely inhibitory neurons of size <italic>N</italic> (dashed dark gray) and <italic>N</italic><sub><italic>I</italic></sub> (dashed light gray). <bold>(j)</bold> Spiking patterns. Spikes in red are of E neurons, while blue are of I.</p></caption>
<graphic xlink:href="505598v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The presence of strong recurrent excitation introduces a new source of network instability. Stable cortical circuits that would be unstable in the absence of feedback inhibition are called inhibition stabilized (<xref ref-type="bibr" rid="c42">42</xref>, <xref ref-type="bibr" rid="c43">43</xref>), with balanced networks being one subclass of those. We next asked whether this more general class of model would also exhibit boosting of chaos at the oscillatory transition. We study the two cell-type case by including recurrent excitation in a four-to-one ratio, as observed in cortex(<xref ref-type="bibr" rid="c44">44</xref>) (<xref rid="fig5" ref-type="fig">Fig. 5<bold>e</bold></xref>). We require identical input currents statistics to inhibitory and excitatory neurons (see <italic>Methods</italic>), and identical delays within and across the populations. To guarantee the existence of a balanced state, we used a parametrization (<xref ref-type="bibr" rid="c3">3</xref>) that relies on the strength of the E to I connections, <italic>ϵ</italic>. We find that the strength of the EI loop has a stabilizing effect on the asynchronous irregular regime, requiring increasingly longer delays for the oscillatory instability to occur. We notice that when the inputs to the different cell-type populations are identical, like in this case, the network synchronizes without a lag between the E and I populations (<xref ref-type="bibr" rid="c19">19</xref>). By replacing 80% of the neurons with excitatory neurons, the network is comparatively less chaotic for moderate delays (<xref rid="fig5" ref-type="fig">Fig. 5<bold>g-i</bold></xref>, compare dashed dark-grey line with colored ones). On the other hand, for a fixed amount of inhibitory neurons, increasing the intensity of the E-I feedback loop <italic>ϵ</italic> intensifies chaos for all delays (<xref rid="fig5" ref-type="fig">Fig. 5 <bold>g-i</bold></xref>, compare dashed light-grey line with colored ones, see also <xref rid="figS11" ref-type="fig">Fig. S11</xref> of the <italic>Supplementary Information</italic>).</p>
</sec>
<sec id="s1d">
<title>Absence of oscillatory boosting in rate networks</title>
<p>Next, we asked whether rate networks, a fundamentally different type of neuronal network model, would exhibit a transition to collective oscillations with delayed interactions, and if so, whether this transition would lead to an increase in noise entropy. We analyze a variety of both classic (<xref ref-type="bibr" rid="c45">45</xref>) and purely inhibitory rate networks (<xref ref-type="bibr" rid="c46">46</xref>), which we name weakly-delayed rate networks. We incorporate delays perturbatively by expanding the delayed activity around the non-delayed activity in powers of the delay up to order <italic>k</italic> (𝒪 (<italic>k</italic>)) (see <italic>Methods</italic>, and Section 3 of the <italic>Supplementary Information</italic>). By incorporating this expansion, we increase the degrees of freedom of a non-delayed network by <italic>N</italic> 𝒪 (<italic>k</italic>), where N is the number of neurons. A scheme of the effective network after a second order expansion is shown in <xref ref-type="fig" rid="figS13"><italic>Supplementary Figure</italic> S13<bold>a</bold></xref>.</p>
<p>We analyze the dependence on the delay for the balanced inhibitory network and find that, although it has a transition from a chaotic regime (<xref rid="figS14" ref-type="fig">Fig. S14<bold>a</bold></xref> top panel) to clock-like synchrony (<xref rid="figS14" ref-type="fig">Fig. S14<bold>a</bold></xref> bottom) after a critical delay as in the spiking network without heterogeneity, we did not observe a coexistence of chaos with a collective oscillation at intermediate delays (unlike (<xref ref-type="bibr" rid="c47">47</xref>) under the presence of feedback). We computed the entire spectrum of Lyapunov exponents of the classic (<xref rid="figS13" ref-type="fig">Fig. S13</xref>) and the inhibitory network (<xref rid="figS14" ref-type="fig">Fig. S14</xref>) in the standard way (see Ref. (<xref ref-type="bibr" rid="c48">48</xref>)) as a function of the synaptic delay. We find that, before the collapse to a limit cycle in the inhibitory network and for all the delays analyzed in the classic network, the maximum Lyapunov exponent decreases (<xref rid="figS13" ref-type="fig">Figs. S13<bold>e</bold></xref> and <xref ref-type="fig" rid="figS14">S14<bold>c</bold></xref>), the metric entropy is constant (<xref rid="figS13" ref-type="fig">Figs. S13<bold>g</bold></xref> and <xref ref-type="fig" rid="figS14">S14<bold>e</bold></xref>) and the attractor dimension increases linearly with the delay (<xref rid="figS13" ref-type="fig">Figs. S13<bold>f</bold></xref> and <xref ref-type="fig" rid="figS14">S14<bold>d</bold></xref>), as we observed in spiking networks before the first oscillatory instability, and as was described in scalar differential equations and low dimensional maps (<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c29">29</xref>).</p>
</sec>
<sec id="s1e">
<title>Resonant boosting of chaos</title>
<p>If the mere existence of collective oscillations was a sufficient condition to observe an intensification of all chaotic measures, then driving the network with an inhomogeneous Poisson process, harmonically modulated at frequencies smaller than the cutoff (<xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c49">49</xref>), should induce oscillations at the population level and intensify chaos as quantified by all ergodic measures described above.</p>
<p>To test this hypothesis, we first include an external harmonic drive to a non-delayed purely inhibitory network. Each neuron receives, besides the recurrent input, an external input composed of K spike-trains that are sampled from an inhomogeneous Poisson process with a harmonically modulated rate with baseline <italic>I</italic><sub>0</sub>, amplitude <italic>I</italic><sub>1</sub>, and frequency <italic>f</italic> (<xref rid="fig6" ref-type="fig">Fig. 6<bold>a</bold></xref>, left). We find that increasing the amplitude of the harmonic drive successfully induces population oscillations (<xref rid="fig6" ref-type="fig">Fig. 6<bold>a</bold></xref>, right panels) as apparent from the raster plots. Interestingly, for low frequencies, even if there is a collective rhythm, chaos is not intensified but is reduced instead. The larger the amplitude of the oscillatory modulation of the incoming spike-trains, the more strongly reduced the first Lyapunov exponent (<xref rid="fig6" ref-type="fig">Fig. 6<bold>b</bold></xref>), the metric entropy (<xref rid="fig6" ref-type="fig">Fig. 6<bold>c</bold></xref>) and the attractor dimension are (<xref rid="fig6" ref-type="fig">Fig. 6<bold>d</bold></xref>). Reduction of chaos by an external noisy input has been previously observed in very similar systems to the one studied here (<xref ref-type="bibr" rid="c6">6</xref>) and has been extensively characterized both analytically and numerically in rate networks (14, 48, 50–52).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Fig. 6.</label>
<caption><title>Resonant boosting of chaos is quenched by transient synchrony</title>
<p><bold>(a)</bold> (left) Diagram of a non-delayed inhibitory network driven by harmonic external input. Each neuron receives, besides its recurrent input, K external spike trains sampled from an inhomogeneous Poisson process with a harmonically modulated rate with baseline <italic>I</italic><sub>0</sub>, amplitude <italic>I</italic><sub>1</sub>, and frequency <italic>f</italic>, designed to mimic the oscillatory input after the transition to oscillations in <xref ref-type="fig" rid="fig3">Figure 3 <bold>(a)</bold></xref> (right) Raster plots of a network activity driven by external inputs of frequencies <italic>f</italic> = {1, 20} Hz. <bold>(b)</bold> maximum Lyapunov exponent, <bold>(c)</bold> metric entropy and <bold>(d)</bold> attractor dimension, as a function of the frequency of the cosine-modulated Poisson input <italic>f</italic>, for increasing values of the input drive’s oscillatory amplitude <italic>I</italic><sub>1</sub>. The top dashed line in panel (c) is the value the entropy with a constant input (not Poisson, i.e. that of <xref ref-type="fig" rid="fig3">Fig. 3</xref>). The bottom dashed line is that of a non-harmonically modulated Poisson drive. Notice how the external input drive reduces chaos proportionally to the intensity of the drive for smaller frequencies and how this behavior is reversed in the neighborhood of the resonant frequency of the network.<bold>(e)</bold> (left) Diagram of a non-delayed inhibitory network driven by an external input with transient oscillations. Each neuron receives K external spike trains sampled from an inhomogeneous Poisson process with a rate given by transient oscillations of a different spiking network model (see (<xref ref-type="bibr" rid="c10">10</xref>) and <italic>Methods</italic>) with baseline <italic>I</italic><sub>0</sub>, amplitude <italic>I</italic><sub>1</sub>, and peak frequency <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline99.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. <bold>(e)</bold> (right) Raster plots of a network activity driven by external inputs of with peak frequencies of <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline100.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> Hz. <bold>(f)</bold> Maximum Lyapunov exponent, <bold>(g)</bold> metric entropy and <bold>(h)</bold> Attractor dimension, as a function of the frequency of the transient-oscillation-modulated Poisson input <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline101.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, for increasing values of the input drive’s oscillatory amplitude <italic>I</italic><sub>1</sub>. Notice how, compared to the harmonic case, a drive with transient synchrony and drifting frequencies, recruits responses of the network that would either reduce or intensify chaos.</p></caption>
<graphic xlink:href="505598v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The network has a non-monotonic response to oscillatory inputs of increasing frequency, as indicated by the pronounced peak of synchronization index in Fig. <bold>??b</bold>. If we were to increase the coupling strength of the network to a critical value of K, this peak would develop into a full resonance (<xref ref-type="bibr" rid="c38">38</xref>). The generation of self-sustained oscillations at the population level shown in <xref rid="fig3" ref-type="fig">Fig. 3</xref> is reached when the drive that generates a resonance in the network is internally generated. Here we show that for values of weaker coupling than the critical ones, driving the network with an oscillatory drive already shows signatures of this developing resonance, and that being in the neighborhood of that resonance is a sufficient condition for chaos to be intensified.</p>
<p>The network response to the weakest amplitude of the modulation (peach line, <italic>I</italic><sub>1</sub> = 0.1), is the scenario that better captures the activity of the network at the onset of the oscillatory instability in the non-driven network. In that case, the maximum Lyapunov exponent (<xref rid="fig6" ref-type="fig">Fig. 6<bold>b</bold></xref>), the metric entropy (<xref rid="fig6" ref-type="fig">Fig. 6<bold>c</bold></xref>) and the attractor dimension (<xref rid="fig6" ref-type="fig">Fig. 6<bold>d</bold></xref>) all peak at a drive frequency of 21 Hz (see black guiding line).</p>
<p>The values of the frequency of the external input at which the maximum Lyapunov exponent, the metric entropy and the attractor dimension change from being suppressed by the external drive to being boosted by the drive are different for each measure. This can be better understood by looking at how the entire Lyapunov spectrum depends on the driver frequency (<xref rid="figS12" ref-type="fig">Supplementary Fig. S12<bold>c</bold></xref>). With increasing frequency of the external drive, the positive Lyapunov exponents become more positive, but the negative ones become more negative at earlier frequencies and grow more strongly than the positive ones, resulting in a sharp decrease of the attractor dimension (fewer negative exponents are needed to cancel the sum that defines the dimension, see <xref ref-type="fig" rid="fig2">Fig.2<bold>f</bold></xref>).</p>
<p>Oscillatory activity in the cortex is not perfectly periodic, it is usually confined to short episodes with frequencies that fluctuate over time (<xref ref-type="bibr" rid="c53">53</xref>). This type of transient oscillatory activity can be leveraged to flexibly gate information between areas (<xref ref-type="bibr" rid="c10">10</xref>), but whether a network driven by such transient oscillatory activity can enhance or reduce network chaos remains unknown. Here we asked whether driving a network with transient oscillatory bursts would lead to an increase in noise entropy, or if the broad spectrum of these signals would allow recruitment of suppressive responses, resulting either in weak modulation or overall suppression. We generated an inhomogeneous Poisson process with a rate modulated by transient oscillatory bursts, which were simulated as described in (<xref ref-type="bibr" rid="c10">10</xref>), and drove a non-delayed network with this drive (<xref rid="fig6" ref-type="fig">Fig. 6<bold>d</bold></xref>, left). We changed the peak frequency of the signals <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline3.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> by re-scaling the temporal axis (see <italic>Methods</italic>). We found that this type of drive largely cancels the intensification of chaos at the resonant frequency. The dependence of the maximum Lyapunov exponent on the peak frequency of the oscillatory burst of the input drive <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline4.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is shown in <xref rid="fig6" ref-type="fig">Figure 6<bold>fii</bold></xref>. The maximum Lyapunov exponent is only significantly modulated in the neighborhood of the resonant frequency, having only a 5% increase rather than the almost 40% increase in the harmonic drive case. The metric entropy is also only weakly modulated by this type of drive. Because the bursts are transient and the frequency of the burst is not fixed, this input drive recruits suppressive responses from low frequencies, leading to an overall weak modulation of the metric entropy (<xref rid="fig6" ref-type="fig">Fig. 6<bold>g</bold></xref>) and a reduction of the attractor dimension (<xref rid="fig6" ref-type="fig">Fig. 6<bold>h</bold></xref>).</p>
</sec>
</sec>
<sec id="s2">
<title>Discussion</title>
<sec id="s2a">
<title>Delayed interactions in dynamical systems</title>
<p>We developed a framework that allowed us to compute, for the first time, the entire spectrum of Lyapunov exponents in an effectively delayed high-dimensional system and therefore exactly characterize the dynamics on the attractor. We note that delayed systems can have an infinite-dimensional phase-space exploration, and therefore a <italic>discrete</italic> spectrum of Lyapunov exponents is not guaranteed to exist. We mapped the pulsecoupled spiking network with delayed interactions to an equivalent system with fixed and finite degrees of freedom. We found that the Lyapunov spectrum is invariant to network size, giving rise to extensive chaos, with a finite dimension of the attractor for finite network size (<xref rid="fig2" ref-type="fig">Fig. 2</xref>), as also found in other systems with infinite-dimensional phase space (<xref ref-type="bibr" rid="c54">54</xref>). We showed that, prior to the oscillatory instability, the attractor dimension increases linearly with the interaction delay, while the top 5% of the LEs (including the maximum LE) decrease in magnitude, while the number of positive LEs increases. This tension between a reduction in magnitude and an increase in the number of LEs leads to the remarkable fact that the metric entropy of the network is independent of the delay up to the oscillatory instability (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). These same exact dependencies were found in classic studies of delayed dynamical systems, for either low-dimensional maps or scalar differential equations (<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c36">36</xref>). Remarkably, we find that these dependencies are also present in weakly delayed rate networks (<xref rid="figS12" ref-type="fig">Figs. S12</xref>-<xref ref-type="fig" rid="figS13">S13</xref>), where the transition to oscillations is accompanied by a complete collapse of chaos. These findings suggest that, in the absence of a mean-field bifurcation, the reshaping of the attractor dynamics with increasing delay is of a universal nature.</p>
</sec>
<sec id="s2b">
<title>Chaos in spiking networks</title>
<p>Balanced networks, as originally described for binary neurons, are chaotic with an infinitely large maximum Lyapunov exponent (<xref ref-type="bibr" rid="c39">39</xref>). Although the statistics of the balanced-state networks appear to be largely independent of the single neuron model (<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c55">55</xref>), its dynamical stability is not common or intrinsic to the balanced state. Random networks of pulse-coupled, inhibitory leaky integrateand-fire (LIF) neurons have stable dynamics (<xref ref-type="bibr" rid="c55">55</xref>, <xref ref-type="bibr" rid="c56">56</xref>): After a transient of irregular activity whose duration scales exponentially with network size, the dynamics settle in a periodic orbit (<xref ref-type="bibr" rid="c56">56</xref>, <xref ref-type="bibr" rid="c57">57</xref>). However, finite size perturbations reveal a phase space of simultaneously diverging and contracting trajectories (<xref ref-type="bibr" rid="c58">58</xref>, <xref ref-type="bibr" rid="c59">59</xref>). Trajectories initially separated on average by less than a critical value, converge to each other exponentially (local stability), while those separated further diverge exponentially. The inclusion of longer time scales, via synaptic dynamics, can lead to a loss of dynamical stability, resulting in extensive chaotic dynamics after a critical value of the synaptic time constant (<xref ref-type="bibr" rid="c56">56</xref>, <xref ref-type="bibr" rid="c60">60</xref>, <xref ref-type="bibr" rid="c61">61</xref>).</p>
<p>Incorporating a dynamical mechanism for the generation of the action potential, as studied here for the quadratic integrate-and-fire neuron, leads to extensive chaos to sufficiently large external input (<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c6">6</xref>) even in purely inhibitory pulse-coupled networks. Here, we showed that the inclusion of delays does not affect the extensive nature of chaos, as including delays does not lead to the development of deterministic chaos in LIF neurons (<xref ref-type="bibr" rid="c56">56</xref>). We have not observed any transition to dynamical stability (i.e. to a negative maximum lyapunov exponent) that coexists with spiking variability, and chaos only collapses when the dynamics converge to clock-like synchrony. Future work will need to determine whether oscillatory activity can actively modulate the number of neurons contributing to the most stable directions at any given point in time, given that those neurons can respond reliably to the same presentation of the stimulus despite the chaotic dynamics (<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c6">6</xref>)</p>
</sec>
<sec id="s2c">
<title>Collective oscillations and chaos</title>
<p>Our results demonstrate that the emergence of sparse, collective oscillations induces a complete dynamical reconfiguration in models of the cortical circuitry. We find that increasing the recurrent delayed inhibition, either by increasing the synaptic delay <italic>δ</italic> (<xref rid="fig3" ref-type="fig">Figs. 3</xref>-<xref ref-type="fig" rid="fig5">5</xref>), or the number of presynaptic inhibitory inputs (<xref rid="figS6" ref-type="fig">Supplementary Fig. S6</xref>), leads to a transition from an asynchronous irregular state characteristic of balanced-state networks, to a state with irregular activity at the single cell level but with collective rhythms at the populations level. This transition occurs via a Hopf bifurcation, analogous to that described previously in models lacking a dynamical mechanism for the generation of the action potential (<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c31">31</xref>). Recent theoretical work has described the transition to collective oscillations of the QIF with increasing in-degree (<xref ref-type="bibr" rid="c62">62</xref>, <xref ref-type="bibr" rid="c63">63</xref>), and work has also described the mean field dynamics of similar networks to us, reporting rate chaos (chaos in the mean field equations) with increasing input current to the cells (<xref ref-type="bibr" rid="c64">64</xref>). This is very different from the chaos reported here, in which the population rates are not chaotic. Previous work has found a coexistence of chaotic dynamics with sparse oscillations in networks of LIF neurons with slow synapses (<xref ref-type="bibr" rid="c4">4</xref>), but how oscillatory activity impacts the chaotic dynamics of the network remained elusive.</p>
<p>We used a powerful approach (<xref ref-type="bibr" rid="c38">38</xref>) to compare the equivalent system studied here and the true, infinite-dimensional delayed system and found that at the onset of the theoretically predicted value of the synaptic delay for the onset of collective oscillations (<xref rid="fig4" ref-type="fig">Fig. 4</xref>) chaos is greatly intensified, by simultaneously increasing the maximum Lyapunov exponent, the dimension of the chaotic attractor and the metric entropy. This finding is insensitive to the presence of single-cell (<xref rid="fig5" ref-type="fig">Figs. 5</xref>, <xref ref-type="fig" rid="figS8">S8</xref>, <xref ref-type="fig" rid="figS9">S9</xref>) and cell-type (5, S11) heterogeneity, and holds true in the entire region of the parameter space analyzed (S10). We found that this reconfiguration is only possible in models that explicitly incorporate a dynamical mechanism for the generation of the action potential and is not present in rate models (<xref rid="figS12" ref-type="fig">Figs. S12</xref> and <xref ref-type="fig" rid="figS13">S13</xref>), in which we have not found chaotic, asynchronous activity to coexist with collective oscillations (but see (<xref ref-type="bibr" rid="c65">65</xref>) for oscillatory rate networks in the presence of structured connectivity).</p>
<p>To investigate the mechanisms underlying such intensification of chaos, we studied a non-delayed network driven by a harmonically-modulated Poisson process. We found that chaos is reduced for low frequencies of the drive, analogously to what is found in noise-driven rate networks (<xref ref-type="bibr" rid="c66">66</xref>). Nevertheless, for frequencies in the neighborhood of the resonant frequency, the chaotic measures are intensified. This boosting of chaos with an external asynchronous oscillatory drive is to our knowledge one of a kind and has strong resemblance to what is found in classic maps driven by a harmonic input. This finding is in stark contrast to what is found in rate network models (<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c50">50</xref>), for which a sufficiently strong harmonic drive is able to completely abolish chaos at all frequencies of the harmonic drive.</p>
</sec>
<sec id="s2d">
<title>Transient oscillations lead to moderate boost of network chaos</title>
<p>Oscillatory activity in cortex is far from perfectly periodic, cortical rhythms are in fact characterized by transient oscillatory bursts of drifting frequency (<xref ref-type="bibr" rid="c53">53</xref>, <xref ref-type="bibr" rid="c67">67</xref>) that occur in a seemingly stochastic fashion. Theoretical work has shown how to flexibly route information, leveraging those noise-induced transients of rhythmic activity (<xref ref-type="bibr" rid="c10">10</xref>). Our results here show that when input spike trains of the network are modulated by a transient oscillatory amplitude, the response of the network to such input, recruits suppressive responses from lower frequencies, leading to a very modest change in the Lyapunov exponent and in the metric entropy, and a mild decrease in the attractor dimension. These effects are in strong contrast with the periodic drive case studied, in which the maximum Lyapunov exponent and the metric entropy undergo a 40 % increase compared to the non-driven case. Future work will need to elucidate whether input patterns generated by a chaotic network with transient oscillatory burst would lead to chaos suppression as seen here with Poisson spiking patterns, or would lead to an increase of chaos in the target network.</p>
<p>We emphasize that generally, in dissipative systems (including our networks and possibly in contrast with Hamiltonian models (<xref ref-type="bibr" rid="c68">68</xref>–<xref ref-type="bibr" rid="c70">70</xref>)) a positive entropy rate is possible despite the existence of an invariant measure (<xref ref-type="bibr" rid="c71">71</xref>) (i.e. despite the fact that probability density over the attractor is not changing over time), because the measure is singular and therefore is a “bottomless source of entropy” (<xref ref-type="bibr" rid="c72">72</xref>, <xref ref-type="bibr" rid="c73">73</xref>).</p>
</sec>
</sec>
<sec id="s3">
<title>Materials and Methods</title>
<sec id="s3a">
<title>Network Model</title>
<p>We considered a network of N quadratic integrate- and-fire (QIF) neurons
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="505598v2_eqn1.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="505598v2_eqn2.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>τ</italic><sub><italic>i</italic></sub> is the membrane time and <italic>v</italic><sub><italic>i</italic></sub> is the voltage of the neuron <italic>i</italic>. The term <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline5.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> describes the synaptic current that a neuron <italic>i</italic> receives at time <italic>t</italic> given that its pre-synaptic neighbors <italic>j</italic> emitted spikes in the times <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline6.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> after a delay <italic>δ</italic>. The connectivity <italic>J</italic><sub><italic>ij</italic></sub> has a binomial distribution with probability <italic>K/N</italic> where 1 ≪ <italic>K</italic> ≪ <italic>N</italic>. When a connection exists, its strength takes a constant value depending on the type of the pre and post cell: <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline7.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, for <italic>x, y</italic> = <italic>E, I</italic>. The second term is an constant external input <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline8.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, <italic>X</italic><sup>0</sup> = <italic>E</italic><sub>0</sub>, <italic>I</italic><sub>0</sub> for excitatory and inhibitory neurons respectively (see Section A in <italic>Supplementary Information</italic>)</p>
<sec id="s3a1">
<label>A.</label>
<title>Phase reduction for integrate-and-fire models</title>
<p>In networks of pulse-coupled units receiving constant external inputs, and whose voltage dynamics have a defined threshold <italic>x</italic><sub><italic>t</italic></sub> and a reset values <italic>x</italic><sub><italic>r</italic></sub>, the evolution between network spikes <italic>t</italic><sub><italic>s</italic></sub> is defined by a propagator function or map (<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c74">74</xref>–<xref ref-type="bibr" rid="c76">76</xref>)
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="505598v2_eqn3.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The function <italic>f</italic> evolves the state of the neuron <italic>i</italic> after the last spike in the network, <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline9.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, to the state just before the next spike at <italic>t</italic><sub><italic>s</italic>+1</sub>. If the neuron <italic>i</italic> is not postsynaptic to the neuron <italic>j</italic><sup><italic>∗</italic></sup> that produces an spike at <italic>t</italic><sub><italic>s</italic>+1</sub>, the state variable will be left unaffected, and then <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline10.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. On the contrary, if the neuron <italic>i</italic><sup><italic>∗</italic></sup> is postsynaptic to <italic>j</italic><sup><italic>∗</italic></sup>, then its state variable has to be further updated:
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="505598v2_eqn4.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<xref ref-type="disp-formula" rid="eqn3">Eqs 3</xref> and <xref ref-type="disp-formula" rid="eqn4">4</xref>, form the basis of the iterative evolution of the network from spike to spike. The Eqs for the QIF neuron used in this paper were derived in Ref. (<xref ref-type="bibr" rid="c3">3</xref>), a detailed derivation can also be found in Section B of the <italic>Supplementary Information</italic>)</p>
</sec>
<sec id="s3a2">
<label>B.</label>
<title>Delayer single-compartment-axon</title>
<p>Synaptic and axonic delays consistent with the framework above specified were incorporated into the network by means of the introduction of <italic>delayer</italic> variables, chosen to be QIF neurons in the excitable (non-periodically spiking) regime.
<disp-formula id="eqn5">
<alternatives><graphic xlink:href="505598v2_eqn5.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The solution of <xref ref-type="disp-formula" rid="eqn5">Eq. 5</xref> for all three cases (i) <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline11.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, (ii)<inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline12.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, (iii) <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline13.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> ln <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline14.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The transition between the regimes (i),(ii),(iii) is dynamically forbidden.</p>
<p>We can define a change of variables
<disp-formula id="eqn6">
<alternatives><graphic xlink:href="505598v2_eqn6.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
that simplifies the solution of <xref ref-type="disp-formula" rid="eqn5">Eq. 5</xref> reads:
<disp-formula id="eqn7">
<alternatives><graphic xlink:href="505598v2_eqn7.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
In this representation a propagator function <italic>η</italic> and an update function <italic>γ</italic>, analogous to <italic>f</italic> and <italic>g</italic> of Eqs. (<xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c4">4</xref>) can as well be defined:
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="505598v2_eqn8.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn9">
<alternatives><graphic xlink:href="505598v2_eqn9.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The <italic>ξ</italic> variable, when initially below the threshold value <italic>ξ</italic><sub><italic>t</italic></sub> = −1 <italic>ξ</italic><sub>0</sub> =<italic>&lt;</italic> −1 takes a finite time to spike given by <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline15.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (see <xref rid="figS1" ref-type="fig">Figure S1</xref>). If the reset value is set to <italic>ξ</italic><sub><italic>r</italic></sub> = 0, there will be no dynamics after the spiking event. If while in its resting state, an incoming spike from neuron <italic>j</italic><sup><italic>∗</italic></sup> is received, then its state will be modified according to <italic>γ</italic>(0) and will depend only on the parameters of the system <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline16.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. If the incoming spike has a sufficiently large weight then the <italic>ξ</italic> variable will be pushed beyond its unstable fixed point and emit a spike after a fraction of time given by:
<disp-formula id="eqn10">
<alternatives><graphic xlink:href="505598v2_eqn10.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
before relaying it to the next neuron. More concretely, the value of <italic>J</italic><sub><italic>l∗j∗</italic></sub> has to be bigger than <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline17.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the distance between the fixed points in <xref ref-type="disp-formula" rid="eqn5">equation 5</xref>. For the case in which <italic>J</italic><sub><italic>l∗j∗</italic></sub> is independent of the neuronal pair, the delay introduced by the SCA will only depend on its own parameters, and then <italic>δ</italic><sub><italic>l∗j∗</italic></sub> = <italic>δ</italic><sub><italic>l∗</italic></sub>. The singlecompartment-axon (SCA), is then defined by <xref ref-type="disp-formula" rid="eqn9">Eqs. 9</xref>, with a reset value <italic>ξ</italic><sub><italic>r</italic></sub> = 0 for the “exact delay” framework. If the SCA has a reset value that is equal to any value between the threshold <italic>ξ</italic><sub><italic>T</italic></sub> = −1 and zero, the delay will depend on the dynamics of the network and change from spike to spike. The delay, in that case, will be :
<disp-formula id="eqn11">
<alternatives><graphic xlink:href="505598v2_eqn11.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
For <italic>J &gt;</italic> 0, this equation has solutions for <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline18.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The smaller the value of <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline19.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the larger the range of <italic>ξ</italic> to which it can be reset, and the faster the function in the logarithm reaches a value that is only weakly dependent on <italic>ξ</italic>. This configuration is what we will can call “dynamic delay” framework.</p>
<p>In both cases, when the SCA is arranged post-synaptically to each neuron in a network, it will delay the transmission of the spike by an amount given by <xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref> or <xref ref-type="disp-formula" rid="eqn11">11</xref>, while preserving the desirable features of iterable maps.</p>
</sec>
<sec id="s3a3">
<label>C.</label>
<title>Equivalent delayed network</title>
<p>A diagram of the network architecture of a delayed network with balanced state properties is shown in <xref rid="fig2" ref-type="fig">Fig. 2<bold>a</bold></xref>. A spiking neuron <italic>j</italic><sup><italic>∗</italic></sup> (in orange, left), described by its phase variable <italic>ϕ</italic><sub><italic>j</italic></sub> receives independent and large inputs proportional to the square root of the average amount of connections per neuron <italic>K</italic>. Its spike is instantaneously transmitted to the SCA, of variable <italic>ξ</italic><sub><italic>l</italic></sub> in green. The parameters of the SCA define the time it will take for the spike to be transmitted, from the SCA to the postsynaptic neurons <italic>i</italic>, whose phase variable is <italic>ϕ</italic><sub><italic>i</italic></sub>.</p>
<p>The connectivity matrix, 𝕁 has then the form in <xref ref-type="disp-formula" rid="eqn12">Eq. 12</xref>. For ease of notation, indexes <italic>i</italic> and <italic>j</italic> (from 1 to N) will be reserved for phase neurons and <italic>l</italic> and <italic>m</italic> (from N+1 to M = 2N) for the SCA. The connectivity matrix is a block matrix describing an ErdősRényi random graph on the upper right block, with weights proportional to <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline20.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and a diagonal matrix in the bottom left:
<disp-formula id="eqn12">
<alternatives><graphic xlink:href="505598v2_eqn12.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The event-based simulation of the system defined in Eqs (<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>) is done by, after each spike, evolving the network dynamics by propagating both the somas by <xref ref-type="disp-formula" rid="eqnS10">Eqs. S10</xref> and the SCA 9 up to the next spike time. If the next spike time is from a soma, then the postsynaptic SCAs have to be updated according to <xref ref-type="disp-formula" rid="eqn9">Eq. 9</xref>, and if it was from an SCA, postsynaptic somas should be updated as in <xref ref-type="disp-formula" rid="eqnS10">Eq. S10</xref>.</p>
</sec>
<sec id="s3a4">
<label>D.</label>
<title>Characterization of network activity</title>
<p>The firing rates of each neuron <italic>ν</italic><sub><italic>i</italic></sub> were calculated by summing all the spikes and dividing by the time spanned between the first and the last spike. The mean population rate is then <italic>ν</italic>. The coefficient of variation is the square root of the squared inter-spike-interval times the squared rate, i.e. <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline21.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The level of synchrony of the network was assessed by the synchronization index <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline22.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> Where, <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline23.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the mean activity in the phase representation. The variables <italic>σ</italic><sub><italic>ϕ</italic>(<italic>t</italic>)</sub> and <italic>σ</italic><sub><italic>ϕ</italic> (<italic>t</italic>)</sub> are the standard deviation of the mean phase over time or, respectively, of the phase trace <italic>V</italic><sub><italic>i</italic></sub>(<italic>t</italic>) of each individual neuron <italic>i</italic>. The <italic>χ</italic> coefficient is bounded to the unit interval 0 <italic>&lt; χ &lt;</italic> 1, with vanishing values indicating asynchronous dynamics. The amplitude <italic>A</italic> (Hz) and the frequency <italic>f</italic> (Hz) of the population activity were calculated by making a binning histogram of a 1 ms bin of the spikes of the entire network. This quantity normalized by the bin size and the number of neurons defines a multi-unit like signal (MUA). The mean peak high defines the amplitude <italic>A</italic> (Hz). The frequency of the MUA was obtained as the inverse of the first autocorrelation peak.</p>
</sec>
<sec id="s3a5">
<label>E.</label>
<title>Simulation specifics and parameters</title>
<p>The simulations took default parameters given by <xref rid="tbl1" ref-type="table">table 1</xref>. Deviations from these default parameters are indicated below. The simulations are event-based using the iterative map described in the previous sections and following Ref.(<xref ref-type="bibr" rid="c3">3</xref>). In short, one needs to calculate the next spike time in the network, coming either from a soma or from a SCA. For that, it is sufficient to find the soma with the largest phase and the SCA with value closer to 1. For each simulation, random topologies (ErdsRényi) for the neuronal block (<xref ref-type="disp-formula" rid="eqn12">Eq. 12</xref>), random initial conditions, and an initial guess on the input currents to the neurons are generated. By means of root finding algorithms (Regula Falsi and Ridders’ method) (<xref ref-type="bibr" rid="c77">77</xref>), a guess on the input current <italic>I</italic> needed to have a target firing rate was made from Eq. S3. After a simulation lasting S<sub>R</sub> spikes per neuron, the rate was calculated and a new guess on the currents is made. This procedure is then iterated until the target mean firing rate is found with 1% precision. The multiplying factor that measures the distance to the balance condition is defined by <italic>Is</italic> = <italic>I/I</italic><sub>0</sub>. Once the appropriate value of the current <italic>I</italic> is found, the network can be warmed up to disregard transients, which can be large in delayed systems by a time equivalent to S<sub>W</sub> spikes per neuron. Finally, a random orthonormal matrix Q is chosen and the QR algorithm described in the theory section below was left to warm up for a time duration equivalent to S<sub>WONS</sub> spikes. This guarantees some degree of alignment of the first Lyapunov vector to the first vector of the orthonormal system. The simulation runs for a time equivalent to S<sub>C</sub> spikes per neuron.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>List of default network parameters.</title></caption>
<graphic xlink:href="505598v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>Deviations from the default which are not indicated explicitly are as follows: In <xref rid="fig5" ref-type="fig">Figure 5</xref> N=8000 for panels <bold>f-j</bold>. <xref rid="figS2" ref-type="fig">Figure S2</xref> had N=400, while in S10(g-h),(p-q), (y-z) N was 2000, and in those cases S<sub>R</sub>=S<sub>Q</sub>=S<sub>WONS</sub>=600. In <xref rid="figS10" ref-type="fig">Figure S10</xref> N=5000.</p>
<p>In <xref rid="fig3" ref-type="fig">Figures 3</xref>, <xref rid="fig4" ref-type="fig">4</xref>, <xref ref-type="fig" rid="figS3">S3</xref>, <xref ref-type="fig" rid="figS4">S4</xref>, <xref ref-type="fig" rid="figS6">S6</xref>, <xref ref-type="fig" rid="figS7">S7</xref> no heterogeneity was present (i.e.<inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline24.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>).</p>
<p>The driven network simulations of <xref rid="fig6" ref-type="fig">Fig. 6</xref> were performed as follows: Each neuron in the recurrent network receives an independent stream of inhibitory external input spikes of strength <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline25.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> that are generated from an inhomogeneous Poisson process with a rate that is modulated either I) sinusoidally with frequency <italic>f</italic> and amplitude <italic>I</italic><sub>1</sub> around a constant rate <italic>ν</italic><sub>in</sub> = <italic>Kν</italic> or II) by the outcome of the LFP obtained by simulations of networks with transient synchrony (identical to that in <xref rid="fig1" ref-type="fig">Fig 1</xref> of (<xref ref-type="bibr" rid="c10">10</xref>)). We implemented the sinusoidally modulated input drive for each neuron by drawing the inter-spike intervals of the inhomogeneous Poisson input from the distribution <italic>p</italic>(Δ<italic>t</italic><sub>ISI</sub>) = − log(<italic>x</italic>)<italic>/</italic>(<italic>ν</italic><sub>in</sub><italic>τ</italic> <sup>m</sup>(1 + <italic>I</italic><sub>1</sub> sin(2<italic>πf t</italic>)), where <italic>x</italic> is uniformly distributed between 0 and 1, <italic>I</italic><sub>1</sub> is the relative amplitude of the input modulation, <italic>f</italic> is the frequency of the input modulation and <italic>ν</italic><sub>in</sub> is the external input rate that we chose to be 5 times the mean firing rate <italic>ν</italic>. We set <italic>J</italic><sub>0</sub> = 1. This implementation results precisely in the desired sinusoidally modulated Poisson rate for a relative input modulation strength <italic>I</italic><sub>1</sub> ≪ 1 and <italic>f</italic> ≪ <italic>ν</italic><sub>in</sub>. By adapting the constant external input current iteratively using the bisection scheme described above, we obtained a target firing rate of 5 Hz. We chose <italic>ν</italic><sub>in</sub> = <italic>Kν</italic> = 500Hz, thus that on average neurons in the recurrent network receive the same number of recurrent spikes as external input spikes. After a simulation lasting S<sub>R</sub> = 10<sup>3</sup> spikes per recurrent neuron, the rate was calculated and a new guess of the currents was made. This procedure is iterated until the target mean firing rate was found with 1% precision. Once the appropriate value of the current <italic>I</italic> is found, the orthonormalization procedure described below was done iteratively where the interval size was adapted to achieve a condition number of the <italic>Q</italic> matrix between 1e2 and 1e6 with an initial guess of 5000 spikes. We performed a warmup of both the recurrent network state and the state of the orthonormal system. The simulation ran for a 60s. Results in <xref rid="fig6" ref-type="fig">Fig. 6</xref> show averages over 10 ErdsRényi network realizations.</p>
</sec>
<sec id="s3a6">
<label>F.</label>
<title>Calculation of the critical delay</title>
<p>In the regime in which each neuron receives a large number of inputs per integration time, with each input making only a small contribution to the net input, and under the assumption that all neurons are the same, the dynamics of the network can be analyzed under the so-called <italic>diffusion approximation</italic> (<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c78">78</xref>). We used the method described in (<xref ref-type="bibr" rid="c38">38</xref>) to do this calculation, and is detailed in the Section C in the <italic>Supplementary Information</italic></p>
</sec>
<sec id="s3a7">
<label>G.</label>
<title>Ergodic theory measures characterizing the attractor</title>
<p>The dimensionality <italic>D</italic> of the attractor of the delayed circuit can be then computed by means of a classic conjecture by Kaplan and York (<xref ref-type="bibr" rid="c79">79</xref>). The conjecture states that the attractor dimension is given by the linear interpolation of the number of Lyapunov exponents that are needed for its cumulative sum to vanish (see <xref rid="fig2" ref-type="fig">Fig. 2<bold>f</bold></xref>, top, see also Sect. A in <italic>Supplementary Information</italic>):
<disp-formula id="eqn13">
<alternatives><graphic xlink:href="505598v2_eqn13.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Where <italic>l</italic> is such that <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline26.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline27.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>The metric entropy <italic>H</italic>, is an upper bound on the rate at which the chaotic dynamics contributes to the KolmogorovSinai entropy, in other words, an upper bound on the average information gained by making a new measurement in a sequence of measurements of the state of the dynamical system (<xref ref-type="bibr" rid="c32">32</xref>) (see also Sect. A of <italic>Supplementary Information</italic>). We compute this metric entropy via the Pesin identity (<xref ref-type="bibr" rid="c80">80</xref>), which equates the metric entropy with the sum of the positive Lyapunov exponents (see also <xref rid="fig2" ref-type="fig">Fig. 2<bold>f</bold></xref>):
<disp-formula id="eqn14">
<alternatives><graphic xlink:href="505598v2_eqn14.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
In <xref rid="fig3" ref-type="fig">Figs. 3</xref>,<xref rid="fig5" ref-type="fig">5</xref> and <xref rid="fig6" ref-type="fig">6</xref>, we show the intrinsic metric entropy in bits per second, normalized by the mean firing rate of the network. In other words
<disp-formula id="eqn15">
<alternatives><graphic xlink:href="505598v2_eqn15.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Keeping in mind that the measures derived from the Lyapunov spectrum are upper bounds and that no rigorous statement can be made in favor of the equalities, we nevertheless refer to them as the metric entropy and the attractor dimension.</p>
</sec>
<sec id="s3a8">
<label>H.</label>
<title>Lyapunov spectrum of delayed spiking neuronal networks</title>
<p>Given a neuronal model <italic>F</italic> (<italic>V</italic><sub><italic>i</italic></sub>) and a model for the SCA that are exactly solvable, the network can be propagated between spikes and updated by <xref ref-type="disp-formula" rid="eqnS10">Eqs. S10</xref> and <xref ref-type="disp-formula" rid="eqnS7">S7</xref>. From these equations, a Jacobian 𝕃 (<bold><italic>x</italic></bold>(<italic>n</italic>)) at each spike time can be obtained, from which the estimation of the Lyapunov dimension <italic>D</italic> (<xref ref-type="disp-formula" rid="eqn13">Eq.13</xref>) and the metric entropy <italic>H</italic> (Eq.15) are possible via QR decomposition. The terms of the Jacobian and an explanation of the terms can be found in Section B of the <italic>Supplementary Information</italic>.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We thank M. Monteforte and M. Puelma Touzel for discussions, and L. Abbott and T. Nguyen for feedback on this manuscript. This work was supported by the German Research Foundation (Deutsche Forschungsgemeinschaft, DFG) through SFB 1528, SFB 889, SFB 1286, SPP 2205, DFG 436260547 in relation to NeuroNex (National Science Foundation 2015276) &amp; under Germanys Excellence Strategy - EXC 2067/1-390729940; by the Leibniz Association (project K265/2019); and the Niedersächsisches Vorab of the VolkswagenStiftung through the Göttingen Campus Institute for Dynamics of Biological Networks.This work was partially supported by the Federal Ministry for Education and Research (BMBF) under grant no. 01GQ1005B (to A.P., R.E. F.W.), by a GGNB Excellence Stipend of the University of Göttingen (to A.P.), by a Swartz Fellowship for Theory in Neuroscience (A.P.) through CRC 889 by the Deutsche Forschungsgemeinschaft and by the VolkswagenStiftung under grant no. ZN2632 (to F.W.)</p>
</ack>
<ref-list>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><given-names>MN</given-names> <surname>Shadlen</surname></string-name>, <string-name><given-names>WT</given-names> <surname>Newsome</surname></string-name>, <article-title>The variable discharge of cortical neurons: implications for connectivity, computation, and information coding</article-title>. <source>The J. neuroscience : official journal Soc. for Neurosci</source>. <volume>18</volume>, <fpage>3870</fpage>–<lpage>96</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><given-names>WR</given-names> <surname>Softky</surname></string-name>, <string-name><given-names>C</given-names> <surname>Koch</surname></string-name>, <article-title>The highly irregular firing of cortical cells is inconsistent with temporal integration of random EPSPs</article-title>. <source>The J. neuroscience : official journal Soc. for Neurosci</source>. <volume>13</volume>, <fpage>334</fpage>–<lpage>50</lpage> (<year>1993</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Monteforte</surname></string-name>, <string-name><given-names>F</given-names> <surname>Wolf</surname></string-name>, <article-title>Dynamical entropy production in spiking neuron networks in the balanced state</article-title>. <source>Phys. Rev. Lett</source>. <volume>105</volume>, <fpage>1</fpage>–<lpage>4</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><given-names>S</given-names> <surname>Luccioli</surname></string-name>, <string-name><given-names>S</given-names> <surname>Olmi</surname></string-name>, <string-name><given-names>A</given-names> <surname>Politi</surname></string-name>, <string-name><given-names>A</given-names> <surname>Torcini</surname></string-name>, <article-title>Collective dynamics in sparse networks</article-title>. <source>Phys. Rev. Lett</source>. <volume>109</volume>, <fpage>1</fpage>–<lpage>5</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><given-names>G</given-names> <surname>Lajoie</surname></string-name>, <string-name><given-names>KK</given-names> <surname>Lin</surname></string-name>, <string-name><given-names>E</given-names> <surname>Shea-Brown</surname></string-name>, <article-title>Chaos and reliability in balanced spiking networks with temporal drive</article-title>. <source>Phys. Rev. E - Stat. Nonlinear, Soft Matter Phys</source>. <volume>87</volume>, <fpage>1</fpage>–<lpage>15</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><given-names>G</given-names> <surname>Lajoie</surname></string-name>, <string-name><given-names>JP</given-names> <surname>Thivierge</surname></string-name>, <string-name><given-names>E</given-names> <surname>Shea-Brown</surname></string-name>, <article-title>Structured chaos shapes spike-response noise entropy in balanced neural networks</article-title>. <source>Front. computational neuroscience</source> <volume>8</volume>, <fpage>1</fpage>–<lpage>10</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><given-names>ZF</given-names> <surname>Mainen</surname></string-name>, <string-name><given-names>TJ</given-names> <surname>Sejnowski</surname></string-name>, <article-title>Reliability of spike timing in neocortical neurons</article-title>. <source>Science</source> <volume>268</volume>, <fpage>1503</fpage>–<lpage>6</lpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><given-names>C</given-names> <surname>Ribrault</surname></string-name>, <string-name><given-names>K</given-names> <surname>Sekimoto</surname></string-name>, <string-name><given-names>A</given-names> <surname>Triller</surname></string-name>, <article-title>From the stochasticity of molecular processes to the variability of synaptic transmission</article-title>. <source>Nat. Rev. Neurosci</source>. <volume>12</volume>, <fpage>375</fpage>–<lpage>387</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><given-names>P</given-names> <surname>Fries</surname></string-name>, <article-title>Neuronal Gamma-Band Synchronization as a Fundamental Process in Cortical Computation</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>32</volume>, <fpage>209</fpage>–<lpage>224</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><given-names>A</given-names> <surname>Palmigiano</surname></string-name>, <string-name><given-names>T</given-names> <surname>Geisel</surname></string-name>, <string-name><given-names>F</given-names> <surname>Wolf</surname></string-name>, <string-name><given-names>D</given-names> <surname>Battaglia</surname></string-name>, <article-title>Flexible information routing by transient synchrony</article-title>. <source>Nat. Neurosci</source>. <volume>20</volume>, <fpage>1014</fpage>–<lpage>1022</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><given-names>U</given-names> <surname>Parlitz</surname></string-name>, <string-name><given-names>W</given-names> <surname>Lauterborn</surname></string-name>, <article-title>Period-doubling cascades and devils staircases of the driven van der Pol oscillator</article-title>. <source>Phys. Rev. A</source> <volume>36</volume>, <fpage>1428</fpage>–<lpage>1434</lpage> (<year>1987</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><given-names>T</given-names> <surname>Shinbrot</surname></string-name>, <string-name><given-names>C</given-names> <surname>Grebogi</surname></string-name>, <string-name><given-names>JA</given-names> <surname>Yorke</surname></string-name>, <string-name><given-names>E</given-names> <surname>Ott</surname></string-name>, <article-title>Using small perturbations to control chaos</article-title>. <source>Nature</source> <volume>363</volume>, <fpage>411</fpage>–<lpage>417</lpage> (<year>1993</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><given-names>Y</given-names> <surname>Braiman</surname></string-name>, <string-name><given-names>I</given-names> <surname>Goldhirsch</surname></string-name>, <article-title>Taming chaotic dynamics with weak periodic perturbations</article-title>. <source>Phys. Rev. Lett</source>. <volume>66</volume>, <fpage>2545</fpage>–<lpage>2548</lpage> (<year>1991</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><given-names>K</given-names> <surname>Rajan</surname></string-name>, <string-name><given-names>LF</given-names> <surname>Abbott</surname></string-name>, <string-name><given-names>H</given-names> <surname>Sompolinsky</surname></string-name>, <article-title>Stimulus-dependent suppression of chaos in recurrent neural networks</article-title>. <source>Phys. Rev. E - Stat. Nonlinear, Soft Matter Phys</source>. <volume>82</volume>, <fpage>1</fpage>–<lpage>5</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><given-names>N</given-names> <surname>Brunel</surname></string-name>, <string-name><given-names>V</given-names> <surname>Hakim</surname></string-name>, <article-title>Fast Global Oscillations in Networks of Integrate-and-Fire Neurons with Low Firing Rates</article-title>. <source>Neural Comput</source>. <volume>11</volume>, <fpage>1621</fpage>–<lpage>1671</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><given-names>S</given-names> <surname>Ostojic</surname></string-name>, <string-name><given-names>N</given-names> <surname>Brunel</surname></string-name>, <string-name><given-names>V</given-names> <surname>Hakim</surname></string-name>, <article-title>Synchronization properties of networks of electrically coupled neurons in the presence of noise and heterogeneities</article-title>. <source>J. Comput. Neurosci</source>. <volume>26</volume>, <fpage>369</fpage>–<lpage>392</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><given-names>N</given-names> <surname>Brunel</surname></string-name>, <string-name><given-names>PE</given-names> <surname>Latham</surname></string-name>, <article-title>Firing rate of the noisy quadratic integrate-and-fire neuron</article-title>. <source>Neural computation</source> <volume>15</volume>, <fpage>2281</fpage>–<lpage>2306</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><given-names>C</given-names> <surname>Geisler</surname></string-name>, <etal>et al.</etal>, <article-title>Contributions of Intrinsic Membrane Dynamics to Fast Network Oscillations With Irregular Neuronal Discharges</article-title>. <source>J. Neurophysiol</source>. <volume>94</volume>, <fpage>4344</fpage>–<lpage>4361</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><given-names>N</given-names> <surname>Brunel</surname></string-name>, <article-title>What Determines the Frequency of Fast Network Oscillations With Irregular Neural Discharges? I. Synaptic Dynamics and Excitation-Inhibition Balance</article-title>. <source>J. Neurophysiol</source>. <volume>90</volume>, <fpage>415</fpage>–<lpage>430</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><given-names>P</given-names> <surname>Tiesinga</surname></string-name>, <string-name><given-names>TJ</given-names> <surname>Sejnowski</surname></string-name>, <article-title>Cortical Enlightenment: Are Attentional Gamma Oscillations Driven by ING or PING?</article-title> <source>Neuron</source> <volume>63</volume>, <fpage>727</fpage>–<lpage>732</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><given-names>Ja</given-names> <surname>Cardin</surname></string-name>, <etal>et al.</etal>, <article-title>Driving fast-spiking cells induces gamma rhythm and controls sensory responses</article-title>. <source>Nature</source> <volume>459</volume>, <fpage>663</fpage>–<lpage>7</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><given-names>AB</given-names> <surname>Saleem</surname></string-name>, <etal>et al.</etal>, <article-title>Subcortical Source and Modulation of the Narrowband Gamma Oscillation in Mouse Visual Cortex</article-title>. <source>Neuron</source> <volume>93</volume>, <fpage>315</fpage>–<lpage>322</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><given-names>J</given-names> <surname>Veit</surname></string-name>, <string-name><given-names>R</given-names> <surname>Hakim</surname></string-name>, <string-name><given-names>MP</given-names> <surname>Jadi</surname></string-name>, <string-name><given-names>TJ</given-names> <surname>Sejnowski</surname></string-name>, <string-name><given-names>H</given-names> <surname>Adesnik</surname></string-name>, <article-title>Cortical gamma band synchronization through somatostatin interneurons</article-title>. <source>Nat. Neurosci</source>. <volume>20</volume>, <fpage>951</fpage>–<lpage>959</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><given-names>R</given-names> <surname>Hakim</surname></string-name>, <string-name><given-names>K</given-names> <surname>Shamardani</surname></string-name>, <string-name><given-names>H</given-names> <surname>Adesnik</surname></string-name>, <article-title>A neural circuit for gamma-band coherence across the retinotopic map in mouse visual cortex</article-title>. <source>eLife</source> <volume>7</volume>, <fpage>1</fpage>–<lpage>17</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><given-names>U</given-names> <surname>Ernst</surname></string-name>, <string-name><given-names>K</given-names> <surname>Pawelzik</surname></string-name>, <string-name><given-names>T</given-names> <surname>Geisel</surname></string-name>, <article-title>Synchronization induced by temporal delays in pulse-coupled oscillators</article-title>. <source>Phys. Rev. Lett</source>. <volume>74</volume>, <fpage>1570</fpage>–<lpage>1573</lpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><given-names>W</given-names> <surname>Gerstner</surname></string-name>, <article-title>Rapid phase locking in systems of pulse-coupled oscillators with delays</article-title>. <source>Phys. review letters</source> <volume>76</volume>, <fpage>1755</fpage>–<lpage>1758</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Dhamala</surname></string-name>, <string-name><given-names>VK</given-names> <surname>Jirsa</surname></string-name>, <string-name><given-names>M</given-names> <surname>Ding</surname></string-name>, <article-title>Enhancement of neural synchrony by time delay</article-title>. <source>Phys. review letters</source> <volume>92</volume>, <fpage>074104</fpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><given-names>M Le</given-names> <surname>Berre</surname></string-name>, <etal>et al.</etal>, <article-title>Conjecture on the dimensions of chaotic attractors of delayed-feedback dynamical systems</article-title>. <source>Phys. Rev. A</source> <volume>35</volume>, <fpage>4020</fpage>–<lpage>4022</lpage> (<year>1987</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><given-names>J Doyne</given-names> <surname>Farmer</surname></string-name>, <article-title>Chaotic attractors of an infinite-dimensional dynamical system</article-title>. <source>Phys. D: Nonlinear Phenom</source>. <volume>4</volume>, <fpage>366</fpage>–<lpage>393</lpage> (<year>1982</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><given-names>N</given-names> <surname>Brunel</surname></string-name>, <string-name><given-names>JP</given-names> <surname>Nadal</surname></string-name>, <article-title>Mutual information, Fisher information, and population coding</article-title>. <source>Neural computation</source> <volume>10</volume>, <fpage>1731</fpage>–<lpage>57</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="other"><string-name><given-names>N</given-names> <surname>Brunel</surname></string-name>, <article-title>Phase diagrams of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title>. <source>Neurocomputing</source> 32-33, <fpage>307</fpage>–<lpage>312</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="other"><string-name><given-names>JD</given-names> <surname>Farmer</surname></string-name>, <source>Information Dimension and the Probabilistic Structure of Chaos</source> (<year>1982</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><given-names>P</given-names> <surname>Ashwin</surname></string-name>, <string-name><given-names>M</given-names> <surname>Timme</surname></string-name>, <article-title>Unstable Attractors: Existence and Robustness in Networks of Oscillators With Delayed Pulse Coupling</article-title>. <source>Nonlinearity</source> <volume>18</volume>, <fpage>29</fpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><given-names>N</given-names> <surname>Brunel</surname></string-name>, <string-name><given-names>V</given-names> <surname>Hakim</surname></string-name>, <article-title>Sparsely synchronized neuronal oscillations</article-title>. <source>Chaos</source> <volume>18</volume>, <fpage>015113</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><given-names>S</given-names> <surname>Lepri</surname></string-name>, <string-name><given-names>G</given-names> <surname>Giacomelli</surname></string-name>, <string-name><given-names>A</given-names> <surname>Politi</surname></string-name>, <string-name><given-names>FT</given-names> <surname>Arecchi</surname></string-name>, <article-title>High-dimensional chaos in delayed dynamical systems</article-title>. <source>Phys. D: Nonlinear Phenom</source>. <volume>70</volume>, <fpage>235</fpage>–<lpage>249</lpage> (<year>1994</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><given-names>T</given-names> <surname>Jüngling</surname></string-name>, <string-name><given-names>W</given-names> <surname>Kinzel</surname></string-name>, <article-title>Scaling of Lyapunov exponents in chaotic delay systems</article-title>. <source>arXiv preprint arXiv:1210.3528</source> <volume>0</volume>, <fpage>1</fpage>–<lpage>4</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Timme</surname></string-name>, <string-name><given-names>F</given-names> <surname>Wolf</surname></string-name>, <string-name><given-names>T</given-names> <surname>Geisel</surname></string-name>, <article-title>Coexistence of regular and irregular dynamics in complex networks of pulse-coupled oscillators</article-title>. <source>Phys. review letters</source> <volume>89</volume>, <fpage>258701</fpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><given-names>MJE</given-names> <surname>Richardson</surname></string-name>, <article-title>Spike-train spectra and network response functions for non-linear integrate- and-fire neurons</article-title>. <source>Biol. Cybern</source>. <volume>99</volume>, <fpage>381</fpage>–<lpage>392</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><given-names>C van</given-names> <surname>Vreeswijk</surname></string-name>, <string-name><given-names>H</given-names> <surname>Sompolinsky</surname></string-name>, <article-title>Chaotic balanced state in a model of cortical circuits</article-title>. <source>Neural computation</source> <volume>10</volume>, <fpage>1321</fpage>–<lpage>71</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><given-names>D</given-names> <surname>Golomb</surname></string-name>, <string-name><given-names>J</given-names> <surname>Rinzel</surname></string-name>, <article-title>Dynamics of globally coupled inhibitory neurons with heterogeneity</article-title>. <source>Phys. Rev. E</source> <volume>48</volume>, <fpage>4810</fpage>–<lpage>4814</lpage> (<year>1993</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><given-names>JA</given-names> <surname>White</surname></string-name>, <string-name><given-names>CC</given-names> <surname>Chow</surname></string-name>, <string-name><given-names>J</given-names> <surname>Ritt</surname></string-name>, <string-name><given-names>C</given-names> <surname>Soto-Treviño</surname></string-name>, <string-name><given-names>N</given-names> <surname>Kopell</surname></string-name>, <article-title>Synchronization and oscillatory dynamics in heterogeneous, mutually inhibited neurons</article-title>. <source>J. Comput. Neurosci</source>. <volume>5</volume>, <fpage>5</fpage>–<lpage>16</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><given-names>H</given-names> <surname>Ozeki</surname></string-name>, <string-name><given-names>IM</given-names> <surname>Finn</surname></string-name>, <string-name><given-names>ES</given-names> <surname>Schaffer</surname></string-name>, <string-name><given-names>KD</given-names> <surname>Miller</surname></string-name>, <string-name><given-names>D</given-names> <surname>Ferster</surname></string-name>, <article-title>Inhibitory Stabilization of the Cortical Network Underlies Visual Surround Suppression</article-title>. <source>Neuron</source> <volume>62</volume>, <fpage>578</fpage>–<lpage>592</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><given-names>F</given-names> <surname>Wolf</surname></string-name>, <string-name><given-names>R</given-names> <surname>Engelken</surname></string-name>, <string-name><given-names>M</given-names> <surname>Puelma-Touzel</surname></string-name>, <string-name><given-names>JDF</given-names> <surname>Weidinger</surname></string-name>, <string-name><given-names>A</given-names> <surname>Neef</surname></string-name>, <article-title>Dynamical models of cortical circuits</article-title>. <source>Curr. Opin. Neurobiol</source>. <volume>25</volume>, <fpage>228</fpage>–<lpage>236</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><given-names>R</given-names> <surname>Tremblay</surname></string-name>, <string-name><given-names>S</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>B</given-names> <surname>Rudy</surname></string-name>, <article-title>GABAergic Interneurons in the Neocortex: From Cellular Properties to Circuits</article-title>. <source>Neuron</source> <volume>91</volume>, <fpage>260</fpage>–<lpage>292</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><given-names>H</given-names> <surname>Sompolinsky</surname></string-name>, <string-name><given-names>A</given-names> <surname>Crisanti</surname></string-name>, <string-name><given-names>HJ</given-names> <surname>Sommers</surname></string-name>, <article-title>Chaos in random neural networks</article-title>. <source>Phys. Rev. Lett</source>. <volume>61</volume>, <fpage>259</fpage>–<lpage>262</lpage> (<year>1988</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><given-names>J</given-names> <surname>Kadmon</surname></string-name>, <string-name><given-names>H</given-names> <surname>Sompolinsky</surname></string-name>, <article-title>Transition to chaos in random neuronal networks</article-title>. <source>Phys. Rev. X</source> <volume>5</volume>, <fpage>1</fpage>–<lpage>28</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="other"><string-name><given-names>J</given-names> <surname>Kadmon</surname></string-name>, <string-name><given-names>J</given-names> <surname>Timcheck</surname></string-name>, <string-name><given-names>S</given-names> <surname>Ganguli</surname></string-name>, <article-title>Predictive coding in balanced neural networks with noise, chaos and delays</article-title>. <source>Adv. Neural Inf. Process. Syst</source>. 2020-December, <fpage>1</fpage>–<lpage>12</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><given-names>R</given-names> <surname>Engelken</surname></string-name>, <string-name><given-names>F</given-names> <surname>Wolf</surname></string-name>, <string-name><given-names>LF</given-names> <surname>Abbott</surname></string-name>, <article-title>Lyapunov spectra of chaotic recurrent neural networks</article-title>. <source>bioarXiv</source> <volume>2</volume> (<year>2020</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><given-names>N</given-names> <surname>Brunel</surname></string-name>, <string-name><given-names>FS</given-names> <surname>Chance</surname></string-name>, <string-name><given-names>N</given-names> <surname>Fourcaud</surname></string-name>, <string-name><given-names>LF</given-names> <surname>Abbott</surname></string-name>, <article-title>Effects of synaptic noise and filtering on the frequency response of spiking neurons</article-title>. <source>Phys. Rev. Lett</source>. <volume>86</volume>, <fpage>2186</fpage>–<lpage>2189</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="other"><string-name><given-names>R</given-names> <surname>Engelken</surname></string-name>, <string-name><given-names>A</given-names> <surname>Ingrosso</surname></string-name>, <string-name><given-names>R</given-names> <surname>Khajeh</surname></string-name>, <string-name><given-names>S</given-names> <surname>Goedeke</surname></string-name>, <string-name><given-names>LF</given-names> <surname>Abbott</surname></string-name>, <source>Input correlations impede suppression of chaos and learning in balanced rate networks</source>. pp. <fpage>1</fpage>–<lpage>20</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><given-names>J</given-names> <surname>Schuecker</surname></string-name>, <string-name><given-names>S</given-names> <surname>Goedeke</surname></string-name>, <string-name><given-names>M</given-names> <surname>Helias</surname></string-name>, <article-title>Optimal Sequence Memory in Driven Random Networks</article-title>. <source>Phys. Rev. X</source> <volume>8</volume>, <fpage>41029</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><given-names>L</given-names> <surname>Molgedey</surname></string-name>, <string-name><given-names>J</given-names> <surname>Schuchhardt</surname></string-name>, <string-name><given-names>HG</given-names> <surname>Schuster</surname></string-name>, <article-title>Suppressing chaos in neural networks by noise</article-title>. <source>Phys. Rev. Lett</source>. <volume>69</volume>, <fpage>3717</fpage>–<lpage>3719</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><given-names>SP</given-names> <surname>Burns</surname></string-name>, <string-name><given-names>D</given-names> <surname>Xing</surname></string-name>, <string-name><given-names>MJ</given-names> <surname>Shelley</surname></string-name>, <string-name><given-names>RM</given-names> <surname>Shapley</surname></string-name>, <article-title>Searching for autocoherence in the cortical network with a time-frequency analysis of the local field potential</article-title>. <source>The J. neuroscience</source> <volume>30</volume>, <fpage>4033</fpage>–<lpage>4047</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><given-names>J</given-names> <surname>Mallet-Pare</surname></string-name>t, <article-title>Negatively invariant sets of compact maps and an extension of a theorem of Cartwright</article-title>. <source>J. Differ. Equations</source> <volume>22</volume>, <fpage>331</fpage>–<lpage>348</lpage> (<year>1976</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><given-names>S</given-names> <surname>Jahnke</surname></string-name>, <string-name><given-names>RM</given-names> <surname>Memmesheimer</surname></string-name>, <string-name><given-names>M</given-names> <surname>Timme</surname></string-name>, <article-title>Stable irregular dynamics in complex neural networks</article-title>. <source>Phys. Rev. Lett</source>. <volume>100</volume>, <fpage>2</fpage>–<lpage>5</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><string-name><given-names>S</given-names> <surname>Jahnke</surname></string-name>, <string-name><given-names>RM</given-names> <surname>Memmesheimer</surname></string-name>, <string-name><given-names>M</given-names> <surname>Timme</surname></string-name>, <article-title>How Chaotic is the Balanced State?</article-title> <source>Front. Comput. Neurosci</source>. <volume>3</volume>, <fpage>13</fpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><string-name><given-names>R</given-names> <surname>Zillmer</surname></string-name>, <string-name><given-names>N</given-names> <surname>Brunel</surname></string-name>, <string-name><given-names>D</given-names> <surname>Hansel</surname></string-name>, <article-title>Very long transients, irregular firing, and chaotic dynamics in networks of randomly connected inhibitory integrate-and-fire neurons</article-title>. <source>Phys. Rev. E - Stat. Nonlinear, Soft Matter Phys</source>. <volume>79</volume>, <fpage>1</fpage>–<lpage>13</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Monteforte</surname></string-name>, <string-name><given-names>F</given-names> <surname>Wolf</surname></string-name>, <article-title>Dynamic flux tubes form reservoirs of stability in neuronal circuits</article-title>. <source>Phys. Rev. X</source> <volume>2</volume>, <fpage>041007</fpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><string-name><given-names>MP</given-names> <surname>Touzel</surname></string-name>, <string-name><given-names>F</given-names> <surname>Wolf</surname></string-name>, <article-title>Statistical mechanics of spike events underlying phase space partitioning and sequence codes in large-scale models of neural circuits</article-title>. <source>Phys. Rev. E</source> <volume>99</volume>, <fpage>1</fpage>–<lpage>16</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><string-name><given-names>S</given-names> <surname>Olmi</surname></string-name>, <string-name><given-names>A</given-names> <surname>Politi</surname></string-name>, <string-name><given-names>A</given-names> <surname>Torcini</surname></string-name>, <article-title>Collective chaos in pulse-coupled neural networks</article-title>. <source>EPL (Euro-physics Lett</source>. <volume>92</volume>, <fpage>60007</fpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="other"><string-name><given-names>MP</given-names> <surname>Touzel</surname></string-name>, <source>Ph.D. thesis</source> (<year>2014</year>).</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><string-name><given-names>M Di</given-names> <surname>Volo</surname></string-name>, <string-name><given-names>A</given-names> <surname>Torcini</surname></string-name>, <article-title>Transition from Asynchronous to Oscillatory Dynamics in Balanced Spiking Networks with Instantaneous Synapses</article-title>. <source>Phys. Rev. Lett</source>. <volume>121</volume>, <fpage>128301</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><string-name><given-names>M Di</given-names> <surname>Volo</surname></string-name>, <string-name><given-names>M</given-names> <surname>Segneri</surname></string-name>, <string-name><given-names>DS</given-names> <surname>Goldobin</surname></string-name>, <string-name><given-names>A</given-names> <surname>Politi</surname></string-name>, <string-name><given-names>A</given-names> <surname>Torcini</surname></string-name>, <article-title>Coherent oscillations in balanced neural networks driven by endogenous fluctuations</article-title>. <source>Chaos</source> <volume>32</volume> (<year>2022</year>).</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><string-name><given-names>H</given-names> <surname>Bi</surname></string-name>, <string-name><given-names>M di</given-names> <surname>Volo</surname></string-name>, <string-name><given-names>A</given-names> <surname>Torcini</surname></string-name>, <article-title>Asynchronous and Coherent Dynamics in Balanced Excitatory-Inhibitory Spiking Networks</article-title>. <source>Front. Syst. Neurosci</source>. <volume>15</volume>, <fpage>1</fpage>–<lpage>33</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><string-name><given-names>ID</given-names> <surname>Landau</surname></string-name>, <string-name><given-names>H</given-names> <surname>Sompolinsky</surname></string-name>, <article-title>Coherent chaos in a recurrent neural network with structured connectivity</article-title>. <source>PLoS Comput. Biol</source>. <volume>14</volume>, <fpage>1</fpage>–<lpage>27</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><string-name><given-names>S</given-names> <surname>Goedeke</surname></string-name>, <string-name><given-names>J</given-names> <surname>Schuecker</surname></string-name>, <string-name><given-names>M</given-names> <surname>Helias</surname></string-name>, <article-title>Noise dynamically suppresses chaos in neural networks</article-title>. <source>arXiv</source> <volume>22</volume>, <fpage>1</fpage>–<lpage>5</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><string-name><given-names>SP</given-names> <surname>Burns</surname></string-name>, <string-name><given-names>D</given-names> <surname>Xing</surname></string-name>, <string-name><given-names>RM</given-names> <surname>Shapley</surname></string-name>, <article-title>Is gamma-band activity in the local field potential of V1 cortex a “clock” or filtered noise?</article-title> <source>The J. neuroscience : official journal Soc. for Neurosci</source>. <volume>31</volume>, <fpage>9658</fpage>–<lpage>9664</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><string-name><given-names>T</given-names> <surname>Mora</surname></string-name>, <string-name><given-names>S</given-names> <surname>Deny</surname></string-name>, <string-name><given-names>O</given-names> <surname>Marre</surname></string-name>, <article-title>Dynamical criticality in the collective activity of a population of retinal neurons</article-title>. <source>Phys. Rev. Lett</source>. <volume>114</volume>, <fpage>1</fpage>–<lpage>13</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><string-name><given-names>E</given-names> <surname>Schneidman</surname></string-name>, <string-name><given-names>MJ</given-names> <surname>Berry</surname></string-name>, <string-name><given-names>R</given-names> <surname>Segev</surname></string-name>, <string-name><given-names>W</given-names> <surname>Bialek</surname></string-name>, <article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title>. <source>Nature</source> <volume>440</volume>, <fpage>1007</fpage>–<lpage>1012</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><string-name><given-names>L</given-names> <surname>Meshulam</surname></string-name>, <string-name><given-names>JL</given-names> <surname>Gauthier</surname></string-name>, <string-name><given-names>CD</given-names> <surname>Brody</surname></string-name>, <string-name><given-names>DW</given-names> <surname>Tank</surname></string-name>, <string-name><given-names>W</given-names> <surname>Bialek</surname></string-name>, <article-title>Collective Behavior of Place and Non-place Neurons in the Hippocampal Network</article-title>. <source>Neuron</source> <volume>96</volume>, <fpage>1178</fpage>–<lpage>1191</lpage>.e4 (<year>2017</year>).</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><string-name><given-names>LS</given-names> <surname>Young</surname></string-name>, <article-title>What are SRB measures, and which dynamical systems have them?</article-title> <source>J. Stat. Phys</source>. <volume>108</volume>, <fpage>733</fpage>–<lpage>754</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><string-name><given-names>D</given-names> <surname>Ruelle</surname></string-name>, <article-title>Smooth dynamics and new theoretical ideas in nonequilibrium statistical mechanics</article-title>. <source>J. Stat. Phys</source>. <volume>95</volume>, <fpage>66</fpage>, (<year>1998</year>).</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><string-name><given-names>D</given-names> <surname>Ruelle</surname></string-name>, <article-title>Positivity of entropy production in nonequilibrium statistical mechanics</article-title>. <source>J. Stat. Phys</source>. <volume>85</volume>, <fpage>1</fpage>–<lpage>23</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Timme</surname></string-name>, <string-name><given-names>F</given-names> <surname>Wolf</surname></string-name>, <string-name><given-names>T</given-names> <surname>Geisel</surname></string-name>, <article-title>Topological Speed Limits to Network Synchronization</article-title>. <source>Phys. Rev. Lett</source>. <volume>92</volume>, <fpage>074101</fpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Timme</surname></string-name>, <string-name><given-names>F</given-names> <surname>Wolf</surname></string-name>, <article-title>The simplest problem in the collective dynamics of neural networks: is synchrony stable?</article-title> <source>Nonlinearity</source> <volume>21</volume>, <fpage>1579</fpage>–<lpage>1599</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><string-name><given-names>RE</given-names> <surname>Mirollo</surname></string-name>, <string-name><given-names>SH</given-names> <surname>Strogatz</surname></string-name>, <article-title>Synchronization of pulse-coupled biological oscillators</article-title>. <source>SIAM J. on Appl. Math</source>. <volume>50</volume>, <fpage>1645</fpage>–<lpage>1662</lpage> (<year>1990</year>).</mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="book"><string-name><given-names>E</given-names> <surname>Ziegel</surname></string-name>, <string-name><given-names>W</given-names> <surname>Press</surname></string-name>, <string-name><given-names>B</given-names> <surname>Flannery</surname></string-name>, <string-name><given-names>S</given-names> <surname>Teukolsky</surname></string-name>, <string-name><given-names>W</given-names> <surname>Vetterling</surname></string-name>, <source>Numerical Recipes: The Art of Scientific Computing</source>. (<publisher-name>Cambridge University Press</publisher-name> 1988,) Vol. <volume>29</volume>, p. <fpage>501</fpage> (<year>1987</year>).</mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><string-name><given-names>MJE</given-names> <surname>Richardson</surname></string-name>, <article-title>Firing-rate response of linear and nonlinear integrate-and-fire neurons to modulated current-based and conductance-based synaptic drive</article-title>. <source>Phys. Rev. E - Stat. Non-linear, Soft Matter Phys</source>. <volume>76</volume>, <fpage>1</fpage>–<lpage>15</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="journal"><string-name><given-names>J</given-names> <surname>Kaplan</surname></string-name>, <string-name><given-names>J</given-names> <surname>Yorke</surname></string-name>, <article-title>Chaotic behavior of multi-dimensional difference equations</article-title>. <source>H. Peitgen H. Walther</source> (eds), Funct. differential equations approximation fixed points, Lect. notes Math. Springer-Verlag <volume>730</volume>, <fpage>204</fpage>–<lpage>227</lpage> (<year>1979</year>).</mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><string-name><given-names>JP</given-names> <surname>Eckmann</surname></string-name>, <string-name><given-names>D</given-names> <surname>Ruelle</surname></string-name>, <article-title>Ergodic theory of chaos and strange attractors</article-title>. <source>Rev. Mod. Phys</source>. <volume>57</volume>, <fpage>617</fpage>–<lpage>656</lpage> (<year>1985</year>).</mixed-citation></ref>
</ref-list>
<sec id="s4">
<title>Supporting Information Text</title>
<sec id="s4a">
<label>1.</label>
<title>Delayed spiking networks</title>
<p>Here we describe the methods for Figs. 1-7. All spiking network simulations were simulated in an event based manner as described in the sections below.</p>
<sec id="s4a1">
<label>A.</label>
<title>Network Model</title>
<p>We considered a network of N quadratic integrate and fire (QIF) neurons
<disp-formula id="eqnS1">
<alternatives><graphic xlink:href="505598v2_eqnS1.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqnS2">
<alternatives><graphic xlink:href="505598v2_eqnS2.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>τ</italic><sub><italic>i</italic></sub> is the membrane time and <italic>v</italic><sub><italic>i</italic></sub> is the voltage of the neuron <italic>i</italic>. The term <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline28.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the total input current, consisting on two terms. The first one describes the synaptic current that a neuron <italic>i</italic> receives at time <italic>t</italic> given that its pre-synaptic neighbors <italic>j</italic> emitted spikes in the times <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline29.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Every time a neuron spikes, takes a time <italic>δ</italic> to arrive to its post-synaptic target. The connectivity <italic>J</italic><sub><italic>ij</italic></sub> has a binomial distribution with probability <italic>K/N</italic> where 1 ≪ <italic>K</italic> ≪ <italic>N</italic>. When a connection exists, its strength takes a constant value that only depends on whether the presynaptic and postsynaptic cells are excitatory or inhibitory.<inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline30.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, for <italic>x, y</italic> = <italic>E, I</italic>. The second term is an constant external input <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline31.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, <italic>I</italic><sub>0</sub> for excitatory and inhibitory neurons respectively.</p>
<p>The mean firing rate of the E and I populations is given by (<xref ref-type="bibr" rid="sc1">1</xref>, <xref ref-type="bibr" rid="sc2">2</xref>) <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline32.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline33.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The mean input current is given by <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline34.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>; where <italic>X</italic><sub>0</sub>, is a positive (excitatory) input current to the population <italic>x</italic>: <italic>E</italic><sub>0</sub> for the excitatory population and <italic>I</italic><sub>0</sub> for the inhibitory one.The variance of the input currents are given by <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline35.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula></p>
<p>For the particular case of networks of inhibitory neurons, the condition of a finite mean input current requires that <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline36.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. In the limit of large K the firing rate must then satisfy <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline37.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. For networks of excitatory and inhibitory neurons, we request that the first moment of the firing rates distributions are the same (<italic>ν</italic><sub><italic>E</italic></sub> = <italic>ν</italic><sub><italic>I</italic></sub>), and that the variances of the input currents are equal <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline38.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Together, these conditions impose further constraints on the synaptic weights: <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline39.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The weight matrix is written as a function of three parameters: <italic>J</italic><sub>0</sub> <italic>η</italic> = <italic>J /J &lt;</italic> 1 and <italic>ϵ</italic> = <italic>J /J</italic> (<xref ref-type="bibr" rid="sc5">5</xref>). The synaptic weights then have the form <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline40.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The stable solutions with positive rates are found for <italic>η</italic> and <italic>ϵ</italic> such that <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline41.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (<xref ref-type="bibr" rid="sc5">5</xref>).</p>
</sec>
<sec id="s4a2">
<label>B.</label>
<title>Phase reduction for integrate and fire models</title>
<p>In networks of pulse coupled units receiving constant external inputs, and whose voltage dynamics have a defined threshold <italic>x</italic><sub><italic>t</italic></sub> and a reset values <italic>x</italic><sub><italic>r</italic></sub>, the evolution between network spikes <italic>t</italic><sub><italic>s</italic></sub> is defined by a propagator function or map (<xref ref-type="bibr" rid="sc6">6</xref>–<xref ref-type="bibr" rid="sc10">10</xref>)
<disp-formula id="eqnS4">
<alternatives><graphic xlink:href="505598v2_eqnS4.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The function <italic>f</italic> evolves the state of the neuron <italic>i</italic> after the last spike in the network, <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline42.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, to the state just before the next spike at <italic>t</italic><sub><italic>s</italic>+1</sub>. If the neuron <italic>i</italic> is not postsynaptic to the neuron <italic>j</italic><sup><italic>∗</italic></sup> that produces an spike at <italic>t</italic><sub><italic>s</italic>+1</sub>, the state variable will be left unaffected, and then <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline43.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. On the contrary, if the neuron <italic>i</italic><sup><italic>∗</italic></sup> is postsynaptic to <italic>j</italic><sup><italic>∗</italic></sup>, then its state variable has to be further updated:
<disp-formula id="eqnS5">
<alternatives><graphic xlink:href="505598v2_eqnS5.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Eqs, S4 and S5, form the basis of the iterative evolution of the network from spike to spike. Its precise form will depend on the neuron model considered, as will the expression for the time to the next spike.</p>
<p>Analytically solvable neuron models which in absence of any recurrent input have periodic spiking activity can be mapped to a phase variable that linearly evolves in time. This variable, <italic>ϕ</italic>, has a propagator function <italic>f</italic> (<italic>ϕ</italic>) that is a linear function of the inter-spike times. All the information about the particular neuron model can be condensed in the form of the update function <italic>g</italic>.</p>
<p>The QIF neuron defined in <xref ref-type="disp-formula" rid="eqnS1">Eq. S1</xref>, is analytically solvable in either its supra-threshold or its excitable regime; different solutions of <xref ref-type="disp-formula" rid="eqnS1">Eq. S1</xref> can be obtained depending on the sign of <italic>I</italic><sub><italic>i</italic></sub> (in the absence of recurrent connections). In the supra-threshold regime (<italic>I &gt;</italic> 0) the solution to <xref ref-type="disp-formula" rid="eqnS1">Eq. S1</xref> with initial conditions <italic>V</italic> (<italic>t</italic><sub>0</sub>) = <italic>V</italic><sub>0</sub> is
<disp-formula id="eqnS6">
<alternatives><graphic xlink:href="505598v2_eqnS6.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The propagator function <italic>f</italic> and the update function <italic>g</italic> in the voltage representation for the supra-threshold QIF are then:
<disp-formula id="eqnS7">
<alternatives><graphic xlink:href="505598v2_eqnS7.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqnS8">
<alternatives><graphic xlink:href="505598v2_eqnS8.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
We see that from <xref ref-type="disp-formula" rid="eqnS7">Eq. S7</xref>, that the variable change <italic>ϕ</italic> = 2 arctan <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline44.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> reduces the dynamics to a linearly evolving phase, <italic>ϕ</italic> ∈ (−<italic>π, π</italic>). The free period can be obtained requesting that <italic>V</italic><sub><italic>t</italic></sub> = +∞ and <italic>V</italic><sub><italic>r</italic></sub> = −∞ : <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline45.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This yields a value for the frequency <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline46.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The propagator and update functions for <italic>ϕ</italic> are
<disp-formula id="eqnS9">
<alternatives><graphic xlink:href="505598v2_eqnS9.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqnS10">
<alternatives><graphic xlink:href="505598v2_eqnS10.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Where PRC is the phase response curve.</p>
</sec>
<sec id="s4a3">
<label>C.</label>
<title>Calculation of the critical delay</title>
<p>Here we described the method to describe the transition to population oscillations and how to obtain the critical delay following the approach by Richardson (<xref ref-type="bibr" rid="sc11">11</xref>, <xref ref-type="bibr" rid="sc12">12</xref>). The single neuron dynamics are then given by <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline47.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Here, <italic>E</italic>(<italic>t</italic>) = <italic>τ KJν</italic>(<italic>t δ</italic>) and <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline48.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> calculated in standard manner under the assumption of uncorrelated activity in the asynchronous irregular state (<xref ref-type="bibr" rid="sc2">2</xref>–<xref ref-type="bibr" rid="sc4">4</xref>), <italic>f</italic> (<italic>v</italic>) = <italic>v</italic><sup>2</sup> for the case of the QIF neuron here considered and <italic>η</italic><sub><italic>i</italic></sub>(<italic>t</italic>) is white gaussian noise. If the number of inputs per neuron <italic>K</italic> is fixed, then the dynamics of the distribution of voltages <italic>P</italic> (<italic>v, t</italic>) are described by a Fokker-Plank equation.
<disp-formula id="eqnS11">
<alternatives><graphic xlink:href="505598v2_eqnS11.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqnS12">
<alternatives><graphic xlink:href="505598v2_eqnS12.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>J</italic>(<italic>v, t</italic>) is the probability current and the voltage-independent diffusion coefficient is given by <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline49.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Richardson (<xref ref-type="bibr" rid="sc11">11</xref>, <xref ref-type="bibr" rid="sc12">12</xref>) developed a numerical method to i) evaluate numerically the steady state solution <italic>p</italic><sub>0</sub>(<italic>v</italic>) and find self consistently the steady state firing rate <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline50.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and ii) the linear response to parameter modulations, which allows to self-consistently calculate the critical paramete where the asynchronous irregular state loses stability to a synchronous irregular one. Here we give a brief summary of how to compute, for an otherwise fixed set of parameters, the critical delay <italic>δ</italic><sub><italic>c</italic></sub>. If <italic>E</italic> and <italic>σ</italic> are constant, a steady state will be reached in which the probability distribution is independent of time. In this case, the stationary firing rate can be self consistently found by the normalization condition of the probability distribution. This was done analytically in (<xref ref-type="bibr" rid="sc3">3</xref>, <xref ref-type="bibr" rid="sc4">4</xref>) for the leaky integrate and fire (LIF) neuron and in (<xref ref-type="bibr" rid="sc13">13</xref>) for the quadratic integrate and fire (QIF). Although the general-time dependent solution for nonlinear equations is inaccessible, significant progress can be made by studying the linear response to a harmonically modulated parameter. <italic>α</italic> = <italic>α</italic><sub>0</sub> + <italic>α</italic><sub>1</sub> exp(<italic>iωt</italic>).</p>
<p>We first solve the harmonic modulation of the input current <italic>E</italic>(<italic>t</italic>) for a network of uncoupled neurons, following Ref (<xref ref-type="bibr" rid="sc11">11</xref>) and then ask for self-consistency. To do this, we decompose the probability current <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline51.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the density <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline52.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, the input current <italic>E</italic> = <italic>E</italic><sub>0</sub> + <italic>E</italic><sub>1</sub><italic>e</italic><sup><italic>iωt</italic></sup> and the firing rate <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline53.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Inserting those in <xref ref-type="disp-formula" rid="eqnS12">Eq. S12</xref>, and separating contributions to the modulation coming from <italic>E</italic><sub>1</sub> and from <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline54.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (see sections 3.3 and 4.1 in Ref (<xref ref-type="bibr" rid="sc11">11</xref>) for details) we obtain two equations. One, defines the <italic>E</italic><sub>0</sub> that satisfies the steady state with the target firing rate <italic>r</italic><sub>0</sub>. The other is the equation that relates <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline55.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where <italic>Â</italic>(<italic>ω</italic>) is a function of the frequency <italic>ω</italic>, obtained using theRef (<xref ref-type="bibr" rid="sc11">11</xref>) method.</p>
</sec>
</sec>
<sec id="s4b">
<title>Calculation of the critical delay</title>
<p>To compute the critical delay that destabilizes the asynchronous irregular state, we need to require that the input modulation <italic>E</italic><sub>1</sub>, which had so far been considered external to the network, arises from network activity. This means that <italic>E</italic>(<italic>t</italic>) = <italic>τ KJν</italic>(<italic>t</italic> − <italic>δ</italic>) ≈ <italic>E</italic><sub>0</sub><italic>r</italic><sub>0</sub> + <italic>E</italic><sub>1</sub><italic>e</italic><sup><italic>iωt</italic></sup>, <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline56.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> where <italic>E</italic><sub><italic>s</italic></sub> = <italic>τ KJ &lt;</italic> 0 (for inhibitory networks). The self-consistent equation (analogous to Eq. (52) in Ref (<xref ref-type="bibr" rid="sc11">11</xref>), reduced to the delta coupled network case) is 1 = − |<italic>E</italic><sub><italic>s</italic></sub>| <italic>e</italic><sup>−<italic>iωδ</italic></sup><italic>Â</italic>(<italic>iω</italic>). Writing the (known function) <italic>Â</italic> = |<italic>A</italic>(<italic>ω</italic>)| <italic>e</italic><sup><italic>iϕ</italic>(<italic>ω</italic>)</sup>, separating in real and imaginary part we have effectively two equations with two unknowns:
<disp-formula id="eqnS13">
<alternatives><graphic xlink:href="505598v2_eqnS13.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqnS14">
<alternatives><graphic xlink:href="505598v2_eqnS14.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
From which we can obtain the value of the frequency <italic>ω</italic> and the critical delay <italic>δ</italic> that destabilizes the asynchronous irregular state.</p>
</sec>
<sec id="s4c">
<label>2.</label>
<title>Ergodic theory of effectively delayed spiking neuronal networks</title>
<sec id="s4c1">
<label>A.</label>
<title>Background and definitions</title>
<p>The simplest measure of the dimensionality of a chaotic attractor is called Kolmogorov capacity or box-counting dimension (<xref ref-type="bibr" rid="sc14">14</xref>). It is a metric-based measure in the sense that it does not take into account the density of orbits on the attractor. Given a parallelepiped of side <italic>ϵ</italic>, the number <italic>N</italic> (<italic>ϵ</italic>) that is needed to cover the set of points conforming the attractor <italic>A</italic> is expected to satisfy <italic>N</italic> (<italic>ϵ</italic>) <italic>ϵ</italic><sup>−<italic>dc</italic></sup>. The capacity of the set can then be defined as <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline57.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. If instead of considering the number of hypercubes of side <italic>ϵ</italic> to cover the attractor, we consider the number of hypercubes <italic>N</italic> (<italic>ϵ, ϑ</italic>) needed to cover a fraction <italic>ϑ</italic> of the attractor, then the <italic>ϑ</italic> capacity can be defined as <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline58.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. This measure, which for <italic>ϑ</italic> = 1 is directly the capacity and otherwise will be called <italic>D</italic><sub><italic>µ</italic></sub>, satisfies <italic>D</italic><sub><italic>µ</italic></sub> ≤ <italic>D</italic><sub><italic>C</italic></sub>. Finally, a point-wise dimension <italic>D</italic><sub><italic>p</italic></sub>(<italic>x</italic>) can be as well defined by estimating the exponent with which the total probability within a ball of radius <italic>ϵ</italic> decreases as the radius vanishes (<xref ref-type="bibr" rid="sc14">14</xref>) <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline59.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>For a definition of an attractor dimension that is related to the measure of the attractor, it is necessary to define a partition ℬ= {<italic>B</italic><sub><italic>i</italic></sub>} that covers the phase space. At each measurement, the trajectory of the system can be found in one <italic>B</italic><sub><italic>i</italic></sub> such that for sufficiently long times, a frequency of occurrence <italic>P</italic><sub><italic>i</italic></sub> can be assigned to each <italic>B</italic><sub><italic>i</italic></sub>. When the size of the elements of the partition tends to zero, <italic>P</italic> defines a probability density such that its sum over the attractor is equivalent to the measure on the attractor <italic>µ</italic>(<italic>A</italic>). Then, the frequency of occurrence can be written as <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline60.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>When observing the state of the dynamical system at a given point if time, the information that is gained about the state depends on the resolution of the instrument, the <italic>diameter</italic> of the partition used. This diameter is basically given by the largest size of the elements <italic>B</italic><sub><italic>i</italic></sub> of the chosen partition. If the partition has diameter <italic>ϵ</italic>, then the information gained by making a measurement is given by <italic>I</italic>(ℬ (<italic>ϵ</italic>)) = − <sub><italic>i</italic>=1</sub> <italic>µ</italic>(<italic>B</italic><sub><italic>i</italic></sub>(<italic>ϵ</italic>)) log (<italic>µ</italic>(<italic>B</italic><sub><italic>i</italic></sub>(<italic>ϵ</italic>))). If now, from all the possible partitions to be chosen, we choose that one that minimizes the expression, then the information dimension <italic>D</italic><sub><italic>I</italic></sub> is defined as <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline61.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>Kaplan (<xref ref-type="bibr" rid="sc15">15</xref>), defined a measure of an attractor, the Lyapunov dimension, as a function of the Lyapunov exponents:
<disp-formula id="eqnS15">
<alternatives><graphic xlink:href="505598v2_eqnS15.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Where <italic>l</italic> is such that <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline62.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline63.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Kaplan (<xref ref-type="bibr" rid="sc15">15</xref>) conjectured initially that the Lyapunov dimension <italic>D</italic> was generally equal to the fractal (box counting) dimension, or a lower bound to it (<xref ref-type="bibr" rid="sc14">14</xref>). This form of the Kaplan-York conjecture can be found in the literature (<xref ref-type="bibr" rid="sc15">15</xref>, <xref ref-type="bibr" rid="sc16">16</xref>) and in text books (<xref ref-type="bibr" rid="sc17">17</xref>, <xref ref-type="bibr" rid="sc18">18</xref>). In further work by both Kaplan and York (<xref ref-type="bibr" rid="sc19">19</xref>) and York and colleagues (<xref ref-type="bibr" rid="sc14">14</xref>), this conjecture was updated to a form which includes information of the density of orbits over the attractor. They specifically conjecture that <italic>D</italic> = <italic>D</italic><sub><italic>C</italic></sub> (<italic>ϑ</italic>). as in Ref. (<xref ref-type="bibr" rid="sc14">14</xref>) further conjectured that this equality and the original Kaplan York conjecture all hold. Rigorous results show that <italic>D</italic><sub><italic>P</italic></sub> = <italic>D</italic><sub><italic>I</italic></sub> = lim<sub><italic>ϑ</italic>→1</sub> <italic>D</italic><sub><italic>C</italic></sub>(<italic>ϑ</italic>) ≤ <italic>D</italic> for invertible smooth maps (as reviewed in (<xref ref-type="bibr" rid="sc14">14</xref>, <xref ref-type="bibr" rid="sc20">20</xref>)). The conjecture has been shown to hold whenever a Sinai-Ruelle-Bowen (SRB) measure (defined as a measure that is absolutely continuous along the unstable manifolds) exists.</p>
<p>Regarding the relation of the metric entropy with the Lyapunov exponents, Ruelle proved (<xref ref-type="bibr" rid="sc20">20</xref>) that :
<disp-formula id="eqnS16">
<alternatives><graphic xlink:href="505598v2_eqnS16.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The equality, known as the Pesin identity, was shown to hold true for SRB measures. If the system is hyperbolic, the tangent space to the trajectory can be decomposed in the direct sum of its linear stable and unstable subspaces <sup><xref ref-type="fn" rid="fn1">†</xref></sup>. Hyperbolicity implies the existence of an SRB measure, and that the Pesin identity and the Kaplan York conjecture hold. Nevertheless, ruling out the hyperbolicity of the system allows for no statement regarding the existence of an SRB measure. Keeping in mind that the measures derived from the Lyapunov spectrum are upper bounds and that no rigorous statement can be made in favor of the equalities, we will nevertheless refer to them as the metric entropy and the attractor dimension.</p>
</sec>
<sec id="s4c2">
<label>B.</label>
<title>Lyapunov spectrum of delayed spiking neuronal networks</title>
<p>Given a neuronal model <italic>F</italic> (<italic>V</italic><sub><italic>i</italic></sub>) and a model for the SCA that are exactly solvable, the network can be propagated between spikes and updated by <xref ref-type="disp-formula" rid="eqnS10">Eqs. S10</xref> and <xref ref-type="disp-formula" rid="eqnS7">S7</xref>. From these equations, a Jacobian L(<bold><italic>x</italic></bold>(<italic>n</italic>)) at each spike time can be obtained, from which the estimation of the Lyapunov dimension <italic>D</italic> (<xref ref-type="disp-formula" rid="eqnS15">Eq.S15</xref>) and the metric entropy <italic>H</italic> (<xref ref-type="disp-formula" rid="eqnS16">Eq.S16</xref>) are possible via QR decomposition.</p>
<sec id="s4c2a">
<label>B.1.</label>
<title>Derivation of the single spike Jacobian</title>
<p>In the following, we will focus on the derivation of 𝕃 (<bold><italic>x</italic></bold>(<italic>n</italic>)) for the delayed system of neuronal types that allow for a phase representation. In this case, the Jacobian elements can be written in terms of the phase response curve (PRC) (defined in <xref ref-type="disp-formula" rid="eqnS10">Eq. S10</xref> for the QIF case).</p>
<p>When a SCA with associated variable <italic>ξ</italic><sub><italic>m</italic></sub><italic>∗</italic>, spikes at <italic>τ</italic><sub><italic>s</italic>+1</sub> ∈ [<italic>t</italic><sub><italic>s</italic></sub>, <italic>t</italic><sub><italic>s</italic>+1</sub>], the soma units that are not postsynaptic to the axon will evolve independently of when the spike is exactly, following <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline64.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and the ones that are postsynaptic to it will be both propagated and updated <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline65.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (see <xref ref-type="disp-formula" rid="eqnS10">Eq. S10</xref>).</p>
<p>Equivalently, when a spike is emitted by a neuron, it will be received only by its own associated SCA. The effect of the incoming spike on the dynamics of the SCA will then be <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline66.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. SCA that are not post-synaptic to the spiking neuron will evolve with <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline67.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (see <italic>Methods</italic>).</p>
<p>In order to calculate the single spike Jacobian, we first summarize the equations for the phase neuron and the SCA. We define:
<disp-formula id="eqnS17">
<alternatives><graphic xlink:href="505598v2_eqnS17.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Where the supra-index <italic>ξ</italic> was added to emphasize that those constants are only meaningful for SCA.</p>
<p>The propagation and update functions for the neurons <italic>f</italic> and <italic>g</italic>, and for the SCA, <italic>η</italic> and <italic>γ</italic>, for a network of QIF neurons with QIF SCA’s are:</p>
<sec id="s4c2a1">
<title>Soma equations</title>
<p>Propagator
<disp-formula id="eqnS18">
<alternatives><graphic xlink:href="505598v2_eqnS18.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
With derivatives
<disp-formula id="eqnS19">
<alternatives><graphic xlink:href="505598v2_eqnS19.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Update function
<disp-formula id="eqnS20">
<alternatives><graphic xlink:href="505598v2_eqnS20.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
With derivatives
<disp-formula id="eqnS21">
<alternatives><graphic xlink:href="505598v2_eqnS21.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Where the phase response curve was defined in <xref ref-type="disp-formula" rid="eqnS10">Eq. S10</xref>.</p>
</sec>
<sec id="s4c2a2">
<title>SCA equations</title>
<p>Propagator
<disp-formula id="eqnS22">
<alternatives><graphic xlink:href="505598v2_eqnS22.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
With derivatives
<disp-formula id="eqnS23">
<alternatives><graphic xlink:href="505598v2_eqnS23.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Update function
<disp-formula id="eqnS24">
<alternatives><graphic xlink:href="505598v2_eqnS24.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
With derivatives
<disp-formula id="eqnS25">
<alternatives><graphic xlink:href="505598v2_eqnS25.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
For a generic neuron model with state variable <italic>x</italic>, that evolves with function <italic>f</italic> and updates with function <italic>g</italic>, the derivative with respect to some other neuron variable (possibly defined by a different neuron model) <italic>y</italic> can be written as:
<disp-formula id="eqnS26a">
<alternatives><graphic xlink:href="505598v2_eqnS26a.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqnS26b">
<alternatives><graphic xlink:href="505598v2_eqnS26b.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqnS26c">
<alternatives><graphic xlink:href="505598v2_eqnS26c.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqnS26d">
<alternatives><graphic xlink:href="505598v2_eqnS26d.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqnS26e">
<alternatives><graphic xlink:href="505598v2_eqnS26e.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Where <italic>i</italic><sup>0</sup> is the index corresponding to the non-postsynaptic neurons, <italic>i</italic><sup><italic>∗</italic></sup> for the postsynaptic ones and the spiking neuron <italic>j</italic><sup><italic>∗</italic></sup>. Note that if the spiking neuron is reset, then only the last term (Eq. S26e) survives given that <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline68.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and therefore has null derivative. The term <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline69.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline70.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> can be extracted from the fact that <italic>f</italic> (<italic>y</italic><sub><italic>j</italic></sub><italic>∗</italic> (<italic>t</italic><sub><italic>s</italic></sub>), <italic>τ</italic><sub><italic>s</italic>+1</sub> − <italic>t</italic><sub><italic>s</italic></sub>)) = <italic>X</italic><sub><italic>T</italic></sub> and therefore:
<disp-formula id="eqnS27">
<alternatives><graphic xlink:href="505598v2_eqnS27.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
In the case we are analyzing here,
<disp-formula id="eqnS28">
<alternatives><graphic xlink:href="505598v2_eqnS28.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The terms of the Jacobian for the delayed system can be summarized as follows:</p>
</sec>
<sec id="s4c2a3">
<title>Non-postsynaptic somas <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline71.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula></title>
<p>Contribute to the Jacobian only with diagonal terms from <xref ref-type="disp-formula" rid="eqnS26b">Eq. S26b</xref>
<disp-formula id="eqnS29">
<alternatives><graphic xlink:href="505598v2_eqnS29.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
</p>
</sec>
<sec id="s4c2a4">
<title>Postsynaptic somas <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline72.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula></title>
<p>Contribute with non diagonal terms (when receiving a spike from a delayer SCA <italic>ξ</italic><sub><italic>m</italic></sub><italic>∗</italic>, from <xref ref-type="disp-formula" rid="eqnS26d">Eq. S26d</xref> and <xref ref-type="disp-formula" rid="eqnS26e">S26e</xref>
<disp-formula id="eqnS30">
<alternatives><graphic xlink:href="505598v2_eqnS30.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
</p>
</sec>
<sec id="s4c2a5">
<title>Spiking neuron <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline73.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula></title>
<p>Only one term in the Jacobian, from <xref ref-type="disp-formula" rid="eqnS26e">Eq. S26e</xref>, <xref ref-type="disp-formula" rid="eqnS26c">Eq. S26c</xref> and <xref ref-type="disp-formula" rid="eqnS26d">S26d</xref> vanish
<disp-formula id="eqnS31">
<alternatives><graphic xlink:href="505598v2_eqnS31.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
</p>
</sec>
<sec id="s4c2a6">
<title>Non-postsynaptic SCA <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline74.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula></title>
<p>Contribute with diagonal terms from <xref ref-type="disp-formula" rid="eqnS26b">Eq. S26b</xref>
<disp-formula id="eqnS32">
<alternatives><graphic xlink:href="505598v2_eqnS32.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
</p>
</sec>
<sec id="s4c2a7">
<title>Postsynaptic SCA <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline75.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula></title>
<p>Contribute with non-diagonal terms (when receiving a spike from a neuron <italic>ϕ</italic><sub><italic>j</italic></sub><italic>∗</italic> <bold>)</bold> from <xref ref-type="disp-formula" rid="eqnS26d">Eq. S26d</xref> and <xref ref-type="disp-formula" rid="eqnS26e">S26e</xref>
<disp-formula id="eqnS33">
<alternatives><graphic xlink:href="505598v2_eqnS33.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
</p>
</sec>
<sec id="s4c2a8">
<title>Spiking SCA <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline76.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula></title>
<p>Only one term in the Jacobian, from <xref ref-type="disp-formula" rid="eqnS26e">Eq. S26e</xref>. <xref ref-type="disp-formula" rid="eqnS26c">Eq. S26c</xref> and <xref ref-type="disp-formula" rid="eqnS26d">S26d</xref> cancel one each other
<disp-formula id="eqnS34">
<alternatives><graphic xlink:href="505598v2_eqnS34.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
If one chooses <italic>ξ</italic><sub><italic>R</italic></sub> = 0, then the delay introduced in the network is exact, and the equation vanishes <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline77.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for the <italic>delayer</italic> SCA. In this case, the Jacobian is going to be singular. If the reset value is small but nonzero, an invertible Jacobian is obtained. In this last case, is necessary to ensure that the reset value is passed by the singularity at <italic>ξ</italic><sub><italic>l</italic></sub> = 1 − <italic>A</italic><sub><italic>lj</italic></sub> (see S24), so an incoming spike takes finite time to drive the SCA to threshold (i.e. there is a solution for <xref ref-type="disp-formula" rid="eqnS35">Eq. S35</xref>, see below). Then it is necessary that <italic>ξ</italic><sub><italic>R</italic></sub> <italic>&gt;</italic> 1 − <italic>A</italic><sub><italic>lj</italic></sub>.
<disp-formula id="eqnS35">
<alternatives><graphic xlink:href="505598v2_eqnS35.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
</p>
</sec>
</sec>
</sec>
<sec id="s4c3">
<title>C. The direct method for the estimation of the first LE</title>
<p>We computed the first Lyapunov exponent via direct method in order to numerically corroborate the equations derived in the previous subsection. For this, after a long warm-up, we perturbed the network by adding a random vector of norm <italic>ϵ</italic>. After a short simulation of <italic>T</italic> = 100 ms the norm <italic>β</italic> of the difference between the perturbed and the unperturbed final states, simulated separately, is stored. The iteration of this procedure N times leads to the estimation of the first Lyapunov exponent via the following formula:
<disp-formula id="eqnS36">
<alternatives><graphic xlink:href="505598v2_eqnS36.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The comparison between the direct method and the one derived in the above subsection are shown in <xref rid="figS8" ref-type="fig">Figure S8<bold>b</bold></xref>, for <italic>ϵ</italic> = 10<sup>−10</sup> and N=5000, showing good agreement.</p>
</sec>
</sec>
<sec id="s4d">
<label>3.</label>
<title>Delayed rate networks</title>
<p>We study two examples of rate networks. The activity <italic>x</italic><sub><italic>i</italic></sub> of each unit <italic>i</italic> is defined by
<disp-formula id="eqnS37">
<alternatives><graphic xlink:href="505598v2_eqnS37.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>τ</italic> is the time constant, <italic>F</italic><sub><italic>i</italic></sub> is a scalar continuous function, <bold>x</bold>(<italic>t</italic>) = <italic>{x</italic><sub>1</sub>(<italic>t</italic>), …, <italic>x</italic><sub><italic>N</italic></sub> (<italic>t</italic>)<italic>}</italic> is the vector of all activities and <italic>δ</italic> is the interaction delay.</p>
<sec id="s4d1">
<label>A.</label>
<title>Classic rate network without Dale’s law</title>
<p>We modify a classic network of <italic>N</italic> rate units originally studied by (<xref ref-type="bibr" rid="sc21">21</xref>) to incorporate delays. In this case,
<disp-formula id="eqnS38">
<alternatives><graphic xlink:href="505598v2_eqnS38.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>ϕ</italic> is a nonlinear input-output function we choose to be <italic>ϕ</italic>(<italic>x</italic>) = tanh(<italic>x</italic>), <italic>g</italic> is a gain parameter, N is the number of neurons and <italic>J</italic><sub><italic>ij</italic></sub> is the connectivity matrix whose elements are Gaussian distributed with zero mean and unit variance. In the case of <italic>δ</italic> = 0, the system has a stable trivial fixed point for <italic>g &lt;</italic> 1. At <italic>g</italic> = 1, the system looses linear stability and a chaotic attractor is created. Because the fixed point is the same for each neuron, the linearized delayed system can be studied by proposing a solution of the form <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline78.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where <italic>u</italic><sub><italic>k</italic></sub> are the eigenvectors of <italic>J</italic>. The relationship between the eigenvalues of J, <italic>µ</italic><sub><italic>k</italic></sub>, and the decay rates <italic>ν</italic><sub><italic>k</italic></sub> determining the linear stability of the delayed system is given by (see also (<xref ref-type="bibr" rid="sc22">22</xref>–<xref ref-type="bibr" rid="sc24">24</xref>)):
<disp-formula id="eqnS39">
<alternatives><graphic xlink:href="505598v2_eqnS39.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Then, for each mode, the stability is given by the solutions of <italic>H</italic>(<italic>ν</italic>) = <italic>ν</italic> + 1 − <italic>µe</italic><sup>−<italic>νδ</italic></sup> = 0. The stability boundary is no longer a line at <italic>g</italic> = 1 but is defined by off-centered Archimedean spirals (<xref ref-type="bibr" rid="sc23">23</xref>) that always contain the <italic>g</italic> = 1 point. If the elements of <italic>J</italic> are correlated, then the eigenvalue distribution of <italic>J</italic> will be ellipsoidal (<xref ref-type="bibr" rid="sc25">25</xref>) and system can loose stability via intersections of the stability spiral with the eigenvalue spectrum giving rise to limit cycle oscillations (<xref ref-type="bibr" rid="sc24">24</xref>). Here we focus on the chaotic regime, for <italic>g &gt;</italic> 1, and for simplicity we look at uncorrelated configurations of <italic>J</italic> but those seem not to affect our results. We notice that the mean field theory developed in (<xref ref-type="bibr" rid="sc21">21</xref>) would not be changed by including delayed interactions, and therefore we do not expect any change in the dynamics.</p>
</sec>
<sec id="s4d2">
<label>B.</label>
<title>Inhibitory rate network in the balanced state</title>
<p>Following Ref. (<xref ref-type="bibr" rid="sc26">26</xref>), we study a network of <italic>N</italic> inhibitory units which we modify to incorporate delayed interactions. In this case,
<disp-formula id="eqnS40">
<alternatives><graphic xlink:href="505598v2_eqnS40.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Where <italic>m</italic><sub>0</sub> is the target mean rate, and <italic>J</italic><sub><italic>ij</italic></sub> is defined as in the classic SCS case. The nonlinearity in this case is given by a threshold linear function <italic>ϕ</italic>(<italic>x</italic>) = [<italic>x</italic>]<sub>+</sub>. The size of the network N, the sparsity K and the strength of the inhibitory connections <italic>J</italic><sub>0</sub> are related to the variables <italic>g</italic><sub>0</sub> and <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline79.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> by <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline80.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline81.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
</sec>
<sec id="s4d3">
<label>C.</label>
<title>Small delay approximation</title>
<p>We study the system above by making a small delay approximation. We define <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline82.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Then, <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline83.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> can be approximated by a Taylor expansion of order <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline84.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Each time derivative of <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline85.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, defines a new variable we call <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline86.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. The infinite dimensional system defined in Eqs. (S38, S40) can then be approximated by the 𝒪 + 1 dimensional system given by
<disp-formula id="eqnS41">
<alternatives><graphic xlink:href="505598v2_eqnS41.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqnS42">
<alternatives><graphic xlink:href="505598v2_eqnS42.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqnS43">
<alternatives><graphic xlink:href="505598v2_eqnS43.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Where <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline87.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>We notice that the small delay approximation, is nothing more than expanding the term <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline88.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="eqnS39">Eq. S39</xref> in powers of the delay <italic>δ</italic>. When comparing the eigenvalues of the exact delayed system against with the eigenvalues of the system under the small delay approximation, we therefore compare the solutions <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline89.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> of <xref ref-type="disp-formula" rid="eqnS39">Eq. S39</xref> with the solutions <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline90.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> of
<disp-formula id="eqnS44">
<alternatives><graphic xlink:href="505598v2_eqnS44.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The difference among these two is shown in <xref rid="figS13" ref-type="fig">Figure S13<bold>a</bold></xref>.</p>
</sec>
</sec>
<sec id="s4e">
<label>4.</label>
<title>Ergodic theory of chaos for weakly delayed rate networks</title>
<p>Defining the N dimensional matrix 𝔸 with components <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline91.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, at each point in time, the Jacobian 𝕃 of the (𝒪 + 1)N system can be written as:
<disp-formula id="eqnS45">
<alternatives><graphic xlink:href="505598v2_eqnS45.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where 𝕀 is the N dimensional identity matrix. Concretely, for the delayed SCS system defined in <xref ref-type="disp-formula" rid="eqnS38">Eq. S38</xref> we have
<disp-formula id="eqnS46">
<alternatives><graphic xlink:href="505598v2_eqnS46.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
whereas for the purely inhibitory network
<disp-formula id="eqnS47">
<alternatives><graphic xlink:href="505598v2_eqnS47.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Defining now the vector <bold>z</bold>(<italic>t</italic>) = <bold>x</bold>(<italic>t</italic>), <bold>y</bold><sup>0</sup>(<italic>t</italic>), <bold>y</bold><sup>1</sup>(<italic>t</italic>), …, <bold>y</bold><sup>𝒪−1</sup>(<italic>t</italic>), the evolution of the system defined in S41, can be re-written by defining G such that <italic>ż</italic><sub><italic>i</italic></sub> = <italic>G</italic><sub><italic>i</italic></sub>(<bold>z</bold>).</p>
<p>The calculation of the Lyapunov exponents can be done following Engelken et al. (<xref ref-type="bibr" rid="sc27">27</xref>).</p>
<fig id="figS1" position="float" fig-type="figure">
<label>Fig. S1.</label>
<caption><title>Single compartment axonunit</title>
<p><bold>(a)</bold> Three possible representation for the dynamics of the QIF neuron. (Left) A quadratic integrate and fire (QIF), has two fixed points at <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline102.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, an stable one (black) and an unstable one (white). This neuron is solvable analytically either in the tonic spiking regime or the excitable regime but nor in both. The neuron model diverges in finite time, emitting a spike. (Middle) A classic variable transformation was introduced by (<xref ref-type="bibr" rid="sc28">28</xref>) and is known as the theta neuron. (Right) We introduce a variable transformation, the <italic>ξ</italic> representation, tailored to the SCA. This representation is analytically solvable. The SCA at rest in <italic>ξ</italic> = 0 is pulled towards −∞ when receiving a spike, and evolves towards zero, emitting a spike at <italic>ξ</italic> = −1 <bold>(b)</bold> Phase space of the SCA <italic>ξ</italic> (Top) and the canonical QIF representations (Bottom) <bold>(c)</bold> Time evolution without reset to the fixed point <italic>ξ</italic><sub><italic>r</italic></sub> = <italic>ξ</italic><sub><italic>t</italic></sub> = −1. When the <italic>ξ</italic> variable crosses -1 (Top), in the voltage representation the voltage diverges (Bottom) <bold>(c)</bold> Time evolution resetting the SCA to its stable fixed point i.e. <italic>ξ</italic><sub><italic>r</italic></sub> = 0 in both representations.</p></caption>
<graphic xlink:href="505598v2_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Fig. S2.</label>
<caption><title>Dependence on the reset values of the single compartment axon</title>
<p>(see <xref ref-type="disp-formula" rid="eqnS35">Eq. S35</xref> and <italic>Methods</italic>); <italic>ξ</italic><sub><italic>r</italic></sub> = 0 corresponding to the exact delay framework.<bold>(a)</bold> Full spectrum for different values of the reset. The spectrum is composed of two branches (left). The first one (middle)characterizes the biologically relevant degrees of freedom (up to <italic>i</italic> = <italic>N</italic>, where N is the number of soma units in the network). The second branch (right) is defined by the time it takes for the axon to relax back to its fixed point. In the “exact delay” framework where <italic>ξ</italic><sub><italic>r</italic></sub> = 0, that time constant is zero and therefore the second branch of the Lyapunov exponents diverges to−∞. If the reset point is close to the fixed point the SCA will relax faster than what its dictated by its time constant <italic>τ</italic> <sup><italic>m,ξ</italic></sup>. As can be seen, the first part of the spectrum seems to be independent on the choice of <italic>ξ</italic><sub><italic>r</italic></sub>, given that the time constant is chosen such that they dynamics are not visibly affected. This results on identical dependencies of the normalized metric entropy <italic>h</italic> <bold>(b)</bold> and attractor dimension <bold>(c)</bold> of the value of the delay.</p></caption>
<graphic xlink:href="505598v2_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Fig. S3.</label>
<caption><title>Convergence of the Lyapunov spectrum and finite size effects in the first Lyapunov exponent.</title>
<p><bold>(a)</bold> Example traces of Lyapunov exponents for different values of the delay. <bold>(b)</bold> Difference between different different LE traces for different instantiations of the random connectivity |Δ<italic>λ</italic><sub><italic>i</italic></sub>| get smaller with time. Over the course of seconds simulations the LE converge to its true value. <bold>(c)</bold> Distributions of coefficient of variations and firing rate distributions for the example rasters in <xref rid="fig3" ref-type="fig">Figure 3</xref> of the main text. <bold>(d)</bold> First Lyapunov exponent as a function of the delay for different network sizes N. The larger the network the clearer the downward dependence of the first exponent with the delay before the transition. <bold>(e)</bold> MSD indicating that the increase in the exponent arises from the mean field Hopf bifurcation.</p></caption>
<graphic xlink:href="505598v2_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Fig. S4.</label>
<caption><title>Structure of the Lyapunov Spectrum (K=50).</title>
<p><bold>(a)</bold> Fraction of positive Lyapunov Exponents. <bold>(b)</bold> Slope of the <italic>λ</italic><sub><italic>i</italic></sub> computed by <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline103.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where <italic>n &lt;</italic> 10, where the transition occurs and <italic>λ</italic><sub><italic>i</italic></sub>(0) is the spectrum of the non=-delayed system <bold>(c)</bold> Cumulative sum of the magnitude computed in (b). The minimum <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline104.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> indicates the fraction of LE that have an initially negative dependence with the delay as computed by taking the <italic>n</italic> th difference. Approximately the first 5% LE decrease their magnitude with increasing delay before the transition. <bold>(d)</bold> All biologically-relevant Lyapunov exponents as a function of the delay for a single realization of the random connectivity. After the transition at <italic>δ</italic><sub><italic>c</italic></sub> = 3 ms the maximum lyapunov exponent vanishes (the trivial exponent is present because of the translation invariance of the system) and the rest of the spectrum relaxes to the inverse of the membrane time constant <italic>tau</italic><sub><italic>m</italic></sub> = 10 ms. After the instability, all positive Lyapunov exponents increase in magnitude (the LE that was initially zero is in dash). Negative LE decrease in magnitude. <bold>(e)</bold> Fraction of LE that have a negative slope with the delay before the transition as a function of the <italic>n</italic>th difference. <bold>(f)</bold> Slope of the <italic>λ</italic><sub><italic>i</italic></sub> computed by <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline105.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for n after the oscillatory transition. Right after the transition most LE have a larger value that in the non-delayed case <bold>(g)</bold> Cumulative sum of the magnitude computed in (f)</p></caption>
<graphic xlink:href="505598v2_figS4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Fig. S5.</label>
<caption><title>Numerical precision of the Richardson’s method.</title>
<p><bold>(a)</bold> The threshold-integration method developed by Richardson (<xref ref-type="bibr" rid="sc11">11</xref>) involves integrating the first derivative of the voltage probability distribution and that of the current backwards from threshold to a lower bound of the voltage. The QIF neuron used here does not have a threshold or a reset value, given that it diverges in finite time and therefore the critical delay computed with this method will depend on the choice of the integration bounds. Empirically nevertheless we see that the estimation changes very little after a certain threshold value. Critical delay for a network of K=50, target firing rate =5 Hz, and other parameters as default, as a function of <italic>dV</italic> (the integration step) and the value of the threshold chosen for integration. <bold>b-c)</bold> Cross sections of a)</p></caption>
<graphic xlink:href="505598v2_figS5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Fig. S6.</label>
<caption><title>Transition to collective oscillations as a function of the average indegree K.</title>
<p>Because of the balanced-state scaling in which the connection weights scale as <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline106.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, increasing K also makes each connection weaker. <bold>(a)</bold> Inhibitory network with delays. <bold>(b)</bold> Maximum Lyapunov exponent as a function of K for different values of the delay. The critical value of the indegree at which the asynchronous irregular state loses stability to a collective oscillation is computed as in <xref rid="fig4" ref-type="fig">Figure 4</xref> of the main text, and indicated by the vertical arrows in matching colors.<bold>(c)</bold> metric entropy as a function of K for different values of the delay. <bold>(d)</bold> Attractor dimension as a function of the delay. Even if the attractor dimension decreases with K up to the oscillatory instability, it suddenly increases at the transition. All measures of chaos increase at the oscillatory transition.</p></caption>
<graphic xlink:href="505598v2_figS6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS7" position="float" fig-type="figure">
<label>Fig. S7.</label>
<caption><p><bold>(a)</bold> Non delayed E-I network in the balanced state. The asynchronous irregular state loses stability with increasing inhibitory time constant as described theoretically in (<xref ref-type="bibr" rid="sc2">2</xref>). We see that at the onset of collective oscillations, all chaotic measures increase. <bold>(b)</bold> Maximum Lyapunov exponent as a function of of the inhibitory time constant <italic>τ</italic><sub><italic>I</italic></sub> <bold>(c)</bold>. Raster plots for different values of the inhibitory time constant. <bold>(d)</bold> metric entropy as a function of <italic>τ</italic><sub><italic>I</italic></sub> <bold>(e)</bold> Attractor dimension as a function of <italic>τ</italic><sub><italic>I</italic></sub>.</p></caption>
<graphic xlink:href="505598v2_figS7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS8" position="float" fig-type="figure">
<label>Fig. S8.</label>
<caption><title>Single cell heterogeneity perpetuates chaos.</title>
<p><bold>(a)</bold> Network of inhibitory neurons with heterogeneity and raster plot of the network activity for different delays, observe the difference in the spiking pattern after the first oscillatory transition (middle bottom panel) and after the second one (bottom panel). <bold>(b)</bold> Direct computation of the maximum LE, by perturbing the trajectory numerically is compared with the method developed here (<italic>N</italic> = 5000) showing good agreement. <bold>(c)</bold> MSD and <bold>(d)</bold> Maximum LE for different network sizes as a function of the delay for an example heterogeneous network of <italic>σ</italic> = 0.5. The dashed line in <bold>(c)</bold> indicates the MSD for an homogeneous network for reference. The maximum exponent converges to a size independent value which increases with the network size, indicating that finite size network fluctuations only lead to an under-estimation of the chaotic nature of the circuit.</p></caption>
<graphic xlink:href="505598v2_figS8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS9" position="float" fig-type="figure">
<label>Fig. S9.</label>
<caption><title>Impact of heterogeneity.</title>
<p><bold>(a)</bold> Amplitude and <bold>(b)</bold> frequency of the population oscillation as a function of the delay for different values of membrane time constant heterogeneity (uniform distribution of mean 10ms and standard deviation <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline107.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>). <bold>(d)</bold> The maximum Lyapunov exponent and the <bold>(e)</bold> coefficient of variation have similar dependencies to the delay. <bold>(f)</bold> <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline108.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the ratio between the input current to the neurons needed to reach the target firing rate compared to what its predicted theoretically from the balanced state condition Eq. S3</p></caption>
<graphic xlink:href="505598v2_figS9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS10" position="float" fig-type="figure">
<label>Fig. S10.</label>
<caption><title>Robustness of the findings.</title>
<p><bold>(a-i)</bold> Robustness for different values of the mean firing rate <bold>(a)</bold> Amplitude of the time dependent population rate (also multi-unit activity, MUA) as a function of the synaptic delay <italic>δ</italic>. <bold>(b)</bold> Frequency of the oscillation vs <italic>δ</italic>. The values depicted are shown only when the MUA amplitude departed from baseline <bold>(c)</bold> Standard deviation of the firing rate distribution vs <italic>δ</italic>. (d) <inline-formula><alternatives><inline-graphic xlink:href="505598v2_inline109.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the ratio between the input current to the neurons needed to reach the target firing rate compared to what its predicted theoretically from the balanced state condition Eq. S3 This number quantifies a departure from the balanced state equation. <bold>(e)</bold> Maximum Lyapunov Exponent vs <italic>δ</italic>. (f) Coefficient of variation vs <italic>δ</italic>. (g) metric entropy vs <italic>δ</italic>. Notice that its not normalized by the firing rate and the units are then bits per second. (h) Lyapunov Dimension. Parameters: Default parameters as in <xref rid="tbl1" ref-type="table">table 1</xref> (see <italic>Methods</italic>), except for panels (g-h) where N=2000 and the changes indicated in the figure. <bold>i)</bold> Top: Dependence of the critical delay with the mean rate <italic>ν</italic>. Top-Middle: Frequency at the onset of collective oscillations as a function of <italic>ν</italic>. Middle Bottom: Maximum value of the maximum Lyapunov exponent as a function of <italic>ν</italic>. Bottom: value of the delay at which the maximum lyapunov exponent peaks vs the value of the delay at which the coefficient of variation peaks. <bold>(j-r)</bold> Robustness for different values of the connectivity strength <italic>J</italic><sub>0</sub>. Panels identical to (a-i). <bold>(s-zz)</bold> Robustness for different values of K. Panels identical to (a-i).</p></caption>
<graphic xlink:href="505598v2_figS10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS11" position="float" fig-type="figure">
<label>Fig. S11.</label>
<caption><title>Dependence of the maximum Lyapunov exponent on the intensity of the EI loop.</title>
<p><bold>(a)</bold> The maximum Lyapunov exponent is invariant under small changes in the excitation ratio parameter <italic>η</italic> (see Eq. A). <bold>(b)</bold> Different values of the feedback loop <italic>ϵ</italic> delay the transition to collective rhythms and push the peak of the exponent towards larger delays. Parameters: Default parameters except for N=5000</p></caption>
<graphic xlink:href="505598v2_figS11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS12" position="float" fig-type="figure">
<label>Fig. S12.</label>
<caption><p><bold>(a)</bold> Harmonically driven case <bold>(b)</bold> Synchronization index as a function of the frequency of the harmonic drive <italic>f</italic> <bold>(c)</bold> Entire spectrum of the LE of the driven network for different values of the frequency of the harmonic input drive (left), and a third of the spectrum for frequency of the drive <italic>f</italic> between 15 and 30 Hz. Notice how when the maximum Lyapunov exponent starts to increase, the negative exponents become more negative, resulting unexpectedly in a decrease instead of an increase of the attractor dimension for frequencies smaller than the critical one.</p></caption>
<graphic xlink:href="505598v2_figS12.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS13" position="float" fig-type="figure">
<label>Fig. S13.</label>
<caption><title>Classic rate networks behave like classic scalar delayed systems</title>
<p><bold>(a)</bold> Diagram of the small-delay approximation network. There will be 𝒪 <italic>N</italic> extra units for an approximation of order 𝒪 and a network of <italic>N</italic> neurons (see <italic>Methods</italic>) <bold>(b)</bold> Distribution of perturbation decay rates <italic>ν</italic><sub><italic>k</italic></sub>, associated to the eigenvalue <italic>λ</italic><sub><italic>k</italic></sub> of the random matrix <italic>J</italic>, for the exact delayed system (purple) and for the small delayed approximation (turquoise) of order 𝒪 = 2 (left column) or 𝒪 = 4 (right column) <bold>(c)</bold> Lyapunov spectrum of the approximate system for orders 2, 3 and 4 for an intermediate delay <bold>(d)</bold> Mean square deviation <bold>(e)</bold> Maximum Lyapunov exponent <bold>(f)</bold> Attractor dimension and <bold>(g)</bold> Metric entropy as a function of the synaptic delay. Notice that the metric entropy is in bits per second.</p></caption>
<graphic xlink:href="505598v2_figS13.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS14" position="float" fig-type="figure">
<label>Fig. S14.</label>
<caption><title>Inhibitory delayed rate networks behave like classic scalar delayed systems</title>
<p><bold>(a)</bold> Diagram of the small-delay approximation network. There will be 𝒪 <italic>N</italic> extra units for an approximation of order 𝒪 and a network of <italic>N</italic> neurons (see <italic>Methods</italic>) Both the delayed rate network and the approximated “weakly delayed” networks used here have a transition from fully developed chaos (top) to clock-like synchrony. <bold>(b)</bold> Mean square deviation from the mean is vanishingly small before the critical delay indicating asynchronous dynamics. <bold>(c)</bold> First Lyapunov exponent as a function of the delay <bold>(d)</bold> Attractor dimension <bold>(e)</bold> Metric entropy.</p></caption>
<graphic xlink:href="505598v2_figS14.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="sc1"><label>1.</label><mixed-citation publication-type="journal"><string-name><given-names>C van</given-names> <surname>Vreeswijk</surname></string-name>, <string-name><given-names>H</given-names> <surname>Sompolinsky</surname></string-name>, <article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title>. <source>Sci. (New York, N.Y</source>.) <volume>274</volume>, <fpage>1724</fpage>–<lpage>6</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="sc2"><label>2.</label><mixed-citation publication-type="journal"><string-name><given-names>C van</given-names> <surname>Vreeswijk</surname></string-name>, <string-name><given-names>H</given-names> <surname>Sompolinsky</surname></string-name>, <article-title>Chaotic balanced state in a model of cortical circuits</article-title>. <source>Neural computation</source> <volume>10</volume>, <fpage>1321</fpage>–<lpage>71</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="sc3"><label>3.</label><mixed-citation publication-type="journal"><string-name><given-names>N</given-names> <surname>Brunel</surname></string-name>, <string-name><given-names>V</given-names> <surname>Hakim</surname></string-name>, <article-title>Fast Global Oscillations in Networks of Integrate-and-Fire Neurons with Low Firing Rates</article-title>. <source>Neural Comput</source>. <volume>11</volume>, <fpage>1621</fpage>–<lpage>1671</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="sc4"><label>4.</label><mixed-citation publication-type="other"><string-name><given-names>N</given-names> <surname>Brunel</surname></string-name>, <article-title>Phase diagrams of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title>. <source>Neurocomputing</source> 32-33, <fpage>307</fpage>–<lpage>312</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="sc5"><label>5.</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Monteforte</surname></string-name>, <string-name><given-names>F</given-names> <surname>Wolf</surname></string-name>, <article-title>Dynamical entropy production in spiking neuron networks in the balanced state</article-title>. <source>Phys. Rev. Lett</source>. <volume>105</volume>, <fpage>1</fpage>–<lpage>4</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="sc6"><label>6.</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Timme</surname></string-name>, <string-name><given-names>F</given-names> <surname>Wolf</surname></string-name>, <string-name><given-names>T</given-names> <surname>Geisel</surname></string-name>, <article-title>Topological Speed Limits to Network Synchronization</article-title>. <source>Phys. Rev. Lett</source>. <volume>92</volume>, <fpage>074101</fpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="sc7"><label>7.</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Timme</surname></string-name>, <string-name><given-names>F</given-names> <surname>Wolf</surname></string-name>, <article-title>The simplest problem in the collective dynamics of neural networks: is synchrony stable?</article-title> <source>Nonlinearity</source> <volume>21</volume>, <fpage>1579</fpage>–<lpage>1599</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="sc8"><label>8.</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Timme</surname></string-name>, <string-name><given-names>F</given-names> <surname>Wolf</surname></string-name>, <string-name><given-names>T</given-names> <surname>Geisel</surname></string-name>, <article-title>Coexistence of regular and irregular dynamics in complex networks of pulse-coupled oscillators</article-title>. <source>Phys. review letters</source> <volume>89</volume>, <fpage>258701</fpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="sc9"><label>9.</label><mixed-citation publication-type="journal"><string-name><given-names>U</given-names> <surname>Ernst</surname></string-name>, <string-name><given-names>K</given-names> <surname>Pawelzik</surname></string-name>, <string-name><given-names>T</given-names> <surname>Geisel</surname></string-name>, <article-title>Synchronization induced by temporal delays in pulse-coupled oscillators</article-title>. <source>Phys. Rev. Lett</source>. <volume>74</volume>, <fpage>1570</fpage>–<lpage>1573</lpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="sc10"><label>10.</label><mixed-citation publication-type="journal"><string-name><given-names>RE</given-names> <surname>Mirollo</surname></string-name>, <string-name><given-names>SH</given-names> <surname>Strogatz</surname></string-name>, <article-title>Synchronization of pulse-coupled biological oscillators</article-title>. <source>SIAM J. on Appl. Math</source>. <volume>50</volume>, <fpage>1645</fpage>–<lpage>1662</lpage> (<year>1990</year>).</mixed-citation></ref>
<ref id="sc11"><label>11.</label><mixed-citation publication-type="journal"><string-name><given-names>MJE</given-names> <surname>Richardson</surname></string-name>, <article-title>Spike-train spectra and network response functions for non-linear integrate-and-fire neurons</article-title>. <source>Biol. Cybern</source>. <volume>99</volume>, <fpage>381</fpage>–<lpage>392</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="sc12"><label>12.</label><mixed-citation publication-type="journal"><string-name><given-names>MJE</given-names> <surname>Richardson</surname></string-name>, <article-title>Firing-rate response of linear and nonlinear integrate-and-fire neurons to modulated current-based and conductance-based synaptic drive</article-title>. <source>Phys. Rev. E - Stat. Nonlinear, Soft Matter Phys</source>. <volume>76</volume>, <fpage>1</fpage>–<lpage>15</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="sc13"><label>13.</label><mixed-citation publication-type="journal"><string-name><given-names>N</given-names> <surname>Brunel</surname></string-name>, <string-name><given-names>PE</given-names> <surname>Latham</surname></string-name>, <article-title>Firing rate of the noisy quadratic integrate-and-fire neuron</article-title>. <source>Neural computation</source> <volume>15</volume>, <fpage>2281</fpage>–<lpage>2306</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="sc14"><label>14.</label><mixed-citation publication-type="journal"><string-name><given-names>JD</given-names> <surname>Farmer</surname></string-name>, <string-name><given-names>E</given-names> <surname>Ott</surname></string-name>, <string-name><given-names>JA</given-names> <surname>Yorke</surname></string-name>, <article-title>The dimension of chaotic attractors</article-title>. <source>Phys. D: Nonlinear Phenom</source>. <volume>7</volume>, <fpage>153</fpage>–<lpage>180</lpage> (<year>1983</year>).</mixed-citation></ref>
<ref id="sc15"><label>15.</label><mixed-citation publication-type="journal"><string-name><given-names>J</given-names> <surname>Kaplan</surname></string-name>, <string-name><given-names>J</given-names> <surname>Yorke</surname></string-name>, <article-title>Chaotic behavior of multi-dimensional difference equations</article-title>. <source>H. Peitgen H. Walther</source> (eds), Funct. differential equations approximation fixed points, Lect. notes Math. Springer-Verlag <volume>730</volume>, <fpage>204</fpage>–<lpage>227</lpage> (<year>1979</year>).</mixed-citation></ref>
<ref id="sc16"><label>16.</label><mixed-citation publication-type="journal"><string-name><given-names>H</given-names> <surname>Froehling</surname></string-name>, <string-name><given-names>JP</given-names> <surname>Crutchfield</surname></string-name>, <string-name><given-names>D</given-names> <surname>Farmer</surname></string-name>, <string-name><given-names>NH</given-names> <surname>Packard</surname></string-name>, <string-name><given-names>R</given-names> <surname>Shaw</surname></string-name>, <article-title>On determining the dimension of chaotic flows</article-title>. <source>Phys. D: Nonlinear Phenom</source>. <volume>3</volume>, <fpage>605</fpage>–<lpage>617</lpage> (<year>1981</year>).</mixed-citation></ref>
<ref id="sc17"><label>17.</label><mixed-citation publication-type="book"><string-name><given-names>M</given-names> <surname>Cencini</surname></string-name>, <string-name><given-names>F</given-names> <surname>Cecconi</surname></string-name>, <string-name><given-names>A</given-names> <surname>Vulpiani</surname></string-name>, <source>Chaos: From Simple Models to Complex Systems</source>. (<publisher-name>World Scientific Publishing Co</publisher-name>.), p. <fpage>460</fpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="sc18"><label>18.</label><mixed-citation publication-type="book"><string-name><given-names>HG</given-names> <surname>Schuster</surname></string-name>, <string-name><given-names>W</given-names> <surname>Just</surname></string-name>, <source>Deterministic Chaos: An Introduction: Fourth Edition</source>. (<publisher-name>WILEY-VCH Verlag GmbH &amp; Co</publisher-name>.), pp. <fpage>1</fpage>–<lpage>287</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="sc19"><label>19.</label><mixed-citation publication-type="journal"><string-name><given-names>P</given-names> <surname>Frederickson</surname></string-name>, <string-name><given-names>JL</given-names> <surname>Kaplan</surname></string-name>, <string-name><given-names>ED</given-names> <surname>Yorke</surname></string-name>, <string-name><given-names>JA</given-names> <surname>Yorke</surname></string-name>, <article-title>The liapunov dimension of strange attractors</article-title>. <source>J. Differ. Equations</source> <volume>49</volume>, <fpage>185</fpage>–<lpage>207</lpage> (<year>1983</year>).</mixed-citation></ref>
<ref id="sc20"><label>20.</label><mixed-citation publication-type="journal"><string-name><given-names>JP</given-names> <surname>Eckmann</surname></string-name>, <string-name><given-names>D</given-names> <surname>Ruelle</surname></string-name>, <article-title>Ergodic theory of chaos and strange attractors</article-title>. <source>Rev. Mod. Phys</source>. <volume>57</volume>, <fpage>617</fpage>–<lpage>656</lpage> (<year>1985</year>).</mixed-citation></ref>
<ref id="sc21"><label>21.</label><mixed-citation publication-type="journal"><string-name><given-names>H</given-names> <surname>Sompolinsky</surname></string-name>, <string-name><given-names>A</given-names> <surname>Crisanti</surname></string-name>, <string-name><given-names>HJ</given-names> <surname>Sommers</surname></string-name>, <article-title>Chaos in random neural networks</article-title>. <source>Phys. Rev. Lett</source>. <volume>61</volume>, <fpage>259</fpage>–<lpage>262</lpage> (<year>1988</year>).</mixed-citation></ref>
<ref id="sc22"><label>22.</label><mixed-citation publication-type="journal"><string-name><given-names>VK</given-names> <surname>Jirsa</surname></string-name>, <string-name><given-names>M</given-names> <surname>Ding</surname></string-name>, <article-title>Will a large complex system with time delays be stable?</article-title> <source>Phys. Rev. Lett</source>. <volume>93</volume>, <fpage>1</fpage>–<lpage>4</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="sc23"><label>23.</label><mixed-citation publication-type="journal"><string-name><given-names>J</given-names> <surname>Feng</surname></string-name>, <string-name><given-names>VK</given-names> <surname>Jirsa</surname></string-name>, <string-name><given-names>M</given-names> <surname>Ding</surname></string-name>, <article-title>Synchronization in networks with random interactions: Theory and applications</article-title>. <source>Chaos</source> <volume>16</volume>, <fpage>1</fpage>–<lpage>21</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="sc24"><label>24.</label><mixed-citation publication-type="journal"><string-name><given-names>C</given-names> <surname>Bimbard</surname></string-name>, <string-name><given-names>E</given-names> <surname>Ledoux</surname></string-name>, <string-name><given-names>S</given-names> <surname>Ostojic</surname></string-name>, <article-title>Instability to a heterogeneous oscillatory state in randomly connected recurrent networks with delayed interactions</article-title>. <source>Phys. Rev. E</source> <volume>94</volume> (<year>2016</year>).</mixed-citation></ref>
<ref id="sc25"><label>25.</label><mixed-citation publication-type="journal"><string-name><given-names>HJ</given-names> <surname>Sommers</surname></string-name>, <string-name><given-names>A</given-names> <surname>Crisanti</surname></string-name>, <string-name><given-names>H</given-names> <surname>Sompolinsky</surname></string-name>, <string-name><given-names>Y</given-names> <surname>Stein</surname></string-name>, <article-title>Spectrum of large random asymmetric matrices</article-title>. <source>Phys. Rev. Lett</source>. <volume>60</volume>, <fpage>1895</fpage>–<lpage>1898</lpage> (<year>1988</year>).</mixed-citation></ref>
<ref id="sc26"><label>26.</label><mixed-citation publication-type="journal"><string-name><given-names>J</given-names> <surname>Kadmon</surname></string-name>, <string-name><given-names>H</given-names> <surname>Sompolinsky</surname></string-name>, <article-title>Transition to chaos in random neuronal networks</article-title>. <source>Phys. Rev. X</source> <volume>5</volume>, <fpage>1</fpage>–<lpage>28</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="sc27"><label>27.</label><mixed-citation publication-type="journal"><string-name><given-names>R</given-names> <surname>Engelken</surname></string-name>, <string-name><given-names>F</given-names> <surname>Wolf</surname></string-name>, <string-name><given-names>LF</given-names> <surname>Abbott</surname></string-name>, <article-title>Lyapunov spectra of chaotic recurrent neural networks</article-title>. <source>bioarXiv</source> <volume>2</volume> (<year>2020</year>).</mixed-citation></ref>
<ref id="sc28"><label>28.</label><mixed-citation publication-type="journal"><string-name><given-names>GB</given-names> <surname>Ermentrout</surname></string-name>, <string-name><given-names>N</given-names> <surname>Kopell</surname></string-name>, <article-title>Parabolic Bursting in an Excitable System Coupled with a Slow Oscillation</article-title>. <source>SIAM J. on Appl. Math</source>. <volume>46</volume>, <fpage>233</fpage>–<lpage>253</lpage> (<year>1986</year>).</mixed-citation></ref>
</ref-list>
<fn-group>
<fn id="fn1">
<label><sup>†</sup></label>
<p>This means that the tangent spaces are never co-linear, there is a finite angle between them, and the tangent space can be written as <italic>TM</italic> = <italic>E</italic><sup><italic>u</italic></sup> ⊕ <italic>E</italic><sup><italic>s</italic></sup></p>
</fn>
</fn-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90378.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Gjorgjieva</surname>
<given-names>Julijana</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Technical University of Munich</institution>
</institution-wrap>
<city>Freising</city>
<country>Germany</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This work provides a <bold>valuable</bold> characterization of the chaotic dynamics of high-dimensional spiking networks in the presence of internally generated oscillations due to synaptic delays or externally generated oscillations due to external input. The authors provide <bold>convincing</bold> analytical and numerical calculations to support their claims, however, the paper suffers from heavy mathematical jargon that reduces its impact. The paper could be revised to provide interpretations of the results so that it can be accessible to a broader neuroscience audience. In its current form, findings will be of interest mostly to researchers working at the interface between theoretical neuroscience, applied mathematics, and physics.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90378.1.sa2</article-id>
<title-group>
<article-title>None</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>**Reviewer #1 (Public Review):</p>
<p>
**</p>
<p>
Summary:</p>
<p>
Cortical activity displays high trial-to-trial variability and oscillatory transients. These dynamical features have implications for how information is encoded and transmitted in the brain. While trial-to-trial variability has been widely studied, via mathematical models, in asynchronous dynamical states, works investigating variability in synchronous states are more sparse. In this study, the authors characterise the nature of the chaotic attractor underlying neural activity at the onset of oscillations induced by transmission delays. They find that variability is boosted by delay-induced oscillations in comparison to the asynchronous state.</p>
<p>Strengths:</p>
<p>1. Quantifying the chaotic nature of high-dimensional neural activity is a hard mathematical challenge. This work builds upon prior theoretical work to study how spike chaos is affected by oscillatory mean activity, a phenomenon frequently observed in the cortex.</p>
<p>2. The evidence supporting all findings appears to be highly robust.</p>
<p>3. The manuscript is well written.</p>
<p>Weaknesses:</p>
<p>1. The core contribution of the paper is a description of chaotic activity as delays are increased (Fig. 2). Within the main text, it is noted that two instabilities leading to oscillatory activity emerge. However, the definition and nature of these two transitions lack some clarity. In particular, whether the two transitions are &quot;real&quot; (meaning that they separate three distinct regimes of activity), or whether they rather correspond to different measures of the same underlying instability, remains opaque.</p>
<p>2. While the mathematical aspects of the analysis are discussed in detail, the biological implications of the findings remain rather less clear. In particular, a discussion regarding the implications of the findings for cortical coding is missing. Furthermore, while the authors have put forth efforts to contextualize their findings within the domain of the dynamical systems and applied math literature, the relationship with the corresponding neuroscience literature seems less developed.</p>
<p>3. The connection with biology is also hindered by the fact that measures used to characterise trial-to-trial variability (metric entropy and Kaplan-Yorke dimension) significantly differ from those commonly used in the analysis of experimental data, and these measures are not contextualized within the manuscript.</p>
<p>4. The text comprises a significant amount of undefined mathematical jargon.</p>
<p>5. For the purpose of the mathematical analysis, the original delayed model is substituted with an effectively delayed version. The authors convincingly demonstrate an alignment between the outcomes from the two models. This alignment appears to be unaffected by variations in the reset parameter of the effective model (Fig. S2). Nonetheless, a systematic discussion on the efficacy and limitations of this replacement approach seems absent. Under what circumstances are the two models equivalent? Conversely, when does their correspondence become very poor?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90378.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
The authors investigate the effect of oscillatory activity on the chaotic dynamics of high-dimensional networks. The network oscillations are internally generated by synaptic delays which are known to produce oscillations. The authors demonstrate that the intensity of the chaos and the dimension of the chaotic attractor picks at a delay value. A similar effect is found when an external input drives the network. In this case, these quantities pick at the network's resonant frequency. This shows that the intensity of the chaotic dynamics can be boosted by internally or externally generated oscillations.</p>
<p>Strengths:</p>
<p>
The paper is technically solid. They introduce a novel method to perform calculations of the Lyapunov spectrum in networks with delays, which have infinite dimensions, effectively transforming it into a network of finite dimensions. The conclusions of the paper are supported by strong analytical calculations and novel and intensive numerical methods.</p>
<p>Weaknesses:</p>
<p>
The main weakness is that is difficult to find the relevance of the paper's findings to neuroscience. It is not clear to me that measures such as the rate of production of entropy of a chaotic attractor in spiking networks, its dimension, and its Lyapunov spectra are experimentally relevant. Moreover, the authors make little to no attempt to provide interpretations for these quantities nor put their work in a broader context in the field of systems neuroscience. The paper also is written in an overly technical way with sometimes the use of technical jargon which might be difficult to follow for a non-expert in mean field theories and statistical physics.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90378.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
In this work, the authors propose a novel method for analyzing spiking neuron network models with delays. By modeling the delay as an additional axonal component to relay spikes, the infinite-dimensional system of the delayed network is transformed into a system of finite dimensions. This allows the calculation of the entire spectrum of Lyapunov exponents which provide information on the dimensionality of attractor and noise entropy of network responses. The authors demonstrate that chaos intensifies at the onset of oscillations as synaptic delay increases. This is surprising since network oscillation has been thought to indicate regular firing activity. The authors find similar results in different types of networks and in networks driven by oscillatory inputs, suggesting that the boosting of chaos by oscillation can be a general feature of spiking networks.</p>
<p>Strengths:</p>
<p>
This work builds on the authors' past work on characterizing chaos in spiking networks and extends to include synaptic delays. The transformation of a delayed network into a network of two-compartment neurons, modeling the spike generation and transmission, is novel and interesting. This allows for an analytical expression of the single spike Jacobian of the network dynamics, which can be used to calculate the full spectrum of Lyapunov exponents.</p>
<p>The analysis is rigorous and the parameter study is comprehensive.</p>
<p>Weaknesses:</p>
<p>
Because the delayed interaction is spike-triggered, effectively it only requires N variables to count the remaining time since the last spike from each neuron. The axon component only implements the delay time to transmit a spike with no interaction with other neurons. It seems that the axon component can be simply modeled as a variable counting the time since the last spike and does not need to be modeled as a QIF model. Is there any advantage of modeling the axon component as a QIF model? The supplemental figure S2 considers the case of &quot;dynamic delay&quot;, where delay time can depend on network activity, but the Lyapunov exponents seem to be largely independent of the reset parameter.</p>
<p>In most of the results, the network mean firing rate is kept at a fixed value while the delay time parameter varies. What would be the results if only the delay parameter changes? It would be helpful if the authors could provide some reasoning as to why it is a better comparison with the network rate kept as a constant.</p>
<p>The majority of the neurons have a CV below 1 (Fig 2d and Fig S3c). This indicates that many neurons are in the mean-driven regime. This is different from balanced networks where CVs are around 1. It would be helpful for the authors to comment on this discrepancy.</p>
</body>
</sub-article>
</article>