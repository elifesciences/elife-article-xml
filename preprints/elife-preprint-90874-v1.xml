<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">90874</article-id>
<article-id pub-id-type="doi">10.7554/eLife.90874</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.90874.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Jointly looking to the past and the future in visual working memory</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7857-4456</contrib-id>
<name>
<surname>Liu</surname>
<given-names>Baiwei</given-names>
</name>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Alexopoulou</surname>
<given-names>Zampeta-Sofia</given-names>
</name>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7434-1751</contrib-id>
<name>
<surname>van Ede</surname>
<given-names>Freek</given-names>
</name>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><institution>Institute for Brain and Behavior Amsterdam, Department of Experimental and Applied Psychology, Vrije Universiteit Amsterdam</institution>, <country>The Netherlands</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Spering</surname>
<given-names>Miriam</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>The University of British Columbia</institution>
</institution-wrap>
<city>Vancouver</city>
<country>Canada</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Correspondence: Baiwei Liu (<email>b.liu@vu.nl</email>), Freek van Ede (<email>freek.van.ede@vu.nl</email>)</corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-11-28">
<day>28</day>
<month>11</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP90874</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-07-29">
<day>29</day>
<month>07</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-08-02">
<day>02</day>
<month>08</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.01.30.526235"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Liu et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Liu et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-90874-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Working memory enables us to bridge past sensory information to upcoming future behaviour. Accordingly, by its very nature, working memory is concerned with two components: the past and the future. Yet, in conventional laboratory tasks, these two components are often conflated, such as when sensory information in working memory is encoded and tested at the same location. We developed a task in which we dissociated the past (encoded location) and future (to-be-tested location) attributes of visual contents in working memory. This enabled us to independently track the utilisation of past and future memory attributes through gaze, as observed during mnemonic selection. Our results reveal the joint consideration of past and future locations. This was prevalent even at the single-trial level of individual saccades that were jointly biased to the past and future. This uncovers the rich nature of working memory representations, whereby both past and future memory attributes are retained and can be accessed together when memory contents become relevant for behaviour.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Working memory is a fundamental cognitive function that enables us to hold onto past sensory information in service of upcoming future behaviour <sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref></sup>. Accordingly, by its very nature, working memory is concerned with two components: the past and the future.</p>
<p>In conventional laboratory tasks, past and future components are often conflated such as when sensory information in working memory is tested at the same location as where it was encoded. By contrast, in the dynamic situations we face every day, sensory information often disappears at one specific location (where it enters visual working memory) but becomes relevant at another location (as also in <sup><xref ref-type="bibr" rid="c3">3</xref>–<xref ref-type="bibr" rid="c5">5</xref></sup>). Imagine trying to capture a photograph of a precious bird species that you just saw disappear behind a building. Your working memory of the bird is likely to consider not only where you last saw the bird, but also where you expect it to re-appear to capture it on camera. Such situations raise an interesting, underexplored question: when past and future locations of memory contents are not the same, does the brain code internal representations with regard to past, future, or both?</p>
<p>To address this question, we developed a task in which we dissociated the encoding (past) and to-be-tested (future) locations associated with visual representations in working memory. This enabled us to experimentally isolate past and future memory attributes and to track their respective utilisation through spatial biases in gaze behaviour in healthy human volunteers.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>Twenty-five human volunteers performed a working-memory task in which visual memory items were encoded and tested at different locations (<bold><xref ref-type="fig" rid="fig1">Fig. 1a</xref></bold>). Participants memorised two coloured gratings with different orientations presented either vertically or horizontally. The crucial manipulation was that we always tested memory content in the orthogonal axis and at a predictable location (depending on the future rule that we varied across sessions; see <bold><xref ref-type="fig" rid="figS1">Supplementary Fig. 1</xref></bold> for the four possible rules).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Directional gaze biases by past and future locations during mnemonic selection overlap in time.</title>
<p><bold>a)</bold> Task schematic. Participants memorised two oriented gratings with different colours presented either vertically or horizontally. Following a delay, a colour change of the central fixation dot prompted participants to select the colour-matching item from working memory to report its orientation later. After another delay, two test gratings appeared transiently and participants compared the cued memory grating to the relevant test grating (clockwise/counter-clockwise judgment) that was determined by the ‘future rule’ that was stable within each block. Dash lines serve to explain the association between the encoding and test locations and were never presented in the actual experiment. <bold>b-c)</bold> Time courses of gaze shift rates (number of saccades per second) for shifts toward and away from the encoded (panel b) and to-be-tested (panel c) locations. <bold>d)</bold> Overlays and comparisons of the difference in gaze-shift rates (toward minus away) for the past (encoded) location and the future (to-be-tested) location. Horizontal lines indicate significant temporal cluster (cluster-based permutation test, P &lt; 0.001). Data are presented as mean values with shading reflecting 95% confidence intervals, calculated across participants (n = 25).</p></caption>
<graphic xlink:href="526235v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>After a delay period, we cued the relevant memory item by changing the colour of the central fixation dot. At this stage, participants were required to select the colour-matching grating from working memory in order to compare it to the upcoming test stimulus (clockwise/counter-clockwise judgement). Crucially, we always presented two stimuli at the test-phase of which only one was relevant, as determined by the future rule. For example, in <bold><xref ref-type="fig" rid="fig1">Figure 1a</xref></bold>, after the green memory item is cued, the relevant test stimulus will be the right stimulus, given the future rule (top item tested on right) in this session.</p>
<p>Participants were able to perform this dynamic visual working-memory task, with an average accuracy of 70±2 percent correct (mean ± SEM) and an average reaction time of 1218 ± 125 ms.</p>
<sec id="s2a">
<title>Gaze reveals the use of both past and future memory attributes that are considered at overlapping time windows</title>
<p>To track the utilisation of past and/or future locations associated with working memory contents, we tracked spatial biases in gaze following the cue to select either memory item. Specifically, we focused on directional biases in saccades, that we have previously shown to be sensitive to selective spatial attention <sup><xref ref-type="bibr" rid="c6">6</xref>–<xref ref-type="bibr" rid="c8">8</xref></sup>, even when directed internally <sup><xref ref-type="bibr" rid="c9">9</xref>–<xref ref-type="bibr" rid="c11">11</xref></sup>.</p>
<p>As shown in <bold><xref ref-type="fig" rid="fig1">Figure 1b</xref></bold>, after cue onset, saccades became biased in the direction of the encoded (past) location of the selected memorized target, as demonstrated by significantly more gaze shifts toward vs. away from the encoded location of the target (<bold><xref ref-type="fig" rid="fig1">Fig. 1b</xref></bold>; cluster P &lt; 0.001), starting from around 200 ms after the cue. This is consistent with our prior demonstrations of directional eye-movement biases within the spatial lay-out of working memory <sup><xref ref-type="bibr" rid="c9">9</xref>–<xref ref-type="bibr" rid="c11">11</xref></sup>. In our current task, this uniquely reveals how the retention of items in working memory continues to rely on their past encoded locations, even if items are known to become relevant (tested) at another location.</p>
<p>Having established that past (encoded) memory locations were still utilised by participants in our task (despite us never asking about memory-item locations), an interesting question becomes when the future memory attribute (test location) would be considered after the selection cue. Intuitively, participants may select the relevant item at its past location, before considering the relevant future test location – which would yield a serial pattern of past-before-future. In contrast, as shown in <bold><xref ref-type="fig" rid="fig1">Figure 1c</xref></bold> and <bold><xref ref-type="fig" rid="fig1">d</xref></bold>, we found a similar saccade bias to the relevant future location (<bold><xref ref-type="fig" rid="fig1">Fig. 1c</xref></bold>; cluster P &lt; 0.001). Strikingly, this future bias also emerged early after the cue. An overlay of the spatial biases (toward vs. away) in the orthogonally manipulated past and future axes (<bold><xref ref-type="fig" rid="fig1">Fig. 1d</xref></bold>), revealed consideration of past and future locations at overlapping time windows. Gaze biases in both axes were driven predominantly by microsaccades (<bold><xref ref-type="fig" rid="figS2">Supplementary Fig. 2</xref></bold>) and occurred similarly in horizontal-to-vertical and vertical-to-horizontal trials (<bold><xref ref-type="fig" rid="figS3">Supplementary Fig. 3</xref></bold>). These data thus suggest the joint consideration – or “activation” – of past and future memory attributes, at least when analysing past and future memory attributes separately. Below we provide additional single-trial (single-saccade) evidence for this interpretation.</p>
</sec>
<sec id="s2b">
<title>Individual saccades reveal truly joint consideration of past and future memory attributes</title>
<p>In principle, the observed joint activation of the past and future locations associated with the cued memory content in the trial-averaged and participant-averaged data could result from two alternative scenarios with different interpretations. First, either the past or the future alone may be considered in different trials and/or participants, without past and future memory attributes ever being considered together. Alternatively, participants may truly consider both past and future memory attributes jointly at the single-trial, single-saccade level.</p>
<p>While it is notoriously hard to disentangle the single-trial interpretation of averaged data <sup><xref ref-type="bibr" rid="c12">12</xref></sup>, we were here able to do so by interrogating the individual saccade characteristics for which the two alternative scenarios make different predictions. In the first scenario, saccades should be biased to either the past or the future location, but there should be no dependency between them (i.e. a saccade may be biased to the past location regardless of the future location, and vice versa). In contrast, in the second scenario, with truly joint consideration of the past and future, there should be a clear dependency: it should be those saccades that are biased to the past that are also biased to the future. In other words, the future-biased saccades should predominantly be driven by the past-biased saccades, and vice versa.</p>
<p>To disentangle these alternatives at the single-trial level of individual saccades, we focused on the first saccades after the cue, in the 200-600 ms window. We previously identified this windows as the relevant window for microsaccade biases by internal selective attention <sup><xref ref-type="bibr" rid="c9">9</xref></sup>, and this is also where we observed the overlapping past and future biases in the average here (see <bold><xref ref-type="fig" rid="fig1">Figure 1d</xref></bold>). To facilitate visualisation and quantification, we rotated all detected saccades to match a consistent coordinate frame, in which the past location is represented horizontally (right=towards, left=away), and the future location vertically (top=toward, bottom=away; see <bold><xref ref-type="fig" rid="fig2">Fig. 2a</xref></bold>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Individual saccades are jointly biased to past and future memory attributes.</title>
<p><bold>a)</bold> The distribution of the direction of the first saccades we detected after cue onset relative to past (horizontal) and future (vertical) locations. Data from the different sessions were rotated to match a common coordinate frame. <bold>b)</bold> The bias toward the past (x-axis) as a function of saccade direction with regard to the future (y-axis). <bold>c)</bold> The bias toward the future (y-axis) as a function of saccade direction with regard to the past (x-axis). In <bold>b-c</bold>, The bold black line indicates the significant temporal cluster (cluster-based permutation test, P &lt; 0.001). Data are presented as mean values with shading indicating 95% confidence intervals, calculated across participants (n = 25). <bold>d)</bold> The percentage of identified first saccades toward or away from the future location as a function of whether the same saccades were also biased toward or away from the past location. Error bars in panel d indicate ±1 SEM calculated across participants (n = 25).</p></caption>
<graphic xlink:href="526235v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>As shown in <bold><xref ref-type="fig" rid="fig2">Figure 2a</xref></bold>, the majority of first-detected microsaccades in our window of interest were made toward the past (right &gt; left) and toward the future (top &gt; bottom), replicating our prior analyses. Critically, this visualisation and quantification enabled us to disentangle the two alterative sketched above in which past and future saccades at the single-trial (individual-saccade) level were either independent (past bias regardless of future and future bias regardless of past) or dependent (joint past and future bias). Our data supported the latter.</p>
<p><bold><xref ref-type="fig" rid="fig2">Figure 2b</xref></bold> shows the bias toward the past as a function of saccade direction with regard to the future. Likewise, <bold><xref ref-type="fig" rid="fig2">Figure 2c</xref></bold>, shows the future bias as a function of whether saccades were also biased to the past. In both cases, we see a clear dependency: the past bias is particularly pronounced for saccades that also have a future bias (<bold><xref ref-type="fig" rid="fig2">Fig. 2b</xref></bold>) and, vice versa, the future bias is most pronounced for saccades that also have a past bias (<bold><xref ref-type="fig" rid="fig2">Fig 2c</xref></bold>). This is perhaps best appreciated by the binarised quantification of these same data in <bold><xref ref-type="fig" rid="fig2">Figure 2d</xref></bold>: showing the percentage of identified first saccades toward or away from the future location, as a function of whether the same saccades were also biased toward or away from the past location. We found a clear interaction (F(1, 24) = 18.1, P &lt; 0.001, partial η<sup><xref ref-type="bibr" rid="c2">2</xref></sup> = 0.43), whereby the bias toward the future location was exclusively observed for those saccades that were also biased toward the past location. Indeed, follow-up t-tests revealed no difference future bias for saccades that were away from the past (t(24) = 1.13, P<sub>Bonferroni</sub> = 1, d = 0.23), but a clear future effect for saccades that were toward the past (t(24) = 4.65, P<sub>Bonferroni</sub> &lt; 0.001, d = 0.93). This would not be expected if single saccades cared exclusively about either the past or the future location. Instead, this provides singletrial-level support with the truly joint consideration of past and future memory attributes.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Here, we brought the study of visual working memory into a dynamic context by experimentally dissociating (orthogonalising) the past (last-seen location) and future (to-be-tested location) attributes of visual memory contents, and independently tracking the utilisation of these two attributes through gaze. Doing so, we unveil a novel, fundamental property of working memory – the joint availability and utilisation of past and future memory attributes. As such, our data provide key support for the proposal that memory is fundamentally future-oriented <sup><xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c14">14</xref></sup>, while also reminding us that past memory attributes are not forgotten, even when these do not become relevant again.</p>
<p>Our finding of joint utilisation of past and future memory attributes emerged from at least two alternative scenarios of how the brain may deal with dynamic everyday working memory demands in which memory content is encoded at one location but needed at another. First, memory contents could have directly been remapped (cf. <sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c15">15</xref>–<xref ref-type="bibr" rid="c17">17</xref></sup>) to their future-relevant location (in which case we should have only found a future gaze bias). Second, when dealing with multiple memory contents, contents could be stored at the past location at first and the future location could be considered only after relevant memory content has been selected (in which case the past bias should have <italic>preceded</italic> the future bias). In contrast, our data suggest that the brain simultaneously retains the copy of both past and futurerelevant locations in working memory, and (re)activates each during mnemonic selection.</p>
<p>By capitalising on the discrete nature of saccades, we were able to demonstrate the truly joint consideration of past and future attributes, at the single-trial level. As such we were able to bypass a fundamental challenge of disentangling multiple potential single-trial interpretations when only having trial-average data available (for related discussions, see <sup><xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c19">19</xref></sup>). For example, when only considering the temporally overlapping past and future signals at the trial-average level, it was impossible to tell whether these joint effects resulted from a mix of trials and/or participants that relied on either the past <italic>or</italic> the future. By considering the discrete events – the individual saccades at the single-trial level – we could demonstrate how biases to the past and the future co-existed at the single-saccade level, supporting a truly joint consideration of past and future.</p>
<p>In current study, we tracked spatial attention using microsaccades. Compared to the commonly used online indicator of spatial attention, such as electrophysiological measures, microsaccades have important features that make them a promising complementary tool for uncovering the mechanisms of spatial attention <sup><xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c20">20</xref></sup>, including when directed internally as we have shown here. First, as we have discussed above, microsaccades are discrete events, allowing to track spatial attention at the singletrial level. Second, microsaccades are not limited to tracking spatial attention toward the left or right visual field, as electrophysiology indicators often are. These two aspects were paramount to our demonstration of joint single-trial consideration of past and future memory attributes, and are likely to open additional doors for future investigations.</p>
<p>While the past gaze bias that we report here replicates our own prior studies <sup><xref ref-type="bibr" rid="c9">9</xref>–<xref ref-type="bibr" rid="c11">11</xref></sup>, here we for the first time demonstrate a similar bias to the future-relevant memory location that is similarly driven by microsaccades. This signal may reflect either of two situations: the selection of a future-copy of the cued memory content or anticipatory attention to its the anticipated location of its associated teststimulus. Either way, by the nature of our experimental design, this future signal is memory-content specific, as the two memory contents were always associated with opposite testing locations. Accordingly, our data reveal how this future feature can be accessed from memory together with the specific content that it is associated with, implying joint storage and utilisation of past and future memory attributes.</p>
<p>Our data complement other recent studies investigating visual working memory in more dynamic contexts <sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c21">21</xref>–<xref ref-type="bibr" rid="c24">24</xref></sup>, and showcase the rich nature of working memory representations. For example, akin to our finding of joint consideration of past and future locations in working memory, recent studies have uncovered the joint consideration of allocentric and egocentric spatial frames <sup><xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c25">25</xref></sup>. While storing memory content at a single location (or with reference to a single frame) would appear more intuitive and efficient, our data reveal an intriguing alternative. We speculate that the joint retention of multiple spatial attributes may make memories more robust, as well as more flexible for serving continuously evolving demands during everyday behaviour.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Ethics</title>
<p>Experimental procedures were reviewed and approved by the local Ethics Committee at the Vrije Universiteit Amsterdam. Each participant provided written consent before participation and was reimbursed €10/hour.</p>
</sec>
<sec id="s4b">
<title>Participants</title>
<p>Twenty-five healthy human volunteers participated in the study (age range: 20-27; 10 male and 15 female; 23 right-handed; 10 corrected-to-normal vision: 5 glasses and 5 lenses). Sample size of 25 was determined a-priori based on previous publications from the lab with similar experimental designs and that relied on the same outcome measure <sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c26">26</xref></sup>). To achieve the intended sample size, three participants were replaced due to chance-level performance.</p>
</sec>
<sec id="s4c">
<title>Stimuli and procedure</title>
<p>Participants performed a visual working memory task in which we orthogonalised the encoding and to-be-tested location in order to track the utilisation of past and future working-memory attributes.</p>
<p>Participants were required to encode and maintain two visual items (tilted coloured gratings) in working memory to later report one of their orientations (<bold><xref ref-type="fig" rid="fig1">Fig. 1a</xref></bold>). Each trial began with a brief (250 ms) encoding display in which two to-be-memorised gratings with different colours and orientations appeared vertically or horizontally on either side of the fixation dot, at 4 degrees visual angle. After a retention delay of 1250 ms, the fixation dot changed colour for 1000 ms. This colour change served as a 100% valid retro-cue, prompting participants to select the colour-matching target memory item. The retro-cue was followed by another retention delay of 500 ms, before the test display. The test display always contained two black gratings with different orientations that were presented vertically or horizontally on either side of the fixation dot (again at 4 visual degrees to each side). Based on a rule that described in the following paragraph, one of the black gratings was relevant to the task (the test grating), while the other merely served as a filler. After seeing the test display, participants were required to compare the cued memory grating to the relevant test grating and report whether the memory grating should be turned clockwise or counterclockwise rotated to match the relevant test grating in the test display.</p>
<p>In the encoding display, the gratings were randomly assigned two distinct colours: green (RGB: 133, 194, 18) and purple (RGB: 197, 21, 234) and two distinct orientations ranging from 0° to 180° with a minimum difference of 20° between each other. During the test display, the gratings are always black (RGB: 64, 64, 64). The relevant grating was always rotated 20 degrees in either a clockwise or counterclockwise direction compared to the to-be-tested memory grating. The orientation of the irrelevant grating in the test display was chosen randomly.</p>
<p>The unique element of our task was that we dissociated the encoding and testing location by presenting the gratings in the test display on the orthogonal axis as where the items were presented on the encoding display. Relevant examples can be found in <bold><xref ref-type="fig" rid="fig1">Figure 1a</xref></bold> and <bold><xref ref-type="fig" rid="figS1">Supplementary Figure 1</xref></bold>. For example, if the two memory items appeared vertically at the top and bottom at encoding, the test gratings would appear horizontally to the left and right in the test display (see <bold><xref ref-type="fig" rid="fig1">Fig. 1a</xref></bold>).</p>
<p>To to-be-tested location was always linked to the encoded location by virtue of a future rule. For counterbalancing reasons, we used four unique rules that were presented across four sessions (<bold><xref ref-type="fig" rid="figS1">Supplementary Fig. 1</xref></bold>) and that always remained stable within a session. In rule 1: the two memory items were encoded vertically on the top and bottom. If the top memory item was cued, the relevant test grating would be on the right, while if the bottom item would be cued, the relevant test grating would be on the left in the test display. In rule 2, the mapping was reversed: the two memory items would again be encoded vertically, but this time if the top memory item would be cued the relevant test grating is on the left, while if the bottom item is cued the relevant test grating is on the right. Rules 3 and 4 follow the same logic, except that now the memory items are presented horizontally, and the test gratings vertically. Before every session, participants were notified of the future rule that applied to the upcoming session.</p>
<p>To ensure the use of the future rule, we made it task relevant by always presenting two test gratings in the test display of which only one was relevant: the one that matched the future rule (i.e., without applying the future rule, one would not know which test grating was to be used). It is for this reason that we consider the future location an attribute of the memory items, as each memory item was associated with its own unique test location.</p>
<p>In total, the study consisted of 4 sessions, that each contained 5 blocks of 32 trials each. At the start of each session, participants were notified of the session-specific future rule and then practiced the task with the current rule for 16 trials before starting the formal session. We did not include practice trials in our analyses. The study lasted approximately 70 minutes per participant.</p>
</sec>
<sec id="s4d">
<title>Eye-tracking acquisition and pre-processing</title>
<p>Gaze was tracked from a single eye (right eye in all participants except 1 for which the left eye provided a better signal) using an EyeLink 1000 (SR Research) at a sampling rate of 1000 Hz. The eye tracker camara was positioned on the table ~5 cm in front of the monitor and ~65 cm away from the eyes. Gaze position was tracked continuously along the horizontal and vertical axes. Before recording, the built-in calibration and validation protocols from the EyeLink software were used to calibrate the eye tracker.</p>
<p>After recording, the eye-tracking data were converted from the original .edf to the .asc format and analysed in Matlab using the Fieldtrip analysis toolbox <sup><xref ref-type="bibr" rid="c27">27</xref></sup> in combination with custom code. Blinks were marked by detecting NaN clusters in the eye-tracking data. All data from 100 ms before to 100 ms after the detected NaN clusters were also set to NaN to eliminate residual blink artefacts. Finally, data were epoched from −1000 to +2000 ms relative to after the onset of the retro-cue.</p>
</sec>
<sec id="s4e">
<title>Gaze-shift detection</title>
<p>We focused our analysis on spatial biases in gaze shifts (saccades/microsaccades). To identify gaze shifts, we employed a velocity-based method that we established in our prior studies <sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup>, that builds on other velocity-based detection methods (e.g. <sup><xref ref-type="bibr" rid="c6">6</xref></sup>). First, gaze velocity was calculated by taking the Euclidean distance between temporally successive gaze-position values in the 2-dimensional plane (horizontal and vertical gaze position). Velocity was smoothed with a Gaussian-weighted moving average filter with a 7-ms sliding window (using the built-in function “smoothdata” in MATLAB). When the velocity exceeded a trial-based threshold of 5 times the median velocity, we marked the first sample after the threshold crossing as the onset of a saccade. To avoid counting the same saccade multiple times, a minimum delay of 100 ms between successive saccades was imposed. Saccade magnitude and direction were calculated by estimating the difference between pre-saccade gaze position (−50 to 0 ms before saccade onset) vs. the post-saccade gaze position (50 to 100 ms after saccade onset).</p>
<p>To focus our analysis on (micro)saccades that were driven by attention, we here focused on the “start microsaccade” defined as the saccade that moves gaze away from the fixation dot (as opposed to saccades that bring gaze back to fixation). We extracted start saccades using a method that we recently validated in another study <sup><xref ref-type="bibr" rid="c28">28</xref></sup>. For each detected saccade, we estimated the pre- and post-saccade distance from the central fixation dot. If the post-saccade distance was larger than the pre-saccade distance, we defined the saccade as a start microsaccade. To minimise the contribution of gaze drift, we defined gaze positions associated with looking at the central fixation dot using the median gaze position in the fixation period from [−0.8 to −0.2 ms] relative to cue onset.</p>
<p>Time courses of gaze-shift rates (in Hz) were quantified using a sliding time window of 50 ms, advanced in steps of 1 ms. We additionally decomposed shift-rates into a time-magnitude representation (as in <sup><xref ref-type="bibr" rid="c9">9</xref></sup>), showing the time-resolved rate of attention-driven shifts (toward vs. away) per second, as a function of the saccade size (see <bold><xref ref-type="fig" rid="figS2">Supplementary Fig. 2</xref>)</bold>. For magnitude sorting, we used successive magnitude bins of 0.2 visual degree in steps of 0.04 visual degree.</p>
</sec>
<sec id="s4f">
<title>Individual shift level analysis</title>
<p>For our analysis at the single-trial, single-saccade level, we focused on the first start saccade observed during the 200-600 ms post-cue period that we have previously identified as the relevant time window for the effect of interest <sup><xref ref-type="bibr" rid="c9">9</xref></sup>. For all ‘first saccades’, we then looked at the spatial distribution of saccade directions relative to the relevant past (encoding) and future (to-be-tested) locations associated with the cued memory item. To facilitate visualisation and quantification, we rotated all detected saccades to a common coordinate system, in which the past location was represented horizontally (left=away, right=toward), and the future location vertically (top=toward, bottom=away).</p>
<p>To obtain the proportional distribution of saccades in this coordinate system (see <bold><xref ref-type="fig" rid="fig2">Fig. 2A</xref></bold>), we used successive angular bins of 20 degrees in steps of 1 angle degree and simply calculated the proportion of saccades in each angular bin (with angles being defined with reference to past and future locations).</p>
<p>To quantify whether saccades were jointly biased to the past and future (as opposed to either alone), we decomposed these distributions into the bias toward the past (toward vs. away from past) as a function of saccade direction with regard to the future (<bold><xref ref-type="fig" rid="fig2">Fig. 2b</xref></bold>) and the bias toward the future (toward vs. away from future) as a function of saccade direction with regard to the past (<bold><xref ref-type="fig" rid="fig2">Fig. 2c</xref></bold>). Finally, we binarised saccades as either going toward or away from the past and toward or away from the future (<bold><xref ref-type="fig" rid="fig2">Fig. 2d</xref></bold>). This enabled us to test whether saccades were jointly biased to the past and the future (which would predict an interaction whereby those same saccades that were biased to the past were also biased to the future).</p>
</sec>
<sec id="s4g">
<title>Statistical analysis</title>
<p>To evaluate the reliability of patterns in our gaze data, we used a cluster-based permutation approach <sup><xref ref-type="bibr" rid="c29">29</xref></sup>. This method is ideal for evaluating patterns at multiple neighbouring points while circumventing the problem of multiple comparisons. We used this approach for all evaluations involving a series of data, such as along a time axis (<bold><xref ref-type="fig" rid="fig1">Fig. 1b-d</xref></bold>; <bold><xref ref-type="fig" rid="fig2">Fig. 2b-c</xref></bold>).</p>
<p>To create a permutation distribution, we randomly permuted the trial-average data at the participant level 10000 times and identified the largest clusters found at each time. The p-values of the clusters in the original data were calculated as the proportion of permutations for which the size of the largest cluster after permutation was larger than the size of the observed cluster in the original, non-permuted data. We created the permutation distribution using Fieldtrip with default cluster settings (grouping adjacent same-signed data points that were significant in a mass univariate t-test at a two-sided alpha level of 0.05 and defining cluster size as the sum of all t-values in a cluster).</p>
<p>In addition, statistical evaluation for the single-trial, single-saccade analysis was performed on the binarised quantification of saccade proportions using a two-way repeated-measures ANOVA with the factors past (toward/away) and future (toward/away). ANOVA results were complemented with Bonferroni-corrected post-hoc t-tests. For measures of effect size we used Partial eta squared (for our ANOVA) and Cohen’s d (for follow-up t-tests). P-values of follow-up t-tests were Bonferroni corrected.</p>
</sec>
</sec>
</body>
<back>
<sec id="d1e699">
<title>Data availability</title>
<p>All data will be made publicly available before publication.</p>
</sec>
<sec id="d1e706">
<title>Code availability</title>
<p>Relevant code associated with the here-presented analyses will be made available through GitHub before publication.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>D’Esposito</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Postle</surname>, <given-names>B. R.</given-names></string-name> <article-title>The Cognitive Neuroscience of Working Memory</article-title>. <source>Annu. Rev. Psychol</source>. <volume>66</volume>, <fpage>115</fpage>–<lpage>142</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="other"><string-name><surname>van Ede</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name> <article-title>Turning Attention Inside Out: How Working Memory Serves Behavior</article-title>. <source>Annu. Rev. Psychol</source>. (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.1146/annurev-psych-021422-041757</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Brincat</surname>, <given-names>S. L.</given-names></string-name> <etal>et al.</etal> <article-title>Interhemispheric transfer of working memories</article-title>. <source>Neuron</source> <volume>109</volume>, <fpage>1055</fpage>–<lpage>1066</lpage>.<page-range>e4</page-range> (<year>2021</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Doherty</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Rao</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mesulam</surname>, <given-names>M. M.</given-names></string-name> &amp; <string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name> <article-title>Synergistic Effect of Combined Temporal and Spatial Expectations on Visual Attention</article-title>. <source>J. Neurosci</source>. <volume>25</volume>, <fpage>8259</fpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Woodman</surname>, <given-names>G. F.</given-names></string-name>, <string-name><surname>Vogel</surname>, <given-names>E. K.</given-names></string-name> &amp; <string-name><surname>Luck</surname>, <given-names>S. J.</given-names></string-name> <article-title>Flexibility in visual working memory: Accurate change detection in the face of irrelevant variations in position</article-title>. <source>Vis. cogn</source>. <volume>20</volume>, <fpage>1</fpage>–<lpage>28</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Engbert</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Kliegl</surname>, <given-names>R.</given-names></string-name> <article-title>Microsaccades uncover the orientation of covert attention</article-title>. <source>Vision Res</source>. <volume>43</volume>, <fpage>1035</fpage>–<lpage>1045</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Hafed</surname>, <given-names>Z. M.</given-names></string-name> &amp; <string-name><surname>Clark</surname>, <given-names>J. J.</given-names></string-name> <article-title>Microsaccades as an overt measure of covert attention shifts</article-title>. <source>Vision Res</source>. <volume>42</volume>, <fpage>2533</fpage>–<lpage>2545</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Rolfs</surname>, <given-names>M.</given-names></string-name> <article-title>Microsaccades: Small steps on a long way</article-title>. <source>Vision Res</source>. <volume>49</volume>, <fpage>2415</fpage>–<lpage>2441</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Liu</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name> &amp; <string-name><surname>van Ede</surname>, <given-names>F.</given-names></string-name> <article-title>Functional but not obligatory link between microsaccades and neural modulation by covert spatial attention</article-title>. <source>Nat. Commun</source>. <volume>13</volume>, <fpage>3503</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>van Ede</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Deden</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name> <article-title>Looking ahead in working memory to guide sequential behaviour</article-title>. <source>Curr. Biol</source>. <volume>31</volume>, <fpage>R779</fpage>–<lpage>R780</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>van Ede</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Chekroud</surname>, <given-names>S. R.</given-names></string-name> &amp; <string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name> <article-title>Human gaze tracks attentional focusing in memorized visual space</article-title>. <source>Nat. Hum. Behav</source>. <volume>3</volume>, <fpage>462</fpage>–<lpage>470</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Stokes</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Spaak</surname>, <given-names>E.</given-names></string-name> <article-title>The Importance of Single-Trial Analyses in Cognitive Neuroscience</article-title>. <source>Trends Cogn. Sci</source>. <volume>20</volume>, <fpage>483</fpage>–<lpage>486</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name> &amp; <string-name><surname>Stokes</surname>, <given-names>M. G.</given-names></string-name> <article-title>Premembering Experience: A Hierarchy of Time-Scales for Proactive Attention</article-title>. <source>Neuron</source> <volume>104</volume>, <fpage>132</fpage>–<lpage>146</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Schacter</surname>, <given-names>D. L.</given-names></string-name>, <string-name><surname>Addis</surname>, <given-names>D. R.</given-names></string-name> &amp; <string-name><surname>Buckner</surname>, <given-names>R. L.</given-names></string-name> <article-title>Remembering the past to imagine the future: the prospective brain</article-title>. <source>Nat. Rev. Neurosci</source>. <volume>8</volume>, <fpage>657</fpage>–<lpage>661</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Duhamel</surname>, <given-names>J.-R.</given-names></string-name>, <string-name><surname>Colby</surname>, <given-names>C. L.</given-names></string-name> &amp; <string-name><surname>Goldberg</surname>, <given-names>M. E.</given-names></string-name> <article-title>The Updating of the Representation of Visual Space in Parietal Cortex by Intended Eye Movements</article-title>. <source>Science</source> <volume>255</volume>, <fpage>90</fpage>–<lpage>92</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="other"><string-name><surname>He</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Ekman</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Vandenbroucke</surname>, <given-names>A. R. E.</given-names></string-name> &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name> <article-title>Visual working memory representations in visual and parietal cortex do not remap after eye movements</article-title>. <source>bioRxiv</source> <fpage>747329</fpage> (<year>2020</year>) doi:<pub-id pub-id-type="doi">10.1101/747329</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Rolfs</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Jonikaitis</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Deubel</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Cavanagh</surname>, <given-names>P.</given-names></string-name> <article-title>Predictive remapping of attention across eye movements</article-title>. <source>Nat. Neurosci</source>. <volume>14</volume>, <fpage>252</fpage>–<lpage>256</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Jones</surname>, <given-names>S. R.</given-names></string-name> <article-title>When brain rhythms aren’t ‘rhythmic’: implication for their mechanisms and meaning</article-title>. <source>Curr. Opin. Neurobiol</source>. <volume>40</volume>, <fpage>72</fpage>–<lpage>80</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name> &amp; van <string-name><surname>Ede</surname>, <given-names>F.</given-names></string-name> <article-title>Under the Mind’s Hood: What We Have Learned by Watching the Brain at Work</article-title>. <source>J. Neurosci</source>. <volume>40</volume>, <fpage>89</fpage>–<lpage>100</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="book"><string-name><surname>Engbert</surname>, <given-names>R.</given-names></string-name> <chapter-title>Microsaccades: a microcosm for research on oculomotor control, attention, and visual perception</chapter-title>. in <source>Progress in Brain Research</source> (eds. <person-group person-group-type="editor"><string-name><surname>Martinez-Conde</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Macknik</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Martinez</surname>, <given-names>L. M.</given-names></string-name>, <string-name><surname>Alonso</surname>, <given-names>J.-M.</given-names></string-name> &amp; <string-name><surname>Tse</surname>, <given-names>P. U.</given-names></string-name></person-group>) vol. <volume>154</volume> <fpage>177</fpage>–<lpage>192</lpage> (<publisher-name>Elsevier</publisher-name>, <year>2006</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="web"><string-name><surname>Chung</surname>, <given-names>Y. H.</given-names></string-name>, <string-name><surname>Schurgin</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Brady</surname>, <given-names>T.</given-names></string-name> <source>The role of motion in visual working memory for dynamic stimuli: more lagged but more precise representations of moving objects</source>. (<year>2022</year>) doi:<pub-id pub-id-type="doi">10.31234/osf.io/cu3zg</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>de Vries</surname>, <given-names>I. E. J.</given-names></string-name>, <string-name><surname>Slagter</surname>, <given-names>H. A.</given-names></string-name> &amp; <string-name><surname>Olivers</surname>, <given-names>C. N. L.</given-names></string-name> <article-title>Oscillatory Control over Representational States in Working Memory</article-title>. <source>Trends Cogn. Sci</source>. <volume>24</volume>, <fpage>150</fpage>–<lpage>162</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Draschkow</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name> &amp; <string-name><surname>van Ede</surname>, <given-names>F.</given-names></string-name> <article-title>Multiple spatial frames for immersive working memory</article-title>. <source>Nat. Hum. Behav</source>. <volume>6</volume>, <fpage>536</fpage>–<lpage>544</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Xie</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal> <article-title>Geometry of sequence working memory in macaque prefrontal cortex</article-title>. <source>Science</source> <volume>375</volume>, <fpage>632</fpage>–<lpage>639</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Fiehler</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Wolf</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Klinghammer</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Blohm</surname>, <given-names>G.</given-names></string-name> <article-title>Integration of egocentric and allocentric information during memory-guided reaching to images of a natural environment</article-title>. <source>Front. Hum. Neurosci</source>. <volume>8</volume>, (<year>2014</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Ede</surname>, <given-names>F. van</given-names></string-name>, <string-name><surname>Board</surname>, <given-names>A. G.</given-names></string-name> &amp; <string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name> <article-title>Goal-directed and stimulus-driven selection of internal representations</article-title>. <source>Proc National Acad Sci</source> <volume>117</volume>, <fpage>24590</fpage>–<lpage>24598</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fries</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Schoffelen</surname>, <given-names>J.-M.</given-names></string-name> <article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title>. <source>Comput. Intel. Neurosc</source>. <volume>2011</volume>, <fpage>156869</fpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="other"><string-name><surname>Liu</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name> &amp; <string-name><surname>van Ede</surname>, <given-names>F.</given-names></string-name> <article-title>Microsaccades transiently lateralise EEG alpha activity</article-title>. <source>bioRxiv</source> 2022.09.02.506318 (<year>2022</year>) doi:<pub-id pub-id-type="doi">10.1101/2022.09.02.506318</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name> <article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title>. <source>J. Neurosci. Methods</source> <volume>164</volume>, <fpage>177</fpage>–<lpage>190</lpage> (<year>2007</year>).</mixed-citation></ref>
</ref-list>
<ack>
<title>Acknowledgements</title>
<p>This research was supported by an ERC Starting Grant from the European Research Council (MEMTICIPATION, 850636) and an NWO Vidi grant by the Dutch Research Council (grant number 14721) to F.v.E.</p>
</ack>
<sec id="d1e1603">
<title>Author contributions</title>
<p>B.L and F.v.E designed and programmed the experiments, B.L. and S.A acquired the data. B.L, S.A, and F.v.E analysed and interpreted the data. B.L and F.v.E drafted and revised the manuscript.</p>
</sec>
<sec id="d1e1611">
<title>Competing interests statement</title>
<p>The authors declare no competing interests.</p>
</sec>
<sec id="d1e1618">
<title>Supplementary Information – <xref ref-type="fig" rid="figS1">Figures S1</xref>–<xref ref-type="fig" rid="figS3">S3</xref></title>
<fig id="figS1" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 1.</label>
<caption><title>Four possible associations between encoding and test locations.</title></caption>
<graphic xlink:href="526235v1_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 2.</label>
<caption><title>Early saccade biases by past and future memory attributes are predominantly driven by microsaccades.</title>
<p>Difference in gaze-shift rates toward minus away relative to past location (panel <bold>a</bold>) or future location (panel <bold>b</bold>), as a function of saccade size (y axes). For reference, dashed horizontal lines indicate 1° visual angle. Additionally, for each panel, we separately show the difference in gaze-shift rates (toward minus away) in time course at the top (collapsed over all depicted saccade sizes) and the difference in number of gaze-shift as a function of gaze-shift magnitude to the right (collapsed over all depicted times).</p></caption>
<graphic xlink:href="526235v1_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 3.</label>
<caption><title>Trials with vertical or horizontal configurations show similar joint consideration of past and future memory attributes.</title>
<p>Conventions as in Main <xref ref-type="fig" rid="fig1">Figure 1b-d</xref>, separately for trials that encoded items horizontally and tested items vertically (panel <bold>a</bold>) and trials that encoded items vertically and tested items horizontally (panel <bold>b</bold>).</p></caption>
<graphic xlink:href="526235v1_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90874.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Spering</surname>
<given-names>Miriam</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>The University of British Columbia</institution>
</institution-wrap>
<city>Vancouver</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study advances our understanding of how past and future information is jointly considered in visual working memory by studying gaze biases in a memory task that dissociates the locations during encoding and memory tests. The evidence supporting the conclusions is <bold>convincing</bold>, with state-of-the-art gaze analyses that build on a recent series of experiments introduced by the authors. This work, with further improvements incorporating the existing literature, will be of broad interest to vision scientists interested in the interplay of vision, eye movements, and memory.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90874.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this study, the authors offer a fresh perspective on how visual working memory operates. They delve into the link between anticipating future events and retaining previous visual information in memory. To achieve this, the authors build upon their recent series of experiments that investigated the interplay between gaze biases and visual working memory. In this study, they introduce an innovative twist to their fundamental task. Specifically, they disentangle the location where information is initially stored from the location where it will be tested in the future. Participants are tasked with learning a novel rule that dictates how the initial storage location relates to the eventual test location. The authors leverage participants' gaze patterns as an indicator of memory selection. Intriguingly, they observe that microsaccades are directed toward both the past encoding location and the anticipated future test location. This observation is noteworthy for several reasons. Firstly, participants' gaze is biased towards the past encoding location, even though that location lacks relevance to the memory test. Secondly, there's a simultaneous occurrence of an increased gaze bias towards both the past and future locations. To explore this temporal aspect further, the authors conduct a compelling analysis that reveals the joint consideration of past and future locations during memory maintenance. Notably, microsaccades biased towards the future test location also exhibit a bias towards the past encoding location. In summary, the authors present an innovative perspective on the adaptable nature of visual working memory. They illustrate how information relevant to the future is integrated with past information to guide behavior.</p>
<p>This short manuscript presents one experiment with straightforward analyses, clear visualizations, and a convincing interpretation. For their analysis, the authors focus on a single time window in the experimental trial (i.e., 0-1000 ms after retro cue onset). While this time window is most straightforward for the purpose of their study, other time windows are similarly interesting for characterizing the joint consideration of past and future information in memory. First, assessing the gaze biases in the delay period following the cue offset would allow the authors to determine whether the gaze bias towards the future location is sustained throughout the entire interval before the memory test onset. Presumably, the gaze bias towards the past location may not resurface during this delay period, but it is unclear how the bias towards the future location develops in that time window. Also, the disappearance of the retro cue constitutes a visual transient that may leave traces on the gaze biases which speaks again for assessing gaze biases also in the delay period following the cue offset.</p>
<p>Moreover, assessing the gaze bias before retro-cue onset allows the authors to further characterize the observed gaze biases in their study. More specifically, the authors could determine whether the future location is considered already during memory encoding and the subsequent delay period (i.e., before the onset of the retro cue). In a trial, participants encode two oriented gratings presented at opposite locations. The future rule indicates the test locations relative to the encoding locations. In their example (Figure 1a), the test locations are shifted clockwise relative to the encoding location. Thus, there are two pairs of relevant locations (each pair consists of one stimulus location and one potential test location) facing each other at opposite locations and therefore forming an axis (in the illustration the axis would go from bottom left to top right). As the future rule is already known to the participants before trial onset it is possible that participants use that information already during encoding. This could be tested by assessing whether more microsaccades are directed along the relevant axis as compared to the orthogonal axis. The authors should assess whether such a gaze bias exists already before retro cue onset and discuss the theoretical consequences for their main conclusions (e.g., is the future location only jointly used if the test location is implicitly revealed by the retro cue).</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90874.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
The manuscript by Liu et al. reports a task that is designed to examine the extent to which &quot;past&quot; and &quot;future&quot; information is encoded in working memory that combines a retro cue with rules that indicate the location of an upcoming test probe. An analysis of microsaccades on a fine temporal scale shows the extent to which shifts of attention track the location of the location of the encoded item (past) and the location of the future item (test probe). The location of the encoded grating of the test probe was always on orthogonal axes (horizontal, vertical) so that biases in microsaccades could be used to track shifts of attention to one or the other axis (or mixtures of the two). The overall goal here was then to (1) create a methodology that could tease apart memory for the past and future, respectively, (2) to look at the time-course attention to past/future, and (3) to test the extent to which microsaccades might jointly encode past and future memoranda. Finally, some remarks are made about the plausibility of various accounts of working memory encoding/maintenance based on the examination of these time courses.</p>
<p>Strengths:</p>
<p>
This research has several notable strengths. It has a clear statement of its aims, is lucidly presented, and uses a clever experimental design that neatly orthogonalizes &quot;past&quot; and &quot;future&quot; as operationalized by the authors. Figure 1b-d shows fairly clearly that saccade directions have an early peak (around 300ms) for the past and a &quot;ramping&quot; up of saccades moving in the forward direction. This seems to be a nice demonstration the method can measure shifts of attention at a fine temporal resolution and differentiate past from future-oriented saccades due to the orthogonal cue approach. The second analysis shown in Figure 2, reveals a dependency in saccade direction such that saccades toward the probe future were more likely also to be toward the encoded location than away from the encoded direction. This suggests saccades are jointly biased by both locations &quot;in memory&quot;.</p>
<p>Weaknesses:</p>
<p>
1. The &quot;central contribution&quot; (as the authors characterize it) is that &quot;the brain simultaneously retains the copy of both past and future-relevant locations in working memory, and (re)activates each during mnemonic selection&quot;, and that: &quot;... while it is not surprising that the future location is considered, it is far less trivial that both past and future attributes would be retained and (re)activated together. This is our central contribution.&quot; However, to succeed at the task, participants must retain the content (grating orientation, past) and probe location (future) in working memory during the delay period. It is true that the location of the grating is functionally irrelevant once the cue is shown, but if we assume that features of a visual object are bound in memory, it is not surprising that location information of the encoded object would bias processing as indicated by microsaccades. Here the authors claim that joint representation of past and future is &quot;far less trivial&quot;, this needs to be evaluaed from the standpoint of prior empirical data on memory decay in such circumstances, or some reference to the time-course of the &quot;unbinding&quot; of features in an encoded object.</p>
<p>2. The authors refer to &quot;future&quot; and &quot;past&quot; information in working memory and this makes sense at a surface level. However, once the retrocue is revealed, the &quot;rule&quot; is retrieved from long-term memory, and the feature (e.g. right/left, top/bottom) is maintained in memory like any other item representation. Consider the classic test of digit span. The digits are presented and then recalled. Are the digits of the past or future? The authors might say that one cannot know, because past and future are perfectly confounded. An alternative view is that some information in working memory is relevant and some is irrelevant. In the digit span task, all the digits are relevant. Relevant information is relevant precisely because it is thought be necessary in the future. Irrelevant information is irrelevant precisely because it is not thought to be needed in the immediate future. In the current study, the orientation of the grating is relevant, but its location is irrelevant; and the location of the test probe is also relevant.</p>
<p>3. It is not clear how the authors interpret the &quot;joint representation&quot; of past and future. Put aside &quot;future&quot; and &quot;past&quot; for a moment. If there are two elements in memory, both of which are associated with spatial bindings, the attentional focus might be a spatial average of the associated spatial indices. One might also view this as an interference effect, such that the location of the encoded location attracts spatial attention since it has not been fully deleted/removed from working memory. Again, for the impact of the encoded location to be exactly zero after the retrieval cue, requires zero interference or instantaneous decay of the bound location information. It would be helpful for the authors to expand their discussion to further explain how the results fit within a broader theoretical framework and how it fits with empirical data on how quickly an irrelevant feature of an object can be deleted from working memory.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90874.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study utilizes saccade metrics to explore, what the authors term the &quot;past and future&quot; of working memory. The study features an original design: in each trial, two pairs of stimuli are presented, first a vertical pair and then a horizontal one. Between these two pairs comes the cue that points the participant to one target of the first pair and another of the second pair. The task is to compare the two cued targets. The design is novel and original but it can be split into two known tasks - the first is a classic working memory task (a post-cue informs participants which of two memorized items is the target), which the authors have used before; and the second is a classic spatial attention task (a pre-cue signal that attention should be oriented left or right), which was used by numerous other studies in the past. The combination of these two tasks in one design is novel and important, as it enables the examination of the dynamics and overlapping processes of these tasks, and this has a lot of merit. However, each task separately is not new. There are quite a few studies on working memory and microsaccades and many on spatial attention and microsaccades. I am concerned that the interpretation of &quot;past vs. future&quot; could mislead readers to think that this is a new field of research, when in fact it is the (nice) extension of an existing one. Since there are so many studies that examined pre-cues and post-cues relative to microsaccades, I expected the interpretation here to rely more heavily on the existing knowledge base in this field. I believe this would have provided a better context of these findings, which are not only on &quot;past&quot; vs. &quot;future&quot; but also on &quot;working memory&quot; vs. &quot;spatial attention&quot;.</p>
</body>
</sub-article>
</article>