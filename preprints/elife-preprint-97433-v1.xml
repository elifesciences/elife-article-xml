<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">97433</article-id>
<article-id pub-id-type="doi">10.7554/eLife.97433</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.97433.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>behaviorMate: An <italic>Intranet</italic> of Things Approach for Adaptable Control of Behavioral and Navigation-Based Experiments</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0498-5743</contrib-id>
<name>
<surname>Bowler</surname>
<given-names>John C</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="author-notes" rid="n1">+</xref>
<xref ref-type="corresp" rid="cor1">†</xref>
</contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0005-0035-6749</contrib-id>
<name>
<surname>Zakka</surname>
<given-names>George</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="author-notes" rid="n1">+</xref>
<xref ref-type="corresp" rid="cor1">†</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1841-6317</contrib-id>
<name>
<surname>Yong</surname>
<given-names>Hyun Choong</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Wenke</given-names>
</name>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2280-0119</contrib-id>
<name>
<surname>Rao</surname>
<given-names>Bovey</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Liao</surname>
<given-names>Zhenrui</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Priestley</surname>
<given-names>James B</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7064-0252</contrib-id>
<name>
<surname>Losonczy</surname>
<given-names>Attila</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="corresp" rid="cor1">†</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Neuroscience</institution></aff>
<aff id="a2"><label>2</label><institution>Doctoral Program in Neurobiology and Behavior</institution></aff>
<aff id="a3"><label>3</label><institution>Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University</institution>, New York, NY 10027 <country>USA</country></aff>
<aff id="a4"><label>4</label><institution>Department of Neurobiology University of Utah, Salt Lake City</institution>, UT 84112, <country>USA</country></aff>
<aff id="a5"><label>5</label><institution>Brain Mind Institute, École polytechnique fédérale de Lausanne</institution></aff>
<aff id="a6"><label>6</label><institution>Aquabyte, San Francisco</institution>, CA 94111</aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Kemere</surname>
<given-names>Caleb</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Rice University</institution>
</institution-wrap>
<city>Houston</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Colgin</surname>
<given-names>Laura L</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Texas at Austin</institution>
</institution-wrap>
<city>Austin</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>+</label><p>indicates equal contribution</p></fn>
<corresp id="cor1"><label>†</label>Correspondence should be addressed to: A.L.: <email>al2856@columbia.edu</email>, J.C.B.: <email>jack.bowler@utah.edu</email>, G.Z.: <email>gz2333@columbia.edu</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-07-11">
<day>11</day>
<month>07</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP97433</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-04-05">
<day>05</day>
<month>04</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-04-05">
<day>05</day>
<month>04</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.12.04.569989"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Bowler et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Bowler et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-97433-v1.pdf"/>
<abstract>
<label>1</label>
<title>Abstract</title>
<p>Investigators conducting behavioral experiments often need precise control over the timing of the delivery of stimuli to subjects and to collect the precise times of the subsequent behavioral responses. Furthermore, investigators want fine-tuned control over how various multi-modal cues are presented. behaviorMate takes an “Intranet of Things” approach, using a networked system of hardware and software components for achieving these goals. The system outputs a file with integrated timestamp-event pairs that investigators can then format and process using their own analysis pipelines. We present an overview of the electronic components and GUI application that make up behaviorMate as well as mechanical designs for compatible experimental rigs to provide the reader with the ability to set up their own system. A wide variety of paradigms are supported, including goal-oriented learning, random foraging, and context switching. We demonstrate behaviorMate’s utility and reliability with a range of use cases from several published studies and benchmark tests. Finally, we present experimental validation demonstrating different modalities of hippocampal place field studies. Both treadmill with burlap belt and virtual reality with running wheel paradigms were performed to confirm the efficacy and flexibility of the approach. Previous solutions rely on proprietary systems that may have large upfront costs or present frameworks that require customized software to be developed. behaviorMate uses open-source software and a flexible configuration system to mitigate both concerns. behaviorMate has a proven record for head-fixed imaging experiments and could be easily adopted for task control in a variety of experimental situations.</p>
</abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>added direct URLs from the supplemental materials page in the manuscript to important resources including the behaviorMate website, key settings files, electronics schematics and CAD files.</p></fn>
</fn-group>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://www.losonczylab.org/behaviorMate/">https://www.losonczylab.org/behaviorMate/</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>2</label><title>Introduction</title>
<p>In this work, we present <italic>behaviorMate</italic>, an open-source system of modular components that communicate with each other over a local area network (LAN), orchestrated by a custom Java application. We validate our approach through benchmarking and experimental results, demonstrating the ability of behaviorMate to control head-fixed navigational experiments both in a self-powered treadmill as well as in a fully visual virtual reality (VR) setup with a running wheel. Depending on the nature of the investigation, this solution permits researchers to select the configuration of cues they wish to present at run-time, resulting in a system that is capable of adapting quickly to incorporate changes to experimental protocols. This design makes it easy to adapt to shifting demands of research on the same behavioral setups while producing a consistent, easy-to-parse output file describing the experiment.</p>
<p>Proprietary systems such as National Instrument’s Data Acquisition Systems (DAQ) and their accompanying software controller, LabView, typically have large upfront and licensing costs for each experimental rig their system is deployed on. behaviorMate exclusively uses open-source software and is simple to construct with parts that are significantly cheaper. The customized electronics may be ordered as printed circuit boards (PCB) and assembled by hand or purchased preassembled. The latter approach saves assembly time while adding significant expense; however, this can be mitigated by bulk ordering. The total cost of one of our VR rigs including 5 computer monitors, Android computers to render the visual scene, a frame for holding the monitors, and a light-weight running wheel was approximately $3700. The total cost for a belt-based treadmill system is around $1700. For two-photon imaging, additional components may be needed, such as a light-tight and laser resistant enclosure, which which were not factored into the calculation. While other open-source and modular behavioral control systems have been developed (<xref ref-type="bibr" rid="c3">Akam et al. 2022</xref>), behaviorMate does not require the user to write software code and uses compiled Arduino programs that do not incur the overhead of code interpreters to maximize performance.</p>
<p>Many experiments rely on using head-restrained animals to study the neuronal circuits underlying complex behaviors; such immobilization poses a fundamental challenge to behavioral research as it stymies animals’ natural movement through their environments. In order to address this issue, a plethora of “treadmill” systems have been developed relying on belt-based systems (<xref ref-type="bibr" rid="c48">Royer et al. 2012</xref>, <xref ref-type="bibr" rid="c38">Lovett-Barron et al. 2014</xref>, <xref ref-type="bibr" rid="c32">Jordan et al. 2021</xref>), VR (<xref ref-type="bibr" rid="c14">Dombeck et al. 2010</xref>), or augmented reality (AR), where experimenters move “spatial” cues to create the illusion of locomotion without actual movement in physical space (<xref ref-type="bibr" rid="c31">Jayakumar et al. 2019</xref>). Belt-based systems involve placing an animal on a physical belt treadmill where, as the animal runs, physical cues affixed to the belt provide spatial information. VR- and AR-systems operate under closed loop control, where the environment responds to the animal’s behavior (<xref ref-type="bibr" rid="c15">Dombeck &amp; Reiser 2012</xref>). Normally, these cues are visually displayed on one or more computer monitors, but olfactory and auditory stimuli have also been utilized at regular virtual distance intervals to enrich the experience with more salient cues. Furthermore, belt-based and VR/AR-based elements have been combined to enrich experiences and enable more complex behavioral tasks. At times, traditional “open-loop” stimulus may be required, such as timed cue presentations, allowing pre- and post-event neuronal activity to be examined. Increasingly, studies combine open- and closed-loop behavioral paradigms to identify stimulus-response profiles or behavioral state driven changes to stimulus responses. The proliferation of various experimental paradigms demands a high level of flexibility in experimental setups, so that methods can be integrated within single experiments or across lines of research to interrogate neuronal function.</p>
<p>Many of the spatial representations observed in freely moving animals are conserved in head-restrained animal setups (<xref ref-type="bibr" rid="c4">Aronov &amp; Tank 2014</xref>, <xref ref-type="bibr" rid="c14">Dombeck et al. 2010</xref>). Observations within the medial temporal lobe, focused on all subregions of the hippocampus as well as the entorhinal cortex, have shown that prominent neural correlates of navigation behaviors exist in both the treadmill belt style systems as well as in the visual only VR systems (<xref ref-type="bibr" rid="c4">Aronov &amp; Tank 2014</xref>, <xref ref-type="bibr" rid="c14">Dombeck et al. 2010</xref>, <xref ref-type="bibr" rid="c48">Royer et al. 2012</xref>, <xref ref-type="bibr" rid="c12">Danielson et al. 2016</xref>). In CA1 (<xref ref-type="bibr" rid="c13">Danielson et al. 2017</xref>, <xref ref-type="bibr" rid="c58">Zaremba et al. 2017</xref>) and CA3 (<xref ref-type="bibr" rid="c52">Terada et al. 2022</xref>), place cells, goal-related tuning, and other feature-selective tuning have been observed. Additionally, in visual VR systems, grid cell-like tuning has been observed in the medial entorhinal cortex (MEC), as was distance tuning (<xref ref-type="bibr" rid="c4">Aronov &amp; Tank 2014</xref>, <xref ref-type="bibr" rid="c30">Heys et al. 2014</xref>). The increased control over reward distribution and cue manipulation that VR systems afford has been pivotal for recent findings relating to how cells in CA1 form place fields, uncovering novel mechanisms underlying synaptic plasticity (<xref ref-type="bibr" rid="c25">Gonzalez et al. 2023</xref>, <xref ref-type="bibr" rid="c7">Bittner et al. 2017</xref>, <xref ref-type="bibr" rid="c44">Priestley et al. 2022</xref>), demonstrating the power of these systems to test specific theoretical predictions.</p>
</sec>
<sec id="s2">
<label>3</label><title>Overview</title>
<p>behaviorMate is an integrated system composed of several sub-components that communicate on LAN using JSON-formatted packets transmitted via a standard Datagram packet protocol (UDP) (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). This approach allows for modularity and encapsulation of different concerns. We dub the resulting system as an “<italic>Intranet</italic> of Things” approach, mimicking the Internet of Things systems commonly found in modern “smart homes,” which communicate exclusively on a dedicated LAN using statically assigned internet protocol (IP) addresses. This allows for a central user interface (UI) or “real-time” controller to seamlessly send and receive messages from various accessory downstream devices that have dedicated simple functions. Importantly, the techniques we describe are possible implementations of the behaviorMate system. In our experimental setups, we use behaviorMate to combine <italic>in vivo</italic> head-fixed mouse navigation experiments with two-photon (2p) microscopy, focusing on one-dimensional (1D) spatial navigation tasks (<xref rid="fig3" ref-type="fig">Fig. 3E</xref>). Due to its modular design, however, behaviorMate can be easily reconfigured to provide closed-loop or open-loop control during a variety of behavioral paradigms (and has been used for non-navigation based studies; such as the head-fixed trace fear conditioning task described in Ahmed et al. has been implemented on this system).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Overview of the behaviorMate system with optional components</title>
<p>Diagram shows the modular components behaviorMate can interact with. The colored arrows show the direction of information flow. For example, the Position Tracking module only receives position updates. The Visual Display module sends data to the Display Controllers to render the scene. Devices using GPIO may both send and receive data to and and from behaviorMate.</p></caption>
<graphic xlink:href="569989v4_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In addition to running a PC-based UI, we combine Arduino open-source micro controllers with custom designed printed circuit boards (PCBs) to run the experiments (<xref rid="fig1" ref-type="fig">Fig. 1</xref>, <xref rid="fig2" ref-type="fig">2D, E</xref> <italic>also see</italic> Supplementary Materials). The circuits include Ethernet adapters allowing the Arduinos to communicate on the network as well as connectors that interface them to sensors or actuators for controlling the experiments or reporting the behavioral state back to the UI. We outline a general-purpose input/output (GPIO) circuit that can connect to a variety of sensors and reward valves generally using a transistor-transistor logic (TTL) pulse/square wave as well as I<sup>2</sup>C and serial peripheral interface (SPI) protocols (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>), a position tracking circuit (<xref rid="fig2" ref-type="fig">Fig. 2E</xref>) that is dedicated mainly to reading updates of the animal’s location from a rotary encoder, and a VR controller setup which can be used to provide a more immersive visual display to animals during the behaviors. Importantly these individual components can be swapped out or added to with increased experimental demands. For additional integration of software components or testing, the UI can be configured to send messages to the PC’s <monospace>localhost</monospace> address (i.e. 127.0.0.1 on most PCs). To speed up development, we created virtual implementations of the electronics in Python code to test UI changes before testing them on a physical system. Most modern programming languages, including Python, have built-in support for JSON processing and UDP networking making component and new feature testing easy with the help of utility scripts that can be written in users’ preferred environments (<italic>see</italic> Supplementary Materials for references to example Python code).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Details of UI function.</title>
<p>(<bold>A</bold>) Screenshot of the UI. The interface provides a snapshot of the animal’s current status (<italic>center</italic>) as well as cumulative information about the current experiment (<italic>upper-left</italic>). The control panel along the left side provide: <italic>Top.</italic> an input for a project name and mouse id, which control where the resulting behavior file will be saved (these boxes are disabled when an experiment is currently running), <italic>Middle.</italic> controls to trigger the GPIO to send a TTL pulse i.e. to issue a water reward or turn on an LED, and <italic>Bottom.</italic> controls to start/stop experiments as well as to load settings files. (<bold>B</bold>) Details of UI event loop. The UI is continuously executing an event loop on every update step which checks for messages, writes the behavior file and updates the display. (<bold>C</bold>) JSON formatted message passing. The PC sends and receives JSON formatted messages via UDP to active components of the system. Messages have a nested structure to allow for internal routing and subsequent actions to be taken. (<bold>D</bold>) Rendering of the GPIO circuit used. <italic>Left.</italic> JST headers connected to I/O pins and pull-down resisters to allow for easy connections with peripheral devices that can be activated by TTL pulses (both at 3.3V and 12V, depending on the pins used). Additionally, power connections and a BNC output trigger are provided. <italic>Right.</italic> This board attaches to an Arduino Due microcontroller. (<bold>E</bold>) Updates to the position have a dedicated circuit for decoding quadrature information and sending updates to the PC. Connections for a quadrature based rotary encoder, power, and Ethernet are provided. Additionally, an input for a “Lap Reset” sensor is provided and headers to interface with an Arduni Mini.</p></caption>
<graphic xlink:href="569989v4_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Lastly, since our main focus is on 1D navigational tasks for two-photon imaging, we provide two designs for physical rigs, which hold animals fixed in place under a microscope. However, they could also be adapted to any other methods for <italic>in-vivo</italic> recording. One system is a self-powered, belt-based treadmill system, wherein animals are placed on a fabric belt that they pull beneath them to advance their virtual position. Locations and contexts can be distinguished in this setup by having various fabrics and physical cues. Alternatively, the other system provides navigational contexts through a series of computer displays for a purely visual experience, but allowing for additional flexibility of within-task manipulations to context and cues. We describe the details of both of these systems and, additionally, note that with the following methods, it is possible to combine aspects from both belt-based and VR systems to meet novel experimental designs. To clarify, for the remainder of the paper, “treadmill” will refer to these belt-based systems while “VR” will refer to the visual-based system in which animals moving a plastic running wheel.</p>
</sec>
<sec id="s3">
<label>4</label><title>The User Interface</title>
<p>The central hub for running experiments is the behaviorMate user interface (UI) (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>). The UI collects input from various sensors, assembling information about virtual position and behavior state data asynchronously. The UI performs three main functions: (1) accepting user input and displaying the current state of experiments to ensure proper functioning, (2) writing hardware information in a time-stamped streaming text file which can be processed later for analysis of the experiment, and (3) serving as the central communications hub for sending and receiving UDP-formatted messages to the downstream components. The system is event-driven such that the hardware components send messages to the UI when the state changes rather than the UI continuously polling. There are currently two versions of the UI. The first is a long-term support (LTS) release that evolved alongside the experimental systems and has been used for data collection for numerous published studies; the second is a streamlined beta version that has been rewritten to take advantage of the JavaFX visual development toolbox. Java was chosen for the UI because it is cross-platform in that it can be run on Windows, Linux, and Mac, and supports standard networking protocols. For consistency, interchangeability, and space optimization, we run the UI on an Intel(TM) NUC mini PC with a 2.4GHz base-frequency (up to 4.2GHz based on processor load) 4-core i5-1135G7 processor, 16GB DDR4-3200MHz RAM, and a 512GB NVMe M.2 SSD hard drive running Window 10 Professional Edition, although many other configurations are supported. Notably, due to the system’s modular architecture, different UIs could be implemented in any programming language and swapped in without impacting the rest of the system. The only requirement for interchangeability of the UI is to implement sending and receiving of the same JSON formatted UDP messages packets to the downstream hardware components.</p>
<p>The UI executes in three distinct phases during run-time. First a setup routine is executed, then the program enters the main event loop, and finally a termination routine is executed when an experimental trial is completed (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). The main event loop begins executing as soon as the UI is loaded, but additional steps for checking and logging the contexts’ state are added during the execution of a trial. Devices still receive input and deliver output to the UI even when a trial is not currently running, allowing experimenters to confirm that everything is working before starting the trial. Experiments using behaviorMate are configured by user-generated JSON files with key-value pairs specified in the documentation. These settings files can be manually generated in a text editor or programmatically generated using a script (<italic>see also</italic> Supplementary Materials).</p>
<p>The main structure of the application logic is an event loop that iterates over enabled contexts. This will define presentation schemes for both simple stimuli such as an LED lighting up and water reward delivery, as well as more complex stimuli like flashing LEDs, groups of stimuli, or even visual VR scenes. A context can be applied globally or to one or several locations; a list of locations and rules governing an individual context’s activation is referred to as a Context List. When a Context List is active, the devices assigned to it will be triggered. For example, a user could configure an LED which blinks at several designated locations along the track. Context Lists are implemented in the UI using an Object-Oriented approach that streamlines implementing logic for governing the presentation and control of novel cue types. The main rules governing how a Context List works are implemented in a <monospace>check</monospace> method which is passed the current state of the experiment as input arguments and returns a Boolean variable as <monospace>true</monospace> or <monospace>false</monospace> corresponding to whether a Context List should be active or not. In this way novel Context List types may be added to the program easily through inheritance of a base class and overriding this one method. Additionally, Context Lists are instantiated using the “Factory Method” pattern (<xref ref-type="bibr" rid="c21">Gamma et al. 1994</xref>) and appended to a dynamic list. This pattern allows for adding complex functionality while minimizing the impact on performance and increasing code reuse and modularity. Existing Context List types can be modified by applying “decorators” (<xref ref-type="bibr" rid="c21">Gamma et al. 1994</xref>), resulting in <italic>composable</italic> behavioral paradigms, meaning that novel experiments can be implemented based on wrapping Context Lists with on or several of the existing decorators, without requiring any software modification. Settings files following JSON syntax specify the configuration as well as the context lists and decorators. For example, a particular decorator can be configured to cause a context to only be active on even laps while a second only permits the contexts to be active after a certain amount of time has passed. Composing these two rules means it is possible to trigger the contexts on even laps, only after 5 have passed since starting the experiment.</p>
</sec>
<sec id="s4">
<label>5</label><title>Behavior Tracking &amp; Control</title>
<sec id="s4a">
<label>5.1</label><title>GPIO/Behavior Controller</title>
<p>The primary GPIO circuit we implement, referred to as the Behavior Controller, is composed of an Arduino attached to an integrated circuit (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>). The custom PCB handles voltage regulation and provides the necessary connectors to operate downstream devices. The Arduino program distributed with behaviorMate controls all connected devices and communication between them and the UI. The program wraps “action” messages with “routes” (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>). This pattern of routing messages simplifies debugging since it is clear which classes of the Arduino code received the messages and where replies were generated. The messages are <monospace>JSON</monospace> formatted text, so they are human-readable and can be simulated within the behaviorMate PC for testing and debugging purposes. The PCB has various connectors and other components that makes it easier to connect sensors and actuators to the Arduino and provides an Ethernet adapter for the Arduino to permit LAN communication. The Behavior Controller has several important functions; it receives signals from the UI to activate rewards and cues such as odor valves, LEDs, and tone generators and passes input from sensors to the PC. In our setup for 2p imaging, the Behavior Controller also sends a synchronizing signal to the microscope to initiate imaging after a trial is started through the behaviorMate UI. The timestamp of this signal is used to align information about the animals’ behavior with recorded microscopy data. By default, this signal is a TTL pulse (3.3 V in our implementation, <xref rid="fig2" ref-type="fig">Fig. 2D</xref> <italic>also see</italic> Supplementary Materials) that can be configured to activate other types of recording devices such as video cameras or electrophysiology interfaces.</p>
</sec>
<sec id="s4b">
<label>5.2</label><title>Position Controller</title>
<p>The position controller’s function is to detect animal movement and report it to behaviorMate. It is composed of an Arduino Nano attached to a custom circuit which has an onboard quadrature decoder, 16 bit counter, and connectors for attaching the rotary encoder and lap reset circuit. For either treadmill or running wheel based setups (i.e. for a VR setup), a quadrature rotary encoder device is coupled to the shaft of a wheel that turns as the animal runs. The turning of the rotary encoder generates necessary signals, or “ticks”, which are passed to the quadrature decoder to calculate the instantaneous velocity of the animals. These quadrature ticks are decoded and counted in the counter. The custom circuit (<xref rid="fig2" ref-type="fig">Fig. 2E</xref>) uses a 10MHz oscillator which is capable of counting high resolution encoders (&gt;4096 ticks/turn) at greater than 1K RPM. The onboard counter enables the tick counting to be decoupled from the speed of the on-board Arduino. Consequently, the Arduino can check the counter at a much slower rate (100Hz in our setup, but can be as slow as 5Hz) without losing count of the ticks. The Arduino is only responsible for checking the counter and generating a <monospace>JSON</monospace> formatted text string to transmit to the behavior PC via Ethernet, and only transmits when there is movement. Significantly, this eliminates polling from the behavior PC and allows the communication between the Arduino and the PC to be one way instead of bidirectional. The behavior PC then translates the counts into a meaningful measurement of the linear distance traveled by the animal on the belt or running wheel.</p>
</sec>
</sec>
<sec id="s5">
<label>6</label><title>Behavioral Apparatus</title>
<sec id="s5a">
<label>6.1</label><title>Treadmill system</title>
<p>Self-powered treadmill systems have been extensively used in the study of navigational behaviors (<xref ref-type="bibr" rid="c48">Royer et al. 2012</xref>, <xref ref-type="bibr" rid="c54">Tuncdemir et al. 2022</xref>, <xref ref-type="bibr" rid="c38">Lovett-Barron et al. 2014</xref>, <xref ref-type="bibr" rid="c26">Grienberger &amp; Magee 2022</xref>, <xref ref-type="bibr" rid="c22">Geiller et al. 2017</xref>). The following section outlines a protocol for the construction of a behaviorMate compatible mouse treadmill setup that has been used for head-fixed imaging (<xref ref-type="bibr" rid="c35">Kaufman et al. 2020a</xref>, <xref ref-type="bibr" rid="c52">Terada et al. 2022</xref>, <xref ref-type="bibr" rid="c46">Rolotti et al. 2022b</xref>). In combination with the software and circuits described above, the treadmill system provides a flexible platform for implementing novel experimental paradigms.</p>
<sec id="s5a1">
<label>6.1.1</label><title>Frame</title>
<p>The treadmill system requires a frame to support it and maintain wheel alignment, so we provide designs for a frame made of extruded aluminum rails (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>). The rails support wheels on either end for a fabric “belt” that can be moved by a mouse. In addition to the wheels, we added two wheel-rollers that support the belt near the animal’s fixed position (<xref rid="fig3" ref-type="fig">Fig. 3E</xref>). This design does not have a platform directly beneath the mouse, allowing the belt to absorb vibrations, which reduces motion artifacts and allows for a stable field of view during <italic>in vivo</italic> imaging experiments. Treadmill belts are constructed from 2-inch wide fabric ribbon and may be embellished by linking multiple fabrics or adding various physical cues such as foam balls, sequins, or hook-and-loop fasteners for texture (<xref rid="fig3" ref-type="fig">Fig. 3F</xref>). These cues help the animal to orient itself and are useful for information encoding. In this setup, the animal is head-fixed beneath a microscope, but is able to move the belt and their attached physical cues which mimics navigation while still allowing investigators to perform imaging of cellular and even sub-cellular regions of interest. To further optimize the treadmill for imaging, a two goniometer device placed underneath the belt is used to adjust the angle of the animal’s head along the roll (side-to-side) and pitch (forward and backward) axes. This setup allows the investigator to level the field of view, ensuring the neuronal plane of interest is perpendicular to the optical axis of the microscope, thus increasing the number of neurons that can be imaged and improves image clarity. Finally, the frame has a sliding mechanism to adjust the distance between the wheels and accommodate various sized belts to ensure proper tension. Proper belt tension ensures the animal can move easily and reduces the chance of belt slippage along the wheels.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Details of Treadmill System.</title>
<p>(<bold>A</bold>) CAD model of voluntary treadmill that can expand or contract to accommodate different sized running belts. Implant holders sit on goniometers to allow mouse head angle to be adjusted. (<bold>B</bold>) Lap time error is defined for each lap (full turn of the treadmill belt) as the difference between the time recorded by behaviorMate and the computer vision benchmark in seconds. (<bold>C</bold>) Absolute error is defined for each lap as the difference between the position in behaviorMate and the benchmark position in millimeters. (<bold>D</bold>) The time to complete a lap according to behaviorMate and the benchmark were nearly identical. (<bold>E</bold>) Schematic of head-fixed behavioral task. Two-photon objective. (<bold>F</bold>) Cue-rich belts for treadmill behavioral tasks. Belts are easily interchangeable. (<bold>G</bold>) Example test condition spanning 3 days and involving 5 sensory cues. Each row represents a sensory modality (visual, auditory, etc.). Cues are tied to locations on the belt or wheel as indicated by the colored rectangles. Locations and durations of cue presentations can be changed across trials.</p></caption>
<graphic xlink:href="569989v4_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s5a2">
<label>6.1.2</label><title>Electronics and Peripherals</title>
<p>Interfacing between the treadmill and behaviorMate requires the previously described electronics. Any number of optional peripheral devices such as LEDs, odor release systems, etc., may be added. For position tracking, a rotary encoder is attached to one of the two wheel shafts and connected to the position tracking circuit to track the animal’s position. The behaviorMate UI handles the conversion of the rotary encoder’s spinning to the linear distance traveled. It also maintains the animal’s current lap number. An additional “lap-reset” feature prevents drift from accumulating between the digital tracking and physical treadmill belt. We provide designs for an optical lap-reset circuit for determining when a complete revolution of the belt has been made (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>). Setting up a lap-reset circuit additionally permits a calibration function that can be temporarily enabled prior to running an experiment to adjust the scaling between rotary encoder “ticks” and distance moved along the treadmill belt.</p>
<p>Generally position tracking drift is small (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>) but can occur (especially with more “slippery fabrics” or if belt tension is set too low). It is important to accurately specify the track length and calibrate the position scale factor to tightly couple the PC and the electronics. If the system is not set up properly or the belt is not sufficiently tight, the belt may slip, leading to inaccurate position readings. Therefore, we provide a benchmark of the position tracking performance under normal conditions. To compute the linear distance traveled, a small LED was attached to the top of the belt and the entire treadmill system was moved to a completely dark room with an IR camera pointed directly at the belt. The belt was moved by hand and upon each complete revolution, the LED would pass within the field of view of the camera. OpenCV (<xref ref-type="bibr" rid="c10">Bradski 2000</xref>) was used to extract the lap completion times from the recorded video. These were taken to be the “ground truth” lap completion times. The quantities of interest were the time and position differences between the LED and those tracked by the behaviorMate UI. The mean difference between lap times was 0.102 seconds (<xref rid="fig3" ref-type="fig">Fig. 3B, D</xref>), while the mean difference between position upon lap completion was 13 mm and did not exceed 23 mm (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>). Given that the mean and max differences were a fraction of the average size of a subject mouse and significantly smaller than the typical size of reward zones, these metrics were deemed more than acceptable. Enabling the lap-reset feature on self-powered treadmill prevents accumulation of these errors, ensuring that position, as tracked by the UI, is aligned with the fabric belt and cue presentations are aligned with the positions tracked by the UI.</p>
<p>The UI synchronizes any number of additional sensory cues such as LEDs, tones, and odor release systems, which can be present at pre-defined times or locations along the belt (<xref rid="fig3" ref-type="fig">Fig. 3G</xref>). For example, a liquid reward can be coupled to a specific location on the treadmill belt that is either <italic>operant</italic> (delivered only if the animal licks within a “reward zone”) or <italic>non-operant</italic> (delivered immediately when the animal reaches a “reward zone”). These “reward zones” are defined by a location and radius, so there is user-specified tolerance for the areas animals can receive operant rewards. The rules governing the rewards are fully customizable and specified in the settings file for the UI.</p>
</sec>
<sec id="s5a3">
<label>6.1.3</label><title>Use in Past Studies</title>
<p>The physical design of the treadmill was inspired by Royer et al. and influenced by several other hippocampus studies focusing on behavioral navigation tasks (<xref ref-type="bibr" rid="c33">Kaifosh et al. 2013</xref>, <xref ref-type="bibr" rid="c38">Lovett-Barron et al. 2014</xref>, <xref ref-type="bibr" rid="c58">Zaremba et al. 2017</xref>). Our design is flexible and has been validated by the myriad experimental paradigms that have been successfully implemented in other labs as well (<xref ref-type="bibr" rid="c53">Tuncdemir et al. 2023</xref>, <xref ref-type="bibr" rid="c56">Vancura et al. 2023</xref>, <xref ref-type="bibr" rid="c41">O’Hare et al. 2022</xref>, <xref ref-type="bibr" rid="c47">Rolotti et al. 2022a</xref>, <xref ref-type="bibr" rid="c54">Tuncdemir et al. 2022</xref>, <xref ref-type="bibr" rid="c46">Rolotti et al. 2022b</xref>, <xref ref-type="bibr" rid="c52">Terada et al. 2022</xref>, <xref ref-type="bibr" rid="c23">Geiller et al. 2022</xref>, <xref ref-type="bibr" rid="c8">Blockus et al. 2021</xref>, <xref ref-type="bibr" rid="c27">Grosmark et al. 2021</xref>, <xref ref-type="bibr" rid="c19">Dudok et al. 2021a</xref>,<xref ref-type="bibr" rid="c18">b</xref>, <xref ref-type="bibr" rid="c24">Geiller et al. 2020</xref>, <xref ref-type="bibr" rid="c35">Kaufman et al. 2020a</xref>, <xref ref-type="bibr" rid="c55">Turi et al. 2019</xref>, <xref ref-type="bibr" rid="c58">Zaremba et al. 2017</xref>, <xref ref-type="bibr" rid="c13">Danielson et al. 2017</xref>, <xref ref-type="bibr" rid="c12">2016</xref>). Multi-modal cues can be incorporated across several days of experiments to probe different aspects of navigation and associative learning (<xref rid="fig3" ref-type="fig">Fig. 3G</xref>). For example, the treadmill has been used to test goal-oriented learning, where goal locations are “hidden” and animals must report their understanding of the correct location by licking at the reward spout in order to trigger water delivery (<xref ref-type="bibr" rid="c58">Zaremba et al. 2017</xref>, <xref ref-type="bibr" rid="c12">Danielson et al. 2016</xref>). During these tasks, novel cues can be presented by introducing new treadmill belts as well as novel non-spatial cues such as background tones, blinking LEDs, or odor stimuli. Moving the reward location forces animals to learn updates to task rules mid-experiment and has been used to investigate deficits in mouse models of psychiatric disorder (<xref ref-type="bibr" rid="c58">Zaremba et al. 2017</xref>) as well as neural circuit analysis of the mechanisms behind reward learning (<xref ref-type="bibr" rid="c35">Kaufman et al. 2020a</xref>) and the development of signals underpinning spatial navigation (<xref ref-type="bibr" rid="c46">Rolotti et al. 2022b</xref>, <xref ref-type="bibr" rid="c52">Terada et al. 2022</xref>, <xref ref-type="bibr" rid="c12">Danielson et al. 2016</xref>). Cues can be tied to time, location, or both. Switching between a time-dependent presentation and a location-dependent one provides a powerful tool for assessing how the same cue may be processed differently based on the current context, location, or task (<xref ref-type="bibr" rid="c52">Terada et al. 2022</xref>, <xref ref-type="bibr" rid="c54">Tuncdemir et al. 2022</xref>). In addition to cue presentation, the setup has also been used to trigger optogenetic stimulation (<xref ref-type="bibr" rid="c36">Kaufman et al. 2020b</xref>, <xref ref-type="bibr" rid="c23">Geiller et al. 2022</xref>, <xref ref-type="bibr" rid="c41">O’Hare et al. 2022</xref>, <xref ref-type="bibr" rid="c46">Rolotti et al. 2022b</xref>); this coupling of navigational cues provided by the treadmill belt with stimulation in hippocampal region CA1 was used for the first successful optical induction of place fields (<xref ref-type="bibr" rid="c46">Rolotti et al. 2022b</xref>).</p>
</sec>
</sec>
<sec id="s5b">
<label>6.2</label><title>VR System</title>
<p>The use of Virtual Reality (VR) cues can add significant flexibility to behavioral studies and has thus become increasingly popular in neuroscience. Although different VR implementations have been described, for this manuscript, VR is defined as closed-loop control from the behaviorMate UI that integrates position updates virtually and advances cues past the animal to create the illusion of movement, unlike a belt which physically moves past the animal. Various systems for accomplishing this have been proposed and implemented previously (<xref ref-type="bibr" rid="c14">Dombeck et al. 2010</xref>, <xref ref-type="bibr" rid="c29">Harvey et al. 2012</xref>, <xref ref-type="bibr" rid="c49">Sheffield et al. 2017</xref>, <xref ref-type="bibr" rid="c28">Hainmueller &amp; Bartos 2018</xref>, <xref ref-type="bibr" rid="c11">Campbell et al. 2018</xref>, <xref ref-type="bibr" rid="c6">Arriaga &amp; Han 2019</xref>, <xref ref-type="bibr" rid="c5">2017</xref>, <xref ref-type="bibr" rid="c17">Dudok et al. 2021</xref>). Mice can learn to use VR cues as landmarks for traversing environments to specific locations. Neuronal circuits thought to be involved in navigation in freely moving mice have been shown to be similarly engaged during the described VR tasks (<xref ref-type="bibr" rid="c15">Dombeck &amp; Reiser 2012</xref>, <xref ref-type="bibr" rid="c14">Dombeck et al. 2010</xref>, <xref ref-type="bibr" rid="c4">Aronov &amp; Tank 2014</xref>). behaviorMate represents a novel contribution in that it incorporates visual VR displays into its architecture for 1D navigation and is designed to adapt seamlessly to novel arrangements of VR displays with minimal computational load for each additional display (<xref ref-type="bibr" rid="c44">Priestley et al. 2022</xref>, <xref ref-type="bibr" rid="c9">Bowler &amp; Losonczy 2023</xref>). This is achieved by rendering the virtual environments on separate android devices (one per display) with a separate application (VRMate). Since the same behaviorMate UI is used for treadmill and VR experiments, all of the experimental paradigms described in the treadmill section can be implemented in the VR system with minimal changes to the settings file. Additionally, VR permits rapid visual context switches and the addition of novel virtual objects in a familiar scene. Furthermore, “hybrid” setups are also supported in which visual displays can be added to physical treadmills in order to combine both tactile and visual cues.</p>
<sec id="s5b1">
<label>6.2.1</label><title>VRMate</title>
<p>VRMate is the companion program to behaviorMate that runs the visual VR interface to display cues to animals as they move in virtual space. VRMate is a program developed using the Unity(TM) game engine and written in C#; it listens for incoming UDP packets carrying position updates from the behaviorMate UI. In our implementation, each visual display is connected to an ODROID C4 which is a single-board computer equipped with an Amlogic S905X3 Quad-Core Cortex-A55 (2.016GHz) ARMv8-A Processor, 4 GiB of DDR4 RAM, Mali-G31 MP2 GPU, and 1 LAN port, all mounted on a single PCB and running the Android operating system. Each display-ODROID pair runs an instance of the VRMate program, which means scene rendering is done independently by dedicated hardware for each monitor. This allows for scalability to any number of additional displays without compromising frame rate or resolution. Prior to starting an experiment, behaviorMate supplies each instance of VRMate with all the information it needs to correctly display the virtual environment from a particular point of view (<xref rid="fig4" ref-type="fig">Fig. 4F</xref>). Notably, Unity is platform independent, so VRMate can be built for Windows, OSX, and other systems. Therefore, it is also not strictly necessary to use dedicated Android devices, and it is possible to run behaviorMate and VRMate on the same PC. Various setups can be configured to meet individual researchers’ needs.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Details of VR System.</title>
<p>(<bold>A</bold>) The complete assembly. 5 monitors are suspended in an octagonal pattern in front of the subject and display the virtual scenes. (<bold>B</bold>) A running wheel attached to a goniometer to allow for angle of subject’s head to be adjusted. Placed directly underneath the objective. (<bold>C</bold>) Box plot describing the delay between when the running wheel is moved and when the virtual scene is updated to reflect the new position. (<bold>D</bold>) Plot showing the difference between the position of the animal computed by behaviorMate and the ground-truth position tracked using computer vision. (<bold>E</bold>) Box plot describing the absolute difference between the current position of the mouse according to behaviorMate and the computer vision benchmark at each point in time. The median error was found to be 3.3 mm. (<bold>F</bold>) A 2-D projection of the displays which will resemble what the test animal will see. When viewed on the actual monitors, the scene will appear more immersive. (<bold>I</bold>) A VR scene as the subject moves down the virtual track. Modifying a few numerical parameters in the settings files allows one to change the appearance and view angle of a scene. Bottom: A scene with all red color shaders removed. (<bold>G</bold>) Left: Top view of implant. This side will make contact with microscope objective. Right: Bottom view. This side will make contact with the mouse’s brain. (<bold>H</bold>) Sketch of how the implant will appear after being surgically attached to skull.</p></caption>
<graphic xlink:href="569989v4_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Specifically for researchers conducting imaging experiments, Unity supports the implementation of custom “shader programs” which can be enabled through the UI. Implementing custom shaders allows you to edit color-pixel values in the visual scenes just prior to when they are rendered. For example, this means it is possible to remove a particular color emission from the scenes or display with just a single color (<xref rid="fig4" ref-type="fig">Fig. 4F</xref>). This is particularly useful for experiments where green or red fluorescent proteins are used to indicate neuronal activity. In this case, rendering scenes with the green or red component of each pixel set to 0 minimizes the risk of artifacts in the recorded data.</p>
</sec>
<sec id="s5b2">
<label>6.2.2</label><title>Frame and Display Setup</title>
<p>Our standard setup for immersive mouse VR experiments contains 5 monitors surrounding the animal. We supply plans for a VR frame to construct this (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>, <italic>also see</italic> Supplementary Materials). The VR frame is designed with extruded aluminum rails and holds the displays at the proper angle and distance from the animal. The goal is to completely encompass the animal’s visual field with the virtual scene, so the displays are placed in an octagonal setup with the animal at the center. The angle between each display is 135 degrees and the distance between the mouse and the center display is 15 inches. Since mice primarily have a visual field oriented overhead (<xref ref-type="bibr" rid="c16">Dräger &amp; Olsen 1980</xref>), the mouse is positioned 5 inches above the bottom edge of each monitor in order to align the visual stimulus to the animals field of view.</p>
<p>In cases when the standard 5 monitor setup will not fit due to space constraints, we use a wireless, tablet-based setup. VRMate is built to run on the Android OS and can be installed on any Android tablet. Our tablet system consists of 5 “Fire HD 10” (2019, 9th Gen), 10.1 inch tablets with a screen resolution of 1920 x 1200 IPS while running Android 9. Additional specs include 2 GB RAM, 4xARM Cortex-A73 (2.0 GHZ) and 4xARM Cortex-A53 (2.0 GHz) processors, and a ARM Mali-G72 MP3 GPU. Tablets may be connected to the behaviorMate intranet using a wireless router. The PC running behaviorMate connects through this router to interface with the displays. Each device on the network must be assigned a unique static IP address. This is achieved by configuring the IP tables on the Wi-Fi-enabled router to bind each tablet’s IP address to its MAC address. A low-latency router with multiple antennae is recommended and should be placed as close as possible to the tablet displays.</p>
<p>Typical VR setups position animals on top of a running wheel which is less bulky and mechanically simpler than the fabric belt treadmill system. The running wheel design used in our setup, as described previously by Warren et al., is lightweight and optimized to have low rotational momentum. This provides animals with a natural feeling of running while minimizing the effort required to traverse the virtual environments. Mice are head-fixed on top of the wheel using custom head restraining bars and a matching holder using a dove-tail and clamping system. The implant holders (<xref rid="figs1" ref-type="fig">Fig. S1B</xref>) are two L-shaped components machined from solid steel. They have two M6 holes for being attached to pillar posts of aluminum rails. In addition, the implant holders each have a channel which allows the clamps to slide and accommodate head-bars of different lengths. Both the clamps and clamp holders have thumb screws not shown in the figure. The screw for the clamps tightly grips the head-bar and thus minimizes motion artifacts in imaging data. The screw for the clamp holders fixes the clamps’ position and reduces motion artifacts. The head restraining bars for mice presented with visual cues (<xref rid="fig4" ref-type="fig">Fig. 4G</xref>) are designed to assist with blocking light that may otherwise be detected during imaging. These can be 3D printed from titanium and reused between animals if properly cleaned and sterilized.</p>
<p>In some cases, a modified running wheel design with goniometers is required to tilt both the implant holders and the entire wheel along the roll and pitch axes (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>). For head-fixed two-photon imaging, this is used to align the imaging plane to maximize the size of the field of view or, as needed, level the glass coverslip placed over the brain (which minimizes the amount of the material the beam must travel through and maximizes image clarity). For this modified design, aluminum rails are arranged in a “U” shape and then placed on top of two goniometers. The implant holders sit on top of these rails, and the wheel is suspended below the rails, offset from the goniometers. The entire assembly sits on sliding mounts connected to pillar posts so the elevation of the animal’s head can be adjusted.</p>
</sec>
<sec id="s5b3">
<label>6.2.3</label><title>Example setup</title>
<p>A simple VR system could have 2 displays in an arrow configuration directly in front of a mouse which is head fixed and mounted on top of a running wheel. Attached to the back of each monitor would be a small Android computer (ODROID) connected via an HDMI cable. The left screen could be given an IP address of 192.168.1.141. The right machine will be assigned 192.168.1.142. Static IP assignment is done through the Android devices’ settings menu. Arduinos running the Behavior and Position Controllers are given the IPs of 192.168.1.101 and 192.168.1.102 respectively (which is the default in our supplied code (<italic>see Supplementary Materials</italic>). Finally, a PC running the UI is assigned an IP of 192.168.1.100. We use standard IP address and ports for each of the displays and electronics; however, the IPs that behaviorMate will use are specified in the settings file and determined at run-time (the Arduino IP assignments are made when the devices are loaded). Each device is also assigned a port which can be selected arbitrarily and can be changed as needed. The only requirement is that all devices are on the same subnet. In this example setup, all devices share the same network prefix of 192.168.1.x so they are part of the same subnet and will be able to communicate. Given that all components are connected to the same local network, usually via an unmanaged switch, packets will arrive to their intended recipient.</p>
<p>When the animal runs, turning the wheel and rotary encoder, behaviorMate will receive and integrate movement updates from the Position Controller. Once the experimenter begins a trial from the UI, the PC will evaluate which contexts are active and send position updates to both screens simultaneously. The VRMate instances will receive these packets and render their view of the same scene independently. Latency between the UI and the computers running VRMate is fairly low (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>) so this does not present an issue. As the animal runs, it enters a reward zone. behaviorMate will send a message to the Behavior Controller triggering the “reward context”, causing a reward valve to release a drop of water or sucrose reward. As the mouse runs, the visual scene updates accordingly. Once the mouse travels a linear distance equal to the “track_length” parameter specified in the settings file, behaviorMate advances the lap count and resets the position to 0. Between-lap “time outs” or dark periods may also be configured. Furthermore, the user may choose to run a “context-switch” experiment: this type of experiment might involve one scene being displayed on odd laps and another on even laps. If a scene change is necessary, between laps, behaviorMate will send a message to both running instances of VRMate that specify the switch to the alternate scene.</p>
</sec>
<sec id="s5b4">
<label>6.2.4</label><title>VR System Performance</title>
<p>One key concern with closed-loop control VR experimental setups is the latency between when the animal moves the physical wheel and when the displays update position in the virtual environment. We therefore performed a benchmarking test on our standard 5 monitors VR/running wheel setup (with scenes rendered by ODROIDs connected to the UI via an unmanaged Ethernet switch). An impulse was applied to the running wheel, and the time taken for the screens to update was observed. The screen delay was measured using a high-speed 150 fps camera. Latency was sampled 20 times and found to have a median value of 0.067 seconds, with no sample exceeding 0.087 seconds (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>). Since mouse reaction times are approximately 0.1 seconds (<xref ref-type="bibr" rid="c15">Dombeck &amp; Reiser 2012</xref>, <xref ref-type="bibr" rid="c39">Mauk &amp; Buonomano 2004</xref>), VR position updates using this system are nearly instantaneous to the mouse.</p>
<p>As with the fabric treadmill, another characteristic that should be considered for the running wheel is the accuracy of the position tracking system. Accuracy refers to how closely the mouse’s virtual position recorded by behaviorMate matches the true linear distance it has run on the wheel. To find the “true” linear distance traveled, a small dim LED was attached to the side of the running wheel, and the whole setup was placed in a dark enclosure, so the only significant source of light was the LED. The wheel was advanced by hand, and using OpenCV (<xref ref-type="bibr" rid="c10">Bradski 2000</xref>), the total distance traversed by the LED was extracted. Concurrently, behaviorMate was detecting the current position using the attached rotary encoder. Graphs of position vs. time from the data collected using computer-vision and the data recorded by behaviorMate were overlaid. From 4D, it is clear that there is little discrepancy between the two positions. Across all time points, the mean difference between the true and recorded position was 4.1 mm.</p>
</sec>
<sec id="s5b5">
<label>6.2.5</label><title>Use in Past Experiments</title>
<p>While the the introduction of a purely visual VR option for behaviorMate represents one of the newest updates to the setup, studies have already been published that specifically leverage this capability, primarily to examine the effect of rapid within-session context changes (<xref ref-type="bibr" rid="c44">Priestley et al. 2022</xref>, <xref ref-type="bibr" rid="c9">Bowler &amp; Losonczy 2023</xref>). VR experiments have the ability to transport animals to completely novel scenes which has been useful for investigating the mechanisms of synaptic plasticity, since it is possible to capture the animals first exposure to an environment in a controlled way (<xref ref-type="bibr" rid="c44">Priestley et al. 2022</xref>). Since the visual VR setup is integrated into behaviorMate, it is also possible to test how animals respond to changes in the rules governing reward delivery and visual scene presentation. The system further allows for simultaneous delivery of multi-modal stimuli simultaneously such as audio and visual. Thus, our system comprises an adaptable framework for probing the function of navigational circuits in the brain and the relationship between goal and context representations (<xref ref-type="bibr" rid="c9">Bowler &amp; Losonczy 2023</xref>).</p>
</sec>
</sec>
</sec>
<sec id="s6">
<label>7</label><title>Experimental Validation</title>
<p>To investigate if spatially tuned ‘place cells’ (<xref ref-type="bibr" rid="c42">O’Keefe &amp; Dostrovsky 1971</xref>) with similar properties emerge in both the belt treadmill system (TM) and the VR running wheel system, we performed 2p imaging in CA1 of the hippocampus while animals explored in either VR or TM. Mice were injected with GCaMP8m (AAV1-syn-GCaMP8m-WPRE, Addgene) and implanted with a glass window to have optical access to CA1 (<xref rid="fig5" ref-type="fig">Fig. 5</xref>). Once mice recovered from surgeries, they were trained to do a spatial navigation task in either VR or TM. After the mice were fully trained, 2p imaging was performed during navigation. Calcium signals were processed with Suite2p for motion correction, cell segmentation, and fluorescent signal and neuropil extraction (<xref ref-type="bibr" rid="c43">Pachitariu et al. 2017</xref>). Extracted raw fluorescent signals were corrected for neuropil contamination, and resulting ΔF/F was used to estimate spike events using OASIS (<xref ref-type="bibr" rid="c20">Friedrich et al. 2017</xref>). A median filter was applied to binarize spike amplitudes into bins of position, and all subsequent analyses used this binarized train as an estimation of each ROIs activity. We recorded on average 1533<italic>±</italic>481 and 1057<italic>±</italic>169 CA1 neurons in VR and TM, respectively. Although VR had significantly fewer CA1 neurons being classified as place cells compared to TM (VR: 34.9% (3253/9195), TM: 44.9% (6101/13454), two sample z-test, z=14.96, p=1.27e-50), place cells tiled the entire track in both VR and TM (<italic>see</italic> Supplementary Materials). In all subsequent analyses, we used CA1 neurons identified as a place cell unless otherwise stated. To assess differences in the calcium signal of place cells found in context-environment pairs, we first compared the ΔF/F amplitude and frequency of individual ROIs identified as place cells. Only ΔF/F events that were 3 standard deviations above the mean ΔF/F were included in these analyses. To examine the effect of environment (VR vs. TM) and context (familiar (F) vs. novel (N)) on ΔF/F amplitude and frequency, we conducted two-way repeated measures of ANOVA with both environment and context as a within-subject factors. There was no significant effect of environment or context on mean ΔF/F amplitude (environment: F(1,5)=1.9387, p=0.222, context: F(1,5)=0.0037, p=0.9537) or on mean ΔF/F frequency (environment: F(1,5)=4.37, p=0.091, context: F(1,5)=0.0789, p=0.7901). These results suggest that calcium transient properties of place cells observed in both environments and contexts were similar. Next, we quantified individual place cells’ specificity, sensitivity, and spatial information to identify any differences in place cell firing properties between context-environment pairs. As described above, two-way repeated measures of ANOVA were used with both context and environment as within-subject factors. The two-way ANOVA revealed a significant effect of environment on sensitivity (environment: F(1,5)=9.85, p=0.0257, context: F(1,5)=3.34, p=0.13) and specificity (environment: F(1,5)=53.32, p=0.0008, context: F(1,5)=2.55, p=0.17), suggesting that place cells in TM have higher trial-by-trial in-field firing compared to those from VR. However, we did not observe any significant effect of environment or context on spatial information (<xref ref-type="bibr" rid="c51">Skaggs &amp; Mcnaughton 1998</xref>) (environment: F(1,5)=0.74, p=0.43, context: F(1,5)=3.79, p=0.11). These results are consistent with previous findings that place cells in VR have reduced trial-by-trial reliability (<xref ref-type="bibr" rid="c45">Ravassard et al. 2013</xref>, <xref ref-type="bibr" rid="c1">Aghajan et al. 2015</xref>), but still encode spatial information (<xref ref-type="bibr" rid="c14">Dombeck et al. 2010</xref>, <xref ref-type="bibr" rid="c4">Aronov &amp; Tank 2014</xref>, <xref ref-type="bibr" rid="c28">Hainmueller &amp; Bartos 2018</xref>, <xref ref-type="bibr" rid="c44">Priestley et al. 2022</xref>). The observed differences in the properties of VR and TM place cells may be the result of the absence of proximal visual cues and tactile cues (which are speculated to be more salient than visual cues) during VR tasks. Next, we wanted to investigate if place cells remap in both VR and TM environments when mice are exposed to a novel context (<xref ref-type="bibr" rid="c40">Muller &amp; Kubie 1987</xref>, <xref ref-type="bibr" rid="c37">Leutgeb et al. 2005</xref>). As expected, a subset of place cells changed their place tuning properties when exposed to novel contexts (<xref rid="fig6" ref-type="fig">Fig. 6A,B</xref>). Moreover, we did not observe differences in the between context rate map correlation between VR and TM, suggesting that remapping happens in both environment types. To further quantify the extent of remapping in each environment, we built a naive Bayes decoder to decode position based on population activity of CA1 neurons. When the decoder was trained with population activity from familiar trials and tested with population activity from a held-out familiar trial, the absolute decoding error was significantly below chance (p&lt;0.05) in both VR and TM. However, when the decoder trained with familiar trials was tested to decode position in a novel trial, the absolute decoding error was at the chance level (<xref rid="fig6" ref-type="fig">Fig. 6C,D</xref>) in both VR and TM. These suggest that place cells remap in both VR and TM when exposed to novel context.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>2-photon imaging of CA1 population for experimental validation</title>
<p><bold>(A)</bold> Field of view obtained from virtual reality (VR) and treadmill (TM) <bold>(B)</bold> Examples of place cells recorded using VR or TM. In both familiar and novel contexts, place cells encoding different locations on the track are found. Note that they are different place cells from individual animals. <bold>(C)</bold> Mean Δf/f amplitude of place cells recorded each context-environment pair (F = familiar context, N = novel context). <bold>(D)</bold> Mean Δf/f frequency of place cells. For (C) and (D), only significant Δf/f events were used. See methods for details. <bold>(E)</bold> Place sensitivity in each context-environment pair. Significant main effect of environment (VR vs. TM, p = 0.0257). <bold>(F)</bold> Place cell specificity in each context-environment pair. Significant main effect of environment (VR vs. TM, p = 0.008) <bold>(G)</bold> Spatial information (bits/event) in each context-environment pair. Two-way repeated measures of ANOVA was used unless otherwise stated. Refer to Table ST1 for ANOVA table. For all panels, n.s. = non-significant, *P&lt;0.05, ***P&lt;0.001</p></caption>
<graphic xlink:href="569989v4_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Place cells remap in both environments</title>
<p><bold>(A)</bold> Place cells recorded in VR or TM sorted by their familiar peak (top) or novel peak (bottom) locations. A subset of CA1 neurons remaps when exposed to novel context in both VR and TM. <bold>(B)</bold> Rate map correlation of individual place cells between familiar and novel contexts. No difference was observed between VR and TM (Wilcoxon rank-sum test, F=1.38, p=0.17). <bold>(C)</bold> Confusion matrices of the Bayesian decoder for VR and TM. F-F indicates a model trained and tested with trials from the familiar context. F-N is a model trained with trials from familiar context, and tested with a novel trial. <bold>(D)</bold> Absolute decoding error for the maximum likelihood estimator. A dashed line indicates a chance level (88±1.96cm for VR/ 50cm for TM, see methods). Note that when the decoder is trained and tested with familiar context trials, absolute decoding accuracy is below chance in both VR and TM (VR: 27.62±1.44cm, p&lt;0.01, TM: 17.78±0.44cm, p&lt;0.01, permutation test). However, when the decoder is trained with familiar context trials, but tested with novel context trials, decoding accuracy was increased to the chance level (VR: 99.80±3.95cm, p=1.00, TM: 56.82±2.46cm, p=1.00).</p></caption>
<graphic xlink:href="569989v4_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s7">
<label>8</label><title>Discussion</title>
<p>behaviorMate is, most generally, a solution for controlling and collecting feedback from devices in behavioral experiments. The behaviorMate methodology is built around an “Intranet of Things” approach, expanding to incorporate additional components to meet experimental demands but also compatible with sparser setups to ease complexity when not needed. It has typically been used concurrently with neuronal imaging, since the time-stamped output of behavior events is relatively simple to align with imaging data, providing experimenters with the ability to correlate neuronal events with behavior. In addition, behaviorMate is designed to be flexible, expanding to incorporate additional components as specified in the settings while making minimal assumptions. Experimenters may incorporate other techniques such as imaging and electrophysiology recordings or just run behavior. For experimental paradigms involving any combination of displays, one-dimensional movement (wheels, treadmills, etc.), sensors (LEDs, Capacitance lick sensors, etc.), and actuators (odor release systems, solenoid valves, etc.), this is a cost-effective and reliable solution.</p>
<p>The main modules of behaviorMate are the Java GUI application, referred to as the UI, Behavior Controller, and Position Controller. The UI coordinates most communication between devices, although sometimes devices directly communicate with each other for performance reasons. The Behavior Controller sends messages to and receives data from nearly any kind of peripheral devices such as 2-photon microscopes, LEDs, speakers, computer displays, electrophysiology systems, etc. The Position Controller is solely responsible for receiving position updates from the rotary encoder. Any number of additional devices may be used by connecting them to the behavior controller using standard connectors. Furthermore, the companion software of VRMate allows for low-cost integration of virtual reality experiments that are already compatible with behaviorMate. The appeal of behaviorMate is its modularity, reliability, and open source code base.</p>
<p>Finally, many published studies have relied on this system and the resulting data collected (<xref ref-type="bibr" rid="c9">Bowler &amp; Losonczy 2023</xref>, <xref ref-type="bibr" rid="c53">Tuncdemir et al. 2023</xref>, <xref ref-type="bibr" rid="c56">Vancura et al. 2023</xref>, <xref ref-type="bibr" rid="c44">Priestley et al. 2022</xref>, <xref ref-type="bibr" rid="c41">O’Hare et al. 2022</xref>, <xref ref-type="bibr" rid="c47">Rolotti et al. 2022a</xref>, <xref ref-type="bibr" rid="c54">Tuncdemir et al. 2022</xref>, <xref ref-type="bibr" rid="c46">Rolotti et al. 2022b</xref>, <xref ref-type="bibr" rid="c52">Terada et al. 2022</xref>, <xref ref-type="bibr" rid="c23">Geiller et al. 2022</xref>, <xref ref-type="bibr" rid="c8">Blockus et al. 2021</xref>, <xref ref-type="bibr" rid="c27">Grosmark et al. 2021</xref>, <xref ref-type="bibr" rid="c19">Dudok et al. 2021a</xref>,<xref ref-type="bibr" rid="c18">b</xref>, <xref ref-type="bibr" rid="c24">Geiller et al. 2020</xref>, <xref ref-type="bibr" rid="c35">Kaufman et al. 2020a</xref>, <xref ref-type="bibr" rid="c55">Turi et al. 2019</xref>, <xref ref-type="bibr" rid="c58">Zaremba et al. 2017</xref>, <xref ref-type="bibr" rid="c13">Danielson et al. 2017</xref>, <xref ref-type="bibr" rid="c12">2016</xref>). These paradigms varied greatly and included random foraging, goal-oriented learning, multi-modal cue presentations, and context switches (both in VR environments and physical treadmills). Instructions for downloading the UI software and the companion VRMate software, and assembling a Behavior Controller, Position Controller, running wheel, VR monitor frame, and treadmill, can be found on the Losonczy Lab website (<ext-link ext-link-type="uri" xlink:href="http://www.losonczylab.org/software">www.losonczylab.org/software</ext-link>).</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>J.C.B. is supported by NIMH F31NS110316. J.B.P. is supported by the EPFL ELISIR fellowship and the Fondation Marina Cuennet-Mauvernay. A.L. is NIMH R01MH124047, NIMH R01MH124867, NINDS R01NS121106,NINDS U01NS115530, NINDS R01NS133381, NINDS R01NS131728, NIA RF1AG080818. We thank Drs. Ali Kaufman and Ally Lowell for troubleshooting and debugging the initial iterations of behaviorMate as well as Dr. Andres Grosmark for continued feedback on the project. Additionally, we thank Dr. Stephanie Herrlinger, Abhishek Shah, and all other members of the Losonczy lab for comments and discussion on the manuscript.</p>
</ack>
<sec id="s8">
<title>Author Contributions</title>
<p>J.C.B. and A.L. conceived of the original method and experimental requirements. Funding support from A.L. J.C.B. developed software with assistance from W.L. and Z.L. W.L developed custom PCBs. G.Z. developed a JavaFX version of behaviorMate, performed benchmarking and mechanical design. J.C.B., G.Z. and H.C.Y. wrote the original draft with input from all authors. H.C.Y. performed the experimental validation with assistance from B.R. J.B.P. developed the VR layout. J.C.B., G.Z., B.R., Z.L. and J.B.P. performed testing and debugging. All authors contributed to review and editing.</p>
</sec>
<sec id="s9">
<title>Declaration of Interests</title>
<p>The authors declare no competing interests.</p>
</sec>
<sec id="s10">
<label>9</label><title>Methods</title>
<sec id="s10a">
<label>9.1</label><title>Lead Contact and Materials Availability</title>
<p>Further information and requests for resources and reagents should be directed to the Lead Contact Attila Losonczy (<email>al2856@columbia.edu</email>). All unique resources generated in this study are available from the Lead Contact with a completed Materials Transfer Agreement.</p>
</sec>
<sec id="s10b">
<label>9.2</label><title>Experimental Model and Subject Details</title>
<sec id="s10b1">
<label>9.2.1</label><title>Animals</title>
<p>All experiments were conducted in accordance with the NIH guidelines and with the approval of the Columbia University Institutional Animal Care and Use Committee. Experiments were performed with six adult mice between 8-16 weeks. 5 mice were Unc5b crossed with C57Bl/6, and 1 mouse was VGAT-cre crossed with Ai9 (Jackson Laboratory). 3 female and 3 male mice were used in this experiment to balance for sex.</p>
</sec>
<sec id="s10b2">
<label>9.2.2</label><title>Surgical procedures</title>
<p>Surgeries were done as previously described (<xref ref-type="bibr" rid="c38">Lovett-Barron et al. 2014</xref>). Briefly, animals were anesthetized with isoflurane and injected with jGCaMP8m (rAAV1-syn-GCaMP8m-WPRE, Addgene: 162375-AAV1) in dorsal CA1 (AP: 2.1, ML: 1.4, DV: 1.3, 1,2, 1,1, 100uL per site) using Nanoject III (Drummond). Animals were allowed to recover from the injection surgery for 7 days, and the cortex lying above CA1 was aspirated to implant a glass cannula to enable optical access to CA1. Finally, a titanium headpost was cemented to the skull using dental cement to head-fix animals for 2-photon functional imaging. Analgesics were provided as per Columbia University Institutional Animal Care and Use Committee prior and after the surgeries for up to 3 days.</p>
</sec>
<sec id="s10b3">
<label>9.2.3</label><title>2-photon imaging and behavior</title>
<p>After recovering from surgeries, animals were habituated to head fixation in a VR environment for 15 minutes. Subsequently, animals were trained to run on a lightweight wheel, and 5% sucrose was given at random locations to motivate animals’ to run in the VR or TM. When animals reliably ran at least 60 laps in VR, or 30 laps in TM, the reward was fixed to either 1 or 2 locations. A 3m VR track was used for all animals except jy030, which had 4m VR track instead, and 2m TM belt was used in all animals. For VR experiments, animals were imaged for one session, and the context switch happened after animals ran 30 laps in familiar context. Animals were required to run at least another 30 laps in the novel context. In TM, imaging was done over two days (first day: familiar belt, second day: novel belt). 2p imaging was performed using a 8kHz resonant scanner (Bruker) and a 16x Nikon water immersion objective (0.8 NA, 3mm working distance). In order to image GCaMP8m signals, 920nm excitation wavelength was used (Coherent), and the power from the tip of the objective never exceeded 100mW. GCaMP signals were collected using a GaAsP photomultiplier tube detector (Hamamatsu, 7422P-40) following amplification via a custom dual stage preamplifier (1.4 x 105 dB, Bruker). All imaging was performed using 512 x 512 pixels with digital zoom between 1 and 1.4.</p>
</sec>
<sec id="s10b4">
<label>9.2.4</label><title>Calcium signal processing</title>
<p>Imaging data was processed as previously described (<xref ref-type="bibr" rid="c44">Priestley et al. 2022</xref>). In short, the SIMA software package (<xref ref-type="bibr" rid="c34">Kaifosh et al. 2014</xref>) was used to organize the imaging data set, and the imaging data was processed with Suite2p (<xref ref-type="bibr" rid="c43">Pachitariu et al. 2017</xref>) for motion correction, signal/neuropil extraction, and ROI detection. Following ROI detection, individual ROIs were manually curated using the Suite2p GUI to exclude non-somatic ROIs. ΔF/F signals were calculated after subtracting the neuropil and correcting for baseline.</p>
</sec>
<sec id="s10b5">
<label>9.2.5</label><title>Data analysis</title>
<p>As previously described, all analyses were done using binarized event signals (<xref ref-type="bibr" rid="c2">Ahmed et al. 2020</xref>, <xref ref-type="bibr" rid="c44">Priestley et al. 2022</xref>). Briefly, ΔF/F signals were deconvolved using OASIS (<xref ref-type="bibr" rid="c20">Friedrich et al. 2017</xref>), and the resulting estimated spike amplitude train was binarized by selecting events whose amplitudes were above 4 median absolute deviations of the raw trace. We do not claim that this reflects the true spiking activities of individual ROIs. Binarized event trains were binned into 4cm bins and smoothed with a Gaussian filter (S.D. = 2) to calculate tuning curves. Individual ROIs were classified as a place cell by identifying bins that have activities greater than the 99th percentile of surrogate tuning curves (<xref ref-type="bibr" rid="c44">Priestley et al. 2022</xref>). In short, surrogate average tuning curves were calculated by performing 1,000 circular shuffles of their occupancy of individual trials. Bins that had activity greater than 99th percentile of the tuning curves were identified as potentially significant bins. In order to be classified as a place cell, ROIs needed to have at least 3 consecutive significant bins (12cm), but less than 25 consecutive bins (100cm). Moreover, in order to avoid spurious detection of significant bins, binarized events must have been detected in at least 50% of trials within those significant bins.</p>
<p>For mean ΔF/F amplitude and frequency calculations, only bins with 3 standard deviations above the mean were used. For place cell sensitivity, representing the proportion of laps that had active events within the significant field, the number of laps that had at least 1 binarized event within the significantly detected fields was divided by the total number of laps. For place cell specificity, total number of events within the significant field was divided by the total number of events observed within the lap. Then, it was averaged across all laps to have a single value for each ROI. If multiple fields were detected, they were computed separately and averaged across fields to have a single value for each ROI. For spatial information, it was calculated as described previously (<xref ref-type="bibr" rid="c51">Skaggs &amp; Mcnaughton 1998</xref>). For remapping analysis, one animal was used for the analysis (jy065). Given that TM imaging happened across two days, FOVs recorded from TM were matched using CellReg (<xref ref-type="bibr" rid="c50">Sheintuch et al. 2017</xref>), and spatial mask correlation was used to find the same ROIs. Results were manually curated within the Suite2p GUI. This was not necessary for VR since the context switch happened within the same session. To calculate rate map correlation, the Pearson correlation coefficient was computed between average tuning curves from the famliar and novel contexts. For TM, given that place cells may anchor on seams of the belt, the shift that generated the maximum population vector correlation between familiar and novel contexts was calculated. All ROIs recorded in the novel context were circularly shifted by the same amount, and the rate map correlation was calculated as described above. A naive Bayesian classifier was used to decode animals’ position on the track (<xref ref-type="bibr" rid="c59">Zhang et al. 1998</xref>). For each frame and spatial bin:</p>
<disp-formula>
<graphic xlink:href="569989v4_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<p>where N is the total number of cells, <italic>f<sub>i</sub></italic>(<italic>pos</italic>) is the average binned activity of a cell, <italic>τ</italic> is the bin size, and <italic>a<sub>i</sub></italic>is the activity on the frame. C is the normalization constant. The position bin with the highest probability was selected as a decoded position. Chance level for VR was calculated by randomly shuffling cell labels of the testing set. For TM, a chance level was determined as track length / 4, which is a chance level of a circular environment (50cm). The decoder was trained with n-1 trials and tested on the left out trial. To match ROI numbers recorded between VR and TM, 500 ROIs were randomly selected, and the decoding accuracy was calculated 50 times. Average absolute decoding accuracy was compared to the chance level calculated from each iteration, and the number of times that it was below the chance level was considered to be its p value.</p>
</sec>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Aghajan</surname>, <given-names>Z. M.</given-names></string-name>, <string-name><surname>Acharya</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Moore</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Cushman</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Vuong</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Mehta</surname>, <given-names>M. R.</given-names></string-name> (<year>2015</year>), ‘<article-title>Impaired spatial selectivity and intact phase precession in two-dimensional virtual reality</article-title>.’, <source>Nature neuroscience</source> <volume>18</volume>, <fpage>121</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Ahmed</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Priestley</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Castro</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Stefanini</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Canales</surname>, <given-names>A. S. S.</given-names></string-name>, <string-name><surname>Balough</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Lavoie</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Mazzucato</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Fusi</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2020</year>), ‘<article-title>Hippocampal network reorganization underlies the formation of a temporal association memory</article-title>.’, <source>Neuron</source> <volume>107</volume>, <fpage>283</fpage>–<lpage>291.e6</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Akam</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Lustig</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rowland</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Kapanaiah</surname>, <given-names>S. K.</given-names></string-name>, <string-name><surname>Esteve-Agraz</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Panniello</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Márquez</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kohl</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Kätzel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Costa</surname>, <given-names>R. M.</given-names></string-name> &amp; <string-name><surname>Walton</surname>, <given-names>M. E.</given-names></string-name> (<year>2022</year>), ‘<article-title>Open-source, python-based, hardware and software for controlling behavioural neuroscience experiments</article-title>’, <source>eLife</source> <volume>11</volume>, <fpage>e67846</fpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.7554/eLife.67846</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Aronov</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Tank</surname>, <given-names>D. W.</given-names></string-name> (<year>2014</year>), ‘<article-title>Engagement of neural circuits underlying 2d spatial navigation in a rodent virtual reality system</article-title>’, <source>Neuron</source> <volume>84</volume>, <fpage>442</fpage>–<lpage>456</lpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.042</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Arriaga</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Han</surname>, <given-names>E. B.</given-names></string-name> (<year>2017</year>), ‘<article-title>Dedicated hippocampal inhibitory networks for locomotion and immobility</article-title>’, <source>Journal of Neuroscience</source> <volume>37</volume>(<issue>38</issue>), <fpage>9222</fpage>–<lpage>9238</lpage>. <bold>URL:</bold> <ext-link>https://www.jneurosci.org/content/37/38/9222</ext-link></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Arriaga</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Han</surname>, <given-names>E. B.</given-names></string-name> (<year>2019</year>), ‘<article-title>Structured inhibitory activity dynamics in new virtual environments</article-title>’, <source>eLife</source> <volume>8</volume>, <fpage>e47611</fpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.7554/eLife.47611</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Bittner</surname>, <given-names>K. C.</given-names></string-name>, <string-name><surname>Milstein</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Grienberger</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Romani</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Magee</surname>, <given-names>J. C.</given-names></string-name> (<year>2017</year>), <article-title>‘Behavioral time scale synaptic plasticity underlies ca1 place fields</article-title>.’, <source>Science (New York, N.Y.)</source> <volume>357</volume>, <fpage>1033</fpage>–<lpage>1036</lpage>. <bold>URL:</bold> <ext-link>http://www.ncbi.nlm.nih.gov/pubmed/28883072</ext-link></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Blockus</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Rolotti</surname>, <given-names>S. V.</given-names></string-name>, <string-name><surname>Szoboszlay</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Peze-Heidsieck</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ming</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Schroeder</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Apostolo</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Vennekens</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Katsamba</surname>, <given-names>P. S.</given-names></string-name>, <string-name><surname>Bahna</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Mannepalli</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ahlsen</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Honig</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Shapiro</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>de Wit</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Polleux</surname>, <given-names>F.</given-names></string-name> (<year>2021</year>), ‘<article-title>Synaptogenic activity of the axon guidance molecule robo2 underlies hippocampal circuit function</article-title>.’, <source>Cell reports</source> <volume>37</volume>, <fpage>109828</fpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="web"><string-name><surname>Bowler</surname>, <given-names>J. C.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2023</year>), ‘<article-title>Direct cortical inputs to hippocampal area ca1 transmit complementary signals for goal-directed navigation</article-title>’, <source>Neuron</source>. <bold>URL:</bold> <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0896627323007018">https://linkinghub.elsevier.com/retrieve/pii/S0896627323007018</ext-link></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Bradski</surname>, <given-names>G.</given-names></string-name> (<year>2000</year>), ‘<article-title>The OpenCV Library</article-title>’, <source>Dr. Dobb’s Journal of Software Tools</source>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Campbell</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Ocko</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Mallory</surname>, <given-names>C. S.</given-names></string-name>, <string-name><surname>Low</surname>, <given-names>I. I. C.</given-names></string-name>, <string-name><surname>Ganguli</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Giocomo</surname>, <given-names>L. M.</given-names></string-name> (<year>2018</year>), ‘<article-title>Principles governing the integration of landmark and self-motion cues in entorhinal cortical codes for navigation</article-title>.’, <source>Nature neuroscience</source> <volume>21</volume>, <fpage>1096</fpage>–<lpage>1106</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Danielson</surname>, <given-names>N. B.</given-names></string-name>, <string-name><surname>Kaifosh</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Zaremba</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Lovett-Barron</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Tsai</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Denny</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Balough</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Goldberg</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Drew</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Hen</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Kheirbek</surname>, <given-names>M. A.</given-names></string-name> (<year>2016</year>), ‘<article-title>Distinct contribution of Adult-Born hippocampal granule cells to context encoding</article-title>’, <source>Neuron</source> <volume>90</volume>(<issue>1</issue>), <fpage>101</fpage>–<lpage>112</lpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.019</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Danielson</surname>, <given-names>N. B.</given-names></string-name>, <string-name><surname>Turi</surname>, <given-names>G. F.</given-names></string-name>, <string-name><surname>Ladow</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Chavlis</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Petrantonakis</surname>, <given-names>P. C.</given-names></string-name>, <string-name><surname>Poirazi</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2017</year>), ‘<article-title>In vivo imaging of dentate gyrus mossy cells in behaving mice</article-title>.’, <source>Neuron</source> <volume>93</volume>, <fpage>552</fpage>–<lpage>559.e4</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Dombeck</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Harvey</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Tian</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Looger</surname>, <given-names>L. L.</given-names></string-name> &amp; <string-name><surname>Tank</surname>, <given-names>D. W.</given-names></string-name> (<year>2010</year>), ‘<article-title>Functional imaging of hippocampal place cells at cellular resolution during virtual navigation</article-title>’, <source>Nature Neuroscience</source> <volume>13</volume>(<issue>11</issue>), <fpage>1433</fpage>–<lpage>1440</lpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.1038/nn.2648</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Dombeck</surname>, <given-names>D. A.</given-names></string-name> &amp; <string-name><surname>Reiser</surname>, <given-names>M. B.</given-names></string-name> (<year>2012</year>), ‘<article-title>Real neuroscience in virtual worlds</article-title>’, <source>Current Opinion in Neurobiology</source> <volume>22</volume>(<issue>1</issue>), <fpage>3</fpage>–<lpage>10</lpage>. <collab>Neurotechnology</collab>. <bold>URL:</bold> <ext-link>https://www.sciencedirect.com/science/article/pii/S0959438811001814</ext-link></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Dräger</surname>, <given-names>U. C.</given-names></string-name> &amp; <string-name><surname>Olsen</surname>, <given-names>J. F.</given-names></string-name> (<year>1980</year>), ‘<article-title>Origins of crossed and uncrossed retinal projections in pigmented and albino mice</article-title>.’, <source>The Journal of comparative neurology</source> <volume>191</volume>, <fpage>383</fpage>–<lpage>412</lpage>. <bold>URL:</bold> <ext-link>http://www.ncbi.nlm.nih.gov/pubmed/7410600</ext-link></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Dudok</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Klein</surname>, <given-names>P. M.</given-names></string-name>, <string-name><surname>Hwaun</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>B. R.</given-names></string-name>, <string-name><surname>Yao</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Fong</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Bowler</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Terada</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sparks</surname>, <given-names>F. T.</given-names></string-name>, <string-name><surname>Szabo</surname>, <given-names>G. G.</given-names></string-name>, <string-name><surname>Farrell</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Berg</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Daigle</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Tasic</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Dimidschstein</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Fishell</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zeng</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Soltesz</surname>, <given-names>I.</given-names></string-name> (<year>2021</year>), ‘<article-title>Alternating sources of perisomatic inhibition during behavior</article-title>’, <source>Neuron</source> <volume>109</volume>(<issue>6</issue>), <fpage>997</fpage>–<lpage>1012.e9</lpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.1016/j.neuron.2021.01.003</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Dudok</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Klein</surname>, <given-names>P. M.</given-names></string-name>, <string-name><surname>Hwaun</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>B. R.</given-names></string-name>, <string-name><surname>Yao</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Fong</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Bowler</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Terada</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sparks</surname>, <given-names>F. T.</given-names></string-name>, <string-name><surname>Szabo</surname>, <given-names>G. G.</given-names></string-name>, <string-name><surname>Farrell</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Berg</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Daigle</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Tasic</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Dimidschstein</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Fishell</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zeng</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Soltesz</surname>, <given-names>I.</given-names></string-name> (<year>2021b</year>), ‘<article-title>Alternating sources of perisomatic inhibition during behavior</article-title>.’, <source>Neuron</source> <volume>109</volume>, <fpage>997</fpage>–<lpage>1012.e9</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Dudok</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Szoboszlay</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Paul</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Klein</surname>, <given-names>P. M.</given-names></string-name>, <string-name><surname>Liao</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Hwaun</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Szabo</surname>, <given-names>G. G.</given-names></string-name>, <string-name><surname>Geiller</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Vancura</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>B.-S.</given-names></string-name>, <string-name><surname>McKenzie</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Homidan</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Klaver</surname>, <given-names>L. M. F.</given-names></string-name>, <string-name><surname>English</surname>, <given-names>D. F.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>Z. J.</given-names></string-name>, <string-name><surname>Buzsáki</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Soltesz</surname>, <given-names>I.</given-names></string-name> (<year>2021a</year>), ‘<article-title>Recruitment and inhibitory action of hippocampal axo-axonic cells during behavior</article-title>.’, <source>Neuron</source> <volume>109</volume>, <fpage>3838</fpage>–<lpage>3850.e8</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Friedrich</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Paninski</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Staneva</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Chklovskii</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Pnevmatikakis</surname>, <given-names>E.</given-names></string-name> (<year>2017</year>), ‘<article-title>Fast online deconvolution of calcium imaging data</article-title>’, <source>PLOS Computational Biology</source> <volume>13</volume>, <fpage>e1005423</fpage>. <bold>URL:</bold> <ext-link ext-link-type="uri" xlink:href="http://dx.plos.org/10.1371/journal.pcbi.1005423">http://dx.plos.org/10.1371/journal.pcbi.1005423</ext-link></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="book"><string-name><surname>Gamma</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Helm</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Vlissides</surname>, <given-names>J. M.</given-names></string-name> (<year>1994</year>), <source>Design Patterns: Elements of Reusable Object-Oriented Software</source>, <volume>1</volume> edn, <publisher-name>Addison-Wesley Professional</publisher-name>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Geiller</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Fattahi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Choi</surname>, <given-names>J.-S.</given-names></string-name> &amp; <string-name><surname>Royer</surname>, <given-names>S.</given-names></string-name> (<year>2017</year>), ‘<article-title>Place cells are more strongly tied to landmarks in deep than in superficial ca1</article-title>’, <source>Nature Communications</source> <volume>8</volume>, <fpage>14531</fpage>. <bold>URL:</bold> <ext-link ext-link-type="uri" xlink:href="http://www.nature.com/articles/ncomms14531">http://www.nature.com/articles/ncomms14531</ext-link></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Geiller</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Sadeh</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Rolotti</surname>, <given-names>S. V.</given-names></string-name>, <string-name><surname>Blockus</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Vancura</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Negrean</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Murray</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Rózsa</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Polleux</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Clopath</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2022</year>), ‘<article-title>Local circuit amplification of spatial selectivity in the hippocampus</article-title>.’, <source>Nature</source> <volume>601</volume>, <fpage>105</fpage>–<lpage>109</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Geiller</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Vancura</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Terada</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Troullinou</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Chavlis</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tsagkatakis</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Tsakalides</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Ócsai</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Poirazi</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Rózsa</surname>, <given-names>B. J.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2020</year>), ‘<article-title>Large-scale 3d two-photon imaging of molecularly identified ca1 interneuron dynamics in behaving mice</article-title>.’, <source>Neuron</source> <volume>108</volume>, <fpage>968</fpage>–<lpage>983.e9</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Gonzalez</surname>, <given-names>K. C.</given-names></string-name>, <string-name><surname>Negrean</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Liao</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Polleux</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2023</year>), ‘<article-title>Synaptic basis of behavioral timescale plasticity 1 2 3 4 5</article-title>’, <source>bioRxiv</source>. URL: <pub-id pub-id-type="doi">10.1101/2023.10.04.560848</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Grienberger</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Magee</surname>, <given-names>J. C.</given-names></string-name> (<year>2022</year>), ‘<article-title>Entorhinal cortex directs learning-related changes in ca1 representations</article-title>.’, <source>Nature</source>. URL: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/36323779">http://www.ncbi.nlm.nih.gov/pubmed/36323779</ext-link></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Grosmark</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Sparks</surname>, <given-names>F. T.</given-names></string-name>, <string-name><surname>Davis</surname>, <given-names>M. J.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2021</year>), ‘<article-title>Reactivation predicts the consolidation of unbiased long-term cognitive maps</article-title>’, <source>Nature Neuroscience</source> <volume>24</volume>, <fpage>1574</fpage>–<lpage>1585</lpage>. <bold>URL:</bold> <ext-link>https://www.nature.com/articles/s41593-021-00920-7</ext-link></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Hainmueller</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Bartos</surname>, <given-names>M.</given-names></string-name> (<year>2018</year>), ‘<article-title>Parallel emergence of stable and dynamic memory engrams in the hippocampus</article-title>’, <source>Nature</source> <volume>558</volume>(<issue>7709</issue>), <fpage>292</fpage>–<lpage>296</lpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.1038/s41586-018-0191-2</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Harvey</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Coen</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Tank</surname>, <given-names>D. W.</given-names></string-name> (<year>2012</year>), ‘<article-title>Choice-specific sequences in parietal cortex during a virtual-navigation decision task</article-title>’, <source>Nature</source> <volume>484</volume>(<issue>7392</issue>), <fpage>62</fpage>–<lpage>68</lpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.1038/nature10918</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Heys</surname>, <given-names>J. G.</given-names></string-name>, <string-name><surname>Rangarajan</surname>, <given-names>K. V.</given-names></string-name> &amp; <string-name><surname>Dombeck</surname>, <given-names>D. A.</given-names></string-name> (<year>2014</year>), ‘<article-title>The functional micro-organization of grid cells revealed by cellular-resolution imaging</article-title>’, <source>Neuron</source> <volume>84</volume>, <fpage>1079</fpage>–<lpage>1090</lpage>. <bold>URL:</bold> <ext-link>https://linkinghub.elsevier.com/retrieve/pii/S0896627314009684</ext-link></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Jayakumar</surname>, <given-names>R. P.</given-names></string-name>, <string-name><surname>Madhav</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Savelli</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Blair</surname>, <given-names>H. T.</given-names></string-name>, <string-name><surname>Cowan</surname>, <given-names>N. J.</given-names></string-name> &amp; <string-name><surname>Knierim</surname>, <given-names>J. J.</given-names></string-name> (<year>2019</year>), ‘<article-title>Recalibration of path integration in hippocampal place cells</article-title>’, <source>Nature</source> <volume>566</volume>, <fpage>533</fpage>–<lpage>537</lpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.1038/s41586-019-0939-3</pub-id> <ext-link>http://www.nature.com/articles/s41586-019-0939-3</ext-link></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Jordan</surname>, <given-names>J. T.</given-names></string-name>, <string-name><surname>McDermott</surname>, <given-names>K. D.</given-names></string-name>, <string-name><surname>Frechou</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Shtrahman</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Gonçalves</surname>, <given-names>J. T.</given-names></string-name> (<year>2021</year>), ‘<article-title>Treadmill-based task for assessing spatial memory in head-fixed mice</article-title>.’, <source>STAR protocols</source> <volume>2</volume>, <fpage>100770</fpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Kaifosh</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Lovett-Barron</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Turi</surname>, <given-names>G. F.</given-names></string-name>, <string-name><surname>Reardon</surname>, <given-names>T. R.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2013</year>), ‘<article-title>Septo-hippocampal GABAergic signaling across multiple modalities in awake mice</article-title>’, <source>Nature Neuroscience</source> <volume>16</volume>(<issue>9</issue>), <fpage>1182</fpage>–<lpage>1184</lpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.1038/nn.3482</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="web"><string-name><surname>Kaifosh</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Zaremba</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Danielson</surname>, <given-names>N. B.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2014</year>), ‘<article-title>Sima: Python software for analysis of dynamic fluorescence imaging data</article-title>’, <source>Frontiers in Neuroinformatics</source> <volume>8</volume>. <bold>URL:</bold> <ext-link ext-link-type="uri" xlink:href="http://journal.frontiersin.org/article/10.3389/fninf.2014.00080/abstract">http://journal.frontiersin.org/article/10.3389/fninf.2014.00080/abstract</ext-link></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Kaufman</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Geiller</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2020a</year>), ‘<article-title>A role for the locus coeruleus in hippocampal ca1 place cell reorganization during spatial reward learning</article-title>’, <source>Neuron</source> <volume>105</volume>(<issue>6</issue>), <fpage>1018</fpage>–<lpage>1026.e4</lpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.1016/j.neuron.2019.12.029</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Kaufman</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Geiller</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2020b</year>), ‘<article-title>A role for the locus coeruleus in hippocampal CA1 place cell reorganization during spatial reward learning</article-title>’, <source>Neuron</source> <volume>105</volume>(<issue>6</issue>), <fpage>1018</fpage>–<lpage>1026.e4</lpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.1016/j.neuron.2019.12.029</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Leutgeb</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Leutgeb</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Barnes</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Moser</surname>, <given-names>E. I.</given-names></string-name>, <string-name><surname>McNaughton</surname>, <given-names>B. L.</given-names></string-name> &amp; <string-name><surname>Moser</surname>, <given-names>M.-B.</given-names></string-name> (<year>2005</year>), ‘<article-title>Independent codes for spatial and episodic memory in hippocampal neuronal ensembles</article-title>’, <source>Science</source> <volume>309</volume>, <fpage>619</fpage>–<lpage>623</lpage>. <bold>URL:</bold> <ext-link>https://www.science.org/doi/10.1126/science.1114037</ext-link></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Lovett-Barron</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kaifosh</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kheirbek</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Danielson</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Zaremba</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Reardon</surname>, <given-names>T. R.</given-names></string-name>, <string-name><surname>Turi</surname>, <given-names>G. F.</given-names></string-name>, <string-name><surname>Hen</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Zemelman</surname>, <given-names>B. V.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2014</year>), ‘<article-title>Dendritic inhibition in the hippocampus supports fear learning</article-title>’, <source>Science</source> <volume>343</volume>(<issue>6173</issue>), <fpage>857</fpage>–<lpage>863</lpage>. <bold>URL:</bold> <ext-link>https://www.science.org/doi/abs/10.1126/science.1247485</ext-link></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Mauk</surname>, <given-names>M. D.</given-names></string-name> &amp; <string-name><surname>Buonomano</surname>, <given-names>D. V.</given-names></string-name> (<year>2004</year>), ‘<article-title>The neural basis of temporal processing</article-title>’, <source>Annual Review of Neuroscience</source> <volume>27</volume>(<issue>1</issue>), <fpage>307</fpage>–<lpage>340</lpage>. PMID: <pub-id pub-id-type="doi">15217335</pub-id>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144247</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Muller</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Kubie</surname>, <given-names>J.</given-names></string-name> (<year>1987</year>), ‘<article-title>The effects of changes in the environment on the spatial firing of hippocampal complex-spike cells</article-title>’, <source>The Journal of Neuroscience</source> <volume>7</volume>, <fpage>1951</fpage>–<lpage>1968</lpage>. <bold>URL:</bold> <ext-link>https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.07-07-01951.1987</ext-link></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>O’Hare</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Gonzalez</surname>, <given-names>K. C.</given-names></string-name>, <string-name><surname>Herrlinger</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Hirabayashi</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hewitt</surname>, <given-names>V. L.</given-names></string-name>, <string-name><surname>Blockus</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Szoboszlay</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rolotti</surname>, <given-names>S. V.</given-names></string-name>, <string-name><surname>Geiller</surname>, <given-names>T. C.</given-names></string-name>, <string-name><surname>Negrean</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chelur</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Polleux</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2022</year>), <article-title>‘Compartment-specific tuning of dendritic feature selectivity by intracellular ca2+ release</article-title>.’, <source>Science (New York, N.Y.)</source> <volume>375</volume>, <fpage>eabm1670</fpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>O’Keefe</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Dostrovsky</surname>, <given-names>J.</given-names></string-name> (<year>1971</year>), ‘<article-title>The hippocampus as a spatial map. preliminary evidence from unit activity in the freely-moving rat</article-title>’, <source>Brain Research</source> <volume>34</volume>, <fpage>171</fpage>–<lpage>175</lpage>. <bold>URL:</bold> <ext-link>https://linkinghub.elsevier.com/retrieve/pii/0006899371903581</ext-link></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Stringer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Dipoppa</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Schröder</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Rossi</surname>, <given-names>L. F.</given-names></string-name>, <string-name><surname>Dalgleish</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name> (<year>2017</year>), ‘<article-title>Suite2p: beyond 10,000 neurons with standard two-photon microscopy</article-title>’, <source>bioRxiv. <bold>URL</bold></source>: <pub-id pub-id-type="doi">10.1101/061507</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Priestley</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Bowler</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Rolotti</surname>, <given-names>S. V.</given-names></string-name>, <string-name><surname>Fusi</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2022</year>), ‘<article-title>Signatures of rapid plasticity in hippocampal ca1 representations during novel experiences</article-title>’, <source>Neuron</source> <volume>110</volume>(<issue>12</issue>), <fpage>1978</fpage>–<lpage>1992.e6</lpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.1016/j.neuron.2022.03.026</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Ravassard</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kees</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Willers</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ho</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Aharoni</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Cushman</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Aghajan</surname>, <given-names>Z. M.</given-names></string-name> &amp; <string-name><surname>Mehta</surname>, <given-names>M. R.</given-names></string-name> (<year>2013</year>), <article-title>‘Multisensory control of hippocampal spatiotemporal selectivity</article-title>.’, <source>Science (New York, N.Y.)</source> <volume>340</volume>, <fpage>1342</fpage>–<lpage>1346</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Rolotti</surname>, <given-names>S. V.</given-names></string-name>, <string-name><surname>Ahmed</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Szoboszlay</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Geiller</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Negrean</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Blockus</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Gonzalez</surname>, <given-names>K. C.</given-names></string-name>, <string-name><surname>Sparks</surname>, <given-names>F. T.</given-names></string-name>, <string-name><surname>Canales</surname>, <given-names>A. S. S.</given-names></string-name>, <string-name><surname>Tuttman</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Peterka</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Zemelman</surname>, <given-names>B. V.</given-names></string-name>, <string-name><surname>Polleux</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2022b</year>), ‘<article-title>Local feedback inhibition tightly controls rapid formation of hippocampal place fields</article-title>’, <source>Neuron</source> <volume>110</volume>, <fpage>783</fpage>–<lpage>794.e6</lpage>. <bold>URL:</bold> <ext-link>https://linkinghub.elsevier.com/retrieve/pii/S089662732100996X</ext-link></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Rolotti</surname>, <given-names>S. V.</given-names></string-name>, <string-name><surname>Blockus</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Sparks</surname>, <given-names>F. T.</given-names></string-name>, <string-name><surname>Priestley</surname>, <given-names>J. B.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2022a</year>), ‘<article-title>Reorganization of ca1 dendritic dynamics by hippocampal sharp-wave ripples during learning</article-title>.’, <source>Neuron</source> <volume>110</volume>, <fpage>977</fpage>–<lpage>991.e4</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Royer</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Zemelman</surname>, <given-names>B. V.</given-names></string-name>, <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chance</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Magee</surname>, <given-names>J. C.</given-names></string-name> &amp; <string-name><surname>Buzsáki</surname>, <given-names>G.</given-names></string-name> (<year>2012</year>), ‘<article-title>Control of timing, rate and bursts of hippocampal place cells by dendritic and somatic inhibition</article-title>’, <source>Nature Neuroscience</source> <volume>15</volume>(<issue>5</issue>), <fpage>769</fpage>–<lpage>775</lpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.1038/nn.3077</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Sheffield</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Adoff</surname>, <given-names>M. D.</given-names></string-name> &amp; <string-name><surname>Dombeck</surname>, <given-names>D. A.</given-names></string-name> (<year>2017</year>), ‘<article-title>Increased prevalence of calcium transients across the dendritic arbor during place field formation</article-title>’, <source>Neuron</source> <volume>96</volume>(<issue>2</issue>), <fpage>490</fpage>–<lpage>504.e5</lpage>. <bold>URL:</bold> <ext-link>https://www.sciencedirect.com/science/article/pii/S0896627317308735</ext-link></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Sheintuch</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Rubin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Brande-Eilat</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Geva</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Sadeh</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Pinchasof</surname>, <given-names>O.</given-names></string-name> &amp; <string-name><surname>Ziv</surname>, <given-names>Y.</given-names></string-name> (<year>2017</year>), ‘<article-title>Tracking the same neurons across multiple days in ca2+ imaging data</article-title>.’, <source>Cell reports</source> <volume>21</volume>, <fpage>1102</fpage>–<lpage>1115</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Skaggs</surname>, <given-names>W. E.</given-names></string-name> &amp; <string-name><surname>Mcnaughton</surname>, <given-names>B. L.</given-names></string-name> (<year>1998</year>), ‘<article-title>Spatial firing properties of hippocampal ca1 populations in an environment containing two visually identical regions</article-title>’, <source>The Journal of neuroscience</source> <volume>18</volume>, <fpage>8455</fpage>–<lpage>66</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Terada</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Geiller</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Liao</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>O’Hare</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Vancura</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2022</year>), ‘<article-title>Adaptive stimulus selection for consolidation in the hippocampus</article-title>’, <source>Nature</source> <volume>601</volume>(<issue>7892</issue>), <fpage>240</fpage>–<lpage>244</lpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.1038/s41586-021-04118-6</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Tuncdemir</surname>, <given-names>S. N.</given-names></string-name>, <string-name><surname>Grosmark</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Chung</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Luna</surname>, <given-names>V. M.</given-names></string-name>, <string-name><surname>Lacefield</surname>, <given-names>C. O.</given-names></string-name>, <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Hen</surname>, <given-names>R.</given-names></string-name> (<year>2023</year>), ‘<article-title>Adult-born granule cells facilitate remapping of spatial and non-spatial representations in the dentate gyrus</article-title>.’, <source>Neuron</source>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Tuncdemir</surname>, <given-names>S. N.</given-names></string-name>, <string-name><surname>Grosmark</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Turi</surname>, <given-names>G. F.</given-names></string-name>, <string-name><surname>Shank</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bowler</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Ordek</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hen</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Lacefield</surname>, <given-names>C. O.</given-names></string-name> (<year>2022</year>), ‘<article-title>Parallel processing of sensory cue and spatial information in the dentate gyrus</article-title>’, <source>Cell Reports</source> <volume>38</volume>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Turi</surname>, <given-names>G. F.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>W. K.</given-names></string-name>, <string-name><surname>Chavlis</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pandi</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>O’Hare</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Priestley</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Grosmark</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Liao</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Ladow</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>J. F.</given-names></string-name>, <string-name><surname>Zemelman</surname>, <given-names>B. V.</given-names></string-name>, <string-name><surname>Poirazi</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2019</year>), ‘<article-title>Vasoactive intestinal polypeptide-expressing interneurons in the hippocampus support goal-oriented spatial learning</article-title>’, <source>Neuron</source> <volume>101</volume>, <fpage>1150</fpage>–<lpage>1165.e8</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Vancura</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Geiller</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Grosmark</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2023</year>), ‘<article-title>Inhibitory control of sharp-wave ripple duration during learning in hippocampal recurrent networks</article-title>.’, <source>Nature neuroscience</source> <volume>26</volume>, <fpage>788</fpage>–<lpage>797</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Warren</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Hoffman</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>E. Y.</given-names></string-name>, <string-name><surname>Hong</surname>, <given-names>Y. K.</given-names></string-name>, <string-name><surname>Bruno</surname>, <given-names>R. M.</given-names></string-name> &amp; <string-name><surname>Sawtell</surname>, <given-names>N. B.</given-names></string-name> (<year>2021</year>), ‘<article-title>A rapid whisker-based decision underlying skilled locomotion in mice</article-title>’, <source>eLife</source> <volume>10</volume>, <fpage>e63596</fpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.7554/eLife.63596</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Zaremba</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Diamantopoulou</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Danielson</surname>, <given-names>N. B.</given-names></string-name>, <string-name><surname>Grosmark</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Kaifosh</surname>, <given-names>P. W.</given-names></string-name>, <string-name><surname>Bowler</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Liao</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Sparks</surname>, <given-names>F. T.</given-names></string-name>, <string-name><surname>Gogos</surname>, <given-names>J. A.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> (<year>2017</year>), ‘<article-title>Impaired hippocampal place cell dynamics in a mouse model of the 22q11.2 deletion</article-title>’, <source>Nature Neuroscience</source> <volume>20</volume>(<issue>11</issue>), <fpage>1612</fpage>–<lpage>1623</lpage>. <bold>URL:</bold> <pub-id pub-id-type="doi">10.1038/nn.4634</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ginzburg</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>McNaughton</surname>, <given-names>B. L.</given-names></string-name> &amp; <string-name><surname>Sejnowski</surname>, <given-names>T. J.</given-names></string-name> (<year>1998</year>), ‘<article-title>Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells</article-title>.’, <source>Journal of neurophysiology</source> <volume>79</volume>, <fpage>1017</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
</ref-list>
<sec id="s11">
<label>10</label><title>Supplementary Materials</title>
<sec id="s11a">
<label>10.1</label><title>Table of Contents</title>
<list list-type="bullet">
<list-item><p><bold>Link to website for quickstart guides and file downloads:</bold> <ext-link ext-link-type="uri" xlink:href="https://www.losonczylab.org/behaviormate">https://www.losonczylab.org/behaviormate</ext-link></p></list-item>
<list-item><p><bold>Link to example settings files:</bold> <ext-link ext-link-type="uri" xlink:href="https://github.com/losonczylab/behaviorMate/tree/main/example_settings_files">https://github.com/losonczylab/behaviorMate/tree/main/example_settings_files</ext-link></p></list-item>
<list-item><p><bold>Link to source code for the UI:</bold> <ext-link ext-link-type="uri" xlink:href="https://github.com/losonczylab/behaviorMate">https://github.com/losonczylab/behaviorMate</ext-link></p></list-item>
<list-item><p><bold>Link to online java documentation for the UI:</bold> <ext-link ext-link-type="uri" xlink:href="https://www.losonczylab.org/behaviorMate-1.0.0">https://www.losonczylab.org/behaviorMate-1.0.0</ext-link></p></list-item>
<list-item><p><bold>Link to electronics schematics:</bold> <ext-link ext-link-type="uri" xlink:href="https://github.com/losonczylab/Hardware/tree/main/Electronics">https://github.com/losonczylab/Hardware/tree/main/Electronics</ext-link></p></list-item>
<list-item><p><bold>Link to treadmill and VR CAD files:</bold> <ext-link ext-link-type="uri" xlink:href="https://github.com/losonczylab/Hardware/tree/main">https://github.com/losonczylab/Hardware/tree/main</ext-link></p></list-item>
</list>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1.</label>
<caption><title>Alternative VR system</title>
<p>(<bold>A</bold>) A version of the VR system that can better fit in tight spaces. Can be made to be self-contained in the event it needs to be moved or swapped with other systems. (<bold>B</bold>) Fixes the position of the mouse’s head by tightly clamping onto each side of the bar implanted into its skull. The screws used to close the two clamps are not shown.</p></caption>
<graphic xlink:href="569989v4_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2.</label>
<caption><title>Additional place cell properties</title>
<p><bold>(A)</bold> Proportion of place cells in VR and TM. VR had significantly less ROIs that were classified as a place cell compared to TM (two sample z-test, F=14.96 p=1.27e<italic><sup>−</sup></italic><sup>50</sup>). <bold>(B)</bold> Proportion of place cells in each context-environment pair. <bold>(C)</bold> Mean velocity per lap in each context-environment pair. Significant main effect of environment (F(1,5)=35.95, p=0.0019). <bold>(D)</bold> Peak location of place cells recorded in each context-environment pair. Note that place cells tile entire track in all pairs.</p></caption>
<graphic xlink:href="569989v4_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Table ST1.</label><caption><title>ANOVA Results</title></caption>
<graphic xlink:href="569989v4_tbls1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97433.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kemere</surname>
<given-names>Caleb</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Rice University</institution>
</institution-wrap>
<city>Houston</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This work represents a new toolkit for implementing virtual reality experiments in head-fixed animals. It is a <bold>valuable</bold> contribution to the field and the evidence for its utility and performance is <bold>solid</bold>. Some minor improvements in the material presented - including clarifying design decisions and providing more details about design features - would improve the readability and thereby potentially increase its impact.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97433.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Bowler et al. present a thoroughly tested system for modularized behavioral control of navigation-based experiments, particularly suited for pairing with 2-photon imaging but applicable to a variety of techniques. This system, which they name behaviorMate, represents a valuable contribution to the field. As the authors note, behavioral control paradigms vary widely across laboratories in terms of hardware and software utilized and often require specialized technical knowledge to make changes to these systems. Having a standardized, easy-to-implement, and flexible system that can be used by many groups is therefore highly desirable. This work will be of interest to systems neuroscientists looking to integrate flexible head-fixed behavioral control with neural data acquisition.</p>
<p>Strengths:</p>
<p>The present manuscript provides compelling evidence of the functionality and applicability of behaviorMate. The authors report benchmark tests for real-time update speed between the animal's movement and the behavioral control, on both the treadmill-based and virtual reality (VR) setups. Further, they nicely demonstrate and quantify reliable hippocampal place cell coding in both setups, using synchronized 2-photon imaging. This place cell characterization also provides a concrete comparison between the place cell properties observed in treadmill-based navigation vs. visual VR in a single study, which itself is a helpful contribution to the field.</p>
<p>Documentation for installing and operating behaviorMate is available via the authors' lab website and linked in the manuscript.</p>
<p>Weaknesses:</p>
<p>The following comments are mostly minor suggestions intended to add clarity to the paper and provide context for its significance.</p>
<p>(1) As VRMate (a component of behaviorMate) is written using Unity, what is the main advantage of using behaviorMate/VRMate compared to using Unity alone paired with Arduinos (e.g. Campbell et al. 2018), or compared to using an existing toolbox to interface with Unity (e.g. Alsbury-Nealy et al. 2022, DOI: 10.3758/s13428-021-01664-9)? For instance, one disadvantage of using Unity alone is that it requires programming in C# to code the task logic. It was not entirely clear whether VRMate circumvents this disadvantage somehow -- does it allow customization of task logic and scenery in the GUI? Does VRMate add other features and/or usability compared to Unity alone? It would be helpful if the authors could expand on this topic briefly.</p>
<p>(2) The section on &quot;context lists&quot;, lines 163-186, seemed to describe an important component of the system, but this section was challenging to follow and readers may find the terminology confusing. Perhaps this section could benefit from an accompanying figure or flow chart, if these terms are important to understand.</p>
<p>(2a) Relatedly, &quot;context&quot; is used to refer to both when the animal enters a particular state in the task like a reward zone (&quot;reward context&quot;, line 447) and also to describe a set of characteristics of an environment (Figure 3G), akin to how &quot;context&quot; is often used in the navigation literature. To avoid confusion, one possibility would be to use &quot;environment&quot; instead of &quot;context&quot; in Figure 3G, and/or consider using a word like &quot;state&quot; instead of &quot;context&quot; when referring to the activation of different stimuli.</p>
<p>(3) Given the authors' goal of providing a system that is easily synchronizable with neural data acquisition, especially with 2-photon imaging, I wonder if they could expand on the following features:</p>
<p>(3a) The authors mention that behaviorMate can send a TTL to trigger scanning on the 2P scope (line 202), which is a very useful feature. Can it also easily generate a TTL for each frame of the VR display and/or each sample of the animal's movement? Such TTLs can be critical for synchronizing the imaging with behavior and accounting for variability in the VR frame rate or sampling rate.</p>
<p>(3b) Is there a limit to the number of I/O ports on the system? This might be worth explicitly mentioning.</p>
<p>(3c) In the VR version, if each display is run by a separate Android computer, is there any risk of clock drift between displays? Or is this circumvented by centralized control of the rendering onset via the &quot;real-time computer&quot;?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97433.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors present behaviorMate, an open-source behavior recording and control system including a central GUI and compatible treadmill and display components. Notably, the system utilizes the &quot;Intranet of things&quot; scheme and the components communicate through a local network, making the system modular, which in turn allows user to easily configure the setup to suit their experimental needs. Overall, behaviorMate is a valuable resource for researchers performing head-fixed imaging studies, as the commercial alternatives are often expensive and inflexible to modify.</p>
<p>Strengths and Weaknesses:</p>
<p>The manuscript presents two major utilities of behaviorMate: (1) as an open-source alternative to commercial behavior apparatus for head-fixed imaging studies, and (2) as a set of generic schema and communication protocols that allows the users to incorporate arbitrary recording and stimulation devices during a head-fixed imaging experiment. I found the first point well-supported and demonstrated in the manuscript. Indeed, the documentation, BOM, CAD files, circuit design, source, and compiled software, along with the manuscript, create an invaluable resource for neuroscience researchers looking to set up a budget-friendly VR and head-fixed imaging rig. Some features of behaviorMate, including the computer vision-based calibration of the treadmill, and the decentralized, Android-based display devices, are very innovative approaches and can be quite useful in practical settings. However, regarding the second point, my concern is that there is not adequate documentation and design flexibility to allow the users to incorporate arbitrary hardware into the system. In particular:</p>
<p>(1) The central controlling logic is coupled with GUI and an event loop, without a documented plugin system. It's not clear whether arbitrary code can be executed together with the GUI, hence it's not clear how much the functionality of the GUI can be easily extended without substantial change to the source code of the GUI. For example, if the user wants to perform custom real-time analysis on the behavior data (potentially for closed-loop stimulation), it's not clear how to easily incorporate the analysis into the main GUI/control program.</p>
<p>(2) The JSON messaging protocol lacks API documentation. It's not clear what the exact syntax is, supported key/value pairs, and expected response/behavior of the JSON messages. Hence, it's not clear how to develop new hardware that can communicate with the behaviorMate system.</p>
<p>(3) It seems the existing control hardware and the JSON messaging only support GPIO/TTL types of input/output, which limits the applicability of the system to more complicated sensor/controller hardware. The authors mentioned that hardware like Arduino natively supports serial protocols like I2C or SPI, but it's not clear how they are handled and translated to JSON messages.</p>
<p>Additionally, because it's unclear how easy to incorporate arbitrary hardware with behaviorMate, the &quot;Intranet of things&quot; approach seems to lose attraction. Since currently, the manuscript focuses mainly on a specific set of hardware designed for a specific type of experiment, it's not clear what are the advantages of implementing communication over a local network as opposed to the typical connections using USB.</p>
<p>In summary, the manuscript presents a well-developed open-source system for head-fixed imaging experiments with innovative features. The project is a very valuable resource to the neuroscience community. However, some claims in the manuscript regarding the extensibility of the system and protocol may require further development and demonstration.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97433.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this work, the authors present an open-source system called behaviourMate for acquiring data related to animal behavior. The temporal alignment of recorded parameters across various devices is highlighted as crucial to avoid delays caused by electronics dependencies. This system not only addresses this issue but also offers an adaptable solution for VR setups. Given the significance of well-designed open-source platforms, this paper holds importance.</p>
<p>Advantages of behaviorMate:</p>
<p>The cost-effectiveness of the system provided.</p>
<p>The reliability of PCBs compared to custom-made systems.</p>
<p>Open-source nature for easy setup.</p>
<p>Plug &amp; Play feature requiring no coding experience for optimizing experiment performance (only text-based Json files, 'context List' required for editing).</p>
<p>Points to clarify:</p>
<p>While using UDP for data transmission can enhance speed, it is thought that it lacks reliability. Are there error-checking mechanisms in place to ensure reliable communication, given its criticality alongside speed?</p>
<p>Considering this year's price policy changes in Unity, could this impact the system's operations?</p>
<p>Also, does the Arduino offer sufficient precision for ephys recording, particularly with a 10ms check?</p>
<p>Could you clarify the purpose of the Sync Pulse? In line 291, it suggests additional cues (potentially represented by the Sync Pulse) are needed to align the treadmill screens, which appear to be directed towards the Real-Time computer. Given that event alignment occurs in the GPIO, the connection of the Sync Pulse to the Real-Time Controller in Figure 1 seems confusing. Additionally, why is there a separate circuit for the treadmill that connects to the UI computer instead of the GPIO? It might be beneficial to elaborate on the rationale behind this decision in line 260. Moreover, should scenarios involving pupil and body camera recordings connect to the Analog input in the PCB or the real-time computer for optimal data handling and processing?</p>
<p>Given that all references, as far as I can see, come from the same lab, are there other labs capable of implementing this system at a similar optimal level?</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97433.1.sa4</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bowler</surname>
<given-names>John C</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0498-5743</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Zakka</surname>
<given-names>George</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0005-0035-6749</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Yong</surname>
<given-names>Hyun Choong</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1841-6317</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Wenke</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rao</surname>
<given-names>Bovey</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2280-0119</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Liao</surname>
<given-names>Zhenrui</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Priestley</surname>
<given-names>James B</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Losonczy</surname>
<given-names>Attila</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7064-0252</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We thank the reviewers for their comments and will revise the manuscript to provide more comprehensive clarifications to aide readers’ understanding of behaviorMate. Additionally, we intend to take several steps which could provide further insights and improve the ease of use for new behaviorMate users: (1) to release an expanded and annotated library of existing settings and VR scene files, (2) improve the online documentation of context lists and decorators which allow behaviorMate to run custom experimental paradigms without writing code, and (3) release online API details of the JSON messaging protocol that is used between behaviorMate, the Arduinos, and the VRMate program which could be especially helpful to developers interested in expanding or modifying the system. Here we provide a few brief points of clarification to some of the concerns raised by the reviewers.</p>
<p>Firstly, we clarify the system’s focus on modularity and flexibility. behaviorMate leverages the “Intranet of Things” framework to provide a low-cost platform that relies on asynchronous message passing between independent networked devices. While our current VR implementation typically involves a PC, 2 Arduinos, and an Android device per VR display, the behaviorMate GUI can be configured without editing any source code to listen on additional ports for UDP messages which will be automatically timestamped and logged. Since the current implementation of the behaviorMate GUI can be configured through the settings file to send and receive JSON-formatted messages on arbitrary ports, third-party devices could be configured to listen and respond to these messages also without editing the UI source code. More specialized responsibilities or tasks that require higher temporal precision (such as position tracking) are handled by dedicated circuits so as to not overload the general purpose one. This provides a level of encapsulation/separation of concerns since components can be optimized for performance of a single tasks—a feature that is especially desirable given resource limitations on the most common commercially available microcontrollers.</p>
<p>A number of methods exist for synchronizing recording devices like microscopes or electrophysiology recordings with behaviorMate’s time-stamped logs of actuators and sensors. For example, the GPIO circuit can be configured to send sync triggers, or receive timing signals as input, alternatively a dedicated circuit could record frame start signals and relay them to the PC to be logged indecently of the GPIO (enabling a high-resolution post-hoc alignment of the time stamps). The optimal method to use varies based on the needs of the experiment. For example, if very high temporal precision is needed, such as during electrophysiology experiments, a high-speed data acquisition (DAQ) circuit to capture a fixed interval readout might be beneficial. behaviorMate could still be set up as normal to provide closed and open-loop task control at behaviorally relevant timescales alongside a DAQ circuit recording events at a consistent temporal resolution. While this would increase the relative cost of the recording setup, identical rigs for training animals could still be configured without the DAQ circuit avoiding the additional cost and complexity.</p>
<p>VRMate provides the interface between Unity and behaviorMate—therefore using the two systems together mean that no Unity or C# programming is necessary. VRMate provides a prespecified set of visual cues that can be scaled in 3 dimensions and have textures applied to them, permitting a wide variety of different scenes to be displayed. All VRMate scene details are additionally logged by behaviorMate to allow for consistency checks across experiments. The VRMate project also includes “editor scripts” that provide a drag-and-drop utility in Unity Editor for developing new scenes. Since the details pertaining to specific scenes and view angle are loaded at runtime via JSON-formatted UDP messages, it is not necessary to recompile VRMate in order to use this feature. Since we send individual position updates to VRMate from the PC, any issues with clock drift would be limited to the refresh rate of the Unity program that fast enough to be perceived as instantaneous and we have thoroughly tested the timing differences between displays using high-speed cameras and found them to be negligible. While we find using 5 separate Android computers to render scenes as described an optimal solution to maximize flexibility, it would also be possible to render all scenes on a single PC to further mitigate this concern depending on experimental demands. Finally, our treadmill implementations of behaviorMate use no monitor displays, however due to the modular design of behaviorMate virtual cues could be seamlessly added by added to any such setup by a VR context to the settings files.</p>
<p>One last point to mention is that while our project is not affected by the recent changes in pricing structure of the Unity project, since the compiled software does not need to be regenerated to update VR scenes, or implement new task logic since this is handled by the behaviorMate GUI. This means the current state of the VRMate program is robust to any future pricing changes or other restructuring of the Unity program and does not rely on continued support of Unity. Additionally, the solution presented in VRMate has many benefits, however, a developer could easily adapt any open-source VR Maze project to receive the UDP-based position updates from behaviorMate or develop their own novel VR solutions. We intend to update the VR section of the manuscript to make all of this information clearer in the document as well as to provide the additional online documentation in the materials linked in the supplemental information.</p>
</body>
</sub-article>
</article>