<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">84141</article-id>
<article-id pub-id-type="doi">10.7554/eLife.84141</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.84141.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Endotaxis: A neuromorphic algorithm for mapping, goal-learning, navigation, and patrolling</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Tony</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rosenberg</surname>
<given-names>Matthew</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jing</surname>
<given-names>Zeyu</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Perona</surname>
<given-names>Pietro</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Meister</surname>
<given-names>Markus</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>meister@caltech.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution>Division of Biology and Biological Engineering, California Institute of Technology</institution></aff>
<aff id="a2"><label>2</label><institution>Division of Engineering and Applied Science, California Institute of Technology</institution></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>École Normale Supérieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Behrens</surname>
<given-names>Timothy E</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="others"><p>{<email>tonyzhang@caltech.edu</email>, <email>zjing@caltech.edu</email>, <email>perona@caltech.edu</email>, <email>meister@caltech.edu</email>}, <email>mhr3@princeton.edu</email></p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-03-24">
<day>24</day>
<month>03</month>
<year>2023</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2023-11-15">
<day>15</day>
<month>11</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP84141</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2022-11-09">
<day>09</day>
<month>11</month>
<year>2022</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2022-10-10">
<day>10</day>
<month>10</month>
<year>2022</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.09.24.461751"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2023-03-24">
<day>24</day>
<month>03</month>
<year>2023</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.84141.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.84141.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.84141.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.84141.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.84141.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Zhang et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Zhang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-84141-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>An animal entering a new environment typically faces three challenges: explore the space for resources, memorize their locations, and navigate towards those targets as needed. Experimental work on exploration, mapping, and navigation has mostly focused on simple environments – such as an open arena [<xref ref-type="bibr" rid="c68">68</xref>], a pond [<xref ref-type="bibr" rid="c42">42</xref>], or a desert [<xref ref-type="bibr" rid="c44">44</xref>] – and much has been learned about neural signals in diverse brain areas under these conditions [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c54">54</xref>]. However, many natural environments are highly structured, such as a system of burrows, or of intersecting paths through the underbrush. Similarly, for many cognitive tasks, a sequence of simple actions can give rise to complex solutions. Here we propose an algorithm that learns the structure of a complex environment, discovers useful targets during exploration, and navigates back to those targets by the shortest path. It makes use of a behavioral module common to all motile animals, namely the ability to follow an odor to its source [<xref ref-type="bibr" rid="c4">4</xref>]. We show how the brain can learn to generate internal “virtual odors” that guide the animal to any location of interest. This <italic>endotaxis</italic> algorithm can be implemented with a simple 3-layer neural circuit using only biologically realistic structures and learning rules. Several neural components of this scheme are found in brains from insects to humans. Nature may have evolved a general mechanism for search and navigation on the ancient backbone of chemotaxis.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>New section on nonlinear processing (Fig 7). New co-author. Extended discussion.</p></fn>
</fn-group>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/markusmeister/Endotaxis-2023">https://github.com/markusmeister/Endotaxis-2023</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Animals navigate their environment to look for resources – such as shelter, food, or a mate – and to exploit such resources once they are found. Efficient navigation requires knowing the structure of the environment: which locations are connected to which others [<xref ref-type="bibr" rid="c64">64</xref>]. One would like to understand how the brain acquires that knowledge, what neural representation it adopts for the resulting map, how it tags significant locations in that map, and how that knowledge gets read out for decision-making during navigation. Here we propose a mechanism that solves all these problems and operates reliably in diverse and complex environments.</p>
<p>One algorithm for finding a valuable resource is common to all animals: chemotaxis. Every motile species has a way to track odors through the environment, either to find the source of the odor or to avoid it [<xref ref-type="bibr" rid="c4">4</xref>]. This ability is central to finding food, connecting with a mate, and avoiding predators. It is believed that brains originally evolved to organize the motor response in pursuit of chemical stimuli. Indeed some of the oldest regions of the mammalian brain, including the hippocampus, seem organized around an axis that processes smells [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c31">31</xref>].</p>
<p>The specifics of chemotaxis, namely the methods for finding an odor and tracking it, vary by species, but the toolkit always includes a search strategy based on trial-and-error: Try various actions that you have available, then settle on the one that makes the odor stronger [<xref ref-type="bibr" rid="c4">4</xref>]. For example a rodent will weave its head side-to-side, sampling the local odor gradient, then move in the direction where the smell is stronger. Worms and maggots follow the same strategy. Dogs track a ground-borne odor trail by casting across it side-to-side. Flying insects perform similar casting flights. Bacteria randomly change direction every now and then, and continue straight as long as the odor improves [<xref ref-type="bibr" rid="c5">5</xref>]. We propose that this universal behavioral module for chemotaxis can be harnessed to solve general problems of search and navigation in a complex environment, even when tell-tale odors are not available.</p>
<p>For concreteness, consider a mouse exploring a labyrinth of tunnels (<xref rid="fig1" ref-type="fig">Fig 1A</xref>). The maze may contain a source of food that emits an odor (<xref rid="fig1" ref-type="fig">Fig 1A1</xref>). That odor will be strongest at the source and decline with distance along the tunnels of the maze. The mouse can navigate to the food location by simply following the odor gradient uphill. Suppose that the mouse discovers some other interesting locations that <italic>do not</italic> emit a smell, like a source of water, or the exit from the labyrinth (<xref rid="fig1" ref-type="fig">Fig 1A2</xref>-<xref rid="fig3" ref-type="fig">3</xref>). It would be convenient if the mouse could tag such a location with an odorous material, so it may be found easily on future occasions. Ideally, the mouse would carry with it multiple such odor tags, so it can mark different targets each with its specific recognizable odor.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>A mechanism for endotaxis.</title>
<p><bold>A:</bold> A constrained environment of tunnels linked by intersections, with special locations offering food, water, and the exit. <bold>1:</bold> A real odor emitted by the food source decreases with distance (shading). <bold>2:</bold> A virtual odor tagged to the water source. <bold>3:</bold> A virtual odor tagged to the exit. <bold>4:</bold> Abstract representation of this environment by a graph of nodes (intersections) and edges (tunnels). <bold>B:</bold> A neural circuit to implement endotaxis. Open circles: four populations of neurons that represent “resource”, “point”, “map”, and “goal”. Arrows: signal flow. Solid circles: synapses. Point cells have small receptive fields localized in the environment and excite map cells. Map cells excite each other (green synapses) and also excite goal cells (blue synapses). Resource cells signal the presence of a resource, e.g. cheese, water, or the exit. Map synapses and goal synapses are modified by activity-dependent plasticity. A “mode” switch selects among various goal signals depending on the animal’s need. They may be virtual odors (water, exit) or real odors (cheese). Another goal cell (clock) may report how recently the agent has visited a location. The output of the mode switch gets fed to the chemotaxis module for gradient ascent. Mathematical symbols used in the text: <italic>u</italic><sub><italic>i</italic></sub> is the output of a point cell at location <italic>i, w</italic><sub><italic>i</italic></sub> is the input to the corresponding map cell, <italic>v</italic><sub><italic>i</italic></sub> is the output of that map cell, <bold>M</bold> is the matrix of synaptic weights among map cells, <bold>G</bold> are the synaptic weights from the map cells onto goal cells, and <italic>r</italic><sub><italic>k</italic></sub> is the output of goal cell <italic>k</italic>.</p></caption>
<graphic xlink:href="461751v3_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Here we show that such tagging does not need to be physical. Instead we propose a mechanism by which the mouse’s brain may compute a “virtual odor” signal that declines with distance from a chosen target. That neural signal can be made available to the chemotaxis module as though it were a real odor, enabling navigation up the gradient towards the target. Because this goal signal is computed in the brain rather than sensed externally, we call this hypothetical process <italic>endotaxis</italic>.</p>
<p>The developments reported here were inspired by a recent experimental study with mice navigating a complex labyrinth [<xref ref-type="bibr" rid="c49">49</xref>] that includes 63 three-way junctions. Among other things, we observed that mice could learn the location of a resource in the labyrinth after encountering it just once, and perfect a direct route to that target location after <italic>∼</italic>10 encounters. Furthermore, they could navigate back out of the labyrinth using a direct route they had not traveled before, even on the first attempt. Finally, the animals spent most of their waking time patrolling the labyrinth, even long after they had perfected the routes to rewarding locations. These patrols covered the environment efficiently, avoiding repeat visits to the same location. All this happened within a few hours of the animal’s first encounter with the labyrinth. Our modeling efforts here are aimed at explaining these remarkable phenomena of rapid spatial learning in a new environment: one-shot learning of a goal location, zero-shot learning of a return route, and efficient patrolling of a complex maze. In particular we want to do so with a biologically plausible mechanism that could be built out of neurons.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<sec id="s2a">
<label>2.1</label>
<title>A neural circuit to implement endotaxis</title>
<p><xref rid="fig1" ref-type="fig">Figure 1B</xref> presents a neural circuit model that implements three goals: mapping the connectivity of the environment; tagging of goal locations with a virtual odor; and navigation towards those goals. The model includes four types of neurons: resource cells, point cells, map cells, and goal cells.</p>
<sec id="s2a1">
<title>Resource cells</title>
<p>These are sensory neurons that fire when the animal encounters an interesting resource, for example water or food, that may form a target for future navigation. Each resource cell is selective for a specific kind of stimulus. The circuitry that produces these responses is not part of the model.</p>
</sec>
<sec id="s2a2">
<title>Point cells</title>
<p>This layer of cells represents the animal’s location. <sup><xref ref-type="fn" rid="fn1">1</xref></sup> Each neuron in this population has a small response field within the environment. The neuron fires when the animal enters that response field. We assume that these point cells exist from the outset as soon as the animal enters the environment. Each cell’s response field is defined by some conjunction of external and internal sensory signals at that location.</p>
</sec>
<sec id="s2a3">
<title>Map cells</title>
<p>This layer of neurons learns the structure of the environment, namely how the various locations are connected in space. The map cells get excitatory input from point cells in a one-to-one fashion. These input synapses are static. The map cells also excite each other with all-to-all connections. These recurrent synapses are modifiable according to a local plasticity rule. After learning, they represent the topology of the environment.</p>
</sec>
<sec id="s2a4">
<title>Goal cells</title>
<p>Each goal cell serves to mark the locations of a special resource in the map of the environment. The goal cell receives excitatory input from a resource cell, which gets activated whenever that resource is present. It also receives excitatory synapses from map cells. Those synapses are strengthened when the presynaptic map cell is active at the same time as the resource cell.</p>
<p>After the map and goal synapses have been learned, each goal cell carries a virtual odor signal for its assigned resource. The signal increases systematically as the animal moves closer to a location with that resource. A mode switch selects one among many possible virtual odors (or real odors) to be routed to the chemotaxis module for odor tracking. <sup><xref ref-type="fn" rid="fn2">2</xref></sup> The animal then pursues its chemotaxis search strategy to maximize that odor, which leads it to the selected tagged location.</p>
<sec id="s2a4a">
<label>2.1.1</label>
<title>Why does the circuit work?</title>
<p>The key insight is that the output of the goal cell declines systematically with the distance of the animal from the target location. This relationship holds even if the environment is constrained with a complex connectivity graph (<xref rid="fig1" ref-type="fig">Fig 1A4</xref>). Here we explain how this comes about, with mathematical details to follow.</p>
<p>In a first phase, the animal explores the environment while the circuit builds a map. When the animal moves from one location to an adjacent one, those two point cells fire in rapid succession. That leads to a Hebbian strengthening of the excitatory synapses between the two corresponding map cells (<xref rid="fig2" ref-type="fig">Fig 2A-B</xref>). In this way the recurrent network of map cells learns the connectivity of the graph that describes the environment. To a first approximation, the matrix of synaptic connections among the map cells will converge to the correlation matrix of their inputs [<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c21">21</xref>], which in turn reflects the adjacency matrix of the graph (<xref ref-type="disp-formula" rid="eqn1">Eqn 1</xref>). Now the brain can use this adjacency information to find the shortest path to a target.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>The phases of endotaxis during exploration, goal-tagging, and navigation.</title>
<p>A portion of the circuit in <xref rid="fig1" ref-type="fig">Figure 1</xref> is shown, including a single goal cell that responds to the water resource. Bottom shows a graph of the environment, with nodes linked by edges, and the agent’s current location shaded in orange. Each node has a point cell that reports the presence of the agent to a corresponding map cell. Map cells are recurrently connected (green) and feed convergent signals onto the goal cell. <bold>A:</bold> Initially the recurrent synapses are weak (empty circles). <bold>B:</bold> During exploration the agent moves between two adjacent nodes on the graph, and that strengthens (arrowhead) the connection between their corresponding map cells (filled circles). <bold>C:</bold> After exploration the map synapses reflect the connectivity of the graph. Now the map cells have an extended profile of activity (darker = more active), centered on the agent’s current location <italic>x</italic> and decreasing from there with distance on the graph. <bold>D:</bold> When the agent reaches the water source <italic>y</italic> the goal cell gets activated by the sensation of water, and this triggers plasticity (arrowhead) at its input synapses. Thus the state of the map at the water location gets stored in the goal synapses. This event represents tagging of the water location. <bold>E:</bold> During navigation, as the agent visits different nodes, the map state gets filtered through the goal synapses to excite the goal cell. This produces a signal in the goal cell that declines with the agent’s distance from the water location.</p></caption>
<graphic xlink:href="461751v3_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>After this map learning, the output of the map network is a hump of activity, centered on the current location <italic>x</italic> of the animal and declining with distance along the various paths in the graph of the environment (<xref rid="fig2" ref-type="fig">Fig 2C</xref>). If the animal moves to a different location <italic>y</italic>, the map output will change to another hump of activity, now centered on <italic>y</italic> (<xref rid="fig2" ref-type="fig">Fig 2D</xref>). The overlap of the two hump-shaped profiles will be large if nodes <italic>x</italic> and <italic>y</italic> are close on the graph, and small if they are distant. Fundamentally the endotaxis network computes that overlap.</p>
<p>Suppose the animal visits <italic>y</italic> and finds water there. Then the water resource cell fires, triggering synaptic learning in the goal synapses. That stores the current profile of map activity <italic>v</italic><sub><italic>i</italic></sub>(<italic>y</italic>) in the synapses <italic>G</italic><sub><italic>ki</italic></sub> onto the goal cell <italic>k</italic> that responds to water (<xref rid="fig2" ref-type="fig">Fig 2D</xref>), <xref ref-type="disp-formula" rid="eqn9">Eqn 9</xref>). When the animal subsequently moves to a different location <italic>x</italic>, the goal cell <italic>k</italic> receives the current map output <bold>v</bold>(<italic>x</italic>) filtered through the previously stored synaptic template <bold>v</bold>(<italic>y</italic>) (<xref rid="fig2" ref-type="fig">Fig 2E</xref>). This is the desired measure of overlap (<xref ref-type="disp-formula" rid="eqn10">Eqn 10</xref>). Under suitable conditions this goal signal declines monotonically with the shortest graph-distance between <italic>x</italic> and <italic>y</italic>, as we will demonstrate both analytically and in simulations (<xref ref-type="sec" rid="s2b">Sections 2.2</xref>, <xref ref-type="sec" rid="s2c">2.3</xref>).</p>
</sec>
</sec>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Theory of endotaxis</title>
<p>Here we formalize the processes of <xref rid="fig2" ref-type="fig">Figure 2</xref> in a concrete mathematical model. The model is simple enough to allow some exact predictions for its behavior. The present section develops an analytical understanding of endotaxis that will help guide the numerical simulations in subsequent parts.</p>
<p>The environment is modeled as a graph consisting of <italic>n</italic> nodes, with adjacency matrix
<disp-formula id="eqn1">
<graphic xlink:href="461751v3_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We suppose the graph is undirected, meaning that every link can be traversed in both directions,
<disp-formula id="ueqn1">
<graphic xlink:href="461751v3_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Movements of the agent are modeled as a sequence of steps along that graph. During exploration, the agent performs a walk that tries to cover the entire environment. In the process, it learns the adjacency matrix <bold>A</bold>. During navigation, the agent uses that knowledge to travel to a known target.</p>
<p>For an agent navigating a graph, it is very useful to know the shortest graph distance between any two nodes
<disp-formula id="eqn2">
<graphic xlink:href="461751v3_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Given this information, one can navigate the shortest route from <italic>x</italic> to <italic>y</italic>: for each of the neighbors of <italic>x</italic>, look up its distance to <italic>y</italic> and step to the neighbor with the shortest distance. Then repeat that process until <italic>y</italic> is reached. Thus, the shortest route can be navigated one step at a time without any high-level advanced planning. This is the core idea behind endotaxis.</p>
<p>The network of <xref rid="fig1" ref-type="fig">Fig 1B</xref> effectively computes the shortest graph distances. We implement the circuit as a textbook linear rate model [<xref ref-type="bibr" rid="c15">15</xref>]. Each map unit <italic>i</italic> has a synaptic input <italic>w</italic><sub><italic>i</italic></sub> that it converts to an output <italic>v</italic><sub><italic>i</italic></sub>,
<disp-formula id="eqn3">
<graphic xlink:href="461751v3_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>γ</italic> is the gain of the units. The input consists of an external signal <italic>u</italic><sub><italic>i</italic></sub> summed with a recurrent feedback through a connection matrix <bold>M</bold>
<disp-formula id="eqn4">
<graphic xlink:href="461751v3_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>M</italic><sub><italic>ij</italic></sub> is the synaptic strength from unit <italic>j</italic> to <italic>i</italic>.</p>
<p>The point neurons are one-hot encoders of location. A point neuron fires if the agent is at that location; all the others are silent:
<disp-formula id="eqn5">
<graphic xlink:href="461751v3_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>δ</italic><sub><italic>ix</italic></sub> is the Kronecker delta.</p>
<p>So the vector of all map outputs is
<disp-formula id="eqn6">
<graphic xlink:href="461751v3_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where u is the one-hot input from point cells.</p>
<p>Now consider goal cell number <italic>k</italic> that is associated to a particular location <italic>y</italic>, because its resource is present at that node. The goal cell sums input from all the map units <italic>v</italic><sub><italic>i</italic></sub>, weighted by its goal synapses <italic>G</italic><sub><italic>ki</italic></sub>. So with the agent at node <italic>x</italic> the goal signal <italic>r</italic><sub><italic>k</italic></sub> is:
<disp-formula id="eqn7">
<graphic xlink:href="461751v3_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we write <bold>g</bold><sub><italic>k</italic></sub> for the <italic>k</italic><sup>th</sup> row vector of the goal synapse matrix <bold>G</bold>. This is the set of synapses from all map cells onto the specific goal cell in question.</p>
<p>Suppose now that the agent has learned the structure of the environment perfectly, such that the map synapses are a copy of the graph’s adjacency matrix (1),
<disp-formula id="eqn8">
<graphic xlink:href="461751v3_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Similarly, suppose that the agent has acquired the goal synapses perfectly, namely proportional to the map output at the goal location <italic>y</italic>:
<disp-formula id="eqn9">
<graphic xlink:href="461751v3_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Then as the agent moves to another location <italic>x</italic>, the goal cell reports a signal
<disp-formula id="eqn10">
<graphic xlink:href="461751v3_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the matrix
<disp-formula id="eqn11">
<graphic xlink:href="461751v3_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
It has been shown [<xref ref-type="bibr" rid="c40">40</xref>] that for small values of <italic>γ</italic> the elements of the resolvent matrix
<disp-formula id="eqn12">
<graphic xlink:href="461751v3_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
are monotonically related to the shortest graph distances <bold>D</bold>. Specifically,
<disp-formula id="eqn13">
<graphic xlink:href="461751v3_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Building on that, the matrix <bold>E</bold> becomes
<disp-formula id="eqn14">
<graphic xlink:href="461751v3_eqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The limit is dominated by the term with the smallest exponent, which occurs when <italic>z</italic> lies on a shortest path from <italic>x</italic> to <italic>y</italic>
<disp-formula id="ueqn2">
<graphic xlink:href="461751v3_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we have used the undirected nature of the graph, namely <italic>D</italic><sub><italic>zx</italic></sub> = <italic>D</italic><sub><italic>xz</italic></sub>.</p>
<p>Therefore,
<disp-formula id="eqn15">
<graphic xlink:href="461751v3_eqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>D</italic><sub><italic>xy</italic></sub> is the smallest number of steps needed to get from node <italic>y</italic> to node <italic>x</italic>.</p>
<p><xref rid="fig3" ref-type="fig">Figure 3</xref> illustrates this relationship with numerical results on a binary tree graph. As expected, for small <italic>γ</italic> the goal signal decays exponentially with graph distance (<xref rid="fig3" ref-type="fig">Fig 3B</xref>). Therefore an agent that makes local turning decisions to maximize that goal signal will reach the goal by the shortest possible path.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Theory of the goal signal.</title>
<p>Dependence of the goal signal on graph distance, and the consequences for endotaxis navigation. <bold>A:</bold> The graph representing a binary tree labyrinth [<xref ref-type="bibr" rid="c49">49</xref>] serves for illustration. Suppose the endotaxis model has acquired the adjacency matrix perfectly: <bold>M</bold> = <bold>A</bold>. We compute the goal signal <italic>E</italic><sub><italic>xy</italic></sub> between any two nodes on the graph, and compare the results at different values of the map gain <italic>γ</italic>. <bold>B:</bold> Dependence of the goal signal <italic>E</italic><sub><italic>xy</italic></sub> on the graph distance <italic>D</italic><sub><italic>xy</italic></sub> between the two nodes. Mean ± SD, error bars often smaller than markers. The maximal distance on this graph is 12. Note logarithmic vertical axis. The signal decays exponentially over many log units. At high <italic>γ</italic> the decay distance is greater. <bold>C:</bold> A detailed look at the goal signal, each point is for a pair of nodes (<italic>x, y</italic>). For low <italic>γ</italic> the decay with distance is strictly monotonic. At high <italic>γ</italic> there is overlap between the values at different distances. As <italic>γ</italic> exceeds the critical value <italic>γ</italic><sub>c</sub> = 0.38 the distance-dependence breaks down. <bold>D:</bold> Using the goal signal for navigation. For every pair of start and end nodes we navigate the route by following the goal signal and compare the distance traveled to the shortest graph distance. For all routes with the same graph distance we plot the median navigated distance with 10% and 90% quantiles. Variable gain at a constant noise value of <italic>ϵ</italic> = 0.01. <bold>E:</bold> As in panel (D) but varying the noise at a constant gain of <italic>γ</italic> = 0.34.</p></caption>
<graphic xlink:href="461751v3_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The exponential decay of the goal signal represents a challenge for practical implementation with biological circuits. Neurons have a finite signal-to-noise ratio, so detecting minute differences in the firing rate of a goal neuron will be unreliable. Because the goal signal changes by a factor of <italic>γ</italic> across every link in the graph, one wants to set the map neuron gain <italic>γ</italic> as large as possible. However, there is a <italic>critical gain</italic> value <italic>γ</italic><sub>c</sub> that sets a strict upper limit:
<disp-formula id="eqn16">
<graphic xlink:href="461751v3_eqn16.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
For larger <italic>γ &gt; γ</italic><sub>c</sub> the goal signal <italic>E</italic><sub><italic>xy</italic></sub> no longer represents graph distances [<xref ref-type="bibr" rid="c40">40</xref>]. The largest eigenvalue of the adjacency matrix in turn is related to the number of edges per node. For graphs with 2 to 4 edges per node, <italic>γ</italic><sub>c</sub> is typically about 0.3. The graph in <xref rid="fig3" ref-type="fig">Figure 3A</xref> has <italic>γ</italic><sub>c</sub> ≈0.383, and indeed <italic>E</italic><sub><italic>xy</italic></sub> becomes erratic as <italic>γ</italic> approaches that value (<xref rid="fig3" ref-type="fig">Fig 3C</xref>).</p>
<p>To implement the finite dynamic range explicitly, we add some noise to the goal signal of <xref ref-type="disp-formula" rid="eqn10">Eqn 10</xref>:
<disp-formula id="eqn17">
<graphic xlink:href="461751v3_eqn17.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the noise <italic>η</italic> has a gaussian distribution with full width <italic>ϵ</italic>:
<disp-formula id="eqn18">
<graphic xlink:href="461751v3_eqn18.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The scale <italic>ϵ</italic> of this noise is expressed relative to the maximum value of the goal signal. If the agent must decide between two goal signals separated by less than <italic>ϵ</italic>, the noise will take a toll on the resulting navigation performance.</p>
<p>Of course neurons everywhere within the network will carry some noise. We lump the cumulative effects of that into the final readout step, because that allows for efficient calculations, see <xref ref-type="sec" rid="s4b">section 4.2</xref>.<sup><xref ref-type="fn" rid="fn3">3</xref></sup> What is a reasonable value for this effective readout noise? For reference, humans and animals can routinely discriminate sensory stimuli that differ by only 1%, for example the pitch of tones or the intensity of a light, especially if they occur in close succession. Clearly the neurons all the way from receptors to perception must represent those small differences. Thus we will use <italic>ϵ</italic> = 0.01 as a reference noise value in many of the results presented here.</p>
<statement id="alg1">
<label>Algorithm 1</label>
<title>Navigation</title>
<p><fig id="alg1a" position="float" fig-type="figure">
<graphic xlink:href="461751v3_alg1.tif" mimetype="image" mime-subtype="tiff"/>
</fig></p>
</statement>
<p>The process of navigation towards a chosen goal signal is formalized in <xref ref-type="statement" rid="alg1">Algorithm 1</xref>. At each node the agent inspects the goal signal that would be obtained at all the neighboring nodes, corrupted by the readout noise <italic>η</italic>. Then it steps to the neighbor with the highest value. Suppose the agent starts at node <italic>x</italic> and navigates following the goal signal for node <italic>y</italic>. The resulting navigation route <italic>x</italic> = <italic>s</italic><sub>0</sub>, <italic>s</italic><sub>1</sub>, …, <italic>s</italic><sub><italic>n</italic></sub> = <italic>y</italic> has <italic>L</italic><sub><italic>xy</italic></sub> = <italic>n</italic> steps. Navigation is perfect if this equals the shortest graph distance, <italic>L</italic><sub><italic>xy</italic></sub> = <italic>D</italic><sub><italic>xy</italic></sub>. We will assess deviations from perfect performance by the excess length of the routes.</p>
<p><xref rid="fig3" ref-type="fig">Figure 3D-E</xref> illustrates how the navigated path distance <italic>L</italic><sub><italic>xy</italic></sub> depends on the noise level <italic>ϵ</italic> and the gain <italic>γ</italic>. For small gain or high noise the goal signal extends only over a graph distance of 5-6 links. Beyond that the navigated distance <italic>L</italic><sub><italic>xy</italic></sub> begins to exceed the graph distance <italic>D</italic><sub><italic>xy</italic></sub>. As the gain increases, the goal signal extends further through the graph and navigation becomes reliable over longer distances (<xref rid="fig3" ref-type="fig">Fig 3D</xref>). Eventually, however, the goal signal loses its monotonic distance dependence (<xref rid="fig3" ref-type="fig">Fig 3C</xref>). At that stage, navigation across the graph may fail because the agent gets trapped in a local maximum of the goal signal. This can happen even before the critical gain value is reached (<xref rid="fig3" ref-type="fig">Fig 3C</xref>). For the example in <xref rid="fig3" ref-type="fig">Fig 3</xref> the highest useful gain is <italic>γ</italic> = 0.34 whereas <italic>γ</italic><sub>c</sub> = 0.383.</p>
<p>For any given value of the gain, navigation improves with lower noise levels, as expected (<xref rid="fig3" ref-type="fig">Fig 3E</xref>). At the reference value of <italic>ϵ</italic> = 0.01, navigation is perfect even across the 12 links that separate the most distant points on this graph.</p>
<p>In summary, this analysis spells out the challenges that need to be met for endotaxis to work properly. First, during the learning phase, the agent must reliably extract the adjacency matrix of the graph, and copy it into its map synapses. Second, during the navigation phase, the agent must evaluate the goal signal with enough resolution to distinguish the values at alternative nodes. The neuronal gain <italic>γ</italic> plays a central role: With <italic>γ</italic> too small, the goal signal decays rapidly with distance and vanishes into the noise just a few steps away from the goal. But at large <italic>γ</italic> the network computation becomes unstable.</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Acquisition of map and targets during exploration</title>
<p>As discussed above, the goal of learning during exploration is that the agent acquires a copy of the graph’s adjacency matrix in its map synapses, <bold>M</bold> ≈ <bold>A</bold>, and stores the map output at a goal location <italic>y</italic> in the goal synapses <bold>g</bold>≈ <bold>v</bold>(<italic>y</italic>). Here we explore how the rules for synaptic plasticity in the map and goal networks allow that to happen. <xref ref-type="statement" rid="alg2">Algorithm 2</xref> spells out the procedure we implemented for learning from a random walk through the environment.</p>
<p>The map synapses <italic>M</italic><sub><italic>ij</italic></sub> start out at zero strength. When the agent moves from node <italic>j</italic> = <italic>s</italic>(<italic>t</italic>) at time <italic>t</italic> to node <italic>i</italic> = <italic>s</italic>(<italic>t</italic> + 1), the map cells <italic>j</italic> and <italic>i</italic> are excited in close succession. When that happens, the agent potentiates the synapses between those two neurons to <italic>M</italic><sub><italic>ji</italic></sub> = <italic>M</italic><sub><italic>ij</italic></sub> = 1. Of course, a map cell can also get activated through the recurrent network, and we must distinguish that from direct input from its point cell. We found that a simple threshold criterion is sufficient. Here <italic>θ</italic> is a threshold applied to both the pre- and post-synaptic activity, and the map synapse gets established only if both neurons respond above threshold. The tuning requirements for this threshold are discussed below.</p>
<p>The goal synapses <italic>G</italic><sub><italic>kj</italic></sub> similarly start out at zero strength. Consider a particular goal cell <italic>k</italic>, and suppose its corresponding resource cell has activity <italic>F</italic><sub><italic>ky</italic></sub> when the agent is at location <italic>y</italic>. When a positive resource signal arrives, that means the agent is at a goal location. If the goal signal <italic>r</italic><sub><italic>k</italic></sub> received from the map output is smaller than the resource signal <italic>F</italic><sub><italic>ky</italic></sub>, then the goal synapses get incremented by something proportional to the current map output. Learning at the goal synapses saturates when the goal signal correctly predicts the resource signal. The learning rate <italic>α</italic> sets how fast that will happen. Note that both the learning rules for map and goal synapses are Hebbian and strictly local: Each synapse is modified based only on signals available in the pre- and post-synaptic neurons.</p>
<p>To illustrate the process of map and goal learning we simulate an agent exploring a simple ring graph by a random walk (<xref rid="fig4" ref-type="fig">Fig 4</xref>). At first, there are no targets in the environment that can deliver a resource (<xref rid="fig4" ref-type="fig">Fig 4A</xref>). Then we add one target location, and later a second one. Finally we add a new link to the graph that makes a connection clear across the environment. As the agent explores the graph, we will track how its representations evolve by monitoring the map synapses and the profile of the goal signal.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Learning the map and the targets during exploration.</title>
<p><bold>(A)</bold> Simulation of a random walk on a ring with 14 nodes. Left: Layout of the ring, with resource locations marked in blue. The walk progresses in 800 time steps (top to bottom); with the agent’s position marked in red (nodes 0-13, horizontal axis). At each time the color map shows the goal signal that would be produced if the agent were at position ‘Node’. White horizontal lines mark the appearance of a target at <italic>t</italic> = 200, a second target with the same resource at <italic>t</italic> = 400, and a new link across the ring at step <italic>t</italic> = 600. <bold>(B)</bold> The matrix <bold>M</bold> of map synapses at various times. The pixel in row <italic>i</italic> and column <italic>j</italic> represents the matrix element <italic>M</italic><sub><italic>ij</italic></sub>. Color purple = 0. Note the first few steps (number above graph) each add a new synapse. Eventually, <bold>M</bold> reflects the adjacency matrix of nodes on the graph. <bold>(C)</bold> Goal signals just before and just after the agent encounters the first target. <bold>(D)</bold> Goal signals just before and just after the agent encounters the second target. <bold>(E)</bold> Goal signals just before and just after the agent travels the new link for the first time. Parameters: <italic>γ</italic> = 0.32, <italic>θ</italic> = 0.27, <italic>α</italic> = 0.3.</p></caption>
<graphic xlink:href="461751v3_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>At the outset, every time the agent steps to a new node, the map synapse corresponding to that link gets potentiated (<xref rid="fig4" ref-type="fig">Fig 4B</xref>). After enough steps, the agent has executed every link on the graph, and the matrix of map synapses resembles the full adjacency matrix of the graph (<xref rid="fig4" ref-type="fig">Fig 4B</xref>). At this stage the agent has learned the connectivity of the environment.</p>
<statement id="alg2">
<label>Algorithm 2</label>
<title>Map and goal learning</title>
<p><fig id="alg2a" position="float" fig-type="figure">
<graphic xlink:href="461751v3_alg2.tif" mimetype="image" mime-subtype="tiff"/>
</fig></p>
</statement>
<p>Once a target appears in the environment it takes the agent a few random steps to encounter it. At that moment the goal synapses get potentiated for the first time, and suddenly a goal signal appears in the goal cell (<xref rid="fig4" ref-type="fig">Fig 4C</xref>). The profile of that goal signal is fully formed and spreads through the entire graph thanks to the pre-established map network. By following this goal signal uphill the agent can navigate along the shortest path to the target from any node on the graph. Note that the absolute scale of the goal signal grows a little every time the agent visits the goal (<xref rid="fig4" ref-type="fig">Fig 4A</xref>) and eventually saturates.</p>
<p>Some time later we introduce a second target elsewhere in the environment (<xref rid="fig4" ref-type="fig">Fig 4D</xref>). When the agent encounters it along its random walk, the goal synapses get updated, and the new goal signal has two peaks in its profile. Again, this goal signal grows during subsequent visits. By following that signal uphill from any starting point, the agent will be led to a nearby target by the shortest possible path.</p>
<p>When a new link appears, the agent eventually discovers it on its random walk. At that point the goal signal changes instantaneously to incorporate the new route (<xref rid="fig4" ref-type="fig">Fig 4E</xref>). An agent following the new goal signal from node 13 on the ring will now be led to a target location in just 3 steps, using the shortcut, whereas previously it took 5 steps.</p>
<p>This simulation illustrates how the structure of the environment is acquired separately from the location of resources. The agent can explore and learn the map of the environment even without any resources present (<xref rid="fig4" ref-type="fig">Fig 4B</xref>). This learning takes place among the map synapses in the endotaxis circuit (<xref rid="fig1" ref-type="fig">Fig 1B</xref>). When a resource is found, its location gets tagged within that established map through learning by the goal synapses. The resulting goal signal is available immediately without the need for further learning (<xref rid="fig4" ref-type="fig">Fig 4C</xref>). If the distribution of resources changes, the knowledge in the map remains unaffected (<xref rid="fig4" ref-type="fig">Fig 4D</xref>) but the goal synapses can change quickly to incorporate the new target. Vice versa, if the graph of the environment changes, the map synapses get updated, and that adapts the goal signal to the new situation even without further change in the goal synapses (<xref rid="fig1" ref-type="fig">Fig 1E</xref>).</p>
<p>What happens if a previously existing link disappears from the environment, for example because one corridor of the mouse burrow caves in? Ideally the agent would erase that link from the cognitive map. The learning algorithm <xref ref-type="statement" rid="alg2">Alg 2</xref> is designed for rapid and robust acquisition of a cognitive map starting from zero knowledge, and does not contain a provision for forgetting. However, one can add a biologically plausible rule for synaptic depression that gradually erases memory of a link if the agent never travels it. Details are presented in Supplement <xref ref-type="sec" rid="s4d">section 4.4</xref> (<xref rid="fig10" ref-type="fig">Fig 10</xref>). For the sake of simplicity we continue the present analysis of endotaxis based on the simple 3-parameter algorithm presented above (<xref ref-type="statement" rid="alg2">Alg 2</xref>).</p>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Choice of learning rule</title>
<p>The map learning rule in <xref ref-type="statement" rid="alg2">Alg 2</xref> produces full strength synapses <italic>M</italic><sub><italic>ij</italic></sub> and <italic>M</italic><sub><italic>ji</italic></sub> after a single co-activation of the two neurons. A more common approach to synaptic learning uses small incremental updates, and stabilizes the update rule with some form of normalization, based on the average pre-or post-synaptic activity over many steps [<xref ref-type="bibr" rid="c24">24</xref>]. For example, pre-synaptic normalization leads the synaptic network to learn a transition probability matrix [<xref ref-type="bibr" rid="c19">19</xref>]
<disp-formula id="ueqn3">
<graphic xlink:href="461751v3_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Instead, we adopted the instantaneous update model for two reasons: Most importantly, this allows the agent to learn a route after the first traversal, which is needed to explain the rapid learning observed in experimental animals. For example, <xref ref-type="sec" rid="s2h">Section 2.8</xref> models accurate homing after the first excursion into the labyrinth. Furthermore, when we repeated the analysis of <xref rid="fig3" ref-type="fig">Figure 3</xref> using the transition matrix <italic>T</italic><sub><italic>ij</italic></sub> instead of the adjacency matrix <italic>A</italic><sub><italic>ij</italic></sub>, the goal signal correlated more weakly with distance, and even with the optimal gain setting the range of correct navigation was considerably reduced.</p>
<p>This rapid learning rule reflects an implicit assumption that the environment is static, such that the learned transition will always be available. For adaptation to slow changes in the environment, see <xref ref-type="sec" rid="s4d">Section 4.4</xref>. Note also that the above procedure <xref ref-type="statement" rid="alg2">Alg 2</xref> updates both synapses between neurons <italic>i</italic> and <italic>j</italic>. This assumes implicitly that the experienced edge on the graph can also be traversed in the opposite direction, which applies to many navigation problems. To learn a directed environment – such as a city map with one-way streets, or a game in which moves cannot be reversed – one may use a directed learning rule that requires the pre-synaptic neuron to fire before the post-synaptic neuron. This will update only the synapse <italic>M</italic><sub><italic>ij</italic></sub> representing the edge that was actually traveled. For all simulations in this article we will use the symmetric learning rule.</p>
</sec>
<sec id="s2e">
<label>2.5</label>
<title>Navigation using the learned goal signal</title>
<p>We now turn to the “exploitation” component of endotaxis, namely use of the learned information to navigate towards targets. In the simulations of <xref rid="fig5" ref-type="fig">Figure 5</xref> we allow the agent to explore a graph. Every node on the graph drives a separate resource cell, thus the agent simultaneously learns goal signals to every node. After a random walk sufficient to cover the graph several times, we test the agent’s ability to navigate to the goals by ascending on the learned goal signal. For that purpose we teleport the agent to an arbitrary start node in the graph and ask how many steps it takes to reach the goal node following the policy of <xref ref-type="statement" rid="alg1">Alg 1</xref>. In these tests, the learning of map and goal synapses was turned off during the navigation phase, so we could separately assess how learning and navigating affect the performance. However, there is no functional requirement for this, and indeed one of the attractive features of this model is that learning and navigation can proceed in parallel at all times.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Navigation using the learned map and targets.</title>
<p><bold>(A-C)</bold> Ring with 50 nodes. <bold>(A)</bold> Goal signal for a single target location (blue dot on left icon) after learning during random exploration with 10,000 steps. Color scale is logarithmic, yellow=high. Note the monotonic decay of the goal signal with graph distance from the target. <bold>(B)</bold> Results of all-to-all navigation where every node is a separate goal. For all pairs of nodes, this shows the navigated distance vs the graph distance. Median ± 10/90 percentiles for all routes with the same graph distance. “Ideal” navigation would follow the identity. The actual navigation is ideal over short distances, then begins to deviate from ideal at a critical distance that depends on the noise level <italic>ϵ</italic>. <bold>(C)</bold> As in (B) over a wider range, note logarithmic axis. Noise <italic>ϵ</italic> = 0.01. Includes comparison to navigation by a random walk; and navigation using the optimal goal signal based on knowledge of the graph structure and target location. <italic>γ</italic> = 0.41, <italic>θ</italic> = 0.39, <italic>α</italic> = 0.1. <bold>(D-F)</bold> As in (A-C) for a binary tree graph with 127 nodes. <bold>(D)</bold> Goal signal to the node marked on the left icon. This was the reward port in the labyrinth experiments of [<xref ref-type="bibr" rid="c49">49</xref>]. White lines separate the branches of the tree. <italic>γ</italic> = 0.33, <italic>θ</italic> = 0.30, <italic>α</italic> = 0.1. <bold>(G-I)</bold> As in (A-C) for a “Tower of Hanoi” graph with 81 nodes. <italic>γ</italic> = 0.29, <italic>θ</italic> = 0.27, <italic>α</italic> = 0.1.</p></caption>
<graphic xlink:href="461751v3_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><xref rid="fig5" ref-type="fig">Figure 5A-C</xref> shows results on a ring graph with 50 nodes. With suitable values of the model parameters (<italic>γ, θ, α</italic>) – more on that later – the agent learns a goal signal that declines monotonically with distance from the target node (<xref rid="fig5" ref-type="fig">Fig 5A</xref>). The ability to ascend on that goal signal depends on the noise level <italic>ϵ</italic>, which determines whether the agent can sense the difference in goal signal at neighboring nodes. At a high noise level <italic>ϵ</italic> = 0.1 the agent finds the target by the shortest route from up to 5 links away (<xref rid="fig5" ref-type="fig">Fig 5B</xref>); beyond that range some navigation errors creep in. At a low noise level of <italic>ϵ</italic> = 0.005 navigation is perfect up to 10 links away. Every factor of two increase in noise seems to reduce the range of navigation by about one link.</p>
<p>How does the process of learning the map of the environment affect the ultimate navigation performance? <xref rid="fig5" ref-type="fig">Figure 5C</xref> makes that comparison by considering an agent with oracular knowledge of the graph structure and target location (<xref ref-type="disp-formula" rid="eqn8">Eqns 8</xref> and <xref ref-type="disp-formula" rid="eqn9">9</xref>). Interestingly this barely improves the distance range for perfect navigation. By contrast, an agent performing a random walk with zero knowledge of the environment would take about 40 times longer to reach the target than by using endotaxis (<xref rid="fig5" ref-type="fig">Fig 5C</xref>).</p>
<p>The ring graph is particularly simple, but how well does endotaxis learn in a more realistic environment? <xref rid="fig5" ref-type="fig">Figure 5D-F</xref> shows results on a binary tree graph with 6 levels: This is the structure of a maze used in a recent study on mouse navigation [<xref ref-type="bibr" rid="c49">49</xref>]. In those experiments, mice learned quickly how to reach the reward location (blue dot in <xref rid="fig5" ref-type="fig">Fig 5D</xref>) from anywhere within the maze. Indeed, the endotaxis agent can learn a goal signal that declines monotonically with distance from the reward port (<xref rid="fig5" ref-type="fig">Fig 5D</xref>). At a noise level of <italic>ϵ</italic> = 0.01 navigation is perfect over distances of 9 links, and close to perfect over the maximal distance of 12 links that occurs in this maze (<xref rid="fig5" ref-type="fig">Fig 5E</xref>). Again, the challenge of having to learn the map affects the performance only slightly (<xref rid="fig5" ref-type="fig">Fig 5F</xref>). Finally, comparison with the random agent shows that endotaxis shortens the time to target by a factor of 100 on this graph (<xref rid="fig5" ref-type="fig">Fig 5F</xref>).</p>
<p><xref rid="fig5" ref-type="fig">Figure 5G-I</xref> shows results for a more complex graph that represents a cognitive task, namely the game “Tower of Hanoi”. Disks of different sizes are stacked on three pegs, with the constraint that no disk can rest on top a smaller one. The game is solved by rearranging the pile of disks from the center peg to another. In any state of the game there are either 2 or 3 possible actions, and they form an interesting graph with many loops (<xref rid="fig5" ref-type="fig">Fig 5G</xref>). The player starts at the top node (all disks on the center peg) and the two possible solutions correspond to the bottom left and right corners. Again, random exploration leads the endotaxis agent to learn the connectivity of the game and to discover the solutions. The resulting goal signal decays systematically with graph distance from the solution (<xref rid="fig5" ref-type="fig">Fig 5G</xref>). At a noise of <italic>ϵ</italic> = 0.01 navigation is perfect once the agent gets to within 9 moves of the target (<xref rid="fig5" ref-type="fig">Fig 5H</xref>). This is not quite sufficient for an error-free solution from the starting position, which requires 15 moves. However, compared to an agent executing random moves, endotaxis speeds up the solution by a factor of 10 (<xref rid="fig5" ref-type="fig">Fig 5I</xref>). If the game is played with only 3 disks, the maximal graph distance is 7, and endotaxis solves it perfectly at <italic>ϵ</italic> = 0.01.</p>
<p>These results show that endotaxis functions well in environments with very different structure: linear, tree-shaped, and cyclic. Random exploration in conjunction with synaptic learning can efficiently acquire the connectivity of the environment and the location of targets. With a noise level of 1%, the resulting goal signal allows perfect navigation over distances of ∼9 steps, independent of the nature of the graph. This is a respectable range: Personal experience suggests that we rarely learn routes that involve more than 9 successive decisions. Chess openings, which are often played in a fast and reflexive fashion, last about 10 moves.</p>
</sec>
<sec id="s2f">
<label>2.6</label>
<title>Parameter sensitivity</title>
<p>The endotaxis model has only 3 parameters: the gain <italic>γ</italic> of map units, the threshold <italic>θ</italic> for learning at map synapses, and the learning rate <italic>α</italic> at goal synapses. How does performance depend on these parameters? Do they need to be tuned precisely? And does the optimal tuning depend on the spatial environment? There is a natural hierarchy to the parameters if one separates the process of learning from that of navigation. Suppose the circuit has learned the structure of the environment perfectly, such that the map synapses reflect the adjacencies (<xref ref-type="disp-formula" rid="eqn8">Eqn 8</xref>), and the goal synapses reflect the map output at the goal (<xref ref-type="disp-formula" rid="eqn9">Eqn 9</xref>). Then the optimal navigation performance of the endotaxis system depends only on the gain <italic>γ</italic> and the noise level <italic>ϵ</italic>. For a given <italic>γ</italic>, in turn, the precision of map learning depends only on the threshold <italic>θ</italic> (see <xref ref-type="statement" rid="alg2">Alg 2</xref>). Finally, if the gain is set optimally and the map was learned properly, the identification of targets depends only on the goal learning rate <italic>α</italic>. <xref rid="fig6" ref-type="fig">Figure 6</xref> explores these relationships in turn.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Sensitivity of performance to the model parameters.</title>
<p>On each of the three graphs we simulated endotaxis for all-to-all navigation, where each node serves as a start and a goal node. The performance measure was the range of the goal signal, defined as the graph distance over which at least half the navigated routes follow the shortest path. The exploration path for synaptic learning was of medium length, visiting each edge on the graph approximately 10 times. The noise was set to <italic>ϵ</italic> = 0.01. <bold>(A)</bold> Ring graph with 50 nodes. <bold>Left:</bold> Dependence of the goal signal range on the gain <italic>γ</italic> and the threshold <italic>θ</italic> for learning map synapses. Performance increases with higher gain until it collapses beyond the critical value. For each gain there is a sharply defined range of useful thresholds, with lower values at lower gain. <bold>Right:</bold> Dependence of the goal signal range on the learning rate <italic>α</italic> at goal synapses, and the length of the exploratory walk, measured in visits per edge of the graph. For a short walk (1 edge visit) a high learning rate is best. For a long walk (100 edge visits) a lower learning rate wins out. <bold>(B)</bold> As in (A) for the Binary tree maze with 127 nodes. <bold>(C)</bold> As in (A) for the Tower of Hanoi graph with 81 nodes.</p></caption>
<graphic xlink:href="461751v3_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We simulated the learning phase of endotaxis as in the preceding section (<xref rid="fig5" ref-type="fig">Fig 5B, E, H</xref>), using a noise level of <italic>ϵ</italic> = 0.01, and systematically varying the model parameters (<italic>γ, θ, α</italic>). For each parameter set we measured the graph distance over which at least half of the navigated routes were perfect. We defined this distance as the range of the goal signal.</p>
<p>For example, on the ring graph (<xref rid="fig6" ref-type="fig">Fig 6A</xref>) the signal range improves with gain, until performance collapses beyond a maximal gain value. This is just as predicted by the theory (<xref rid="fig3" ref-type="fig">Fig 3</xref>), except that the maximal gain <italic>γ</italic><sub>max</sub> = 0.41 is somewhat below the critical value <italic>γ</italic><sub>c</sub> = 0.5. Clearly the added complications of having to learn the map and goal locations take their toll at high gain. Below the maximal cutoff, the dependence of performance on gain is rather gentle: For example a 14% change in gain from 0.35 to 0.40 leads to a 26% change in performance. At any given gain value, there is a range of values for the threshold <italic>θ</italic> within which the map is learned perfectly. Note that this range is generous and does not require precise adjustment: For example, under a near-maximal gain of 0.38, the threshold can vary freely over a 35% range.</p>
<p>Once the gain and synaptic threshold are set so as to acquire the map synapses, the quality of goal learning depends only on the learning rate <italic>α</italic>. With large <italic>α</italic>, a single visit to the goal fully potentiates the goal synapses so they don’t get updated further. This allows for a fast acquisition of that target, but at the risk of imperfect learning, because the map may not be fully explored yet. A small <italic>α</italic> will update the synapses only partially over many successive visits to the goal. This leads to a poor performance after short exploration, because the weak goal signal competes with noise, but superior performance after long explorations: a trade-off between speed of learning and accuracy. Precisely this speed-accuracy tradeoff is seen in the simulations (<xref rid="fig6" ref-type="fig">Fig 6A</xref>, right): A high learning rate is optimal for short explorations, but for longer ones a small learning rate wins out. An intermediate value of <italic>α</italic> = 1 delivers a good compromise performance.</p>
<p>We found qualitatively similar behavior for the other two environments studied here: The binary maze graph (<xref rid="fig6" ref-type="fig">Fig 6B</xref>) and the Tower of Hanoi graph (<xref rid="fig6" ref-type="fig">Fig 6C</xref>). In each case, the maximal usable gain is slightly below the critical value <italic>γ</italic><sub>c</sub> of that graph. A learning rate of <italic>α</italic> = 1 delivers intermediate results. For long explorations, a lower learning rate is best.</p>
<p>In summary this sensitivity analysis shows that the optimal parameter set for endotaxis does depend on the environment. This is not altogether surprising: Every neural network needs to adapt to the distribution of inputs it receives so as to perform optimally. At the same time, the required tuning is rather generous, allowing at least 10-20% slop in the parameters for reasonable performance. Furthermore, a single parameter set of <italic>γ</italic> = 0.29, <italic>θ</italic> = 0.26, <italic>α</italic> = 1 performs quite well on both the binary maze and the Tower of Hanoi graphs, which are dramatically different in character.</p>
</sec>
<sec id="s2g">
<label>2.7</label>
<title>A saturating activation function improves navigation</title>
<p>So far, the model of the map network used neurons with a linear activation function (<xref ref-type="disp-formula" rid="eqn3">Eqn 3</xref>), meaning the output <italic>v</italic> is simply proportional to the input, <italic>v</italic> = <italic>γw</italic>. We also explored non-linear activation functions <italic>v</italic> = <italic>f</italic> (<italic>w</italic>), and found that the performance of endotaxis improves under certain conditions [<xref ref-type="bibr" rid="c19">19</xref>]. The most important feature is that <italic>f</italic> (<italic>w</italic>) should saturate for inputs <italic>x</italic> that are larger than the output of the point cells (<italic>u</italic> = 1 in <xref ref-type="disp-formula" rid="eqn4">Eqn 4</xref>). The detailed shape matters little, so for illustration we will use a linear-flat activation curve (<xref rid="fig7" ref-type="fig">Fig 7A</xref>):
<disp-formula id="eqn19">
<graphic xlink:href="461751v3_eqn19.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula></p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Benefits of a nonlinear activation function.</title>
<p><bold>(A)</bold> The activation function relating a map neuron’s output <italic>v</italic> to its total input <italic>w</italic>. Red: Linear function with gain <italic>γ</italic>. Blue: Nonlinear function with saturation at <italic>w &gt;</italic> 1. <bold>(B-D)</bold> Range of the goal signal, as defined in <xref rid="fig6" ref-type="fig">Fig 6</xref>, as a function of the gain <italic>γ</italic> (noise <italic>ϵ</italic> = 0.01). Range increases with gain up to a maximal value. The maximal range achieved is higher with nonlinear activation (blue) than linear activation (red). Results for the ring graph <bold>(B)</bold>, binary tree maze <bold>(C)</bold>, and Tower of Hanoi graph <bold>(D). (E)</bold> Output of map cells during early exploration of the ring graph (gain <italic>γ</italic> = 0.49). Suppose the agent has walked back and forth between nodes 2 and 5, so all their corresponding map synapses are established (black bars). Then the agent steps to node 6 for the first time (cyan). Lines plot the output of the map cells with the agent at locations 2, 3, 4, 5, or 6. Dotted line indicates the maximal possible setting of the threshold <italic>θ</italic> in the learning rule. With linear activation <bold>(left)</bold> a map cell receiving purely recurrent input (4) may produce a signal larger than threshold (arrowhead above the dotted line). Thus cells 4 and 6 would form an erroneous synapse. With a saturating activation function <bold>(right)</bold> the map amplitude stays constant throughout learning, and this confound does not happen. <bold>(F)</bold> The goal signal from an end node of the binary maze, plotted along the path from another end node. Map and goal synapses set to their optimal values assuming full knowledge of the graph and the target (gain <italic>γ</italic> = 0.37). With linear activation (red), the goal signal has a local maximum, so navigation to the target fails. With a saturating activation function (blue) the goal signal is monotonic and leads the agent to the target.</p></caption>
<graphic xlink:href="461751v3_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><xref rid="fig7" ref-type="fig">Figure 7B-D</xref> reports the range of navigation on the three sample graphs, defined and computed from simulations as in the preceding section (<xref rid="fig6" ref-type="fig">Fig 6</xref>). The effective range is the largest graph distance over which the median trajectory chooses the shortest route. As observed using linear map neurons (<xref rid="fig6" ref-type="fig">Fig 6</xref>), the range increases with the gain <italic>γ</italic> until it collapses beyond some maximal value (<xref rid="fig7" ref-type="fig">Fig 7B-D</xref>). However, the saturating activation function allowed for higher gain values, which led to considerable increases in the range of navigation: by a factor of 2.2 for the ring graph, and 1.5 for the Tower of Hanoi graph. On the binary maze, the saturating activation function allowed perfect navigation over the maximal distance available of 12 steps.</p>
<p>The enhanced performance was a result of better map learning as well as better navigation. To understand the former, consider <xref rid="fig7" ref-type="fig">Figure 7E</xref>: Here the agent has begun to learn the ring graph by walking back and forth between a few nodes (2-5), thus establishing all their pairwise map synapses; then it steps to a new node (6). With a linear activation function (<xref rid="fig7" ref-type="fig">Fig 7E</xref> left), the recurrent synapses enhance the map output, so the map signal with the agent in the explored region (2-5) is considerably larger than after stepping to the new node. This interferes with the mechanism for map learning: The learning rule must identify which of the map cells represents the current location of the agent, and does so by setting a threshold on the output signal (<xref ref-type="statement" rid="alg2">Alg 2</xref>). In the present example, this leads to erroneous synapses, because a map cell that receives only recurrent input (4) produces outputs larger than the threshold (arrowhead in <xref rid="fig7" ref-type="fig">Fig 7E</xref>). With the saturating activation function (<xref rid="fig7" ref-type="fig">Fig 7E</xref> right), the directly activated map cells always have the largest output signal, so the learning rule can operate without errors.</p>
<p>The saturating activation function also helps after learning is complete. In <xref rid="fig7" ref-type="fig">Figure 7F</xref>, the agent is given perfect knowledge of the binary maze map, then asked to use the resulting goal signals to navigate from one end node to another. With a linear activation function, the goal signal has a large local maximum that traps the agent. The nonlinear activation function produces a monotonic goal signal that leads the agent to the target.</p>
<p>Both these aspects of enhanced performance can be traced to the normalizing effect of the non-linearity, that keeps the peak output of the map constant. Such normalization could be performed by other mechanisms as well, for example a global inhibitory feedback among the map neurons.</p>
<p>In summary, this section shows that altering details of the model can substantially extend its performance. For the remainder of this article we will return to the linear activation curve, because interesting behavioral phenomena can be observed even with the simple linear model.</p>
</sec>
<sec id="s2h">
<label>2.8</label>
<title>Navigating a partial map: homing behavior</title>
<p>We have seen that endotaxis can learn both connections in the environment and the locations of targets after just one visit (<xref rid="fig6" ref-type="fig">Fig 6</xref>.) This suggests that the agent can navigate well on whatever portion of the environment it has already seen, before covering it exhaustively. To illustrate this we analyze an ethologically relevant instance.</p>
<p>Consider a mouse that enters an unfamiliar environment for the first time, such as a labyrinth constructed by fiendish graduate students [<xref ref-type="bibr" rid="c49">49</xref>]. Given the uncertainties about what lurks inside, the mouse needs to retain the ability to flee back to the entrance as fast as possible. For concreteness take the mouse trajectory in <xref rid="fig8" ref-type="fig">Figure 8A</xref>. The animal has entered the labyrinth (location 1), made its way to one of the end nodes (3), then explored further to another end node (4). Suppose it needs to return to the entrance now. One way would be to retrace all its steps. But the shorter way is to take a left at (2) and cut out the unnecessary branch to (3). Experimentally we found that mice indeed take the short direct route instead of retracing their path [<xref ref-type="bibr" rid="c49">49</xref>]. They can do so even on the very first visit of an unfamiliar labyrinth. Can endotaxis explain this behavior?</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><title>Homing by endotaxis.</title>
<p><bold>(A)</bold> A binary tree maze as used in [<xref ref-type="bibr" rid="c49">49</xref>]. A simulated mouse begins to explore the labyrinth (colored trajectory, purple=early, yellow=late), traveling from the entrance (1) to one of the end nodes (3), then to another end node (4). Can it return to the entrance from there using endotaxis? <bold>(B)</bold> Goal signal learned by the end of the walk in (A), displayed as in <xref rid="fig5" ref-type="fig">Fig 5D</xref>, purple=0. Note the goal signal is non-zero only at the nodes that have been encountered so far. From all those nodes it increases monotonically toward the entrance. <bold>(C)</bold> Detailed plot of the goal signal along the shortest route for homing. Parameters <italic>γ</italic> = 0.33, <italic>θ</italic> = 0.30, <italic>α</italic> = 10, <italic>ϵ</italic> = 0.01.</p></caption>
<graphic xlink:href="461751v3_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We assume that the entrance is a salient location, so the agent dedicates a goal cell to the root node of the binary tree. <xref rid="fig8" ref-type="fig">Figure 8B</xref> plots the goal signal after the path in panel A, just as the agent wants to return home. The goal signal is non-zero only at the locations the agent has visited along its path. It clearly increases monotonically towards the entrance (<xref rid="fig8" ref-type="fig">Fig 8C</xref>). At a noise level of <italic>ϵ</italic> = 0.01 the agent can navigate to the entrance by the shortest path without error. Note specifically that the agent does not retrace its steps when arriving at location (2), but instead turns toward (1).</p>
<p>One unusual aspect of homing is that the goal is identified first, before the agent has even entered the environment to explore it. That strengthens the goal synapse from the sole map cell that is active at the entrance. Only subsequently does the agent build up map synapses that allow the goal signal to spread throughout the map network. Still, in this situation, the single synapse onto the goal cell is sufficient to convey a robust signal for homing.</p>
</sec>
<sec id="s2i">
<label>2.9</label>
<title>Efficient patrolling</title>
<p>Beside exploring and exploiting, a third mode of navigating the environment is patrolling. At this stage the animal knows the lay of the land, and has perhaps discovered some special locations, but continues to patrol the environment for new opportunities or threats. In our study of mice freely interacting with a large labyrinth, the animals spent more than 85% of the time patrolling the maze [<xref ref-type="bibr" rid="c49">49</xref>]. This continued for hours after they had perfected the targeting of reward locations and the homing back to the entrance. Presumably, the goal of patrolling is to cover the entire environment frequently and efficiently so as to spot any changes as soon as they develop. So the ideal path in patrolling would visit every node on the graph in the smallest number of steps possible. In the binary-tree maze used for our experiments, that optimal patrol path takes 252 steps: It visits every end node of the labyrinth exactly once without any repeats (<xref rid="fig9" ref-type="fig">Fig 9A</xref>).</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9.</label>
<caption><title>Patrolling by endotaxis.</title>
<p><bold>(A) Left:</bold> A binary tree maze as used in [<xref ref-type="bibr" rid="c49">49</xref>], plotted here so every node has a different vertical offset. <bold>Right:</bold> A perfect patrol path through this environment. It visits every node in 252 steps, then starts over. <bold>(B)</bold> Patrolling efficiency of different agents on the binary tree maze. The focus here is on the 64 end nodes of the labyrinth. We ask how many distinct end nodes are found (vertical axis) as a function of the number of end nodes visited (horizontal axis). For the perfect patrolling path, that relationship is the identity (‘perfect’). For a random walk, the curve is shifted far to the right (‘random’, note log axis). Ten mice in [<xref ref-type="bibr" rid="c49">49</xref>] showed patrolling behavior within the shaded range. Solid lines are the endotaxis agent, operating at different noise levels <italic>ϵ</italic>. Note <italic>ϵ</italic> = 0.01 produces perfect patrolling; in fact, panel A is a path produced by this agent. Higher noise levels lead to lower efficiency. The behavior of mice corresponds to <italic>ϵ</italic>≈ 1. Gain <italic>γ</italic> = 0.33, habituation <italic>β</italic> = 1.2, with recovery time <italic>τ</italic> = 100 steps.</p></caption>
<graphic xlink:href="461751v3_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10.</label>
<caption><title>Forgetting a link during exploration.</title>
<p><bold>(A)</bold> Simulation of a random walk on a ring with 14 nodes as in <xref rid="fig4" ref-type="fig">Fig 4</xref>. Left: Layout of the ring, with resource locations marked in blue. The walk progresses in 1000 time steps (top to bottom); with the agent’s position marked in red (nodes 0-13, horizontal axis). At each time the color map shows the goal signal that would be produced if the agent were at position ‘Node’. White horizontal lines mark the appearance of a new link between nodes 4 and 11 at <italic>t</italic> = 200, and disappearance of that link at <italic>t</italic> = 400. <bold>(B)</bold> The matrix <bold>M</bold> of map synapses at various times. The pixel in row <italic>i</italic> and column <italic>j</italic> represents the matrix element <italic>M</italic><sub><italic>ij</italic></sub>. Color purple = 0. Note the first few steps (number above graph) each add a new synapse. Eventually, <bold>M</bold> reflects the adjacency matrix of nodes on the graph, and changes as a link is added and removed. <bold>(C)</bold> Goal signals just before and just after the agent travels the new link. <bold>(D)</bold> Goal signals just before the link disappears and at the end of the walk. <bold>(E)</bold> Strength of two synapses in the map, <italic>M</italic><sub>4,5</sub> and <italic>M</italic><sub>4,11</sub>, plotted against time during the random walk. Model parameters: <italic>γ</italic> = 0.32, <italic>θ</italic> = 0.27, <italic>α</italic> = 0.3, <italic>δ</italic> = 0.1.</p></caption>
<graphic xlink:href="461751v3_fig10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Real mice don’t quite execute this optimal path, but their patrolling behavior is much more efficient than random (<xref rid="fig9" ref-type="fig">Fig 9B</xref>). They avoid revisiting areas they have seen recently. Could endotaxis implement such an efficient patrol of the environment? The task is to steer the agent to locations that haven’t been visited recently. One can formalize this by imagining a resource called “neglect” distributed throughout the environment. At each location neglect increases with time, then resets to zero the moment the agent visits there. To use this in endotaxis one needs a goal cell that represents neglect.</p>
<p>We add to the core model a goal cell that represents “neglect”. It receives excitation from every map cell, via synapses that are equal and constant in strength (see clock symbol in <xref rid="fig1" ref-type="fig">Fig 1B</xref>). This produces a goal signal that is approximately constant everywhere in the environment. Now suppose that the point neurons undergo a form of habituation: When a point cell fires because the agent walks through its field, its sensitivity decreases by some habituation factor. That habituation then decays over time until the point cell recovers its original sensitivity. As a result, the most recently visited points on the graph produce a smaller goal signal. Endotaxis based on this goal signal will therefore lead the agent to the areas most in need of a visit.</p>
<p><xref rid="fig9" ref-type="fig">Figure 9B</xref> illustrates that this is a powerful way to implement efficient patrols. Here we modeled en-dotaxis on the binary-tree labyrinth, using the standard parameters useful for exploration, exploitation, and homing in previous sections. To this we added a habituation in the point cells with exponential recovery dynamics. Formally, the procedure is defined by <xref ref-type="statement" rid="alg3">Algorithm 3</xref>. Again, we turned off the learning rules (<xref ref-type="statement" rid="alg2">Alg 2</xref>) during this simulation, to observe the effects of habituation in isolation. A fully functioning agent can keep the learning rules on at all times (<xref rid="fig11" ref-type="fig">Fig 11</xref>).</p>
<fig id="fig11" position="float" fig-type="figure">
<label>Figure 11.</label>
<caption><title>Navigation performance with and without habituation.</title>
<p>Navigated distance on the binary-tree maze, displayed as in <xref rid="fig5" ref-type="fig">Fig 5E</xref>. <bold>Left:</bold> An agent with strong habituation: <italic>β</italic> = 1.2, <italic>τ</italic> = 100. <bold>Right:</bold> no habituation: <italic>β</italic> = 0. The agent learned the map and the goal signals for every node during a random walk with 30,000 steps. Then the agent navigated between all pairs of points on the maze. Graphs show the median ± 10/90 percentile of the navigated distance for all routes with the same graph distance. Other model parameters: <italic>γ</italic> = 0.33, <italic>θ</italic> = 0.30, <italic>α</italic> = 0.1, <italic>ϵ</italic> as listed.</p></caption>
<graphic xlink:href="461751v3_fig11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<statement id="alg3">
<label>Algorithm 3</label>
<title>Patrolling</title>
<p><fig id="alg3a" position="float" fig-type="figure">
<graphic xlink:href="461751v3_alg3.tif" mimetype="image" mime-subtype="tiff"/>
</fig></p>
</statement>
<p>With appropriate choices of habituation <italic>β</italic> and recovery time <italic>τ</italic> the agent does in fact execute a perfect patrol path on the binary tree, traversing every edge of the graph exactly once, and then repeating that sequence indefinitely (<xref rid="fig9" ref-type="fig">Fig 9A</xref>). For this to work, some habituation must persist for the time taken to traverse the entire tree; in this simulation we used <italic>τ</italic> = 100 steps on a graph that requires 252 steps. As in all applications of endotaxis, the performance also depends on the readout noise <italic>ϵ</italic>. For increasing readout noise, the agent’s behavior transitions gradually from the perfect patrol to a random walk (<xref rid="fig9" ref-type="fig">Fig 9B</xref>). The patrolling behavior of real mice is situated about halfway along that range, at an equivalent readout noise of <italic>ϵ</italic> = 1 (<xref rid="fig9" ref-type="fig">Fig 9B</xref>).</p>
<p>Finally, this suggests a unified explanation for exploration and patrolling: In both modes, the agent follows the output of the “neglect” cell, which is just the sum total of the map output. However, in the early exploration phase, when the agent is still assembling the cognitive map, it gives the neglect signal zero or low weight, so the turning decisions are dominated by the readout noise and produce something close to a random walk. Later on, the agent assigns a higher weight to the neglect signal, so it exceeds the readout noise, and shifts the behavior towards systematic patrolling. In our simulations, an intrinsic readout noise of <italic>ϵ</italic> = 0.01 is sufficiently low to enable even a perfect patrol path (<xref rid="fig9" ref-type="fig">Fig 9B</xref>).</p>
<p>In summary, the core model of endotaxis can be enhanced by adding a basic form of habituation at the input neurons. This allows the agent to implement an effective patrolling policy that steers towards regions which have been neglected for a while. Of course, habituation among point cells will also change the dynamics of map learning during the exploration phase. We found that both map and goal synapses are still learned effectively, and navigation to targets is only minimally affected by habituation (Suppl Fig 11).</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<sec id="s3a">
<label>3.1</label>
<title>Summary of claims</title>
<p>We have presented a biologically plausible neural mechanism that can support learning, navigation, and problem solving in complex environments. The algorithm, called <italic>endotaxis</italic>, offers an end-to-end solution for assembling a cognitive map (<xref rid="fig4" ref-type="fig">Fig 4</xref>), memorizing interesting targets within that map, navigating to those targets (<xref rid="fig5" ref-type="fig">Fig 5</xref>), as well as accessory functions like instant homing (<xref rid="fig8" ref-type="fig">Fig 8</xref>) and effective patrolling (<xref rid="fig9" ref-type="fig">Fig 9</xref>). Conceptually, it is related to chemotaxis, namely the ability to follow an odor signal to its source, which is shared universally by most or all motile animals. The endotaxis network creates an internal “virtual odor” which the animal can follow to reach any chosen target location (<xref rid="fig1" ref-type="fig">Fig 1</xref>). When the agent begins to explore the environment, the network learns both the structure of the space, namely which points are connected, and the location of valuable resources (<xref rid="fig4" ref-type="fig">Fig 4</xref>), even after a single experience (<xref rid="fig4" ref-type="fig">Figs 4</xref>, <xref rid="fig8" ref-type="fig">8</xref>). The agent can then navigate back to those target locations efficiently from any point in the environment (<xref rid="fig5" ref-type="fig">Fig 5</xref>). Beyond spatial navigation, endotaxis can also learn the solution to purely cognitive tasks (<xref rid="fig5" ref-type="fig">Fig 5</xref>) that can be formulated as search on a graph (Sec 2.2). It takes as given two elementary facts: the existence of place cells that fire when the animal is at a specific location; and a behavioral module that allows the animal to follow an odor gradient uphill. The proposed circuit (<xref rid="fig1" ref-type="fig">Fig 1</xref>) provides the interface from the place cells to the virtual odor gradient. In the following sections we consider how these findings relate to phenomena of animal behavior and neural circuitry, and prior art in the area of theory and modeling.</p>
</sec>
<sec id="s3b">
<label>3.2</label>
<title>Theories and models of spatial learning</title>
<p>Broadly speaking, endotaxis can be seen as a form of reinforcement learning [<xref ref-type="bibr" rid="c60">60</xref>]: The agent learns from rewards or punishments in the environment and develops a policy that allows for subsequent navigation to special locations. The goal signal in endotaxis plays the role of a value function in reinforcement learning theory. From experience, the agent learns to compute that value function for every location and control its actions accordingly. Within the broad universe of reinforcement learning algorithms, endotaxis combines some special features as well as limitations that are inspired by empirical phenomena of animal learning, and also make it suitable for a biological implementation.</p>
<p>First, most of the learning happens without any reinforcement. During the exploratory random walk, endotaxis learns the topology of the environment, specifically by updating the synapses in the map network (<bold>M</bold> in <xref rid="fig1" ref-type="fig">Fig 1B</xref>). Rewards are not needed for this map learning, and indeed the goal signal remains zero during this period (<xref rid="fig4" ref-type="fig">Fig 4</xref>). Once a reward is encountered, the goal synapses (<bold>G</bold> in <xref rid="fig1" ref-type="fig">Fig 1B</xref>) get set, and the goal signal instantly spreads through the known portion of the environment. Thus, the agent learns how to navigate to the goal location from a single reinforcement (<xref rid="fig4" ref-type="fig">Fig 4</xref>). This is possible because the ground has been prepared, as it were, by learning a map. In animal behavior the acquisition of a cognitive map without rewards is called <italic>latent learning</italic>. Early debates in animal psychology pitched latent learning and reinforcement learning as alternative explanations [<xref ref-type="bibr" rid="c63">63</xref>]. Instead, in the endotaxis algorithm, neither can function without the other, as the goal signal explicitly depends on both the map and goal synapses (<xref ref-type="disp-formula" rid="eqn17">Eqn 17</xref>, <xref ref-type="statement" rid="alg2">Alg 1</xref>).</p>
<p>More specifically, the neural signals in endotaxis bear some similarity to the so-called <italic>successor representation</italic> [<xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c55">55</xref>]. This is a proposal for how the brain might encode the current state of the agent, intended to simplify the mathematics of time-difference reinforcement learning. In that representation, there is a neuron for every state of the agent, and the activity of neuron <italic>j</italic> is the time-discounted probability that the agent will find itself at state <italic>j</italic> in the future. Similarly, the output of the endotaxis map network is related to future states of the agent and follows a similar functional dependence on distance [<xref ref-type="bibr" rid="c40">40</xref>] (<xref ref-type="disp-formula" rid="eqn6">Eqn 6</xref>). However, despite these formal similarities, the underlying logic is quite different. In the successor representation, <italic>γ</italic> plays the role of a temporal discount factor for rewards [<xref ref-type="bibr" rid="c14">14</xref>]; essentially it is the proportionality factor in the agent’s belief that “time is money”. In this picture, varying <italic>γ</italic> allows the agent to make predictions with different time horizons [<xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c55">55</xref>]. In endotaxis, there is no time/reward tradeoff. The agent simply wants the shortest path to the goal. The map network reflects the objective connectivity of the environment to the farthest extent possible. Here <italic>γ</italic> is the gain of the map neurons that, when properly chosen, allows the neural network to perform that computation. The agent may want to tune <italic>γ</italic> to the statistics of the environment, although we showed that a common value of <italic>γ</italic> works quite well across environments (<xref rid="fig6" ref-type="fig">Fig 6</xref>). <sup><xref ref-type="fn" rid="fn4">4</xref></sup></p>
<p>Second, endotaxis does not tabulate the list of available actions at each state. That information remains externalized in the environment: The agent simply tries whatever actions are available at the moment, then picks the best one. This is a characteristically biological mode of action and most organisms have a behavioral routine that executes such trial-and-error. This “externalized cognition” simplifies the learning task: For any given navigation policy the agent needs to learn only one scalar function of location, namely the goal signal. By comparison, many machine learning algorithms develop a value function for state-action pairs, which then allows more sophisticated planning [<xref ref-type="bibr" rid="c41">41</xref>, <xref ref-type="bibr" rid="c60">60</xref>]. The relative simplicity of the endotaxis circuit depends on the limitation to learning only state functions.</p>
<p>Some key elements of the endotaxis model have appeared in prior work, starting with the notion of ascending a scalar goal signal during navigation [<xref ref-type="bibr" rid="c50">50</xref>, <xref ref-type="bibr" rid="c52">52</xref>, <xref ref-type="bibr" rid="c66">66</xref>]. Several models assume the existence of a map layer, in which individual neurons stand for specific places, and the excitatory synapses between neurons represent the connections between those places [<xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c33">33</xref>, <xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c47">47</xref>, <xref ref-type="bibr" rid="c53">53</xref>, <xref ref-type="bibr" rid="c65">65</xref>, <xref ref-type="bibr" rid="c66">66</xref>]. Then the agent somehow reads out those connections in order to find the shortest path between its current location (the start node) and a desired target (the end node).</p>
<p>Very different schemes have been proposed for this readout of the map. The most popular scheme is to somehow inject a signal into the desired end node, let it propagate backward through the network, and read out the magnitude or gradient of the signal near the start node [<xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c33">33</xref>, <xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c47">47</xref>]. In general this requires some accessory system that can look up which neuron in the map corresponds to the desired end node, and which neuron to the agent’s current location or its neighbors; often these accessory functions remain unspecified [<xref ref-type="bibr" rid="c33">33</xref>, <xref ref-type="bibr" rid="c53">53</xref>, <xref ref-type="bibr" rid="c66">66</xref>]. By contrast, in the endotaxis model the signal is propagated in the forward direction starting with the activity of the place cell at the agent’s current location. The signal strength is read out at the goal location: The goal neuron is the same neuron that also responds directly to the rewarding feature at the goal location. For example the proximity to water is read out by a neuron that is also excited when the animal drinks water. In this way the brain doesn’t need to maintain a separate lookup table for goal neurons. If the agent wants to find water, it should simply follow the same neuron that fires when it drinks.</p>
<p>Another distinguishing feature of endotaxis is that it operates continuously. Many models for navigation have to separate the phase of spatial learning from the phase of goal-directed navigation. Sometimes plasticity needs to be turned off or reset during one phase or the other [<xref ref-type="bibr" rid="c47">47</xref>, <xref ref-type="bibr" rid="c50">50</xref>]. Sometimes a special signal must be injected during goal-seeking [<xref ref-type="bibr" rid="c66">66</xref>]. Sometimes the rules change depending on whether the agent approaches or leaves a target [<xref ref-type="bibr" rid="c8">8</xref>]. Again this requires additional supervisory systems that often go unexplained. By contrast, endotaxis is “always on”. Whether the animal explores a new environment, navigates to a target, or patrols a well-known graph, the synaptic learning rules are always the same. The animal chooses its policy by setting the mode switch that selects one of the available goal signals for the taxis module (<xref rid="fig1" ref-type="fig">Fig 1</xref>). Nothing has to change under the hood in the operation of the circuit. All the same signals are used for map learning, target learning, and navigation.</p>
<p>In summary, various components of the endotaxis model have appeared in other proposed schemes for spatial learning and navigation. The present model stands out in that all the essential functions are covered in a feed-forward and neuromorphically plausible manner, without invoking unexplained control schemes.</p>
</sec>
<sec id="s3c">
<label>3.3</label>
<title>Animal behavior</title>
<p>The millions of animal species no doubt use a wide range of mechanisms to get around their environment, and it is worth specifying which types of navigation endotaxis might solve. First, the learning mechanism proposed here applies to complex environments, namely those in which discrete paths form sparse connections between points. For a rodent and many other terrestrial animals, the paths they may follow are usually constrained by obstacles or by the need to remain under cover. In those conditions the brain cannot assume that the distance between points is given by euclidean geometry, or that beacons for a goal will be visible in a straight line from far away, or that a target can be reached by following a known heading. As a concrete example, a mouse wishing to exit from deep inside a labyrinth (<xref rid="fig8" ref-type="fig">Fig 8A</xref>, [<xref ref-type="bibr" rid="c49">49</xref>]) can draw little benefit from knowing the distance and heading of the entrance.</p>
<p>Second, we are focusing on the early experience with a new environment. Endotaxis can get an animal from zero knowledge to a cognitive map that allows reliable navigation towards goals discovered on a previous foray. It explains how an animal can return home from inside a complex environment on the first attempt [<xref ref-type="bibr" rid="c49">49</xref>], or navigate to a special location after encountering it just once (<xref rid="fig6" ref-type="fig">Figs 6</xref>, <xref rid="fig8" ref-type="fig">8</xref>). But it does not implement more advanced routines of spatial reasoning, such as stringing a habitual sequence of actions together into one, or deliberating internally to plan entire routes. Clearly, given enough time in an environment, animals may develop algorithms other than the beginner’s choice proposed here.</p>
<p>A key characteristic of endotaxis, distinct from other forms of navigation, is the reliance on trial-and-error. The agent does not deliberate to plan the shortest path to the goal. Instead, it finds the shortest path by locally sampling the real-world actions available at its current point, and choosing the one that maximizes the virtual odor signal. In fact, there is strong evidence that animals navigate by real-world trial-and-error, at least in the early phase of learning [<xref ref-type="bibr" rid="c48">48</xref>]. Lashley [<xref ref-type="bibr" rid="c36">36</xref>], in his first scientific paper on visual discrimination in the rat, reported that rats at a decision point often hesitate “with a swaying back and forth between the passages”. These actions – called “vicarious trial and error” – look eerily like sniffing out an odor gradient, but they occur even in absence of any olfactory cues. Similar behaviors occur in arthropods [<xref ref-type="bibr" rid="c62">62</xref>] and humans [<xref ref-type="bibr" rid="c51">51</xref>] when poised at a decision point. We suggest that the animal does indeed sample a gradient, not of an odor, but of an internally generated virtual odor that reflects the proximity to the goal. The animal seems to use the same policy of spatial sampling that it would apply to a real odor signal.</p>
<p>Frequently, a rodent stopped at a maze junction merely turns its head side-to-side, rather than walking down a corridor to sample the gradient. Within the endotaxis model, this could be explained if some of the point cells in the lowest layer (<xref rid="fig1" ref-type="fig">Fig 1B</xref>) are selective for head direction or for the view down a specific corridor. During navigation, activation of that “direction cell” systematically precedes activation of point cells further down that corridor. Therefore, the direction cell gets integrated into the map network. From then on, when the animal turns in that direction, this action takes a step along the graph of the environment without requiring a walk in ultimately fruitless directions. In this way the agent can sample the goal gradient while minimizing energy expenditure.</p>
<p>Once the animal gains familiarity with the environment, it performs fewer of the vicarious trial and error movements, and instead moves smoothly through multiple intersections in a row [<xref ref-type="bibr" rid="c48">48</xref>]. This may reflect a transition between different modes of navigation, from the early endotaxis, where every action gets evaluated on its real-world merit, to a mode where many actions are strung together into behavioral motifs. Eventually the animal may also develop an internal forward model for the effects of its own actions, which would allow for prospective planning of an entire route [<xref ref-type="bibr" rid="c32">32</xref>, <xref ref-type="bibr" rid="c46">46</xref>]. An interesting direction for future research is to seek a neuromorphic circuit model for such action planning; perhaps it can be built naturally on top of the endotaxis circuit.</p>
</sec>
<sec id="s3d">
<label>3.4</label>
<title>Brain circuits</title>
<p>The key elements in the proposed circuitry (<xref rid="fig1" ref-type="fig">Fig 1</xref>) are: A large population of neurons with sparsely selective responses; massive convergence from that population onto a smaller set of output neurons; and synaptic plasticity at the output neurons gated by signals from the animal’s experience. A prominent instance of this motif is found in the mushroom body of the arthropod brain [<xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c58">58</xref>]. Here the Kenyon cells, with their sparse odor responses [<xref ref-type="bibr" rid="c57">57</xref>] play the role of both point and map cells. They are strongly recurrently connected, in fact most of the Kenyon cell output synapses are onto other Kenyon cells [<xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c61">61</xref>]. Kenyon cells converge onto a much smaller set of mushroom body output neurons [<xref ref-type="bibr" rid="c3">3</xref>], which play the role of goal cells. Plasticity at the synapse between Kenyon cells and output neurons is gated by neuromodulators that encode rewards or punishments [<xref ref-type="bibr" rid="c11">11</xref>]. Mushroom body output neurons are known to guide the turning decisions of the insect [<xref ref-type="bibr" rid="c3">3</xref>], perhaps through their projections to the central complex [<xref ref-type="bibr" rid="c37">37</xref>], an area critical to the animal’s turning behavior [<xref ref-type="bibr" rid="c30">30</xref>]. Conceivably, this is where the insect’s basic chemotaxis module is implemented.</p>
<p>In the conventional view, the mushroom body helps with odor discrimination and forms memories of discrete odors that are associated with salient experience [<xref ref-type="bibr" rid="c27">27</xref>]. Subsequently the animal can seek or avoid those odors. But the endotaxis model suggests a different interpretation: Insects can also use odors as landmarks in the environment. In this more general form of navigation, the odor is not a goal in itself, but serves to mark a route towards some entirely different goal [<xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c56">56</xref>]. A Kenyon cell, through its sparse odor selectivity, may be active at only one place in the environment, and thus provide the required location-selective input to the endotaxis circuit. Recurrent synapses among Kenyon cells will learn the connectivity among these odor-defined locations, and the output neurons will learn to produce a goal signal that leads the insect to a rewarding location, which itself may not even have a defined odor.</p>
<p>Bees and certain ants rely strongly on vision for their navigation. Here the insect uses discrete panoramic views of the landscape as markers for its location [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c59">59</xref>, <xref ref-type="bibr" rid="c67">67</xref>]. In those species the mushroom body receives massive input from visual areas of the brain. If the Kenyon cells respond sparsely to the landscape views, like the point cells in <xref rid="fig1" ref-type="fig">Figure 1</xref>, then the mushroom body can tie together these discrete vistas into a cognitive map that supports navigation towards arbitrary goal locations.</p>
<p>The same circuit motifs are commonly found in other brain areas, including the mammalian neocortex and hippocampus. While the synaptic circuitry there is less understood than in the insect brain, one can record from neurons more conveniently. Much of that work on neuronal signals during navigation has focused on the rodent hippocampal formation [<xref ref-type="bibr" rid="c43">43</xref>], and it is instructive to compare these recordings to the expectations from the endotaxis model. The three cell types in the model – point cells, map cells, and goal cells – all have place fields, in that they fire preferentially in certain regions within the graph of the environment. However, they differ in important respects:</p>
<p>The place field is smallest for a point cell; somewhat larger for a map cell, owing to recurrent connections in the map network; and larger still for goal cells, owing to additional pooling in the goal network. Such a wide range of place field sizes has indeed been observed in surveys of the rodent hippocampus, spanning at least a factor of 10 in diameter [<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c68">68</xref>]. Some place cells show a graded firing profile that fills the available environment. Furthermore one finds more place fields near the goal location of a navigation task, even when that location has no overt markers [<xref ref-type="bibr" rid="c29">29</xref>]. Both of those characteristics are expected of the goal cells in the endotaxis model.</p>
<p>The endotaxis model assumes that point cells exist from the very outset in any environment. Indeed, many place cells in the rodent hippocampus appear within minutes of the animal’s entry into an arena [<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c68">68</xref>]. Furthermore, any given environment activates only a small fraction of these neurons. Most of the “potential place cells” remain silent, presumably because their sensory trigger feature doesn’t match any of the locations in the current environment [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c18">18</xref>]. In the endotaxis model, each of these sets of point cells is tied into a different map network, which would allow the circuit to maintain multiple cognitive maps in memory [<xref ref-type="bibr" rid="c45">45</xref>].</p>
<p>Goal cells, on the other hand, are expected to have large place fields, centered on a goal location, but extending over much of the environment, so the animal can follow the gradient of their activity [<xref ref-type="bibr" rid="c10">10</xref>]. Indeed such cells have been reported in rat cortex [<xref ref-type="bibr" rid="c28">28</xref>]. In the endotaxis model, a goal cell appears suddenly when the animal first arrives at a memorable location, the input synapses from the map network are potentiated, and the neuron immediately develops a place field (<xref rid="fig4" ref-type="fig">Fig 4</xref>). This prediction is reminiscent of a startling experimental observation in recordings from hippocampal area CA1: A neuron can suddenly start firing with a fully formed place field that may be located anywhere in the environment [<xref ref-type="bibr" rid="c7">7</xref>]. This event appears to be triggered by a calcium plateau potential in the dendrites of the place cell, which potentiates the excitatory synaptic inputs the cell receives. A surprising aspect of this discovery was the large extent of the resulting place field, which requires the animal several seconds to cover. Subsequent cellular measurements indeed revealed a plasticity mechanism that extends over several seconds [<xref ref-type="bibr" rid="c38">38</xref>]. The endotaxis model relies on just such a plasticity rule for map learning (<xref ref-type="statement" rid="alg2">Alg 2</xref>), that can correlate events at subsequent nodes on the agent’s trajectory.</p>
</sec>
<sec id="s3e">
<label>3.5</label>
<title>Outlook</title>
<p>Endotaxis is a hypothetical neural circuit solution to the problems of spatial exploration, learning, and navigation. Its compact circuit structure and all-in-one functionality suggest that it would fit in even the smallest brains. Effectively, endotaxis represents a brain module that could be interposed between a spatial-sensing module, that produces place cells, and a taxis module, that delivers the movements to ascend a goal signal. It further relies on some high-level policy that sets the “mode switch” by which the animal chooses what goal to pursue. Future research might get at this behavioral control mechanism through a program of anatomical module tracing: First find the neural circuit that controls chemotaxis behavior. Then test if that module receives a convergence of goal signals from other circuits with non-olfactory information. If so, the mechanism of arbitrage that routes one or another goal signal to the taxis module should reveal the high-level coordination of the animal’s behavior. <sup><xref ref-type="fn" rid="fn5">5</xref></sup> Given the recent technical developments in mapping the connectome [<xref ref-type="bibr" rid="c16">16</xref>], we believe that such a program of module-tracing is within reach, probably first for the insect brain.</p>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Methods and Supplementary Information</title>
<sec id="s4a">
<label>4.1</label>
<title>Simulations</title>
<p>Numerical simulations were performed as described, see <xref ref-type="statement" rid="alg1">Algorithms 1</xref>, <xref ref-type="statement" rid="alg2">2</xref>, <xref ref-type="statement" rid="alg3">3</xref>, <xref ref-type="statement" rid="alg4">4</xref>. Parameter settings are listed in the text and figure captions. The sensitivity to parameters is reported in <xref rid="fig6" ref-type="fig">Figure 6</xref>. Code that produced all the results is available in a public repository.</p>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Average navigated distance</title>
<p>In the text we often assess the performance of an endotaxis agent by considering point-to-point navigation between all pairs of points on a graph. Given the readout noise <italic>ϵ</italic> that affects the goal signal, navigation is a stochastic process with many random decisions along the route. Different random instantiations of the process will produce routes of different lengths. Fortunately, there is a way to calculate the expectation value of the route length without any Monte-Carlo simulation.</p>
<p>Consider navigation to goal node <italic>y</italic>. From the state of the network (<bold>M</bold> and <bold>G</bold>) we compute the goal signal <italic>E</italic><sub><italic>yj</italic></sub> at every node <italic>j</italic>. When the agent is at node <italic>j</italic> it chooses among the neighbor nodes the one with the highest sum of goal signal and noise (<xref ref-type="statement" rid="alg1">Alg 1</xref>). Based on the goal signal <italic>E</italic><sub><italic>yj</italic></sub> and the noise <italic>ϵ</italic> one can compute the probability for each such possible step from <italic>j</italic>. This leads to a transition matrix for the random walk
<disp-formula id="ueqn4">
<graphic xlink:href="461751v3_ueqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Subsequent decisions along the route are independent of each other. Hence the process is a Markov chain. Then we make use of a well-known result for first-capture times on a Markov chain to compute the expected number of steps to arrival at <italic>y</italic> starting from any node <italic>x</italic>.</p>
<p>Note the method assumes that the process is stationary Markov, such that the goal signal <italic>E</italic><sub><italic>xy</italic></sub> does not change in the course of navigation. In our analysis of patrolling (<xref rid="fig9" ref-type="fig">Figs 9</xref> and <xref rid="fig11" ref-type="fig">11</xref>) this assumption is violated, because the habituation state of the point cells depends on what path the agent took to the current node. In those cases we resorted to Monte Carlo simulations to estimate the distribution of route lengths.</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Nonlinear activation function</title>
<p>The activation function of a map neuron is the relationship of input to output
<disp-formula id="eqn20">
<graphic xlink:href="461751v3_eqn20.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where (<xref ref-type="disp-formula" rid="eqn4">Eqn 4</xref>)
<disp-formula id="eqn21">
<graphic xlink:href="461751v3_eqn21.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
is the input to the map neuron. Most of the report assumes a linear activation function (<xref ref-type="disp-formula" rid="eqn3">Eqn 3</xref>)
<disp-formula id="eqn22">
<graphic xlink:href="461751v3_eqn22.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
For <xref rid="fig7" ref-type="fig">Figure 7</xref> we used a saturating function instead (<xref ref-type="disp-formula" rid="eqn19">Eqn 19</xref>):
<disp-formula id="eqn23">
<graphic xlink:href="461751v3_eqn23.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The recurrent network equation <italic>v</italic><sub><italic>i</italic></sub> = <italic>f</italic> (<italic>u</italic><sub><italic>i</italic></sub>+ ∑<sub><italic>j</italic></sub> <italic>M</italic><sub><italic>ij</italic></sub><italic>v</italic><sub><italic>j</italic></sub>) was solved using Python’s <monospace>fsolve</monospace>.</p>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>Forgetting of links and resources</title>
<p>In <xref ref-type="sec" rid="s2c">section 2.3</xref> we discuss the learning algorithm that acquires the connectivity of the environment and the locations of resources. It reacts rapidly to the appearance of new links in the environment: As soon as the agent travels from one point to another, the synapse between the corresponding map cells gets established. Suppose now that a previously existing link becomes blocked: How can one remove the corresponding synapse from the map? A simple solution would be to let all synapses decay over time, balanced by strengthening whenever a link gets traveled. In that case the entire map would be forgotten when the animal goes to sleep for a few hours, whereas it is clear that animals retain such maps over many days. Instead, one wants a mode of <italic>active</italic> forgetting: Memory of the link from node <italic>i</italic> to <italic>j</italic> should be weakened only if the agent find itself at node <italic>i</italic> and repeatedly chooses not to go to <italic>j</italic>. We formalize this in <xref ref-type="statement" rid="alg4">Algorithm 4</xref>, which differs only slightly from <xref ref-type="statement" rid="alg2">Algorithm 2</xref>.</p>
<p>Here the added parameter <italic>δ</italic> determines how much a map synapse gets depressed each time the corresponding link is not chosen. Similarly, goal synapses decay if their prediction for a resource exceeds the resource signal received by the goal cell. The synaptic learning rule resembles the BCM rule [<xref ref-type="bibr" rid="c6">6</xref>]: Synaptic modification is conditional on presynaptic activity, and leads to either potentiation or depression depending on the level of post-synaptic activity.</p>
<statement id="alg4">
<label>Algorithm 4</label>
<title>Learning and forgetting</title>
<p><fig id="alg4a" position="float" fig-type="figure">
<graphic xlink:href="461751v3_alg4.tif" mimetype="image" mime-subtype="tiff"/>
</fig></p>
</statement>
<p><xref rid="fig10" ref-type="fig">Figure 10</xref> illustrates this process with a simulation analogous to <xref rid="fig4" ref-type="fig">Fig 4</xref>. The agent explores a ring graph by a random walk. At some point a new link appears clear across the ring. Later on that link disappears again. Acquisition of the link happens very quickly, within a single time step (<xref rid="fig10" ref-type="fig">Fig 10A, C</xref>). Forgetting that link takes longer, on the order of several hundred steps (<xref rid="fig10" ref-type="fig">Fig 10A, D, E</xref>). In this simulation <italic>δ</italic> = 0.1, so the map synapses decay by about 10% whenever a link is not traveled. One could, of course, accelerate that with a higher <italic>δ</italic>, but at the cost of destabilizing the entire map. Even the synapses for intact links get depressed frequently (<xref rid="fig10" ref-type="fig">Fig 10E</xref>), because the random choices of the agent lead it to take any given link only a fraction of the time.</p>
<p>One limitation of the endotaxis agent is that it does not keep a record of what actions are available at each node. Instead, it leaves that information in the environment (see Discussion) and simply tries all the actions that are available. When faced with a blocked tunnel, the endotaxis agent does not know that this was previously available. Clearly, a more advanced model of the world that includes a state-action table would allow more effective editing of the cognitive map.</p>
</sec>
<sec id="s4e">
<label>4.5</label>
<title>Habituation in point cells</title>
<p>In <xref ref-type="sec" rid="s2i">section 2.9</xref> we discuss an extension of the core endotaxis model in which a point neuron undergoes habituation after the agent passes through its node. With every visit, the neuron’s sensitivity declines by a factor e<sup><italic>−β</italic></sup>. Between visits the sensitivity gradually returns towards 1 with an exponential recovery time of <italic>τ</italic> steps, see <xref ref-type="statement" rid="alg3">Algorithm 3</xref>.</p>
<p>This addition to the model changes the dynamics of the network input throughout the phases of exploration, navigation, and patrolling. We explored how the resulting performance is affected, by applying a strong habituation that decays slowly (<italic>β</italic> = 1.2, <italic>τ</italic> = 100) and comparing to the basic model with no habituation (<italic>β</italic> = 0). During the learning phase, when the map and goal synapses are established via a random walk, the main change is that it takes somewhat longer to learn the map. This is because synaptic updates happen only when both pre- and post-synaptic map cells exceed a threshold (see <xref ref-type="statement" rid="alg2">Alg 2</xref>), and that requires that both of the respective point neurons be in a high-sensitivity state. Remarkably all the parameter settings (<italic>γ, θ, α</italic>) that support learning and navigating under standard conditions (<xref rid="fig6" ref-type="fig">Fig 6</xref>), also work well when habituation takes place.</p>
<p>To illustrate the overall effect that habituation has on performance, we simulated learning and navigation on the binary-tree graph of <xref rid="fig9" ref-type="fig">Fig 9</xref>. For every pair of start and end nodes we asked how the actual navigated distance compared to the shortest graph distance. <xref rid="fig11" ref-type="fig">Figure 11</xref> shows that performance is affected only slightly. At the standard noise value <italic>ϵ</italic> = 0.01 used in other simulations, the range of navigation extends over 10 or more steps under both conditions.</p>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><surname>Aboitiz</surname>, <given-names>F.</given-names></string-name> and <string-name><surname>Montiel</surname>, <given-names>J. F.</given-names></string-name> (<year>2015</year>). <article-title>Olfaction, navigation, and the origin of isocortex</article-title>. <source>Frontiers in Neuroscience</source>, <volume>9</volume>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><surname>Alme</surname>, <given-names>C. B.</given-names></string-name>, <string-name><surname>Miao</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Jezek</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Treves</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Moser</surname>, <given-names>E. I.</given-names></string-name>, and <string-name><surname>Moser</surname>, <given-names>M.-B.</given-names></string-name> (<year>2014</year>). <article-title>Place cells in the hippocampus: Eleven maps for eleven rooms</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>111</volume>(<issue>52</issue>):<fpage>18428</fpage>–<lpage>18435</lpage>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><surname>Aso</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Sitaraman</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ichinose</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kaun</surname>, <given-names>K. R.</given-names></string-name>, <string-name><surname>Vogt</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Belliart-Guerin</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Placais</surname>, <given-names>P. Y.</given-names></string-name>, <string-name><surname>Robie</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Yamagata</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Schnaitmann</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Rowell</surname>, <given-names>W. J.</given-names></string-name>, <string-name><surname>Johnston</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Ngo</surname>, <given-names>T. T.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Korff</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Nitabach</surname>, <given-names>M. N.</given-names></string-name>, <string-name><surname>Heberlein</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Preat</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Branson</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Tanimoto</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Rubin</surname>, <given-names>G. M.</given-names></string-name> (<year>2014</year>). <article-title>Mushroom body output neurons encode valence and guide memory-based action selection in Drosophila</article-title>. <source>Elife</source>, <volume>3</volume>:<fpage>e04580</fpage>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><surname>Baker</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Dickinson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Findley</surname>, <given-names>T. M.</given-names></string-name>, <string-name><surname>Gire</surname>, <given-names>D. H.</given-names></string-name>, <string-name><surname>Louis</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Suver</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Verhagen</surname>, <given-names>J. V.</given-names></string-name>, <string-name><surname>Nagel</surname>, <given-names>K. I.</given-names></string-name>, and <string-name><surname>Smear</surname>, <given-names>M. C.</given-names></string-name> (<year>2018</year>). <article-title>Algorithms for Olfactory Search across Species</article-title>. <source>The Journal of Neuroscience</source>, <volume>38</volume>(<issue>44</issue>):<fpage>9383</fpage>–<lpage>9389</lpage>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><surname>Berg</surname>, <given-names>H. C.</given-names></string-name> (<year>1988</year>). <article-title>A physicist looks at bacterial chemotaxis</article-title>. <source>Cold Spring Harb Symp Quant Biol</source>, <volume>53</volume> Pt 1:<fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><surname>Bienenstock</surname>, <given-names>E. L.</given-names></string-name>, <string-name><surname>Cooper</surname>, <given-names>L. N.</given-names></string-name>, and <string-name><surname>Munro</surname>, <given-names>P. W.</given-names></string-name> (<year>1982</year>). <article-title>Theory for the development of neuron selectivity: Orientation specificity and binocular interaction in visual cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>2</volume>(<issue>1</issue>):<fpage>32</fpage>–<lpage>48</lpage>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><string-name><surname>Bittner</surname>, <given-names>K. C.</given-names></string-name>, <string-name><surname>Milstein</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Grienberger</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Romani</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Magee</surname>, <given-names>J. C.</given-names></string-name> (<year>2017</year>). <article-title>Behavioral time scale synaptic plasticity underlies CA1 place fields</article-title>. <source>Science</source>, <volume>357</volume>(<issue>6355</issue>):<fpage>1033</fpage>–<lpage>1036</lpage>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><string-name><surname>Blum</surname>, <given-names>K. I.</given-names></string-name> and <string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name> (<year>1996</year>). <article-title>A Model of Spatial Map Formation in the Hippocampus of the Rat</article-title>. <source>Neural Computation</source>, <volume>8</volume>(<issue>1</issue>):<fpage>85</fpage>–<lpage>93</lpage>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><surname>Buehlmann</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Wozniak</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Goulard</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Webb</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Graham</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Niven</surname>, <given-names>J. E.</given-names></string-name> (<year>2020</year>). <article-title>Mushroom Bodies Are Required for Learned Visual Navigation, but Not for Innate Visual Behavior, in Ants</article-title>. <source>Current Biology</source>, <volume>30</volume>(<issue>17</issue>):<fpage>3438</fpage>–<lpage>3443</lpage>.e2.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><surname>Burgess</surname>, <given-names>N.</given-names></string-name> and <string-name><surname>O’Keefe</surname>, <given-names>J.</given-names></string-name> (<year>1996</year>). <article-title>Neuronal computations underlying the firing of place cells and their role in navigation</article-title>. <source>Hippocampus</source>, <volume>6</volume>(<issue>6</issue>):<fpage>749</fpage>–<lpage>762</lpage>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><surname>Cohn</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Morantte</surname>, <given-names>I.</given-names></string-name>, and <string-name><surname>Ruta</surname>, <given-names>V.</given-names></string-name> (<year>2015</year>). <article-title>Coordinated and Compartmentalized Neuromodulation Shapes Sensory Processing in Drosophila</article-title>. <source>Cell</source>, <volume>163</volume>(<issue>7</issue>):<fpage>1742</fpage>–<lpage>1755</lpage>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><string-name><surname>Collett</surname>, <given-names>T. S.</given-names></string-name> and <string-name><surname>Collett</surname>, <given-names>M.</given-names></string-name> (<year>2002</year>). <article-title>Memory use in insect visual navigation</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>3</volume>(<issue>7</issue>):<fpage>542</fpage>–<lpage>552</lpage>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="book"><string-name><surname>Corneil</surname>, <given-names>D. S.</given-names></string-name> and <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name> (<year>2015</year>). <chapter-title>Attractor Network Dynamics Enable Preplay and Rapid Path Planning in Maze–like Environments</chapter-title>. <source>In Advances in Neural Information Processing Systems</source>, volume <volume>28</volume>. <publisher-name>Curran Associates, Inc</publisher-name>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name> (<year>1993</year>). <article-title>Improving generalization for temporal difference learning: The successor representation</article-title>. <source>Neural Computation</source>, <volume>5</volume>(<issue>4</issue>):<fpage>613</fpage>–<lpage>624</lpage>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="book"><string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name> and <string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name> (<year>2001</year>). <chapter-title>Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems</chapter-title>. <source>Computational Neuroscience</source>. <publisher-name>MIT Press, Cambridge, Mass</publisher-name>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="other"><string-name><surname>Dorkenwald</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Matsliah</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sterling</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Schlegel</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>S.-c.</given-names></string-name>, <string-name><surname>McKellar</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Costa</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Eichler</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Yin</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Silversmith</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Schneider-Mizell</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Jordan</surname>, <given-names>C. S.</given-names></string-name>, <string-name><surname>Brittain</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Halageri</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kuehner</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ogedengbe</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Morey</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Gager</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kruk</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Perlman</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Deutsch</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bland</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Sorek</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Macrina</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Bae</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Mu</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Nehoran</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Mitchell</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Popovych</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Jia</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Castro</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kemnitz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Ih</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bates</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Eckstein</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Funke</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Collman</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Bock</surname>, <given-names>D. D.</given-names></string-name>, <string-name><surname>Jefferis</surname>, <given-names>G. S. X. E.</given-names></string-name>, <string-name><surname>Seung</surname>, <given-names>H. S.</given-names></string-name>, <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Consortium</surname>, <given-names>t. F.</given-names></string-name> (<year>2023</year>). <article-title>Neuronal wiring diagram of an adult brain</article-title>. <source>bioRxiv</source>, page 2023.06.27.546656.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><string-name><surname>Eichler</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Litwin-Kumar</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Park</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Andrade</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Schneider-Mizell</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Saumweber</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Huser</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Eschbach</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gerber</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Fetter</surname>, <given-names>R. D.</given-names></string-name>, <string-name><surname>Truman</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Priebe</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name>, <string-name><surname>Thum</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Zlatic</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Cardona</surname>, <given-names>A.</given-names></string-name> (<year>2017</year>). <article-title>The complete connectome of a learning and memory centre in an insect brain</article-title>. <source>Nature</source>, <volume>548</volume>(<issue>7666</issue>):<fpage>175</fpage>–<lpage>182</lpage>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><string-name><surname>Epsztein</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Brecht</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Lee</surname>, <given-names>A. K.</given-names></string-name> (<year>2011</year>). <article-title>Intracellular determinants of hippocampal CA1 place and silent cell activity in a novel environment</article-title>. <source>Neuron</source>, <volume>70</volume>:<fpage>109</fpage>–<lpage>20</lpage>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><string-name><surname>Fang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Aronov</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Abbott</surname>, <given-names>LF.</given-names></string-name>, and <string-name><surname>Mackevicius</surname>, <given-names>E. L.</given-names></string-name> (<year>2023</year>). <article-title>Neural learning rules for generating flexible predictions and computing the successor representation</article-title>. <source>eLife</source>, <volume>12</volume>:<fpage>e80680</fpage>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><string-name><surname>Frank</surname>, <given-names>L. M.</given-names></string-name>, <string-name><surname>Stanley</surname>, <given-names>G. B.</given-names></string-name>, and <string-name><surname>Brown</surname>, <given-names>E. N.</given-names></string-name> (<year>2004</year>). <article-title>Hippocampal Plasticity across Multiple Days of Exposure to Novel Environments</article-title>. <source>Journal of Neuroscience</source>, <volume>24</volume>(<issue>35</issue>):<fpage>7681</fpage>–<lpage>7689</lpage>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><surname>Galtier</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Faugeras</surname>, <given-names>O.</given-names></string-name>, and <string-name><surname>Bressloff</surname>, <given-names>P.</given-names></string-name> (<year>2012</year>). <article-title>Hebbian Learning of Recurrent Connections: A Geometrical Perspective</article-title>. <source>Neural computation</source>, <volume>24</volume>:<fpage>2346</fpage>–<lpage>83</lpage>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><surname>Garvert</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Dolan</surname>, <given-names>R. J.</given-names></string-name>, and <string-name><surname>Behrens</surname>, <given-names>T. E.</given-names></string-name> (<year>2017</year>). <article-title>A map of abstract relational knowledge in the human hippocampal–entorhinal cortex</article-title>. <source>eLife</source>, <volume>6</volume>:<fpage>e17086</fpage>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><surname>Gaussier</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Revel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Banquet</surname>, <given-names>J. P.</given-names></string-name>, and <string-name><surname>Babeau</surname>, <given-names>V.</given-names></string-name> (<year>2002</year>). <article-title>From view cells and place cells to cognitive map learning: Processing stages of the hippocampal system</article-title>. <source>Biological Cybernetics</source>, <volume>86</volume>(<issue>1</issue>):<fpage>15</fpage>–<lpage>28</lpage>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name> and <string-name><surname>Kistler</surname>, <given-names>W. M.</given-names></string-name> (<year>2002</year>). <article-title>Mathematical formulations of Hebbian learning</article-title>. <source>Biological Cybernetics</source>, <volume>87</volume>(<issue>5</issue>):<fpage>404</fpage>–<lpage>415</lpage>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><string-name><surname>Glasius</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Komoda</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Gielen</surname>, <given-names>S. C. A. M.</given-names></string-name> (<year>1996</year>). <article-title>A biologically inspired neural net for trajectory formation and obstacle avoidance</article-title>. <source>Biological Cybernetics</source>, <volume>74</volume>(<issue>6</issue>):<fpage>511</fpage>–<lpage>520</lpage>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><string-name><surname>Gorchetchnikov</surname>, <given-names>A.</given-names></string-name> and <string-name><surname>Hasselmo</surname>, <given-names>M. E.</given-names></string-name> (<year>2005</year>). <article-title>A biophysical implementation of a bidi-rectional graph search algorithm to solve multiple goal navigation tasks</article-title>. <source>Connection Science</source>, <volume>17</volume>(<issue>1-2</issue>):<fpage>145</fpage>–<lpage>166</lpage>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><string-name><surname>Heisenberg</surname>, <given-names>M.</given-names></string-name> (<year>2003</year>). <article-title>Mushroom body memoir: From maps to models</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>4</volume>(<issue>4</issue>):<fpage>266</fpage>–<lpage>275</lpage>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><string-name><surname>Hok</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Save</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Lenck-Santini</surname>, <given-names>P. P.</given-names></string-name>, and <string-name><surname>Poucet</surname>, <given-names>B.</given-names></string-name> (<year>2005</year>). <article-title>Coding for spatial goals in the prelimbic/infralimbic area of the rat frontal cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>102</volume>(<issue>12</issue>):<fpage>4602</fpage>–<lpage>4607</lpage>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><string-name><surname>Hollup</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Molden</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Donnett</surname>, <given-names>J. G.</given-names></string-name>, <string-name><surname>Moser</surname>, <given-names>M.-B.</given-names></string-name>, and <string-name><surname>Moser</surname>, <given-names>E. I.</given-names></string-name> (<year>2001</year>). <article-title>Accumulation of Hippocampal Place Fields at the Goal Location in an Annular Watermaze Task</article-title>. <source>Journal of Neuroscience</source>, <volume>21</volume>(<issue>5</issue>):<fpage>1635</fpage>–<lpage>1644</lpage>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><string-name><surname>Honkanen</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Adden</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>da Silva Freitas</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Heinze</surname>, <given-names>S.</given-names></string-name> (<year>2019</year>). <article-title>The insect central complex and the neural basis of navigational strategies</article-title>. <source>Journal of Experimental Biology</source>, <volume>222</volume>(<issue>Suppl_1</issue>):<fpage>jeb188854</fpage>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><string-name><surname>Jacobs</surname>, <given-names>L. F.</given-names></string-name> (<year>2012</year>). <article-title>From chemotaxis to the cognitive map: The function of olfaction</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>109</volume>(<issue>Supplement 1</issue>):<fpage>10693</fpage>–<lpage>10700</lpage>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><string-name><surname>Kay</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Chung</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Sosa</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Schor</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Karlsson</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Larkin</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>D. F.</given-names></string-name>, and <string-name><surname>Frank</surname>, <given-names>L. M.</given-names></string-name> (<year>2020</year>). <article-title>Constant Sub-second Cycling between Representations of Possible Futures in the Hippocampus</article-title>. <source>Cell</source>, <volume>180</volume>(<issue>3</issue>):<fpage>552</fpage>–<lpage>567</lpage>.e25.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><surname>Khajeh-Alijani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Urbanczik</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Senn</surname>, <given-names>W.</given-names></string-name> (<year>2015</year>). <article-title>Scale-Free Navigational Planning by Neuronal Traveling Waves</article-title>. <source>PLOS ONE</source>, <volume>10</volume>(<issue>7</issue>):<fpage>e0127269</fpage>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><string-name><surname>Kjelstrup</surname>, <given-names>K. B.</given-names></string-name>, <string-name><surname>Solstad</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Brun</surname>, <given-names>V. H.</given-names></string-name>, <string-name><surname>Hafting</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Leutgeb</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Witter</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Moser</surname>, <given-names>E. I.</given-names></string-name>, and <string-name><surname>Moser</surname>, <given-names>M.-B.</given-names></string-name> (<year>2008</year>). <article-title>Finite scale of spatial representation in the hippocampus</article-title>. <source>Science</source>, <volume>321</volume>(<issue>5885</issue>):<fpage>140</fpage>–<lpage>143</lpage>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><string-name><surname>Knaden</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Graham</surname>, <given-names>P.</given-names></string-name> (<year>2016</year>). <article-title>The Sensory Ecology of Ant Navigation: From Natural Environments to Neural Mechanisms</article-title>. In <person-group person-group-type="editor"><string-name><surname>Berenbaum</surname>, <given-names>M. R.</given-names></string-name>, editor</person-group>, <source>Annual Review of Entomology</source>, volume <volume>61</volume>, pages <fpage>63</fpage>–<lpage>76</lpage>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><string-name><surname>Lashley</surname>, <given-names>K. S.</given-names></string-name> (<year>1912</year>). <article-title>Visual discrimination of size and form in the albino rat</article-title>. <source>Journal of Animal Behavior</source>, <volume>2</volume>(<issue>5</issue>):<fpage>310</fpage>–<lpage>331</lpage>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><string-name><surname>Li</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Lindsey</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Marin</surname>, <given-names>E. C.</given-names></string-name>, <string-name><surname>Otto</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Dreher</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dempsey</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Stark</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Bates</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Pleijzier</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Schlegel</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Nern</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Takemura</surname>, <given-names>S.-y.</given-names></string-name>, <string-name><surname>Eckstein</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Francis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Braun</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Parekh</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Costa</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Scheffer</surname>, <given-names>L. K.</given-names></string-name>, <string-name><surname>Aso</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Jefferis</surname>, <given-names>G. S.</given-names></string-name>, <string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name>, <string-name><surname>Litwin-Kumar</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Waddell</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Rubin</surname>, <given-names>G. M.</given-names></string-name> (<year>2020</year>). <article-title>The connectome of the adult Drosophila mushroom body provides insights into function</article-title>. <source>eLife</source>, <volume>9</volume>:<fpage>e62576</fpage>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><string-name><surname>Magee</surname>, <given-names>J. C.</given-names></string-name> and <string-name><surname>Grienberger</surname>, <given-names>C.</given-names></string-name> (<year>2020</year>). <article-title>Synaptic Plasticity Forms and Functions</article-title>. <source>Annual Review of Neuroscience</source>, <volume>43</volume>(<issue>1</issue>):<fpage>95</fpage>–<lpage>117</lpage>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><string-name><surname>Martinet</surname>, <given-names>L.-E.</given-names></string-name>, <string-name><surname>Sheynikhovich</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Benchenane</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Arleo</surname>, <given-names>A.</given-names></string-name> (<year>2011</year>). <article-title>Spatial Learning and Action Planning in a Prefrontal Cortical Network Model</article-title>. <source>PLOS Computational Biology</source>, <volume>7</volume>(<issue>5</issue>):<fpage>e1002045</fpage>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="other"><string-name><surname>Meister</surname>, <given-names>M.</given-names></string-name> (<year>2023</year>). <article-title>A fast algorithm for All-Pairs-Shortest-Paths suitable for neural networks</article-title>. <source>arXiv</source>, 2308.07403 [cs, q-bio].</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="other"><string-name><surname>Moerland</surname>, <given-names>T. M.</given-names></string-name>, <string-name><surname>Broekens</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Plaat</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Jonker</surname>, <given-names>C. M.</given-names></string-name> (<year>2022</year>). <article-title>Model-based Reinforcement Learning: A Survey</article-title>. <source>arXiv</source>, 2006.16712 [cs, stat].</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><string-name><surname>Morris</surname>, <given-names>R. G. M.</given-names></string-name>, <string-name><surname>Garrud</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Rawlins</surname>, <given-names>J. N. P.</given-names></string-name>, and <string-name><surname>O’Keefe</surname>, <given-names>J.</given-names></string-name> (<year>1982</year>). <article-title>Place navigation impaired in rats with hippocampal lesions</article-title>. <source>Nature</source>, <volume>297</volume>(<issue>5868</issue>):<fpage>681</fpage>–<lpage>683</lpage>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><string-name><surname>Moser</surname>, <given-names>M.-B.</given-names></string-name>, <string-name><surname>Rowland</surname>, <given-names>D. C.</given-names></string-name>, and <string-name><surname>Moser</surname>, <given-names>E. I.</given-names></string-name> (<year>2015</year>). <article-title>Place Cells, Grid Cells, and Memory</article-title>. <source>Cold Spring Harbor Perspectives in Biology</source>, <volume>7</volume>(<issue>2</issue>):<fpage>a021808</fpage>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><string-name><surname>Müller</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Wehner</surname>, <given-names>R.</given-names></string-name> (<year>1988</year>). <article-title>Path integration in desert ants, Cataglyphis fortis</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>85</volume>(<issue>14</issue>):<fpage>5287</fpage>–<lpage>5290</lpage>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><string-name><surname>Muller</surname>, <given-names>R. U.</given-names></string-name>, <string-name><surname>Kubie</surname>, <given-names>J. L.</given-names></string-name>, and <string-name><surname>Saypoff</surname>, <given-names>R.</given-names></string-name> (<year>1991</year>). <article-title>The hippocampus as a cognitive graph (abridged version)</article-title>. <source>Hippocampus</source>, <volume>1</volume>(<issue>3</issue>):<fpage>243</fpage>–<lpage>246</lpage>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><string-name><surname>Nyberg</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Duvelle</surname>, <given-names>É.</given-names></string-name>, <string-name><surname>Barry</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Spiers</surname>, <given-names>H. J.</given-names></string-name> (<year>2022</year>). <article-title>Spatial goal coding in the hippocampal formation</article-title>. <source>Neuron</source>, <volume>110</volume>(<issue>3</issue>):<fpage>394</fpage>–<lpage>422</lpage>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><string-name><surname>Ponulak</surname>, <given-names>F.</given-names></string-name> and <string-name><surname>Hopfield</surname>, <given-names>J. J.</given-names></string-name> (<year>2013</year>). <article-title>Rapid, parallel path planning by propagating wavefronts of spiking neural activity</article-title>. <source>Frontiers in Computational Neuroscience</source>, <volume>7</volume>:<fpage>98</fpage>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><string-name><surname>Redish</surname>, <given-names>A. D.</given-names></string-name> (<year>2016</year>). <article-title>Vicarious trial and error</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>17</volume>(<issue>3</issue>):<fpage>147</fpage>–<lpage>159</lpage>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><string-name><surname>Rosenberg</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Perona</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Meister</surname>, <given-names>M.</given-names></string-name> (<year>2021</year>). <article-title>Mice in a labyrinth exhibit rapid learning, sudden insight, and efficient exploration</article-title>. <source>eLife</source>, <volume>10</volume>:<fpage>e66175</fpage>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><string-name><surname>Samsonovich</surname>, <given-names>A. V.</given-names></string-name> and <string-name><surname>Ascoli</surname>, <given-names>G. A.</given-names></string-name> (<year>2005</year>). <article-title>A simple neural network model of the hippocampus suggesting its pathfinding role in episodic memory retrieval</article-title>. <source>Learning &amp; Memory</source>, <volume>12</volume>(<issue>2</issue>):<fpage>193</fpage>–<lpage>208</lpage>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><string-name><surname>Santos-Pata</surname>, <given-names>D.</given-names></string-name> and <string-name><surname>Verschure</surname>, <given-names>P. F. M. J.</given-names></string-name> (<year>2018</year>). <article-title>Human Vicarious Trial and Error Is Predictive of Spatial Navigation Performance</article-title>. <source>Frontiers in Behavioral Neuroscience</source>, <volume>12</volume>:<fpage>237</fpage>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><string-name><surname>Schmajuk</surname>, <given-names>N. A.</given-names></string-name> and <string-name><surname>Thieme</surname>, <given-names>A. D.</given-names></string-name> (<year>1992</year>). <article-title>Purposive behavior and cognitive mapping: A neural network model</article-title>. <source>Biological Cybernetics</source>, <volume>67</volume>(<issue>2</issue>):<fpage>165</fpage>–<lpage>174</lpage>.</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><string-name><surname>Schölkopf</surname>, <given-names>B.</given-names></string-name> and <string-name><surname>Mallot</surname>, <given-names>H. A.</given-names></string-name> (<year>1995</year>). <article-title>View-Based Cognitive Mapping and Path Planning</article-title>. <source>Adaptive Behavior</source>, <volume>3</volume>(<issue>3</issue>):<fpage>311</fpage>–<lpage>348</lpage>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="other"><string-name><surname>Sosa</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Giocomo</surname>, <given-names>L. M.</given-names></string-name> (<year>2021</year>). <article-title>Navigating for reward</article-title>. <source>Nature Reviews Neuroscience</source>, pages <fpage>1</fpage>–<lpage>16</lpage>.</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><string-name><surname>Stachenfeld</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Botvinick</surname>, <given-names>M. M.</given-names></string-name>, and <string-name><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name> (<year>2017</year>). <article-title>The hippocampus as a predictive map</article-title>. <source>Nature Neuroscience</source>, <volume>20</volume>(<issue>11</issue>):<fpage>1643</fpage>–<lpage>1653</lpage>.</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="journal"><string-name><surname>Steck</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Hansson</surname>, <given-names>B. S.</given-names></string-name>, and <string-name><surname>Knaden</surname>, <given-names>M.</given-names></string-name> (<year>2009</year>). <article-title>Smells like home: Desert ants, Cataglyphis fortis, use olfactory landmarks to pinpoint the nest</article-title>. <source>Frontiers in Zoology</source>, <volume>6</volume>(<issue>1</issue>):<fpage>5</fpage>.</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><string-name><surname>Stopfer</surname>, <given-names>M.</given-names></string-name> (<year>2014</year>). <article-title>Central processing in the mushroom bodies</article-title>. <source>Current Opinion in Insect Science</source>, <volume>6</volume>:<fpage>99</fpage>–<lpage>103</lpage>.</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="journal"><string-name><surname>Strausfeld</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Sinakevitch</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Brown</surname>, <given-names>S. M.</given-names></string-name>, and <string-name><surname>Farris</surname>, <given-names>S. M.</given-names></string-name> (<year>2009</year>). <article-title>Ground plan of the insect mushroom body: Functional and evolutionary implications</article-title>. <source>The Journal of comparative neurology</source>, <volume>513</volume>(<issue>3</issue>):<fpage>265</fpage>–<lpage>291</lpage>.</mixed-citation></ref>
<ref id="c59"><label>[59]</label><mixed-citation publication-type="journal"><string-name><surname>Sun</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Yue</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Mangan</surname>, <given-names>M.</given-names></string-name> (<year>2020</year>). <article-title>A decentralised neural model explaining optimal integration of navigational strategies in insects</article-title>. <source>eLife</source>, <volume>9</volume>:<fpage>e54026</fpage>.</mixed-citation></ref>
<ref id="c60"><label>[60]</label><mixed-citation publication-type="book"><string-name><surname>Sutton</surname>, <given-names>R. S.</given-names></string-name> and <string-name><surname>Barto</surname>, <given-names>A. G.</given-names></string-name> (<year>2018</year>). <source>Reinforcement Learning: An Introduction</source>. <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="c61"><label>[61]</label><mixed-citation publication-type="journal"><string-name><surname>Takemura</surname>, <given-names>S.-y.</given-names></string-name>, <string-name><surname>Aso</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hige</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Wong</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>C. S.</given-names></string-name>, <string-name><surname>Rivlin</surname>, <given-names>P. K.</given-names></string-name>, <string-name><surname>Hess</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Parag</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Berg</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Katz</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Olbris</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Plaza</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Umayam</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Aniceto</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>L.-A.</given-names></string-name>, <string-name><surname>Lauchie</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ogundeyi</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Ordish</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Shinomiya</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sigmund</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Takemura</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tran</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>G. C.</given-names></string-name>, <string-name><surname>Rubin</surname>, <given-names>G. M.</given-names></string-name>, and <string-name><surname>Scheffer</surname>, <given-names>L. K.</given-names></string-name> (<year>2017</year>). <article-title>A connectome of a learning and memory center in the adult Drosophila brain</article-title>. <source>eLife</source>, <volume>6</volume>:<fpage>e26975</fpage>.</mixed-citation></ref>
<ref id="c62"><label>[62]</label><mixed-citation publication-type="journal"><string-name><surname>Tarsitano</surname>, <given-names>M.</given-names></string-name> (<year>2006</year>). <article-title>Route selection by a jumping spider (Portia labiata) during the locomotory phase of a detour</article-title>. <source>Animal Behaviour</source>, <volume>72</volume>(<issue>6</issue>):<fpage>1437</fpage>–<lpage>1442</lpage>.</mixed-citation></ref>
<ref id="c63"><label>[63]</label><mixed-citation publication-type="journal"><string-name><surname>Thistlethwaite</surname>, <given-names>D.</given-names></string-name> (<year>1951</year>). <article-title>A critical review of latent learning and related experiments</article-title>. <source>Psychological Bulletin</source>, <volume>48</volume>(<issue>2</issue>):<fpage>97</fpage>–<lpage>129</lpage>.</mixed-citation></ref>
<ref id="c64"><label>[64]</label><mixed-citation publication-type="journal"><string-name><surname>Tolman</surname>, <given-names>E. C.</given-names></string-name> (<year>1948</year>). <article-title>Cognitive maps in rats and men</article-title>. <source>Psychological Review</source>, <volume>55</volume>(<issue>4</issue>):<fpage>189</fpage>–<lpage>208</lpage>.</mixed-citation></ref>
<ref id="c65"><label>[65]</label><mixed-citation publication-type="journal"><string-name><surname>Trullier</surname>, <given-names>O.</given-names></string-name> and <string-name><surname>Meyer</surname>, <given-names>J.-A.</given-names></string-name> (<year>2000</year>). <article-title>Animat navigation using a cognitive graph</article-title>. <source>Biological Cybernetics</source>, <volume>83</volume>(<issue>3</issue>):<fpage>271</fpage>–<lpage>285</lpage>.</mixed-citation></ref>
<ref id="c66"><label>[66]</label><mixed-citation publication-type="journal"><string-name><surname>Voicu</surname>, <given-names>H.</given-names></string-name> and <string-name><surname>Schmajuk</surname>, <given-names>N.</given-names></string-name> (<year>2000</year>). <article-title>Exploration, Navigation and Cognitive Mapping</article-title>. <source>Adaptive Behavior</source>, <volume>8</volume>(<issue>3-4</issue>):<fpage>207</fpage>–<lpage>223</lpage>.</mixed-citation></ref>
<ref id="c67"><label>[67]</label><mixed-citation publication-type="journal"><string-name><surname>Webb</surname>, <given-names>B.</given-names></string-name> and <string-name><surname>Wystrach</surname>, <given-names>A.</given-names></string-name> (<year>2016</year>). <article-title>Neural mechanisms of insect navigation</article-title>. <source>Current Opinion in Insect Science</source>, <volume>15</volume>:<fpage>27</fpage>–<lpage>39</lpage>.</mixed-citation></ref>
<ref id="c68"><label>[68]</label><mixed-citation publication-type="journal"><string-name><surname>Wilson</surname>, <given-names>M. A.</given-names></string-name> and <string-name><surname>McNaughton</surname>, <given-names>B. L.</given-names></string-name> (<year>1993</year>). <article-title>Dynamics of the hippocampal ensemble code for space</article-title>. <source>Science</source>, <volume>261</volume>(<issue>5124</issue>):<fpage>1055</fpage>–<lpage>1058</lpage>.</mixed-citation></ref>
</ref-list>
<sec id="s5">
<title>Data and code availability</title>
<p>Data and code to reproduce the reported results are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/markusmeister/Endotaxis-2023">https://github.com/markusmeister/Endotaxis-2023</ext-link>. Following acceptance of the manuscript they will be archived in a permanent public repository.</p>
<p>Acknowledgments</p>
<p>Funding: This work was supported by the Simons Collaboration on the Global Brain (grant 543015 to MM and 543025 to PP), by NSF award 1564330 to PP, and by a gift from Google to PP.</p>
</sec>
<sec id="s6">
<title>Competing interests</title>
<p>The authors declare no competing interests.</p>
</sec>
<sec id="s7">
<title>Colleagues</title>
<p>We thank Kyu Hyun Lee and Ruben Portugues for comments.</p>
<fn-group>
<fn id="fn1">
<label><sup>1</sup></label>
<p>We avoid the term ‘place cell’ here because (1) that term has a technical meaning in the rodent hippocampus, whereas the arguments here extend to species that don’t have a hippocampus; (2) all the cells in this network have a place field, but it is smallest for the point cells.</p></fn>
<fn id="fn2">
<label><sup>2</sup></label>
<p>The mode switch effectively determines the animal’s behavioral policy. In this report we do not consider how or why the animal chooses one mode or another.</p></fn>
<fn id="fn3">
<label><sup>3</sup></label>
<p>In the circuit of <xref rid="fig1" ref-type="fig">Figure 1B</xref> one can envision that the readout noise gets added after the mode switch.</p></fn>
<fn id="fn4">
<label><sup>4</sup></label>
<p>These differences in how the problem is formulated can lead to slightly different mathematical expressions, e.g. compare the role of <italic>γ</italic> in <xref ref-type="disp-formula" rid="eqn6">Eqn 6</xref> with <xref ref-type="disp-formula" rid="eqn2">Eqn 2</xref> of [<xref ref-type="bibr" rid="c19">19</xref>].</p></fn>
<fn id="fn5">
<label><sup>5</sup></label>
<p>…and – if it exists – the site of free will.</p></fn>
</fn-group>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.84141.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>École Normale Supérieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> work proposes a framework inspired by chemotaxis for understanding how the brain might implement behaviours related to navigating toward a goal. The evidence supporting the conceptual claim is <bold>convincing</bold>. The manuscript proposes a hypothesis that would be of interest to the broad systems neuroscience community, although it was noted the relationship to existing similar hypotheses could be clarified.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.84141.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This paper presents a highly compelling and novel hypothesis for how the brain could generate signals to guide navigation toward remembered goals. Under this hypothesis, which the authors call &quot;Endotaxis&quot;, the brain co-opts its ancient ability to navigate up odor gradients (chemotaxis) by generating a &quot;virtual odor&quot; that grows stronger the closer the animal is to a goal location. This idea is compelling from an evolutionary perspective and a mechanistic perspective. The paper is well-written and delightful to read.</p>
<p>The authors develop a detailed model of how the brain may perform &quot;Endotaxis&quot;, using a variety of interconnected cell types (point, map, and goal cells) to inform the chemotaxis system. They tested the ability of this model to navigate in several state spaces, representing both physical mazes and abstract cognitive tasks. The Endotaxis model performed reasonably well across different environments and different types of goals.</p>
<p>The authors further tested the model using parameter sweeps and discovered a critical level of network gain, beyond which task performance drops. This critical level approximately matched analytical derivations.</p>
<p>Overall, this paper provides a very compelling model for how neural circuits may have evolved the ability to navigate towards remembered goals, using ancient chemotaxis circuits.</p>
<p>This framework will likely be very important for understanding how the hippocampus (and other memory/navigation-related circuits) interfaces with other processes in the brain, giving rise to memory-guided behavior.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.84141.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The manuscript presents a computational model of how an organism might learn a map of the structure of its environment and the location of valuable resources through synaptic plasticity, and how this map could subsequently be used for goal-directed navigation.</p>
<p>The model is composed of 'map cells', which learn the structure of the environment in their recurrent connections, and 'goal-cell' which store the location of valued resources with respect to the map cell population. Each map cell corresponds to a particular location in the environment due to receiving external excitatory input at this location. The synaptic plasticity rule between map cells potentiates synapses when activity above a specified threshold at the pre-synaptic neuron is followed by above-threshold activity at the post-synaptic neuron. The threshold is set such that map neurons are only driven above this plasticity threshold by the external excitatory input, causing synapses to only be potentiated between a pair of map neurons when the organism moves directly between the locations they represent. This causes the weight matrix between the map neurons to learn the adjacency for the graph of locations in the environment, i.e. after learning the synaptic weight matrix matches the environment's adjacency matrix. Recurrent activity in the map neuron population then causes a bump of activity centred on the current location, which drops off exponentially with the diffusion distance on the graph. Each goal cell receives input from the map cells, and also from a 'resource cell' whose activity indicates the presence or absence of a given values resource at the current location. Synaptic plasticity potentiates map-cell to goal-cell synapses in proportion to the activity of the map cells at time-points when the resource cell is active. This causes goal cell activity to increase when the activity of the map cell population is similar to the activity where the resource was obtained. The upshot of all this is that after learning the activity of goal cells decreases exponentially with the diffusion distance from the corresponding goal location. The organism can therefore navigate to a given goal by doing gradient ascent on the activity of the corresponding goal cell. The process of evaluating these gradients and using them to select actions is not modelled explicitly, but the authors point to the similarity of this mechanism to chemotaxis (ascending a gradient of odour concentration to reach the odour source), and the widespread capacity for chemotaxis in the animal kingdom, to argue for its biological plausibility. The ideas are interesting and the presentation of the results in the manuscript is generally clear.</p>
<p>Closely related ideas have been explored in previous work, and there are some aspects of how the work relates to previous literature that it would be useful to clarify. Several lines of work have proposed learning long-range relationships between states in the environment, to enable navigation to rewarding goals by effectively descending distance gradients. The most well-known of these in the neuroscience literature is the Successor Representation (SR) (Dayan 1993), which is defined as the expected discounted future occupancy of each state given the current state. As noted in the discussion, this is closely related to the representation learnt by the map cells in the current model. The key difference is that the successor representation uses state-state transitions under a given policy (a mapping from states to actions), whereas the current model uses the adjacency matrix between states, which depends only on the environment and hence is independent of the policy followed while the representation is learnt (given sufficient exploration). This policy independence is useful, as the SR can fail to generate good routes to goals when these are very different from the policy under which it was learned (see Russek et al. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005768">https://doi.org/10.1371/journal.pcbi.1005768</ext-link>). However, there are several prior proposals for policy-independent SR-like mechanisms that it would be useful to discuss. Baram et al. (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/421461">https://doi.org/10.1101/421461</ext-link>) propose navigating to goals by doing gradient descent on diffusion distances, computed as powers of the adjacency matrix as in the current work. One limitation of using the adjacency matrix is that it does not handle situations where transitions between states are probabilistic, which is not a big issue for navigation in physical space but is for applying the mechanism to cognitive tasks more broadly. There are prior ideas for learning policy-independent representations similar to the SR that do not have this limitation. Kaelbling (Learning to achieve goals, IJCAI, 1993) proposed using an off-policy learning rule similar to Q-learning, to learn shortest path distances between states. Piray and Daw <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-021-25123-3">https://doi.org/10.1038/s41467-021-25123-3</ext-link>) consider a default representation, which is a successor-like representation under a generic default policy, building on the Linear Markov Decision Process (LMDP) framework of Todorov (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0710743106">https://doi.org/10.1073/pnas.0710743106</ext-link>). Also relevant to the current study is the work of Fang et al. (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.80680">https://doi.org/10.7554/eLife.80680</ext-link>) who, as in the current work, propose using recurrent network dynamics to compute a long-range representation (the SR) from synaptic weights that store local transition information.</p>
<p>One other area where I felt the work could be better integrated with the existing literature was the discussion of mapping the model onto brain circuits. An interesting and attractive aspect of the work is the idea that the relatively high-level operation of goal-directed navigation could be built on top of evolutionarily older mechanisms for ascending odour gradients. Given this framing, I was expecting the discussion of brain circuits to consider interactions between spatial mapping systems and regions involved in olfactory processing. However the discussion of mammalian brains focussed exclusively on the hippocampus without any link to olfaction, which feels like a missed opportunity. I am not an expert on olfaction, but one region that seems particularly interesting in this context is the olfactory tubercle (see Wesson &amp; Wilson <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neubiorev.2010.08.004">https://doi.org/10.1016/j.neubiorev.2010.08.004</ext-link> for a review). This region is contiguous with the ventral striatum and has similar local circuitry, receives strong input from olfactory regions, but also input from the hippocampal formation, and a strong dopaminergic innervation from VTA. This suggests a mapping of the model to brain circuits in which map cells in the hippocampal formation project to goal cells in the olfactory tubercle, with the dopaminergic input acting as resource cells (note that different dopamine neuron populations appear to respond to different reward types, see e.g. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41586-022-04954-0">https://doi.org/10.1038/s41586-022-04954-0</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2023.05.09.540067">https://doi.org/10.1101/2023.05.09.540067</ext-link>). I was also surprised not to see any discussion of internally generated sequential activity in the hippocampus as a possible mechanism for the look-ahead needed to evaluate the goal distance gradient, particularly given the authors suggest that vicarious trial and error (VTE) is a behavioural signature of this gradient sampling, and it is known that during VTE hippocampus plays out internally generated sequences of possible future locations (see Redish <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn.2015.30">https://doi.org/10.1038/nrn.2015.30</ext-link>).</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.84141.2.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This paper describes an algorithm that provides a general mechanism for goal-directed behaviour in a biologically plausible neural form.</p>
<p>The method depends on substantial simplifying assumptions. The simulated animal effectively moves through an environment consisting of discrete locations and can reliably detect when it is in each location. Whenever it moves from one location to an adjacent location, it perfectly learns the connectivity between these two locations (changes the value in an adjacency matrix to 1). This creates a graph of connections that reflects the explored environment. In this graph, the current location gets input activation and this spreads to all connected nodes multiplied by a constant decay (adjusted to the branching number of the graph) so that as the number of connection steps increases the activation decreases. Some locations will be marked as goals through experiencing a resource of a specific identity there and subsequently will be activated by an amount proportional to their distance in the graph from the current location, i.e., their activation will increase if the agent moves a step closer and decrease if it moves a step further away. Hence by making such exploratory movements, the animal can decide which way to move to obtain a specified goal.</p>
<p>Although the algorithm is presented within a conceptual framework of chemotaxis, I.e., making movements to sample a local gradient and move up it, the approach relates closely to previous models of exploration, learning, and navigation that similarly establish (through experience) a graph structure to represent how locations are connected and use some form of activity-propagation from the current node or goal node to identify a (shortest) route between them. Many of these similarly claim to be plausible neural circuits. The current authors argue that the current algorithm has several desirable features with respect to such previous work: for example, the 'readout' of the path does not require explicit 'look-up' and activation of the goal node (although it does require a choice of which goal node is currently connected to behavior); and does not require any separate control or rules for learning vs. navigation phases. By comparison to the successor representation method used in RL, which also appears related, they note that the gain (decay) factor is not equivalent to a temporal discount and that their method learns only state-state transitions, allowing the value of actions to be externalised, I.e., calculated by trying alternative actions to see which increases the activation at the goal node the most. On the other hand, it should be noted that some issues addressed in previous models, such as uncertainty over the current state or probabilistic state(-action) transitions are not addressed in this work.</p>
<p>The algorithm presents some elegant features with respect to previous work such as conceptually separating the 'goal' nodes from the state (location) graph (I.e. 'goals' are not just special target states within the graph) so that a small number of goals can become associated to (potentially) multiple regions of the state graph where they are satisfied, or near to being satisfied. This architecture is suggested, in the discussion, to resemble the insect mushroom body (MB), where it is known that a small number of output neurons (MBONs, putative goal neurons) are activated by plastic connections from Kenyon cells (KCs, putative state neurons). However, it goes substantially beyond any available evidence to claim that KC connectivity could support the acquisition of a graph (in the form of an adjacency matrix) representing the structure of the environment: KCs show sparse distributed activity (not one active node per state); it seems unlikely that any two arbitrary KCs can (rapidly) become connected; and as yet has not been demonstrated that KC connectivity is plastic at all.</p>
<p>The results presented are fairly straightforward given the simplification of the tasks, as described above. They show 1) in practical terms, the spreading signal travels further for a larger decay but becomes erratic as the decay parameter (map neuron gain) approaches its theoretical upper bound and decreases below noise levels beyond a certain distance. Both follow the theory but it is perhaps helpful to see that there is a viable range of values of the gain for which the mechanism works, that is, it is not highly dependent on precise tuning. 2) That different graph structures can be acquired and used to approach goal locations (not surprising). 3) That simultaneous learning and exploitation of the graph only minimally affects the performance over starting with perfect knowledge of the graph. 4) That the parameters interact in expected ways. 5) That the separation of goals from states can be used flexibly e.g. the homing behaviour (a goal state is learned before any of the map is learned) and the patrolling behaviour (a goal cell that monitors all states for how recently they were visited). It is also interesting to link the mechanism of exploration of neighbouring states to observed scanning behaviours in navigating animals. It would have been interesting to explore whether the parameters could be dynamically tuned, based on the overall graph activity.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.84141.2.sa4</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Tony</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rosenberg</surname>
<given-names>Matthew</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jing</surname>
<given-names>Zeyu</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Perona</surname>
<given-names>Pietro</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Meister</surname>
<given-names>Markus</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<p>Thank you for the detailed and constructive reviews. We revised the paper accordingly, and a point-by-point reply appears below. The main changes are:</p>
<list list-type="bullet">
<list-item><p>An extended discussion section that places our work in context with other related developments in theory and modeling.</p>
</list-item>
<list-item><p>A new results section that demonstrates a substantial improvement in performance from a non-linear activation function. This led to addition of a co-author.</p>
</list-item>
<list-item><p>The mathematical proof that the resolvent of the adjacency matrix leads to the shortest path distances has been moved to a separate article, available as a preprint and attached to this resubmission. This allows us to present that work in the context of graph theory, and focus the present paper on neural modeling.</p>
</list-item>
</list>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>This paper presents a highly compelling and novel hypothesis for how the brain could generate signals to guide navigation towards remembered goals. Under this hypothesis, which the authors call &quot;Endotaxis&quot;, the brain co-opts its ancient ability to navigate up odor gradients (chemotaxis) by generating a &quot;virtual odor&quot; that grows stronger the closer the animal is to a goal location. This idea is compelling from an evolutionary perspective and a mechanistic perspective. The paper is well-written and delightful to read.</p>
<p>The authors develop a detailed model of how the brain may perform &quot;Endotaxis&quot;, using a variety of interconnected cell types (point, map, and goal cells) to inform the chemotaxis system. They tested the ability of this model to navigate in several state spaces, representing both physical mazes and abstract cognitive tasks. The Endotaxis model performed reasonably well across different environments and different types of goals.</p>
<p>The authors further tested the model using parameter sweeps and discovered a critical level of network gain, beyond which task performance drops. This critical level approximately matched analytical derivations.</p>
<p>My main concern with this paper is that the analysis of the critical gain value (gamma_c) is incomplete, making the implications of these analyses unclear. There are several different reasonable ways in which the Endotaxis map cell representations might be normalized, which I suspect may lead to different results. Specifically, the recurrent connections between map cells may either be an adjacency matrix, or a normalized transition matrix. In the current submission, the recurrent connections are an unnormalized adjacency matrix. In a previous preprint version of the Endotaxis manuscript, the recurrent connections between the map cells were learned using Oja's rule, which results in a normalized state-transition matrix (see &quot;Appendix 5: Endotaxis model and the successor representation&quot; in &quot;Neural learning rules for generating flexible predictions and computing the successor representation&quot;, your reference 17). The authors state &quot;In summary, this sensitivity analysis shows that the optimal parameter set for endotaxis does depend on the environment&quot;. Is this statement, and the other conclusions of the sensitivity analysis, still true if the learned recurrent connections are a properly normalized state-transition matrix?</p>
</disp-quote>
<p>Yes, this is an interesting topic. In v.1 of our bioRxiv preprint we used Oja’s rule for learning, which will converge on a map connectivity that reflects the transition probabilities. The matrix M becomes a left-normalized or right-normalized stochastic matrix, depending on whether one uses the pre-synaptic or the post-synaptic version of Oja’s rule. This is explained well in Appendix 5 of Fang 2023.</p>
<p>In the present version of the model we use a rule that learns the adjacency matrix A, not the transition matrix T. The motivation is that we want to explain instances of oneshot learning, where an agent acquires a route after traversing it just once. For example, we had found experimentally that mice can execute a complex homing route on the first attempt.</p>
<p>An agent can establish whether two nodes are connected (adjacency) the very first time it travels from one node to the other. Whereas it can evaluate the transition probability for that link only after trying this and all the other available links on multiple occasions. Hence the normalization terms in Oja’s rule, or in the rule used by Fang 2023, all involve some time-averaging over multiple visits to the same node. This implements a gradual learning process over many experiences, rather than a one-shot acquisition on the first experience.</p>
<p>Still one may ask whether there are advantages to learning the transition matrix rather than the adjacency matrix. We looked into this with the following results:</p>
<p>•   The result that (1/γ − A)−1 is monotonically related to the graph distances D in the limit of small γ (a proof now moved to the Meister 2023 preprint) , holds also for the
transition matrix T. The proof follows the same steps. So in the small gain limit, the navigation model would work with T as well.</p>
<p>•   If one uses the transition matrix to compute the network output (1/γ − T)-1  then the critical gain value is γc = 1. It is well known that the largest eigenvalue of any Markov transition matrix is 1, and the critical gain γc is the inverse of that. This result is independent of the graph. So this offers the promise that the network could use the same gain parameter γ regardless of the environment.</p>
<p>•   In practice, however, the goal signal turned out to be less robust when based on T than when based on A. We illustrate this with the attached Author response image 1. This replicates the analysis in Figure 3 of the manuscript, using the transition matrix instead of the adjacency matrix. Some observations:</p>
<p>•   Panel B: The goal signal follows an exponential dependence on graph distance much more robustly for the model with A than with T. This holds even for small gain values where the exponential decay is steep.</p>
<p>•   Panel C: As one raises the gain closer to the critical value, the goal signal based on T scatters much more than when based on A.</p>
<p>•   Panels D, E: Navigation based on A works better than based on T. For example, using the highest practical gain value, and a readout noise of ϵ = 0.01, navigation based on T has a range of only 8 steps on this graph, whereas navigation based on A ranges over 12 steps, the full size of this graph.</p>
<p>We have added a section “Choice of learning rule” to explain this. The Author response image 1 is part of the code notebook on Github.</p>
<fig id="sa4fig1">
<label>Author response image 1.</label>
<graphic mime-subtype="jpg" xlink:href="elife-84141-sa4-fig1.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>Overall, this paper provides a very compelling model for how neural circuits may have evolved the ability to navigate towards remembered goals, using ancient chemotaxis circuits.</p>
<p>This framework will likely be very important for understanding how the hippocampus (and other memory/navigation-related circuits) interfaces with other processes in the brain, giving rise to memory-guided behavior.</p>
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>The manuscript presents a computational model of how an organism might learn a map of the structure of its environment and the location of valuable resources through synaptic plasticity, and how this map could subsequently be used for goal-directed navigation.</p>
<p>The model is composed of 'map cells', which learn the structure of the environment in their recurrent connections, and 'goal-cell' which stores the location of valued resources with respect to the map cell population. Each map cell corresponds to a particular location in the environment due to receiving external excitatory input at this location. The synaptic plasticity rule between map cells potentiates synapses when activity above a specified threshold at the pre-synaptic neuron is followed by above-threshold activity at the post-synaptic neuron. The threshold is set such that map neurons are only driven above this plasticity threshold by the external excitatory input, causing synapses to only be potentiated between a pair of map neurons when the organism moves directly between the locations they represent. This causes the weight matrix between the map neurons to learn the adjacency for the graph of locations in the environment, i.e. after learning the synaptic weight matrix matches the environment's adjacency matrix. Recurrent activity in the map neuron population then causes a bump of activity centred on the current location, which drops off exponentially with the diffusion distance on the graph. Each goal cell receives input from the map cells, and also from a 'resource cell' whose activity indicates the presence or absence of a given values resource at the current location. Synaptic plasticity potentiates map-cell to goal-cell synapses in proportion to the activity of the map cells at time points when the resource cell is active. This causes goal cell activity to increase when the activity of the map cell population is similar to the activity where the resource was obtained. The upshot of all this is that after learning the activity of goal cells decreases exponentially with the diffusion distance from the corresponding goal location. The organism can therefore navigate to a given goal by doing gradient ascent on the activity of the corresponding goal cell. The process of evaluating these gradients and using them to select actions is not modelled explicitly, but the authors point to the similarity of this mechanism to chemotaxis (ascending a gradient of odour concentration to reach the odour source), and the widespread capacity for chemotaxis in the animal kingdom, to argue for its biological plausibility.</p>
<p>The ideas are interesting and the presentation in the manuscript is generally clear. The two principle limitations of the manuscript are: i) Many of the ideas that the model implements have been explored in previous work. ii) The mapping of the circuit model onto real biological systems is pretty speculative, particularly with respect to the cerebellum.</p>
<p>Regarding the novelty of the work, the idea of flexibly navigating to goals by descending distance gradients dates back to at least Kaelbling (Learning to achieve goals, IJCAI, 1993), and is closely related to both the successor representation (cited in manuscript) and Linear Markov Decision Processes (LMDPs) (Piray and Daw, 2021, <ext-link ext-link-type="uri" xlink:href="https://doi.org/">https://doi.org/</ext-link> 10.1038/s41467-021-25123-3, Todorov, 2009 <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0710743106">https://doi.org/10.1073/pnas.0710743106</ext-link>). The specific proposal of navigating to goals by doing gradient descent on diffusion distances, computed as powers of the adjacency matrix, is explored in Baram et al. 2018 (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/421461">https://doi.org/10.1101/421461</ext-link>), and the idea that recurrent neural networks whose weights are the adjacency matrix can compute diffusion distances are explored in Fang et al. 2022 (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2022.05.18.492543">https://doi.org/10.1101/2022.05.18.492543</ext-link>). Similar ideas about route planning using the spread of recurrent activity are also explored in Corneil and Gerstner (2015, cited in manuscript). Further exploration of this space of ideas is no bad thing, but it is important to be clear where prior literature has proposed closely related ideas.</p>
</disp-quote>
<p>We have added a discussion section on “Theories and models of spatial learning” with a survey of ideas in this domain and how they come together in the Endotaxis model.</p>
<disp-quote content-type="editor-comment">
<p>Regarding whether the proposed circuit model might plausibly map onto a real biological system, I will focus on the mammalian brain as I don't know the relevant insect literature. It was not completely clear to me how the authors think their model corresponds to mammalian brain circuits. When they initially discuss brain circuits they point to the cerebellum as a plausible candidate structure (lines 520-546). Though the correspondence between cerebellar and model cell types is not very clearly outlined, my understanding is they propose that cerebellar granule cells are the 'map-cells' and Purkinje cells are the 'goal-cells'. I'm no cerebellum expert, but my understanding is that the granule cells do not have recurrent excitatory connections needed by the map cells. I am also not aware of reports of place-field-like firing in these cell populations that would be predicted by this correspondence. If the authors think the cerebellum is the substrate for the proposed mechanism they should clearly outline the proposed correspondence between cerebellar and model cell types and support the argument with reference to the circuit architecture, firing properties, lesion studies, etc.</p>
</disp-quote>
<p>On further thought we agree that the cerebellum-like circuits are not a plausible substrate for the endotaxis algorithm. The anatomy looks compelling, but plasticity at the synapse is anti-hebbian, and - as the reviewer points out - there is little evidence for recurrence among the inputs. We changed the discussion text accordingly.</p>
<disp-quote content-type="editor-comment">
<p>The authors also discuss the possibility that the hippocampal formation might implement the proposed model, though confusingly they state 'we do not presume that endotaxis is localized to that structure' (line 564).</p>
</disp-quote>
<p>We have removed that confusing bit of text.</p>
<disp-quote content-type="editor-comment">
<p>A correspondence with the hippocampus appears more plausible than the cerebellum, given the spatial tuning properties of hippocampal cells, and the profound effect of lesions on navigation behaviours. When discussing the possible relationship of the model to hippocampal circuits it would be useful to address internally generated sequential activity in the hippocampus. During active navigation, and when animals exhibit vicarious trial and error at decision points, internally generated sequential activity of hippocampal place cells appears to explore different possible routes ahead of the animal (Kay et al. 2020, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cell.2020.01.014">https://doi.org/10.1016/j.cell.2020.01.014</ext-link>, Reddish 2016, https:// <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1038/nrn.2015.30">doi.org/10.1038/nrn.2015.30</ext-link>). Given the emphasis the model places on sampling possible future locations to evaluate goal-distance gradients, this seems highly relevant.</p>
</disp-quote>
<p>In our model, the possible future locations are sampled in real life, with the agent moving there or at least in that direction, e.g. via VTE movements. In this simple form the model has no provision for internal planning, and the animal never learns any specific route sequence. One can envision extending such a model with some form of sequence learning that would then support an internal planning mechanism. We mention this in the revised discussion section, along with citation of these relevant articles.</p>
<disp-quote content-type="editor-comment">
<p>Also, given the strong emphasis the authors place on the relationship of their model to chemotaxis/odour-guided navigation, it would be useful to discuss brain circuits involved in chemotaxis, and whether/how these circuits relate to those involved in goal-directed navigation, and the proposed model.</p>
</disp-quote>
<p>The neural basis of goal-directed navigation is probably best understood in the insect brain. There the locomotor decisions seem to be initiated in the central complex, whose circuitry is getting revealed by the fly connectome projects. This area receives input from diverse sensory areas that deliver the signal on which the decisions are based. That includes the mushroom body, which we argue has the anatomical structure to implement the endotaxis algorithm. It remains a mystery how the insect chooses a particular goal for pursuit via its decisions. It could be revealing to force a change in goals (the mode switch in the endotaxis circuit) while recording from brain areas like the central complex.
Our discussion now elaborates on this.</p>
<disp-quote content-type="editor-comment">
<p>Finally, it would be useful to clarify two aspects of the behaviour of the proposed algorithm:</p>
<p>1. When discussing the relationship of the model to the successor representation (lines 620-627), the authors emphasise that learning in the model is independent of the policy followed by the agent during learning, while the successor representation is policy dependent. The policy independence of the model is achieved by making the synapses between map cells binary (0 or 1 weight) and setting them to 1 following a single transition between two locations. This makes the model unsuitable for learning the structure of graphs with probabilistic transitions, e.g. it would not behave adaptively in the widely used two-step task (Daw et al. 2011, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/">https://doi.org/10.1016/</ext-link>
j.neuron.2011.02.027) as it would fail to differentiate between common and rare transitions. This limitation should be made clear and is particularly relevant to claims that the model can handle cognitive tasks in general. It is also worth noting that there are algorithms that are closely related to the successor representation, but which learn about the structure of the environment independent of the subjects policy, e.g. the work of Kaelbling which learns shortest path distances, and the default representation in the work of Piray and Daw (both referenced above). Both these approaches handle probabilistic transition structures.</p>
</disp-quote>
<p>Yes. Our problem statement assumes that the environment is a graph with fixed edge weights. The revised text mentions this and other assumptions in a new section “Choice of learning rule”.</p>
<disp-quote content-type="editor-comment">
<p>1. As the model evaluates distances using powers of adjacency matrix, the resulting distances are diffusion distances not shortest path distances. Though diffusion and shortest path distances are usually closely correlated, they can differ systematically for some graphs (see Baram et al. ci:ted above).</p>
</disp-quote>
<p>The recurrent network of map cells implements a specific function of the adjacency matrix, namely the resolvent (Eqn 7). We have a mathematical proof that this function delivers the shortest graph distances exactly, in the limit of small gain (γ in Eqn 7), and that this holds true for all graphs. For practical navigation in the presence of noise, one needs to raise the gain to something finite. Figure 3 analyzes how this affects deviations from the shortest graph distance, and how nonetheless the model still supports effective navigation over a surprising range. The mathematical details of the proof and further exploration of the resolvent distance at finite gain have been moved to a separate article, which is cited from here, and attached to the submission. The preprint by Baram et al. is cited in that article.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>This paper argues that it has developed an algorithm conceptually related to chemotaxis that provides a general mechanism for goal-directed behaviour in a biologically plausible neural form.</p>
<p>The method depends on substantial simplifying assumptions. The simulated animal effectively moves through an environment consisting of discrete locations and can reliably detect when it is in each location. Whenever it moves from one location to an adjacent location, it perfectly learns the connectivity between these two locations (changes the value in an adjacency matrix to 1). This creates a graph of connections that reflects the explored environment. In this graph, the current location gets input activation and this spreads to all connected nodes multiplied by a constant decay (adjusted to the branching number of the graph) so that as the number of connection steps increases the activation decreases. Some locations will be marked as goals through experiencing a resource of a specific identity there, and subsequently will be activated by an amount proportional to their distance in the graph from the current location, i.e., their activation will increase if the agent moves a step closer and decrease if it moves a step further away. Hence by making such exploratory movements, the animal can decide which way to move to obtain a specified goal.</p>
<p>I note here that it was not clear what purpose, other than increasing the effective range of activation, is served by having the goal input weights set based on the activation levels when the goal is obtained. As demonstrated in the homing behaviour, it is sufficient to just have a goal connected to a single location for the mechanism to work (i.e., the activation at that location increases if the animal takes a step closer to it); and as demonstrated by adding a new graph connection, goal activation is immediately altered in an appropriate way to exploit a new shortcut, without the goal weights corresponding to this graph change needing to be relearnt.</p>
</disp-quote>
<p>As the reviewer states, allowing a graded strengthening of multiple synapses from the map cells increases the effective range of the goal signal. We have now confirmed this in simulations. For example, in the analysis of Fig 3E, a single goal synapse enables perfect navigation only over a range of 7 steps, whereas the distributed goal synapses allow perfect navigation over the full 12 steps. This analysis is included in the code notebook on Github.</p>
<disp-quote content-type="editor-comment">
<p>Given the abstractions introduced, it is clear that the biological task here has been reduced to the general problem of calculating the shortest path in a graph. That is, no real-world complications such as how to reliably recognise the same location when deciding that a new node should be introduced for a new location, or how to reliably execute movements between locations are addressed. Noise is only introduced as a 1% variability in the goal signal. It is therefore surprising that the main text provides almost no discussion of the conceptual relationship of this work to decades of previous work in calculating the shortest path in graphs, including a wide range of neural- and hardwarebased algorithms, many of which have been presented in the context of brain circuits.</p>
<p>The connection to this work is briefly made in appendix A.1, where it is argued that the shortest path distance between two nodes in a directed graph can be calculated from equation 15, which depends only on the adjacency matrix and the decay parameter (provided the latter falls below a given value). It is not clear from the presentation whether this is a novel result. No direct reference is given for the derivation so I assume it is novel. But if this is a previously unknown solution to the general problem it deserves to be much more strongly featured and either way it needs to be appropriately set in the context of previous work.</p>
</disp-quote>
<p>As far as we know this proposal for computing all-pairs-shortest-path is novel. We could not find it in textbooks or an extended literature search. We have discussed it with two graph theorist colleagues, who could not recall seeing it before, although the proof of the relationship is elementary. Inspired by the present reviewer comment, we chose to publish the result in a separate article that can focus on the mathematics and place it in the appropriate context of prior work in graph theory. For related work in the area of neural modeling please see our revised discussion section.</p>
<disp-quote content-type="editor-comment">
<p>Once this principle is grasped, the added value of the simulated results is somewhat limited. These show: 1) in practical terms, the spreading signal travels further for a smaller decay but becomes erratic as the decay parameter (map neuron gain) approaches its theoretical upper bound and decreases below noise levels beyond a certain distance. Both follow the theory. 2) that different graph structures can be acquired and used to approach goal locations (not surprising) .3) that simultaneous learning and exploitation of the graph only minimally affects the performance over starting with perfect knowledge of the graph. 4) that the parameters interact in expected ways. It might have been more impactful to explore whether the parameters could be dynamically tuned, based on the overall graph activity.</p>
</disp-quote>
<p>This is a good summary of our simulation results, but we differ in the assessment of their value. In our experience, simulations can easily demolish an idea that seemed wonderful before exposure to numerical reality. For example, it is well known that one can build a neural integrator from a recurrent network that has feedback gain of exactly 1. In practical simulations, though, these networks tend to be fickle and unstable, and require unrealistically accurate tuning of the feedback gain. In our case, the theory predicts that there is a limited range of gains that should work, below the critical value, but large enough to avoid excessive decay of the signal. Simulation was needed to test what this practical range was, and we were pleasantly surprised that it is not ridiculously small, with robust navigation over a 10-20% range. Similarly, we did not predict that the same parameters would allow for effective acquisition of a new graph, learning of targets within the graph, and shortest-route navigation to those targets, without requiring any change in the operation of the network.</p>
<disp-quote content-type="editor-comment">
<p>Perhaps the most biologically interesting aspect of the work is to demonstrate the effectiveness, for flexible behaviour, of keeping separate the latent learning of environmental structure and the association of specific environmental states to goals or values. This contrasts (as the authors discuss) with the standard reinforcement learning approach, for example, that tries to learn the value of states that lead to reward. Examples of flexibility include the homing behaviour (a goal state is learned before any of the map is learned) and the patrolling behaviour (a goal cell that monitors all states for how recently they were visited). It is also interesting to link the mechanism of exploration of neighbouring states to observed scanning behaviours in navigating animals.</p>
<p>The mapping to brain circuits is less convincing. Specifically, for the analogy to the mushroom body, it is not clear what connectivity (in the MB) is supposed to underlie the graph structure which is crucial to the whole concept. Is it assumed that Kenyon cell connections perform the activation spreading function and that these connections are sufficiently adaptable to rapidly learn the adjacency matrix? Is there any evidence for this?</p>
</disp-quote>
<p>Yes, there is good evidence for recurrent synapses among Kenyon cells (map cells in the model), and for reward-gated synaptic plasticity at the synapses onto mushroom body output cells (goal cells in our model). We have expanded this material in the discussion section. Whether those functions are sufficient to learn the structure of a spatial environment has not been explored; we hope our paper might give an impetus, and are exploring behavioral experiments on flies with colleagues.</p>
<disp-quote content-type="editor-comment">
<p>As discussed above, the possibility that an algorithm like 'endotaxis' could explain how the rodent place cell system could support trajectory planning has already been explored in previous work so it is not clear what additional insight is gained from the current model.</p>
</disp-quote>
<p>Please see our revised discussion section on “theories and models of spatial learning”.
In short, some ingredients of the model have appeared in prior work, but we believe that the present formulation offers an unexpectedly simple end-to-end solution for all components of navigation: exploration, target learning, and goal seeking.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>Major concern:</p>
<p>See the public review. How do the results change depending on whether the recurrent connections between map cells are an adjacency matrix vs. a properly normalized statetransition matrix? I'm especially asking about results related to critical gain (gamma_c), and the dependence of the optimal parameter values on the environment.</p>
</disp-quote>
<p>Please see our response above including the attached reviewer figure.</p>
<disp-quote content-type="editor-comment">
<p>Minor concerns:</p>
<p>It is not always clear when the learning rule is symmetric vs asymmetric (undirected vs directed graph), and it seems to switch back and forth. For example, line 127 refers to a directed graph; Fig 2B and the intro describe symmetric Hebbian learning. Most (all?) of the simulations use the symmetric rule. Please make sure it's clear.</p>
</disp-quote>
<p>For simplicity we now use a symmetric rule throughout, as is appropriate for undirected graphs. We mention that a directed learning rule could be used to learn directed graphs.
See the section on “choice of learning rule”.
M_ij is not defined when it's first introduced (eq 4). Consider labeling the M's and the G's in Fig 2.</p>
<p>Done.</p>
<disp-quote content-type="editor-comment">
<p>The network gain factor (gamma, eq 4) is distributed over both external and recurrent inputs (v = gamma(u + Mv)), instead of local to the recurrent weights like in the Successor Representation. This notational choice is obviously up to the authors. I raise slight concern for two reasons -- first, distributing gamma may affect some of the parameter sweep results (see major concern), and second, it may be confusing in light of how gamma is used in the SR literature (see reviewer's paper for the derivation of how SR is computed by an RNN with gain gamma).</p>
</disp-quote>
<p>In our model, gamma represents the (linear) activation function of the map neuron, from synaptic input to firing output. Because the synaptic input comes from point cells and also from other map cells, the gain factor is applied to both. See for example the Dayan &amp; Abbott book Eqn 7.11, which at steady state becomes our Eqn 4. In the formalism of Fang 2023 (Eqn 2), the factor γ is only applied to the recurrent synaptic input J ⋅ f, but somehow not to the place cell input ϕ. Biophysically, one could imagine applying the variable gain only to the recurrent synapses and not the feed-forward ones. Instead we prefer to think of it as modulating the gain of the neurons, rather than the synapses. The SR literature follows conventions from the early reinforcement learning papers, which were unconstrained by thinking about neurons and synapses. We have added a footnote pointing the reader to the uses of γ in different papers.</p>
<disp-quote content-type="editor-comment">
<p>In eq 13, and simulations, noise is added to the output only, not to the activity of recurrently connected neurons. It is possible this underestimates the impact of noise since the same magnitude of noise in the recurrent network (map cells) could have a compounded effect on the output.</p>
</disp-quote>
<p>Certainly. The equivalent output noise represents the cumulative effect of noise everywhere in the network. We argue that a cumulative effect of 1% is reasonable given the overall ability of animals at stimulus discrimination, which is also limited by noise everywhere in the network. This has been clarified in the text.</p>
<disp-quote content-type="editor-comment">
<p>Fig 3 E, F, it looks like the navigated distance may be capped. I ask because the error bars for graph distance = 12 are so small/nonexistent. If it's capped, this should be in the legend.</p>
</disp-quote>
<p>Correct. 12 is the largest distance on this graph. This has been added to the caption.</p>
<disp-quote content-type="editor-comment">
<p>Fig 3D legend, what does &quot;navigation failed&quot; mean? These results are not shown.</p>
</disp-quote>
<p>On those occasions the agent gets trapped at a local maximum of the goal signal other than the intended goal. We have removed that line as it is not needed to interpret the data.</p>
<disp-quote content-type="editor-comment">
<p>Line 446, typo (Lateron).</p>
</disp-quote>
<p>Fixed.</p>
<disp-quote content-type="editor-comment">
<p>Line 475, I'm a bit confused by the discussion of birds and bats. Bird behavior in the real world does involve discrete paths between points. Even if they theoretically could fly between any points, there are costs to doing so, and in practice, they often choose discrete favorite paths. It is definitely plausible that animals that can fly could also employ Endotaxis, so it is confusing to suggest they don't have the right behavior for Endotaxis, especially given the focus on fruit flies later in the discussion.</p>
</disp-quote>
<p>Good points, we removed that remark. Regarding fruit flies, they handle much important business while walking, such as tracking a mate, fighting rivals over food, finding a good oviposition site.</p>
<disp-quote content-type="editor-comment">
<p>Section 9.3, I'm a bit confused by the discussion of cerebellum-like structures, because I don't think they have as dense recurrent connections as needed for the map cells in Endotaxis. Are you suggesting they are analogous to the output part of Endotaxis only, not the whole thing?</p>
</disp-quote>
<p>Please see our reply in the public review. We have removed this discussion of cerebellar circuits.</p>
<disp-quote content-type="editor-comment">
<p>Line 541, &quot;After sufficient exploration...&quot;, clarify that this is describing learning of just the output synapses, not the recurrent connections between map cells?</p>
</disp-quote>
<p>We have revised this entire section on the arthropod mushroom body.</p>
<disp-quote content-type="editor-comment">
<p>In lines 551-556, the discussion is confusing and possibly not consistent with current literature. How can a simulation prove that synapses in the hippocampus are only strengthened among immediately adjacent place fields? I'd suggest either removing this discussion or adding further clarification. More broadly, the connection between Endotaxis and the hippocampus is very compelling. This might also be a good point to bring up BTSP (though you do already bring it up later).</p>
</disp-quote>
<p>As suggested, we removed this section.</p>
<disp-quote content-type="editor-comment">
<p>Line 621 &quot;The successor representation (at least as currently discussed) is designed to improve learning under a particular policy&quot; That's not actually accurate. Ref 17 (reviewer's manuscript, cited here) is not policy-specific, and instead just learns the transition statistics experienced by the animal, using a biologically plausible learning rule that is very similar to the Endotaxis map cell learning rule (see our Appendix 5, comparing to Endotaxis, though that was referencing the previous version of the Endotaxis preprint where Oja's rule was used).</p>
</disp-quote>
<p>We have edited this section in the discussion and removed the reference to policyspecific successor representations.</p>
<disp-quote content-type="editor-comment">
<p>Line 636 &quot;Endotaxis is always on&quot; ... this was not clear earlier in the paper (e.g. line 268, and the separation of different algorithms, and &quot;while learning do&quot; in Algorithm 2).</p>
</disp-quote>
<p>The learning rules are suspended during some simulations so we can better measure the effects of different parts of endotaxis, in particular learning vs navigating. There is no interference between these two functions, and an agent benefits from having the learning rules on all the time. The text now clarifies this in the relevant sections.</p>
<disp-quote content-type="editor-comment">
<p>Section 9.6, I like the idea of tracing different connected functions. But when you say &quot;that could lead to the mode switch&quot;... I'm a bit confused about what is meant here. A mode switch doesn't need to happen in a different brain area/network, because winnertake-all could be implemented by mutual inhibition between the different goal units.</p>
</disp-quote>
<p>This is an interesting suggestion for the high-level control algorithm. A Lorenzian view is that the animal’s choice of mode depends on internal states or drives, such as thirst vs hunger, that compete with each other. In that picture the goal cells represent options to be pursued, whereas the choice among the options occurs separately. But one could imagine that the arbitrage between drives happens through a competition at the level of goal cells: For example the consumption of water could lead to adaptation of the water cell, such that it loses out in the winner-take-all competition, the food cell takes over, and the mouse now navigates towards food. In this closed-loop picture, the animal doesn’t have to “know” what it wants at any given time, it just wants the right thing. This could eliminate the homunculus entirely! Of course this is all a bit speculative. We have edited the closing comments in a way that leaves open this possibility.</p>
<disp-quote content-type="editor-comment">
<p>Line 697-704, I need more step-by-step explanation/derivation.</p>
</disp-quote>
<p>We now derive the properties of E step by step starting from Eqn (14). The proof that leads to Eqn 14 is now in a separate article (available as a preprint and attached to this submission).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<list list-type="bullet">
<list-item><p>Please include discussion and comparison to previous work of graph-based trajectory planning using spreading activation from the current node and/or the goal node. Here is a (far from comprehensive) list of papers that present similar algorithms:</p>
</list-item></list>
<p>Glasius, R., Komoda, A., &amp; Gielen, S. C. (1996). A biologically inspired neural net for trajectory formation and obstacle avoidance. Biological Cybernetics, 74(6), 511-520.</p>
<p>Gaussier, P., Revel, A., Banquet, J. P., &amp; Babeau, V. (2002). From view cells and place cells to cognitive map learning: processing stages of the hippocampal system. Biological cybernetics, 86(1), 15-28.</p>
<p>Gorchetchnikov A, Hasselmo ME. A biophysical implementation of a bidirectional graph search algorithm to solve multiple goal navigation tasks. Connection Science. 2005;17(1-2):145-166</p>
<p>Martinet, L. E., Sheynikhovich, D., Benchenane, K., &amp; Arleo, A. (2011). Spatial learning and action planning in a prefrontal cortical network model. PLoS computational biology, 7(5), e1002045.</p>
<p>Ponulak, F., &amp; Hopfield, J. J. (2013). Rapid, parallel path planning by propagating wavefronts of spiking neural activity. Frontiers in computational neuroscience, 7, 98.</p>
<p>Khajeh-Alijani, A., Urbanczik, R., &amp; Senn, W. (2015). Scale-free navigational planning by neuronal traveling waves. PloS one, 10(7), e0127269.</p>
<p>Adamatzky, A. (2017). Physical maze solvers. All twelve prototypes implement 1961 Lee algorithm. In Emergent computation (pp. 489-504). Springer, Cham.</p>
</disp-quote>
<p>Please see our reply to the public review above, and the new discussion section on “Theories and models of spatial learning”, which cites most of these papers among others.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Please explain, if it is the case, why the goal cell learning (other than a direct link between the goal and the corresponding map location) and calculation of the overlapping 'goal signal' is necessary, or at least advantageous.</p>
</list-item></list>
</disp-quote>
<p>Please see our reply in the public review above.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Map cells are initially introduced (line 84) as getting input from &quot;only one or a few point cells&quot;. The rest of the paper seems to assume only one. Does it work when this is 'a few'? Does it matter that 'a few' is an option?</p>
</list-item></list>
</disp-quote>
<p>We simplified the text here to “only one point cell”. A map cell with input from two distant locations creates problems. After learning the map synapses from adjacencies in the environment, the model now “believes” that those two locations are connected. This distorts the graph on which the graph distances are computed and introduces errors in the resulting goal signals. One can elaborate the present toy model with a much larger population of map cells that might convey more robustness, but that is beyond our current scope.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>(line 539 on) Please explain what feature in the mushroom body (or other cerebellumlike) circuits is proposed to correspond to the learning of connections in the adjacency matrix in the model.</p>
</list-item></list>
</disp-quote>
<p>Please see our response to this critique in the public review above. In the mushroom body, the Kenyon cells exhibit sparse responses and are recurrently connected. These would correspond to map cells in Endotaxis. For vertebrate cerebellum-like circuits, the correspondence is less compelling, and we have removed this topic from the discussion.</p>
</body>
</sub-article>
</article>