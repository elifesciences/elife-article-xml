<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">100830</article-id>
<article-id pub-id-type="doi">10.7554/eLife.100830</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.100830.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Minimal background noise enhances neural speech tracking: Evidence of stochastic resonance</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Herrmann</surname>
<given-names>Björn</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>bherrmann@research.baycrest.org</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03gp5b411</institution-id><institution>Rotman Research Institute, Baycrest Academy for Research and Education</institution></institution-wrap>, <city>North York</city>, <country country="CA">Canada</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>Department of Psychology, University of Toronto</institution></institution-wrap>, <city>Toronto</city>, <country country="CA">Canada</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Chait</surname>
<given-names>Maria</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>King</surname>
<given-names>Andrew J</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-12-11">
<day>11</day>
<month>12</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-03-10">
<day>10</day>
<month>03</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP100830</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-07-10">
<day>10</day>
<month>07</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-06-24">
<day>24</day>
<month>06</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.06.19.599692"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-12-11">
<day>11</day>
<month>12</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.100830.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.100830.1.sa2">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.100830.1.sa1">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.100830.1.sa0">Reviewer #2 (Public review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Herrmann</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Herrmann</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-100830-v2.pdf"/>
<abstract>
<title>Abstract</title><p>Neural activity in auditory cortex tracks the amplitude-onset envelope of continuous speech, but recent work counter-intuitively suggests that neural tracking increases when speech is masked by background noise, despite reduced speech intelligibility. Noise-related amplification could indicate that stochastic resonance – the response facilitation through noise – supports neural speech tracking, but a comprehensive account is lacking. In five human electroencephalography (EEG) experiments, the current study demonstrates a generalized enhancement of neural speech tracking due to minimal background noise. Results show that a) neural speech tracking is enhanced for speech masked by background noise at very high SNRs (∼30 dB SNR) where speech is highly intelligible; b) this enhancement is independent of attention; c) it generalizes across different stationary background maskers, but is strongest for 12-talker babble; and d) it is present for headphone and free-field listening, suggesting that the neural-tracking enhancement generalizes to real-life listening. The work paints a clear picture that minimal background noise enhances the neural representation of the speech onset-envelope, suggesting that stochastic resonance contributes to neural speech tracking. The work further highlights non-linearities of neural tracking induced by background noise that make its use as a biological marker for speech processing challenging.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Electroencephalography</kwd>
<kwd>speech encoding</kwd>
<kwd>story listening</kwd>
<kwd>temporal response function</kwd>
<kwd>background noise</kwd>
<kwd>attention</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>- Changed the structure of the manuscript
- added supplementary analyses
- provided clarifications throughout</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Speech in everyday life is often masked by background sound, such as music or speech by other people, making speech comprehension challenging, especially for older adults (<xref ref-type="bibr" rid="c81">Pichora-Fuller et al., 2016</xref>; <xref ref-type="bibr" rid="c41">Herrmann and Johnsrude, 2020</xref>). Speech comprehension challenges are a serious barrier to social participation (<xref ref-type="bibr" rid="c71">Nachtegaal et al., 2009</xref>; <xref ref-type="bibr" rid="c39">Heffernan et al., 2016</xref>) and can have long-term negative health consequences, such as cognitive decline (<xref ref-type="bibr" rid="c60">Lin and Albert, 2014</xref>; <xref ref-type="bibr" rid="c79">Panza et al., 2019</xref>). Understanding how individuals encode speech in the presence of background sound is thus an important area of research and clinical application. One successful approach to characterize speech encoding in the brain is to quantify how well neural activity recording with electroencephalography (EEG) tracks relevant speech features of continuous speech, such as the amplitude envelope (<xref ref-type="bibr" rid="c18">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="c19">Crosse et al., 2021</xref>). Greater speech tracking has been associated with higher speech intelligibility (<xref ref-type="bibr" rid="c28">Ding et al., 2014</xref>; <xref ref-type="bibr" rid="c99">Vanthornhout et al., 2018</xref>; <xref ref-type="bibr" rid="c59">Lesenfants et al., 2019</xref>), leading to the suggestion that speech tracking could be a useful clinical biomarker, for example, in individuals with hearing loss (<xref ref-type="bibr" rid="c34">Gillis et al., 2022</xref>; <xref ref-type="bibr" rid="c77">Palana et al., 2022</xref>; <xref ref-type="bibr" rid="c89">Schmitt et al., 2022</xref>). Counterintuitively, however, neural speech tracking has been shown to increase in the presence of background masking sound even at masking levels for which speech intelligibility is decreased (<xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). The causes of this increase in speech tracking under background masking are unclear.</p>
<p>In many everyday situations, a listener must block out ambient, stationary background noise, such as multi-talker babble in a busy restaurant. For low signal-to-noise ratios (SNRs) between speech and background noise, neural speech tracking decreases relative to high SNRs (<xref ref-type="bibr" rid="c27">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c99">Vanthornhout et al., 2018</xref>; <xref ref-type="bibr" rid="c111">Zou et al., 2019</xref>; <xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>), possibly reflecting the decreased speech understanding under speech masking. In contrast, for speech in moderate background noise, when a listener can understand most words, neural speech tracking can be increased relative to clear speech (<xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). This has been interpreted to reflect the increased attention required to understand speech (<xref ref-type="bibr" rid="c38">Hauswald et al., 2022</xref>; <xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). However, the low-to-moderate SNRs (e.g., &lt;10 dB) typically used to study neural speech tracking require listeners to invest cognitively and thus do not allow distinguishing a cognitive mechanism from the possibility that the noise per se leads to an increase in neural tracking, for example, through stochastic resonance, where background noise amplifies the response of a system to an input (<xref ref-type="bibr" rid="c56">Kitajo et al., 2007</xref>; <xref ref-type="bibr" rid="c67">McDonnell and Ward, 2011</xref>; <xref ref-type="bibr" rid="c57">Krauss et al., 2016</xref>). Examining neural speech tracking under high SNRs (e.g., &gt;20 dB SNR), for which individuals can understand speech with ease, is needed to understand the noise-related tracking enhancement.</p>
<p>A few previous studies suggest that noise per se may be a relevant factor. For example, the neural-tracking increase has been observed for speech masked by multi-talker background babble (<xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>), but appears to be less present for noise that spectrally matches the speech signal (<xref ref-type="bibr" rid="c27">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c94">Synigal et al., 2023</xref>), suggesting that the type of noise is important. Other research indicates that neural responses to tone bursts can increase in the presence of minimal noise (i.e., high SNRs) relative to clear conditions (<xref ref-type="bibr" rid="c3">Alain et al., 2009</xref>; <xref ref-type="bibr" rid="c1">Alain et al., 2012</xref>; <xref ref-type="bibr" rid="c2">Alain et al., 2014</xref>), pointing to the critical role of noise in amplifying neural responses independent of speech.</p>
<p>Understanding the relationship between neural speech tracking and background noise is critical because neural tracking is frequently used to investigate consequences of hearing loss for speech processing (<xref ref-type="bibr" rid="c84">Presacco et al., 2019</xref>; <xref ref-type="bibr" rid="c23">Decruy et al., 2020</xref>; <xref ref-type="bibr" rid="c98">Van Hirtum et al., 2023</xref>). Moreover, older adults often exhibit enhanced neural speech tracking (<xref ref-type="bibr" rid="c83">Presacco et al., 2016</xref>; <xref ref-type="bibr" rid="c12">Brodbeck et al., 2018b</xref>; <xref ref-type="bibr" rid="c14">Broderick et al., 2021</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>) which is thought to be due to a loss of inhibition and increased internal noise (<xref ref-type="bibr" rid="c109">Zeng, 2013</xref>; <xref ref-type="bibr" rid="c5">Auerbach et al., 2014</xref>; <xref ref-type="bibr" rid="c57">Krauss et al., 2016</xref>; <xref ref-type="bibr" rid="c110">Zeng, 2020</xref>; <xref ref-type="bibr" rid="c42">Herrmann and Butler, 2021</xref>). The age-related neural tracking enhancement may be harder to understand if external, sound-based noise also drives increases in neural speech tracking.</p>
<p>The current study comprises 5 EEG experiments in younger adults that aim to investigate how neural speech tracking is affected by different degrees of background masking (Experiment 1), whether neural tracking enhancements are due to attention investment (Experiment 2), the generalizability of changes in neural speech tracking for different masker types (Experiments 3 and 4), and whether effects generalize from headphone to free-field listening (Experiment 5). The results point to a highly generalizable enhancement in neural speech tracking at minimal background masking levels that is independent of attention, suggesting that stochastic resonance plays a critical role.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Experiment 1: Enhanced neural speech tracking due to minimal background babble</title>
<p>Experiment 1 aimed to investigate the neural tracking of speech in the presence of different degrees of background masking. Participants (N=22, median age: 23.5 years) listened to ∼2 min stories either in quiet (clear) or in the presence of 12-talker background babble at SNRs ranging from highly intelligible (+30 dB SNR) to moderately difficult intelligibility (–2 dB SNR) while EEG was recorded. Previous work demonstrated that speech at SNRs above ∼12 dB is as intelligibility as clear speech (<xref ref-type="bibr" rid="c47">Holder et al., 2018</xref>; <xref ref-type="bibr" rid="c86">Rowland et al., 2018</xref>; <xref ref-type="bibr" rid="c91">Spyridakou et al., 2020</xref>; <xref ref-type="bibr" rid="c49">Irsik et al., 2022</xref>). Participants answered story comprehension questions and rated gist understanding. Gist ratings for sentences have been shown to correlate highly with intelligibility (word report; <xref ref-type="bibr" rid="c21">Davis and Johnsrude, 2003</xref>; <xref ref-type="bibr" rid="c85">Ritz et al., 2022</xref>). The neural tracking of the amplitude-onset envelope of the speech was analyzed using temporal response function (TRF) approaches (<xref ref-type="bibr" rid="c45">Hertrich et al., 2012</xref>; <xref ref-type="bibr" rid="c18">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="c19">Crosse et al., 2021</xref>).</p>
<p>Comprehension accuracy and gist ratings did not differ between clear speech and any of the SNR levels (p<sub>FDR</sub> &gt; 0.05; thresholded using False Discover Rate [FDR]; <xref ref-type="bibr" rid="c7">Benjamini and Hochberg, 1995</xref>; <xref ref-type="bibr" rid="c33">Genovese et al., 2002</xref>; <xref rid="fig1" ref-type="fig">Figure 1A</xref>). Because gist ratings appeared to change somewhat with SNR (<xref rid="fig1" ref-type="fig">Figure 1A</xref>, right), an explorative piece-wise regression was calculated (<xref ref-type="bibr" rid="c68">McZgee and Carleton, 1970</xref>; <xref ref-type="bibr" rid="c100">Vieth, 1989</xref>; <xref ref-type="bibr" rid="c96">Toms and Lesperance, 2003</xref>). The piece-wise regression revealed a breaking point at +15.6 dB SNR, such that gist ratings decreased for +15.6 dB SNR and lower (t<sub>21</sub> = 3.008, p = 0.007, d = 0.641; right to left in <xref rid="fig1" ref-type="fig">Figure 1A</xref>), whereas gist ratings did not change for +15.6 dB SNR and above (t<sub>21</sub> = 0.214, p = 0.832, d = 0.046).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Results for Experiment 1.</title>
<p><bold>A:</bold> Accuracy of story comprehension (left) and gist ratings (right). <bold>B:</bold> EEG prediction accuracy. <bold>C:</bold> Temporal response functions (TRFs). <bold>D:</bold> P1-N1 and P2-N1 amplitude difference for different speech-clarity conditions. Topographical distributions reflect the average across all speech-clarity conditions. The black asterisk close to the x-axis indicates a significant difference relative to the clear condition (FDR-thresholded). The absence of an asterisk indicates that there was no significant difference. Error bars reflect the standard error of the mean.</p></caption>
<graphic xlink:href="599692v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>EEG prediction accuracy did not differ between clear speech and any SNR level (p<sub>FDR</sub> &gt; 0.05; <xref rid="fig1" ref-type="fig">Figure 1B</xref>). In contrast, the P1-N1 amplitude of the TRF was significantly greater for all SNRs relative to clear speech (p<sub>FDR</sub> ≤ 0.05; <xref rid="fig1" ref-type="fig">Figure 1C, D</xref>; results were comparable when using the amplitude envelope instead of the amplitude-onset envelope, Figure 1–figure supplement 1). Explorative piece-wise regression revealed a breaking point at +9.2 dB SNR, showing a significant linear increase in P1-N1 amplitude from +28.4 dB to +9.2 dB SNR (right to left in <xref rid="fig1" ref-type="fig">Figure 1D</xref>; t<sub>21</sub> = −5.131, p = 4.4 · 10<sup>−5</sup>, d = 1.094), whereas no significant trend was observed for SNRs from about +9.2 dB to −0.4 dB SNR (t<sub>21</sub> = 1.001, p = 0.328, d = 0.214). No differences were found for the P2-N1 amplitude (p<sub>FDR</sub> &gt; 0.05; <xref rid="fig1" ref-type="fig">Figure 1C, D</xref>).</p>
<p>Experiment 1 replicates the previously observed enhancement in the neural tracking of the speech onset-envelope for speech presented in moderate background babble (<xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). Critically, Experiment 1 expands the previous work by showing that background babble also leads to enhanced neural speech tracking for very minimal background masking levels (∼30 dB SNR). A noise-related enhancement at moderate babble levels (9 dB SNR) has previously been interpreted to result from increased attention or effort to understand speech (<xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). However, speech for very high SNRs (&gt;15 dB) used in Experiment 1 is highly intelligible and thus should require little or no effort to understand. It is therefore unlikely that increased attention or effort drive the increase in neural speech tracking in background babble. Nonetheless, participants attended to the speech in Experiment 1, and it can thus not be fully excluded that attention investment played a role in the noise-related enhancement.</p>
</sec>
<sec id="s2b">
<title>Experiment 2: Noise-related enhancement in speech tracking is unrelated to attention</title>
<p>To investigate the role of attention in the noise-related enhancement of neural speech tracking, participants in Experiment 2 (N=22; median age: 23 years) were presented with stories under the same speech-clarity conditions as for Experiment 1 while performing a visual 1-back digit memory task and ignoring the stories. EEG was recorded concurrently. Story comprehension and gist were not assessed to avoid that participants feel they should pay attention to the speech materials.</p>
<p>The behavioral results showed no significant differences for hit rate or response time in the visual 1-back task between the clear condition and any of the SNRs (p<sub>FDR</sub> &gt; 0.05; <xref rid="fig2" ref-type="fig">Figure 2A</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Results for Experiment 2.</title>
<p><bold>A:</bold> Hit rate (left) and response times (right) for the visual 1-back task. <bold>B:</bold> EEG prediction accuracy. <bold>C:</bold> Temporal response functions (TRFs). <bold>D:</bold> P1-N1 and P2-N1 amplitude difference for different speech-clarity conditions. Topographical distributions reflect the average across all speech-clarity conditions. The black asterisk close to the x-axis indicates a significant difference relative to the clear condition (FDR-thresholded). The absence of an asterisk indicates that there was no significant difference. Error bars reflect the standard error of the mean.</p></caption>
<graphic xlink:href="599692v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>EEG prediction accuracy did not differ between clear speech and any SNR level (p<sub>FDR</sub> &gt; 0.05; <xref rid="fig3" ref-type="fig">Figure 3B</xref>). In contrast, and similar to Experiment 1, the P1-N1 amplitude of the TRF was significantly greater for all SNR levels relative to clear speech (p<sub>FDR</sub> ≤ 0.05; <xref rid="fig3" ref-type="fig">Figure 3C, D</xref>). An explorative piece-wise regression revealed a breaking point at +4.4 dB SNR, showing a significant linear increase in P1-N1 amplitude from +28.4 dB to +4.4 dB SNR (t<sub>21</sub> = −3.506, p = 0.002, d = 0.747; right to left in <xref rid="fig3" ref-type="fig">Figure 3D</xref>), whereas the P1-N1 amplitude decreased from +4.4 dB to −0.4 dB SNR (t<sub>21</sub> = 2.416, p = 0.025, d = 0.515). No differences were found for the P2-N1 amplitude (p<sub>FDR</sub> &gt; 0.05; <xref rid="fig3" ref-type="fig">Figure 3C, D</xref>). There was no correlation between visual task performance and the noise-related enhancement in P1-N1 amplitude (Figure 2–figure supplement 1), speaking against the possibility that high-performers in the visual task expanded effort/attention to speech.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Depiction of stimulus samples.</title>
<p><bold>A:</bold> Time courses for clear speech and speech to which background babble of speech-matched noise was added at 20 dB SNR (all sound mixtures were normalized to the same root-mean-square amplitude). The first 6 s of a story are shown. <bold>B:</bold> Spectrograms of the samples in Panel A. <bold>C:</bold> Power spectra for clear speech, babble, and speech-matched noise. In C, only background babble/noise is displayed, without added speech.</p></caption>
<graphic xlink:href="599692v3_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The results of Experiment 2 show that, under diverted attention, neural speech tracking is enhanced for speech presented in background babble at very high SNRs (∼30 dB SNR) relative to clear speech. Because participants did not pay attention to the speech in Experiment 2, the results indicate that attention is unlikely to drive the masker-related enhancement in neural tracking observed here and previously (<xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). The enhancement may thus be exogenously rather than endogenously driven. Previous work suggests the type of masker may play an important role in whether speech tracking is enhanced by background sound. A masker-related enhancement was reported for a 12-talker babble masker (<xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>), whereas the effect was absent, or relatively small, for a stationary noise that spectrally matched the speech signal (<xref ref-type="bibr" rid="c27">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c111">Zou et al., 2019</xref>; <xref ref-type="bibr" rid="c94">Synigal et al., 2023</xref>). Sound normalization also differed. The work observing the enhancement normalized all SNR conditions to the same overall amplitude (<xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). This leads to a reduction in the speech-signal amplitude as SNR decreases, which, in fact, works against the observation that neural speech tracking is enhanced for speech in babble. In the other studies, the level of the speech signal was kept the same for all materials and background noise was added at specific SNRs (<xref ref-type="bibr" rid="c27">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c94">Synigal et al., 2023</xref>). Using this normalization approach, the overall amplitude of the sound mixture increases with decreasing SNR. Experiment 3 was conducted to disentangle these different potential contributions to the masker-related enhancement in neural speech tracking.</p>
</sec>
<sec id="s2c">
<title>Experiment 3: Masker-related enhancement of neural tracking is greater for babble than speech-matched noise</title>
<p>In Experiment 3, participants listened to stories in quiet (clear), in background babble, and in noise that spectrally matched the speech signal, both at 10, 15, and 20 dB SNR. Auditory materials were either normalized to the same root-mean square amplitude (i.e., the intensity of the speech signal decreased with increasing SNR; referred to as ‘lower intensity’ of speech) or the speech signal was kept at the same level across stories and the background sound was added to it at different SNRs (referred to as ‘higher intensity’ of the speech). Sample time courses, spectrograms, and spectra are shown in <xref rid="fig3" ref-type="fig">Figure 3</xref>. Participants answered story comprehension questions and rated gist understanding.</p>
<p>The analysis of behavioral performance revealed significantly lower comprehension performance and gist ratings, compared to clear speech, for babble-masked speech that was normalized such that the speech level was slightly lower than for clear speech (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). The rmANOVA (Masker Type, Normalization Type; excluding clear speech) for the proportion of correctly answered comprehension questions did not reveal any effects or interactions (for all p &gt; 0.1). The rmANOVA for gist ratings revealed an effect of Normalization Type (F<sub>1,22</sub> = 4.300, p = 0.05, ω<sup>2</sup> = 0.008), showing higher gist ratings when the speech signal had a ‘higher’ compared to a ‘lower’ intensity, whereas the other effects and interactions were not significant (for all p &gt; 0.05).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Results for Experiment 3.</title>
<p><bold>A:</bold> Accuracy of story comprehension (left) and gist ratings (right). Higher vs. lower intensity refers to the two sound-level normalization types, one resulting a slightly lower intensity of the speech signal in the sound mixture than the other. <bold>B:</bold> EEG prediction accuracy. <bold>C:</bold> Temporal response functions (TRFs). <bold>D:</bold> P1-N1 (left) and P2-N1 (right) amplitude difference for clear speech and different speech-masking and sound normalization conditions. In panels A, B, and D, a colored asterisk close to the x-axis indicates a significant difference relative to the clear condition (FDR-thresholded). The specific color of the asterisk – blue vs red – indicates the normalization type (higher vs lower speech level, respectively). The absence of an asterisk indicates that there was no significant difference relative to clear speech. Error bars reflect the standard error of the mean.</p></caption>
<graphic xlink:href="599692v3_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The analysis of EEG prediction accuracy revealed no differences between clear speech and any of the masked speech (for all p<sub>FDR</sub> &gt; 0.05; <xref rid="fig4" ref-type="fig">Figure 4B</xref>). The rmANOVA, focusing on the two factors (Normalization Type, Masker Type), did not reveal any significant effects nor interactions (for all p &gt; 0.25).</p>
<p>The analysis of the TRFs revealed the following results. For the babble masker, the P1-N1 amplitudes were larger for all SNRs compared to clear speech, for both sound-level normalization types (for all p<sub>FDR</sub> ≤ 0.05). For the speech-matched noise, P1-N1 amplitudes were larger for all SNRs compared to clear speech for the normalization resulting in lower speech intensity (for all p<sub>FDR</sub> ≤ 0.05), but only for 10 dB SNR for the normalization resulting in higher speech intensity (15 dB SNR was significant for an uncorrected t-tests). The rmANOVA revealed larger P1-N1 amplitudes for the 12-talker babble compared to the speech-matched noise masker (effect of Masker Type: F<sub>1,22</sub> = 32.849, p = 9.1 · 10<sup>−6</sup>, ω<sup>2</sup> = 0.162) and larger amplitudes for lower SNRs (effect of SNR: F<sub>2,44</sub> = 22.608, p = 1.8 · 10<sup>−7</sup>, ω<sup>2</sup> = 0.049). None of the interactions nor the effect of Normalization Type were significant (for all p &gt; 0.05).</p>
<p>Analysis of the P2-N1 revealed larger amplitudes for speech masked by speech-matched noise at 10 dB and 15 dB SNR, for both normalization types (for all p<sub>FDR</sub> ≤ 0.05). None of the other masked speech conditions differed from clear speech. The rmANOVA revealed an effect of SNR (F<sub>2,44</sub> = 26.851, p = 2.7 · 10<sup>−8</sup>, ω<sup>2</sup> = 0.024), Masker Type (F<sub>1,22</sub> = 5.859, p = 0.024, ω<sup>2</sup> = 0.023), and a SNR × Masker Type interaction (F<sub>2,44</sub> = 7.684, p = 0.001, ω<sup>2</sup> = 0.007). The interaction was due to an increase in P2-N1 amplitude with decreasing SNR for the speech-matched noise (all p<sub>Holm</sub> ≤ 0.05), whereas P2-N1 amplitudes for babble did not differ significantly between SNR conditions (all p<sub>Holm</sub> &gt; 0.05).</p>
<p>The results of Experiment 3 show that neural speech tracking increases for babble and speech-matched noise maskers compared to clear speech, but that the 12-talker babble masker leads to a greater enhancement compared to the speech-matched noise. Slight variations in the level of the speech signal in the sound mixture (resulting from different sound-level normalization procedures) do not seem to overly impact the results. Because Experiment 3 indicates that the type of background noise may affect the degree of masker-related enhancement, we conducted Experiment 4 to investigate whether different types of commonly used noises lead to similar enhancements in neural speech tracking.</p>
</sec>
<sec id="s2d">
<title>Experiment 4: Neural-tracking enhancements generalize across different masker types</title>
<p>In Experiment 4, participants listened to stories in quiet (clear) or in the presence of different types of background maskers at +20 dB SNR while EEG was recorded. Masker types included stationary noise that spectrally matched the speech signal (Experiment 3), white noise, pink noise, 12-talker babble (Experiments 1-3), and three newly created 12-talker babbles. The three new babble maskers were used to ensure there is nothing specific about the babble masker used in Experiments 1-3 and our previous work (<xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>) that could lead to an enhanced tracking response. The three new babble maskers varied in spectral properties associated with different voice genders (male, female, male + female) made of 12 speech streams generated using an artificial intelligence based speech synthesizer. The amplitude spectra for clear speech and each masker type are shown in <xref rid="fig5" ref-type="fig">Figure 5</xref>. Participants answered comprehension questions and rated gist understanding.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Spectra for clear speech and different background noises.</title></caption>
<graphic xlink:href="599692v3_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Comprehension accuracy and gist ratings for the clear story did not significantly differ from the data for masked speech (for all p<sub>FDR</sub> &gt; 0.05; <xref rid="fig6" ref-type="fig">Figure 6A</xref>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Results for Experiment 4.</title>
<p><bold>A:</bold> Accuracy of story comprehension (left) and gist ratings (right). <bold>B:</bold> EEG prediction accuracy. <bold>C:</bold> Temporal response functions (TRFs). <bold>D:</bold> P1-N1 and P2-N1 amplitude difference for clear speech and different speech-masking conditions. Topographical distributions reflect the average across all conditions. In panels A, B, and D, the black asterisk close to the x-axis indicates a significant difference relative to the clear condition (FDR-thresholded). The absence of an asterisk indicates that there was no significant difference relative to clear speech. Error bars reflect the standard error of the mean.</p></caption>
<graphic xlink:href="599692v3_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>EEG prediction accuracy did not significantly differ between clear speech and any of the masker types (for all p<sub>FDR</sub> &gt; 0.05; <xref rid="fig6" ref-type="fig">Figure 6B</xref>). In contrast, the TRF P1-N1 amplitude was larger for all masker types, expect for white noise, compared to clear speech (for all p<sub>FDR</sub> ≤ 0.05; <xref rid="fig6" ref-type="fig">Figure 6D</xref>; the difference between clear speech and speech masked by white noise was significant when uncorrected; p = 0.05). There were no differences among the four different babble maskers (for all p &gt; 0.6), indicating that different voice genders of the 12-talker babble do not differentially affect the masker-related enhancement in neural speech tracking. However, the P1-N1 amplitude was larger for speech masked by the babble maskers (collapsed across the four babble maskers) compared to speech masked by white noise (t<sub>19</sub> = 4.133, p = 5.7 · 10<sup>−4</sup>, d = 0.68), pink noise (t<sub>19</sub> = 5.355, p = 3.6 · 10<sup>−5</sup>, d = 1.197) and the noise that spectrally matched to speech (t<sub>19</sub> = 3.039, p = 0.007, d = 0.68). There were no differences among the three noise maskers (for all p &gt; 0.05). The P2-N1 amplitude for clear speech did not different from the P2-N1 amplitude for masked speech (for all p<sub>FDR</sub> &gt; 0.05; <xref rid="fig6" ref-type="fig">Figure 6D</xref>, right).</p>
<p>The results of Experiment 4 replicate the results from Experiments 1-3 by showing that babble noise at a high SNR (20 dB) increases neural speech tracking. Experiment 4 further shows that the neural-tracking enhancement generalizes across different noises, albeit a bit less for white noise (significant only when uncorrected for multiple comparisons). Results from Experiment 4 also replicate the larger tracking enhancement for speech in babble noise compared to speech in speech-matched noise observed in Experiment 3.</p>
</sec>
<sec id="s2e">
<title>Experiment 5: Neural-tracking enhancement generalizes to free-field listening</title>
<p>Sounds in Experiments 1-4 were presented via headphones, which is comparable to previous work using headphones or in-ear phones (<xref ref-type="bibr" rid="c3">Alain et al., 2009</xref>; <xref ref-type="bibr" rid="c1">Alain et al., 2012</xref>; <xref ref-type="bibr" rid="c27">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c2">Alain et al., 2014</xref>; <xref ref-type="bibr" rid="c13">Broderick et al., 2018</xref>; <xref ref-type="bibr" rid="c22">Decruy et al., 2019</xref>; <xref ref-type="bibr" rid="c97">Tune et al., 2021</xref>; <xref ref-type="bibr" rid="c94">Synigal et al., 2023</xref>; <xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). However, headphones or in-ear phones attenuate external sound sources such that clear speech is arguably presented in ‘unnatural’ quiet. In everyday life, speech typically reaches our ears in free-field space. Experiment 5 examines whether the noise-related enhancement in neural speech tracking also generalizes to free-field listening. Participants listened to stories in quiet and in 12-talker babble (+10, +15, +20 dB SNR) either via headphones or loudspeakers.</p>
<p>Comprehension accuracy and gist ratings are shown in <xref rid="fig7" ref-type="fig">Figure 7A</xref>. There were no differences between clear speech and speech masked by background babble for any of the conditions, with the exception of a lower gist rating for the 20 dB SNR loudspeaker condition (<xref rid="fig7" ref-type="fig">Figure 7A</xref>, right).</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7:</label>
<caption><title>Results for Experiment 5.</title>
<p><bold>A:</bold> Accuracy of story comprehension (left) and gist ratings (right). <bold>B:</bold> EEG prediction accuracy. <bold>C:</bold> Temporal response functions (TRFs). <bold>D:</bold> P1-N1 and P2-N1 amplitude difference for clear speech and different speech-masking and sound-delivery conditions. In panels A, B, and D, a colored asterisk close to the x-axis indicates a significant difference relative to the clear condition (FDR-thresholded). The specific color of the asterisk – blue vs red – indicates the sound-delivery type. The absence of an asterisk indicates that there was no significant difference relative to clear speech. Error bars reflect the standard error of the mean.</p></caption>
<graphic xlink:href="599692v3_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>For the EEG prediction accuracy, there were no differences between the clear conditions and any of the masked speech conditions (<xref rid="fig7" ref-type="fig">Figure 7B</xref>). The rmANOVA (3 SNRs [+10, +15, +20 dB] × 2 Sound Delivery methods) did not reveal effects or interactions involving SNR or Sound Delivery (ps &gt; 0.15).</p>
<p>The analysis of TRF amplitudes revealed significantly larger P1-N1 amplitudes for all masked speech conditions compared to clear speech (for all p<sub>FDR</sub> ≤ 0.05; <xref rid="fig7" ref-type="fig">Figure 7D</xref>, left). A rmANOVA (3 SNRs × 2 Sound Delivery methods) did not reveal any effects or interaction involving SNR or Sound Delivery (ps &gt; 0.1). For the P2-N1 amplitude, there were no differences between clear speech and any of the masked speech conditions (for all p<sub>FDR</sub> &gt; 0.05; <xref rid="fig7" ref-type="fig">Figure 7D</xref>, right). The rmANOVA revealed an effect of SNR (F<sub>2,42</sub> = 3.953, p = 0.027, ω<sup>2</sup> = 0.005), caused by the lower P2-N1 amplitude for the +15 dB SNR conditions compared to the +10 dB SNR (t<sub>42</sub> = 2.493, p<sub>Holm</sub> = 0.05, d = 0.171) and the +20 dB SNR condition (t<sub>42</sub> = 2.372, p<sub>Holm</sub> = 0.05, d = 0.162). There was no effect of Sound Delivery nor a SNR × Sound Delivery interaction (ps &gt; 0.5).</p>
<p>The results of Experiment 5 show that the enhanced neural tracking of speech associated with minor background babble is unrelated to delivering sounds via headphones (which typically attenuate sounds in the environment). Instead, minor background babble at +20 dB SNR also increased the neural tracking of speech under free-field (loudspeaker) conditions, thus pointing to the generalizability of the phenomenon to conditions more akin to naturalistic listening scenarios.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In 5 EEG experiments, the current study investigated the degree to which background masking sounds at high SNRs, for which speech is highly intelligible, affect neural speech tracking. The results show that 12-talker babble enhances neural tracking at very high SNRs (∼30 dB) relative to clear speech (Experiment 1) and that this enhancement is present even when participants carry out an unrelated visual task (Experiment 2), suggesting that attention or effort do not cause the noise-related neural-tracking enhancement. The results further show that the enhancement of neural speech tracking is greater for speech in the presence of 12-talker babble compared to a stationary noise that spectrally matches the speech (Experiments 3 and 4), although both masker types spectrally overlap with the speech. The enhancement was also greater for 12-talker babble compared to pink noise and white noise (Experiment 4). Finally, the enhanced neural speech tracking generalizes from headphone to free-field listening (Experiment 5), pointing to the real-world nature of the tracking enhancement. Overall, the current study paints a clear picture of a highly generalized enhancement of neural speech tracking in the presence of minimal background noise, making links to speech intelligibility and listening difficulties in noise challenging.</p>
<sec id="s3a">
<title>Enhanced neural tracking of speech under minimal background noise</title>
<p>Across all five experiments of the current study, speech masked by background noise at high SNRs (up to 30 dB SNR) led to enhanced neural tracking of the amplitude-onset envelope of speech (this was also present for the amplitude envelope, see Figure 1-figure supplement 1). The enhancement was present for different background maskers, but most prominently for 12-talker babble. Previous work on neural speech tracking also observed enhanced neural tracking for speech masked by 12-talker babble at moderate SNRs (∼12 dB; <xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>), consistent with the current study. The current results are also consistent with studies showing a noise-related enhancement to tone bursts (<xref ref-type="bibr" rid="c3">Alain et al., 2009</xref>; <xref ref-type="bibr" rid="c1">Alain et al., 2012</xref>; <xref ref-type="bibr" rid="c2">Alain et al., 2014</xref>), syllable onsets (<xref ref-type="bibr" rid="c80">Parbery-Clark et al., 2011</xref>), and high-frequency temporal modulations in sounds (<xref ref-type="bibr" rid="c102">Ward et al., 2010</xref>; <xref ref-type="bibr" rid="c90">Shukla and Bidelman, 2021</xref>). Other work, using a noise masker that spectrally matches the target speech, have not reported tracking enhancements (<xref ref-type="bibr" rid="c27">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c111">Zou et al., 2019</xref>; <xref ref-type="bibr" rid="c94">Synigal et al., 2023</xref>). However, in these works, SNRs have been lower (&lt;10 dB) to investigate neural tracking under challenging listening conditions. At low SNRs, neural speech tracking decreases (<xref ref-type="bibr" rid="c27">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c111">Zou et al., 2019</xref>; <xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref rid="fig1" ref-type="fig">Figures 1</xref> and <xref rid="fig2" ref-type="fig">2</xref>), thus resulting in an inverted u-shape in relation to SNR for attentive and passive listening (Experiments 1 and 2). Moreover, the speech-tracking enhancement was smaller for speech-matched noise compared to babble noise (<xref rid="fig4" ref-type="fig">Figures 4</xref> and <xref rid="fig6" ref-type="fig">6</xref>), potentially explaining the absence of the enhancement for speech-matched noise at low SNRs in previous work (<xref ref-type="bibr" rid="c27">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c111">Zou et al., 2019</xref>; <xref ref-type="bibr" rid="c94">Synigal et al., 2023</xref>).</p>
<p>The noise-related enhancement in the neural tracking of the speech envelope was greatest for 12-talker babble, but it was also present for speech-matched noise, pink noise, and, to some extent, white noise. The latter three noises bare no perceptional relation to speech, but resemble stationary, background buzzing from industrial noise, heavy rain, waterfalls, wind, or ventilation. Twelve-talker babble – which is also a stationary masker – is clearly recognizable as overlapping speech, but words or phonemes cannot be identified (<xref ref-type="bibr" rid="c9">Bilger, 1984</xref>; <xref ref-type="bibr" rid="c10">Bilger et al., 1984</xref>; <xref ref-type="bibr" rid="c105">Wilson, 2003</xref>; <xref ref-type="bibr" rid="c107">Wilson et al., 2012b</xref>). There may thus be something about the naturalistic, speech nature of the background babble that facilitates neural speech tracking.</p>
<p>The spectral power for both the 12-talker babble and the speech-matched noise overlaps strongly with the spectral properties of the speech signal, although the speech-matched noise most closely resembles the spectrum of speech. Spectral overlap of the background sound and the speech signal could cause energetic masking in the auditory periphery and degrade accurate neural speech representations (<xref ref-type="bibr" rid="c15">Brungart et al., 2006</xref>; <xref ref-type="bibr" rid="c64">Mattys et al., 2012</xref>; <xref ref-type="bibr" rid="c106">Wilson et al., 2012a</xref>; <xref ref-type="bibr" rid="c54">Kidd et al., 2019</xref>). Although peripheral masking would not explain why neural speech tracking is enhanced in the first place, more peripheral masking for the speech-matched noise compared to the 12-talker babble would be consistent with a reduced enhancement for the former compared to the latter masker. However, pink noise and white noise spectrally overlap much less with speech than the other two background maskers, but the neural tracking enhancement did not differ between the speech-matched noise, pink noise, and white noise maskers. This again suggests that there may be something about the speech-nature of the babble masker that drives the larger neural tracking enhancement.</p>
<p>Critically, the current results have implications for research and clinical applications. The neural tracking of the speech envelope has been linked to speech intelligibility (<xref ref-type="bibr" rid="c28">Ding et al., 2014</xref>; <xref ref-type="bibr" rid="c99">Vanthornhout et al., 2018</xref>; <xref ref-type="bibr" rid="c59">Lesenfants et al., 2019</xref>) and has been proposed to be a useful clinical biomarker for speech encoding in the brain (<xref ref-type="bibr" rid="c26">Dial et al., 2021</xref>; <xref ref-type="bibr" rid="c34">Gillis et al., 2022</xref>; <xref ref-type="bibr" rid="c89">Schmitt et al., 2022</xref>; <xref ref-type="bibr" rid="c58">Kries et al., 2024</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). However, speech intelligibility, assessed here via gist ratings (<xref ref-type="bibr" rid="c21">Davis and Johnsrude, 2003</xref>; <xref ref-type="bibr" rid="c85">Ritz et al., 2022</xref>), only declines for SNRs below 15 dB SNR (consistent with intelligibility scores; <xref ref-type="bibr" rid="c49">Irsik et al., 2022</xref>), whereas the neural tracking enhancement is already present for ∼30 dB SNR. This result questions the link between neural envelope tracking and speech intelligibility, or at least makes the relationship non-linear (cf. <xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>). Research on the neural tracking of speech using background noise must thus consider that the noise itself may enhance the tracking.</p>
</sec>
<sec id="s3b">
<title>Potential mechanisms associated with enhanced neural speech tracking</title>
<p>Enhanced neural tracking associated with a stationary background masker or noise-vocoded speech has been interpreted to reflect an attentional gain when listeners must invest cognitively to understand speech (<xref ref-type="bibr" rid="c38">Hauswald et al., 2022</xref>; <xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). However, these works used moderate to low SNRs or moderate speech degradations, making it challenging to distinguish between attentional mechanisms and mechanisms driven by background noise per se. The current study demonstrates that attention unlikely causes the enhanced neural tracking of the speech onset-envelope. First, the tracking enhancement was observed for very high SNRs (∼30 dB) at which speech is highly intelligible (<xref ref-type="bibr" rid="c47">Holder et al., 2018</xref>; <xref ref-type="bibr" rid="c86">Rowland et al., 2018</xref>; <xref ref-type="bibr" rid="c91">Spyridakou et al., 2020</xref>; <xref ref-type="bibr" rid="c49">Irsik et al., 2022</xref>). Arguably, little or no effort is needed to understand speech at ∼30 dB SNR (<xref ref-type="bibr" rid="c86">Rowland et al., 2018</xref>), making attentional gain an unlikely explanation. Importantly, neural speech tracking was enhanced even when participants performed a visual task and were passively presented with the speech materials (<xref rid="fig2" ref-type="fig">Figure 2</xref>). Participants are unlikely to invest effort to understand speech when performing an attention-demanding visual task. Taken together, the current study provides little evidence that noise-related enhancements of neural speech tracking are due to attention or effort investment.</p>
<p>Another possibility put forward in the context of enhanced neural responses to tone bursts in noise (<xref ref-type="bibr" rid="c3">Alain et al., 2009</xref>; <xref ref-type="bibr" rid="c1">Alain et al., 2012</xref>; <xref ref-type="bibr" rid="c2">Alain et al., 2014</xref>) is that background noise increases arousal, which, in turn, amplifies the neural response to sound. However, a few pieces of evidence are inconsistent with this hypothesis. Arousal to minimal background noise habituates quickly (<xref ref-type="bibr" rid="c4">Alvar and Francis, 2024</xref>) and arousal does not appear to affect early sensory responses but rather later, non-sensory responses in EEG (&gt;150 ms; <xref ref-type="bibr" rid="c37">Han et al., 2013</xref>). Moreover, pupil dilation – a measure of arousal (<xref ref-type="bibr" rid="c63">Mathôt, 2018</xref>; <xref ref-type="bibr" rid="c52">Joshi and Gold, 2020</xref>; <xref ref-type="bibr" rid="c16">Burlingham et al., 2022</xref>) – is similar for speech in noise at SNRs ranging from +16 dB to +4 dB SNR (<xref ref-type="bibr" rid="c74">Ohlenforst et al., 2017</xref>; <xref ref-type="bibr" rid="c73">Ohlenforst et al., 2018</xref>), for which neural tracking increases (<xref rid="fig1" ref-type="fig">Figure 1</xref>). Hence, arousal is unlikely to explain the noise-related enhancement in neural speech tracking, but more direct research is needed to clarify this further.</p>
<p>A third potential explanation of enhanced neural tracking is stochastic resonance, reflecting an automatic mechanism in neural circuits (<xref ref-type="bibr" rid="c92">Stein et al., 2005</xref>; <xref ref-type="bibr" rid="c66">McDonnell and Abbott, 2009</xref>; <xref ref-type="bibr" rid="c67">McDonnell and Ward, 2011</xref>). Stochastic resonance has been described as the facilitation of a near-threshold input through noise. That is, a near-threshold stimulus or neuronal input, that alone may be insufficient to drive a neuron beyond its firing threshold, can lead to neuronal firing if noise is added, because the noise increases the stimulus or neuronal input for brief periods, causing a response in downstream neurons (<xref ref-type="bibr" rid="c101">Ward et al., 2002</xref>; <xref ref-type="bibr" rid="c69">Moss et al., 2004</xref>). However, the term is now used more broadly to describe any phenomenon where the presence of noise in a nonlinear system improves the quality of the output signal than when noise is absent (<xref ref-type="bibr" rid="c66">McDonnell and Abbott, 2009</xref>). Stochastic resonance has been observed in humans in several domains, such as in tactile, visual, and auditory perception (<xref ref-type="bibr" rid="c55">Kitajo et al., 2003</xref>; <xref ref-type="bibr" rid="c104">Wells et al., 2005</xref>; <xref ref-type="bibr" rid="c95">Tabarelli et al., 2009</xref>, but see <xref ref-type="bibr" rid="c87">Rufener et al., 2020</xref>). In the current study, speech was presented at suprathreshold levels, but stochastic resonance may still play a role at the neuronal level (<xref ref-type="bibr" rid="c93">Stocks, 2000</xref>; <xref ref-type="bibr" rid="c66">McDonnell and Abbott, 2009</xref>). EEG signals reflect the synchronized activity of more than 10,000 neurons (<xref ref-type="bibr" rid="c72">Niedermeyer and da Silva, 2005</xref>). Some neurons may not receive sufficiently strong input to elicit a response when a person listens to clear speech but may be pushed beyond their firing threshold by the additional, acoustically elicited noise in the neural system. Twelve-talker babble was associated with the greatest noise-related enhancement in neural tracking, possibly because the 12-talker babble facilitated neuronal activity in speech-relevant auditory regions, where the other, non-speech noises were less effective.</p>
</sec>
<sec id="s4">
<title>Conclusions</title>
<p>The current study provides a comprehensive account of a generalized increase in the neural tracking of the amplitude-onset envelope of speech due to minimal background noise. The results show that a) neural speech tracking is enhanced for speech masked by background noise at very high SNRs (∼30 dB), b) this enhancement is independent of attention, c) it generalizes across different stationary background maskers, although being strongest for 12-talker babble, and d) it is present for headphone and free-field listening, suggesting the neural-tracking enhancement generalizes to real-life situations. The work paints a clear picture that minimal background noise enhances the neural representation of the speech envelope. The work further highlights the non-linearities of neural speech tracking as a function of background noise, challenging the feasibility of neural speech tracking as a biological marker for speech processing.</p>
</sec>
</sec>
<sec id="s5">
<title>Methods</title>
<sec id="s5a">
<title>Key resources</title>
<table-wrap id="utbl1" orientation="portrait" position="float">
<graphic xlink:href="599692v3_utbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s5b">
<title>Participants</title>
<p>The current study comprised 5 experiments. Participants were native English speakers or grew up in English-speaking countries (mostly Canada) and have been speaking English since early childhood (&lt;5 years of age). Participants reported having normal hearing abilities and no neurological disease (one person reported having ADHD, but this did not affect their participation). Participants gave written informed consent prior to the experiment and were compensated for their participation. The study was conducted in accordance with the Declaration of Helsinki, the Canadian Tri-Council Policy Statement on Ethical Conduct for Research Involving Humans (TCPS2-2014), and was approved by the Research Ethics Board of the Rotman Research Institute at Baycrest Academy for Research and Education (REB # 21-27).</p>
<p>Twenty-two adults participated in Experiment 1 (median: 23.5 years; range: 18–35 years; 12 male, 9 female, 1 transgender). Twenty-two adults participated in Experiment 2 (median: 23 years; range: 18–31 years; 11 male, 10 female, 1 transgender; five additional datasets were excluded due to low performance in the visual task; see details below). Twenty-three people participated in Experiment 3 (median: 25 years; range: 19–33 years; 7 male, 15 female, 1 transgender; data from one additional person were excluded due to technical issues resulting in missing triggers). Twenty individuals participated in Experiment 4 (median: 25.5 years; range: 19–34 years; 4 male, 15 female, 1 transgender; one additional dataset was excluded due to missing triggers). Twenty-two adults participated in Experiment 5 (median: 26 years; range: 19–34 years; 10 male, 11 female, 1 transgender; one additional dataset was excluded due to spurious triggers and artifacts). Several participants took part in more than one of the experiments, in separate sessions on separate days: 7, 7, 9, 9, and 14 (for Experiments 1-5, respectively) participated only in one experiment; 3 individuals participated in all 5 experiments; 68 unique participants took part across the 5 experiments.</p>
</sec>
<sec id="s5c">
<title>Sound environment and stimulus presentation</title>
<p>Data collection was carried out in a sound-attenuating booth. Sounds were presented via Sennheiser (HD 25-SP II) headphones and computer loudspeakers (Experiment 5) through an RME Fireface 400 external sound card. Stimulation was run using Psychtoolbox (v3.0.14, RRID:SCR_002881) in MATLAB (MathWorks Inc., RRID:SCR_001622) on a Lenovo T480 laptop with Microsoft Windows 7. Visual stimulation was projected into the sound booth via screen mirroring. All sounds were presented at about 65 dB SPL.</p>
</sec>
<sec id="s5d">
<title>Story materials</title>
<p>In each of the 5 experiments, participants listened to 24 stories of about 1:30 to 2:30 min duration each. OpenAI’s ChatGPT (<xref ref-type="bibr" rid="c76">OpenAI et al., 2023</xref>; RRID:SCR_023775) was used to generate each story, five corresponding comprehension questions, and four associated multiple-choice answer options (1 correct, 3 incorrect). Each story was on a different topic (e.g., a long-lost friend, a struggling artist). GPT stories, questions, and answer choices were manually edited wherever needed to ensure accuracy. Auditory story files were generated using Google’s modern artificial intelligence (AI)-based speech synthesizer using the male “en-US-Neural2-J” voice with default pitch, speed, and volume parameters (i.e., 0, 1, and 0, respectively; <ext-link ext-link-type="uri" xlink:href="https://cloud.google.com/text-to-speech/docs/voices">https://cloud.google.com/text-to-speech/docs/voices</ext-link>). Modern AI speech is experienced as very naturalistic and speech perception is highly similar for AI and human speech (<xref ref-type="bibr" rid="c40">Herrmann, 2023</xref>). A new set of 24 stories and corresponding comprehension questions and multiple-choice options were generated for each of the 5 experiments (due to a mistake 8 stories from Experiments 1-4 were also used in Experiment 5). The duration of silences between sentences was about 1 s in the stories generated through Google’s synthesizer, which sounded artificially long. They were thus shortened to 0.6 s to sound more naturalistic.</p>
<p>After each story in Experiments 1, 3, 4, and 5, participants answered the 5 comprehension questions about the story. Each comprehension question comprised four response options (chance level = 25%). Participants further rated the degree to which they understood the gist of what was said in the story, using a 9-point scale that ranged from 1 (strongly disagree) to 9 (strongly agree; the precise wording was: “I understood the gist of the story” and “(Please rate this statement independently of how you felt about the other stories)”). Gist ratings were linearly scaled to range between 0 and 1 to facilitate interpretability similar to the proportion of correct responses (<xref ref-type="bibr" rid="c62">Mathiesen et al., 2024</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). For short sentences, gist ratings have been shown to highly correlated with speech intelligibility scores (<xref ref-type="bibr" rid="c21">Davis and Johnsrude, 2003</xref>; <xref ref-type="bibr" rid="c85">Ritz et al., 2022</xref>). In Experiment 2, participants performed a visual n-back task (detailed below) while stories were presented and no comprehension questions nor the gist rating were administered.</p>
</sec>
<sec id="s5e">
<title>Electroencephalography recordings and preprocessing</title>
<p>Electroencephalographical signals were recorded from 16 scalp electrodes (Ag/Ag–Cl-electrodes; 10-20 placement) and the left and right mastoids using a BioSemi system (Amsterdam, The Netherlands). The sampling frequency was 1024 Hz with an online low-pass filter of 208 Hz. Electrodes were referenced online to a monopolar reference feedback loop connecting a driven passive sensor and a common-mode-sense (CMS) active sensor, both located posteriorly on the scalp.</p>
<p>Offline analysis was conducted using MATLAB software (MathWorks Inc., RRID:SCR_001622). An elliptic filter was used to suppress power at the 60-Hz line frequency (stopband 59.5–60.5 Hz, 80 dB suppression). Data were re-referenced by averaging the signal from the left and right mastoids and subtracting the average separately from each of the 16 channels. Re-referencing to the averaged mastoids was calculated to gain high signal-to-noise ratio for auditory responses at fronto-central-parietal electrodes (<xref ref-type="bibr" rid="c88">Ruhnau et al., 2012</xref>; <xref ref-type="bibr" rid="c43">Herrmann et al., 2013</xref>). Data were filtered with a 0.7-Hz high-pass filter (length: 2449 samples, Hann window) and a 22-Hz low-pass filter (length: 211 samples, Kaiser window).</p>
<p>EEG data were segmented into time series time-locked to story onset and down-sampled to 512 Hz. Independent components analysis (FieldTrip toolbox, <xref ref-type="bibr" rid="c75">Oostenveld et al., 2011</xref>, RRID:SCR_004849) was used to remove signal components reflecting blinks, eye movement, and noise artifacts (<xref ref-type="bibr" rid="c6">Bell and Sejnowski, 1995</xref>; <xref ref-type="bibr" rid="c61">Makeig et al., 1995</xref> <xref ref-type="bibr" rid="c75">Oostenveld et al., 2011</xref>). After the independent components analysis, remaining artifacts were removed by setting the voltage for segments in which the EEG amplitude varied more than 80 µV within a 0.2-s period in any channel to 0 µV (cf. <xref ref-type="bibr" rid="c29">Dmochowski et al., 2012</xref>; <xref ref-type="bibr" rid="c30">Dmochowski et al., 2014</xref>; <xref ref-type="bibr" rid="c17">Cohen and Parra, 2016</xref>; <xref ref-type="bibr" rid="c49">Irsik et al., 2022</xref>; <xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). Data were low-pass filtered at 10 Hz (251 points, Kaiser window) because neural signals in the low-frequency range are most sensitive to acoustic features (<xref ref-type="bibr" rid="c25">Di Liberto et al., 2015</xref>; <xref ref-type="bibr" rid="c112">Zuk et al., 2021</xref>; <xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>).</p>
</sec>
<sec id="s5f">
<title>Calculation of amplitude-onset envelopes</title>
<p>For each clear story (i.e., without background babble or noise), a cochleogram was calculated using a simple auditory-periphery model with 30 auditory filters (<xref ref-type="bibr" rid="c65">McDermott and Simoncelli, 2011</xref>; cutoffs evenly spaced on the ERB scale; <xref ref-type="bibr" rid="c36">Glasberg and Moore, 1990</xref>). The resulting amplitude envelope for each auditory filter was compressed by 0.6 to simulate inner ear compression (<xref ref-type="bibr" rid="c65">McDermott and Simoncelli, 2011</xref>). Such a computationally simple peripheral model has been shown to be sufficient, as compared to complex, more realistic models, for envelope-tracking approaches (<xref ref-type="bibr" rid="c8">Biesmans et al., 2017</xref>). Amplitude envelopes were averaged across auditory filters and low-pass filtered at 40-Hz filter (Butterworth filter, 4<sup>th</sup> order). To obtain the amplitude-onset envelope, the first derivative was calculated and all negative values were set to zero (<xref ref-type="bibr" rid="c45">Hertrich et al., 2012</xref>; <xref ref-type="bibr" rid="c32">Fiedler et al., 2017</xref>; <xref ref-type="bibr" rid="c31">Fiedler et al., 2019</xref>; <xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). The onset-envelope was down-sampled to match the sampling of the EEG data. The amplitude-onset envelope was selected because a) several previous works have used it (<xref ref-type="bibr" rid="c45">Hertrich et al., 2012</xref>; <xref ref-type="bibr" rid="c32">Fiedler et al., 2017</xref>; <xref ref-type="bibr" rid="c11">Brodbeck et al., 2018a</xref>; <xref ref-type="bibr" rid="c20">Daube et al., 2019</xref>; <xref ref-type="bibr" rid="c31">Fiedler et al., 2019</xref>), b) our previous work first observing the enhancement also used the amplitude-onset envelope (<xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>), and c) the amplitude-onset envelope has been suggested to elicit a strong speech tracking response (<xref ref-type="bibr" rid="c45">Hertrich et al., 2012</xref>). Results for analyses using the amplitude envelope instead of the amplitude-onset envelope show similar effects and are provided in the Supplementary Materials (Figure 1-figure supplement 1).</p>
</sec>
<sec id="s5g">
<title>EEG temporal response function and prediction accuracy</title>
<p>A forward model based on the linear temporal response function (TRF; <xref ref-type="bibr" rid="c18">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="c19">Crosse et al., 2021</xref>) was used to quantify the relationship between the amplitude-onset envelope of a story and EEG activity (note that cross-correlation led to very similar results Figure 1-figure supplement 2; cf. <xref ref-type="bibr" rid="c45">Hertrich et al., 2012</xref>). The ridge regularization parameter lambda (λ), which prevents overfitting, was set to 10 based on previous work (<xref ref-type="bibr" rid="c32">Fiedler et al., 2017</xref>; <xref ref-type="bibr" rid="c31">Fiedler et al., 2019</xref>; <xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). Pre-selection of λ based on previous work avoids extremely low and high λ on some cross-validation iterations and avoids substantially longer computational time. Pre-selection of λ also avoids issues if limited data per condition are available, as in the current study (<xref ref-type="bibr" rid="c19">Crosse et al., 2021</xref>).</p>
<p>For each story, 50 25-s data snippets (<xref ref-type="bibr" rid="c18">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="c19">Crosse et al., 2021</xref>) were extracted randomly from the EEG data and corresponding onset-envelope (<xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). Each of the 50 EEG and onset-envelope snippets were held out once as a test dataset, while the remaining non-overlapping EEG and onset-envelope snippets were used as training datasets. Overlapping snippets in the training data were used to increase the amount of data in the training given the short duration of the stories. Speech-clarity levels were randomized across stories and all analyses were conducted similarly for all conditions. Hence, no impact of overlapping training data on the results is expected (consistent with noise-related enhancements observed previously when longer stories and non-overlapping data were used; <xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>). Analyses using cross-correlation, for which data snippets are treated independently, show similar results compared to those reported here using TRFs (Figure 1-figure supplement 2).</p>
<p>For each training dataset, linear regression with ridge regularization was used to map the onset-envelope onto the EEG activity to obtain a TRF model for lags ranging from 0 to 0.4 s (<xref ref-type="bibr" rid="c46">Hoerl and Kennard, 1970</xref>; <xref ref-type="bibr" rid="c18">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="c19">Crosse et al., 2021</xref>). The TRF model calculated for the training data was used to predict the EEG signal for the held-out test dataset. The Pearson correlation between the predicted and the observed EEG data of the test dataset was used as a measure of EEG prediction accuracy (<xref ref-type="bibr" rid="c18">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="c19">Crosse et al., 2021</xref>). Model estimation and prediction accuracy were calculated separately for each of the 50 data snippets per story, and prediction accuracies were averaged across the 50 snippets.</p>
<p>EEG prediction accuracy was calculated because many previous studies report it (e.g., <xref ref-type="bibr" rid="c22">Decruy et al., 2019</xref>; <xref ref-type="bibr" rid="c14">Broderick et al., 2021</xref>; <xref ref-type="bibr" rid="c35">Gillis et al., 2021</xref>; <xref ref-type="bibr" rid="c103">Weineck et al., 2022</xref>; <xref ref-type="bibr" rid="c53">Karunathilake et al., 2023</xref>), but the main focus of the current study is on the TRF weights/amplitude. That is, to investigate the neural-tracking response directly, we calculated TRFs for each training dataset for a broader set of lags, ranging from −0.15 to 0.5 s, to enable similar analyses as for traditional event-related potentials (<xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>). TRFs were averaged across the 50 training datasets and the mean in the time window −0.15 to 0 s was subtracted from the data at each time point (baseline correction).</p>
<p>Data analyses focused on a fronto-central electrode cluster (F3, Fz, F4, C3, Cz, C4) known to be sensitive to neural activity originating from auditory cortex (<xref ref-type="bibr" rid="c70">Näätänen and Picton, 1987</xref>; <xref ref-type="bibr" rid="c82">Picton et al., 2003</xref>; <xref ref-type="bibr" rid="c44">Herrmann et al., 2018</xref>; <xref ref-type="bibr" rid="c50">Irsik et al., 2021</xref>). Prediction accuracies and TRFs were averaged across the electrodes of this fronto-central electrode cluster prior to further analysis.</p>
<p>Analyses of the TRF focused on the P1-N1 and the P2-N1 amplitude differences. The amplitude of individual TRF components (P1, N1, P2) was not analyzed because the TRF time courses for the clear condition had an overall positive shift (see also <xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>) that could bias analyses more favorably towards response differences which may, however, be harder to interpret. The P1, N1, and P2 latencies were estimated from the averaged time courses across participants, separately for each SNR. P1, N1, and P2 amplitudes were calculated for each participant and condition as the mean amplitude in the 0.02 s time window centered on the peak latency. The P1-minus-N1 and P2-minus-N1 amplitude differences were calculated.</p>
</sec>
<sec id="s5h">
<title>Statistical analyses</title>
<p>All statistical analyses were carried out using MATLAB (MathWorks Inc., RRID:SCR_001622) and JASP software (<xref ref-type="bibr" rid="c51">JASP, 2023</xref>; version 0.18.3.0, RRID:SCR_015823).</p>
</sec>
<sec id="s5i">
<title>Experiment 1: Stimuli, procedures, and analyses</title>
<p>Participants listened to 24 stories in 6 blocks (4 stories per block). Three of the 24 stories were played under clear conditions (i.e., without background noise). Twelve-talker babble was added to the other 21 stories (<xref ref-type="bibr" rid="c9">Bilger, 1984</xref>; <xref ref-type="bibr" rid="c10">Bilger et al., 1984</xref>; <xref ref-type="bibr" rid="c107">Wilson et al., 2012b</xref>). Twelve-talker babble is a standardized masker in speech-in-noise tests (<xref ref-type="bibr" rid="c9">Bilger, 1984</xref>; <xref ref-type="bibr" rid="c10">Bilger et al., 1984</xref>) that simulates a crowded restaurant, while not permitting the identification of individual words in the masker (<xref ref-type="bibr" rid="c64">Mattys et al., 2012</xref>). The babble masker was added at SNRs ranging from +30 to –2 dB in 21 steps of 1.6 dB SNR. Speech in background babble at 15 to 30 dB SNR is highly intelligible (<xref ref-type="bibr" rid="c47">Holder et al., 2018</xref>; <xref ref-type="bibr" rid="c86">Rowland et al., 2018</xref>; <xref ref-type="bibr" rid="c91">Spyridakou et al., 2020</xref>; <xref ref-type="bibr" rid="c49">Irsik et al., 2022</xref>). No difference in speech intelligibility during story listening has been found between clear speech and speech masked by a 12-talker babble at +12 dB SNR (<xref ref-type="bibr" rid="c49">Irsik et al., 2022</xref>). Intelligibility typically drops below 90% of correctly reported words for +7 dB and lower SNR levels (<xref ref-type="bibr" rid="c49">Irsik et al., 2022</xref>). Hence, listeners have no trouble understanding speech at the highest SNRs used in the current study. All speech stimuli were normalized to the same root-mean-square amplitude and presented at about 65 dB SPL. Participants listened to each story, and after each story rated gist understanding and answered comprehension questions. Stories were presented in random order. Assignment of speech-clarity levels (clear speech and SNRs) to specific stories was randomized across participants.</p>
<p>Behavioral data (comprehension accuracy, gist ratings), EEG prediction accuracy, and TRFs for the three clear stories were averaged. For the stories in babble, a sliding average across SNRs was calculated for behavioral data, EEG prediction accuracy, and TRFs, such that data for three neighboring SNRs were averaged. Averaging across three stories was calculated to reduce noise in the data and match the averaging of three stories for the clear condition. For TRFs, analyses focused on the P1-N1 and the P2-N1 amplitude differences. For the statistical analyses, the clear condition was compared to each SNR (resulting from the sliding average) using a paired samples t-test. False discovery rate (FDR) was used to account for multiple comparisons (<xref ref-type="bibr" rid="c7">Benjamini and Hochberg, 1995</xref>; <xref ref-type="bibr" rid="c33">Genovese et al., 2002</xref>). In cases where the data indicated a breaking point in behavior or brain response as a function of SNR, an explorative piece-wise regression (broken-stick analysis) with two linear pieces was calculated (<xref ref-type="bibr" rid="c68">McZgee and Carleton, 1970</xref>; <xref ref-type="bibr" rid="c100">Vieth, 1989</xref>; <xref ref-type="bibr" rid="c96">Toms and Lesperance, 2003</xref>). Identification of the breaking point and two pieces was calculated on the across-participant average as the minimum root mean squared error. A linear function was then fit to each participant’s data as a function of SNR and the estimated slope was tested against zero using a one-sample t-test, separately for each of the two pieces.</p>
</sec>
<sec id="s5j">
<title>Experiment 2: Stimuli, procedures, and analyses</title>
<p>A new set of 24 stories was generated and participants were presented with 4 stories in each of 6 blocks. Speech-clarity levels were the same as in Experiment 1 (i.e., clear speech and SNRs ranging from +30 to –2 dB SNR). In Experiment 2, participants were instructed to ignore the stories and instead perform a visual 1-back task. In no part of the experiment were participants instructed to attend to the speech.</p>
<p>For the visual 1-back task, images of white digits (0 to 9) on black background were taken from the MNIST Handwritten Digit Classification Dataset (<xref ref-type="bibr" rid="c24">Deng, 2012</xref>). The digit images were selected, because different images of the same digit differ visually and thus make it challenging to use a simple feature-matching strategy to solve the 1-back task. A new digit image was presented every 0.5 seconds throughout the time over which a story was played (1:30 to 2:30 min). A digit image was presented for 0.25 s followed by a 0.25 s black screen before the next image was presented. The continuous stream of digits contained a digit repetition (albeit a different image sample) every 6 to 12 digits. Participants were tasked with pressing a button on a keyboard as soon as they detected a repetition. We did not include comprehension questions or gist ratings in Experiment 2 to avoid that participants feel they should pay attention to the speech materials. Hit rate and response time were used as behavioral measures.</p>
<p>Analyses examining the effects of SNR in Experiment 2 were similar to the analyses in Experiment 1. Behavioral data (hit rate, response time in 1-back task) and EEG data (prediction accuracy, TRFs) for the three clear stories were averaged and a sliding average procedure across SNRs (three neighbors) was used for stories in babble. Statistical tests compared SNR conditions to the clear condition, including FDR-thresholding. An explorative piece-wise regression with two linear pieces was calculated in cases where the data indicated a breaking point in behavior or brain response as a function of SNR. Data from five participants were excluded from analysis because they performed below 60% in the visual n-back task. A low performance could mean that the participants did not fully attend to the visual task and instead attended to the spoken speech. To avoid this possibility, data from low performers were excluded.</p>
</sec>
<sec id="s5k">
<title>Experiment 3: Stimuli, procedures, and analyses</title>
<p>A new set of 24 stories, comprehension questions, and multiple-choice options were generated. Participants listened to 4 stories in each of 6 blocks. Four of the 24 stories were presented under clear conditions. Ten stories were masked by 12-talker babble at 5 different SNRs (two stories each: +5, +10, +15, +20, +25 dB SNR), whereas the other 10 stories were masked by a stationary noise that spectrally matched the speech signal at 5 different SNRs (two stories each: +5, +10, +15, +20, +25 dB SNR). To obtain the spectrally matched noise, the long-term spectrum of the clear speech signal was calculated using a fast Fourier transform (FFT). The inverted FFT was calculated using the frequency-specific amplitudes estimated from the speech signal jointly with randomly selected phases for each frequency. <xref rid="fig4" ref-type="fig">Figure 4</xref> shows a time course snippet, a power spectrum, and a short snippet of the spectrogram for clear speech, speech masked by babble, and speech masked by the spectrally matched noise. Twelve-talker babble and speech-matched noise spectrally overlap extensively (<xref rid="fig4" ref-type="fig">Figure 4</xref>), but 12-talker babble recognizably contains speech (although individual elements cannot be identified), whereas the speech-matched noise does not contain recognizable speech elements.</p>
<p>For five of the 10 stories per masker type (babble, noise), the speech-plus-masker sound signal (mixed at a specific SNR) was normalized to the same root-mean-square (RMS) amplitude as the clear speech stories. This normalization results in a decreasing level of the speech signal in the speech-plus-masker mixture as SNR decreases. For the other five stories, the RMS amplitude of the speech signal was kept the same for all stories and a masker was added at the specific SNRs. This normalization results in an increasing sound level (RMS) of the sound mixture as SNR decreases. In other words, the speech signal in the sound mixture is played at a slightly lower intensity for the former than for the latter normalization type. We thus refer to this manipulation as speech with ‘lower’ and ‘higher’ intensities for the two normalization types, respectively. Note that these differences in speech level are very minor due to the high SNRs used here. Stories were presented in randomized order and the assignment of stories to conditions was randomized across participants.</p>
<p>Behavioral data (comprehension accuracy, gist ratings), EEG prediction accuracy, and TRFs for the four clear stories were averaged. For the stories in babble and speech-matched noise, a sliding average across SNRs was calculated for behavioral data, EEG prediction accuracy, and TRFs, such that data for four neighboring SNR levels were averaged, separately for the two masker types (babble, noise) and normalization types (adjusted speech level, non-adjusted speech level). Averaging across four stories was calculated to reduce noise in the data and match the number of stories included in the average for the clear condition. For TRFs, analyses focused on the P1-N1 and the P2-N1 amplitude differences. For the statistical analyses, the clear condition was compared to each SNR level (resulting from the sliding average) using a paired samples t-test. False discovery rate (FDR) was used to account for multiple comparisons (<xref ref-type="bibr" rid="c7">Benjamini and Hochberg, 1995</xref>; <xref ref-type="bibr" rid="c33">Genovese et al., 2002</xref>). Differences between masker types and normalization types were examined using a repeated measures analysis of variance (rmANOVA) with the within-participant factors SNR (10, 15, 20 dB SNR), Masker Type (babble, speech-match noise), and Normalization Type (lower vs higher speech levels). Post hoc tests were calculated for significant main effects and interactions. Holm’s methods was used to correct for multiple comparisons (<xref ref-type="bibr" rid="c48">Holm, 1979</xref>). Statistical analyses were carried out in JASP software (v0.18.3; <xref ref-type="bibr" rid="c51">JASP, 2023</xref>, RRID:SCR_015823). Note that JASP uses pooled error terms and degrees of freedom from an rmANOVA for the corresponding post hoc effects. The reported degrees of freedom are thus higher than for direct contrasts had they been calculated independently from the rmANOVA.</p>
</sec>
<sec id="s5l">
<title>Experiment 4: Stimuli, procedures, and analyses</title>
<p>A new set of 24 stories, corresponding comprehension questions, and multiple-choice options were generated. Participants listened to 4 stories in each of 6 blocks. After each story, they answered 5 comprehension questions and rated gist understanding. Three stories were presented in each of 8 conditions: clear speech (no masker), speech with added white noise, pink noise, stationary noise that spectrally matched the speech signal (Experiment 3), 12-talker babble (Experiments 1-3), and three additional 12-talker babbles. The additional babble maskers were created to ensure there is nothing specific about the babble masker used in our previous work (<xref ref-type="bibr" rid="c108">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Panela et al., 2024</xref>) and in Experiments 1-3 that could lead to an enhanced tracking response and to vary spectral properties of the babble associated with different voice genders (male, female). For the three additional babble maskers, 24 text excerpts of about 800 words each were taken from Wikipedia (e.g., about flowers, forests, insurance, etc.). The 24 text excerpts were fed into Google’s AI speech synthesizer to generate 24 continuous speech materials (∼5 min) of which 12 were from male voices and 12 from female voices. The three 12-talker babble maskers were created by adding speech from 6 male and 6 female voices (mixed gender 12-talker babble), speech from the 12 male voices (male gender 12-talker babble), and speech from the 12 female voices (female gender 12-talker babble). Maskers were added to the speech signal at 20 dB SNR and all acoustic stimuli were normalized to the same root-mean-square amplitude. A power spectrum for each of the 7 masker types is displayed in <xref rid="fig5" ref-type="fig">Figure 5</xref>. Stories were presented in randomized order and the assignment of stories to the 8 different conditions was randomized across participants.</p>
<p>Behavioral data (comprehension accuracy, gist ratings), EEG prediction accuracy, and TRFs were averaged across the three stories for each condition. For TRFs, analyses focused on the P1-N1 and the P2-N1 amplitude differences. For the statistical analyses, the clear condition was compared to the masker conditions using a paired samples t-test. FDR was used to account for multiple comparisons (<xref ref-type="bibr" rid="c7">Benjamini and Hochberg, 1995</xref>; <xref ref-type="bibr" rid="c33">Genovese et al., 2002</xref>). Differences between the four different babble maskers, between babble and noise maskers, and between the three noise maskers were also investigated using paired samples t-tests.</p>
</sec>
<sec id="s5m">
<title>Experiment 5: Stimuli, procedures, and analyses</title>
<p>A new set of 24 stories, comprehension questions, and multiple-choice options were generated (due to a mistake 8 stories from Experiments 1-4 were eventually used in Experiment 5; and 16 new stories). Participants listened to 4 stories in each of 6 blocks. After each story, they answered 5 comprehension questions and rated gist understanding. For 3 of the 6 blocks, participants listened to the stories through headphones, as for Experiments 1-4, whereas for the other 3 blocks, participants listened to the stories via computer loudspeakers placed in front of them. Blocks for different sound-delivery conditions (headphones, loudspeakers) alternated within each participant’s session, and the starting condition was counter-balanced across participants. For each sound-delivery condition, participants listened to three stories each under clear conditions, +10 dB, +15 dB, and +20 dB SNR (12 talker babble, generated using Google’s AI voices as described for Experiment 4). Stories were distributed such that the four speech-clarity conditions were presented in each block in randomized order. All acoustic stimuli were normalized to the same root-mean-square amplitude. The sound level of the headphones and the sound level of the loudspeakers (at the location of a participant’s head) were matched.</p>
<p>Behavioral data (comprehension accuracy, gist ratings), EEG prediction accuracy, and TRFs were averaged across the three stories for each speech-clarity and sound-delivery condition. For TRFs, analyses focused on the P1-N1 and the P2-N1 amplitude differences. For the statistical analyses, the clear condition was compared to the masker conditions using a paired samples t-test. FDR was used to account for multiple comparisons (<xref ref-type="bibr" rid="c7">Benjamini and Hochberg, 1995</xref>; <xref ref-type="bibr" rid="c33">Genovese et al., 2002</xref>). To test for differences between sound-delivery types, a rmANOVA was calculated, using the within-participant factors SNR (10, 15, 20 dB SNR; clear speech was not included, because a difference to clear speech was tested directly as just described) and Sound Delivery (headphones, loudspeakers). Post hoc tests were calculated using dependent samples t-tests, and Holm’s methods was used to correct for multiple comparisons (<xref ref-type="bibr" rid="c48">Holm, 1979</xref>).</p>
</sec>
</sec>
</body>
<back>
<sec id="s6">
<title>Materials availability statement</title>
<p>Data and analysis code are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/zs9u5/">https://osf.io/zs9u5/</ext-link>.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank Priya Pandey, Tiffany Lao, and Saba Junaid for their help with data collection. The research was supported by the Canada Research Chair program (CRC-2019-00156) and the Natural Sciences and Engineering Research Council of Canada (Discovery Grant: RGPIN-2021-02602).</p>
</ack>
<sec id="d1e1758" sec-type="additional-information">
<title>Additional information</title>
<sec id="s7">
<title>Authors contributions</title>
<p><bold>Björn Herrmann:</bold> Conceptualization, methodology, formal analysis, investigation, data curation, writing - original draft, writing - review and editing, visualization, supervision, project administration, funding acquisition.</p>
</sec>
</sec>
<sec id="suppd1e1758" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e1749">
<label>Supplementary Figures</label>
<media xlink:href="supplements/599692_file02.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alain</surname> <given-names>C</given-names></string-name>, <string-name><surname>McDonald</surname> <given-names>K</given-names></string-name>, <string-name><surname>Van Roon</surname> <given-names>P</given-names></string-name></person-group> (<year>2012</year>) <article-title>Effects of age and background noise on processing a mistuned harmonic in an otherwise periodic complex sound</article-title>. <source>Hearing Research</source> <volume>283</volume>:<fpage>126</fpage>–<lpage>135</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alain</surname> <given-names>C</given-names></string-name>, <string-name><surname>Roye</surname> <given-names>A</given-names></string-name>, <string-name><surname>Salloum</surname> <given-names>C</given-names></string-name></person-group> (<year>2014</year>) <article-title>Effects of age-related hearing loss and background noise on neuromagnetic activity from auditory cortex</article-title>. <source>Frontiers in Systems Neuroscience</source> <volume>8</volume>:<elocation-id>8</elocation-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alain</surname> <given-names>C</given-names></string-name>, <string-name><surname>Quan</surname> <given-names>J</given-names></string-name>, <string-name><surname>McDonald</surname> <given-names>K</given-names></string-name>, <string-name><surname>Van Roon</surname> <given-names>P</given-names></string-name></person-group> (<year>2009</year>) <article-title>Noise-induced increase in human auditory evoked neuromagnetic fields</article-title>. <source>European Journal of Neuroscience</source> <volume>30</volume>:<fpage>132</fpage>–<lpage>142</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alvar</surname> <given-names>A</given-names></string-name>, <string-name><surname>Francis</surname> <given-names>AL</given-names></string-name></person-group> (<year>2024</year>) <article-title>Effects of background noise on autonomic arousal (skin conductance level)</article-title>. <source>JASA Express Letters</source> <volume>4</volume>:<fpage>013601</fpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Auerbach</surname> <given-names>BD</given-names></string-name>, <string-name><surname>Rodrigues</surname> <given-names>PV</given-names></string-name>, <string-name><surname>Salvi</surname> <given-names>RJ</given-names></string-name></person-group> (<year>2014</year>) <article-title>Central gain control in tinnitus and hyperacusis</article-title>. <source>Frontiers in Neurology</source> <volume>5</volume>:<elocation-id>206</elocation-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bell</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Sejnowski</surname> <given-names>TJ</given-names></string-name></person-group> (<year>1995</year>) <article-title>An information maximization approach to blind separation and blind deconvolution</article-title>. <source>Neural Computation</source> <volume>7</volume>:<fpage>1129</fpage>–<lpage>1159</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benjamini</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Hochberg</surname> <given-names>Y</given-names></string-name></person-group> (<year>1995</year>) <article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source>Journal of the Royal Statistical Society Series B</source> <volume>57</volume>:<fpage>289</fpage>–<lpage>300</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Biesmans</surname> <given-names>W</given-names></string-name>, <string-name><surname>Das</surname> <given-names>N</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name>, <string-name><surname>Bertrand</surname> <given-names>A</given-names></string-name></person-group> (<year>2017</year>) <article-title>Auditory-Inspired Speech Envelope Extraction Methods for Improved EEG-Based Auditory Attention Detection in a Cocktail Party Scenario</article-title>. <source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source> <volume>25</volume>:<fpage>402</fpage>–<lpage>412</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Bilger</surname> <given-names>RC</given-names></string-name></person-group> (<year>1984</year>) <source>Manual for the clinical use of the revised SPIN Test</source>. <publisher-loc>Champaign, IL, USA</publisher-loc>: <publisher-name>The University of Illinois</publisher-name>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bilger</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Nuetzel</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Rabinowitz</surname> <given-names>WM</given-names></string-name>, <string-name><surname>Rzeczkowski</surname> <given-names>C</given-names></string-name></person-group> (<year>1984</year>) <article-title>Standardization of a Test of Speech Perception in Noise</article-title>. <source>Journal of Speech, Language, and Hearing Research</source> <volume>27</volume>:<fpage>32</fpage>–<lpage>48</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brodbeck</surname> <given-names>C</given-names></string-name>, <string-name><surname>Hong</surname> <given-names>LE</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name></person-group> (<year>2018a</year>) <article-title>Rapid Transformation from Auditory to Linguistic Representations of Continuous Speech</article-title>. <source>Current Biology</source> <volume>28</volume>:<fpage>3976</fpage>–<lpage>3983.</lpage> </mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brodbeck</surname> <given-names>C</given-names></string-name>, <string-name><surname>Presacco</surname> <given-names>A</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>S</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name></person-group> (<year>2018b</year>) <article-title>Over-representation of speech in older adults originates from early response in higher order auditory cortex</article-title>. <source>Acta Acust United Acust</source> <volume>104</volume>:<fpage>774</fpage>–<lpage>777</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Broderick</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Crosse</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name></person-group> (<year>2018</year>) <article-title>Electrophysiological Correlates of Semantic Dissimilarity Reflect the Comprehension of Natural, Narrative Speech</article-title>. <source>Current Biology</source> <volume>28</volume>:<fpage>803</fpage>–<lpage>809</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Broderick</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Rofes</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name></person-group> (<year>2021</year>) <article-title>Dissociable electrophysiological measures of natural language processing reveal differences in speech comprehension strategy in healthy ageing</article-title>. <source>Scientific Reports</source> <volume>11</volume>:<fpage>4963</fpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brungart</surname> <given-names>DS</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>PS</given-names></string-name>, <string-name><surname>Simpson</surname> <given-names>BD</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>D</given-names></string-name></person-group> (<year>2006</year>) <article-title>Isolating the energetic component of speech-on-speech masking with ideal time-frequency segregation</article-title>. <source>The Journal of the Acoustical Society of America</source> <volume>120</volume>:<fpage>4007</fpage>–<lpage>4018</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burlingham</surname> <given-names>CS</given-names></string-name>, <string-name><surname>Mirbagheri</surname> <given-names>S</given-names></string-name>, <string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name></person-group> (<year>2022</year>) <article-title>A unified model of the task-evoked pupil response</article-title>. <source>Science Advances</source> <volume>8</volume>:<fpage>eabi9979</fpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname> <given-names>SS</given-names></string-name>, <string-name><surname>Parra</surname> <given-names>LC</given-names></string-name></person-group> (<year>2016</year>) <article-title>Memorable Audiovisual Narratives Synchronize Sensory and Supramodal Neural Responses</article-title>. <source>eNeuro</source> <volume>3</volume>:<fpage>e0203</fpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crosse</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Bednar</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name></person-group> (<year>2016</year>) <article-title>The Multivariate Temporal Response Function (mTRF) Toolbox: A MATLAB Toolbox for Relating Neural Signals to Continuous Stimuli</article-title>. <source>Frontiers in human neuroscience</source> <volume>10</volume>:<fpage>604</fpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crosse</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Zuk</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Nidiffer</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Molholm</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name></person-group> (<year>2021</year>) <article-title>Linear Modeling of Neurophysiological Responses to Speech and Other Continuous Stimuli: Methodological Considerations for Applied Research</article-title>. <source>Frontiers in Neuroscience</source> <volume>15</volume>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Daube</surname> <given-names>C</given-names></string-name>, <string-name><surname>Ince</surname> <given-names>RAA</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>J</given-names></string-name></person-group> (<year>2019</year>) <article-title>Simple Acoustic Features Can Explain Phoneme-Based Predictions of Cortical Responses to Speech</article-title>. <source>Current Biology</source> <volume>29</volume>:<fpage>1924</fpage>–<lpage>1937</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Davis</surname> <given-names>MH</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name></person-group> (<year>2003</year>) <article-title>Hierarchical Processing in Spoken Language Comprehension</article-title>. <source>The Journal of Neuroscience</source> <volume>23</volume>:<fpage>3423</fpage>–<lpage>3431</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Decruy</surname> <given-names>L</given-names></string-name>, <string-name><surname>Vanthornhout</surname> <given-names>J</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name></person-group> (<year>2019</year>) <article-title>Evidence for enhanced neural tracking of the speech envelope underlying age-related speech-in-noise difficulties</article-title>. <source>Journal of Neurophysiology</source> <volume>122</volume>:<fpage>601</fpage>–<lpage>615</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Decruy</surname> <given-names>L</given-names></string-name>, <string-name><surname>Vanthornhout</surname> <given-names>J</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name></person-group> (<year>2020</year>) <article-title>Hearing impairment is associated with enhanced neural tracking of the speech envelope</article-title>. <source>Hearing Research</source> <volume>393</volume>:<fpage>107961</fpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deng</surname> <given-names>L</given-names></string-name></person-group> (<year>2012</year>) <article-title>The mnist database of handwritten digit images for machine learning research</article-title>. <source>IEEE Signal Processing Magazine</source> <volume>29</volume>:<fpage>141</fpage>–<lpage>142</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di Liberto Giovanni</surname> <given-names>M</given-names></string-name>, <string-name><surname>O’Sullivan James</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lalor Edmund</surname> <given-names>C</given-names></string-name></person-group> (<year>2015</year>) <article-title>Low-Frequency Cortical Entrainment to Speech Reflects Phoneme-Level Processing</article-title>. <source>Current Biology</source> <volume>25</volume>:<fpage>2457</fpage>–<lpage>2465</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dial</surname> <given-names>HR</given-names></string-name>, <string-name><surname>Gnanateja</surname> <given-names>GN</given-names></string-name>, <string-name><surname>Tessmer</surname> <given-names>RS</given-names></string-name>, <string-name><surname>Gorno-Tempini</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Chandrasekaran</surname> <given-names>B</given-names></string-name>, <string-name><surname>Henry</surname> <given-names>ML</given-names></string-name></person-group> (<year>2021</year>) <article-title>Cortical Tracking of the Speech Envelope in Logopenic Variant Primary Progressive Aphasia</article-title>. <source>Frontiers in Human Neuroscience</source> <volume>14</volume>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ding</surname> <given-names>N</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name></person-group> (<year>2013</year>) <article-title>Adaptive temporal encoding leads to a background-insensitive cortical representation of speech</article-title>. <source>The Journal of Neuroscience</source> <volume>33</volume>:<fpage>5728</fpage>–<lpage>5735</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ding</surname> <given-names>N</given-names></string-name>, <string-name><surname>Chatterjee</surname> <given-names>M</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name></person-group> (<year>2014</year>) <article-title>Robust cortical entrainment to the speech envelope relies on the spectro-temporal fine structure</article-title>. <source>NeuroImage</source> <volume>88</volume>:<fpage>41</fpage>–<lpage>46</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dmochowski</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Sajda</surname> <given-names>P</given-names></string-name>, <string-name><surname>Dias</surname> <given-names>J</given-names></string-name>, <string-name><surname>Parra</surname> <given-names>LC</given-names></string-name></person-group> (<year>2012</year>) <article-title>Correlated components of ongoing EEG point to emotionally laden attention – a possible marker of engagement?</article-title> <source>Frontiers in Human Neuroscience</source> <volume>6</volume>:<elocation-id>112</elocation-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dmochowski</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Bezdek</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Abelson</surname> <given-names>BP</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Schumacher</surname> <given-names>EH</given-names></string-name>, <string-name><surname>Parra</surname> <given-names>LC</given-names></string-name></person-group> (<year>2014</year>) <article-title>Audience preferences are predicted by temporal reliability of neural processing</article-title>. <source>Nature Communications</source> <volume>29</volume>:<fpage>4567</fpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fiedler</surname> <given-names>L</given-names></string-name>, <string-name><surname>Wöstmann</surname> <given-names>M</given-names></string-name>, <string-name><surname>Herbst</surname> <given-names>SK</given-names></string-name>, <string-name><surname>Obleser</surname> <given-names>J</given-names></string-name></person-group> (<year>2019</year>) <article-title>Late cortical tracking of ignored speech facilitates neural selectivity in acoustically challenging conditions</article-title>. <source>Neuroimage</source> <volume>186</volume>:<fpage>33</fpage>–<lpage>42</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fiedler</surname> <given-names>L</given-names></string-name>, <string-name><surname>Wöstmann</surname> <given-names>M</given-names></string-name>, <string-name><surname>Graversen</surname> <given-names>C</given-names></string-name>, <string-name><surname>Brandmeyer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lunner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Obleser</surname> <given-names>J</given-names></string-name></person-group> (<year>2017</year>) <article-title>Single-channel in-ear-EEG detects the focus of auditory attention to concurrent tone streams and mixed speech</article-title>. <source>Journal of Neural Engineering</source> <volume>14</volume>:<fpage>036020</fpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Genovese</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Lazar</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Nichols</surname> <given-names>T</given-names></string-name></person-group> (<year>2002</year>) <article-title>Thresholding of statistical maps in functional neuroimaging using the false discovery rate</article-title>. <source>NeuroImage</source> <volume>15</volume>:<fpage>870</fpage>–<lpage>878</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gillis</surname> <given-names>M</given-names></string-name>, <string-name><surname>Van Canneyt</surname> <given-names>J</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name>, <string-name><surname>Vanthornhout</surname> <given-names>J</given-names></string-name></person-group> (<year>2022</year>) <article-title>Neural tracking as a diagnostic tool to assess the auditory pathway</article-title>. <source>Hearing Research</source> <volume>426</volume>:<fpage>108607</fpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gillis</surname> <given-names>M</given-names></string-name>, <string-name><surname>Vanthornhout</surname> <given-names>J</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name>, <string-name><surname>Brodbeck</surname> <given-names>C</given-names></string-name></person-group> (<year>2021</year>) <article-title>Neural Markers of Speech Comprehension: Measuring EEG Tracking of Linguistic Speech Representations, Controlling the Speech Acoustics</article-title>. <source>The Journal of Neuroscience</source> <volume>41</volume>:<fpage>10316</fpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glasberg</surname> <given-names>BR</given-names></string-name>, <string-name><surname>Moore</surname> <given-names>BCJ</given-names></string-name></person-group> (<year>1990</year>) <article-title>Derivation of auditory filter shapes from notched-noise data</article-title>. <source>Hearing Research</source> <volume>47</volume>:<fpage>103</fpage>–<lpage>138</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Han</surname> <given-names>L</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Jin</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Luo</surname> <given-names>Y</given-names></string-name></person-group> (<year>2013</year>) <article-title>Low-Arousal Speech Noise Improves Performance in N-Back Task: An ERP Study</article-title>. <source>PLOS One</source> <volume>8</volume>:<fpage>e76261</fpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hauswald</surname> <given-names>A</given-names></string-name>, <string-name><surname>Keitel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>Y-P</given-names></string-name>, <string-name><surname>Rösch</surname> <given-names>S</given-names></string-name>, <string-name><surname>Weisz</surname> <given-names>N</given-names></string-name></person-group> (<year>2022</year>) <article-title>Degradation levels of continuous speech affect neural speech tracking and alpha power differently</article-title>. <source>European Journal of Neuroscience</source> <volume>55</volume>:<fpage>3288</fpage>–<lpage>3302</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heffernan</surname> <given-names>E</given-names></string-name>, <string-name><surname>Coulson</surname> <given-names>NS</given-names></string-name>, <string-name><surname>Henshaw</surname> <given-names>H</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>JG</given-names></string-name>, <string-name><surname>Ferguson</surname> <given-names>MA</given-names></string-name></person-group> (<year>2016</year>) <article-title>Understanding the psychosocial experiences of adults with mild-moderate hearing loss: An application of Leventhal’s self-regulatory model</article-title>. <source>International Journal of Audiology</source> <volume>55</volume>:<fpage>S3</fpage>–<lpage>S12</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name></person-group> (<year>2023</year>) <article-title>The perception of artificial-intelligence (AI) based synthesized speech in younger and older adults</article-title>. <source>International Journal of Speech Technology</source> <volume>26</volume>:<fpage>395</fpage>–<lpage>415</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name></person-group> (<year>2020</year>) <article-title>A Model of Listening Engagement (MoLE)</article-title>. <source>Hearing Research</source> <volume>397</volume>:<fpage>108016</fpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name><surname>Butler</surname> <given-names>BE</given-names></string-name></person-group> (<year>2021</year>) <article-title>Hearing Loss and Brain Plasticity: The Hyperactivity Phenomenon</article-title>. <source>Brain Structure &amp; Function</source> <volume>226</volume>:<fpage>2019</fpage>–<lpage>2039</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name><surname>Henry</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Obleser</surname> <given-names>J</given-names></string-name></person-group> (<year>2013</year>) <article-title>Frequency-specific adaptation in human auditory cortex depends on the spectral variance in the acoustic stimulation</article-title>. <source>Journal of Neurophysiology</source> <volume>109</volume>:<fpage>2086</fpage>–<lpage>2096</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name><surname>Maess</surname> <given-names>B</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name></person-group> (<year>2018</year>) <article-title>Aging Affects Adaptation to Sound-Level Statistics in Human Auditory Cortex</article-title>. <source>The Journal of Neuroscience</source> <volume>38</volume>:<fpage>1989</fpage>–<lpage>1999</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hertrich</surname> <given-names>I</given-names></string-name>, <string-name><surname>Dietrich</surname> <given-names>S</given-names></string-name>, <string-name><surname>Trouvain</surname> <given-names>J</given-names></string-name>, <string-name><surname>Moos</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ackermann</surname> <given-names>H</given-names></string-name></person-group> (<year>2012</year>) <article-title>Magnetic brain activity phase-locked to the envelope, the syllable onsets, and the fundamental frequency of a perceived speech signal</article-title>. <source>Psychophysiology</source> <volume>49</volume>:<fpage>322</fpage>–<lpage>334</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoerl</surname> <given-names>AE</given-names></string-name>, <string-name><surname>Kennard</surname> <given-names>RW</given-names></string-name></person-group> (<year>1970</year>) <article-title>Ridge Regression: Biased Estimation for Nonorthogonal Problems</article-title>. <source>Technometrics</source> <volume>12</volume>:<fpage>55</fpage>–<lpage>67</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holder</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Levin</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Gifford</surname> <given-names>RH</given-names></string-name></person-group> (<year>2018</year>) <article-title>Speech Recognition in Noise for Adults With Normal Hearing: Age-Normative Performance for AzBio, BKB-SIN, and QuickSIN</article-title>. <source>Otology &amp; Neurotology</source> <volume>39</volume>:<fpage>e972</fpage>–<lpage>e978</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holm</surname> <given-names>S</given-names></string-name></person-group> (<year>1979</year>) <article-title>A Simple Sequentially Rejective Multiple Test Procedure</article-title>. <source>Scandinavian Journal of Statistics</source> <volume>6</volume>:<fpage>65</fpage>–<lpage>70</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Irsik</surname> <given-names>VC</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name>, <string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name></person-group> (<year>2022</year>) <article-title>Neural activity during story listening is synchronized across individuals despite acoustic masking</article-title>. <source>Journal of Cognitive Neuroscience</source> <volume>34</volume>:<fpage>933</fpage>–<lpage>950</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Irsik</surname> <given-names>VC</given-names></string-name>, <string-name><surname>Almanaseer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name>, <string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name></person-group> (<year>2021</year>) <article-title>Cortical Responses to the Amplitude Envelopes of Sounds Change with Age</article-title>. <source>The Journal of Neuroscience</source> <volume>41</volume>:<fpage>5045</fpage>–<lpage>5055</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="software"><person-group person-group-type="author"><collab>JASP</collab></person-group> (<year>2023</year>) <source>JASP</source> [Computer software]. <ext-link ext-link-type="uri" xlink:href="https://jasp-stats.org/">https://jasp-stats.org/</ext-link>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Joshi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gold</surname> <given-names>JI</given-names></string-name></person-group> (<year>2020</year>) <article-title>Pupil Size as a Window on Neural Substrates of Cognition</article-title>. <source>Trends in Cognitive Sciences</source> <volume>24</volume>:<fpage>466</fpage>–<lpage>480</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Karunathilake</surname> <given-names>IMD</given-names></string-name>, <string-name><surname>Dunlap</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Perera</surname> <given-names>J</given-names></string-name>, <string-name><surname>Presacco</surname> <given-names>A</given-names></string-name>, <string-name><surname>Decruy</surname> <given-names>L</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kuchinsky</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name></person-group> (<year>2023</year>) <article-title>Effects of aging on cortical representations of continuous speech</article-title>. <source>Journal of Neurophysiology</source> <volume>129</volume>:<fpage>1359</fpage>–<lpage>1377</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kidd</surname> <given-names>G</given-names>, <suffix>Jr</suffix></string-name>., <string-name><surname>Mason</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Best</surname> <given-names>V</given-names></string-name>, <string-name><surname>Roverud</surname> <given-names>E</given-names></string-name>, <string-name><surname>Swaminathan</surname> <given-names>J</given-names></string-name>, <string-name><surname>Jennings</surname> <given-names>T</given-names></string-name>, <string-name><surname>Clayton</surname> <given-names>K</given-names></string-name>, <string-name><surname>Steven Colburn</surname> <given-names>H</given-names></string-name></person-group> (<year>2019</year>) <article-title>Determining the energetic and informational components of speech-on-speech masking in listeners with sensorineural hearing loss</article-title>. <source>The Journal of the Acoustical Society of America</source> <volume>145</volume>:<fpage>440</fpage>–<lpage>457</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kitajo</surname> <given-names>K</given-names></string-name>, <string-name><surname>Nozaki</surname> <given-names>D</given-names></string-name>, <string-name><surname>Ward</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Yamamoto</surname> <given-names>Y</given-names></string-name></person-group> (<year>2003</year>) <article-title>Behavioral Stochastic Resonance within the Human Brain</article-title>. <source>Physical Review Letters</source> <volume>90</volume>:<fpage>218103</fpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kitajo</surname> <given-names>K</given-names></string-name>, <string-name><surname>Doesburg</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Yamanaka</surname> <given-names>K</given-names></string-name>, <string-name><surname>Nazaki</surname> <given-names>D</given-names></string-name>, <string-name><surname>Ward</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Yamamoto</surname> <given-names>Y</given-names></string-name></person-group> (<year>2007</year>) <article-title>Noise-induced large-scale phase synchronization of human-brain activity associated with behavioural stochastic resonance</article-title>. <source>Europhysics Letters</source> <volume>80</volume>:<fpage>40009</fpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krauss</surname> <given-names>P</given-names></string-name>, <string-name><surname>Tziridis</surname> <given-names>K</given-names></string-name>, <string-name><surname>Metzner</surname> <given-names>C</given-names></string-name>, <string-name><surname>Schilling</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hoppe</surname> <given-names>U</given-names></string-name>, <string-name><surname>Schulze</surname> <given-names>H</given-names></string-name></person-group> (<year>2016</year>) <article-title>Stochastic Resonance Controlled Upregulation of Internal Noise after Hearing Loss as a Putative Cause of Tinnitus-Related Neuronal Hyperactivity</article-title>. <source>Frontiers in Neuroscience</source> <volume>10</volume>:<elocation-id>597</elocation-id>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kries</surname> <given-names>J</given-names></string-name>, <string-name><surname>De Clercq</surname> <given-names>P</given-names></string-name>, <string-name><surname>Gillis</surname> <given-names>M</given-names></string-name>, <string-name><surname>Vanthornhout</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lemmens</surname> <given-names>R</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name>, <string-name><surname>Vandermosten</surname> <given-names>M</given-names></string-name></person-group> (<year>2024</year>) <article-title>Exploring neural tracking of acoustic and linguistic speech representations in individuals with post-stroke aphasia</article-title>. <source>Human Brain Mapping</source> <volume>45</volume>:<fpage>e26676</fpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lesenfants</surname> <given-names>D</given-names></string-name>, <string-name><surname>Vanthornhout</surname> <given-names>J</given-names></string-name>, <string-name><surname>Verschueren</surname> <given-names>E</given-names></string-name>, <string-name><surname>Decruy</surname> <given-names>L</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name></person-group> (<year>2019</year>) <article-title>Predicting individual speech intelligibility from the cortical tracking of acoustic- and phonetic-level speech representations</article-title>. <source>Hearing Research</source> <volume>380</volume>:<fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname> <given-names>FR</given-names></string-name>, <string-name><surname>Albert</surname> <given-names>M</given-names></string-name></person-group> (<year>2014</year>) <article-title>Hearing loss and dementia – who is listening?</article-title> <source>Aging &amp; Mental Health</source> <volume>18</volume>:<fpage>671</fpage>–<lpage>673</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Makeig</surname> <given-names>S</given-names></string-name>, <string-name><surname>Bell</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Jung</surname> <given-names>T-P</given-names></string-name>, <string-name><surname>Sejnowski</surname> <given-names>TJ</given-names></string-name></person-group> (<year>1995</year>) <article-title>Independent component analysis of electroencephalographic data</article-title>. In: <conf-name>Advances in Neural Information Processing Systems</conf-name> (<person-group person-group-type="editor"><string-name><surname>Touretzky</surname> <given-names>D</given-names></string-name>, <string-name><surname>Mozer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hasselmo</surname> <given-names>M</given-names></string-name></person-group>, eds), pp <fpage>145</fpage>–<lpage>151</lpage>. <publisher-loc>Cambridge, MA, USA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathiesen</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Van Hedger</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Irsik</surname> <given-names>VC</given-names></string-name>, <string-name><surname>Bain</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name>, <string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name></person-group> (<year>2024</year>) <article-title>Exploring age differences in absorption and enjoyment during story listening</article-title>. <source>Psychology International</source> <volume>6</volume>:<fpage>667</fpage>–<lpage>684</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathôt</surname> <given-names>S</given-names></string-name></person-group> (<year>2018</year>) <article-title>Pupillometry: Psychology, physiology, and function</article-title>. <source>Journal of Cognition</source> <volume>1</volume>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mattys</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Davis</surname> <given-names>MH</given-names></string-name>, <string-name><surname>Bradlow</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Scott</surname> <given-names>SK</given-names></string-name></person-group> (<year>2012</year>) <article-title>Speech recognition in adverse conditions: A review</article-title>. <source>Language and Cognitive Processes</source> <volume>27</volume>:<fpage>953</fpage>–<lpage>978</lpage>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDermott Josh</surname> <given-names>H</given-names></string-name>, <string-name><surname>Simoncelli Eero</surname> <given-names>P</given-names></string-name></person-group> (<year>2011</year>) <article-title>Sound Texture Perception via Statistics of the Auditory Periphery: Evidence from Sound Synthesis</article-title>. <source>Neuron</source> <volume>71</volume>:<fpage>926</fpage>–<lpage>940</lpage>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDonnell</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Abbott</surname> <given-names>D</given-names></string-name></person-group> (<year>2009</year>) <article-title>What Is Stochastic Resonance? Definitions, Misconceptions, Debates, and Its Relevance to Biology</article-title>. <source>PLOS Computational Biology</source> <volume>5</volume>:<fpage>e1000348</fpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDonnell</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Ward</surname> <given-names>LM</given-names></string-name></person-group> (<year>2011</year>) <article-title>The benefits of noise in neural systems: bridging theory and experiment</article-title>. <source>Nature Reviews Neuroscience</source> <volume>12</volume>:<fpage>415</fpage>–<lpage>425</lpage>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McZgee</surname> <given-names>VE</given-names></string-name>, <string-name><surname>Carleton</surname> <given-names>WT</given-names></string-name></person-group> (<year>1970</year>) <article-title>Piecewise Regression</article-title>. <source>Journal of the American Statistical Association</source> <volume>65</volume>:<fpage>1109</fpage>–<lpage>1124</lpage>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moss</surname> <given-names>F</given-names></string-name>, <string-name><surname>Ward</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Sannita</surname> <given-names>WG</given-names></string-name></person-group> (<year>2004</year>) <article-title>Stochastic resonance and sensory information processing: a tutorial and review of application</article-title>. <source>Clinical Neurophysiology</source> <volume>115</volume>:<fpage>267</fpage>–<lpage>281</lpage>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Näätänen</surname> <given-names>R</given-names></string-name>, <string-name><surname>Picton</surname> <given-names>TW</given-names></string-name></person-group> (<year>1987</year>) <article-title>The N1 wave of the human electric and magnetic response to sound: a review and an analysis of the component structure</article-title>. <source>Psychophysiology</source> <volume>24</volume>:<fpage>375</fpage>–<lpage>425</lpage>.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nachtegaal</surname> <given-names>J</given-names></string-name>, <string-name><surname>Smit</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Smits</surname> <given-names>C</given-names></string-name>, <string-name><surname>Bezemer</surname> <given-names>PD</given-names></string-name>, <string-name><surname>van Beek</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Festen</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Kramer</surname> <given-names>SE</given-names></string-name></person-group> (<year>2009</year>) <article-title>The association between hearing status and psychosocial health before the age of 70 years: results from an internet-based national survey on hearing</article-title>. <source>Ear &amp; Hearing</source> <volume>30</volume>:<fpage>302</fpage>–<lpage>312</lpage>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Niedermeyer</surname> <given-names>E</given-names></string-name>, <string-name><surname>da Silva</surname> <given-names>FHL</given-names></string-name></person-group> (<year>2005</year>) <source>Electroencephalography: Basic Principles, Clinical Applications, and Related Fields</source>: <publisher-name>Lippincott Williams &amp; Wilkins</publisher-name>.</mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ohlenforst</surname> <given-names>B</given-names></string-name>, <string-name><surname>Wendt</surname> <given-names>D</given-names></string-name>, <string-name><surname>Kramer</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Naylor</surname> <given-names>G</given-names></string-name>, <string-name><surname>Zekveld</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Lunner</surname> <given-names>T</given-names></string-name></person-group> (<year>2018</year>) <article-title>Impact of SNR, masker type and noise reduction processing on sentence recognition performance and listening effort as indicated by the pupil dilation response</article-title>. <source>Hearing Research</source>.</mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ohlenforst</surname> <given-names>B</given-names></string-name>, <string-name><surname>Zekveld</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Lunner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Wendt</surname> <given-names>D</given-names></string-name>, <string-name><surname>Naylor</surname> <given-names>G</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Versfeld</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Kramer</surname> <given-names>SE</given-names></string-name></person-group> (<year>2017</year>) <article-title>Impact of stimulus-related factors and hearing impairment on listening effort as indicated by pupil dilation</article-title>. <source>Hearing Research</source> <volume>351</volume>:<fpage>68</fpage>–<lpage>79</lpage>.</mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oostenveld</surname> <given-names>R</given-names></string-name>, <string-name><surname>Fries</surname> <given-names>P</given-names></string-name>, <string-name><surname>Maris</surname> <given-names>E</given-names></string-name>, <string-name><surname>Schoffelen</surname> <given-names>JM</given-names></string-name></person-group> (<year>2011</year>) <article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>. <source>Computational Intelligence and Neuroscience</source> <volume>2011</volume>:<elocation-id>156869</elocation-id>.</mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><collab>OpenAI</collab>, <etal>et al.</etal></person-group> (<year>2023</year>) <article-title>GPT-4 Technical Report</article-title>. <source>arXiv</source>.</mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Palana</surname> <given-names>J</given-names></string-name>, <string-name><surname>Schwartz</surname> <given-names>S</given-names></string-name>, <string-name><surname>Tager-Flusberg</surname> <given-names>H</given-names></string-name></person-group> (<year>2022</year>) <article-title>Evaluating the use of cortical entrainment to measure atypical speech processing: A systematic review</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source> <volume>133</volume>:<fpage>104506</fpage>.</mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Panela</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Copelli</surname> <given-names>F</given-names></string-name>, <string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name></person-group> (<year>2024</year>) <article-title>Reliability and generalizability of neural speech tracking in younger and older adults</article-title>. <source>Neurobiology of Aging</source> <volume>134</volume>:<fpage>165</fpage>–<lpage>180</lpage>.</mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Panza</surname> <given-names>F</given-names></string-name>, <string-name><surname>Lozupone</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sardone</surname> <given-names>R</given-names></string-name>, <string-name><surname>Battista</surname> <given-names>P</given-names></string-name>, <string-name><surname>Piccininni</surname> <given-names>M</given-names></string-name>, <string-name><surname>Dibello</surname> <given-names>V</given-names></string-name>, <string-name><surname>La Montagna</surname> <given-names>M</given-names></string-name>, <string-name><surname>Stallone</surname> <given-names>R</given-names></string-name>, <string-name><surname>Venezia</surname> <given-names>P</given-names></string-name>, <string-name><surname>Liguori</surname> <given-names>A</given-names></string-name>, <string-name><surname>Giannelli</surname> <given-names>G</given-names></string-name>, <string-name><surname>Bellomo</surname> <given-names>A</given-names></string-name>, <string-name><surname>Greco</surname> <given-names>A</given-names></string-name>, <string-name><surname>Daniele</surname> <given-names>A</given-names></string-name>, <string-name><surname>Seripa</surname> <given-names>D</given-names></string-name>, <string-name><surname>Nicola</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Logroscino</surname> <given-names>G</given-names></string-name></person-group> (<year>2019</year>) <article-title>Sensorial frailty: age-related hearing loss and the risk of cognitive impairment and dementia in later life</article-title>. <source>Therapeutic Advances in Chronic Disease</source> <volume>10</volume>:<fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parbery-Clark</surname> <given-names>A</given-names></string-name>, <string-name><surname>Marmel</surname> <given-names>F</given-names></string-name>, <string-name><surname>Bair</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kraus</surname> <given-names>N</given-names></string-name></person-group> (<year>2011</year>) <article-title>What subcortical–cortical relationships tell us about processing speech in noise</article-title>. <source>European Journal of Neuroscience</source> <volume>33</volume>:<fpage>549</fpage>–<lpage>557</lpage>.</mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pichora-Fuller</surname> <given-names>MK</given-names></string-name>, <string-name><surname>Kramer</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Eckert</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Edwards</surname> <given-names>B</given-names></string-name>, <string-name><surname>Hornsby</surname> <given-names>BWY</given-names></string-name>, <string-name><surname>Humes</surname> <given-names>LE</given-names></string-name>, <string-name><surname>Lemke</surname> <given-names>U</given-names></string-name>, <string-name><surname>Lunner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Matthen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Mackersie</surname> <given-names>CL</given-names></string-name>, <string-name><surname>Naylor</surname> <given-names>G</given-names></string-name>, <string-name><surname>Phillips</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Richter</surname> <given-names>M</given-names></string-name>, <string-name><surname>Rudner</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sommers</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Tremblay</surname> <given-names>KL</given-names></string-name>, <string-name><surname>Wingfield</surname> <given-names>A</given-names></string-name></person-group> (<year>2016</year>) <article-title>Hearing Impairment and Cognitive Energy: The Framework for Understanding Effortful Listening (FUEL)</article-title>. <source>Ear &amp; Hearing</source> <volume>37</volume> <issue>Suppl 1</issue>:<fpage>5S</fpage>–<lpage>27S</lpage>.</mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Picton</surname> <given-names>TW</given-names></string-name>, <string-name><surname>John</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Dimitrijevic</surname> <given-names>A</given-names></string-name>, <string-name><surname>Purcell</surname> <given-names>DW</given-names></string-name></person-group> (<year>2003</year>) <article-title>Human auditory steady-state responses</article-title>. <source>International Journal of Audiology</source> <volume>42</volume>:<fpage>177</fpage>–<lpage>219</lpage>.</mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Presacco</surname> <given-names>A</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>S</given-names></string-name></person-group> (<year>2016</year>) <article-title>Evidence of degraded representation of speech in noise, in the aging midbrain and cortex</article-title>. <source>Journal of Neurophysiology</source> <volume>116</volume>:<fpage>2346</fpage>–<lpage>2355</lpage>.</mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Presacco</surname> <given-names>A</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>S</given-names></string-name></person-group> (<year>2019</year>) <article-title>Speech-in-noise representation in the aging midbrain and cortex: Effects of hearing loss</article-title>. <source>PLoS ONE</source> <volume>14</volume>:<fpage>e0213899</fpage>.</mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ritz</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wild</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name></person-group> (<year>2022</year>) <article-title>Parametric cognitive load reveals hidden costs in the neural processing of perfectly intelligible degraded speech</article-title>. <source>The Journal of Neuroscience</source> <volume>42</volume>:<fpage>4619</fpage>–<lpage>4628</lpage>.</mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rowland</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Hartley</surname> <given-names>DEH</given-names></string-name>, <string-name><surname>Wiggins</surname> <given-names>IM</given-names></string-name></person-group> (<year>2018</year>) <article-title>Listening in Naturalistic Scenes: What Can Functional Near-Infrared Spectroscopy and Intersubject Correlation Analysis Tell Us About the Underlying Brain Activity?</article-title> <source>Trends in Hearing</source> <volume>22</volume>:<fpage>2331216518804116</fpage>.</mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rufener</surname> <given-names>KS</given-names></string-name>, <string-name><surname>Kauk</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ruhnau</surname> <given-names>P</given-names></string-name>, <string-name><surname>Repplinger</surname> <given-names>S</given-names></string-name>, <string-name><surname>Heil</surname> <given-names>P</given-names></string-name>, <string-name><surname>Zaehle</surname> <given-names>T</given-names></string-name></person-group> (<year>2020</year>) <article-title>Inconsistent effects of stochastic resonance on human auditory processing</article-title>. <source>Scientific Reports</source> <volume>10</volume>:<fpage>6419</fpage>.</mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruhnau</surname> <given-names>P</given-names></string-name>, <string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name><surname>Schröger</surname> <given-names>E</given-names></string-name></person-group> (<year>2012</year>) <article-title>Finding the right control: The mismatch negativity under investigation</article-title>. <source>Clinical Neurophysiology</source> <volume>123</volume>:<fpage>507</fpage>–<lpage>512</lpage>.</mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schmitt</surname> <given-names>R</given-names></string-name>, <string-name><surname>Meyer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Giroud</surname> <given-names>N</given-names></string-name></person-group> (<year>2022</year>) <article-title>Better speech-in-noise comprehension is associated with enhanced neural speech tracking in older adults with hearing impairment</article-title>. <source>Cortex</source> <volume>151</volume>:<fpage>133</fpage>–<lpage>146</lpage>.</mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shukla</surname> <given-names>B</given-names></string-name>, <string-name><surname>Bidelman</surname> <given-names>GM</given-names></string-name></person-group> (<year>2021</year>) <article-title>Enhanced brainstem phase-locking in low-level noise reveals stochastic resonance in the frequency-following response (FFR)</article-title>. <source>Brain Research</source> <volume>1771</volume>:<fpage>147643</fpage>.</mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spyridakou</surname> <given-names>C</given-names></string-name>, <string-name><surname>Rosen</surname> <given-names>S</given-names></string-name>, <string-name><surname>Dritsakis</surname> <given-names>G</given-names></string-name>, <string-name><surname>Bamiou</surname> <given-names>D-E</given-names></string-name></person-group> (<year>2020</year>) <article-title>Adult normative data for the speech in babble (SiB) test</article-title>. <source>International Journal of Audiology</source> <volume>59</volume>:<fpage>33</fpage>–<lpage>38</lpage>.</mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stein</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Gossen</surname> <given-names>ER</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>KE</given-names></string-name></person-group> (<year>2005</year>) <article-title>Neuronal variability: noise or part of the signal?</article-title> <source>Nature Reviews Neuroscience</source> <volume>6</volume>:<fpage>389</fpage>–<lpage>397</lpage>.</mixed-citation></ref>
<ref id="c93"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stocks</surname> <given-names>NG</given-names></string-name></person-group> (<year>2000</year>) <article-title>Suprathreshold Stochastic Resonance in Multilevel Threshold Systems</article-title>. <source>Physical Review Letters</source> <volume>84</volume>:<fpage>2310</fpage>–<lpage>2313</lpage>.</mixed-citation></ref>
<ref id="c94"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Synigal</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name></person-group> (<year>2023</year>) <article-title>Electrophysiological indices of hierarchical speech processing differentially reflect the comprehension of speech in noise</article-title>. <source>BioRxiv</source>.</mixed-citation></ref>
<ref id="c95"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tabarelli</surname> <given-names>D</given-names></string-name>, <string-name><surname>Vilardi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Begliomini</surname> <given-names>C</given-names></string-name>, <string-name><surname>Pavani</surname> <given-names>F</given-names></string-name>, <string-name><surname>Turatto</surname> <given-names>M</given-names></string-name>, <string-name><surname>Ricci</surname> <given-names>L</given-names></string-name></person-group> (<year>2009</year>) <article-title>Statistically robust evidence of stochastic resonance in human auditory perceptual system</article-title>. <source>The European Physical Journal B</source> <volume>69</volume>:<fpage>155</fpage>–<lpage>159</lpage>.</mixed-citation></ref>
<ref id="c96"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Toms</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Lesperance</surname> <given-names>ML</given-names></string-name></person-group> (<year>2003</year>) <article-title>Piecewise regression: A tool for identifying ecological thresholds</article-title>. <source>Ecology</source> <volume>84</volume>:<fpage>2034</fpage>–<lpage>2041</lpage>.</mixed-citation></ref>
<ref id="c97"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tune</surname> <given-names>S</given-names></string-name>, <string-name><surname>Alavash</surname> <given-names>M</given-names></string-name>, <string-name><surname>Fiedler</surname> <given-names>L</given-names></string-name>, <string-name><surname>Obleser</surname> <given-names>J</given-names></string-name></person-group> (<year>2021</year>) <article-title>Neural attentional-filter mechanisms of listening success in middle-aged and older individuals</article-title>. <source>Nature Communications</source> <volume>12</volume>:<fpage>4533</fpage>.</mixed-citation></ref>
<ref id="c98"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Hirtum</surname> <given-names>T</given-names></string-name>, <string-name><surname>Somers</surname> <given-names>B</given-names></string-name>, <string-name><surname>Dieudonné</surname> <given-names>B</given-names></string-name>, <string-name><surname>Verschueren</surname> <given-names>E</given-names></string-name>, <string-name><surname>Wouters</surname> <given-names>J</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name></person-group> (<year>2023</year>) <article-title>Neural envelope tracking predicts speech intelligibility and hearing aid benefit in children with hearing loss</article-title>. <source>Hearing Research</source> <volume>439</volume>:<fpage>108893</fpage>.</mixed-citation></ref>
<ref id="c99"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vanthornhout</surname> <given-names>J</given-names></string-name>, <string-name><surname>Decruy</surname> <given-names>L</given-names></string-name>, <string-name><surname>Wouters</surname> <given-names>J</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name></person-group> (<year>2018</year>) <article-title>Speech Intelligibility Predicted from Neural Entrainment of the Speech Envelope</article-title>. <source>Journal of the Association for Research in Otolaryngology</source> <volume>19</volume>:<fpage>181</fpage>–<lpage>191</lpage>.</mixed-citation></ref>
<ref id="c100"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vieth</surname> <given-names>E</given-names></string-name></person-group> (<year>1989</year>) <article-title>Fitting piecewise linear regression functions to biological responses</article-title>. <source>Journal of Applied Physiology</source> <volume>67</volume>:<fpage>390</fpage>–<lpage>396</lpage>.</mixed-citation></ref>
<ref id="c101"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ward</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Neiman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Moss</surname> <given-names>F</given-names></string-name></person-group> (<year>2002</year>) <article-title>Stochastic resonance in psychophysics and in animal behavior</article-title>. <source>Biological Cybernetics</source> <volume>87</volume>:<fpage>91</fpage>–<lpage>101</lpage>.</mixed-citation></ref>
<ref id="c102"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ward</surname> <given-names>LM</given-names></string-name>, <string-name><surname>MacLean</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Kirschner</surname> <given-names>A</given-names></string-name></person-group> (<year>2010</year>) <article-title>Stochastic Resonance Modulates Neural Synchronization within and between Cortical Sources</article-title>. <source>PLoS ONE</source> <volume>5</volume>:<fpage>e14371</fpage>.</mixed-citation></ref>
<ref id="c103"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weineck</surname> <given-names>K</given-names></string-name>, <string-name><surname>Wen</surname> <given-names>OX</given-names></string-name>, <string-name><surname>Henry</surname> <given-names>MJ</given-names></string-name></person-group> (<year>2022</year>) <article-title>Neural synchronization is strongest to the spectral flux of slow music and depends on familiarity and beat salience</article-title>. <source>eLife</source> <volume>11</volume>:<elocation-id>e75515</elocation-id>.</mixed-citation></ref>
<ref id="c104"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wells</surname> <given-names>C</given-names></string-name>, <string-name><surname>Ward</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Chua</surname> <given-names>R</given-names></string-name>, <string-name><surname>Inglis</surname> <given-names>JT</given-names></string-name></person-group> (<year>2005</year>) <article-title>Touch Noise Increases Vibrotactile Sensitivity in Old and Young</article-title>. <source>Psychological Science</source> <volume>16</volume>:<fpage>313</fpage>–<lpage>320</lpage>.</mixed-citation></ref>
<ref id="c105"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilson</surname> <given-names>RH</given-names></string-name></person-group> (<year>2003</year>) <article-title>Development of a speech-in-multitalker-babble paradigm to assess word-recognition performance</article-title>. <source>Journal of the American Academy of Audiology</source> <volume>14</volume>:<fpage>453</fpage>–<lpage>470</lpage>.</mixed-citation></ref>
<ref id="c106"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilson</surname> <given-names>RH</given-names></string-name>, <string-name><surname>Trivette</surname> <given-names>CP</given-names></string-name>, <string-name><surname>Williams</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Watts</surname> <given-names>KL</given-names></string-name></person-group> (<year>2012a</year>) <article-title>The Effects of Energetic and Informational Masking on the Words-in-Noise Test (WIN)</article-title>. <source>J Am Acad Audiol</source> <volume>23</volume>:<fpage>522</fpage>–<lpage>533</lpage>.</mixed-citation></ref>
<ref id="c107"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilson</surname> <given-names>RH</given-names></string-name>, <string-name><surname>McArdle</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Watts</surname> <given-names>KL</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>SL</given-names></string-name></person-group> (<year>2012b</year>) <article-title>The Revised Speech Perception in Noise Test (R-SPIN)in a Multiple Signal-to-Noise Ratio Paradigm</article-title>. <source>Journal of the American Academy of Audiology</source> <volume>23</volume>:<fpage>590</fpage>–<lpage>605</lpage>.</mixed-citation></ref>
<ref id="c108"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yasmin</surname> <given-names>S</given-names></string-name>, <string-name><surname>Irsik</surname> <given-names>VC</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name>, <string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name></person-group> (<year>2023</year>) <article-title>The effects of speech masking on neural tracking of acoustic and semantic features of natural speech</article-title>. <source>Neuropsychologia</source> <volume>186</volume>:<fpage>108584</fpage>.</mixed-citation></ref>
<ref id="c109"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname> <given-names>F-G</given-names></string-name></person-group> (<year>2013</year>) <article-title>An active loudness model suggesting tinnitus as increased central noise and hyperacusis as increased nonlinear gain</article-title>. <source>Hearing Research</source> <volume>295</volume>:<fpage>172</fpage>–<lpage>179</lpage>.</mixed-citation></ref>
<ref id="c110"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname> <given-names>F-G</given-names></string-name></person-group> (<year>2020</year>) <article-title>Tinnitus and hyperacusis: central noise, gain and variance</article-title>. <source>Current Opinion in Physiology</source> <volume>18</volume>:<fpage>123</fpage>–<lpage>129</lpage>.</mixed-citation></ref>
<ref id="c111"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zou</surname> <given-names>J</given-names></string-name>, <string-name><surname>Feng</surname> <given-names>J</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>T</given-names></string-name>, <string-name><surname>Jin</surname> <given-names>P</given-names></string-name>, <string-name><surname>Luo</surname> <given-names>C</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Pan</surname> <given-names>X</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>F</given-names></string-name>, <string-name><surname>Zheng</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ding</surname> <given-names>N</given-names></string-name></person-group> (<year>2019</year>) <article-title>Auditory and language contributions to neural encoding of speech features in noisy environments</article-title>. <source>NeuroImage</source> <volume>192</volume>:<fpage>66</fpage>–<lpage>75</lpage>.</mixed-citation></ref>
<ref id="c112"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zuk</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Murphy</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Reilly</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name></person-group> (<year>2021</year>) <article-title>Envelope reconstruction of speech and music highlights stronger tracking of speech at low frequencies</article-title>. <source>PLOS Computational Biology</source> <volume>17</volume>:<fpage>e1009358</fpage>.</mixed-citation></ref>
<ref id="c113"><mixed-citation publication-type="data" specific-use="generated"><person-group person-group-type="author"><string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name></person-group> (<year>2025</year>) <article-title>Enhanced Neural Speech Tracking through Noise Indicates Stochastic Resonance in Humans</article-title>. <source>OSF</source> <pub-id pub-id-type="accession">osf.io/zs9u5</pub-id>. <ext-link ext-link-type="uri" xlink:href="https://osf.io/zs9u5/">https://osf.io/zs9u5/</ext-link></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100830.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Chait</surname>
<given-names>Maria</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents an <bold>important</bold> contribution to the understanding of neural speech tracking, demonstrating how minimal background noise can enhance the neural tracking of the amplitude-onset envelope. The evidence, through a well-designed series of EEG experiments, is <bold>convincing</bold>. This work will be of interest to auditory scientists, particularly those investigating biological markers of speech processing.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100830.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This paper presents a comprehensive study of how neural tracking of speech is affected by background noise. Using five EEG experiments and Temporal response function (TRF), it investigates how minimal background noise can enhance speech tracking even when speech intelligibility remains very high. The results suggest that this enhancement is not attention-driven but could be explained by stochastic resonance. These findings generalize across different background noise types, listening conditions, and speech features (envelope onset and envelope), offering insights into speech processing in real-world environments.</p>
<p>I find this paper well-written, the experiments and results are clearly described.</p>
<p>Comments on revisions:</p>
<p>I thank the author for thoughtful revisions and for adequately addressing my comments. The new version is much clearer and improved. I have no further questions.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100830.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The author investigates the role of background noise on EEG-assessed speech tracking in a series of five experiments. In the first experiment the influence of different degrees of background noise is investigated and enhanced speech tracking for minimal noise levels is found. The following four experiments explore different potential influences on this effect, such as attentional allocation, different noise types and presentation mode.</p>
<p>The step-wise exploration of potential contributors to the effect of enhanced speech tracking for minimal background noise is compelling. The motivation and reasoning for the different studies is clear and logical and therefore easy to follow. The results are discussed in a concise and clear way. While I specifically like the conciseness, one inevitable consequence is that not all results are equally discussed in depth.</p>
<p>Based on the results of the five experiments, the authors conclude that the enhancement of speech tracking for minimal background noise is likely due to stochastic resonance. Given broad conceptualizations of stochasitc resonance as noise benefit this is a reasonable conclusion.</p>
<p>This study will likely impact the field as it provides compelling support questioning the relationship between speech tracking and speech processing.</p>
<p>Comments on revisions:</p>
<p>All my previous comments were addressed nicely. Some of the comments were mere curiosity questions that were nicely entertained, even though they were not of direct relevance to the manuscript. I like the addition of the amplitude envelope analysis to the supplementary material as it offers direct comparison of those different methods. My only tiny tiny critic is (which bears no significance), that due to the many rearrangement changes in the marked changes document, the changes of content get buried and hard to see.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100830.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Herrmann</surname>
<given-names>Björn</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>This paper presents a comprehensive study of how neural tracking of speech is a ected by background noise. Using five EEG experiments and Temporal response function (TRF), it investigates how minimal background noise can enhance speech tracking even when speech intelligibility remains very high. The results suggest that this enhancement is not attention-driven but could be explained by stochastic resonance. These findings generalize across di erent background noise types and listening conditions, o ering insights into speech processing in real-world environments. I find this paper well-written, the experiments and results are clearly described. However, I have a few comments that may be useful to address.</p>
</disp-quote>
<p>I thank the reviewer for their positive feedback.</p>
<disp-quote content-type="editor-comment">
<p>(1) The behavioral accuracy and EEG results for clear speech in Experiment 4 di er from those of Experiments 1-3. Could the author provide insights into the potential reasons for this discrepancy? Might it be due to linguistic/ acoustic di erences between the passages used in experiments? If so, what was the rationale behind using di erent passages across di erent experiments?</p>
</disp-quote>
<p>The slight di erences in behavior and EEG magnitudes may be due to several factors. Di erent participants took part in the di erent experiments (with some overlap). Stories and questions were generated using ChatGPT using the same approach, but di erent research assistants have supported story and question generation, and ChatGPT advanced throughout the course of the study, such that di erent versions were used over time (better version control was only recently introduced by OpenAI). The same Google voice was used for all experiments, so this cannot be a factor. Most critically, within each experiment, assignment of speech-clarity conditions to di erent stories was randomized, such that statistical comparisons are una ected by these minor di erences between experiments. The noise-related enhancement generalizes across all experiments, showing that minor di erences in experimental materials do not impact it.</p>
<disp-quote content-type="editor-comment">
<p>(2) Regarding peak amplitude extraction, why were the exact peak amplitudes and latencies of the TRFs for each subject not extracted, and instead, an amplitude average within a 20 ms time window based on the group-averaged TRFs used? Did the latencies significantly di er across di erent SNR conditions?</p>
</disp-quote>
<p>Estimation of peak latency can be challenging if a deflection is not very pronounced in a participant. Especially the N1 was small for some conditions. Using the mean amplitude in a specific time window is very common practice in EEG research that mitigates this issue. Another, albeit less common, approach is to use a Jackknifing procedure to estimate each participant’s latencies (Smulders 2010 Psychophysiology; although this may sometimes not work well). For the revision, I used the Jackknifing approach to estimate peak latencies for each participant and condition, and extracted the mean amplitude around the peak latency. As expected, this approach provides very similar e ects as reported in the main article, here exemplified for Experiments 1 and 2. The results are thus not a ected by this data analysis choice. The estimated latencies di ered across SNRs, e.g., the N1 increased with decreasing SNR (this is less surprising/novel and was thus not added to the manuscript to avoid increasing the amount of information).</p>
<fig id="sa3fig1">
<label>Author response image 1.</label>
<caption>
<title>P1-minus-N1 amplitude for Experiment 1 and 2, using amplitudes centered on individually estimated peak latencies.</title>
<p>The asterisk indicates a significant di erence from the clear speech condition (FDR-thresholded).</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-100830-sa3-fig1.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>(3) How is neural tracking quantified in the current study? Does improved neural tracking correlate with EEG prediction accuracy or individual peak amplitudes? Given the di ering trends between N1 and P2 peaks in babble and speech-matched noise in experiment 3, how is it that babble results in greater envelope tracking compared to speech-matched noise?</p>
</disp-quote>
<p>Neural tracking is generally used for responses resulting from TRF analyses, crosscorrelations, or coherence, where the speech envelope is regressed against the brain signals (see review of Brodbeck &amp; Simon 2020 Current Opinion in Physiology). Correlations between EEG prediction accuracy and individual peak amplitudes was not calculated because the data used for the analyses are not independent. The EEG prediction accuracy essentially integrates information over a longer time interval (here 0–0.4 s), whereas TRF amplitudes are more temporally resolved. If one were to shorten the time interval (e.g., 0.08–0.12 s), then EEG prediction accuracy would look more similar to the TRF results (because the TRF is convolved with the amplitude-onset envelope of the speech [predicted EEG] before calculating the EEG prediction accuracy). Regarding the enhancement di erence between speech-matched noise and babble, I have discussed a possible interpretation in the discussion section. The result is indeed surprising, but it replicates across two experiments (Experiments 3 and 4), and is consistent with previous work using speech-matched noise that did not find the enhancement. I reproduce the part of the discussion here.</p>
<p>“Other work, using a noise masker that spectrally matches the target speech, have not reported tracking enhancements (Ding and Simon, 2013; Zou et al., 2019; Synigal et al., 2023). However, in these works, SNRs have been lower (&lt;10 dB) to investigate neural tracking under challenging listening conditions. At low SNRs, neural speech tracking decreases (Ding and Simon, 2013; Zou et al., 2019; Yasmin et al., 2023; Figures 1 and 2), thus resulting in an inverted u-shape in relation to SNR for attentive and passive listening (Experiments 1 and 2).”</p>
<p>“The noise-related enhancement in the neural tracking of the speech envelope was greatest for 12talker babble, but it was also present for speech-matched noise, pink noise, and, to some extent, white noise. The latter three noises bare no perceptional relation to speech, but resemble stationary, background buzzing from industrial noise, heavy rain, waterfalls, wind, or ventilation. Twelve-talker babble – which is also a stationary masker – is clearly recognizable as overlapping speech, but words or phonemes cannot be identified (Bilger, 1984; Bilger et al., 1984; Wilson, 2003; Wilson et al., 2012b). There may thus be something about the naturalistic, speech nature of the background babble that facilitates neural speech tracking.”</p>
<p>“Twelve-talker babble was associated with the greatest noise-related enhancement in neural tracking, possibly because the 12-talker babble facilitated neuronal activity in speech-relevant auditory regions, where the other, non-speech noises were less e ective.”</p>
<disp-quote content-type="editor-comment">
<p>(4) The paper discusses how speech envelope-onset tracking varies with di erent background noises. Does the author expect similar trends for speech envelope tracking as well? Additionally, could you explain why envelope onsets were prioritized over envelope tracking in this analysis?</p>
</disp-quote>
<p>The amplitude-onset envelope was selected because several previous works have used the amplitude-onset envelope, our previous work that first observed the enhancement also used the amplitude-onset envelope, and the amplitude-onset envelope has been suggested to work better for speech tracking. This was added to the manuscript. For the manuscript revision, analyses were calculated for the amplitude envelope, largely replicating the results for the amplitude-onset envelope. The results for the amplitude envelope are now presented in the Supplementary Materials and referred to in the main text.</p>
<p>“The amplitude-onset envelope was selected because a) several previous works have used it (Hertrich et al., 2012; Fiedler et al., 2017; Brodbeck et al., 2018a; Daube et al., 2019; Fiedler et al., 2019), b) our previous work first observing the enhancement also used the amplitude-onset envelope (Yasmin et al., 2023; Panela et al., 2024), and c) the amplitude-onset envelope has been suggested to elicit a strong speech tracking response (Hertrich et al., 2012). Results for analyses using the amplitude envelope instead of the amplitude-onset envelope show similar e ects and are provided in the Supplementary Materials (Figure 1-figure supplement 1).”</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p>(1) Include all relevant parameters related to data analysis where applicable. For example, provide the filter parameters (Line 154, Line 177, Line 172), and the default parameters of the speech synthesizer (Line 131).</p>
</disp-quote>
<p>Additional filter information and parameter values are provided in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(2) Please share the data and codes or include a justification as to why the data cannot be shared.</p>
</disp-quote>
<p>Data and code are provided on OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/zs9u5/">https://osf.io/zs9u5/</ext-link>). A materials availability statement has been added to the manuscript.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>The author investigates the role of background noise on EEG-assessed speech tracking in a series of five experiments. In the first experiment, the influence of di erent degrees of background noise is investigated and enhanced speech tracking for minimal noise levels is found. The following four experiments explore di erent potential influences on this e ect, such as attentional allocation, di erent noise types, and presentation mode. The step-wise exploration of potential contributors to the e ect of enhanced speech tracking for minimal background noise is compelling. The motivation and reasoning for the di erent studies are clear and logical and therefore easy to follow. The results are discussed in a concise and clear way. While I specifically like the conciseness, one inevitable consequence is that not all results are equally discussed in depth. Based on the results of the five experiments, the author concludes that the enhancement of speech tracking for minimal background noise is likely due to stochastic resonance. Given broad conceptualizations of stochastic resonance as a noise benefit this is a reasonable conclusion. This study will likely impact the field as it provides compelling support questioning the relationship between speech tracking and speech processing.</p>
</disp-quote>
<p>I thank the reviewer for the positive review and thoughtful feedback.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p>As mentioned in the public review, I like the conciseness. However, some points might benefit from addressing them.</p>
<p>(1) The absence of comprehension e ects is on the one hand surprising, as the decreased intelligibility should (theoretically) be visible in this data. On the other hand, from my own experience, the generation of &quot;good&quot; comprehension questions is quite di icult. While it is mentioned in the methods section, that comprehension accuracy and gist rating go hand in hand, this is not the case here. I am wondering if the data here should be rather understood as &quot;there is no di erence in intelligibility&quot; or that comprehension assessment via comprehension questions is potentially not a valid measure.</p>
</disp-quote>
<p>I assume that the reviewer refers to Experiment 1, where SNRs approximately below 15 dB led to reduced gist ratings (used as a proxy for speech intelligibility; Davis and Johnsrude, 2003, J Neurosci; Ritz et al., 2022, J Neurosci). That story comprehension accuracy does not decrease could be due to the comprehension questions themselves (as indicated by the reviewer, “good” questions can be hard to generate, potentially having low sensitivity). On the other hand, speech for the most di icult SNR was still ‘reasonably’ intelligible (gist ratings suggest ~85% of words could be understood), and participants may still have been able to follow the thread of the story. I do not further discuss this point in the manuscript, since it is not directly related to the noise-related enhancement in the neural tracking response, because the enhancement was present for high SNRs for which gist ratings did not show a di erence relative to clear speech (i.e., 20 dB and above).</p>
<disp-quote content-type="editor-comment">
<p>(2) However, if I understood correctly, the &quot;lower&quot; manipulation (same RMS for the whole sound stimulus) of experiment 3 was, what was also used in experiment 1. In experiment 3, unlike 1, there are comprehension e ects. I wondered if there are ideas about why that is.</p>
</disp-quote>
<p>Yes indeed, the ‘lower’ manipulation in Experiment 3 was also used in Experiments 1, 2, 4, and 5. The generation of the stimulus materials was similar across experiments. However, a new set of stories and comprehension questions was used for each experiment and the participants di ered as well (with some overlap). These aspects may have contributed to the di erence.</p>
<disp-quote content-type="editor-comment">
<p>(3) Concerning the prediction accuracy, for a naive reader, some surrounding information would be helpful: What is the purpose/expectation of this measure? Is it to show that all models are above chance?</p>
</disp-quote>
<p>EEG prediction accuracy was included here, mainly because it is commonly used in studies using TRFs. A reader may wonder about EEG prediction accuracy if it were not reported. The hypotheses of the current study are related to the TRF weights/amplitude. This was added to the manuscript.</p>
<p>“EEG prediction accuracy was calculated because many previous studies report it (e.g., Decruy et al., 2019; Broderick et al., 2021; Gillis et al., 2021; Weineck et al., 2022; Karunathilake et al., 2023), but the main focus of the current study is on the TRF weights/amplitude.”</p>
<disp-quote content-type="editor-comment">
<p>(4) Regarding the length of training and test data I got confused: It says per story 50 25-s snippets. As the maximum length of a story was 2:30 min, those snippets were mostly overlapping, right? It seems that depending on the length of the story and the &quot;location within the time series&quot; of the snippets, the number of remaining non-over-lapping snippets is variable. Also, within training, the snippets were overlapping, correct? Otherwise, the data for training would be too short. Again, as a naive reader, is this common, or can overlapping training data lead to overestimations?</p>
</disp-quote>
<p>The short stories made non-overlapping windows not feasible, but the overlap unlikely a ects the current results. Using cross-correlation (Hertrich et al 2012 Psychophysiology; which is completely independent for di erent snippets) instead of TRFs shows the same results (now provided in the supplementary materials). In one of our previous studies where the enhancement was first observed (Yasmin et al. 2023 Neuropsychologia), non-overlapping data were used because the stories were longer. This makes any meaningful impact of the overlap very unlikely. Critically, speech-clarity levels were randomized and all analyses were conducted in the same way for all conditions, thus not confounding any of the results/conclusions. The methods section was extended to further explain the choice of overlapping data snippets.</p>
<p>“Speech-clarity levels were randomized across stories and all analyses were conducted similarly for all conditions. Hence, no impact of overlapping training data on the results is expected (consistent with noise-related enhancements observed previously when longer stories and non-overlapping data were used; Yasmin et al., 2023). Analyses using cross-correlation, for which data snippets are treated independently, show similar results compared to those reported here using TRFs (Figure 1figure supplement 2).”</p>
<disp-quote content-type="editor-comment">
<p>(5) For experiment 1, three stories were clear, while the other 21 conditions were represented by one story each. Presumably, the ratio of 3:1 can a ect TRFs?</p>
</disp-quote>
<p>TRFs were calculated for each story individually and then averaged across three stories: either three clear stories, or three stories in babble for neighboring SNRs. Hence, the same number of TRFs were averaged for clear and noise conditions, avoiding exactly this issue. This was described in the methods section and is reproduced here:</p>
<p>“Behavioral data (comprehension accuracy, gist ratings), EEG prediction accuracy, and TRFs for the three clear stories were averaged. For the stories in babble, a sliding average across SNR levels was calculated for behavioral data, EEG prediction accuracy, and TRFs, such that data for three neighboring SNR levels were averaged. Averaging across three stories was calculated to reduce noise in the data and match the averaging of three stories for the clear condition.”</p>
<disp-quote content-type="editor-comment">
<p>(6) Was there an overlap in participants?</p>
</disp-quote>
<p>Some participants took part in several of the experiments in separate sessions on separate days. This was added to the manuscript.</p>
<p>“Several participants took part in more than one of the experiments, in separate sessions on separate days: 7, 7, 9, 9, and 14 (for Experiments 1-5, respectively) participated only in one experiment; 3 individuals participated in all 5 experiments; 68 unique participants took part across the 5 experiments.”</p>
<disp-quote content-type="editor-comment">
<p>(7) Can stochastic resonance also explain inverted U-shape results with vocoded speech?</p>
</disp-quote>
<p>This is an interesting question. Distortions to the neural responses to noise-vocoding may reflect internal noise, but this would require additional research. For example, the Hauswald study (2022 EJN), showing enhancements due to noise-vocoding, used vocoding channels that also reduced speech intelligibility. The study would ideally be repeated with a greater number of vocoding channels to make sure the e ects are not driven by increased attention due to reduced speech intelligibility. I did not further discuss this in detail in the manuscript as it would go too far away from the experiments of the current study.</p>
<disp-quote content-type="editor-comment">
<p>(8) Typo in the abstract: box sexes is probably meant to say both sexes?</p>
</disp-quote>
<p>This text was removed, because more detailed gender identification is reported in the methods, and the abstract needed shortening to meet the eLife guidelines.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewing Editor Comments:</bold></p>
<p>Interesting series of experiments to assess the influence of noise on cortical tracking in di erent conditions, interpreting the results with the mechanism of stochastic resonance.</p>
</disp-quote>
<p>I thank the editor for their encouraging feedback.</p>
<disp-quote content-type="editor-comment">
<p>For experiment 2, the author wishes to exclude the role of attention, by making participants perform a visual task. Data from low performers on the visual task was excluded, to avoid that participants attended the spoken speech. However, from the high performers on the visual task, how can you be sure that they did not pay attention to the auditory stimuli as well (as auditory attention is quite automatic, and these participants might be good at dividing their attention)? I understand that you can not ask participants about the auditory task during the experiment, but did you ask AFTER the experiment whether they were able to understand the stimuli? I think this is crucial for your interpretation.</p>
</disp-quote>
<p>Participants were not asked whether they were able to understand the stimuli. Participants would unlikely invest e ort/attention in understanding the stories in babble without a speech-related task. Nevertheless, for follow-up analyses, I removed participants who performed above 0.9 in the visual task (i.e., the high performers), and the di erence between clear speech and speech in babble replicates. In the plots, data from all babble conditions above 15 dB SNR (highly intelligible) were averaged, but the results look almost identical if all SNRs are averaged. Moreover, the correlation between visual task performance and the babble-related enhancement was not-significant. These analyses were added to the Supplementary Materials (Figure 2-figure supplement 1).</p>
<disp-quote content-type="editor-comment">
<p>Statistics: inconsistencies across experiments with a lot of simple tests (FDR corrected) and in addition sometimes rmANOVA added - if interactions in rmANOVA are not significant then all the simple tests might not be warranted. So a bit of double dipping and over-testing here, but on the whole the conclusions do not seem to be overstated.</p>
</disp-quote>
<p>The designs of the di erent experiments di ered, thus requiring di erent statistical approaches. Moreover, the di erent tests assess di erent comparisons. For all experiments, contrasting the clear condition to all noise conditions was the main purpose of the experiments. To correct for multiple comparison, the False Discovery Rate correction was used. Repeated-measures ANOVAs were conducted in addition to this – excluding the clear condition because it would not fit into a factorial structure (e.g., Experiment 3) or to avoid analyzing it twice (e.g., Experiment 5) – to investigate di erences between di erent noise conditions. There was thus no over-testing in the presented study.</p>
<disp-quote content-type="editor-comment">
<p>Small points:</p>
<p>Question on methods: For each story, 50 25-s data snippets were extracted (Page 7, line 190). As you have stories with a duration of 1.5 to 2 minutes, does that mean there is a lot of overlap across data snippets? How does that influence the TRF/prediction accuracy?</p>
</disp-quote>
<p>The short stories made non-overlapping windows not feasible, but the overlap unlikely a ects the current results. Using cross-correlation (Hertrich et al 2012 Psychophysiology; which is completely independent for di erent snippets) instead of TRFs shows the same results (newly added Figure 1-figure supplement 2). In one of our previous studies where the enhancement was first observed (Yasmin et al. 2023 Neuropsychologia), non-overlapping data were used because the stories were longer. This makes any meaningful impact of the overlap very unlikely. Critically, speechclarity levels were randomized and all analyses were conducted in the same way for all conditions, thus not confounding any of the results/conclusions. The methods section was extended to further explain the choice of overlapping data snippets.</p>
<p>“Overlapping snippets in the training data were used to increase the amount of data in the training given the short duration of the stories. Speech-clarity levels were randomized across stories and all analyses were conducted similarly for all conditions. Hence, no impact of overlapping training data on the results is expected (consistent with noise-related enhancements observed previously when longer stories and non-overlapping data were used; Yasmin et al., 2023). Analyses using crosscorrelation, for which data snippets are treated independently, show similar results compared to those reported here using TRFs (Figure 1-figure supplement 2).”</p>
<disp-quote content-type="editor-comment">
<p>Results Experiment 3: page 17, line 417: no di erences were found between clear speech and masked speech - is this a power issue (as it does look di erent in the figure, Figure 4b)?</p>
</disp-quote>
<p>I thank the editor for pointing this out. Indeed, I made a minor mistake. Two comparisons were significant after FDR-thresholding. This is now included in the revised Figure 4. I also made sure the mistake was not present for other analyses; which it was not.</p>
</body>
</sub-article>
</article>