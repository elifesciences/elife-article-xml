<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108310</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108310</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108310.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Allocentric and egocentric cues constitute an internal reference frame for real-world visual search</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<name>
<surname>Chen</surname>
<given-names>Yan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">#</xref>
<email>ychen342@buffalo.edu</email>
</contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3165-5416</contrib-id>
<name>
<surname>Xu</surname>
<given-names>Zhe-Xin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="author-notes" rid="n1">#</xref>
<email>brian_xu@hms.harvard.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02n96ep67</institution-id><institution>Shanghai Key Laboratory of Brain Functional Genomics, Key Laboratory of Brain Functional Genomics (Ministry of Education), School of Psychology and Cognitive Science, East China Normal University</institution></institution-wrap>, <city>Shanghai</city>, <country country="CN">China</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01y64my43</institution-id><institution>Department of Psychology, University at Buffalo</institution></institution-wrap>, <city>Buffalo</city>, <country country="US">United States</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Department of Neurobiology, Harvard Medical School</institution></institution-wrap>, <city>Boston</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>SP</surname>
<given-names>Arun</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/04dese585</institution-id><institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bangalore</city>
<country country="IN">India</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Luo</surname>
<given-names>Huan</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>#</label><p>These authors contributed equally to this work.</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-10-13">
<day>13</day>
<month>10</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP108310</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-07-24">
<day>24</day>
<month>07</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-07-03">
<day>03</day>
<month>07</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.04.14.648618"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Chen &amp; Xu</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Chen &amp; Xu</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108310-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Visual search in natural environments involves numerous objects, each composed of countless features. Despite this complexity, our brain efficiently locates targets. Here, we propose that the brain combines multiple reference cues to form an internal reference frame that facilitates real-world visual search. Objects in natural scenes often appear in orientations perceived as upright, enabling quicker recognition. However, how object orientation influences real-world visual search remains unknown. Moreover, the contributions of different reference cues—egocentric, visual context, and gravitational— are not well understood. To answer these questions, we designed a visual search task in virtual reality. Our results revealed an orientation effect independent of set size, suggesting reference frame transformation rather than object rotation. By rotating virtual scenes and participants in a flight simulator, we found that allocentric cues drastically altered search performance. These findings provide novel insights into the efficiency of real-world visual search and its connection to multimodal cognition.</p>
</abstract>
<abstract abstract-type="summary">
<title>Significance</title>
<p>A central question in the behavioral sciences concerns how people efficiently perceive natural environments. Visual search exemplifies this challenge. While research has elucidated the basic mechanisms, traditional theories struggle to explain the remarkable efficiency in real-world scenes. Here, we examine a fundamental property of natural scenes: reference frames. Real-world objects typically appear in consistent orientations, suggesting that orientation may guide search. Yet, the influence of object orientation on real-world search—and which reference cues (egocentric, visual context, or gravitational) determine that orientation—remains unknown. We developed a novel virtual-reality paradigm to address these questions. We demonstrated that humans combine multiple reference cues to form an internal reference frame that guides visual search, providing a novel account of the efficiency of real-world search.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Revised for clarity; Updated Figure 5</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Imagine navigating a busy mall looking for your friend amidst the crowd, or spotting a bottle of juice among stacks of products in a grocery store. These seemingly mundane moments illustrate the remarkable efficiency of visual search, where the nervous system sifts through countless colors, shapes, and motions to find a specific target. How does our brain manage to find that needle in a haystack? What cues guide visual search in natural environments?</p>
<p>While decades of research have uncovered the elements of visual search, such as parallel and serial search<sup><xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c5">5</xref></sup>, traditional theories are limited to simplified, controlled settings and fail to explain search performance involving natural scenes. For example, feature integration theory<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref></sup> predicts that visual search in the real world should be agonizingly slow, as it requires a step-by-step integration of numerous features. In practice, however, our brain performs these tasks with surprising speed and accuracy<sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c6">6</xref></sup>.</p>
<p>Recent studies have revealed several factors contributing to this striking efficiency of real-world visual search, including top-down attention, stimulus saliency, semantic context, prior knowledge, and environmental statistics<sup><xref ref-type="bibr" rid="c7">7</xref>–<xref ref-type="bibr" rid="c13">13</xref></sup>. These factors establish a priority map that guides attention to specific features or spatial locations<sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c10">10</xref></sup>. Another key element of natural scenes involves objects frequently occurring in consistent orientations<sup><xref ref-type="bibr" rid="c14">14</xref>–<xref ref-type="bibr" rid="c16">16</xref></sup>, which may aid visual search.</p>
<p>The role of orientation has been extensively studied in various recognition and discrimination tasks, with evidence suggesting that upright stimuli are processed more quickly and accurately<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c16">16</xref>–<xref ref-type="bibr" rid="c25">25</xref></sup>. Notably, response time scales linearly as an object’s orientation increasingly deviates from its canonical orientation, or as the orientation disparity between two objects increases<sup><xref ref-type="bibr" rid="c25">25</xref>–<xref ref-type="bibr" rid="c27">27</xref></sup>. These findings suggest two possible processes that underlie orientation dependency: mental rotation of objects and reference frame transformation<sup><xref ref-type="bibr" rid="c25">25</xref></sup>. Object mental rotation refers to a mental imagery process that rotates an object to its canonical orientation; in contrast, the latter process involves transforming a coordinate system to align with the object<sup><xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c27">27</xref></sup>. Neural mechanisms that underlie both processes have been proposed, including rotating the neural population vector for mental rotation<sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup> and gain modulation for coordinate transformation<sup><xref ref-type="bibr" rid="c30">30</xref>–<xref ref-type="bibr" rid="c32">32</xref></sup>. While object recognition is likely a necessary step for visual search, it is unknown how orientation influences visual search for real-world objects and which process is involved.</p>
<p>Moreover, our brain represents the world in multiple reference frames<sup><xref ref-type="bibr" rid="c33">33</xref>–<xref ref-type="bibr" rid="c49">49</xref></sup>. For example, when tilting your head to peer through an aisle in a grocery store, the bottle of juice might appear tilted in your eyes. Nevertheless, the brain correctly identifies the bottle by integrating various reference cues, including how objects are oriented relative to the body (egocentric), the surrounding environment (visual context), and gravity<sup><xref ref-type="bibr" rid="c41">41</xref>,<xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c46">46</xref>,<xref ref-type="bibr" rid="c47">47</xref>,<xref ref-type="bibr" rid="c50">50</xref>–<xref ref-type="bibr" rid="c53">53</xref></sup>. It has been hypothesized that the brain forms an internal reference frame by combining multiple reference cues for object perception<sup><xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c25">25</xref></sup>. To date, the role of an internal reference frame and the contributions of different reference cues in real-world visual search remain unknown.</p>
<p>To address this major knowledge gap, we design a real-world visual search task in virtual reality. We demonstrate a shorter response time (RT) and higher accuracy for upright objects than for those oriented horizontally. This orientation dependency is independent of set size, indicating a process outside of serial search is involved, namely the transformation of an internal reference frame rather than individual object rotation. By pairing a flight simulator with a head-mounted display, we independently manipulate the visual scene and participants’ body orientation, disentangling the contributions of different reference cues that are normally overlapped. We show that orientation dependency is significantly altered by visual context and gravitational cues, demonstrating the involvement of multisensory signals and high-level cognitive processes. This study provides a systematic examination of how object orientation influences visual search in naturalistic environments, offering insights into visual processing in the natural world.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Orientation dependency in real-world visual search</title>
<p>We started by asking how object orientation influences the performance of real-world visual search. We designed a psychophysical task in a virtual reality system that consisted of a head-mounted display and a flight simulator (see Methods). Objects were presented in a natural scene in the virtual environment, and participants were asked to report whether a cued target was present (<xref rid="fig1" ref-type="fig">Figure 1</xref>). Eight experimental conditions were introduced, including the combinations between two set sizes (4 and 9), two object orientations (0°, upright; 90°, horizontal), and target presence (present or absent).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Procedure and stimuli.</title>
<p>At the beginning of each trial, a word cue was presented at the center of the display (e.g., “Bear”), indicating the present trial’s search target. After a variable inter-stimulus interval (ISI), objects appeared for 1500 ms or until a response was made by the participant. The next trial started after a 1000-ms delay.</p></caption>
<graphic xlink:href="648618v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>All participants achieved accuracies well above chance, ranging from 81.4% to 95.6%, with a mean of 89.4%. The hit rate ranged from 71.2% to 94.7%, with a mean of 84.4%, and the false alarm rate ranged from 3.2% to 10.1%, with a mean of 5.6%. Discriminability ranged from 1.89 to 3.43, with a mean of 2.65.</p>
<p>We used a linear mixed-effects model to quantify the effects of various factors on RT while accounting for variability across participants and object categories. The model included object orientation, target presence, set size, and interactions between them as fixed effects. Subject identity and object category were included as random effects (see Methods). We found a significant effect of object orientation on normalized RT, with longer RT for upright objects than for horizontal ones (<xref rid="fig2" ref-type="fig">Figure 2A</xref>; estimate=0.002, 95%CI=[0.00055, 0.0034], <italic>t</italic>(10029)=2.7, <italic>p</italic>=0.00695). Participants also showed higher accuracy for upright objects than for horizontal ones (<xref rid="fig2" ref-type="fig">Figure 2B</xref>; estimate=-5.02×10<sup>−4</sup>, 95%CI=[−9.69×10<sup>−4</sup>, −3.63×10<sup>−5</sup>], <italic>t</italic>(11146)=-2.11, <italic>p</italic>=0.0346), suggesting that the difference in RT between these two object orientations cannot be explained by a speed-accuracy trade-off.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Results for Experiment 1.</title>
<p>A, Normalized response time was longer for horizontally oriented objects (90°) than for upright objects (0°) for set sizes 4 (purple line) and 9 (red line). B, The accuracy was higher when objects were upright than when they were horizontal. Format as in A. C, Normalized response time increased as a function of set size for upright (red line) and tilted objects (purple line). D, Difference in intercept and slope of the response time–set size function (lines shown in C) between upright and horizontal objects. Error bars represent S.E.M.</p></caption>
<graphic xlink:href="648618v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>These results demonstrate that real-world visual search is faster and more accurate for upright objects than for horizontal objects, suggesting an orientation dependency in this task. In addition, we found that normalized RT scaled with set size (<xref rid="fig2" ref-type="fig">Figure 2C</xref>; estimate=0.129, 95%CI=[0.116,0.142], <italic>t</italic>(10029)=19.1, <italic>p</italic>=1.26×10<sup>−79</sup>), which suggests that a serial search was involved.</p>
</sec>
<sec id="s2b">
<title>Internal reference frame facilitates visual search</title>
<p>What cognitive process might give rise to this orientation dependency in visual search? Previous studies on object recognition suggest two possibilities<sup><xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c27">27</xref></sup>: individual object rotation and internal reference frame transformation (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). As discussed above, the effect of set size on RT indicates a serial search, in which individual objects are processed one at a time<sup><xref ref-type="bibr" rid="c2">2</xref></sup>. If the observer mentally rotates each object during this serial process<sup><xref ref-type="bibr" rid="c54">54</xref></sup>, we expect set size to have a larger effect on RT for tilted objects than for upright ones. In other words, each additional object adds to the total mental rotation time, resulting in a steeper slope of the RT–set size function for tilted objects (<xref rid="fig3" ref-type="fig">Figure 3A</xref> and <xref rid="fig3" ref-type="fig">B</xref>, bottom row). Note that any process that applies to individual objects necessarily predicts an orientation effect on the slope, not the intercept. For example, if orientation acts as a gain on object processing, orientation dependency should also increase with set size because of the additional process time during serial search. Alternatively, the observer may rotate an internal reference frame to align with the objects and maintain its orientation throughout the serial process<sup><xref ref-type="bibr" rid="c25">25</xref></sup>. Because reference frame transformation happens before serial search and would be constant regardless of the number of objects present, we expect the effect of object orientation to be independent of set size (<xref rid="fig3" ref-type="fig">Figure 3A</xref> and <xref rid="fig3" ref-type="fig">B</xref>, top row).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Illustrations of two possible mechanisms for orientation dependency and additional results for Experiment 1.</title>
<p>A, Top row, an internal reference frame is rotated to align with objects at the beginning of each trial. Bottom row, each object is mentally rotated to a familiar orientation during serial search. B, Top, reference frame transformation predicts an orientation dependency on the intercept of the RT-set size relationship, as the transformation step is independent of the number of objects. Bottom, object rotation predicts a difference in slope between upright and tilted objects, because each object requires additional time for mental rotation. C, Longer normalized response time was found for both target-absent (dashed lines) and present trials (solid lines) for set sizes 4 (dashed blue and solid red lines) and 9 (dashed purple and solid yellow lines). D, Response time varied as a function of effective set size for upright (red) and tilted objects (purple). Lines represent linear fit to the data. E, Difference in intercept and slope of the response time–effective set size functions shown in D. Error bars indicate S.E.M.</p></caption>
<graphic xlink:href="648618v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Our linear mixed-effects model showed that the interaction between object orientation and set size was not significant (estimate=-1.30×10<sup>−4</sup>, 95%CI=[−3.39×10<sup>−4</sup>, 7.91×10<sup>−5</sup>], <italic>t</italic>(10029)=-1.22, <italic>p</italic>=0.223). Furthermore, we observed a significant difference in the intercepts of the RT–set size function for upright and horizontal objects (<xref rid="fig2" ref-type="fig">Figure 2D</xref>; <italic>m</italic>=-0.198, 95%CI=[−0.311, −0.086], <italic>t</italic>(19)=-3.68, <italic>p</italic>=0.00159, paired t-test), but not in the slopes (<xref rid="fig2" ref-type="fig">Figure 2D</xref>; <italic>m</italic>=0.009, 95%CI=[−0.008, 0.0261], <italic>t</italic>(19)=1.11, <italic>p</italic>=0.281). These findings suggest an internal reference frame transformation, rather than individual object rotation.</p>
<p>We also observed a significant difference between target-present and target-absent trials (<xref rid="fig3" ref-type="fig">Figure 3C</xref>; estimate=-0.160, 95%CI=[−0.291, −0.0283], <italic>t</italic>(10029)=-2.383, <italic>p</italic>=0.0172) and a significant interaction between target presence and set size (estimate=-0.0655, 95%CI=[−0.0846, −0.0464], <italic>t</italic>(10029)=-6.730, <italic>p</italic>=1.793×10<sup>−11</sup>). Interestingly, target presence did not significantly interact with object orientation (<xref rid="fig3" ref-type="fig">Figure 3C</xref>; estimate=3.85×10<sup>−4</sup>, 95%CI=[−1.71×10<sup>−3</sup>, 2.48×10<sup>−3</sup>], <italic>t</italic>(10029)=0.360, <italic>p</italic>=0.719). To reconcile these observations, we computed the effective set size to better characterize the relationship between set size and target presence. In target-absent trials, an observer must process all objects in the scene before asserting the target is absent. Therefore, the effective set size equals the actual set size. On the other hand, in target-present trials, the search can end as soon as the target is found. On average, only half of the objects would be processed<sup><xref ref-type="bibr" rid="c2">2</xref></sup>. As a result, the effective set size in target-present trials is half the actual set size. This 1:2 RT ratio between target-present and target-absent trials is well-documented in serial search<sup><xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c55">55</xref></sup>. We hypothesized that RT increases linearly with effective set size and that object orientation affects the intercept, but not the slope, of the RT– effective set size function.</p>
<p>Indeed, we found a strong correlation between effective set size and normalized RT (<xref rid="fig3" ref-type="fig">Figure 3D</xref>; <italic>r</italic>=0.871, <italic>p</italic>=8.95×10<sup>−26</sup> for upright objects, <italic>r</italic>=0.816, <italic>p</italic>=3.22×10<sup>−20</sup> for tilted objects, Pearson’s correlation). Importantly, the intercept, but not the slope, of the RT– effective set size function differs significantly between the two object orientations (<xref rid="fig3" ref-type="fig">Figure 3E</xref>; <italic>m</italic>=-0.192, 95%CI=[−0.271, −0.113], <italic>t</italic>(19)=-5.09, <italic>p</italic>=6.44×10<sup>−5</sup> for intercepts; <italic>m</italic>=0.0119, 95%CI=[−0.00478, 0.0287], <italic>t</italic>(19)=1.49, <italic>p</italic>=0.151 for slopes, two-tailed paired t-test), consistent with our findings of the RT–set size function.</p>
<p>Together, these results demonstrate that the effects of object orientation on RT were independent of set size, which suggests a process separate from the core serial search. These findings are consistent with the hypothesis of reference frame transformation<sup><xref ref-type="bibr" rid="c25">25</xref></sup>, in which the observer aligns an internal reference frame with objects before starting the recognition or search processes.</p>
</sec>
<sec id="s2c">
<title>Allocentric cues contribute to orientation dependency</title>
<p>The findings in the previous section suggest that an internal reference frame is transformed to facilitate visual search for tilted objects. What cues might determine this internal reference frame during a real-world search? Two general types of reference frames might be involved: egocentric and allocentric reference frames<sup><xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c40">40</xref>,<xref ref-type="bibr" rid="c41">41</xref></sup>. In egocentric reference frames, objects are represented relative to the retina, head, or body. In allocentric reference frames, visual context and gravitational cues may be used as references<sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c50">50</xref>,<xref ref-type="bibr" rid="c51">51</xref>,<xref ref-type="bibr" rid="c53">53</xref>,<xref ref-type="bibr" rid="c56">56</xref>,<xref ref-type="bibr" rid="c57">57</xref></sup>. To quantify the effects of different reference frames, we manipulated the visual context and egocentric cues by leveraging a virtual reality system consisting of a flight simulator and a head-mounted display (<xref rid="fig4" ref-type="fig">Figure 4</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Apparatus in Experiment 2.</title>
<p>A, Participants sat in a flight simulator, which allowed full body roll on a block-by-block basis, separating the egocentric reference cues from visual context and gravitational cues. B, The virtual scene presented on the head-mounted display was rotated independently to manipulate visual context. C, The internal reference frame can be modeled as a vector sum of visual context, egocentric, and gravitational cues with unknown weights. While not directly manipulated, gravitational cues can be dissociated from the other two cues by rotating the visual context and the participant’s body in the same direction.</p></caption>
<graphic xlink:href="648618v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In Experiment 2, four conditions were interleaved across blocks, including 1) a Baseline condition in which neither the visual context nor the flight simulator rotated, and all three reference cues were aligned (<xref rid="fig5" ref-type="fig">Figure 5A</xref>); 2) a Visual context condition in which the visual scene was rotated by 90° clockwise while the participants remained upright (<xref rid="fig5" ref-type="fig">Figure 5B</xref>); 3) an Egocentric condition in which the flight simulator rotated by 90° clockwise while the visual context remained upright relative to the world, such that egocentric cues differed from the other two cues (<xref rid="fig5" ref-type="fig">Figure 5C</xref>); and 4) a Gravitational condition, in which both the flight simulator and visual context rotated by 90° clockwise in the same direction, such that gravitational cues differed from these two cues (<xref rid="fig5" ref-type="fig">Figure 5D</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Experimental conditions in Experiment 2.</title>
<p>A, Baseline condition, in which visual context (represented by the tree and cloud), egocentric (the observer’s head and body), and gravitational (gray arrow, g) cues were all upright. 0° object orientation indicated upright with respect to all three cues. B, Visual context condition, in which only the visual context was rotated by 90°. In this case, 90° became upright with respect to the visual context, and 0° was upright with respect to the other two cues. C, Egocentric condition, in which the observer’s full body was rotated by 90°. Here, 90° was upright with respect to egocentric cues, while 0° remained upright to the other two cues. D, Gravitational condition, in which both visual context and the observer’s body were rotated by 90° in the same direction. In this condition, 0° was upright to visual context and egocentric cues, and 90° was upright with respect to gravity.</p></caption>
<graphic xlink:href="648618v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Object orientation was defined relative to gravity in the Baseline, Visual context, and Egocentric conditions, and relative to the body in the Gravitational condition (<xref rid="fig5" ref-type="fig">Figure 5</xref>, bottom row). These coordinates were chosen such that any changes in orientation dependency can be attributed to the rotation of the reference cue under study in each condition. For example, if RT is shorter for upright than tilted objects (with respect to gravity) in the Visual context condition, it would suggest that visual context has no contribution to the orientation dependency. In contrast, if similar RT is found between upright and tilted objects, it would indicate that visual context has a significant effect that cancels out the orientation advantage. Finally, if tilted objects, which would appear upright relative to the visual context, have shorter RT in this condition, it would suggest an even stronger contribution of visual context that reverses the orientation dependency.</p>
<p>We found a significant main effect of object orientation on normalized RT (<xref rid="fig6" ref-type="fig">Figure 6A</xref>; estimate=1.64×10<sup>−3</sup>, 95%CI=[8.13×10<sup>−4</sup>, 2.47×10<sup>−3</sup>], <italic>t</italic>(9865)=3.89, <italic>p</italic>=1.02×10<sup>−4</sup>, linear mixed-effects model; see Methods). Importantly, there were significant interactions between object orientation and visual context (estimate=-3.11×10<sup>−5</sup>, 95%CI=[−4.40×10<sup>−5</sup>, −1.81×10<sup>−5</sup>], <italic>t</italic>(9865)=-4.69, <italic>p</italic>=2.73×10<sup>−6</sup>), between object orientation and egocentric cues (estimate=-2.25×10<sup>−5</sup>, 95%CI=[−3.54×10<sup>−5</sup>, −9.50×10<sup>−6</sup>], <italic>t</italic>(9865)=-3.40, <italic>p</italic>=6.84×10<sup>−4</sup>), and among object orientation, visual context, and egocentric cues (estimate=6.61×10<sup>−5</sup>, 95%CI=[4.58×10<sup>−7</sup>, 8.65×10<sup>−7</sup>], <italic>t</italic>(9865)=6.36, <italic>p</italic>=2.14×10<sup>−10</sup>). These findings suggest that all three reference cues influenced the orientation dependency observed in our visual search task.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Results for Experiment 2.</title>
<p>A, Normalized response time in each experimental condition. 0° and 90° represent upright and tilted objects, respectively. In each condition, object orientation was defined relative to the reference cues not under experimental manipulation (i.e., gravitational reference frame in the Baseline, Visual context, and Egocentric conditions; egocentric reference frame in the Gravitational condition; see <xref ref-type="fig" rid="fig5">Figure 5</xref>). B, Weights for each reference frame inferred from changes in orientation dependency. Error bars represent S.E.M.</p></caption>
<graphic xlink:href="648618v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In the Baseline condition, the effect of object orientation is consistent with that in Experiment 1, with faster RT for upright than tilted objects (<xref rid="fig6" ref-type="fig">Figure 6A</xref>, gray; <italic>t</italic>(9)=-3.6705, <italic>p</italic>=0.0052, 95%CI=[−0.2358, −0.0560], two-tailed paired t-test). In the Visual context condition, changing the orientation of visual context completely reversed the orientation dependency, resulting in a faster response to objects that were tilted with respect to egocentric and gravitational reference frames (<xref rid="fig6" ref-type="fig">Figure 6A</xref>, dark blue; <italic>t</italic>(9)=3.2334, <italic>p</italic>=0.0103, 95%CI=[0.0342, 0.1936], two-tailed paired t-test). This indicates that visual context can largely overwrite the orientation defined by the other two cues. In contrast, rotating the egocentric cues resulted in similar RTs between the two object orientations (<xref rid="fig6" ref-type="fig">Figure 6A</xref>, dark red; <italic>t</italic>(9)=1.0519, <italic>p</italic>=0.3203, 95%CI=[−0.0351, 0.0960], two-tailed paired t-test). This result suggests that egocentric cues were sufficient to eliminate orientation dependency, but not strong enough to reverse it. Finally, orientation dependency was also reversed in the Gravitational condition (<xref rid="fig6" ref-type="fig">Figure 6A</xref>, purple; <italic>t</italic>(9)=5.6931, <italic>p</italic>=0.0003, 95%CI=[0.1100, 0.2551], two-tailed paired t-test), suggesting a strong effect of gravitational cues in determining the effect of object orientation in visual search.</p>
<p>We computed the relative weights for these three reference frames by comparing an orientation dependency index for each condition (<xref rid="fig4" ref-type="fig">Figure 4C</xref> and <xref rid="fig6" ref-type="fig">6B</xref>; see Methods). The weights for all three references were significantly higher than zero (estimate=0.33, 95%CI=[0.23, 0.43], <italic>t</italic>(27)=7.17, <italic>p</italic>=1.04×10<sup>−7</sup> for visual context; estimate=0.21, 95%CI=[0.12, 0.31], <italic>t</italic>(27)=4.61, <italic>p</italic>=8.77×10<sup>−5</sup> for egocentric cues; estimate=0.45, 95%CI=[0.36, 0.55], <italic>t</italic>(27)=9.80, <italic>p</italic>=2.18×10<sup>−10</sup> for gravitational cues; linear mixed-effects model). An analysis of variance revealed that the weights varied significantly across reference cues: <italic>F</italic>(3, 27)=56.22, <italic>p</italic>=9.71×10<sup>−12</sup>. Gravitational cues had the highest weight, visual context ranked second, and the weight for egocentric cues was the lowest (<xref rid="fig6" ref-type="fig">Figure 6B</xref>). Tukey’s HSD test revealed that the weight for gravitational cues is significantly higher than that for egocentric cues (mean difference=0.24, 95%CI=[0.07, 0.41], <italic>p</italic>=0.0047), but the differences between other pairs of weights were not significant (mean difference=0.12, 95%CI=[−0.05, 0.29], <italic>p</italic>=0.217 between visual context and egocentric cues; mean difference=0.12, 95%CI=[−0.05, 0.29], <italic>p</italic>=0.200 between gravitational and visual context cues). It is worth noting that these were relative weightings across three types of reference cues, and although ranked the lowest, egocentric cues had considerable effects, as shown by the absence of orientation dependency in the Egocentric condition (<xref rid="fig6" ref-type="fig">Figure 6A</xref>, compare gray and dark red).</p>
<p>These results demonstrated a drastic change in orientation dependency when different reference frames were manipulated. This confirms our findings in Experiment 1 that an internal reference frame is involved during visual search for tilted objects. Furthermore, these results suggest that multiple ego- and allocentric reference cues constitute the internal reference frame, which may help to establish a stable representation of the world (see Discussion).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this study, we investigated how object orientation and reference cues influence visual search performance in natural scenes. Our results revealed that participants were more efficient at locating upright objects compared to tilted ones, demonstrating an orientation dependency in real-world contexts. We considered two possible mechanisms underlying this orientation dependency: internal reference frame transformation and individual object rotation<sup><xref ref-type="bibr" rid="c25">25</xref></sup>. Further analysis of the relationship between RT and set size revealed that only the intercept, but not the slope, of the RT–set size function differed between upright and tilted objects. This finding rules out the hypothesis of individual object rotation, which predicts a change in the slope of the RT– set size relationship. Instead, our findings suggest that the brain employs an internal reference frame for visual processing, and this internal reference frame is rotated to match the orientation of objects before the search.</p>
<p>By rotating the participants’ bodies and the visual scene in virtual reality, we showed that multiple reference cues, including egocentric, visual context, and gravitational, contribute to the internal reference frame. Notably, allocentric reference frames, namely visual context and gravitational cues, had greater influences on the orientation dependency than egocentric cues. This suggests that multisensory integration and higher-order cognitive mechanisms play a critical role in establishing the internal reference frame during visual search in complex, real-world environments.</p>
<p>What is the advantage of using an internal reference frame? Transforming an internal reference frame, rather than sequentially rotating each object, significantly reduces the computational complexity of visual search in the real world. By realigning the reference frame once before initiating a search, the process requires a fixed amount of cognitive resources that do not depend on set size. This allows the brain to process complex natural scenes in a more energy-efficient manner. In contrast, if objects are rotated individually, each additional object would necessitate additional cognitive resources, making the process more resource-intensive as the visual scene becomes more complex. This study, consistent with prior studies on reference frames<sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c33">33</xref>–<xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c41">41</xref>,<xref ref-type="bibr" rid="c42">42</xref>,<xref ref-type="bibr" rid="c50">50</xref>,<xref ref-type="bibr" rid="c57">57</xref></sup>, demonstrates that the nervous system adapts to environmental statistics and efficiently processes information based on task demands. Our findings suggest that internal reference frame transformation might be another strategy the brain uses to improve visual search efficiency in real-world environments, complementing existing factors such as top-down attention, stimulus saliency, prior knowledge, and semantic context<sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c9">9</xref>–<xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c56">56</xref>,<xref ref-type="bibr" rid="c58">58</xref></sup>.</p>
<p>Visual information is primarily encoded in a retinotopic space in the early stages of visual processing<sup><xref ref-type="bibr" rid="c59">59</xref>–<xref ref-type="bibr" rid="c61">61</xref></sup>. However, a retinotopic representation is often unstable due to movements of the eyes, head, and body as we navigate the environment. To form a stable representation of the world, information about posture and self-motion—including efference copy of motor commands and sensory inputs from multiple modalities—is integrated to transform visual inputs into allocentric coordinates<sup><xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c42">42</xref>,<xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c50">50</xref>,<xref ref-type="bibr" rid="c51">51</xref>,<xref ref-type="bibr" rid="c53">53</xref>,<xref ref-type="bibr" rid="c57">57</xref>,<xref ref-type="bibr" rid="c62">62</xref>–<xref ref-type="bibr" rid="c67">67</xref></sup>. This transformation from egocentric to allocentric reference frames might be initiated in V1<sup><xref ref-type="bibr" rid="c68">68</xref>–<xref ref-type="bibr" rid="c70">70</xref></sup>, continued along both the dorsal visual pathway to the parietal cortex<sup><xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c40">40</xref>,<xref ref-type="bibr" rid="c42">42</xref>–<xref ref-type="bibr" rid="c45">45</xref>,<xref ref-type="bibr" rid="c49">49</xref>,<xref ref-type="bibr" rid="c62">62</xref>,<xref ref-type="bibr" rid="c71">71</xref>–<xref ref-type="bibr" rid="c74">74</xref></sup>, and the ventral pathway to the inferotemporal cortex<sup><xref ref-type="bibr" rid="c46">46</xref>,<xref ref-type="bibr" rid="c47">47</xref>,<xref ref-type="bibr" rid="c75">75</xref></sup>.</p>
<p>Our results show that real-world visual search relies heavily on gravitational cues, which may include vestibular signals and proprioception<sup><xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c51">51</xref>,<xref ref-type="bibr" rid="c57">57</xref></sup>. These findings are supported by a recent neurophysiological study showing that objects are predominantly encoded in a gravitational reference frame in the primate inferotemporal cortex, an area central to object processing<sup><xref ref-type="bibr" rid="c46">46</xref></sup>. In addition, neurons in the anterior inferotemporal cortex exhibit tuning for visual gravity cues akin to the visual context in our study<sup><xref ref-type="bibr" rid="c47">47</xref>,<xref ref-type="bibr" rid="c75">75</xref></sup>. Gravity orientation tuning has also been found in the anterior thalamus, cerebellum, and other regions in primates and rodents<sup><xref ref-type="bibr" rid="c76">76</xref>–<xref ref-type="bibr" rid="c78">78</xref></sup>. Together, these studies provide strong neural evidence for an internal reference frame that integrates visual and nonvisual cues.</p>
<p>Consistent with previous studies that demonstrated the influence of gravity on visual perception<sup><xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c48">48</xref>,<xref ref-type="bibr" rid="c51">51</xref>,<xref ref-type="bibr" rid="c79">79</xref></sup>, we show for the first time that gravitational cues can outweigh egocentric cues in visual search. The greater reliance on these cues might be driven by the complexity of natural objects and scenes, which requires the engagement of high-level visual areas<sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c46">46</xref>,<xref ref-type="bibr" rid="c47">47</xref>,<xref ref-type="bibr" rid="c75">75</xref></sup>. The visual system might rely more on retinotopic or egocentric reference frames when processing simple visual features such as oriented bars and intersecting line segments<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c41">41</xref>,<xref ref-type="bibr" rid="c79">79</xref></sup>. An interesting future direction is to examine the relative weightings of different reference cues in tasks involving stimuli ranging from simple to complex<sup><xref ref-type="bibr" rid="c48">48</xref></sup>. It is possible that the nervous system stores information in multiple reference frames and adaptively chooses an optimal reference based on the context<sup><xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c45">45</xref>,<xref ref-type="bibr" rid="c49">49</xref></sup>.</p>
<p>Another possible explanation for the higher weighting on gravity is the involvement of an internal reference frame in our task. Previous studies primarily used tasks that involved only a single stimulus on the display<sup><xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c79">79</xref></sup>. In these tasks, a reference frame transformation may not be necessary, as mental rotation of a single object would be equally efficient. It is therefore natural that gravity influences perception less when these reference frames are not explicitly used. In contrast, our visual search promotes the use of reference cues rather than mental rotation of each object, leading to greater engagement of other sensory systems needed for a robust internal reference frame. However, as discussed above, gravity-centered representation might be a universal property of the higher visual areas, regardless of the task context<sup><xref ref-type="bibr" rid="c46">46</xref>,<xref ref-type="bibr" rid="c47">47</xref>,<xref ref-type="bibr" rid="c75">75</xref></sup>. Future work that examines the involvement of an internal reference frame in various contexts would provide valuable insights.</p>
<p>Due to technical limitations, we did not measure eye positions during the experiments. Roll vestibular-ocular reflex produces torsional eye movements in the direction opposite to body rotation<sup><xref ref-type="bibr" rid="c80">80</xref></sup>, which may reduce the rotation of retinotopic coordinates in the Egocentric and Gravitational conditions in Experiment 2. This reduction in egocentric orientation might explain the smaller weights for egocentric reference cues. However, because enough time was given for participants to adapt to new body postures after each body rotation, we believe that only static torsion, not torsional eye movement, was involved during task performance. The amount of static torsion is 10°-12° maximum<sup><xref ref-type="bibr" rid="c80">80</xref></sup>, which could account for only ∼11% of errors in our estimation of weights. It has also been shown that static tilted visual images could not induce torsional eye movement<sup><xref ref-type="bibr" rid="c80">80</xref></sup>. Thus, our results in the Visual context condition were unlikely to be influenced by torsion. Future work using reliable eye-tracking in virtual reality would further our understanding of how eye movements interact with the rotation of multiple reference cues.</p>
<p>In summary, this study demonstrates that humans use an internal reference frame in real-world visual search. Using a virtual reality system, we show that multiple allocentric and egocentric reference cues shape this internal reference frame, providing a novel explanation for visual search efficiency in naturalistic environments. Our findings suggest that real-world visual search offers valuable insights into high-level cognitive processes that involve multiple sensory modalities.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>Twenty participants (seven males and thirteen females, mean age 20.40, SD=0.49) with normal or corrected-to-normal vision participated in Experiment 1, and ten of them (five males and five females, mean age 20.90, SD=0.80) participated in Experiment 2. All subjects were new to psychophysical experiments and were unaware of the purpose of the study. Informed written consent was obtained from each participant before data collection. The study was approved by the Institutional Review Board at East China Normal University.</p>
</sec>
<sec id="s4b">
<title>Apparatus</title>
<p>A head-mounted display (HTC Vive, first generation, HTC and Valve Corporation) was used for stimulus presentation. The display had a mean luminance of 61.15 cd/m<sup>2</sup>, a resolution of 1080×1200 pixels, a refresh rate of 90 Hz, and a pixel size of ∼0.09°×0.09°. The field of view of the head-mounted display was approximately 111.5°. Visual stimuli were generated in Unity3D (Unity Technologies, version 5.6.2). Participants sat on a chair inside a custom flight simulator (Zhuoyuan Co.) and were secured by a seatbelt and additional padding around the head and body (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). A wireless joystick was held by the participants to respond to the task. The flight simulator could be rotated along the roll axis at low speed.</p>
</sec>
<sec id="s4c">
<title>Stimuli</title>
<p>The visual stimuli consisted of an outdoor scene with objects placed on the ground (<xref rid="fig1" ref-type="fig">Figure 1</xref>). A matrix of either four or nine objects was presented at the center of the screen in an area measuring ∼49°×53°. The size of each object was around 4.8°×4.8° on the display and was the same for both set sizes, 4 and 9. The spacing between adjacent objects was the same for both set sizes, such that differential effects of visual crowding could be eliminated. Objects included large animals and furniture with a canonical orientation. Their orientation was either 0° (upright) or 90° (horizontal), with all objects sharing the same orientation during each trial. Objects presented on each trial were randomly selected from 89 categories, each comprising two variations with different shapes and colors. In target-present trials, the search target was randomly selected from the presented objects; in target-absent trials, the target was randomly selected from the absent objects. All 3D models used in the experiments were acquired from the Unity Asset Store and were scaled to approximately the same 3D volume.</p>
</sec>
<sec id="s4d">
<title>Experimental conditions</title>
<p>In Experiment 1, two set sizes, 4 and 9, were interleaved across blocks. Each block consisted of 72 trials. Two object orientations, horizontal and upright, were randomly intermixed and counter-balanced across trials. In half of the trials, the array of objects did not include the cued target (target-absent trials). Target-present and target-absent trials were counter-balanced and randomly intermixed within each block. The flight simulator and visual context did not rotate throughout the experiment. Each participant completed two sessions with four blocks in each session.</p>
<p>In Experiment 2, four reference frame conditions were introduced (<xref rid="fig5" ref-type="fig">Figure 5</xref>). 1) In the Baseline condition, the flight simulator and visual context remained upright relative to gravity (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). 2) In the Visual context condition, the visual scene was rotated 90° in either direction relative to gravity (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). 3) In the Egocentric condition, the flight simulator rotated 90° while the visual context remained upright (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). 4) In the Gravitational condition, both the flight simulator and visual context rotated 90° in the same direction, such that only gravitational cues differed from the other two cues (<xref rid="fig5" ref-type="fig">Figure 5D</xref>). These four conditions were separated into blocks and randomly interleaved. The set size was 9 throughout the experiment. Object orientation and target presence conditions were interleaved in the same way as in Experiment 1. Each participant completed two sessions with eight blocks in each session. Importantly, the head-mounted display covered the entire field of vision and did not move relative to the observer’s head throughout the experiment, allowing control over the visual information received by the participants regardless of their body orientation. Note that body rotation only occurred between each block of trials, and we allowed participants to adapt to the new body orientation before resuming the experiment. Therefore, we expect the eyes to reach a steady state that involves only a small amount of torsion, if any<sup><xref ref-type="bibr" rid="c80">80</xref></sup>.</p>
</sec>
<sec id="s4e">
<title>Procedure</title>
<p>On each trial, a word cue was presented for 500 milliseconds, indicating the search target for the current trial (<xref rid="fig1" ref-type="fig">Figure 1</xref>). After a variable inter-stimulus interval (ISI), a set of objects was presented on the screen. The ISI was randomly selected from a set of discrete values ranging from 500 to 1000 milliseconds with a step of 100 milliseconds. Participants were asked to report whether the target was present by pressing one of the two buttons on the joystick. The objects disappeared after a response was made. If a response was not made within 1500 milliseconds, the objects would disappear, and participants would still need to respond before proceeding to the next trial. This 1500-ms stimulus duration encouraged participants to respond as fast and accurately as possible, which possibly prevented them from revisiting the same object multiple times. On average, most correct responses were made within the stimulus presentation period (mean = 1094±375 ms for Experiment 1; mean = 1208±433 ms for Experiment 2). The next trial started 1000 milliseconds after a response was made.</p>
</sec>
<sec id="s4f">
<title>Data preprocessing</title>
<p>For each participant, trials with RT greater than 4 median absolute deviations (MADs) from the median were removed from further analysis. MAD was used because it is more robust to outliers than standard deviation. In addition, trials with RT shorter than 50 ms were classified as random guesses and were therefore removed from analysis. On average, 2.67% of the trials were removed. We then normalized RTs by taking the Z-score for each participant and only used RTs from correct trials for further analyses.</p>
</sec>
<sec id="s4g">
<title>Data analysis</title>
<sec id="s4g1">
<title>Discriminability</title>
<p>The discriminability of each subject was calculated as: <italic>d</italic>′ = <italic>Z</italic>(<italic>hit</italic>) − <italic>Z</italic>(<italic>false alarm</italic>), where <italic>Z</italic>(<italic>hit</italic>) was the Z score of the hit rate and <italic>Z</italic>(<italic>false alarm</italic>) was the Z score of the false alarm rate.</p>
</sec>
<sec id="s4g2">
<title>Effective set size</title>
<p>In target-absent trials, an observer must identify all objects before responding. Therefore, the effective set size would be the same as the actual set size. In contrast, when the target is present, only half of the objects, on average, need to be processed before the target is identified. Therefore, the effective set size would be half of the actual set size in target-present trials<sup><xref ref-type="bibr" rid="c2">2</xref></sup>.</p>
</sec>
<sec id="s4g3">
<title>RT–set size function</title>
<p>A linear function, <italic>z</italic>(<italic>RT</italic>) = <italic>b</italic><sub>1</sub> + <italic>b</italic><sub>2</sub><italic>x</italic>, was fit to the relationship between (effective) set size <italic>x</italic> and the z score of RT, where <italic>b</italic><sub>1</sub> and <italic>b</italic><sub>2</sub> are the intercept and slope of the function, respectively. Regression coefficients were computed using ordinary least squares in MATLAB (MathWorks, MA).</p>
</sec>
<sec id="s4g4">
<title>Linear mixed-effects models</title>
<p>Because of the nature of RT data, there is a large variability across individuals and trials<sup><xref ref-type="bibr" rid="c81">81</xref></sup>. We modeled RT using linear mixed-effects models that considered participants and object categories as random effects on the RT intercept and experimental conditions as fixed effects. These models were fit to trial-level data using the <italic>fitlme</italic> function in MATLAB (MathWorks, MA).</p>
<p>For Experiment 1, we modeled normalized RTs for each trial using three variables and all permutations of interactions between them as fixed effects, including target presence, object orientation, and set size. Random effects on intercept included participant identity and object category in each trial. The object orientation factor in the model was considered a measure of orientation dependency. The interaction between object orientation and set size was considered the effect of orientation on the slope of the RT-set size function. The three-way interaction between object orientation, set size, and target presence was considered the effect of orientation on the slope of the RT-effective set size function.</p>
<p>For Experiment 2, we modeled normalized RTs using object orientation and the orientations of visual context and egocentric reference frame as fixed effects, and the random effects were the same as in Experiment 1. Interactions between Object orientation x Visual context and Object orientation x Egocentric were considered contributions of visual context and egocentric cues, respectively. The three-way interaction between Object orientation x Visual context x Egocentric was considered the effect of gravitational cues.</p>
</sec>
<sec id="s4g5">
<title>Contributions of reference frames</title>
<p>To quantify the relative weights of each reference frame in Experiment 2, we first computed an orientation dependency index (ODI) as the difference in z-scored RT for horizontal and upright objects:
<disp-formula>
<graphic xlink:href="648618v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula></p>
<p>Weights for reference frames were then computed as the change in ODI in each condition compared to the baseline:
<disp-formula>
<graphic xlink:href="648618v2_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula>
<graphic xlink:href="648618v2_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula></p>
<p>Finally, the weights were normalized by their sum for each participant. Note that object orientation was defined relative to the reference frame that was not under manipulation in each experimental condition (i.e., gravitational reference in Baseline, Visual context, and Egocentric conditions; egocentric reference in the Gravitational condition; see <xref rid="fig5" ref-type="fig">Figure 5</xref>). A linear mixed-effects model was fit to the weights with participant identity as a random effect: Weight ∼ Visual context + Egocentric + Gravitational + (1|Subject ID). This provides a statistical assessment of the significance of each weight. An analysis of variance on the model was used to measure variability across reference cues.</p>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="s5" sec-type="data-availability">
<title>Data and code availability</title>
<p>All data and code used in this study are available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/zmt2g/?view_only=ac02a214e86846eead93729b3dc70e6d">https://osf.io/zmt2g/?view_only=ac02a214e86846eead93729b3dc70e6d</ext-link>.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We (Y. C. and Z. X.) performed the experiments while we were students in Dr. Shuguang Kuai’s laboratory at East China Normal University. We thank Dr. Kuai for his support (including the National Natural Science Foundation of China grants 31771209 and 3151160 to S. K.) and Dr. Greg DeAngelis for helpful comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Treisman</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Souther</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>Search asymmetry: a diagnostic for preattentive processing of separable features</article-title>. <source>Journal of Experimental Psychology: General</source> <volume>114</volume>, <fpage>285</fpage> (<year>1985</year>).</mixed-citation></ref>
<ref id="c2"><label>2</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Treisman</surname>, <given-names>A. M.</given-names></string-name> &amp; <string-name><surname>Gelade</surname>, <given-names>G</given-names></string-name></person-group>. <article-title>A feature-integration theory of attention</article-title>. <source>Cognitive psychology</source> <volume>12</volume>, <fpage>97</fpage>–<lpage>136</lpage> (<year>1980</year>).</mixed-citation></ref>
<ref id="c3"><label>3</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Egeth</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Dagenbach</surname>, <given-names>D</given-names></string-name></person-group>. <article-title>Parallel versus serial processing in visual search: further evidence from subadditive effects of visual quality</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source> <volume>17</volume>, <fpage>551</fpage> (<year>1991</year>).</mixed-citation></ref>
<ref id="c4"><label>4</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eckstein</surname>, <given-names>M. P</given-names></string-name></person-group>. <article-title>Visual search: A retrospective</article-title>. <source>Journal of vision</source> <volume>11</volume>, <fpage>14</fpage>–<lpage>14</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c5"><label>5</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolfe</surname>, <given-names>J. M</given-names></string-name></person-group>. <article-title>Approaches to visual search: Feature integration theory and guided search</article-title>. <source>The Oxford handbook of attention</source>, <fpage>11</fpage>–<lpage>55</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c6"><label>6</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolfe</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Alvarez</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Rosenholtz</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Kuzmova</surname>, <given-names>Y. I.</given-names></string-name> &amp; <string-name><surname>Sherman</surname>, <given-names>A. M</given-names></string-name></person-group>. <article-title>Visual search for arbitrary objects in real scenes</article-title>. <source>Attention, Perception, &amp; Psychophysics</source> <volume>73</volume>, <fpage>1650</fpage>–<lpage>1671</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c7"><label>7</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhaoping</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Frith</surname>, <given-names>U</given-names></string-name></person-group>. <article-title>A clash of bottom-up and top-down processes in visual search: The reversed letter effect revisited</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source> <volume>37</volume>, <fpage>997</fpage>–<lpage>1006</lpage> (<year>2011</year>). <pub-id pub-id-type="doi">10.1037/a0023099</pub-id></mixed-citation></ref>
<ref id="c8"><label>8</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>Z</given-names></string-name></person-group>. <article-title>Contextual influences in V1 as a basis for pop out and asymmetry in visual search</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>96</volume>, <fpage>10530</fpage>–<lpage>10535</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c9"><label>9</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolfe</surname>, <given-names>J. M</given-names></string-name></person-group>. <article-title>Guided Search 6.0: An updated model of visual search</article-title>. <source>Psychonomic bulletin &amp; review</source> <volume>28</volume>, <fpage>1060</fpage>–<lpage>1092</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c10"><label>10</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolfe</surname>, <given-names>J. M.</given-names></string-name> &amp; <string-name><surname>Horowitz</surname>, <given-names>T. S</given-names></string-name></person-group>. <article-title>Five factors that guide attention in visual search</article-title>. <source>Nature human behaviour</source> <volume>1</volume>, <fpage>0058</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c11"><label>11</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Josephs</surname>, <given-names>E. L.</given-names></string-name>, <string-name><surname>Draschkow</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Wolfe</surname>, <given-names>J. M.</given-names></string-name> &amp; <string-name><surname>Võ</surname>, <given-names>M. L.-H</given-names></string-name></person-group>. <article-title>Gist in time: Scene semantics and structure enhance recall of searched objects</article-title>. <source>Acta Psychologica</source> <volume>169</volume>, <fpage>100</fpage>–<lpage>108</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c12"><label>12</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beitner</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Helbing</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Draschkow</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Vo</surname>, <given-names>M. L.-H</given-names></string-name></person-group>. <article-title>Get your guidance going: Investigating the activation of spatial priors for efficient search in virtual reality</article-title>. <source>Brain Sciences</source> <volume>11</volume>, <fpage>44</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c13"><label>13</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lauer</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Võ</surname>, <given-names>M. L.-H</given-names></string-name></person-group>. <article-title>The ingredients of scenes that affect object search and perception</article-title>. <source>Human perception of visual information: Psychological and computational perspectives</source>, <fpage>1</fpage>–<lpage>32</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c14"><label>14</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Girshick</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Landy</surname>, <given-names>M. S.</given-names></string-name> &amp; <string-name><surname>Simoncelli</surname>, <given-names>E. P</given-names></string-name></person-group>. <article-title>Cardinal rules: visual orientation perception reflects knowledge of environmental statistics</article-title>. <source>Nature neuroscience</source> <volume>14</volume>, <fpage>926</fpage>–<lpage>932</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c15"><label>15</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Howard</surname>, <given-names>I. P</given-names></string-name></person-group>. <source>Human visual orientation</source>. (<publisher-name>John Wiley &amp; Sons</publisher-name>, <year>1982</year>).</mixed-citation></ref>
<ref id="c16"><label>16</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Appelle</surname>, <given-names>S</given-names></string-name></person-group>. <article-title>Perception and discrimination as a function of stimulus orientation: the” oblique effect” in man and animals</article-title>. <source>Psychological bulletin</source> <volume>78</volume>, <fpage>266</fpage> (<year>1972</year>).</mixed-citation></ref>
<ref id="c17"><label>17</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bülthoff</surname>, <given-names>H. H.</given-names></string-name>, <string-name><surname>Edelman</surname>, <given-names>S. Y.</given-names></string-name> &amp; <string-name><surname>Tarr</surname>, <given-names>M. J</given-names></string-name></person-group>. <article-title>How are three-dimensional objects represented in the brain?</article-title> <source>Cerebral Cortex</source> <volume>5</volume>, <fpage>247</fpage>–<lpage>260</lpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c18"><label>18</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Jolicoeur</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Humphrey</surname>, <given-names>G. K</given-names></string-name></person-group>. <chapter-title>Perception of rotated two-dimensional and three-dimensional objects and visual shapes</chapter-title>. <person-group person-group-type="editor"><string-name><surname>Walsh</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Kulikowski</surname>, <given-names>J.</given-names></string-name></person-group> <source>Perceptual constancy. Why things look as they do</source>, <fpage>69</fpage>–<lpage>123</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c19"><label>19</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lawson</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>Achieving visual object constancy across plane rotation and depth rotation</article-title>. <source>Acta psychologica</source> <volume>102</volume>, <fpage>221</fpage>–<lpage>245</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c20"><label>20</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name></person-group> <chapter-title>Visual object recognition: Can a single mechanism suffice?</chapter-title> (<year>2003</year>). In <person-group person-group-type="editor"><string-name><given-names>M. A.</given-names> <surname>Peterson</surname></string-name> &amp; <string-name><given-names>G.</given-names> <surname>Rhodes</surname></string-name></person-group> (Eds.), <source>Perception of faces, objects, and scenes: Analytic and holistic processes</source> (pp. <fpage>177</fpage>–<lpage>207</lpage>). <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref>
<ref id="c21"><label>21</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name> &amp; <string-name><surname>Bülthoff</surname>, <given-names>H. H</given-names></string-name></person-group>. <article-title>Image-based object recognition in man, monkey and machine</article-title>. <source>Cognition</source> <volume>67</volume>, <fpage>1</fpage>–<lpage>20</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c22"><label>22</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boutsen</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Marendaz</surname>, <given-names>C</given-names></string-name></person-group>. <article-title>Detection of shape orientation depends on salient axes of symmetry and elongation: Evidence from visual search</article-title>. <source>Perception &amp; Psychophysics</source> <volume>63</volume>, <fpage>404</fpage>–<lpage>422</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c23"><label>23</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jolicoeur</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>Orientation congruency effects in visual search</article-title>. <source>Canadian Journal of Psychology/Revue canadienne de psychologie</source> <volume>46</volume>, <fpage>280</fpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c24"><label>24</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>Z.-X.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Kuai</surname>, <given-names>S.-G</given-names></string-name></person-group>. <article-title>The human visual system estimates angle features in an internal reference frame: A computational and psychophysical study</article-title>. <source>Journal of vision</source> <volume>18</volume>, <fpage>10</fpage>–<lpage>10</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c25"><label>25</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Graf</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Coordinate transformations in object recognition</article-title>. <source>Psychological Bulletin</source> <volume>132</volume>, <fpage>920</fpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c26"><label>26</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shepard</surname>, <given-names>R. N.</given-names></string-name> &amp; <string-name><surname>Metzler</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>Mental rotation of three-dimensional objects</article-title>. <source>Science</source> <volume>171</volume>, <fpage>701</fpage>–<lpage>703</lpage> (<year>1971</year>).</mixed-citation></ref>
<ref id="c27"><label>27</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kosslyn</surname>, <given-names>S. M.</given-names></string-name></person-group> <article-title>Mental images and the brain</article-title>. <source>Cognitive neuropsychology</source> <volume>22</volume>, <fpage>333</fpage>–<lpage>347</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c28"><label>28</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Georgopoulos</surname>, <given-names>A. P</given-names></string-name></person-group>. <article-title>Neural aspects of cognitive motor control</article-title>. <source>Current opinion in neurobiology</source> <volume>10</volume>, <fpage>238</fpage>–<lpage>241</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c29"><label>29</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Georgopoulos</surname>, <given-names>A. P.</given-names></string-name>, <string-name><surname>Lurito</surname>, <given-names>J. T.</given-names></string-name>, <string-name><surname>Petrides</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Schwartz</surname>, <given-names>A. B.</given-names></string-name> &amp; <string-name><surname>Massey</surname>, <given-names>J. T</given-names></string-name></person-group>. <article-title>Mental rotation of the neuronal population vector</article-title>. <source>Science</source> <volume>243</volume>, <fpage>234</fpage>–<lpage>236</lpage> (<year>1989</year>).</mixed-citation></ref>
<ref id="c30"><label>30</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Salinas</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Abbott</surname>, <given-names>L. F</given-names></string-name></person-group>. <article-title>Coordinate transformations in the visual system: how to generate gain fields and what to compute with them</article-title>. <source>Prog Brain Res</source> <volume>130</volume>, <fpage>175</fpage>–<lpage>190</lpage> (<year>2001</year>). <pub-id pub-id-type="doi">10.1016/s0079-6123(01)30012-2</pub-id></mixed-citation></ref>
<ref id="c31"><label>31</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andersen</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Essick</surname>, <given-names>G. K.</given-names></string-name> &amp; <string-name><surname>Siegel</surname>, <given-names>R. M</given-names></string-name></person-group>. <article-title>Encoding of spatial location by posterior parietal neurons</article-title>. <source>Science</source> <volume>230</volume>, <fpage>456</fpage>–<lpage>458</lpage> (<year>1985</year>). <pub-id pub-id-type="doi">10.1126/science.4048942</pub-id></mixed-citation></ref>
<ref id="c32"><label>32</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pouget</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Sejnowski</surname>, <given-names>T. J</given-names></string-name></person-group>. <article-title>Spatial transformations in the parietal cortex using basis functions</article-title>. <source>Journal of cognitive neuroscience</source> <volume>9</volume>, <fpage>222</fpage>–<lpage>237</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c33"><label>33</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alexander</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Stern</surname>, <given-names>C. E.</given-names></string-name> &amp; <string-name><surname>Hasselmo</surname>, <given-names>M. E</given-names></string-name></person-group>. <article-title>Gated transformations from egocentric to allocentric reference frames involving retrosplenial cortex, entorhinal cortex, and hippocampus</article-title>. <source>Hippocampus</source> <volume>33</volume>, <fpage>465</fpage>–<lpage>487</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c34"><label>34</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sasaki</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Anzai</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Angelaki</surname>, <given-names>D. E.</given-names></string-name> &amp; <string-name><surname>DeAngelis</surname>, <given-names>G. C</given-names></string-name></person-group>. <article-title>Flexible coding of object motion in multiple reference frames by parietal cortex neurons</article-title>. <source>Nature neuroscience</source> <volume>23</volume>, <fpage>1004</fpage>–<lpage>1015</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c35"><label>35</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Palmer</surname>, <given-names>S. E.</given-names></string-name></person-group> in <source>Object perception</source> <fpage>121</fpage>–<lpage>163</lpage> (<publisher-name>Psychology Press</publisher-name>, <year>2013</year>).</mixed-citation></ref>
<ref id="c36"><label>36</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname>, <given-names>D. H.</given-names></string-name>, <string-name><surname>Harris</surname>, <given-names>L. R.</given-names></string-name> &amp; <string-name><surname>Troje</surname>, <given-names>N. F</given-names></string-name></person-group>. <article-title>Frames of reference for biological motion and face perception</article-title>. <source>Journal of Vision</source> <volume>10</volume>, <fpage>22</fpage>–<lpage>22</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c37"><label>37</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Galati</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Pelle</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Berthoz</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Committeri</surname>, <given-names>G</given-names></string-name></person-group>. <article-title>Multiple reference frames used by the human brain for spatial perception and memory</article-title>. <source>Experimental brain research</source> <volume>206</volume>, <fpage>109</fpage>–<lpage>120</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c38"><label>38</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Troje</surname>, <given-names>N. F</given-names></string-name></person-group>. <article-title>Reference frames for orientation anisotropies in face recognition and biological-motion perception</article-title>. <source>Perception</source> <volume>32</volume>, <fpage>201</fpage>–<lpage>210</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c39"><label>39</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andersen</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>L. H.</given-names></string-name>, <string-name><surname>Bradley</surname>, <given-names>D. C.</given-names></string-name> &amp; <string-name><surname>Crowell</surname>, <given-names>J. A</given-names></string-name></person-group>. <article-title>The contributions of vestibular signals to the representations of space in the posterior parietal cortex</article-title>. <source>Ann N Y Acad Sci</source> <volume>871</volume>, <fpage>282</fpage>–<lpage>292</lpage> (<year>1999</year>). <pub-id pub-id-type="doi">10.1111/j.1749-6632.1999.tb09192.x</pub-id></mixed-citation></ref>
<ref id="c40"><label>40</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Snyder</surname>, <given-names>L. H.</given-names></string-name>, <string-name><surname>Grieve</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Brotchie</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Andersen</surname>, <given-names>R. A</given-names></string-name></person-group>. <article-title>Separate body- and world-referenced representations of visual space in parietal cortex</article-title>. <source>Nature</source> <volume>394</volume>, <fpage>887</fpage>–<lpage>891</lpage> (<year>1998</year>). <pub-id pub-id-type="doi">10.1038/29777</pub-id></mixed-citation></ref>
<ref id="c41"><label>41</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marendaz</surname>, <given-names>C</given-names></string-name></person-group>. <article-title>Nature and dynamics of reference frames in visual search for orientation: Implications for early visual processing</article-title>. <source>Psychological Science</source> <volume>9</volume>, <fpage>27</fpage>–<lpage>32</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c42"><label>42</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brotchie</surname>, <given-names>P. R.</given-names></string-name>, <string-name><surname>Andersen</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>L. H.</given-names></string-name> &amp; <string-name><surname>Goodman</surname>, <given-names>S. J</given-names></string-name></person-group>. <article-title>Head position signals used by parietal neurons to encode locations of visual stimuli</article-title>. <source>Nature</source> <volume>375</volume>, <fpage>232</fpage>–<lpage>235</lpage> (<year>1995</year>). <pub-id pub-id-type="doi">10.1038/375232a0</pub-id></mixed-citation></ref>
<ref id="c43"><label>43</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rosenberg</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Angelaki</surname>, <given-names>D. E</given-names></string-name></person-group>. <article-title>Gravity influences the visual representation of object tilt in parietal cortex</article-title>. <source>Journal of Neuroscience</source> <volume>34</volume>, <fpage>14170</fpage>–<lpage>14180</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c44"><label>44</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>DeAngelis</surname>, <given-names>G. C.</given-names></string-name> &amp; <string-name><surname>Angelaki</surname>, <given-names>D. E</given-names></string-name></person-group>. <article-title>Flexible egocentric and allocentric representations of heading signals in parietal cortex</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>115</volume>, <fpage>E3305</fpage>–<lpage>E3312</lpage> (<year>2018</year>). <pub-id pub-id-type="doi">10.1073/pnas.1715625115</pub-id></mixed-citation></ref>
<ref id="c45"><label>45</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Deangelis</surname>, <given-names>G. C.</given-names></string-name> &amp; <string-name><surname>Angelaki</surname>, <given-names>D. E</given-names></string-name></person-group>. <article-title>Diverse spatial reference frames of vestibular signals in parietal cortex</article-title>. <source>Neuron</source> <volume>80</volume>, <fpage>1310</fpage>–<lpage>1321</lpage> (<year>2013</year>). <pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.006</pub-id></mixed-citation></ref>
<ref id="c46"><label>46</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Emonds</surname>, <given-names>A. M. X.</given-names></string-name>, <string-name><surname>Srinath</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Nielsen</surname>, <given-names>K. J.</given-names></string-name> &amp; <string-name><surname>Connor</surname>, <given-names>C. E</given-names></string-name></person-group>. <article-title>Object representation in a gravitational reference frame</article-title>. <source>eLife</source> <volume>12</volume> (<year>2023</year>). <pub-id pub-id-type="doi">10.7554/eLife.81701</pub-id></mixed-citation></ref>
<ref id="c47"><label>47</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vaziri</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Connor</surname>, <given-names>C. E</given-names></string-name></person-group>. <article-title>Representation of Gravity-Aligned Scene Structure in Ventral Pathway Visual Cortex</article-title>. <source>Curr Biol</source> <volume>26</volume>, <fpage>766</fpage>–<lpage>774</lpage> (<year>2016</year>). <pub-id pub-id-type="doi">10.1016/j.cub.2016.01.022</pub-id></mixed-citation></ref>
<ref id="c48"><label>48</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bock</surname>, <given-names>O. L.</given-names></string-name> &amp; <string-name><surname>Dalecki</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Mental rotation of letters, body parts and scenes during whole-body tilt: Role of a body-centered versus a gravitational reference frame</article-title>. <source>Human movement science</source> <volume>40</volume>, <fpage>352</fpage>–<lpage>358</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c49"><label>49</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mullette-Gillman</surname>, <given-names>O. D. A.</given-names></string-name>, <string-name><surname>Cohen</surname>, <given-names>Y. E.</given-names></string-name> &amp; <string-name><surname>Groh</surname>, <given-names>J. M</given-names></string-name></person-group>. <article-title>Motor-related signals in the intraparietal cortex encode locations in a hybrid, rather than eye-centered reference frame</article-title>. <source>Cerebral Cortex</source> <volume>19</volume>, <fpage>1761</fpage>–<lpage>1775</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c50"><label>50</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harris</surname>, <given-names>L. R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>How our body influences our perception of the world</article-title>. <source>Frontiers in psychology</source> <volume>6</volume>, <fpage>819</fpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c51"><label>51</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dyde</surname>, <given-names>R. T.</given-names></string-name>, <string-name><surname>Jenkin</surname>, <given-names>M. R.</given-names></string-name>, <string-name><surname>Jenkin</surname>, <given-names>H. L.</given-names></string-name>, <string-name><surname>Zacher</surname>, <given-names>J. E.</given-names></string-name> &amp; <string-name><surname>Harris</surname>, <given-names>L. R</given-names></string-name></person-group>. <article-title>The effect of altered gravity states on the perception of orientation</article-title>. <source>Experimental brain research</source> <volume>194</volume>, <fpage>647</fpage>–<lpage>660</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c52"><label>52</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dyde</surname>, <given-names>R. T.</given-names></string-name>, <string-name><surname>Jenkin</surname>, <given-names>M. R.</given-names></string-name> &amp; <string-name><surname>Harris</surname>, <given-names>L. R</given-names></string-name></person-group>. <article-title>The subjective visual vertical and the perceptual upright</article-title>. <source>Experimental Brain Research</source> <volume>173</volume>, <fpage>612</fpage>–<lpage>622</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c53"><label>53</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jenkin</surname>, <given-names>H. L.</given-names></string-name>, <string-name><surname>Jenkin</surname>, <given-names>M. R.</given-names></string-name>, <string-name><surname>Dyde</surname>, <given-names>R. T.</given-names></string-name> &amp; <string-name><surname>Harris</surname>, <given-names>L. R</given-names></string-name></person-group>. <article-title>Shape-from-shading depends on visual, gravitational, and body-orientation cues</article-title>. <source>Perception</source> <volume>33</volume>, <fpage>1453</fpage>–<lpage>1461</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c54"><label>54</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruthruff</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Lachmann</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>Does mental rotation require central mechanisms?</article-title> <source>Journal of Experimental Psychology: Human Perception and Performance</source> <volume>21</volume>, <fpage>552</fpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c55"><label>55</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gilchrist</surname>, <given-names>I. D.</given-names></string-name>, <string-name><surname>North</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Hood</surname>, <given-names>B</given-names></string-name></person-group>. <article-title>Is visual search really like foraging?</article-title> <source>Perception</source> <volume>30</volume>, <fpage>1459</fpage>–<lpage>1464</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c56"><label>56</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lauer</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Willenbockel</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Maffongelli</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Võ</surname>, <given-names>M. L.-H</given-names></string-name></person-group>. <article-title>The influence of scene and object orientation on the scene consistency effect</article-title>. <source>Behavioural Brain Research</source> <volume>394</volume>, <fpage>112812</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c57"><label>57</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harris</surname>, <given-names>L. R.</given-names></string-name>, <string-name><surname>Herpers</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Hofhammer</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Jenkin</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>How much gravity is needed to establish the perceptual upright?</article-title> <source>PLoS One</source> <volume>9</volume>, <fpage>e106207</fpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c58"><label>58</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>X.</given-names></string-name> &amp; <string-name><surname>Zelinsky</surname>, <given-names>G. J</given-names></string-name></person-group>. <article-title>Real-world visual search is dominated by top-down guidance</article-title>. <source>Vision research</source> <volume>46</volume>, <fpage>4118</fpage>–<lpage>4133</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c59"><label>59</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engel</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Glover</surname>, <given-names>G. H.</given-names></string-name> &amp; <string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name></person-group> <article-title>Retinotopic organization in human visual cortex and the spatial precision of functional MRI</article-title>. <source>Cerebral cortex (New York, NY: 1991)</source> <volume>7</volume>, <fpage>181</fpage>–<lpage>192</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c60"><label>60</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tusa</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Palmer</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Rosenquist</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>The retinotopic organization of area 17 (striate cortex) in the cat</article-title>. <source>Journal of Comparative Neurology</source> <volume>177</volume>, <fpage>213</fpage>–<lpage>235</lpage> (<year>1978</year>).</mixed-citation></ref>
<ref id="c61"><label>61</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hubel</surname>, <given-names>D. H.</given-names></string-name> &amp; <string-name><surname>Wiesel</surname>, <given-names>T. N</given-names></string-name></person-group>. <article-title>Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex</article-title>. <source>The Journal of physiology</source> <volume>160</volume>, <fpage>106</fpage> (<year>1962</year>).</mixed-citation></ref>
<ref id="c62"><label>62</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>Z.-X.</given-names></string-name> &amp; <string-name><surname>DeAngelis</surname>, <given-names>G. C</given-names></string-name></person-group>. <article-title>Neural mechanism for coding depth from motion parallax in area MT: gain modulation or tuning shifts?</article-title> <source>Journal of Neuroscience</source> <volume>42</volume>, <fpage>1235</fpage>–<lpage>1253</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c63"><label>63</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parker</surname>, <given-names>P. R.</given-names></string-name>, <string-name><surname>Brown</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Smear</surname>, <given-names>M. C.</given-names></string-name> &amp; <string-name><surname>Niell</surname>, <given-names>C. M</given-names></string-name></person-group>. <article-title>Movement-related signals in sensory areas: roles in natural behavior</article-title>. <source>Trends in neurosciences</source> <volume>43</volume>, <fpage>581</fpage>–<lpage>595</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c64"><label>64</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Warren</surname>, <given-names>P. A.</given-names></string-name> &amp; <string-name><surname>Rushton</surname>, <given-names>S. K</given-names></string-name></person-group>. <article-title>Optic flow processing for the assessment of object movement during ego movement</article-title>. <source>Current Biology</source> <volume>19</volume>, <fpage>1555</fpage>–<lpage>1560</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c65"><label>65</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niehorster</surname>, <given-names>D. C.</given-names></string-name> &amp; <string-name><surname>Li</surname>, <given-names>L</given-names></string-name></person-group>. <article-title>Accuracy and tuning of flow parsing for visual perception of object motion during self-motion</article-title>. <source>i-Perception</source> <volume>8</volume>, <fpage>2041669517708206</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c66"><label>66</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Inaba</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Shinomoto</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Yamane</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Takemura</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Kawano</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>MST neurons code for visual motion in space independent of pursuit eye movements</article-title>. <source>J Neurophysiol</source> <volume>97</volume>, <fpage>3473</fpage>–<lpage>3483</lpage> (<year>2007</year>). <pub-id pub-id-type="doi">10.1152/jn.01054.2006</pub-id></mixed-citation></ref>
<ref id="c67"><label>67</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>Z.-X.</given-names></string-name>, <string-name><surname>Pang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Anzai</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>DeAngelis</surname>, <given-names>G. C</given-names></string-name></person-group>. <article-title>Viewing geometry drives flexible perception of object motion and depth</article-title>. <source>bioRxiv</source>, 2024.2010. 2029.620928 (<year>2024</year>).</mixed-citation></ref>
<ref id="c68"><label>68</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morris</surname>, <given-names>A. P.</given-names></string-name> &amp; <string-name><surname>Krekelberg</surname>, <given-names>B</given-names></string-name></person-group>. <article-title>A stable visual world in primate primary visual cortex</article-title>. <source>Current Biology</source> <volume>29</volume>, <fpage>1471</fpage>–<lpage>1480.e1476</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c69"><label>69</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parker</surname>, <given-names>P. R.</given-names></string-name>, <string-name><surname>Abe</surname>, <given-names>E. T.</given-names></string-name>, <string-name><surname>Leonard</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Martins</surname>, <given-names>D. M.</given-names></string-name> &amp; <string-name><surname>Niell</surname>, <given-names>C. M</given-names></string-name></person-group>. <article-title>Joint coding of visual input and eye/head position in V1 of freely moving mice</article-title>. <source>Neuron</source> <volume>110</volume>, <fpage>3897</fpage>–<lpage>3906.e3895</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c70"><label>70</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miura</surname>, <given-names>S. K.</given-names></string-name> &amp; <string-name><surname>Scanziani</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Distinguishing externally from saccade-induced motion in visual cortex</article-title>. <source>Nature</source> <volume>610</volume>, <fpage>135</fpage>–<lpage>142</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c71"><label>71</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andersen</surname>, <given-names>R. A</given-names></string-name></person-group>. <article-title>Visual and eye movement functions of the posterior parietal cortex</article-title>. <source>Annu Rev Neurosci</source> <volume>12</volume>, <fpage>377</fpage>–<lpage>403</lpage> (<year>1989</year>). <pub-id pub-id-type="doi">10.1146/annurev.ne.12.030189.002113</pub-id></mixed-citation></ref>
<ref id="c72"><label>72</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peltier</surname>, <given-names>N. E.</given-names></string-name>, <string-name><surname>Anzai</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Moreno-Bote</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>DeAngelis</surname>, <given-names>G. C</given-names></string-name></person-group>. <article-title>A neural mechanism for optic flow parsing in macaque visual cortex</article-title>. <source>Current Biology</source> <volume>34</volume>, <fpage>4983</fpage>–<lpage>4997.e4989</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c73"><label>73</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>Z. X.</given-names></string-name> &amp; <string-name><surname>DeAngelis</surname>, <given-names>G. C</given-names></string-name></person-group>. <article-title>Seeing a Three-Dimensional World in Motion: How the Brain Computes Object Motion and Depth During Self-Motion</article-title>. <source>Annu Rev Vis Sci</source> <volume>11</volume> (<year>2025</year>). <pub-id pub-id-type="doi">10.1146/annurev-vision-110323-112124</pub-id></mixed-citation></ref>
<ref id="c74"><label>74</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mullette-Gillman</surname>, <given-names>O. D. A.</given-names></string-name>, <string-name><surname>Cohen</surname>, <given-names>Y. E.</given-names></string-name> &amp; <string-name><surname>Groh</surname>, <given-names>J. M</given-names></string-name></person-group>. <article-title>Eye-centered, head-centered, and complex coding of visual and auditory targets in the intraparietal sulcus</article-title>. <source>Journal of neurophysiology</source> <volume>94</volume>, <fpage>2331</fpage>–<lpage>2352</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c75"><label>75</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Connor</surname>, <given-names>C. E.</given-names></string-name> &amp; <string-name><surname>Knierim</surname>, <given-names>J. J</given-names></string-name></person-group>. <article-title>Integration of objects and space in perception and memory</article-title>. <source>Nat Neurosci</source> <volume>20</volume>, <fpage>1493</fpage>–<lpage>1503</lpage> (<year>2017</year>). <pub-id pub-id-type="doi">10.1038/nn.4657</pub-id></mixed-citation></ref>
<ref id="c76"><label>76</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Angelaki</surname>, <given-names>D. E.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A gravity-based three-dimensional compass in the mouse brain</article-title>. <source>Nature communications</source> <volume>11</volume>, <fpage>1855</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c77"><label>77</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laurens</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Dickman</surname>, <given-names>J. D.</given-names></string-name> &amp; <string-name><surname>Angelaki</surname>, <given-names>D. E</given-names></string-name></person-group>. <article-title>Gravity orientation tuning in macaque anterior thalamus</article-title>. <source>Nature neuroscience</source> <volume>19</volume>, <fpage>1566</fpage>–<lpage>1568</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c78"><label>78</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laurens</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Meng</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Angelaki</surname>, <given-names>D. E</given-names></string-name></person-group>. <article-title>Neural representation of orientation relative to gravity in the macaque cerebellum</article-title>. <source>Neuron</source> <volume>80</volume>, <fpage>1508</fpage>–<lpage>1518</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c79"><label>79</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marendaz</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Stivalet</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Barraclough</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Walkowiac</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>Effect of gravitational cues on visual search for orientation</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source> <volume>19</volume>, <fpage>1266</fpage> (<year>1993</year>).</mixed-citation></ref>
<ref id="c80"><label>80</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kingma</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Stegeman</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Vogels</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>Ocular torsion induced by static and dynamic visual stimulation and static whole body roll</article-title>. <source>European Archives of Oto-rhino-laryngology</source> <volume>254</volume>, <fpage>S61</fpage>–<lpage>S63</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c81"><label>81</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whelan</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>Effective analysis of reaction time data</article-title>. <source>The psychological record</source> <volume>58</volume>, <fpage>475</fpage>–<lpage>482</lpage> (<year>2008</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108310.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>SP</surname>
<given-names>Arun</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/04dese585</institution-id><institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study shows that visual search for upright and rotated objects is affected by rotating participants in a VR and gravitational reference frame. However, the evidence supporting this conclusion is <bold>incomplete</bold>, given the authors' use of normalized response time and the assumption that object recognition across rotations requires mental rotation.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108310.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The current study sought to understand which reference frames humans use when doing visual search in naturalistic conditions. To this end, they had participants do a visual search task in a VR environment while manipulating factors such as object orientation, body orientation, gravitational cues, and visual context (where the ground is). They generally found that all cues contributed to participants' performance, but visual context and gravitational cues impacted performance the most, suggesting that participants represent space in an allocentric reference frame during visual search.</p>
<p>Strengths:</p>
<p>The study is valuable in that it sheds light on which cues participants use during visual search. Moreover, I appreciate the use of VR and precise psychophysical predictions (e.g., slope vs. intercept) to dissociate between possible reference frames.</p>
<p>Weaknesses:</p>
<p>It's not clear what the implications of the study are beyond visual search. Moreover, I have some concerns about the interpretation of Experiment 1, which relies on an incorrect interpretation of mental rotation. Thus, most of the conclusions rely on Experiment 2, which has a small sample size (n = 10). Finally, the statistical analyses could be strengthened with measures of effect size and non-parametric statistics.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108310.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper addresses an interesting issue: how is the search for a visual target affected by its orientation (and the viewer's) relative to other items in the scene and gravity? The paper describes a series of visual search tasks, using recognizable targets (e.g., a cat) positioned within a natural scene. Reaction times and accuracy at determining whether the target was present or absent, trial-to-trial, were measured as the target's orientation, that of the context, and of the viewer themselves (via rotation in a flight simulator) were manipulated. The paper concludes that search is substantially affected by these manipulations, primarily by the reference frame of gravity, then visual context, followed by the egocentric reference frame.</p>
<p>Strengths:</p>
<p>This work is on an interesting topic, and benefits from using natural stimuli in VR / flight simulator to change participants' POV and body position.</p>
<p>Weaknesses:</p>
<p>There are several areas of weakness that I feel should be addressed.</p>
<p>(1) The literature review/introduction seems to be lacking in some areas. The authors, when contemplating the behavioral consequences of searching for a 'rotated' target, immediately frame the problem as one of rotation, per se (i.e., contrasting only rotation-based explanations; &quot;what rotates and in which 'reference frame[s]' in order to allow for successful search?&quot;). For a reader not already committed to this framing, many natural questions arise that are worth addressing.</p>
<p>1a) Why do we need to appeal to rotation at all as opposed to, say, familiarity? A rotated cat is less familiar than a typically oriented one. This is a long-standing literature (e.g., Wang, Cavanagh, and Green (1994)), of course, with a lot to unpack.</p>
<p>1b) What are the triggers for the 'corrective' rotation that presumably brings reference frames back into alignment? What if the rotation had not been so obvious (i.e. for a target that may not have a typical orientation, like a hand, or a ball, or a learned, nonsense object?) or the background had not had such clear orientation (like a cluttered non-naturalistic background of or a naturalistic backdrop, but viewed from an unfamiliar POV (e.g., from above) or a naturalistic background, but not all of the elements were rotated)? What, ultimately, is rotated? The entire visual field? Does that mean that searching for multiple targets at different angles of rotation would interfere with one another?</p>
<p>1c) Relatedly, what is the process by which the visual system comes to know the 'correct' rotation? (Or, alternatively, is 'triggered to realize' that there is a rotation in play?) Is this something that needs to be learned? Is it only learned developmentally, through exposure to gravity? Could it be learned in the context of an experiment that starts with unfamiliar stimuli?</p>
<p>1d) Why the appeal to natural images? I appreciate any time a study can be moved from potentially too stripped-down laboratory conditions to more naturalistic ones, but is this necessary in the present case? Would the pattern of results have been different if these were typical laboratory 'visual search' displays of disconnected object arrays?</p>
<p>1e) How should we reconcile rotation-based theories of 'rotated-object' search with visual search results from zero gravity environments (e.g., for a review, see Leone (1998))?</p>
<p>1f) How should we reconcile the current manipulations with other viewpoint-perspective manipulations (e.g., Zhang &amp; Pan (2022))?</p>
<p>(2) The presentation/interpretation of results would benefit from more elaboration and justification.</p>
<p>2a) All of the current interpretations rely on just the RT data. First, the RT results should also be presented in natural units (i.e., seconds/ms), not normalized. As well, results should be shown as violin plots or something similar that captures distribution - a lot of important information is lost when just presenting one 'average' dot across participants. More fundamentally, I think we need to have a better accounting for performance (percent correct or d') to help contextualize the RT results. We should at least be offered some visualization (Heitz, 2014) of the speed accuracy trade-off for each of the conditions. Following this, the authors should more critically evaluate how any substantial SAT trends could affect the interpretation of results.</p>
<p>2b) Unless I am missing something, the interpretation of the pattern of results (both qualitatively and quantitatively in their 'relative weight' analysis) relies on how they draw their contrasts. For instance, the authors contrast the two 'gravitational' conditions (target 0 deg versus target 90 deg) as if this were a change in a single variable/factor. But there are other ways to understand these manipulations that would affect contrasts. For instance, if one considers whether the target was 'consistent' (i.e., typically oriented) with respect to the context, egocentric, and gravitational frames, then the 'gravitational 0 deg' condition is consistent with context, egocentric view, but inconsistent with gravity. And, the 'gravitational 90 deg' condition, then, is inconsistent with context, egocentric view, but consistent with gravity. Seen this way, this is not a change in one variable, but three. The same is true of the baseline 0 deg versus baseline 90 deg condition, where again we have a change in all three target-consistency variables. The 'one variable' manipulations then would be: 1) baseline 0 versus visual context 0 (i.e., a change only in the context variable); 2) baseline 0 versus egocentric 0 (a change only in the egocentric variable); and 3) baseline 0 versus gravitational 0 (a change only in the gravitational variable). Other contrasts (e.g., gravitational 90 versus context 90) would showcase a change in two variables (in this case, a change in both context and gravity). My larger point is, again, unless I am really missing something, that the choice of how to contrast the manipulations will affect the 'pattern' of results and thereby the interpretation. If the authors agree, this needs to be acknowledged, plausible alternative schemes discussed, and the ultimate choice of scheme defended as the most valid.</p>
<p>2c) Even with this 'relative weight' interpretation, there are still some patterns of results that seem hard to account for. Primarily, the egocentric condition seems hard to account for under any scheme, and the authors need to spend more time discussing/reconciling those results.</p>
<p>2d) Some results are just deeply counterintuitive, and so the reader will crave further discussion. Most saliently for me, based on the results of Experiment 2 (specifically, the fact that gravitational 90 had better performance than gravitational 0), designers of cockpits should have all gauges/displays rotate counter to the airplane so that they are always consistent with gravity, not the pilot. Is this indeed a fair implication of the results?</p>
<p>2e) I really craved some 'control conditions' here to help frame the current results. In keeping with the rhetorical questions posed above in 1a/b/c/d, if/when the authors engage with revisions to this paper, I would encourage the inclusion of at least some new empirical results. For me the most critical would be to repeat some core conditions, but with a symmetric target (e.g. a ball) since that would seem to be the only way (given the current design) to tease out nuisance confounding factors such as, say, the general effect of performing search while sideways (put another way, the authors would have to assume here that search (non-normalized RT's and search performance) for a ball-target in the baseline condition would be identical to that in the gravitational condition.)</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108310.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The study tested how people search for objects in natural scenes using virtual reality. Participants had to find targets among other objects, shown upright or tilted. The main results showed that upright objects were found faster and more accurately. When the scene or body was rotated, performance changed, showing that people use cues from the environment and gravity to guide search.</p>
<p>The manuscript is clearly written and well designed, but there are some aspects related to methods and analyses that would benefit from stronger support.</p>
<p>First, the sample size is not justified with a power analysis, nor is it explained how it was determined. This is an important point to ensure robustness and replicability.</p>
<p>Second, the reaction time data were processed using different procedures, such as the use of the median to exclude outliers and an ad hoc cut-off of 50 ms. These choices are not sufficiently supported by a theoretical rationale, and could appear as post-hoc decisions.</p>
<p>Third, the mixed-model analyses are overall well-conducted; however, the specification of the random structure deserves further consideration. The authors included random intercepts for participants and object categories, which is appropriate. However, they did not include random slopes (e.g., for orientation or set size), meaning that variability in these effects across participants was not modelled. This simplification can make the models more stable, but it departs from the maximal random structure recommended by Barr et al. (2013). The authors do not explicitly justify this choice, and a reviewer may question why participant-specific variability in orientation effects, for example, was not allowed. Given the modest sample sizes (20 in Experiment 1 and 10 in Experiment 2), convergence problems with more complex models are likely. Nonetheless, ignoring random slopes can, in principle, inflate Type I error rates, so this issue should at least be acknowledged and discussed.</p>
</body>
</sub-article>
</article>