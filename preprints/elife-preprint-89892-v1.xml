<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">89892</article-id>
<article-id pub-id-type="doi">10.7554/eLife.89892</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89892.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Ecology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Unsupervised discovery of family specific vocal usage in the Mongolian gerbil</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Peterson</surname>
<given-names>Ralph E</given-names>
</name>
<xref ref-type="corresp" rid="cor1">*</xref>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Choudhri</surname>
<given-names>Aman</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Mitelut</surname>
<given-names>Catalin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Tanelus</surname>
<given-names>Aramis</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Capo-Battaglia</surname>
<given-names>Athena</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Williams</surname>
<given-names>Alex H</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Schneider</surname>
<given-names>David M</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sanes</surname>
<given-names>Dan H</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Center for Neural Science, New York University</institution>, New York, NY</aff>
<aff id="a2"><label>2</label><institution>Columbia University</institution>, New York, NY</aff>
<aff id="a3"><label>3</label><institution>Center for Computational Neuroscience</institution>, Flatiron Institute, New York, NY</aff>
<aff id="a4"><label>4</label><institution>Department of Psychology, New York University</institution>, New York, NY</aff>
<aff id="a5"><label>5</label><institution>Department of Biology, New York University</institution>, New York, NY</aff>
<aff id="a6"><label>6</label><institution>Neuroscience Institute, New York University School of Medicine</institution>, New York, NY</aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Takahashi</surname>
<given-names>Daniel Y</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Federal University of Rio Grande do Norte</institution>
</institution-wrap>
<city>Natal</city>
<country>Brazil</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>For correspondence: <email>rep359@nyu.edu</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-08-31">
<day>31</day>
<month>08</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP89892</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-06-26">
<day>26</day>
<month>06</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-03-13">
<day>13</day>
<month>03</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.11.532197"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Peterson et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Peterson et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-89892-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Many animal species use vocalizations to communicate social information and previous experiments in rodents have identified a range of vocal types that may be used for this purpose. However, social vocalizations are typically acquired during brief interactions between animals with no prior social relationship, and under environmental conditions with limited ethological relevance. Here, we establish long-term acoustic recordings from Mongolian gerbil families, a core social group that uses an array of sonic and ultrasonic vocalizations which vary with social context. Three separate gerbil families (two parents and four pups) were transferred to an enlarged environment and continuous 20-day audio recordings were obtained. We leveraged deep-learning based unsupervised analysis of 583,237 vocalizations to show that gerbils exhibit a more complex vocal repertoire than has been previously reported. Furthermore, gerbils displayed family-specific vocal repertoires, including differences in vocal type usage and transitions. Since gerbils live naturally as extended families in complex underground burrows that are adjacent to other families, these results suggest the presence of a vocal dialect which could be exploited by animals to represent kinship.</p>
<p>These findings offer insight into the naturalistic vocal tendencies of gerbil families and position the Mongolian gerbil as a compelling animal to study the neural basis of vocal communication.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The field of ethology contains rich descriptions of complex behavioral actions, including a wealth of species-specific vocal repertoires. However, natural observations are often incomplete due to limitations in physical access for experimenter observation or behavioral recording. This can be particularly severe for family behaviors which occur in protected or remote environments, such as burrows in the case of fossorial rodent species like naked mole-rats and Mongolian gerbils (<xref ref-type="bibr" rid="c7">Brett 1986</xref>; Scheibler 2006). Some of these limitations have been addressed with laboratory environments that partially recapitulate real-world features (<xref ref-type="bibr" rid="c62">Shemesh &amp; Chen 2023</xref>). However, these studies generally focused on relatively short periods of data collection that consider single animals or dyads with no prior relationship.</p>
<p>While our understanding of social aural communication is sparse, even for humans (<xref ref-type="bibr" rid="c46">Pagel et al., 2013</xref>; Mascar et al., 2018; <xref ref-type="bibr" rid="c60">Schindler et al., 2022</xref>), we know that many vocal cues are learned through social experience, and provide pivotal information about an animal’s identity. For example, a human infant’s ability to discriminate between foreign language phonemes can be preserved by exposure to a live foreign speaker, but not an audiovisual recording (<xref ref-type="bibr" rid="c38">Kuhl et al., 2003</xref>). Even some rodents, such as the naked mole rat, model specific calls based on early social experience (<xref ref-type="bibr" rid="c6">Barker et al., 2021</xref>). The literature for social facilitation of vocal discrimination or production is particularly strong for zebra finches (<xref ref-type="bibr" rid="c18">Eales, 1989</xref>; <xref ref-type="bibr" rid="c16">Derégnaucourt et al., 2013</xref>; <xref ref-type="bibr" rid="c13">Chen et al., 2016</xref>; <xref ref-type="bibr" rid="c44">Narula et al., 2018</xref>). In fact, exposure to a live singing tutor, but not song playback, selectively activates dopamine neurons in the juvenile periaqueductal gray which is thought to mediate learning (<xref ref-type="bibr" rid="c65">Tanaka et al., 2018</xref>). Therefore, our study considers the possibility that there is a diversity of vocalizations within the gerbil family social group.</p>
<p>We chose to focus on families, arguably the canonical social group, which has been predominantly studied during brief and experimentally restricted social encounters (e.g., mating, pup retrieval, aggression) in relatively sterile environments. Our goal was to construct a complete gerbil family social-vocal soundscape during a significant period of development under undisturbed conditions. Unlike many laboratory rodents, gerbils form pair bonds and maintain a family structure across generations. These families are composed of a founding adult pair, and up to 15 extended family members that live cooperatively in underground burrows (Ågren et al., 1989a; Ågren et al., 1989b; <xref ref-type="bibr" rid="c42">Milne-Edwards, 1867</xref>; <xref ref-type="bibr" rid="c58">Scheibler et al., 2004</xref>). Given the darkness and complexity of their burrow systems, gerbils must rely heavily on their auditory system for survival. Sibling bonds established through adolescence facilitate social structure and minimize inbreeding (Ågren, 1984a). Natural burrows are found in multi-family neighborhoods with strictly enforced territorial boundaries (<xref ref-type="bibr" rid="c59">Scheibler et al., 2006</xref>; Ågren et al., 1989a, Ågren et al., 1989b). Like prairie voles, gerbils act cooperatively to hoard food, maintain nests, defend their territory, and care for pups (<xref ref-type="bibr" rid="c20">Elwood, 1975</xref>; <xref ref-type="bibr" rid="c29">Gromov, 2021</xref>).</p>
<p>Therefore, gerbils display a range of rodent-typical behaviors (<xref ref-type="bibr" rid="c33">Hurtado-Parrado et al., 2017</xref>), as well as complex family behaviors. Gerbils also display significant vocal communication in both the ultrasonic and sonic ranges (<xref ref-type="bibr" rid="c66">Ter-Mikaelian et al., 2012</xref>, <xref ref-type="bibr" rid="c37">Kobayasi &amp; Riquimaroux, 2012</xref>) which is likely to be integral to social behaviors. Unlike many other rodent species, gerbils are able to hear within sonic ranges at sensitivities similar to humans (<xref ref-type="bibr" rid="c53">Ryan, 1976</xref>). As a result, there is a rich, contemporary literature on the auditory perceptual skills, peripheral and central physiology, central anatomy, learning, and genomics in this species (<xref ref-type="bibr" rid="c8">Budinger and Scheich, 2009</xref>; <xref ref-type="bibr" rid="c9">Buran et al., 2014</xref>; <xref ref-type="bibr" rid="c30">Happel et al., 2014</xref>; <xref ref-type="bibr" rid="c43">Myoga et al., 2014</xref>; <xref ref-type="bibr" rid="c45">Pachitariu et al., 2015</xref>; <xref ref-type="bibr" rid="c57">Sarro et al., 2015</xref>; <xref ref-type="bibr" rid="c67">von Trapp et al., 2016</xref>; <xref ref-type="bibr" rid="c10">Caras and Sanes, 2017</xref>; <xref ref-type="bibr" rid="c14">Cheng et al., 2019</xref>; <xref ref-type="bibr" rid="c71">Zorio et al., 2019</xref>; <xref ref-type="bibr" rid="c70">Yao et al., 2020</xref>; <xref ref-type="bibr" rid="c5">Amaro et al., 2021</xref>; <xref ref-type="bibr" rid="c47">Paraouty et al., 2021</xref>; <xref ref-type="bibr" rid="c69">Yao and Sanes, 2021</xref>; <xref ref-type="bibr" rid="c56">Saldeitis et al., 2022</xref>; <xref ref-type="bibr" rid="c48">Penikis and Sanes, 2023</xref>).</p>
<p>Here, we made continuous 20-day audio recordings from three separate gerbil families (2 parents, 4 pups) in an enlarged home cage that was isolated from other gerbils and humans. Specifically, we recorded audio over a period beginning when pup auditory cortex is particularly sensitive to acoustic experience and extending to the time when animals are typically weaned (postnatal days 14-34). Our goal was to acquire a descriptive dataset of the spectrotemporal structure of vocalizations emitted throughout daily family life. Using emerging methods in unsupervised vocalization analysis, we quantitatively describe the spectrotemporal structure of vocalizations over multiple timescales and demonstrate that vocal repertoire usage differs between families.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Longitudinal familial audio recording</title>
<p>We obtained acoustic recordings (four microphones, 125 kHz sampling rate) from three separate gerbil families, each containing two adults and four pups (<bold><xref rid="fig1" ref-type="fig">Figure 1A</xref></bold>). Continuous recordings began at P14, lasted 20 days, and pups were weaned (P32) two days prior to recording end (<bold><xref rid="fig1" ref-type="fig">Figure 1B</xref></bold>). As shown in <bold><xref rid="fig1" ref-type="fig">Figure 1C</xref></bold>, we extracted all sound events (yellow) using amplitude thresholding of acoustic power. To isolate vocalizations (blue) from non-vocal sounds (red), we computed the spectral flatness of each sound event and classified sounds with a threshold value of &lt;0.3 as vocalizations. A similar approach has previously been used in mice (<xref ref-type="bibr" rid="c11">Castellucci et al., 2016</xref>), and we verified that a threshold value of 0.3 minimized the number of false positives (<bold><xref rid="figS1" ref-type="fig">Figure S1</xref></bold>). Using this approach, 10,267,972 sound events were extracted, containing 583,237 vocalizations and 9,684,735 non-vocal sounds detected across the three families.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Longitudinal familial audio recording.</title>
<p>(A) Recording apparatus. Four ultrasonic microphones sampled at 125 kHz continuously recorded a family in an enlarged environment. (B) Experiment timeline. Three gerbil families with the same family composition (2 adults, 4 pups) were recorded continuously for 20 days. (C) Extraction of sound events from raw audio using sound amplitude thresholding (Gray threshold = “th_2”, black threshold = “th_1” and “th_3”; see Methods). Vocalizations (n=583,237) are separated from non-vocal sounds (n=9,684,735) using a threshold on spectral flatness (<bold><xref ref-type="fig" rid="figS1">Figure S1</xref></bold>, see methods). (D) Summary of total sound event emission and average emission per hour. (E) Proportion of all sound events that are vocal or non-vocal sounds. (F) Summary of total vocalization emission and average emission per hour.</p></caption>
<graphic xlink:href="532197v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Sound events were produced at an average rate of 6,726 +/-1,260 times per hour (<bold><xref rid="fig1" ref-type="fig">Figure 1D</xref></bold>), which reveals the rate of auditory object processing (Griffiths &amp; Warren, 2004) in a naturalistic setting. Vocalizations represent 6.99 +/-3.07% of all sound events over the recording period (<bold><xref rid="fig1" ref-type="fig">Figure 1E</xref></bold>) and were emitted at an average rate of 405 +/-103 times per hour (<bold><xref rid="fig1" ref-type="fig">Figure 1F</xref>)</bold>.</p>
</sec>
<sec id="s2b">
<title>Unsupervised discovery of the Mongolian gerbil vocal repertoire</title>
<p>To quantify the full array of vocalizations obtained from the three families, we trained a variational autoencoder (VAE) on vocalization spectrograms. The VAE learned a low-dimensional representation of latent acoustic features, thereby enabling analysis of such a large dataset. <bold><xref rid="fig2" ref-type="fig">Figure 2A</xref></bold> shows a schematic of the VAE architecture used (Goffinet et al., 2021), where spectrograms (top; 128×128 pixels) are reduced via a deep convolutional neural network “encoder” to a latent vector (middle; 32-dimensional). A deep convolutional neural network “decoder” then reconstructs a spectrogram (bottom) from the 32-dimensional latent representation. The encoder/decoder networks are jointly trained to minimize the discrepancy between the original and reconstructed spectrograms (<bold><xref rid="figS2" ref-type="fig">Figure S2A-B</xref></bold>), resulting in a low-dimensional latent representation, or “code”, which depicts each vocalization. To cluster vocalizations into distinct categories, we trained a Gaussian Mixture Model (GMM) on VAE latent representations. Using a combination of held-out log likelihood and manual inspection, we selected a model with 70 vocal clusters as a parsimonious description of the data (<bold><xref rid="figS2" ref-type="fig">Figure S2C</xref></bold>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Unsupervised discovery of the Mongolian gerbil vocal repertoire.</title>
<p>Variational autoencoder and clustering. (A) Vocalization spectrograms (top) are input to a variational autoencoder (VAE) which encodes the spectrogram as a 32-D set of latent features (middle). The VAE learns latent features by minimizing the difference between original spectrograms and spectrograms reconstructed from the latent features by the VAE decoder (bottom). A gaussian mixture model (GMM) was trained on the latent features to cluster vocalizations into discrete categories. (B) Representative vocalizations from 12 distinct GMM clusters featuring monosyllabic vocalizations are shown surrounding a UMAP embedding of the latent features. Asterisk denotes vocal type not previously characterized. (C) Examples of multisyllabic vocalizations. White vertical lines indicate boundaries of monosyllabic elements. Asterisks denote multisyllabic vocal types not previously characterized.</p></caption>
<graphic xlink:href="532197v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><bold><xref rid="fig2" ref-type="fig">Figure 2B</xref></bold> shows a UMAP embedding of the VAE latents (center), used for visualization purposes only, which demonstrates that the gerbil vocal repertoire is more discrete than mouse, yet less discrete than zebra finch (<xref ref-type="bibr" rid="c54">Sainburg et al., 2020</xref>; Goffinet et al., 2021). Representative examples from 12 monosyllabic vocalization clusters are shown with their relative position in UMAP space, one of which appears similar in form to naked mole rat family specific chirp (blue box with asterisk; <xref ref-type="bibr" rid="c6">Barker et al., 2021</xref>).</p>
<p>Furthermore, monosyllabic vocalizations (56/70 vocal clusters) can be flexibly strung together to create multisyllabic or “composite” vocalizations (9/70 of vocal clusters; <xref ref-type="bibr" rid="c37">Kobayasi &amp; Riquimaroux, 2012</xref>). The remaining 5 clusters contained a mixture of monosyllabic and multisyllabic vocalizations. <bold><xref rid="fig2" ref-type="fig">Figure 2C</xref></bold> shows 8 examples of multisyllabic vocalizations and their monosyllabic component boundaries, some of which have been reported previously (<xref ref-type="bibr" rid="c37">Kobayasi &amp; Riquimaroux 2012</xref>) and some of which are newly characterized (white asterisks). To assess how family structure influences vocal repertoire usage, we compared vocal usage one day prior and one day after pup weaning, showing a drastic decrease in vocal emission (<bold><xref rid="figS3" ref-type="fig">Figure S3A</xref></bold>). A large-magnitude vocal repertoire change is also observed, with the repertoire confined to a small region of vocal space following weaning (<bold><xref rid="figS3" ref-type="fig">Figure S3B-D</xref></bold>).</p>
</sec>
<sec id="s2c">
<title>Family specific usage of vocal clusters</title>
<p>We next asked whether different gerbil families emit different vocalizations during their day-to-day communication. First, we visualized the vocal repertoire usage of each family as a probability density heatmap and determined that vocal repertoire usage significantly differed between families (<bold><xref rid="fig3" ref-type="fig">Figure 3A</xref>, <xref rid="figS2" ref-type="fig">Figure S2D</xref></bold>). Next, using GMM vocalization clusters, we compared the proportion usage of each vocal cluster for the three families, revealing specific vocal cluster differences between families (<bold><xref rid="fig3" ref-type="fig">Figure 3B</xref></bold>). All families used each of the 70 vocal types (i.e. no cluster usage is 0), but different families relied more heavily on some clusters compared to others. Sorting the GMM cluster labels by the pairwise difference in vocal type usage between families revealed which vocal types differed most (<bold><xref rid="fig3" ref-type="fig">Figure 3C</xref></bold>). Examples of top overexpressed vocal types for each family are shown in <bold><xref rid="fig3" ref-type="fig">Figure 3D</xref></bold>. Families overexpress dissimilar vocal types relative to each other (e.g. vocalizations 4 and 8 in Family 2) and similar vocal types relative to each other (e.g. vocalization 14 in Family 1 and vocalization 1 in Family 3; vocalization 9 in Family 1 and vocalization 5 in Family 2).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Family specific vocal usage.</title>
<p>(A) UMAP probability density plots (axes same as <xref ref-type="fig" rid="fig2">Figure 2B</xref>) show significant differences between family repertoires (p &lt; 0.01, MMD permutation test on latent space; see Methods). (B) Vocal type usage by family. Clusters sorted by cumulative usage across all families. Families show distinct usage patterns of different vocal clusters. (C) Clusters are resorted by the usage difference between families. (D) Spectrogram examples from top differentially used clusters.</p></caption>
<graphic xlink:href="532197v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2d">
<title>Transition structure, not emission structure, shows family specific differences</title>
<p>To assess whether temporal features also harbor family differences, we analyzed vocalization emission over a range of ethologically relevant timescales. First, we summed the total vocal emission for each hour of the day over the entire recording period, which revealed a diurnal activity pattern that was similar across the three families recorded (<bold><xref rid="fig4" ref-type="fig">Figure 4A</xref></bold>). We then analyzed a shorter time scale, the inter-vocalization-interval. The distribution of intervals between subsequent vocalizations is broad, with some vocalizations occurring rapidly after one another (e.g. within tens to hundreds of milliseconds) and others separated by many seconds. The majority of vocalizations occurred in bouts (58.5 +/-0.9%), which we extracted using two criteria: (1) vocalizations within a bout display inter-vocalization-interval of &lt;2 seconds, and (2) a bout contains at least 5 vocalizations (based on <xref ref-type="bibr" rid="c52">Rose et al., 2021</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Transition structure, not emission structure, shows family specific differences.</title>
<p>(A) Vocalizations are emitted in a diurnal cycle. (B) Vocalizations consistently occur in seconds-long bouts across families. (C) Vocalization intervals (onset-to-onset) are consistent across families. (D) Vocalization durations are consistent across families. (E) Raw data examples of bouts. (F) Bouts typically occupy a similar area of vocal space. (G) Vocal cluster transition matrix. Vocalizations strongly favor self-transition. (H) Bigram probability graph. Self and other vocalization transition tendencies show family specific transitions (edges &gt; 0.001 usage shown).</p></caption>
<graphic xlink:href="532197v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The distribution of bout durations, inter-vocalization-intervals, and vocalization durations for each family are highly overlapping and contain the same peaks (<bold><xref rid="fig4" ref-type="fig">Figure 4B-D</xref></bold>), suggesting that the temporal structure of vocal emission does not vary by family. Vocalization bouts show striking structure in vocal type sequencing (<bold><xref rid="fig4" ref-type="fig">Figure 4E-F</xref></bold>), therefore we next assessed whether vocal cluster sequencing varied by family. Vocal cluster transition matrices revealed a strong self-transition preference for all vocal clusters across families (<bold><xref rid="fig4" ref-type="fig">Figure 4G</xref></bold>), however the proportion usage of different transitions (including self-transitions) drastically varied by family (<bold><xref rid="fig4" ref-type="fig">Figure 4H</xref></bold>).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Understanding the neural mechanisms that support natural behaviors depends upon our ability to quantify specific actions over a range of ethologically relevant contexts and timescales (<xref ref-type="bibr" rid="c41">Miller et al., 2022</xref>). In principle, this requires continuous, undisturbed, and longitudinal recording that take place in naturalistic contexts. This need has led to the emergence of a variety of video tools for long-term monitoring, and an associated suite of machine-learning based analyses (<xref ref-type="bibr" rid="c15">Datta et al., 2019</xref>; <xref ref-type="bibr" rid="c49">Pereira et al., 2020</xref>; <xref ref-type="bibr" rid="c62">Shemesh &amp; Chen, 2023</xref>). In contrast, most studies of natural behavior do not acquire and analyze acoustic information over prolonged periods of the life cycle.</p>
<p>Social vocalizations can convey pivotal information about an animal’s identity. For example, female macaques learn to recognize the vocalizations of their own offspring during the second postnatal week, and retain this ability for at least 6 months (Jovanovic et al., 2003; <xref ref-type="bibr" rid="c63">Shizawa et al., 2005</xref>). Similarly, kittens learn their mother’s vocalizations, and Australian sea lions can recall their mother’s voice up to 2 years after weaning (<xref ref-type="bibr" rid="c51">Pitcher et al., 2010</xref>; <xref ref-type="bibr" rid="c64">Szenczi et al., 2016</xref>). Furthermore, the meaning of vocal cues are often learned through long-term social experience. For example, when exposed to a chicken maternal call during development, socially reared mallard ducklings come to prefer it over their own species’ call (<xref ref-type="bibr" rid="c26">Gottlieb, 1993</xref>). Similarly, wood ducklings must be exposed to sibling vocalizations in order to remain selectively responsive to its mother’s assembly call (<xref ref-type="bibr" rid="c24">Gottlieb, 1983</xref>). Horseshoe bats, naked mole rats, and dolphins each model their calls based on early social experience (<xref ref-type="bibr" rid="c34">Jones &amp; Ransome, 1993</xref>; <xref ref-type="bibr" rid="c22">Fripp et al., 2005</xref>; <xref ref-type="bibr" rid="c21">Favaro et al., 2016</xref>; <xref ref-type="bibr" rid="c6">Barker et al., 2021</xref>). Therefore, there is a compelling rationale for exploring the diversity of vocalizations within the family social group.</p>
<p>Here, we make continuous audio recordings of 3 Mongolian gerbil families for 20 days with the goal of characterizing vocal communication in a significant social group (<bold><xref rid="fig1" ref-type="fig">Figure 1</xref></bold>). By expanding the recording duration, and permitting animals undisturbed interaction with their family unit, we predicted that we would capture a larger diversity of the gerbil vocal repertoire and sought to determine the repertoire association with family identity. Capitalizing on advances in computational bioacoustics, which aid in the characterization of complex and high-dimensional vocal behavior (<xref ref-type="bibr" rid="c54">Sainburg et al., 2020</xref>, <xref ref-type="bibr" rid="c55">2021</xref>; Goffinet 2021), we extracted vocalization spectrograms and used a VAE to perform unsupervised analysis of a large number of familial gerbil vocalizations (n=583,237). At least one new vocal type and numerous multisyllabic vocal types were discovered using this approach (<bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold>). Also, we provide evidence that family structure is necessary to elicit the full vocal repertoire (<bold><xref rid="figS3" ref-type="fig">Figure S3</xref></bold>). These findings underscore the advantage of a longitudinal naturalistic approach, and suggest that further elaborations (e.g., providing a larger-scale naturalistic environment) could reveal new aural communication behaviors.</p>
<p>Recent work has shown that rodent vocalizations harbor information about the individual identity and colony membership of the vocalizer (<xref ref-type="bibr" rid="c6">Barker et al. 2021</xref>). To address whether gerbils also exhibit family specific vocal features, we compared GMM-labeled vocal cluster usages across the three recorded families and showed differences in vocal type usage (<bold><xref rid="fig3" ref-type="fig">Figure 3</xref></bold>). The differences align with the definition of human vocal dialect, which is a regional or social variety of language that can differ in pronunciation, grammatical, semantic and/or language use differences (<xref ref-type="bibr" rid="c31">Henry et al., 2015</xref>). In our case, vocal clusters can be rarely observed in some families yet highly over-expressed in others (e.g. analogous to language use differences in humans), or highly expressed in both families, but contain subtle spectrotemporal variations (e.g. analogous to pronunciation differences in humans). Like another fossorial species, the naked mole-rat, it’s possible that gerbils may also possess the ability to acquire family specific vocal behavior through experience (<xref ref-type="bibr" rid="c6">Barker et al. 2021</xref>).</p>
<p>Vocalization emission statistics and behavioral syllable transition patterns can signify differences between groups of animals (<xref ref-type="bibr" rid="c12">Castellucci et al. 2018</xref>, <xref ref-type="bibr" rid="c68">Wiltschko et al., 2015</xref>, <xref ref-type="bibr" rid="c39">Markowitz et al., 2018</xref>). Therefore, it’s possible that vocal emission patterns or vocal cluster transition patterns may be family specific. To address this, we first compared vocalization emission rates over multiple ethologically relevant timescales, which revealed highly consistent emission patterns across families (<bold><xref rid="fig4" ref-type="fig">Figure 4A-D</xref></bold>). First, we observed that vocal emission follows a diurnal pattern, with peaks of activity in the morning and afternoon. This result complements prior work in gerbils showing diurnal activity patterns in gerbil groups for non-vocal behaviors (Pietrewicz 1982), but extends our understanding to vocal behavior. Vocalizations are rarely emitted in isolation.</p>
<p>Rather, they are emitted in sequences (“bouts”) with a modal duration of 4 seconds and a duration distribution that does not vary between families. These emission statistics are somewhat consistent with the common phoneme rate in humans (<xref ref-type="bibr" rid="c19">Edwards and Chang, 2013</xref>; <xref ref-type="bibr" rid="c17">Ding et al., 2017</xref>). Also, the distributions of inter-vocalization interval and vocalization duration did not differ between families. Taken together, the temporal emission structure is highly consistent across families and suggests that these features are likely not exploited for kinship identification. However, this does not rule out the possibility that the sequential organization of vocalizations could vary. Vocalization bouts (<bold><xref rid="fig1" ref-type="fig">Figure 1C</xref>, <xref rid="fig4" ref-type="fig">Figure 4E-F</xref></bold>) show that temporal sequencing of vocalization clusters is non-random and has a compelling transition structure with potential to vary across families. To formally quantify this we calculated vocalization transition matrices for each family, which revealed that all families strongly favor vocalization self-transitions (<bold><xref rid="fig4" ref-type="fig">Figure 4G</xref></bold>), though hinted that non self-transitions (off-diagonal) vary by family. To visualize this, we generated bigram transition graphs of highly expressed vocalization transitions, which provides evidence that vocalization transition structure varies by family (<bold><xref rid="fig4" ref-type="fig">Figure 4H</xref></bold>).</p>
<p>These results reveal that Mongolian gerbil families possess a rich repertoire of vocalizations used during day-to-day communication. Our findings indicate that long-term behavioral monitoring of a core social unit (i.e. the family) reveals richer vocal behavior than has previously been reported in the species. Leveraging unsupervised machine learning to quantify vocalizations, we reveal family-specific vocalization usage and transition structure. Taken together, these findings establish the Mongolian gerbil as a useful model organism for studying the neurobiology of vocal interactions in complex social groups.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Experimental animals</title>
<p>Three gerbil families (<italic>Meriones unguiculatus</italic>, n=6 per family: 2 adults, 4 pups) were used in this study (Charles River). All procedures related to the maintenance and use of animals were approved by the University Animal Welfare Committee at New York University, and all experiments were performed in accordance with the relevant guidelines and regulations.</p>
</sec>
<sec id="s4b">
<title>Audio recording</title>
<p>Four ultrasonic microphones (Avisoft CM16/CMPA48AAF-5V) were synchronously recorded using a National Instruments multifunction data acquisition device (PCI-6143) via BNC connection with a National Instruments terminal block (BNC-2110). The recording was controlled with custom python scripts using the NI-DAQmx library (<ext-link ext-link-type="uri" xlink:href="https://github.com/ni/nidaqmx-python">https://github.com/ni/nidaqmx-python</ext-link>) which wrote samples to disk at a 125 kHz sampling rate. In total, 13.084 TB of raw audio data were acquired across the three families. For further analyses, the four-channel microphone signals were averaged to create a single-channel high-fidelity audio signal.</p>
</sec>
<sec id="s4c">
<title>Audio segmentation</title>
<p>Audio was segmented by amplitude thresholding using the AVA python package (<ext-link ext-link-type="uri" xlink:href="https://github.com/pearsonlab/autoencoded-vocal-analysis">https://github.com/pearsonlab/autoencoded-vocal-analysis</ext-link>). First, sound amplitude traces are calculated by computing spectrograms from raw audio, then summing each column of the spectrogram. The “get_onset_offsets” function, which performs the segmenting, requires the selection of a number of parameters which affect segmenting performance. The following values were tuned via an interactive procedure which validated that the segmenting could detect low amplitude vocalizations and capture individual vocal units apparent by eye:</p>
<p>seg_params = {</p>
<p>‘min_freq’: 500 # minimum frequency</p>
<p>‘max_freq’: 62500, # maximum frequency</p>
<p>‘nperseg’: 512, # FFT</p>
<p>‘noverlap’: 256, # FFT</p>
<p>‘spec_min_val’: -8, # minimum STFT log-modulus</p>
<p>‘spec_max_val’: -7.25, # maximum STFT log-modulus</p>
<p>‘fs’: 125000, # audio sample rate</p>
<p>‘th_1’: 2, # segmenting threshold 1</p>
<p>‘th_2’: 5, # segmenting threshold 2</p>
<p>‘th_3’: 2, # segmenting threshold 3</p>
<p>‘min_dur’:0.03, # minimum syllable duration (s)</p>
<p>‘max_dur’:0.3, # maximum syllable duration (s)</p>
<p>‘smoothing_timescale’: 0.007, # amplitude</p>
<p>‘softmax’: False, # apply softmax to the frequency bins to calculate amplitude ‘temperature’:0.5, # softmax temperature parameter</p>
<p>‘algorithm’: get_onsets_offsets</p>
<p>}</p>
<p>Sound events are detected when the amplitude exceeds ‘th_3’, And sound offset occurs when there is a subsequent local minimum (i.e., amplitude less than ‘th_2’, or ‘th_1’). The maximum and minimum syllable durations were selected based on published duration ranges of gerbil vocalizations (<xref ref-type="bibr" rid="c66">Ter-Mikaelian et al. 2012</xref>, <xref ref-type="bibr" rid="c37">Kobayasi &amp; Riquimaroux, 2012</xref>).</p>
</sec>
<sec id="s4d">
<title>Vocalization extraction</title>
<p>We computed the spectral flatness of each detected sound event using the python package librosa (<ext-link ext-link-type="uri" xlink:href="https://github.com/librosa">https://github.com/librosa</ext-link>). Consistent with prior literature (<xref ref-type="bibr" rid="c11">Castellucci et al., 2016</xref>), we used a threshold on spectral flatness to separate putative vocal and non-vocal sounds. This threshold value was determined empirically, by calculating the false positive vocalization rate (<bold><xref rid="figS1" ref-type="fig">Figure S1</xref></bold>) of groups of randomly sampled vocalizations. For each spectral flatness value in <bold><xref rid="figS1" ref-type="fig">Figure S1B</xref></bold>, 100 randomly sampled vocalization spectrograms less than the working threshold value were assembled into 10×10 grids and visually inspected for false positives (e.g. non-vocal sounds; <bold><xref rid="figS1" ref-type="fig">Figure S1C</xref></bold>). This procedure was repeated 10 times for spectral flatness thresholds of 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, and 0.4. We quantified the false positive vocalization rate for each threshold value and selected 0.3, which had a 5.5 +/-1.96% false positive rate.</p>
</sec>
<sec id="s4e">
<title>Variational Autoencoder Training</title>
<p>Extracted vocalizations were converted to 128×128 pixel spectrograms using the “process_sylls” function from AVA with the following preprocessing parameters:</p>
<p>preprocess_params = {</p>
<p>‘get_spec’: get_spec, # spectrogram maker</p>
<p>‘max_dur’: 0.3, # maximum syllable duration</p>
<p>‘min_freq’: 500, # minimum frequency</p>
<p>‘max_freq’: 62500, # maximum frequency</p>
<p>‘nperseg’: 512, # FFT</p>
<p>‘noverlap’: 256, # FFT</p>
<p>‘spec_min_val’: -8, # minimum log-spectrogram value</p>
<p>‘spec_max_val’: -5, # maximum log-spectrogram value</p>
<p>‘fs’: 125000, # audio sample rate</p>
<p>‘mel’: False, # frequency spacing, mel or linear</p>
<p>‘time_stretch’: True, # stretch short syllables?</p>
<p>‘within_syll_normalize’: False, # normalize spectrogram values on a # spectrogram-by-spectrogram basis</p>
<p>‘max_num_syllables’: None, # maximum number of syllables per directory</p>
<p>‘sylls_per_file’: 100, # syllable per file</p>
<p>‘real_preprocess_params’: (‘min_freq’, ‘max_freq’, ‘spec_min_val’,</p>
<p>‘spec_max_val’, ‘max_dur’), # tunable parameters</p>
<p>‘int_preprocess_params’: (‘nperseg’,’noverlap’), # tunable parameters ‘binary_preprocess_params’: (‘time_stretch’, ‘mel’,</p>
<p>‘within_syll_normalize’), # tunable parameters</p>
<p>}</p>
<p>A VAE was trained for 50 epochs using a model precision of 40. We removed additional false positive vocalizations by inspecting a 2D UMAP embedding of the VAE latent space and removing UMAP clusters containing non-vocal sounds from further analysis.</p>
</sec>
<sec id="s4f">
<title>Gaussian Mixture Model</title>
<p>GMMs were fit to cluster VAE latent feature vectors. To reduce computation time, we fit the model on 7 of 32 VAE latents (<bold><xref rid="figS2" ref-type="fig">Figure S2E</xref></bold>), as these explained 99.5% of the variance in the original feature space. The model was implemented in Stan</p>
<p>(<ext-link ext-link-type="uri" xlink:href="https://mc-stan.org/cmdstanpy">https://mc-stan.org/cmdstanpy</ext-link>). We fit the model using stochastic variational inference, an approximate Bayesian inference technique that recasts the task of learning a posterior distribution as an optimization problem and enables vast speedups (<xref ref-type="bibr" rid="c32">Hoffman et al., 2013</xref>). GMMs typically assume that the whole population selects clusters with the same probabilities, however we modified this assumption to allow, though not enforce, the model to learn different cluster usage patterns for each family. Specifically, we used the following model:</p>
<p>Let <italic>D</italic> be the dimensionality of the VAE latents used (in our case, <italic>D</italic> = 7) and <italic>K</italic> be the number of clusters. Denote our parameters by:</p>
<p>Mixture means (β) for cluster <italic>j</italic>: β <sub><italic>j</italic></sub> ∈ ℝ<sup><italic>D</italic></sup></p>
<p>Mixture covariance matrix (Σ) for cluster <italic>j</italic>: Σ<sub><italic>j</italic></sub> = [<italic>diag</italic>(Σ<sub><italic>j</italic></sub>)]<sup>2</sup>, for Σ<sub><italic>j</italic></sub> ∈ ℝ<sup><italic>D</italic></sup></p>
<p>Cluster usage probabilities for cohort <italic>i</italic>: θ<sub><italic>j</italic></sub> ∈ ℝ<sup>K</sup>, with <inline-formula><alternatives><inline-graphic xlink:href="532197v1_inline1.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula></p>
<p>Cluster assignment for vocalization <italic>k</italic> of cohort <italic>i</italic>: <italic>z</italic> <sub><italic>j</italic></sub> ∈ {1, …, <italic>K</italic>}</p>
<p>We selected our hyperparameters according to Stan’s guidelines for weakly informative priors, yielding the model:</p>
<p>Mixture means for cluster j : β<sub><italic>j</italic></sub> ∼ <italic>Normal</italic><sub><italic>j</italic></sub> (0, 5)</p>
<p>Mixture standard deviations (Σ) for cluster j: Σ<sub><italic>j</italic></sub> ∼ <italic>Half</italic>–<italic>Normal</italic><sub><italic>j</italic></sub> (3)</p>
<p>Cluster usage probabilities for cohort <italic>i</italic>: θ<sub><italic>j</italic></sub> ∼ <italic>Dirichlet</italic>(1, …, 1)</p>
<p>Cluster assignment for vocalization k of cohort i: <italic>z</italic> <sub><italic>j</italic></sub> ∼ <italic>Categorical</italic>(θ<sub><italic>j</italic></sub>)</p>
<p>VAE feature embedding for vocalization k of cohort i: <inline-formula><alternatives><inline-graphic xlink:href="532197v1_inline2.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula></p>
<p>To select the number of clusters, <italic>K</italic>, we held out 25% of our data, trained models with varying values for <italic>K</italic>, and calculated the log probability of seeing the held-out data under each model (<bold><xref rid="figS2" ref-type="fig">Figure S2C</xref></bold>). Combining the held-out log likelihood plot with prior domain knowledge about the number of vocal types and the possibility of variations existing per individual, we selected 70 clusters as the most parsimonious fit. Similar clustering results were achieved using the scikit-learn Gaussian Mixture model class with a diagonal covariance matrix</p>
<p>(<ext-link ext-link-type="uri" xlink:href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html">https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html</ext-link>).</p>
</sec>
<sec id="s4g">
<title>Maximum Mean Discrepancy Permutation Test</title>
<p>Clustering analyses are notoriously challenging (<xref ref-type="bibr" rid="c36">Kleinberg, 2002</xref>). Thus, we performed a complementary analysis to investigate whether different gerbil families utilize different vocal repertoires. In particular, we pursued an approach that makes no assumptions about the number, character, or even existence of vocalization clusters.</p>
<p>Specifically, we used maximum mean discrepancy (MMD) to quantify the difference between two latent distributions of vocalizations. This test considers two sets of observed data points (e.g. N vocalizations from Family 1 and N vocalizations from Family 2), which are assumed to be independent and identically distributed random variables from underlying probability distributions, and returns a distance metric corresponding to the equality of the two distributions (<xref ref-type="bibr" rid="c27">Gretton et al., 2012</xref>). Lower values suggest distributions are more similar and higher values suggest distributions are more dissimilar. We investigated the null hypothesis that the gerbil families used the same vocal repertoire—i.e. that the probability distribution over VAE latent space for each family was identical, corresponding to a MMD<sup>2</sup> distance of zero. To test this null hypothesis, we computed the MMD<sup>2</sup> distance between the empirical distributions of family pairs in batches of 1000 randomly sub-sampled vocalizations. This yielded a histogram of empirically observed MMD<sup>2</sup> distance values for each family pair, which we compared a null distribution generated by randomly permuting the family label attached to each vocalization. The empirically observed MMD<sup>2</sup> distances were much higher than the shufie control, favoring the alternative hypothesis that gerbil families utilize distinct syllable usage statistics (<bold><xref rid="figS2" ref-type="fig">Figure S2</xref></bold>).</p>
</sec>
<sec id="s4h">
<title>Transition Analysis</title>
<p>Vocalization transition sequences were generated by concatenating vocal cluster labels chronologically for each family and calculating the number of transitions for all possible transition types. The resulting transition matrix was normalized such that each row sums to 1, thus reflecting the probability that vocalization <italic>i</italic> transitions to vocalization <italic>i</italic> + 1, i.e. <italic>p</italic><sub><italic>j</italic></sub> (j) (<bold><xref rid="fig4" ref-type="fig">Figure 4G</xref></bold>). The transition matrix used to generate the bigram probability graph in <bold><xref rid="fig4" ref-type="fig">Figure 4H</xref></bold> was normalized such that edge and node widths correspond to the probability of each vocalization pair, i.e. <italic>p</italic>(<italic>i, j</italic>) (<xref ref-type="bibr" rid="c61">Shannon, 1948</xref>).</p>
</sec>
<sec id="s4i">
<title>Data availability</title>
<p>Data and code are available upon request.</p>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Ågren</surname>, <given-names>G.</given-names></string-name>, <year>1984</year>. <article-title>Incest avoidance and bonding between siblings in gerbils</article-title>. <source>Behavioral Ecology and Sociobiology</source>, <volume>14</volume>, pp.<fpage>161</fpage>–<lpage>169</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Ågren</surname>, <given-names>G.</given-names></string-name>, <year>1984</year>. <article-title>Pair formation in the Mongolian gerbil</article-title>. <source>Animal behaviour</source>, <volume>32</volume>(<issue>2</issue>), pp.<fpage>528</fpage>–<lpage>535</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Ågren</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>Q.</given-names></string-name> and <string-name><surname>Zhong</surname>, <given-names>W.</given-names></string-name>, <year>1989</year>. <article-title>Ecology and social behaviour of Mongolian gerbils, Meriones unguiculatus, at Xilinhot, Inner Mongolia, China</article-title>. <source>Animal Behaviour</source>, <volume>37</volume>, pp.<fpage>11</fpage>–<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Ågren</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>Q.</given-names></string-name> and <string-name><surname>Zhong</surname>, <given-names>W.</given-names></string-name>, <year>1989</year>. <article-title>Territoriality, cooperation and resource priority: hoarding in the Mongolian gerbil, Meriones unguiculatus</article-title>. <source>Animal Behaviour</source>, <volume>37</volume>, pp.<fpage>28</fpage>–<lpage>32</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Amaro</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ferreiro</surname>, <given-names>D.N.</given-names></string-name>, <string-name><surname>Grothe</surname>, <given-names>B.</given-names></string-name> and <string-name><surname>Pecka</surname>, <given-names>M.</given-names></string-name>, <year>2021</year>. <article-title>Source identity shapes spatial preference in primary auditory cortex during active navigation</article-title>. <source>Current Biology</source>, <volume>31</volume>(<issue>17</issue>), pp.<fpage>3875</fpage>–<lpage>3883</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Barker</surname>, <given-names>A.J.</given-names></string-name>, <string-name><surname>Veviurko</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Bennett</surname>, <given-names>N.C.</given-names></string-name>, <string-name><surname>Hart</surname>, <given-names>D.W.</given-names></string-name>, <string-name><surname>Mograby</surname>, <given-names>L.</given-names></string-name> and <string-name><surname>Lewin</surname>, <given-names>G.R.</given-names></string-name>, <year>2021</year>. <article-title>Cultural transmission of vocal dialect in the naked mole-rat</article-title>. <source>Science</source>, <volume>371</volume>(<issue>6528</issue>), pp.<fpage>503</fpage>–<lpage>507</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="book"><string-name><surname>Brett</surname>, <given-names>R.A.</given-names></string-name>, <year>1986</year>. <source>The ecology and behaviour of the naked mole-rat, Heterocephalus glaber Ruppell (Rodenti: Bathyergidae) (Doctoral dissertation</source>, <publisher-name>University College London (University of London</publisher-name>)).</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Budinger</surname>, <given-names>E.</given-names></string-name> and <string-name><surname>Scheich</surname>, <given-names>H.</given-names></string-name>, <year>2009</year>. <article-title>Anatomical connections suitable for the direct processing of neuronal information of different modalities via the rodent primary auditory cortex</article-title>. <source>Hearing research</source>, <volume>258</volume>(<issue>1-2</issue>), pp.<fpage>16</fpage>–<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Buran</surname>, <given-names>B.N.</given-names></string-name>, <string-name><surname>von Trapp</surname>, <given-names>G.</given-names></string-name> and <string-name><surname>Sanes</surname>, <given-names>D.H.</given-names></string-name>, <year>2014</year>. <article-title>Behaviorally gated reduction of spontaneous discharge can improve detection thresholds in auditory cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>34</volume>(<issue>11</issue>), pp.<fpage>4076</fpage>–<lpage>4081</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Caras</surname>, <given-names>M.L.</given-names></string-name> and <string-name><surname>Sanes</surname>, <given-names>D.H.</given-names></string-name>, <year>2017</year>. <article-title>Top-down modulation of sensory cortex gates perceptual learning</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>114</volume>(<issue>37</issue>), pp.<fpage>9972</fpage>–<lpage>9977</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Castellucci</surname>, <given-names>G.A.</given-names></string-name>, <string-name><surname>McGinley</surname>, <given-names>M.J.</given-names></string-name> and <string-name><surname>McCormick</surname>, <given-names>D.A.</given-names></string-name>, <year>2016</year>. <article-title>Knockout of Foxp2 disrupts vocal development in mice</article-title>. <source>Scientific reports</source>, <volume>6</volume>(<issue>1</issue>), p.<fpage>23305</fpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Castellucci</surname>, <given-names>G.A.</given-names></string-name>, <string-name><surname>Calbick</surname>, <given-names>D.</given-names></string-name> and <string-name><surname>McCormick</surname>, <given-names>D.</given-names></string-name>, <year>2018</year>. <article-title>The temporal organization of mouse ultrasonic vocalizations</article-title>. <source>PloS one</source>, <volume>13</volume>(<issue>10</issue>), p.<fpage>e0199929</fpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Chen</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Matheson</surname>, <given-names>L.E.</given-names></string-name> and <string-name><surname>Sakata</surname>, <given-names>J.T.</given-names></string-name>, <year>2016</year>. <article-title>Mechanisms underlying the social enhancement of vocal learning in songbirds</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>113</volume>(<issue>24</issue>), pp.<fpage>6641</fpage>–<lpage>6646</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Cheng</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Xian</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Grothe</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Klug</surname>, <given-names>A.</given-names></string-name> and <string-name><surname>McCullagh</surname>, <given-names>E.A.</given-names></string-name>, <year>2019</year>. <article-title>Enhancement of de novo sequencing, assembly and annotation of the Mongolian gerbil genome with transcriptome sequencing and assembly from several different tissues</article-title>. <source>Bmc Genomics</source>, <volume>20</volume>(<issue>1</issue>), pp.<fpage>1</fpage>–<lpage>6</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Datta</surname>, <given-names>S.R.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>D.J.</given-names></string-name>, <string-name><surname>Branson</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Perona</surname>, <given-names>P.</given-names></string-name> and <string-name><surname>Leifer</surname>, <given-names>A.</given-names></string-name>, <year>2019</year>. <article-title>Computational neuroethology: a call to action</article-title>. <source>Neuron</source>, <volume>104</volume>(<issue>1</issue>), pp.<fpage>11</fpage>–<lpage>24</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Derégnaucourt</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Poirier</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Van der Kant</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Van der Linden</surname>, <given-names>A.</given-names></string-name> and <string-name><surname>Gahr</surname>, <given-names>M.</given-names></string-name>, <year>2013</year>. <article-title>Comparisons of different methods to train a young zebra inch (Taeniopygia guttata) to learn a song</article-title>. <source>Journal of Physiology-Paris</source>, <volume>107</volume>(<issue>3</issue>), pp.<fpage>210</fpage>–<lpage>218</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Ding</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Patel</surname>, <given-names>A.D.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Butler</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Luo</surname>, <given-names>C.</given-names></string-name> and <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name>, <year>2017</year>. <article-title>Temporal modulations in speech and music</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, <volume>81</volume>, pp.<fpage>181</fpage>–<lpage>187</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="other"><string-name><surname>Eales</surname>, <given-names>L.A.</given-names></string-name>, <year>1989</year>. <article-title>The influences of visual and vocal interaction on song learning in zebra inches</article-title>. <source>Animal Behaviour</source>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Edwards</surname>, <given-names>E.</given-names></string-name> and <string-name><surname>Chang</surname>, <given-names>E.F.</given-names></string-name>, <year>2013</year>. <article-title>Syllabic (∼ 2–5 Hz) and fluctuation (∼ 1–10 Hz) ranges in speech and auditory processing</article-title>. <source>Hearing research</source>, <volume>305</volume>, pp.<fpage>113</fpage>–<lpage>134</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Elwood</surname>, <given-names>R.W.</given-names></string-name>, <year>1975</year>. <article-title>Paternal and maternal behaviour in the Mongolian gerbil</article-title>. <source>Animal Behaviour</source>, <volume>23</volume>, pp.<fpage>766</fpage>–<lpage>772</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Favaro</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Neves</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Furlati</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pessani</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Martin</surname>, <given-names>V.</given-names></string-name> and <string-name><surname>Janik</surname>, <given-names>V.M.</given-names></string-name>, <year>2016</year>. <article-title>Evidence suggests vocal production learning in a cross-fostered Risso’s dolphin (Grampus griseus)</article-title>. <source>Animal cognition</source>, <volume>19</volume>, pp.<fpage>847</fpage>–<lpage>853</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Fripp</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Owen</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Quintana-Rizzo</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Shapiro</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Buckstaff</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Jankowski</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Wells</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Tyack</surname>, <given-names>P.</given-names></string-name>, <year>2005</year>. <article-title>Bottlenose dolphin (Tursiops truncatus) calves appear to model their signature whistles on the signature whistles of community members</article-title>. <source>Animal cognition</source>, <volume>8</volume>, pp.<fpage>17</fpage>–<lpage>26</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Gofnet</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Brudner</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Mooney</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Pearson</surname>, <given-names>J.</given-names></string-name>, <year>2021</year>. <article-title>Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires</article-title>. <source>Elife</source>, <volume>10</volume>, p.<fpage>e67855</fpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="other"><string-name><surname>Gottlieb</surname>, <given-names>G.</given-names></string-name>, <year>1983</year>. <article-title>Development of species identifcation in ducklings: X</article-title>. <source>Perceptual specifcity in the wood duck embryo requires sib stimulation for maintenance</source>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><collab>Developmental Psychobiology</collab>: <source>The Journal of the International Society for Developmental Psychobiology</source>, <volume>16</volume>(<issue>4</issue>), pp.<fpage>323</fpage>–<lpage>333</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Gottlieb</surname>, <given-names>G.</given-names></string-name>, <year>1993</year>. <article-title>Social induction of malleability in ducklings: Sensory basis and psychological mechanism</article-title>. <source>Animal Behaviour</source>, <volume>45</volume>(<issue>4</issue>), pp.<fpage>707</fpage>–<lpage>719</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Gretton</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Borgwardt</surname>, <given-names>K.M.</given-names></string-name>, <string-name><surname>Rasch</surname>, <given-names>M.J.</given-names></string-name>, <string-name><surname>Schölkopf</surname>, <given-names>B.</given-names></string-name> and <string-name><surname>Smola</surname>, <given-names>A.</given-names></string-name>, <year>2012</year>. <article-title>A kernel two-sample test</article-title>. <source>The Journal of Machine Learning Research</source>, <volume>13</volume>(<issue>1</issue>), pp.<fpage>723</fpage>–<lpage>773</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Grifths</surname>, <given-names>T.D.</given-names></string-name> and <string-name><surname>Warren</surname>, <given-names>J.D.</given-names></string-name>, <year>2004</year>. <article-title>What is an auditory object?</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>5</volume>(<issue>11</issue>), pp.<fpage>887</fpage>–<lpage>892</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Gromov</surname>, <given-names>V.S.</given-names></string-name>, <year>2021</year>. <article-title>Ecology and social behaviour of the Mongolian gerbil: a generalised review</article-title>. <source>Behaviour</source>, <volume>159</volume>(<issue>5</issue>), pp.<fpage>403</fpage>–<lpage>441</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Happel</surname>, <given-names>M.F.</given-names></string-name>, <string-name><surname>Niekisch</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Castiblanco Rivera</surname>, <given-names>L.L.</given-names></string-name>, <string-name><surname>Ohl</surname>, <given-names>F.W.</given-names></string-name>, <string-name><surname>Deliano</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Frischknecht</surname>, <given-names>R.</given-names></string-name>, <year>2014</year>. <article-title>Enhanced cognitive fiexibility in reversal learning induced by removal of the extracellular matrix in auditory cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>111</volume>(<issue>7</issue>), pp.<fpage>2800</fpage>–<lpage>2805</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Henry</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Barbu</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lemasson</surname>, <given-names>A.</given-names></string-name> and <string-name><surname>Hausberger</surname>, <given-names>M.</given-names></string-name>, <year>2015</year>. <article-title>Dialects in animals: Evidence, development and potential functions</article-title>. <source>Animal Behavior and Cognition</source>, <volume>2</volume>(<issue>2</issue>), pp.<fpage>132</fpage>–<lpage>155</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="other"><string-name><surname>Hoffman</surname>, <given-names>M.D.</given-names></string-name>, <string-name><surname>Blei</surname>, <given-names>D.M.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>C.</given-names></string-name> and <string-name><surname>Paisley</surname>, <given-names>J.</given-names></string-name>, <year>2013</year>. <article-title>Stochastic variational inference</article-title>. <source>Journal of Machine Learning Research</source>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Hurtado-Parrado</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>González-León</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Arias-Higuera</surname>, <given-names>M.A.</given-names></string-name>, <string-name><surname>Cardona</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Medina</surname>, <given-names>L.G.</given-names></string-name>, <string-name><surname>García-Muñoz</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sánchez</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Cifuentes</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Forigua</surname>, <given-names>J.C.</given-names></string-name>, <string-name><surname>Ortiz</surname>, <given-names>A.</given-names></string-name> and <string-name><surname>Acevedo-Triana</surname>, <given-names>C.A.</given-names></string-name>, <year>2017</year>. <article-title>Assessing Mongolian gerbil emotional behavior: effects of two shock intensities and response-independent shocks during an extended inhibitory-avoidance task</article-title>. <source>PeerJ</source>, <volume>5</volume>, p.<fpage>e4009</fpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Jones</surname>, <given-names>G.</given-names></string-name> and <string-name><surname>Ransome</surname>, <given-names>R.D.</given-names></string-name>, <year>1993</year>. <article-title>Echolocation calls of bats are infiuenced by maternal efiects and change over a lifetime. Proceedings of the Royal Society of London</article-title>. <source>Series B: Biological Sciences</source>, <volume>252</volume>(<issue>1334</issue>), pp.<fpage>125</fpage>–<lpage>128</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Jovanovic</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Megna</surname>, <given-names>N.L.</given-names></string-name> and <string-name><surname>Maestripieri</surname>, <given-names>D.</given-names></string-name>, <year>2000</year>. <article-title>Early maternal recognition of ofispring vocalizations in rhesus macaques (Macaca mulatta)</article-title>. <source>Primates</source>, <volume>41</volume>(<issue>4</issue>), p.<fpage>421</fpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Kleinberg</surname>, <given-names>J.</given-names></string-name>, <year>2002</year>. <article-title>An impossibility theorem for clustering</article-title>. <source>Advances in neural information processing systems</source>, <volume>15</volume>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Kobayasi</surname>, <given-names>K.I.</given-names></string-name> and <string-name><surname>Riquimaroux</surname>, <given-names>H.</given-names></string-name>, <year>2012</year>. <article-title>Classiffcation of vocalizations in the Mongolian gerbil, Meriones unguiculatus</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>131</volume>(<issue>2</issue>), pp.<fpage>1622</fpage>–<lpage>1631</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Kuhl</surname>, <given-names>P.K.</given-names></string-name>, <string-name><surname>Tsao</surname>, <given-names>F.M.</given-names></string-name> and <string-name><surname>Liu</surname>, <given-names>H.M.</given-names></string-name>, <year>2003</year>. <article-title>Foreign-language experience in infancy: Efiects of short-term exposure and social interaction on phonetic learning</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>100</volume>(<issue>15</issue>), pp.<fpage>9096</fpage>–<lpage>9101</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Markowitz</surname>, <given-names>J.E.</given-names></string-name>, <string-name><surname>Gillis</surname>, <given-names>W.F.</given-names></string-name>, <string-name><surname>Beron</surname>, <given-names>C.C.</given-names></string-name>, <string-name><surname>Neufeld</surname>, <given-names>S.Q.</given-names></string-name>, <string-name><surname>Robertson</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Bhagat</surname>, <given-names>N.D.</given-names></string-name>, <string-name><surname>Peterson</surname>, <given-names>R.E.</given-names></string-name>, <string-name><surname>Peterson</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Hyun</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Linderman</surname>, <given-names>S.W.</given-names></string-name> and <string-name><surname>Sabatini</surname>, <given-names>B.L.</given-names></string-name>, <year>2018</year>. <article-title>The striatum organizes 3D behavior via moment-to-moment action selection</article-title>. <source>Cell</source>, <volume>174</volume>(<issue>1</issue>), pp.<fpage>44</fpage>–<lpage>58</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Mascaro</surname>, <given-names>J.S.</given-names></string-name>, <string-name><surname>Rentscher</surname>, <given-names>K.E.</given-names></string-name>, <string-name><surname>Hackett</surname>, <given-names>P.D.</given-names></string-name>, <string-name><surname>Lori</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Darcher</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rilling</surname>, <given-names>J.K.</given-names></string-name> and <string-name><surname>Mehl</surname>, <given-names>M.R.</given-names></string-name>, <year>2018</year>. <article-title>Preliminary evidence that androgen signaling is correlated with men’s everyday language</article-title>. <source>American Journal of Human Biology</source>, <volume>30</volume>(<issue>4</issue>), p.<fpage>e23136</fpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Miller</surname>, <given-names>C.T.</given-names></string-name>, <string-name><surname>Gire</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hoke</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Huk</surname>, <given-names>A.C.</given-names></string-name>, <string-name><surname>Kelley</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Leopold</surname>, <given-names>D.A.</given-names></string-name>, <string-name><surname>Smear</surname>, <given-names>M.C.</given-names></string-name>, <string-name><surname>Theunissen</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Yartsev</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Niell</surname>, <given-names>C.M.</given-names></string-name>, <year>2022</year>. <article-title>Natural behavior is the language of the brain</article-title>. <source>Current Biology</source>, <volume>32</volume>(<issue>10</issue>), pp.<fpage>R482</fpage>–<lpage>R493</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Milne-Edwards</surname>, <given-names>A.</given-names></string-name>, <year>1867</year>. <article-title>Observations sur quelques mamiferes du nord de la china</article-title>. <source>Ann Sci Nat</source>, <volume>7</volume>, pp.<fpage>375</fpage>–<lpage>377</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Myoga</surname>, <given-names>M.H.</given-names></string-name>, <string-name><surname>Lehnert</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Leibold</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Felmy</surname>, <given-names>F.</given-names></string-name> and <string-name><surname>Grothe</surname>, <given-names>B.</given-names></string-name>, <year>2014</year>. <article-title>Glycinergic inhibition tunes coincidence detection in the auditory brainstem</article-title>. <source>Nature communications</source>, <volume>5</volume>(<issue>1</issue>), p.<fpage>3790</fpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Narula</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Herbst</surname>, <given-names>J.A.</given-names></string-name>, <string-name><surname>Rychen</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Hahnloser</surname>, <given-names>R.H.</given-names></string-name>, <year>2018</year>. <article-title>Learning auditory discriminations from observation is eficient but less robust than learning from experience</article-title>. <source>Nature communications</source>, <volume>9</volume>(<issue>1</issue>), p.<fpage>3218</fpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lyamzin</surname>, <given-names>D.R.</given-names></string-name>, <string-name><surname>Sahani</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Lesica</surname>, <given-names>N.A.</given-names></string-name>, <year>2015</year>. <article-title>State-dependent population coding in primary auditory cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>5</issue>), pp.<fpage>2058</fpage>–<lpage>2073</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Pagel</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Atkinson</surname>, <given-names>Q.D.S.</given-names></string-name> <string-name><surname>Calude</surname>, <given-names>A.</given-names></string-name> and <string-name><surname>Meade</surname>, <given-names>A.</given-names></string-name>, <year>2013</year>. <article-title>Ultraconserved words point to deep language ancestry across Eurasia</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>110</volume>(<issue>21</issue>), pp.<fpage>8471</fpage>–<lpage>8476</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Paraouty</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Rizzuto</surname>, <given-names>C.R.</given-names></string-name> and <string-name><surname>Sanes</surname>, <given-names>D.H.</given-names></string-name>, <year>2021</year>. <article-title>Dopaminergic signaling supports auditory social learning</article-title>. <source>Scientifc reports</source>, <volume>11</volume>(<issue>1</issue>), pp.<fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Penikis</surname>, <given-names>K.B.</given-names></string-name> and <string-name><surname>Sanes</surname>, <given-names>D.H.</given-names></string-name>, <year>2023</year>. <article-title>A redundant cortical code for speech envelope</article-title>. <source>Journal of Neuroscience</source>, <volume>43</volume>(<issue>1</issue>), pp.<fpage>93</fpage>–<lpage>112</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Pereira</surname>, <given-names>T.D.</given-names></string-name>, <string-name><surname>Shaevitz</surname>, <given-names>J.W.</given-names></string-name> and <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name>, <year>2020</year>. <article-title>Quantifying behavior to understand the brain</article-title>. <source>Nature neuroscience</source>, <volume>23</volume>(<issue>12</issue>), pp.<fpage>1537</fpage>–<lpage>1549</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Pietrewicz</surname>, <given-names>A.T.</given-names></string-name>, <string-name><surname>Hof</surname>, <given-names>M.P.</given-names></string-name> and <string-name><surname>Higgins</surname>, <given-names>S.A.</given-names></string-name>, <year>1982</year>. <article-title>Activity rhythms in the Mongolian gerbil under natural light conditions</article-title>. <source>Physiology &amp; Behavior</source>, <volume>29</volume>(<issue>2</issue>), pp.<fpage>377</fpage>–<lpage>380</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Pitcher</surname>, <given-names>B.J.</given-names></string-name>, <string-name><surname>Harcourt</surname>, <given-names>R.G.</given-names></string-name> and <string-name><surname>Charrier</surname>, <given-names>I.</given-names></string-name>, <year>2010</year>. <article-title>The memory remains: long-term vocal recognition in Australian sea lions</article-title>. <source>Animal cognition</source>, <volume>13</volume>, pp.<fpage>771</fpage>–<lpage>776</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Rose</surname>, <given-names>M.C.</given-names></string-name>, <string-name><surname>Styr</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Schmid</surname>, <given-names>T.A.</given-names></string-name>, <string-name><surname>Elie</surname>, <given-names>J.E.</given-names></string-name> and <string-name><surname>Yartsev</surname>, <given-names>M.M.</given-names></string-name>, <year>2021</year>. <article-title>Cortical representation of group social communication in bats</article-title>. <source>Science</source>, <volume>374</volume>(<issue>6566</issue>), p.<fpage>eaba9584</fpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Ryan</surname>, <given-names>A.</given-names></string-name>, <year>1976</year>. <article-title>Hearing sensitivity of the mongolian gerbil, Meriones unguiculatis</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>59</volume>(<issue>5</issue>), pp.<fpage>1222</fpage>–<lpage>1226</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Sainburg</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Thielk</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Gentner</surname>, <given-names>T.Q.</given-names></string-name>, <year>2020</year>. <article-title>Finding, visualizing, and quantifying latent structure across diverse animal vocal repertoires</article-title>. <source>PLoS computational biology</source>, <volume>16</volume>(<issue>10</issue>), p.<fpage>e1008228</fpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="other"><string-name><surname>Sainburg</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Gentner</surname>, <given-names>T.Q.</given-names></string-name>, <year>2021</year>. <article-title>Towards a computational neuroethology of vocal communication: from bioacoustics to neurophysiology, emerging tools and future direction</article-title>. <source>Frontiers in Behavioral Neuroscience</source>, p.<fpage>330</fpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Saldeitis</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Jeschke</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Michalek</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Henschke</surname>, <given-names>J.U.</given-names></string-name>, <string-name><surname>Wetzel</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Ohl</surname>, <given-names>F.W.</given-names></string-name> and <string-name><surname>Budinger</surname>, <given-names>E.</given-names></string-name>, <year>2022</year>. <article-title>Selective Interruption of Auditory Interhemispheric Cross Talk Impairs Discrimination Learning of Frequency-Modulated Tone Direction But Not Gap Detection and Discrimination</article-title>. <source>Journal of Neuroscience</source>, <volume>42</volume>(<issue>10</issue>), pp.<fpage>2025</fpage>–<lpage>2038</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Sarro</surname>, <given-names>E.C.</given-names></string-name>, <string-name><surname>von Trapp</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Mowery</surname>, <given-names>T.M.</given-names></string-name>, <string-name><surname>Kotak</surname>, <given-names>V.C.</given-names></string-name> and <string-name><surname>Sanes</surname>, <given-names>D.H.</given-names></string-name>, <year>2015</year>. <article-title>Cortical synaptic inhibition declines during auditory learning</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>16</issue>), pp.<fpage>6318</fpage>–<lpage>6325</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Scheibler</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Weinandy</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Gattermann</surname>, <given-names>R.</given-names></string-name>, <year>2004</year>. <article-title>Social categories in families of Mongolian gerbils</article-title>. <source>Physiology &amp; behavior</source>, <volume>81</volume>(<issue>3</issue>), pp.<fpage>455</fpage>–<lpage>464</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>Scheibler</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Weinandy</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Gattermann</surname>, <given-names>R.</given-names></string-name>, <year>2006</year>. <article-title>Burrow systems of the Mongolian gerbil (Meriones unguiculatus Milne Edwards, 1867)</article-title>. <source>Mammalian Biology</source>, <volume>71</volume>, pp.<fpage>178</fpage>–<lpage>182</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><surname>Schindler</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Spors</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Demiray</surname>, <given-names>B.</given-names></string-name> and <string-name><surname>Krüger</surname>, <given-names>F.</given-names></string-name>, <year>2022</year>. <article-title>Automatic Behavior Assessment from Uncontrolled Everyday Audio Recordings by Deep Learning</article-title>. <source>Sensors</source>, <volume>22</volume>(<issue>22</issue>), p.<fpage>8617</fpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><string-name><surname>Shannon</surname>, <given-names>C.E.</given-names></string-name>, <year>1948</year>. <article-title>A mathematical theory of communication</article-title>. <source>The Bell system technical journal</source>, <volume>27</volume>(<issue>3</issue>), pp.<fpage>379</fpage>–<lpage>423</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="other"><string-name><surname>Shemesh</surname>, <given-names>Y.</given-names></string-name> and <string-name><surname>Chen</surname>, <given-names>A.</given-names></string-name>, <year>2023</year>. <article-title>A paradigm shift in translational psychiatry through rodent neuroethology</article-title>. <source>Molecular Psychiatry</source>, pp.<fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><string-name><surname>Shizawa</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Nakamichi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hinobayashi</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Minami</surname>, <given-names>T.</given-names></string-name>, <year>2005</year>. <article-title>Playback experiment to test maternal responses of Japanese macaques (Macaca fuscata) to their own infant’s call when the infants were four to six months old</article-title>. <source>Behavioural processes</source>, <volume>68</volume>(<issue>1</issue>), pp.<fpage>41</fpage>–<lpage>46</lpage></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><string-name><surname>Szenczi</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Bánszegi</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Urrutia</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Faragó</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Hudson</surname>, <given-names>R.</given-names></string-name>, <year>2016</year>. <article-title>Mother–ofispring recognition in the domestic cat: Kittens recognize their own mother’s call</article-title>. <source>Developmental Psychobiology</source>, <volume>58</volume>(<issue>5</issue>), pp.<fpage>568</fpage>–<lpage>577</lpage>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><string-name><surname>Tanaka</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name> and <string-name><surname>Mooney</surname>, <given-names>R.</given-names></string-name>, <year>2018</year>. <article-title>A mesocortical dopamine circuit enables the cultural transmission of vocal behaviour</article-title>. <source>Nature</source>, <volume>563</volume>(<issue>7729</issue>), pp.<fpage>117</fpage>–<lpage>120</lpage>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><string-name><surname>Ter-Mikaelian</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rübsamen</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Yapa</surname>, <given-names>W.B.</given-names></string-name>, <year>2012</year>. <article-title>Vocal behavior of the Mongolian gerbil in a seminatural enclosure</article-title>. <source>Behaviour</source>, <volume>149</volume>(<issue>5</issue>), pp.<fpage>461</fpage>–<lpage>492</lpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><string-name><surname>von Trapp</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Buran</surname>, <given-names>B.N.</given-names></string-name>, <string-name><surname>Sen</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Semple</surname>, <given-names>M.N.</given-names></string-name> and <string-name><surname>Sanes</surname>, <given-names>D.H.</given-names></string-name>, <year>2016</year>. <article-title>A decline in response variability improves neural signal detection during auditory task performance</article-title>. <source>Journal of Neuroscience</source>, <volume>36</volume>(<issue>43</issue>), pp.<fpage>11097</fpage>–<lpage>11106</lpage>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><string-name><surname>Wiltschko</surname>, <given-names>A.B.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>M.J.</given-names></string-name>, <string-name><surname>Iurilli</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Peterson</surname>, <given-names>R.E.</given-names></string-name>, <string-name><surname>Katon</surname>, <given-names>J.M.</given-names></string-name>, <string-name><surname>Pashkovski</surname>, <given-names>S.L.</given-names></string-name>, <string-name><surname>Abraira</surname>, <given-names>V.E.</given-names></string-name>, <string-name><surname>Adams</surname>, <given-names>R.P.</given-names></string-name> and <string-name><surname>Datta</surname>, <given-names>S.R.</given-names></string-name>, <year>2015</year>. <article-title>Mapping sub-second structure in mouse behavior</article-title>. <source>Neuron</source>, <volume>88</volume>(<issue>6</issue>), pp.<fpage>1121</fpage>–<lpage>1135</lpage>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><string-name><surname>Yao</surname>, <given-names>J.D.</given-names></string-name> and <string-name><surname>Sanes</surname>, <given-names>D.H.</given-names></string-name>, <year>2021</year>. <article-title>Temporal encoding is required for categorization, but not discrimination</article-title>. <source>Cerebral Cortex</source>, <volume>31</volume>(<issue>6</issue>), pp.<fpage>2886</fpage>–<lpage>2897</lpage>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><string-name><surname>Yao</surname>, <given-names>J.D.</given-names></string-name>, <string-name><surname>Gimoto</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Constantinople</surname>, <given-names>C.M.</given-names></string-name> and <string-name><surname>Sanes</surname>, <given-names>D.H.</given-names></string-name>, <year>2020</year>. <article-title>Parietal cortex is required for the integration of acoustic evidence</article-title>. <source>Current Biology</source>, <volume>30</volume>(<issue>17</issue>), pp.<fpage>3293</fpage>–<lpage>3303</lpage>.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><string-name><surname>Zorio</surname>, <given-names>D.A.</given-names></string-name>, <string-name><surname>Monsma</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sanes</surname>, <given-names>D.H.</given-names></string-name>, <string-name><surname>Golding</surname>, <given-names>N.L.</given-names></string-name>, <string-name><surname>Rubel</surname>, <given-names>E.W.</given-names></string-name> and <string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, <year>2019</year>. <article-title>De novo sequencing and initial annotation of the Mongolian gerbil (Meriones unguiculatus) genome</article-title>. <source>Genomics</source>, <volume>111</volume>(<issue>3</issue>), pp.<fpage>441</fpage>–<lpage>449</lpage>.</mixed-citation></ref>
</ref-list>
<sec>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1.</label>
<caption><title>Vocalization extraction.</title>
<p>(A) Distribution of the spectral flatness of all sound events extracted. Vertical red line = 0.3. (B) False-positive percentage derived from human labeling of noise detected in randomly sampled 10×10 vocalization matrices. Random samples came from putative vocalizations with spectral flatness less than a moving threshold of 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4 (n=10 random samples per group). (C) Example random sample matrix of vocalizations with spectral flatness &lt;0.3. Four false positives observed in this grid.</p></caption>
<graphic xlink:href="532197v1_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2.</label>
<caption><title>VAE training and GMM clustering.</title>
<p>(A) VAE reconstruction examples for different vocalization types. (B) VAE test and training loss show plateau in performance after a few epochs (model used in this study is epoch 50). (C) GMM held-out log likelihood as a function of the number of clusters used during model training. Seventy clusters were used in this study. (D) MMD<sup>2</sup> permutation comparisons. All family comparisons occur greater than expected by chance (p&lt;0.01, independent t-test). (E) Number of latent features used by VAE.</p></caption>
<graphic xlink:href="532197v1_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3.</label>
<caption><title>Pup removal biases vocal repertoire usage.</title>
<p>(A) Pup weaning causes a consistent reduction in vocal emission across families. (B) UMAP probability densities of the vocal repertoire pre and post pup weaning. Example vocalization from high density post-weaning regions. (C.) Difference in probability densities and total percent-change in repertoire pre-post pup weaning. (D) Quantification of day-to-day percent-change throughout the experiment shows that the percent-change magnitude observed in C is rare.</p></caption>
<graphic xlink:href="532197v1_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89892.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Takahashi</surname>
<given-names>Daniel Y</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Federal University of Rio Grande do Norte</institution>
</institution-wrap>
<city>Natal</city>
<country>Brazil</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study provides an experimental paradigm and state-of-the-art analysis method to study the existence of call types and transition differences among Mongolian gerbil families in a naturalistic environment. While the evidence supporting the authors' claims is <bold>solid</bold>, the conclusions would be significantly strengthened by additional analyses and a larger number of families and pup generations. The work will likely be of interest to the auditory neuroscience and vocal neuroethology communities.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89892.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
This research offers an in-depth exploration and quantification of social vocalization within three families of Mongolian gerbils. In an enlarged, semi-natural environment, the study continuously monitored two parent gerbils and their four pups from P14 to P34. Through dimensionality reduction and clustering, a diverse range of gerbil call types was identified. Interestingly, distinct sets of vocalizations were used by different families in their daily interactions, with unique transition structures exhibited across these families. The primary results of this study are compelling, although some elements could benefit from clarification</p>
<p>Strengths:</p>
<p>
Three elements of this study warrant emphasis. Firstly, it bridges the gap between laboratory and natural environments. This approach offers the opportunity to examine natural social behavior within a controlled setting (such as specified family composition, diet, and life stages), maintaining the social relevance of the behavior. Secondly, it seeks to understand short-timescale behaviors, like vocalizations, within the broader context of daily and life-stage timescales. Lastly, the use of unsupervised learning precludes the injection of human bias, such as pre-defined call categories, allowing the discovery of the diversity of vocal outputs.</p>
<p>Weaknesses:</p>
<p>
1. While the notable differences in vocal clusters across families are convincing, the drivers of these differences remain unclear. Are they attributable to &quot;dialect,&quot; call usage, or specific vocalizing individuals (e.g., adults vs. pups)? Further investigation, via a literature review or additional observation, into acoustic differences between adult and pup calls is recommended. Moreover, a consistent post-weaning decrease in the bottom-left cluster (Fig. S3) invites interpretation: could this reflect drops in pup vocalization?</p>
<p>2. Developmental progression, particularly during pre-weaning periods when pup vocal output remains unstable, might be another factor influencing cross-family vocal differences. Representing data from this non-stationary process as an overall density map could result in the loss of time-dependent information. For instance, were dominating call types consistently present throughout the recording period, or were they prominent only at specific times? Displaying the evolution of the density map would enhance understanding of this aspect.</p>
<p>3. Family-specific vocalizations were credited to the transition structure, a finding that may seem obvious if the 1-gram (i.e., the proportion of call types) already differs. This result lacks depth unless it can be demonstrated that, firstly, the transition matrix provides a robust description of the data, and secondly, different families arrange the same set of syllables into unique sequences.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89892.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Peterson et al., perform a series of behavioral experiments to study the repertoire and variance of Mongolian gerbil vocalizations across social groups (families). A key strength of the study is the use of a behavioral paradigm which allows for long term audio recordings under naturalistic conditions. This experimental set-up results in the identification of additional vocalization types. In combination with state of the art methods for vocalization analysis, the authors demonstrate that the distribution of sound types and the transitions between these sound types across three gerbil families is different. This is a highly compelling finding which suggests that individual families may develop distinct vocal repertoires. One potential limitation of the study lies in the cluster analysis used for identifying distinct vocalization types. The authors use a Gaussian Mixed Model (GMM) trained on variational auto Encoder derived latent representation of vocalizations to classify recorded sounds into clusters. Through the analysis the authors identify 70 distinct clusters and demonstrate a differential usage of these sound clusters across families. While the authors acknowledge the inherent challenges in cluster analysis and provide additional analyses (i.e. maximum mean discrepancy, MMD), additional analysis would increase the strength of the conclusions. In particular, analysis with different cluster sizes would be valuable. An additional limitation of the study is that due to the methodology that is used, the authors can not provide any information about the bioacoustic features that contribute to differences in sound types across families which limits interpretations about how the animals may perceive and react to these sounds in an ethologically relevant manner.</p>
<p>The conclusions of this paper are well supported by data, but certain parts of the data analysis should be expanded and more fully explained.</p>
<p>• Can the authors comment on the potential biological significance of the 70 sound clusters? Does each cluster represent a single sound type? How many vocal clusters can be attributed to a single individual? Similarly, can the authors comment on the intra-individual and inter-individual variability of the sound types within and across families?</p>
<p>
• As a main conclusion of the paper rests on the different distribution of sound clusters across families, it is important to validate the robustness of these differences across different cluster parameters. Specifically, the authors state that &quot;we selected 70 clusters as the most parsimonious fit&quot;. Could the authors provide more details about how this was fit? Specifically, could the authors expand upon what is meant by &quot;prior domain knowledge about the number of vocal types...&quot;. If the authors chose a range of cluster values (i.e. 10, 30, 50, 90) does the significance of the results still hold?</p>
<p>
• While VAEs are powerful tools for analyzing complex datasets in this case they are restricted to analysis of spectrogram images. Have the authors identified any acoustic differences (i.e. in pitch, frequency, and other sound components) across families?</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89892.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary: In this study, Peterson et al. longitudinally record and document the vocal repertoires of three Mongolian gerbil families. Using unsupervised learning techniques, they map the variability across these groups, finding that while overall statistics of, e.g., vocal emission rates and bout lengths are similar, families differed markedly in their distributions of syllable types and the transitions between these types within bouts. In addition, the large and rich data are likely to be valuable to others in the field.</p>
<p>Strengths:</p>
<p>
- Extensive data collection across multiple days in multiple family groups.</p>
<p>
- Thoughtful application of modern analysis techniques for analyzing vocal repertoires.</p>
<p>
- Careful examination of the statistical structure of vocal behavior, with indications that these gerbils, like naked mole rats, may differ in repertoire across families.</p>
<p>Weaknesses:</p>
<p>
- The work is largely descriptive, documenting behavior rather than testing a specific hypothesis.</p>
<p>
- The number of families (N=3) is somewhat limited.</p>
</body>
</sub-article>
</article>