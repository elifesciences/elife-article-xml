<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">99547</article-id>
<article-id pub-id-type="doi">10.7554/eLife.99547</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.99547.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Bridging verbal coordination and neural dynamics</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Schwab-Mohamed</surname>
<given-names>Isaïh</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>isaih.mohamed@univ-amu.fr</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Mercier</surname>
<given-names>Manuel R</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Trébuchon</surname>
<given-names>Agnès</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Morillon</surname>
<given-names>Benjamin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lancia</surname>
<given-names>Leonardo</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Schön</surname>
<given-names>Daniele</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/035xkbk20</institution-id><institution>Aix Marseille University, Inserm, INS, Inst Neurosci Syst</institution></institution-wrap>, <city>Marseille</city>, <country country="FR">France</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/035xkbk20</institution-id><institution>Aix-Marseille Univ, Institute of Language, Communication and the Brain</institution></institution-wrap>, <city>Marseille</city>, <country country="FR">France</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05jrr4320</institution-id><institution>APHM, Hôpital de la Timone, Service de Neurophysiologie Clinique</institution></institution-wrap>, <city>Marseille</city>, <country country="FR">France</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05whq8x35</institution-id><institution>Aix Marseille University, CNRS, Laboratoire Parole et Langage (LPL)</institution></institution-wrap>, <city>Aix-en-Provence</city>, <country country="FR">France</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Obleser</surname>
<given-names>Jonas</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Lübeck</institution>
</institution-wrap>
<city>Lübeck</city>
<country>Germany</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-08-21">
<day>21</day>
<month>08</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-05-13">
<day>13</day>
<month>05</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP99547</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-06-02">
<day>02</day>
<month>06</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-04-25">
<day>25</day>
<month>04</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.04.23.590817"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-08-21">
<day>21</day>
<month>08</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.99547.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.99547.1.sa2">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99547.1.sa1">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99547.1.sa0">Reviewer #2 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Schwab-Mohamed et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Schwab-Mohamed et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-99547-v2.pdf"/>
<abstract>
<title>Abstract</title><p>Our use of language, which is profoundly social in nature, essentially takes place in interactive contexts and is shaped by precise coordination dynamics that interlocutors must observe. Thus, language interaction is highly demanding on fast adjustment of speech production. Here, we developed a real-time coupled-oscillators virtual partner that allows - by changing the coupling strength parameters - to modulate the ability to synchronise speech with a virtual speaker. Then, we recorded the intracranial brain activity of 16 patients with drug-resistant epilepsy while they performed a verbal coordination task with the virtual partner (VP). More precisely, patients had to repeat short sentences synchronously with the VP. This synchronous speech task is efficient to highlight both the dorsal and ventral language pathways. Importantly, combining time-resolved verbal coordination and neural activity shows more spatially differentiated patterns and different types of neural sensitivity along the dorsal pathway. More precisely, high-frequency activity in left secondary auditory regions is highly sensitive to verbal coordinative dynamics, while primary regions are not. Finally, the high-frequency activity of the IFG BA44 (bilaterally) seems to specifically index the online coordinative adjustments that are continuously required to compensate deviation from synchronisation. These findings illustrate the possibility and value of using a fully dynamic, adaptive and interactive language task to gather deeper understanding of the subtending neural dynamics involved in speech perception, production as well as their interaction.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>The present revision includes now the data, analyses and results from the right hemisphere, an additional analysis and an extended Discussion.
Figure 1 revised; Materials and Methods section updated; Results section updated; Discussion section revised; References section updated; Supplementary material section updated (4 new figures).</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Language has been most frequently studied by separately assessing perception and production in an isolated context, in contrast to the interactive context that characterizes best its daily usage. The use of language in an interactive context, particularly during conversations, calls upon numerous predictive and adaptive processes (<xref ref-type="bibr" rid="c72">Pickering &amp; Gambi, 2018</xref>). Importantly, analyses of conversational data have highlighted the phenomenon of interactive alignment (<xref ref-type="bibr" rid="c73">Pickering &amp; Garrod, 2004</xref>), illustrating that during a verbal exchange interlocutors tend to imitate each other and to align their linguistic representations on several levels including phonetic, syntactic, or semantic (<xref ref-type="bibr" rid="c28">Garrod &amp; Pickering, 2009</xref>). This is an unconscious and dynamic phenomenon that possibly renders exchanges between speakers more fluid (<xref ref-type="bibr" rid="c51">Marsh et al., 2009</xref>). It consists in mutual anticipation (prediction) and coordination of speech production, leading, for instance, to a reduction of turn-taking durations (<xref ref-type="bibr" rid="c45">Levinson, 2016</xref> ; <xref ref-type="bibr" rid="c14">Corps et al., 2018</xref>).</p>
<p>Recently, in an effort to assess speech and language in more ecological contexts, researchers in neuroscience have used interactive paradigms to study some of these coordinative phenomena. These studies involved turn- taking behaviours such as alternating naming tasks (<xref ref-type="bibr" rid="c58">Mukherjee et al., 2019</xref>), questions and answers investigating motor preparations (<xref ref-type="bibr" rid="c4">Bögels et al., 2015</xref>), or manipulating the turn predictability in end-of-turn detection tasks (<xref ref-type="bibr" rid="c50">Magyari et al., 2014</xref>; for a review see, Bögels et al., 2017). However, the synchronous speech paradigm has been overlooked. This paradigm requires the simultaneous and synchronized production of the same word, sentence, or text between two people. Interestingly, this task is remarkably well performed without any particular training, both between a speaker and a recording as well as between two speakers (<xref ref-type="bibr" rid="c15">Cummins, 2002</xref>; <xref ref-type="bibr" rid="c16">Cummins, 2003</xref>; <xref ref-type="bibr" rid="c1">Assaneo et al., 2019</xref>). As in most joint tasks, individuals must mutually adjust their behaviour (here speech production) to optimize coordination (<xref ref-type="bibr" rid="c17">Cummins, 2009</xref>). Furthermore, synchronous speech favors the emergence of alignment phenomena, such as the fundamental frequency or the syllable onset (<xref ref-type="bibr" rid="c1">Assaneo et al., 2019</xref>; <xref ref-type="bibr" rid="c5">Bradshaw &amp; McGettigan, 2021</xref> ; <xref ref-type="bibr" rid="c6">Bradshaw et al., 2023</xref> ; <xref ref-type="bibr" rid="c7">Bradshaw et al., 2024</xref>). Overall synchronous speech represents a strong interactive framework allowing a good level of experimental control. It offers several possibilities for neurophysiological investigation of both speech perception and production and is an interesting case to consider for models of speech motor control.</p>
<p>Synchronous speech resembles to a certain extent delayed/altered auditory feedback tasks, which involve real- time perturbations in the speech production signal (such as changes in fundamental frequency and delay). These tasks can induce speech errors as well as modulations in speech and voice features (<xref ref-type="bibr" rid="c79">Stuart et al., 2002</xref>; <xref ref-type="bibr" rid="c83">Yamamoto &amp; Kawabata, 2014</xref>; <xref ref-type="bibr" rid="c37">Karlin et al., 2021</xref>). Additionally, these tasks provide insights into predictive models of speech motor control, where the brain generates an internal estimate of production and corrects errors when auditory feedback deviates from the estimate (<xref ref-type="bibr" rid="c32">Hickok et al., 2011</xref>; <xref ref-type="bibr" rid="c34">Houde &amp; Nagarajan, 2011</xref>; <xref ref-type="bibr" rid="c81">Tourville &amp; Guenther, 2011</xref>; Ozerk et al., 2022; <xref ref-type="bibr" rid="c24">Floegel et al., 2023</xref>). Previous studies have revealed increased responses in the superior temporal regions compared to normal feedback conditions (<xref ref-type="bibr" rid="c33">Hirano et al., 1997</xref>; <xref ref-type="bibr" rid="c31">Hashimoto &amp; Sakai, 2003</xref>; <xref ref-type="bibr" rid="c80">Takaso et al., 2010</xref>; Ozerk et al., 2022; <xref ref-type="bibr" rid="c23">Floegel et al., 2020</xref> ; see <xref ref-type="bibr" rid="c52">Meekings &amp; Scott, 2021</xref> for a review of error-monitoring and feedback control in the STG during speech production). However, synchronous speech paradigms allow for the investigation of the neural bases of coordinative behaviour rather than of error correction.</p>
<p>So far, the precise spectro-temporal dynamics and spatial distribution of the cortical networks underlying speech coordination remain unknown. To address this issue, we first developed a real-time coupled-oscillator virtual partner that allows - by changing the coupling strength parameters - to modulate the ability to synchronise speech with a speaker. The virtual partner (<xref ref-type="bibr" rid="c42">Lancia et al.,2017</xref>) and the synchronous speech task were first tested on a control group to ensure the ability of the virtual agent to coordinate its speech production in real time with the participants. In certain conditions, the agent was programmed to actively cooperate with the participants by synchronizing its syllables with theirs. In other conditions, the agent was programmed to deviate from synchronization by producing its syllables between those of the participants. As a result, participants were constantly required to adapt their verbal productions in order to maintain synchronization. Appropriate tuning of the coupling parameters of the virtual agent enabled us to create a variable context of coordination yielding a broad distribution of phase delays between the speaker and agent productions. Subsequently, we leveraged the excellent spatial sensitivity and temporal resolution of stereotactic depth electrodes recordings and acquired neural activity from 16 patients with drug-resistant epilepsy while they performed the adaptive synchronous speech task with the virtual partner.</p>
</sec>
<sec id="s2">
<title>Materials and methods</title>
<sec id="s2a">
<title>Control – participants</title>
<p>30 participants (17 women, mean age 24.7 y, range 19-42 y) took part in the study. All were French native speakers with normal hearing and no neurological disorders. Participants provided written informed consent prior to the experimental session and the experimental protocol was approved by the Institutional Review board of the French Institute of Health (IRB00003888). 5 participants (2 women) were excluded from analysis for poor signal-to-noise ratio in speech recordings.</p>
</sec>
<sec id="s2b">
<title>Patients – participants</title>
<p>16 patients (7 women, mean age 29.8 y, range 17 - 50 y) with pharmacoresistant epilepsy took part in the study. They were included if their implantation map covered at least partially the Heschl’s gyrus and had sufficiently intact diction to support relatively sustained language production. All patients were French native speakers. Neuropsychological assessments carried out before stereotactic EEG (sEEG) recordings indicated that they had intact language functions and met the criteria for normal hearing. In none of them the auditory areas were part of their epileptogenic zone as identified by experienced epileptologists. Recordings took place at the Hôpital de La Timone (Marseille, France). Patients provided written informed consent prior to the experimental session and the experimental protocol was approved by the Institutional Review board of the French Institute of Health (IRB00003888).</p>
</sec>
<sec id="s2c">
<title>Data acquisition</title>
<p>The speech signal was recorded using a microphone (RODE NT1) adjusted on a stand so that it was positioned in front of the participant’s mouth. Etymotic insert earphones (Etymotic Research E-A-R-TONE gold) fitted with 10mm foam eartips were used for sound presentation. The parameters and sound adjustment were set using an external low-latency (∼5ms) sound card (RME Babyface Pro Fs, <xref ref-type="bibr" rid="c38">Kim et al., 2020</xref>), allowing a tailored and temporally precise configuration for each participant. A calibration was made to find a comfortable volume and an optimal balance for both the sound of the participant’s own voice, which was fed back through the headphones, and the sound of the stimuli. The aim of this procedure was that the patient would subjectively perceive their voice and the VP-voice in equal measure. VP voice was delivered at approximately 70dB.</p>
<p>The sEEG signal was recorded using depth electrodes shafts with a 0.8 mm diameter containing 5 to 18 electrode contacts (Dixi Medical or Alcis, Besançon, France). The contacts were 2 mm long and were spaced from each other by 1.5 mm. The placement of the electrode implantations was determined solely on clinical grounds.</p>
<p>Sixteen patients with a total of 236 electrodes (145 in the left hemisphere) and 2395 contacts (1459 in the left hemisphere, see <xref rid="fig1" ref-type="fig">Figure 1</xref>). While this gives a rather sparse coverage of the right hemisphere, we decided, due to the rarity of this type of data, to report results for both hemispheres, with figures for the left hemisphere in the main text and figures for the right hemisphere in the supplementary section. All Patients were recorded in a sound-proof Faraday cage using a 256-channels amplifier (Brain Products), sampled at 1kHz and high-pass filtered at 0.16 Hz.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1</label>
<caption><title>Anatomical localization of the sEEG electrodes for each patient projected in MNI space on the lateral 3D view (top) and on the top view (N=16).</title></caption>
<graphic xlink:href="590817v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2d">
<title>Stimuli</title>
<p>Four stimuli corresponding to four short sentences were pre-recorded by both a female and a male speaker. This allowed to adapt to the natural gender differences in fundamental frequency (i.e. so that the VP gender matched that of the patients). All stimuli were normalised in amplitude.</p>
<p>Stimuli consisted of four sentences: &quot;papi mamie&quot; (/<italic>papi mami</italic>/, grandpa grandma) / &quot;papi m’a dit&quot; (/<italic>papi ma di</italic>/, grandpa told me) / &quot;mamie lavait ma main&quot; (/<italic>mami lavɛ ma mɛ̃</italic>/, grandma washed my hand) / &quot;mamie manie ma main&quot; (/<italic>mami mani ma mɛ̃</italic>/, grandma handles my hand). The four sentences purposely differed in terms of number of syllables (4-4-6-6). Moreover, two of them contained deviations from an otherwise repeating phonological pattern: both in /<italic>papi ma di</italic>/ and in /<italic>mami mani ma mɛ̃</italic>/, the repeated opening/closing of the lips is substituted by the formation and release of a tongue constriction. These manipulations made the sentences more or less easy to articulate (easy-medium-easy-medium). Sentence durations were 2.07, 2.11, 3.16 and 2.87s, respectively with a syllable rate of ∼3Hz.</p>
</sec>
<sec id="s2e">
<title>Experimental design</title>
<p>Participants, comfortably seated in a medical chair, were instructed that they would perform a real-time interactive synchronous speech task with an artificial agent (Virtual Partner, henceforth VP, see next section) that can modulate and adapt to the participant’s speech in real time.</p>
<p>The experiment required 3 steps (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). First, the sentences were presented in written form and the experimenter verified that each participant could pronounce them correctly. Second, a training phase took place. This required repeating each stimulus over and over together with the VP for ∼14 seconds. More precisely, the sentence was first presented on a screen. When the participant pressed the “space” bar, the visual stimulus went off, and the VP started to « speak ». The participant was instructed to repeat the stimuli as synchronously as possible with the VP for the whole trial duration. In the training phase the VP did not adapt to the participant.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2</label>
<caption><title>Paradigm and coordination indexes.</title>
<p>(A) Top: illustration of one trial of the interactive synchronous speech repetition task (orange: virtual partner speech; blue: participant speech; stimulus papi m’a dit repeated 10 times ; only the 10 first seconds are represented). <bold>Bottom:</bold> the four speech utterances used in the task and the experimental procedure. <bold>(B)</bold> Speech signals processing stages. The top panel corresponds to the speech envelope, the second to the phase of speech envelope and the third panel to the phase difference between VP and participant speech envelopes, illustrating the coordination dynamics along one trial. <bold>(C)</bold> Left: distributions of verbal coordination index (phase locking values between VP and participant speech envelopes, for each trial) for all participants (top) and patients. Right: boxplots for control participants (top) and patients showing the trial- averaged verbal coordination index as a function of the virtual partner parameters (in-phase coupling vs coupling with a 180° shift).</p></caption>
<graphic xlink:href="590817v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The training allowed participants to familiarise with the synchronous speech task but also to build a personalised virtual partner model incorporating the articulatory variability of each participant (see section below).</p>
<p>The training was followed by a resting phase, allowing the recording of resting state activity for a period of 5 min. This time also allowed to build, for each participant, the virtual partner model.</p>
<p>The third step was the actual experiment. This was identical to the training but consisted of 24 trials (14s long, speech rate ∼3Hz, yielding ∼1000 syllables). Importantly, the VP varied its coupling behaviour to the participant. More precisely, for a third of the trials the VP had a neutral behaviour (close to zero coupling: k = +/- 0.01). For a third it had a moderate coupling, meaning that the VP synchronised more to the participant speech (k = - 0.09). And for the last third of the trials the VP had a moderate coupling but with a phase shift of pi/2, meaning that it moderately aimed to speak in between the participant syllables (k = + 0.09). The coupling values were empirically determined on the basis of a pilot experiment in order to induce more or less synchronization but keeping the phase-shifted coupling at a rather implicit level. In other terms, while participants knew that the VP would adapt, they did not necessarily know in which direction the coupling went. Depending on patient fatigue, a second experimental session with 24 extra trials, was proposed (for 6 of the 16 patients). The control group participant run a single session of 48 trials.</p>
</sec>
<sec id="s2f">
<title>Virtual partner (principles)</title>
<p>The virtual partner (henceforth VP) used for the experiment allows to generate speech (words or short utterances) while adapting it in real-time to the concurrent speech input. The VP environment is built upon the Psychtoolbox-3 program and operates within MATLAB, leveraging custom C subroutines to boost its performance. Its operation revolves around a loop whose iterations are executed at consistent intervals of Δt = 4ms. During each iteration, the program analyzes the latest segment (25ms) of speech produced by the participant and streams a portion of speech to the output device. More precisely, at each iteration of the main loop, the functioning of the VP can be described in four steps:
<list list-type="order">
<list-item><p>A feature vector is extracted from the last chunk of the input signal.</p></list-item>
<list-item><p>The phase value of the input signal chunk is calculated by mapping the input feature vector onto the corresponding vector of the stimulus signal and retrieving the associated phase value. This step is performed using a dynamic-time-warping algorithm (<xref ref-type="bibr" rid="c19">Dixon, 2005</xref>). To enhance precision, the input chunk is mapped onto several model utterances (all time-aligned with the signal used as a stimulus) that are tailored to the characteristics of the participant’s speech in the training phase.</p></list-item>
<list-item><p>A chunk of stimulus signal is chosen to be sent to the output device. The selection is guided by applying the <xref ref-type="bibr" rid="c39">Kuramoto (1975)</xref> equation to the difference between the phase values representing the current positions of the participant and the VP in their syllabic cycles. This enables real-time lengthening or shortening of speech chunks as needed to adjust towards the preferred phase (0 or π/2). Significantly, in applying this equation, we assign a specific value to the coupling strength parameter « k », linking the behaviour of the VP to that of the participant. Values can vary from trial to trial and could be close to 0 (k = +/- 0.01) resulting in a neutral behaviour, negative values (k = - 0.09) equivalent to a moderate coupling behaviour (tendency to synchronize), or positive values (k = + 0.09) corresponding to a moderate coupling behaviour with a phase shift of π/2 (tendency to speak in between participant syllables).</p></list-item>
<list-item><p>The selected chunk of stimulus signal is integrated into the output stream via WSOLA synthesis (Waveform Similarity Overlap-Add).</p></list-item>
</list>
Of note, the coupling strength were chosen to be rather weak and thus do not allow to reach 0 or π/2 phase synchrony, but rather yield the desired large panel of phase delays in the VP-participant coordinative behaviour (see Figure1).</p>
</sec>
<sec id="s2g">
<title>Data analysis speech signal</title>
<p>Speech signals of the participants and the VP were processed using Praat and Python scripts.</p>
<p>First, raw speech signals were downsampled from 48KHz to 16KHz. Then speech envelope was extracted using a pass-band filter between 2.25 and 5Hz. The phase was computed using the Hilbert transform.</p>
<p>To quantify the degree of coordination of the verbal interaction (verbal coordination index, VCI) we computed the phase locking value using mne_connectivity python function spectral_connectivity_time with the method ‘plv’ based on the phase locking value index proposed by <xref ref-type="bibr" rid="c41">Lachaux et al. (1999)</xref>. Phase locking was computed for each trial on speech temporal envelope (<xref rid="fig1" ref-type="fig">Fig 1</xref>.B), resulting in 24 or 48 verbal coordination indexes (VCI) per patient depending on the number of sessions performed. To assess the effect of the coupling parameter we computed a linear mixed-model contrasting <italic>in-phase coupling trials and trials with a coupling set towards a 180° shift</italic> (lmer(VCI ∼ K + (1|participant)). As expected, because of the varying adaptive behaviour of the VP, VCI varies across trials, indexing more or less efficient coordinative behaviour (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). Moreover, in order to estimate whether the level of performance was greater than chance level, we computed, for each patient and each trial, a null distribution obtained by randomly shifting the phase between the VP and the patient speech (500 times, see Figure S1).</p>
</sec>
<sec id="s2h">
<title>sEEG signal</title>
<sec id="s2h1">
<title>General preprocessing related to electrodes localisation</title>
<p>To increase spatial sensitivity and reduce passive volume conduction from neighbouring regions (<xref ref-type="bibr" rid="c53">Mercier et al., 2017</xref>), the signal was offline re-referenced using bipolar montage. That is, for a pair of adjacent electrode contacts, the referencing led to a virtual channel located at the midpoint locations of the original contacts. To precisely localize the channels, a procedure similar to the one used in the iELVis toolbox was applied (<xref ref-type="bibr" rid="c29">Groppe et al., 2017</xref>). First, we manually identified the location of each channel centroid on the post-implant CT scan using the Gardel software (Medina <xref ref-type="bibr" rid="c82">Villalon et al., 2018</xref>). Second, we performed volumetric segmentation and cortical reconstruction on the pre-implant MRI with the Freesurfer image analysis suite (documented and freely available for download online <ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu/">http://surfer.nmr.mgh.harvard.edu/</ext-link>). This segmentation of the pre-implant MRI with SPM12 provides us with both the tissue probability maps (i.e. gray, white, and cerebrospinal fluid (CSF) probabilities) and the indexed-binary representations (i.e., either gray, white, CSF, bone, or soft tissues). This information allowed us to reject electrodes not located in the brain. Third, the post-implant CT scan was coregistered to the pre-implant MRI via a rigid affine transformation and the pre-implant MRI was registered to the MNI template (MNI 152 Linear), via a linear and a non-linear transformation from SPM12 methods (<xref ref-type="bibr" rid="c70">Penny et al., 2011</xref>), through the FieldTrip toolbox (<xref ref-type="bibr" rid="c65">Oostenveld et al., 2011</xref>). Fourth, applying the corresponding transformations, we mapped channel locations to the pre-implant MRI brain that was labeled using the volume- based Human Brainnetome Atlas (<xref ref-type="bibr" rid="c20">Fan et al., 2016</xref>).</p>
</sec>
<sec id="s2h2">
<title>Signal preprocessing</title>
<p>Continous signal was filtered using 1) a notch filter at 50Hz and harmonics up to 300Hz to remove power line artifacts, 2) a bandpass filter between 0.5Hz and 300Hz.</p>
<p>To define artifacted channel we used the broadband (raw) signal delimited on the experimental task recording. Channels with a variance greater than 2*IQR (interquartile range, i.e. a non-parametric estimate of the standard deviation) were tagged as artifacted channels (6% of the channels). Channels defined as artifacted were excluded from subsequent analysis.</p>
<p>The continuous signal during the task was then epoched from -0.2 to 14s relative to the onset of the first stimulus (repetition) of each trial. Generating 24 or 48 epochs depending on the number of sessions each patient had completed. A baseline correction was applied from -0.1 to 0s. The first 500ms were discarded from the epoched data to avoid the activity burst generated at the onset of the stimulus.</p>
<p>The continuous signal from the resting state session was also epoched in seventeen 14-second non-overlapping epochs.</p>
</sec>
<sec id="s2h3">
<title>Power spectral density</title>
<p>The power spectral density (PSD) computation was conducted for each channels, with the MNE-python function Epochs.compute_psd, trial-by-trial (epochs) in a range of 125 frequencies, logarithmically scaled, ranging from</p>
<p>0.5 to 125 Hz. Six canonical frequency bands were investigated by averaging the power spectra between their respective boundaries: delta (1-4 Hz), theta (4-8 Hz), alpha (8-13 Hz), beta (13-30 Hz), low gamma (30-50 Hz) and high frequency activity or HFa (70-125 Hz).</p>
</sec>
<sec id="s2h4">
<title>Global effect</title>
<p>The global effect of the task (versus period of rest) was computed on each frequency band by subtracting first the mean resting state activity from the mean experimental activity and then dividing by the mean resting state activity. This approach has the advantage of centering the magnitude before expressing it in percentage (<xref ref-type="bibr" rid="c54">Mercier et al., 2022</xref>). For each frequency band and channel, the statistical difference between task activity and the baseline (resting) was estimated with permutation tests (N=1000) using the SciPy library.</p>
</sec>
<sec id="s2h5">
<title>Coupling behavioural and neurophysiological data</title>
<p>Behavioural speech data and neurophysiological data were jointly analysed using three approaches. In the first approach a two step procedure was used. First, for each frequency band, channel and trial, we computed the mean power. Then, for each frequency band and channel, we computed the correlation across trials using a non- parametric correlation metric (Spearman) between the power and the verbal coordination index (VCI between VP and patient speech). A rho value was thus attributed to each channel and frequency band. The significance was assessed using a permutation approach, similar to the one used for the global effect (see above). In the second approach, we computed the phase-amplitude coupling (PAC) between speech phase and HFa. More precisely, we used two measures for the phase: 1) the phase of the speech envelope of the VP, corresponding to the speech input, 2) the instantaneous phase difference between VP and patient phases, corresponding to the instantaneous coordination of participant and virtual partner (see <xref rid="fig2" ref-type="fig">Figure 2B</xref>). As for power, we used the high frequency activity from 70 to 125 Hz (considered as a proxy for activity population-level spiking activity of neurons ; <xref ref-type="bibr" rid="c8">Buzsáki et al., 2012</xref>). The computation was performed using Tensorpac, an open-source Python toolbox for tensor-based phase-amplitude coupling (PAC) measurement in electrophysiological brain signals (<xref ref-type="bibr" rid="c12">Combrisson et al. 2020</xref>). In the third approach, we assessed whether the phase-amplitude relationship (or coupling) depends upon the anticipatory (negative delays) or compensatory (positive delays) behaviour between the VO and the patients’ speech. We computed the average delay in each trial using a cross-correlation approach on speech signals (between patient and VP) with the MATLAB function <italic>xcorr</italic>. A median split (patient-specific ; average median split = 0ms, average sd = 24ms) was applied to conserve a sufficient amount of data, classifying trials below the median as « anticipatory behaviour » and trials above the median as « compensatory behaviour ». Then we conducted the phase-amplitude coupling analyses on positive and negative trials separately.</p>
</sec>
<sec id="s2h6">
<title>Clustering analysis</title>
<p>A spatial unsupervised clustering analysis (k-means) was conducted on all significant channels, separately for the global effect (task versus rest) and brain-behaviour correlation analyses. Precisely, we used the silhouette score method on the k-means result (<xref ref-type="bibr" rid="c75">Rousseeuw, 1987</xref>; <xref ref-type="bibr" rid="c77">Shahapure &amp; Nicholas, 2020</xref>). This provides a measure of consistency within clusters of data (or alternatively the goodness of clusters separation). Scores were computed for difference numbers of clusters (from two to ten). The highest silhouette score indicates the optimal number of clusters. Clustering and slihouette scores were computed using the Scikit-learn’s Kmeans and silhouette score function (<xref ref-type="bibr" rid="c69">Pedregosa et al., 2011</xref>). The statistical difference between spatial clustering in global effect and brain-behaviour correlation was estimated with linear model using the R function <italic>lm</italic> (stat package), post-hoc comparisons were corrected for multiple comparisons using the Tukey test (lsmeans R package ; <xref ref-type="bibr" rid="c43">Lenth, 2016</xref>). The statistical difference between clustering in global effect and behaviour correlation across the number of clusters was estimated using permutation tests (N=1000) by computing the silhouette score difference between the two conditions.</p>
</sec>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Speech coordinative behaviour</title>
<p>The present synchronous speech task allowed us to create a more or less predictable context of coordination, with the objective of obtaining a wide range of coordination variability. Indeed, the intrinsic nature of the task (speech coordination) on one side and the variable coupling parameter of the VP on the other, require continuous subtle adjustments of the participants speech production. The degree of coordination between speech signals (VP and participant) was assessed at the syllabic level computing the phase locking value between the speech temporal envelopes, a measure of the strength of the interaction (or coupling) of two signals. This metric, the verbal coordination index (VCI), is a proxy of the quality of the performance in coordinating speech production with the VP. Overall, the coordinative behaviour was affected, for both controls and patients, by the coupling parameters of the model with a better coordination when the VP was set to synchronize with participants compared to when it was set to speak with a 180° syllabic shift (controls: t ratio = 4.55, p = &lt;.0001; patients: t ratio = 6.53, p = &lt;.0001, see boxplots in <xref rid="fig2" ref-type="fig">Figure 2C</xref>). This produced, as desired, a rather large coupling variability (controls: range : 0.06 - 0.84 ; mean : 0.49 ; median : 0.49 ; patients range : 0.11 - 0.94 ; mean : 0.55 ; median : 0.54, see <xref rid="fig2" ref-type="fig">Fig 2C</xref>). This variability was also present at the individual level (see, for patients only, Figure S1). Nonetheless, while variable, the coordinative behaviour was significantly better than chance for every patient (see Figure S1).</p>
</sec>
<sec id="s3b">
<title>Synchronous speech strongly activates the language network from delta to high gamma range</title>
<p>To investigate the sensitivity of synchronous speech in generating spectrally-resolved neural responses, we first analyzed the neural responses in both a spatially and spectrally resolved manner with respect to a resting-state baseline condition. Overall, neural responses are present in all six canonical frequency bands, from the delta range (1-4 Hz) up to high frequency activity (HFa, 70-125 Hz, see <xref rid="fig3" ref-type="fig">Fig 3A</xref>) with medium to large modulation (increase or decrease) in activity compared to baseline (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). More precisely, while theta, alpha and beta bands show massive desynchronisation, in particular in the STG BA41/42 (primary auditory cortex), STG BA22 (secondary auditory cortex), and IFG BA44 (Broca’s area), the low gamma and HFa bands are dominated by power increase in particular in the auditory cortex (STG BA 41/42) and in the inferior frontal gyrus (IFG BA44). This modulation between synchronization and desynchronization across frequencies was significant (F(5)=6.42, p&lt;.001 ; estimated with linear model using the R function <italic>lm</italic>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3</label>
<caption><title>Power spectrum analyses and correlation with verbal coordination index (left hemisphere).</title> <p>Each dot represents a channel where a significant effect was found either on <bold>(A)</bold> Global activity <bold>(</bold>Task versus Rest) for each frequency band. The activity is expressed in % of power change compared to resting; or on <bold>(B)</bold> Behaviour- related activity: r values of the Spearman correlation across trials between the iEEG power and the verbal coordination index (VCI). <bold>(C)</bold> The proportion of channels where a significant effect was found: in the task vs rest (orange), in the brain-behaviour correlation (green) or for both comparison (blue). The percentage in the center indicates the overall proportion of significant channels from the three categories with respect to the total number of channels.</p></caption>
<graphic xlink:href="590817v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>As expected, the whole language network is strongly involved, including both dorsal and ventral pathways (<xref rid="fig3" ref-type="fig">Fig 3A</xref>). More precisely, in the temporal lobe the superior, middle and inferior temporal gyri, in the parietal lobe the inferior parietal lobule (IPL) and in the frontal lobe the inferior frontal gyrus (IFG) and the middle frontal gyrus (MFG). Similar results are observed in the right hemisphere, neural responses are present across all six frequency bands with medium to large modulation in activity compared to baseline (Figure S2A) in the same regions. Desynchronizations are present in the theta, alpha and beta bands while the low gamma and HFa bands show power increases.</p>
</sec>
<sec id="s3c">
<title>The brain needs behaviour: global vs behaviour-specific neural activity</title>
<p>Comparing the overall activity during the task to activity during rest gives a broad view of the network involved in verbal coordination. However, as stated above, the task was conceived to engender a rather wide range of verbal coordination across trials for each participant. This variety of coordinative behaviours allows to explore the link between verbal and simultaneous neural activity, by computing the correlation across trials between the verbal coordination index and the mean power (see methods). This analysis allows us to estimate the extent to which neural activity in each frequency band is modulated as a function of the quality of verbal coordination.</p>
<p><xref rid="fig3" ref-type="fig">Figure 3B</xref> shows the significant r values (Spearman correlation) for each frequency band in the left hemisphere (see Figure S2B for the right hemisphere). The first observation is a gradual transition in the direction of correlations as we move up frequency bands, from positive correlations at low frequencies to negative ones at high frequencies (F(5)=2.68, p=.02). This effect, present in both hemispheres, mimics the reversed desynchronization/synchronization process in low and high frequency bands reported above. In other words, while in the low frequency bands stronger desynchronization goes along with weaker verbal coordination, in the HFa stronger activity is associated with weaker verbal coordination.</p>
<p>Importantly, compared to the global activity (task vs rest, <xref rid="fig3" ref-type="fig">Fig 3A</xref>), the neural spatial profile of the behaviour- related activity (<xref rid="fig3" ref-type="fig">Fig 3B</xref>) is more clustered, in the left hemisphere. Indeed, silhouette scores are systematically higher for behaviour-related activity compared to global activity, indicating greater clustering consistency across frequency bands (t(106)=7.79, p&lt;.001, see Figure S3). Moreover, silhouette scores are maximal, in particular for HFa, for five clusters (p &lt; .001), located in the IFG BA44, the IPL BA 40 and the STG BA 41/42 and BA22 (see Figure S3).</p>
<p>Comparing in both hemispheres global activity and behaviour-related activities shows that, for each frequency band, approximately 2/3 of the channels are only significant in the global activity (<xref rid="fig3" ref-type="fig">Figure 3C</xref>, orange part). Of the remaining third, half of the significant channels show a modulation of power compared to baseline that also significantly correlates with the quality of behavioural synchronisation (<xref rid="fig3" ref-type="fig">Figure 3C</xref>, blue part). The other half are only visible in the brain-behaviour correlation analysis (<xref rid="fig3" ref-type="fig">Figure 3C</xref>, green part, behaviour-specific).</p>
</sec>
<sec id="s3d">
<title>Spectral profiles in the language network are nuanced by behaviour</title>
<p>In order to further explore the brain-behaviour relation in an anatomically language-relevant network, we focused on high frequency activity (HFa) and on those regions (ROI) of the dorsal pathway recorded in at least 7 patients for the left hemisphere and at least 5 patients for the right hemisphere : STG BA41/42 (primary auditory cortex), STG BA22 (secondary auditory cortex), IPL BA40 (inferior parietal gyrus) and IFG BA44 (inferior frontal gyrus, Broca’s region). To balance data completeness and statistical power, we included only brain regions recorded in at least 7 patients (∼44% of the cohort) for the left hemisphere and at least 5 patients for the right hemisphere (∼31% of the cohort), ensuring sufficient representation while minimizing biases due to sparse data. Within each ROI, using spearman correlation, we quantified the link between neural activity and the degree of behavioural coordination. <xref rid="fig4" ref-type="fig">Figure 4B</xref> (left hemisphere) shows a dramatic decrease of HFa along the dorsal pathway. While left-STG BA41/42 presents the strongest power increase (compared to baseline), it shows no significant correlation with verbal coordination (t(28)=-1.81, p=.08 ; Student’s T test, FDR correction). By contrast, the left-STG BA22 shows both a significant power increase in the HFa and a significant negative correlation between HFa and behaviour (i.e., VCI) (t(29)=-4.40, p&lt;.0001 ; Student’s T test, FDR correction), marking a fine distinction between primary and secondary auditory cortex. Finally, the brain-behaviour correlation is maximal in the left-IFG BA44 (t(26)=-5.60, p&lt;.0001 ; Student’s T test, FDR correction).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4</label>
<caption><title>Group analysis by regions of interest for the left hemisphere.</title>
<p><bold>(A)</bold> Regions of interest (ROI) defined according to the cluster analysis (see Figure S3), the delimitation of regions is based on the Brainnetome atlas. <bold>(B)</bold> For each ROI, boxplots illustrate, in red, channels with significant global power changes (HFa, task vs rest) and, in blue, their corresponding r values (correlation between HFa power and verbal coordination index, VCI). Red and blue stars indicate a significant difference from a null distribution. Dots represent independent iEEG channels. The « n » below each region of interest specifies the number of patients. STG : superior temporal gyrus ; IPL : inferior parietal lobule ; IFG : inferior frontal gyrus ; BA : brodmann area.</p></caption>
<graphic xlink:href="590817v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The decrease in HFa along the dorsal pathway is replicated in the right hemisphere (Figure S4). However, while both the right STG BA41/42 and STG BA22 present a power increase (compared to baseline) — with a stronger increase for the STG BA41/42 — neither shows a significant correlation with verbal coordination (t(45)=-1.65, p=.1 ; t(8)=-0.67, p=.5 ; Student’s T test, FDR correction). By contrast, results in the right IFG BA44 are similar to the one observed in the left hemisphere with a significant power increase associated with a negative brain- behaviour correlation (t(17)=-3.11, p=.01 ; Student’s T test, FDR correction).</p>
</sec>
<sec id="s3e">
<title>The IFG is sensitive to speech coordination dynamics</title>
<p>To model the temporal dynamics of the relation between verbal coordination and neural activity we conducted a behaviour-brain phase amplitude coupling (PAC) analysis at the single trial level. That is, we used the power of high frequency neural activity (HFa: 70-125 Hz) and the low-frequency phase of the behaviour; the latter being either the phase of the speech signals of the virtual partner (VP) or the verbal coordination dynamics (i.e., the phase difference between VP and speaker).</p>
<p>When looking at the analysis of the left hemisphere (<xref rid="fig5" ref-type="fig">Figure 5A</xref>), coupling is strongest as expected in the auditory regions (STG BA41/42 and STG BA22), but it is also present in the left IPL and IFG. Notably, when comparing – within the regions of interest previously described – the PAC with the virtual partner speech and the PAC with the phase difference, the coupling relationship changes when moving along the dorsal pathway: a stronger coupling in the auditory regions with the speech input, no difference between speech and coordination dynamics in the IPL and a stronger coupling for the coordinative dynamics compared to speech signal in the IFG (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). When looking at the right hemisphere, we observe the same changes in the coupling relationship when moving along the dorsal pathway, except that no difference between speech and coordination dynamics is present in the right secondary auditory regions (STG BA22 ; Figure S5). Similar results were obtained when using the phase of the patient speech rather than the VP speech (as a control analysis). Finally, in order to assess whether the phase-amplitude relationship is different for anticipatory (negative delays) and compensatory (positive delays) behaviour between the VO and the patients’ speech we assessed the difference between PAC in trials with negative and positive delays (Figure S6). Although there seems to be a trend in the left IFG with anticipatory behaviour (negative lags) being associated to stronger neural coupling, this difference did not reach significance.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5</label>
<caption><title>Phase-amplitude coupling between virtual partner speech signal or coordination dynamics and high frequency activity (HFa).</title>
<p><bold>A)</bold> Representation of the increase in PAC expressed in % compared to surrogates mean when using the virtual partner speech (left) or the coordination dynamics (phase difference between VP and patient, right). The shaded (slight blue) area corresponds to the location of the IFG BA44<bold>. B)</bold> PAC values for VP (in red) and phase difference (in blue) by regions of interest. Statistical difference between the two types of PAC is calculated using paired Wilcoxon’s test (STG BA41/42 : p=.01 ; STG BA22 : p=.004 ; IPL BA40 : p=.6 ; IFG BA44 : p=.02). Y-axis range has been adjusted to better illustrate the contrast between VP speech and coordination dynamics.</p></caption>
<graphic xlink:href="590817v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>In this study, we investigated speech coordinative adjustments using a novel interactive synchronous speech task (<xref ref-type="bibr" rid="c42">Lancia et al., 2017</xref>). To assess the relation between speech coordination and neural dynamics, we capitalized on the excellent spatiotemporal sensitivity of human stereotactic recordings (sEEG) from 16 patients with drug-resistant epilepsy while they produced short sentences along with a virtual partner (VP). Critically, the virtual partner was able to coordinate and adapt its verbal production in real-time with those of the participants, thus enabling to create a variable context of coordination yielding a broad distribution of phase delays between participant and VP productions. Several interesting findings can be emphasized. Firstly, the task involving both speech production and perception is efficient to highlight both the dorsal and ventral pathways, from low to high frequency activity (<xref rid="fig3" ref-type="fig">Figure 3</xref>). Secondly, spectral profiles of neural responses in the language network are nuanced when combined with behavioural data, highlighting the fact that some regions are involved in the task in a general manner, while others are sensitive to the quality of verbal coordination (<xref rid="fig3" ref-type="fig">Figure 3</xref>). Thirdly, high- frequency activity in left secondary auditory regions shows a stronger sensitivity to behaviour (coordination success) compared to left primary auditory regions (<xref rid="fig4" ref-type="fig">Figure 4</xref>). Finally, the high-frequency activity of the IFG BA44 (bilaterally) specifically indexes the online coordinative adjustments that are continuously required to compensate for deviations from synchronisation (<xref rid="fig5" ref-type="fig">Figure 5</xref>).</p>
<sec id="s4a">
<title>Left secondary auditory regions are more sensitive to coordinative behaviour</title>
<p>In the left hemisphere, the increase of high frequency activity (HFa as compared to rest) in both the STG BA41/42 (primary auditory cortex) and STG BA22 (secondary auditory cortex) is associated with different cognitive functions. Indeed, when considering whether HFa correlates with the behavioural coordination index – here the phase locking value between the patient and speaker speech – only STG BA22 shows a significant (inverse) correlation with a reduced HFa in strongly coordinated trials. This spatial distinction made between primary and secondary auditory regions in terms of their sensitivity to task demands has already been observed in the auditory cortex, particularly in the HFa (<xref ref-type="bibr" rid="c61">Nourski, 2017</xref>). The use of auditory target detection paradigms – phonemic categorisation (<xref ref-type="bibr" rid="c11">Chang et al., 2011</xref>), tone detection (<xref ref-type="bibr" rid="c78">Steinschneider et al., 2014</xref>), and semantic categorisation tasks (<xref ref-type="bibr" rid="c62">Nourski et al., 2015</xref>) – where participants are asked to press a button when they hear the target, revealed task-dependent modulations only in the STG (posterolateral superior temporal gyrus or PLST) and not in Heschl’s gyrus. In these studies, modulations corresponded to increases in activity in response to targets compared to non-targets, which have been interpreted in terms of selective attention or a bias toward behaviourally relevant stimuli (<xref ref-type="bibr" rid="c71">Petkov et al., 2004</xref> ; <xref ref-type="bibr" rid="c55">Mesgarani &amp; Chang, 2012</xref>). Here we extend these findings to a more complex and interactive context.</p>
<p>The observed negative correlation between verbal coordination and high-frequency activity (HFa) in STG BA22 suggests a suppression of neural responses as the degree of behavioural synchrony increases. This result is reminiscent of findings on speaker-induced suppression (SIS), where neural activity in auditory cortex decreases during self-generated speech compared to externally-generated speech (<xref ref-type="bibr" rid="c52">Meekings &amp; Scott, 2021</xref>; <xref ref-type="bibr" rid="c60">Niziolek et al., 2013</xref>). However, our paradigm differs from traditional SIS studies in two critical ways: (1) the speaker’s own voice is always present and predictable from the forward model, /*an/d (2) no passive listening condition was included. Therefore, our findings cannot be directly equated with the original SIS effect. Instead, we propose that the suppression observed here reflects a SIS-related phenomenon specific to the synchronous speech context. Synchronous speech requires simultaneous monitoring of self- and externally-generated speech, a task that is both attentionally demanding and coordinative. This aligns with evidence from <xref ref-type="bibr" rid="c59">Ozker et al. (2022</xref>, <xref ref-type="bibr" rid="c68">2024</xref>), showing that the same neural populations in STG exhibit SIS and heightened responses to feedback perturbations. These findings suggest that SIS and speech monitoring are related processes, where suppressing responses to self-generated speech facilitates error detection. In our study, suppression of HFa as coordination increases may reflect reduced prediction errors due to closer alignment between perceived and produced speech signals. Conversely, increased HFa during poor coordination may signify greater mismatch, consistent with prediction error theories (<xref ref-type="bibr" rid="c34">Houde &amp; Nagarajan, 2011</xref>; <xref ref-type="bibr" rid="c26">Friston et al., 2020</xref>). Furthermore, when self- and externally-generated speech signals are temporally and phonetically congruent, participants may perceive external speech as their own. This echoes the &quot;rubber voice&quot; effect, where external speech resembling self- produced feedback is perceived as self-generated (<xref ref-type="bibr" rid="c84">Zheng et al., 2011</xref>; <xref ref-type="bibr" rid="c47">Lind et al., 2014</xref>; <xref ref-type="bibr" rid="c25">Franken et al., 2021</xref>). While this interpretation remains speculative, future studies could incorporate subjective reports to investigate this phenomenon in more detail.</p>
<p>Furthermore, the absence of correlation in the right STG BA22 (Figure S4) seems in first stance to challenge influential speech production models (e.g. <xref ref-type="bibr" rid="c30">Guenther &amp; Hickok, 2016</xref>) that propose that the right hemisphere is involved in feedback control. However, one needs to consider the the task at stake heavily relied upon temporal mismatches and adjustments. In this context, the left-lateralized sensitivity to verbal coordination reminds of the works of <xref ref-type="bibr" rid="c23">Floegel and colleagues (2020</xref>, 2023) suggesting that both hemispheres are involved depending on the type of error: the right auditory association cortex monitoring preferentially spectral speech features and the left auditory association cortex monitoring preferentially temporal speech features. Nonetheless, the right temporal lobe seems to be sensitive to speech coordinative behaviour, confirming previous findings using fMRI (<xref ref-type="bibr" rid="c36">Jasmin et al., 2016</xref>) and thus showing that the right hemisphere has an important role to play in this type of tasks (e.g. <xref ref-type="bibr" rid="c36">Jasmin et al., 2016</xref>).</p>
</sec>
<sec id="s4b">
<title>Inferior frontal gyrus (BA44) as a site of speech coordination/planning in dynamic context</title>
<p>Our results highlight the involvement of the inferior frontal gyrus (IFG) bilaterally, in particular the BA44 region, in speech coordination. First, trials with a weak verbal coordination (VCI) are accompanied by more prominent high frequency activity (HFa, <xref rid="fig4" ref-type="fig">Fig.4</xref>; Fig.S4). Second, when considering the within-trial time-resolved dynamics, the phase-amplitude coupling (PAC) reveals a tight relation between the low frequency behavioural dynamics (phase) and the modulation of high-frequency neural activity (amplitude, <xref rid="fig5" ref-type="fig">Fig.5B</xref> ; Fig.S5). This relation is strongest when considering the phase adjustments rather than the phase of speech of the VP per se : larger deviations in verbal coordination are accompanied by increase in HFa. Additionally, we also tested for potential effects of different asynchronies (i.e., temporal delay) between the participant’s speech and that of the virtual partner but found no significant differences (Fig.S6). While lack of delay-effect does not permit to conclude about the sensitivity of BA44 to absolute timing of the partner’s speech, its neural dynamics are linked to the ongoing process of resolving phase deviations and maintaining synchrony.</p>
<p>These findings are in line with the importance of higher-level frontal mechanisms for behavioural flexibility and their role in the hierarchical generative models underlying speech perception and production (<xref ref-type="bibr" rid="c13">Cope et al., 2017</xref>). More precisely, they are in line with works redefining the role of Broca’s area (BA44 and BA45) in speech production associating it more to speech planning rather than articulation per se (<xref ref-type="bibr" rid="c22">Flinker et al., 2015</xref>; <xref ref-type="bibr" rid="c2">Basilakos et al., 2018</xref>). Indeed, electrodes covering Broca’s area show a greatest activity before the onset of articulation and not during speech production. This has been interpreted in favour of a role at a prearticulatory stage rather than an « on-line » coordination of the speech articulators at least in picture naming (<xref ref-type="bibr" rid="c76">Schuhmann et al., 2009</xref>), word repetition (<xref ref-type="bibr" rid="c22">Flinker et al., 2015</xref>; <xref ref-type="bibr" rid="c21">Ferpozzi et al., 2018</xref>) and turn-taking (<xref ref-type="bibr" rid="c9">Castellucci et al., 2022</xref>). According to these studies Broca’s area may be a « functional gate » at a preacticulatory stage, allowing the phonetic translation before speech articulation (<xref ref-type="bibr" rid="c21">Ferpozzi et al., 2018</xref>). Our use of a synchronous speech task allows to refine this view by showing that these prearticulatory commands are of continuous rather than discrete nature. In other terms, the discrete (on-off) and ignition-like behaviour of neuronal populations in Broca’s area gating prearticulatory commands before speech may be due to the discrete nature of the tasks used to assess speech production. Notably, picture naming, word repetition, word reading and even turn-onsets imply that speech production is preceded by a silent period during which the speaker listens to speech or watches pictures. By contrast, the synchronous speech task requires continuous temporal adjustments of verbal productions in order to reach synchronisation with the virtual partner. Relatedly, the involvement of IFG in accurate speech timing has been previously shown via thermal manipulation (<xref ref-type="bibr" rid="c49">Long et al., 2016</xref>).</p>
<p>Of note, temporal adjustments (prediction error corrections) are also needed for fluent speech in general, beyond synchronous speech, and give rise to the rhythmic nature of speech. Temporal adjustments possibly take advantage of the auditory input and are referred to as audio-motor interactions that can be modeled as a coupled oscillator (<xref ref-type="bibr" rid="c74">Poeppel &amp; Assaneo, 2020</xref>). Interestingly, during speech perception, the coupling of theta and gamma bands in the auditory cortex reflects tracking of slow speech fluctuations to spiking gamma (<xref ref-type="bibr" rid="c56">Morillon et al., 2010</xref>; <xref ref-type="bibr" rid="c57">Morillon et al., 2012</xref>; <xref ref-type="bibr" rid="c35">Hyafil et al., 2015</xref>; <xref ref-type="bibr" rid="c48">Lizarazu et al. 2019</xref>; <xref ref-type="bibr" rid="c64">Oganian &amp; Chang, 2019</xref>; <xref ref-type="bibr" rid="c44">Leonard et al., 2024</xref>), similar to what we describe in the auditory cortex. By contrast, in the inferior frontal gyrus, the coupling in the high-frequency activity is strongest with the input-output phase difference (input of the VP - output of the speaker), a metric that could possibly reflect the amount of error in the internal computation to reach optimal coordination. This indicates that this region could have an implication in the optimisation of the predictive and coordinative behaviour required by the task. This well fits with the anatomical connectivity that has been described between Broca’s and Wernicke’s territory via the long segment of the arcuate fasciculus, possibly setting the base for a mapping from speech representations in Wernicke’s area to the predictive proprioceptive adjustments processed in Broca’s area (<xref ref-type="bibr" rid="c10">Catani &amp; Ffytche, 2005</xref>; <xref ref-type="bibr" rid="c63">Oestreich et al., 2018</xref>). This also aligns with broader theories on the relationship between perception and action, such as predictive coding and active inference, which propose shared sensory prediction mechanism and neural computational architecture for both processes (<xref ref-type="bibr" rid="c27">Friston et al., 2017</xref>, <xref ref-type="bibr" rid="c26">2020</xref>).</p>
<p>Finally, while the case of synchronous speech may seem quite far away from real-life conversational contexts, the models describing language interaction consider that listeners covertly imitate the speaker’s speech and timely construct a representation of the underlying communicative intention which allows early fluent turn- taking (<xref ref-type="bibr" rid="c72">Pickering &amp; Gambi, 2018</xref>; <xref ref-type="bibr" rid="c45">Levinson, 2016</xref>). Moreover, synchronous speech has recently gained interest in the neuroscience field due to important results showing a relation between anatomo-functional features and synchronization abilities. More precisely, Assaneo and collaborators (2019) used a spontaneous speech synchronization (SSS) test wherein participants produce the syllable /tah/ while listening to a random syllable sequence of predefined pace. The authors identified two groups of participants (high and low synchronisers) characterised by their ability to naturally synchronise their productions more or less easily with the auditory stimulus. Importantly, the ability to synchronise correlates to the degree of lateralization of the arcuate fasciculus, that connects the inferior frontal gyrus and the auditory temporal regions, high synchronizers showing greater lateralization to the left than the low synchronizers. The more this structural connectivity of the arcuate fasciculus is lateralised in the left hemisphere, the more the activity of the IFG is synchronised with the envelope of the audio stimulus of the SSS-test, during a passive listening task. A major limitation of EEG and MEG studies is that they are very sensitive to speech production artefacts, which is not the case of iEEG. Thus, the full dynamics of speech interaction are difficult to assess with surface recordings. Our findings extend these results in several manners. First, speech production is not limited to a single syllable, but to complex utterances. Secondly, the input auditory stimulus is not preset but adapts and changes behaviour in real-time as a function of the dynamic of the « dyad », here patient-virtual partner. Thirdly, and most importantly, iEEG recording allowing speech artefact-free data, we could extend the relation between coordination abilities and anatomical circuitry of the IFG to the neural dynamics of this same region, showing that it plays an important role in the temporal adjustment of speech that are necessary to coordinate to external speech.</p>
<p>To conclude, the present study illustrates the possibility and interest of using a fully dynamic, adaptive and interactive language task to gather deeper understanding of the subtending neural dynamics involved in speech perception, production as well as their interaction. It is worth noting that the influence of specific speech units, such as consonants versus vowels, on speech coordination remains to be explored. In non-interactive contexts, participants show greater sensitivity during the production of stressed vowels, possibly reflecting heightened attentional or motor adjustments (<xref ref-type="bibr" rid="c66">Oschkinat &amp; Hoole, 2022</xref>; <xref ref-type="bibr" rid="c46">Li &amp; Lancia, 2024</xref>). In this study, the VP’s adaptation relies on sensitivity to spectral cues, particularly phonetic transitions, with some (e.g., formant transitions) being more salient than others. However, how these effects manifest in an interactive setting remains an open question, as both interlocutors continuously adjust their speech in real time. Future studies could investigate whether coordination signals, such as phase resets, preferentially align with specific parts of the syllable.</p>
</sec>
</sec>
</body>
<back>
<sec>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1</label>
<caption><title>Distribution of verbal coordination index for each patient (PLV between patient’s speech and VP speech).</title> <p>For each of the sixteen patients, this figure depicts the histogram of the coordination index for all trials (in blue) as well as the null distribution (random phase shift) computed using 500 permutations per trial (in red).</p></caption>
<graphic xlink:href="590817v2_figs1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2</label>
<caption><title>Power spectrum analyses and correlation with verbal coordination index (right hemisphere).</title> <p>Each dot represents a channel where a significant effect was found either on <bold>(A)</bold> Global activity <bold>(</bold>Task versus Rest) for each frequency band. The activity is expressed in % of power change compared to resting; or on <bold>(B)</bold> Behaviour- related activity: r values of the Spearman correlation across trials between the iEEG power and the verbal coordination index (VCI). <bold>(C)</bold> The proportion of channels where a significant effect was found: in the task vs rest (orange), in the brain-behaviour correlation (green) or for both comparison (blue). The percentage in the center indicates the overall proportion of significant channels from the three categories with respect to the total number of channels.</p></caption>
<graphic xlink:href="590817v2_figs2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Figure S3</label>
<caption><title>Cluster analysis (silhouette score).</title> <p>Illustration of the mean silhouette scores according to the number of clusters for global activity (in red) and behaviour-related activity (correlation between power changes and coordination index, in blue). The highest silhouette score was obtained for five clusters in the HFa range for behaviour-related activity (framed in full black square). Bottom right: spatial cluster representation for the highest mean silhouette score value in HFa range.</p></caption>
<graphic xlink:href="590817v2_figs3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Figure S4</label>
<caption><title>Group analysis by regions of interest (right hemisphere).</title> <p>For each ROI, boxplots illustrate, in red, channels with significant global power changes (HFa, task vs rest) and, in blue, their corresponding r values (correlation between HFa power and verbal coordination index, VCI). Red and blue stars indicate a significant difference from a null distribution. Dots represent independent iEEG channels. The « n » below each region of interest specifies the number of patients. STG : superior temporal gyrus ; IPL : inferior parietal lobule ; IFG : inferior frontal gyrus ; BA : Brodmann area.</p></caption>
<graphic xlink:href="590817v2_figs4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Figure S5</label>
<caption><title>Phase-Amplitude Coupling analyses by region of interest (right hemisphere).</title>
<p>PAC expressed in % compared to surrogates when using as phase the virtual partner speech (in red) or the coordination dynamic (phase difference between VP and participant, in blue) and as amplitude the high frequency activity. Statistical difference between the two different types of PAC is calculated using paired wilcoxon test (STG BA41/42 : p&lt;0.0001 ; STG BA22 : p=0.9 ; IPL BA40 : p=0.2 ; IFG BA44 : p=0.01). Y-axis range has been adjusted to better illustrate the contrast between VP speech and coordination dynamic.</p></caption>
<graphic xlink:href="590817v2_figs5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Figure S6</label>
<caption><title>Phase-Amplitude Coupling according to the behavioural delay (left hemisphere).</title> <p>PAC expressed in % compared to surrogates when using as phase the coordination dynamic (phase difference between VP and participant) and as amplitude the high frequency activity. Comparison between PAC on trials with negative and positive delays (see “Coupling behavioural and neurophysiological data” section in Materials and methods). Please note that the y-axis range has been adjusted per panel.</p></caption>
<graphic xlink:href="590817v2_figs6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s8" sec-type="data-availability">
<title>Data availability statement</title>
<p>The conditions of our ethics approval do not permit public archiving of anonymised study data. Readers seeking access to the data should contact Dr. Daniele Schön (daniele.schon@univ-amu.fr). Access will be granted to named individuals in accordance with ethical procedures governing the reuse of clinical data, including completion of a formal data sharing agreement.</p>
</sec>
<ack>
<title>Acknowledgments</title>
<p>We thank all patients for their willingful participation and all the personnel from the epileptology unit.</p>
</ack>
<sec id="d1e1117" sec-type="additional-information">
<title>Additional information</title>
<sec id="s6">
<title>Funding sources</title>
<p>ANR-21-CE28-0010 (to D.S), ANR-20-CE28-0007-01, ERC-CoG-101043344 and Fondation Pour l’Audition (FPA RD-2022-09) (to B.M), ANR-17-EURE-0029 (NeuroMarseille). This work, carried out within the Institute of Convergence ILCB, was also supported by grants from France 2030 (ANR-16-CONV-0002), the French government under the Programme «Investissements d’Avenir», and the Excellence Initiative of Aix-Marseille University (A*MIDEX, AMX-19-IET-004).</p>
</sec>
<sec id="s7">
<title>Author contributions</title>
<p>I.S.M, L.L and D.S. designed research; L.L. wrote the code for the experimental setup ; I.S.M., L.L, A.T. and M.M. acquired data; I.S.M, M.M., L.L and D.S. analyzed data; I.S.M and D.S. wrote the paper; I.S.M, A.T., M.M., B.M., L.L and D.S. edited the paper.</p>
</sec>
<sec id="s9">
<title>Code availability statement</title>
<p>Data analyses were performed using custom scripts in Matlab &amp; Python, and will be available upon publication on Github.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Assaneo</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Ripollés</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Orpella</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>W. M.</given-names></string-name>, <string-name><surname>de Diego-Balaguer</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Spontaneous synchronization to speech reveals neural mechanisms facilitating language learning</article-title>. <source>Nature neuroscience</source>, <volume>22</volume>(<issue>4</issue>), <fpage>627</fpage>–<lpage>632</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Basilakos</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>K. G.</given-names></string-name>, <string-name><surname>Fillmore</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Fridriksson</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Fedorenko</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Functional characterization of the human speech articulation network</article-title>. <source>Cerebral Cortex</source>, <volume>28</volume>(<issue>5</issue>), <fpage>1816</fpage>–<lpage>1830</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bögels</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Levinson</surname>, <given-names>S. C</given-names></string-name></person-group>. (<year>2017</year>). <article-title>The brain behind the response: Insights into turn-taking in conversation from neuroimaging</article-title>. <source>Research on Language and Social Interaction</source>, <volume>50</volume>(<issue>1</issue>), <fpage>71</fpage>–<lpage>89</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bögels</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Magyari</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Levinson</surname>, <given-names>S. C</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Neural signatures of response planning occur midway through an incoming question in conversation</article-title>. <source>Scientific reports</source>, <volume>5</volume>(<issue>1</issue>), <fpage>12881</fpage>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bradshaw</surname>, <given-names>A. R.</given-names></string-name>, &amp; <string-name><surname>McGettigan</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Convergence in voice fundamental frequency during synchronous speech</article-title>. <source>PloS one</source>, <volume>16</volume>(<issue>10</issue>), <fpage>e0258747</fpage>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bradshaw</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Lametti</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Shiller</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Jasmin</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>McGettigan</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Speech motor adaptation during synchronous and metronome-timed speech</article-title>. <source>Journal of Experimental Psychology: General. to articulate the transition</source>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bradshaw</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Wheeler</surname>, <given-names>E. D.</given-names></string-name>, <string-name><surname>McGettigan</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Lametti</surname>, <given-names>D. R</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Sensorimotor learning during synchronous speech is modulated by the acoustics of the other voice</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, 1-11.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buzsáki</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Anastassiou</surname>, <given-names>C. A.</given-names></string-name>, &amp; <string-name><surname>Koch</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2012</year>). <article-title>The origin of extracellular fields and currents—EEG, ECoG, LFP and spikes</article-title>. <source>Nature reviews neuroscience</source>, <volume>13</volume>(<issue>6</issue>), <fpage>407</fpage>–<lpage>420</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Castellucci</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Kovach</surname>, <given-names>C. K.</given-names></string-name>, <string-name><surname>Howard</surname> <suffix>III</suffix>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Greenlee</surname>, <given-names>J. D.</given-names></string-name>, &amp; <string-name><surname>Long</surname>, <given-names>M. A</given-names></string-name></person-group>. (<year>2022</year>). <article-title>A speech planning network for interactive language use</article-title>. <source>Nature</source>, <volume>602</volume>(<issue>7895</issue>), <fpage>117</fpage>–<lpage>122</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Catani</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Ffytche</surname>, <given-names>D. H</given-names></string-name></person-group>. (<year>2005</year>). <article-title>The rises and falls of disconnection syndromes</article-title>. <source>Brain</source>, <volume>128</volume>(<issue>10</issue>), <fpage>2224</fpage>–<lpage>2239</lpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname>, <given-names>E. F.</given-names></string-name>, <string-name><surname>Edwards</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Nagarajan</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Fogelson</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Dalal</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Canolty</surname>, <given-names>R. T.</given-names></string-name>, <etal>…</etal> <string-name><surname>Knight</surname>, <given-names>R.</given-names></string-name> <string-name><surname>T</surname></string-name></person-group>. (<year>2011</year>). <article-title>Cortical spatio-temporal dynamics underlying phonological target detection in humans</article-title>. <source>Journal of cognitive neuroscience</source>, <volume>23</volume>(<issue>6</issue>), <fpage>1437</fpage>–<lpage>1446</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Combrisson</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Nest</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Brovelli</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Ince</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Soto</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Guillot</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Jerbi</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Tensorpac: An open- source Python toolbox for tensor-based phase-amplitude coupling measurement in electrophysiological brain signals</article-title>. <source>PLoS computational biology</source>, <volume>16</volume>(<issue>10</issue>), <fpage>e1008302</fpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cope</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Sohoglu</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Sedley</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Patterson</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>P. S.</given-names></string-name>, <string-name><surname>Wiggins</surname>, <given-names>J.</given-names></string-name>, <etal>…</etal> <string-name><surname>Rowe</surname>, <given-names>J.</given-names></string-name> <string-name><surname>B</surname></string-name></person-group>. (<year>2017</year>). <article-title>Evidence for causal top-down frontal contributions to predictive processes in speech perception</article-title>. <source>Nature communications</source>, <volume>8</volume>(<issue>1</issue>), <fpage>2154</fpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Corps</surname>, <given-names>R. E.</given-names></string-name>, <string-name><surname>Gambi</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Pickering</surname>, <given-names>M. J</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Coordinating utterances during turn-taking: The role of prediction, response preparation, and articulation</article-title>. <source>Discourse processes</source>, <volume>55</volume>(<issue>2</issue>), <fpage>230</fpage>–<lpage>240</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cummins</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2002</year>). <article-title>On synchronous speech</article-title>. <source>Acoustics Research Letters Online</source>, <volume>3</volume>(<issue>1</issue>), <fpage>7</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cummins</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Practice and performance in speech produced synchronously</article-title>. <source>Journal of Phonetics</source>, <volume>31</volume>(<issue>2</issue>), <fpage>139</fpage>–<lpage>148</lpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cummins</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Rhythm as entrainment: The case of synchronous speech</article-title>. <source>Journal of Phonetics</source>, <volume>37</volume>(<issue>1</issue>), <fpage>16</fpage>–<lpage>28</lpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Destrieux</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Fischl</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Dale</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Halgren</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature</article-title>. <source>Neuroimage</source>, <volume>53</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Dixon</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2005</year>, September). <article-title>Live tracking of musical performances using on-line time warping</article-title>. In <conf-name>Proceedings of the 8th International Conference on Digital Audio Effects</conf-name> (Vol. <volume>92</volume>, <fpage>97</fpage>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fan</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Zhuo</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>L.</given-names></string-name>, <etal>…</etal> <string-name><surname>Jiang</surname>, <given-names>T.</given-names></string-name></person-group>. (<year>2016</year>). <article-title>The human brainnetome atlas: a new brain atlas based on connectional architecture</article-title>. <source>Cerebral cortex</source>, <volume>26</volume>(<issue>8</issue>), <fpage>3508</fpage>–<lpage>3526</lpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ferpozzi</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Fornia</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Montagna</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Siodambro</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Castellano</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Borroni</surname>, <given-names>P.</given-names></string-name>, <etal>…</etal> <string-name><surname>Cerri</surname>, <given-names>G.</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Broca’s area as a pre-articulatory phonetic encoder: gating the motor program</article-title>. <source>Frontiers in human neuroscience</source>, <volume>12</volume>, <fpage>64</fpage>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Flinker</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Korzeniewska</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Shestyuk</surname>, <given-names>A. Y.</given-names></string-name>, <string-name><surname>Franaszczuk</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Dronkers</surname>, <given-names>N. F.</given-names></string-name>, <string-name><surname>Knight</surname>, <given-names>R. T.</given-names></string-name>, &amp; <string-name><surname>Crone</surname>, <given-names>N. E</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Redefining the role of Broca’s area in speech</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>112</volume>(<issue>9</issue>), <fpage>2871</fpage>–<lpage>2875</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Floegel</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Fuchs</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Kell</surname>, <given-names>C. A</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Differential contributions of the two cerebral hemispheres to temporal and spectral speech feedback control</article-title>. <source>Nature Communications</source>, <volume>11</volume>(<issue>1</issue>), <fpage>2839</fpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Floegel</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kasper</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Perrier</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Kell</surname>, <given-names>C. A</given-names></string-name></person-group>. (<year>2023</year>). <article-title>How the conception of control influences our understanding of actions</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>24</volume>(<issue>5</issue>), <fpage>313</fpage>–<lpage>329</lpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Franken</surname>, <given-names>M. K.</given-names></string-name>, <string-name><surname>Hartsuiker</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Johansson</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Hall</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Lind</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Speaking With an Alien Voice: Flexible Sense of Agency During Vocal Production</article-title>. <source>Journal of Experimental Psychology-Human perception and performance</source>, <volume>47</volume>(<issue>4</issue>), <fpage>479</fpage>–<lpage>494</lpage>. <pub-id pub-id-type="doi">10.1037/xhp0000799</pub-id></mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Parr</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Yufik</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Sajid</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Price</surname>, <given-names>C. J.</given-names></string-name>, &amp; <string-name><surname>Holmes</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Generative models, linguistic communication and active inference</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, <volume>118</volume>, <fpage>42</fpage>–<lpage>64</lpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>FitzGerald</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Rigoli</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Schwartenbeck</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Pezzulo</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Active inference: a process theory</article-title>. <source>Neural computation</source>, <volume>29</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>49</lpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Garrod</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Pickering</surname>, <given-names>M. J</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Joint action, interactive alignment, and dialog</article-title>. <source>Topics in Cognitive Science</source>, <volume>1</volume>(<issue>2</issue>), <fpage>292</fpage>–<lpage>304</lpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Groppe</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Bickel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Dykstra</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Mégevand</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mercier</surname>, <given-names>M. R.</given-names></string-name>, <etal>…</etal> <string-name><surname>Honey</surname>, <given-names>C.</given-names></string-name> <string-name><surname>J</surname></string-name></person-group>. (<year>2017</year>). <article-title>iELVis: An open source MATLAB toolbox for localizing and visualizing human intracranial electrode data</article-title>. <source>Journal of neuroscience methods</source>, <volume>281</volume>, <fpage>40</fpage>–<lpage>48</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Guenther</surname>, <given-names>F. H.</given-names></string-name>, &amp; <string-name><surname>Hickok</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2016</year>). <chapter-title>Neural models of motor speech control</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Hickok</surname>, <given-names>G</given-names></string-name>, <string-name><surname>Small</surname>, <given-names>S. L.</given-names></string-name></person-group> <source>Neurobiology of language</source> (pp. <fpage>725</fpage>-<lpage>740</lpage>). <publisher-name>Academic Press</publisher-name>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hashimoto</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Sakai</surname>, <given-names>K. L</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Brain activations during conscious self-monitoring of speech production with delayed auditory feedback: An fMRI study</article-title>. <source>Human brain mapping</source>, <volume>20</volume>(<issue>1</issue>), <fpage>22</fpage>–<lpage>28</lpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hickok</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Houde</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Rong</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Sensorimotor integration in speech processing: computational basis and neural organization</article-title>. <source>Neuron</source>, <volume>69</volume>(<issue>3</issue>), <fpage>407</fpage>–<lpage>422</lpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hirano</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kojima</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Naito</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Honjo</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Kamoto</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Okazawa</surname>, <given-names>H.</given-names></string-name>, <etal>…</etal> <string-name><surname>Konishi</surname>, <given-names>J.</given-names></string-name></person-group>. (<year>1997</year>). <article-title>Cortical processing mechanism for vocalization with auditory verbal feedback</article-title>. <source>Neuroreport</source>, <volume>8</volume>(<issue>9</issue>), <fpage>2379</fpage>–<lpage>2382</lpage>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Houde</surname>, <given-names>J. F.</given-names></string-name>, &amp; <string-name><surname>Nagarajan</surname>, <given-names>S. S</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Speech production as state feedback control</article-title>. <source>Frontiers in human neuroscience</source>, <volume>5</volume>, <fpage>82</fpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hyafil</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Fontolan</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kabdebon</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gutkin</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Giraud</surname>, <given-names>A. L</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Speech encoding by coupled cortical theta and gamma oscillations</article-title>. <source>eLife</source>, <volume>4</volume>, <elocation-id>e06213</elocation-id>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jasmin</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>McGettigan</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Agnew</surname>, <given-names>Z. K.</given-names></string-name>, <string-name><surname>Lavan</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Josephs</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Cummins</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Scott</surname>, <given-names>S. K</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Cohesion and joint speech: right hemisphere contributions to synchronized vocal production</article-title>. <source>Journal of Neuroscience</source>, <volume>36</volume>(<issue>17</issue>), <fpage>4669</fpage>–<lpage>4680</lpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Karlin</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Naber</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Parrell</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Auditory feedback is used for adaptation and compensation in speech timing</article-title>. <source>Journal of Speech, Language, and Hearing Research</source>, <volume>64</volume>(<issue>9</issue>), <fpage>3361</fpage>–<lpage>3381</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>K. S.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Max</surname>, <given-names>L</given-names></string-name></person-group>. (<year>2020</year>). <article-title>It’s About Time: Minimizing Hardware and Software Latencies in Speech Research With Real-Time Auditory Feedback</article-title>. <source>Journal of Speech, Language, and Hearing Research</source>, <volume>63</volume>(<issue>8</issue>), <fpage>2522</fpage>–<lpage>2534</lpage>. <pub-id pub-id-type="doi">10.1044/2020_JSLHR-19-00419</pub-id></mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Kuramoto</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>1975</year>). <article-title>Self-entrainment of a population of coupled non-linear oscillators.</article-title> In <conf-name>International Symposium on Mathematical Problems in Theoretical Physics: January 23–29, 1975, Kyoto University, Kyoto, Japan</conf-name> (pp. <fpage>420</fpage>–<lpage>422</lpage>). Springer Berlin Heidelberg.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kurteff</surname>, <given-names>G. L.</given-names></string-name>, <string-name><surname>Lester-Smith</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Martinez</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Currens</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Holder</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Villarreal</surname>, <given-names>C.</given-names></string-name>, <etal>…</etal> <string-name><surname>Hamilton</surname>, <given-names>L.</given-names></string-name> <string-name><surname>S</surname></string-name></person-group>. (<year>2023</year>). <article-title>Speaker-induced suppression in EEG during a naturalistic reading and listening task</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>35</volume>(<issue>10</issue>), <fpage>1538</fpage>–<lpage>1556</lpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lachaux</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Rodriguez</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Martinerie</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Varela</surname>, <given-names>F. J</given-names></string-name></person-group>. (<year>1999</year>). <article-title>Measuring phase synchrony in brain signals</article-title>. <source>Human brain mapping</source>, <volume>8</volume>(<issue>4</issue>), <fpage>194</fpage>–<lpage>208</lpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lancia</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Chaminade</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Prevot</surname>, <given-names>L</given-names></string-name></person-group>. (<year>2017</year>, August). <article-title>Studying the link between inter-speaker coordination and speech imitation through human-machine interactions</article-title>. In <source>Interspeech</source> <volume>2017</volume> (p 859-863).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lenth</surname>, <given-names>R. V</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Least-Squares Means : The R Package lsmeans</article-title>. <source>Journal of Statistical Software</source>, <volume>69</volume>, <fpage>1</fpage>–<lpage>33</lpage>. <pub-id pub-id-type="doi">10.18637/jss.v069.i01</pub-id></mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leonard</surname>, <given-names>M. K.</given-names></string-name>, <string-name><surname>Gwilliams</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sellers</surname>, <given-names>K. K.</given-names></string-name>, <string-name><surname>Chung</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Mischler</surname>, <given-names>G.</given-names></string-name>, <etal>…</etal> <string-name><surname>Chang</surname>, <given-names>E.</given-names></string-name> <string-name><surname>F</surname></string-name></person-group>. (<year>2024</year>). <article-title>Large-scale single-neuron speech sound encoding across the depth of human cortex</article-title>. <source>Nature</source>, <volume>626</volume>(<issue>7999</issue>), <fpage>593</fpage>–<lpage>602</lpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Levinson</surname>, <given-names>S. C</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Turn-taking in human communication–origins and implications for language processing</article-title>. <source>Trends in cognitive sciences</source>, <volume>20</volume>(<issue>1</issue>), <fpage>6</fpage>–<lpage>14</lpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Lancia</surname>, <given-names>L</given-names></string-name></person-group>. (<year>2024</year>). <article-title>A multimodal approach to study the nature of coordinative patterns underlying speech rhythm</article-title>. In <source>Proc. Interspeech</source>, 397-401.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lind</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hall</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Breidegard</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Balkenius</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Johansson</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Speakers’ acceptance of real-time speech exchange indicates that we use auditory feedback to specify the meaning of what we say</article-title>. <source>Psychological Science</source>, <volume>25</volume>(<issue>6</issue>), <fpage>1198</fpage>–<lpage>1205</lpage>. <pub-id pub-id-type="doi">10.1177/0956797614529797</pub-id></mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lizarazu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lallier</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Molinaro</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Phase− amplitude coupling between theta and gamma oscillations adapts to speech rate</article-title>. <source>Annals of the New York Academy of Sciences</source>, <volume>1453</volume>(<issue>1</issue>), <fpage>140</fpage>–<lpage>152</lpage>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Long</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Katlowitz</surname>, <given-names>K. A.</given-names></string-name>, <string-name><surname>Svirsky</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Clary</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Byun</surname>, <given-names>T. M.</given-names></string-name>, <string-name><surname>Majaj</surname>, <given-names>N.</given-names></string-name>, <etal>…</etal> <string-name><surname>Greenlee</surname>, <given-names>J.</given-names></string-name> <string-name><surname>D</surname></string-name></person-group>. (<year>2016</year>). <article-title>Functional segregation of cortical regions underlying speech timing and articulation</article-title>. <source>Neuron</source>, <volume>89</volume>(<issue>6</issue>), <fpage>1187</fpage>–<lpage>1193</lpage>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Magyari</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Bastiaansen</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>De Ruiter</surname>, <given-names>J. P.</given-names></string-name>, &amp; <string-name><surname>Levinson</surname>, <given-names>S. C.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Early anticipation lies behind the speed of response in conversation</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>26</volume>(<issue>11</issue>), <fpage>2530</fpage>–<lpage>2539</lpage>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marsh</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Richardson</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Schmidt</surname>, <given-names>R. C</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Social connection through joint action and interpersonal coordination</article-title>. <source>Topics in cognitive science</source>, <volume>1</volume>(<issue>2</issue>), <fpage>320</fpage>–<lpage>339</lpage>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meekings</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Scott</surname>, <given-names>S. K</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Error in the Superior Temporal Gyrus? A Systematic Review and Activation Likelihood Estimation Meta-Analysis of Speech Production Studies</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>33</volume>(<issue>3</issue>), <fpage>422</fpage>–<lpage>444</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_01661</pub-id></mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mercier</surname>, <given-names>M. R.</given-names></string-name>, <string-name><surname>Bickel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Megevand</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Groppe</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Schroeder</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Mehta</surname>, <given-names>A. D.</given-names></string-name>, &amp; <string-name><surname>Lado</surname>, <given-names>F. A</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Evaluation of cortical local field potential diffusion in stereotactic electro-encephalography recordings: a glimpse on white matter signal</article-title>. <source>Neuroimage</source>, <volume>147</volume>, <fpage>219</fpage>–<lpage>232</lpage>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mercier</surname>, <given-names>M. R.</given-names></string-name>, <string-name><surname>Dubarry</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Tadel</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Avanzini</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Axmacher</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Cellier</surname>, <given-names>D.</given-names></string-name>, <etal>…</etal> <string-name><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Advances in human intracranial electroencephalography research, guidelines and good practices</article-title>. <source>Neuroimage</source>, <volume>260</volume>, <fpage>119438</fpage>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mesgarani</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Chang</surname>, <given-names>E. F</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Selective cortical representation of attended speaker in multi-talker speech perception</article-title>. <source>Nature</source>, <volume>485</volume>(<issue>7397</issue>), <fpage>233</fpage>–<lpage>236</lpage>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morillon</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Lehongre</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Frackowiak</surname>, <given-names>R. S.</given-names></string-name>, <string-name><surname>Ducorps</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kleinschmidt</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Giraud</surname>, <given-names>A. L</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Neurophysiological origin of human brain asymmetry for speech and language</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>107</volume>(<issue>43</issue>), <fpage>18688</fpage>–<lpage>18693</lpage>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morillon</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Liégeois-Chauvel</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bénar</surname>, <given-names>C. G.</given-names></string-name>, &amp; <string-name><surname>Giraud</surname>, <given-names>A. L</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Asymmetric function of theta and gamma activity in syllable processing: an intra-cortical study</article-title>. <source>Frontiers in psychology</source>, <volume>3</volume>, <fpage>24108</fpage>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mukherjee</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Badino</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Hilt</surname>, <given-names>P. M.</given-names></string-name>, <string-name><surname>Tomassini</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Inuggi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Fadiga</surname>, <given-names>L.</given-names></string-name>, <etal>…</etal> <string-name><surname>d’Ausilio</surname>, <given-names>A.</given-names></string-name></person-group>. (<year>2019</year>). <article-title>The neural oscillatory markers of phonetic convergence during verbal interaction</article-title>. <source>Human brain mapping</source>, <volume>40</volume>(<issue>1</issue>), <fpage>187</fpage>–<lpage>201</lpage>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ozker</surname> <given-names>M.</given-names></string-name>, <string-name><surname>Doyle</surname> <given-names>W.</given-names></string-name>, <string-name><surname>Devinsky</surname> <given-names>O.</given-names></string-name>, <string-name><surname>Flinker</surname> <given-names>A</given-names></string-name></person-group> (<year>2022</year>) <article-title>A cortical network processes auditory error signals during human speech production to maintain fluency</article-title> <source>PLoS Biology</source> <volume>20</volume>: <elocation-id>e3001493</elocation-id>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niziolek</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Nagarajan</surname>, <given-names>S. S.</given-names></string-name>, &amp; <string-name><surname>Houde</surname>, <given-names>J. F</given-names></string-name></person-group>. (<year>2013</year>). <article-title>What does motor efference copy represent? Evidence from speech production</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume>(<issue>41</issue>), <fpage>16110</fpage>–<lpage>16116</lpage>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nourski</surname>, <given-names>K. V</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Auditory processing in the human cortex: An intracranial electrophysiology perspective</article-title>. <source>Laryngoscope investigative otolaryngology</source>, <volume>2</volume>(<issue>4</issue>), <fpage>147</fpage>–<lpage>156</lpage>.</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nourski</surname>, <given-names>K. V.</given-names></string-name>, <string-name><surname>Steinschneider</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Oya</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Kawasaki</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Howard</surname> <suffix>III</suffix>, <given-names>M. A</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Modulation of response patterns in human auditory cortex during a target detection task: an intracranial electrophysiology study</article-title>. <source>International Journal of Psychophysiology</source>, <volume>95</volume>(<issue>2</issue>), <fpage>191</fpage>–<lpage>201</lpage>.</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oestreich</surname>, <given-names>L. K.</given-names></string-name>, <string-name><surname>Whitford</surname>, <given-names>T. J.</given-names></string-name>, &amp; <string-name><surname>Garrido</surname>, <given-names>M. I</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Prediction of speech sounds is facilitated by a functional fronto-temporal network</article-title>. <source>Frontiers in Neural Circuits</source>, <volume>12</volume>, <fpage>43</fpage>.</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oganian</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Chang</surname>, <given-names>E. F</given-names></string-name></person-group>. (<year>2019</year>). <article-title>A speech envelope landmark for syllable encoding in human superior temporal gyrus</article-title>. <source>Science advances</source>, <volume>5</volume>(<fpage>11</fpage>), <elocation-id>eaay6279</elocation-id>.</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fries</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Schoffelen</surname>, <given-names>J. M</given-names></string-name></person-group>. (<year>2011</year>). <article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>. <source>Computational intelligence and neuroscience</source>, <volume>2011</volume>, <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oschkinat</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Hoole</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Reactive feedback control and adaptation to perturbed speech timing in stressed and unstressed syllables</article-title>. <source>Journal of Phonetics</source>, <volume>91</volume>, <fpage>101133</fpage>.</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ozker</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Doyle</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Devinsky</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Flinker</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2022</year>). <article-title>A cortical network processes auditory error signals during human speech production to maintain fluency</article-title>. <source>PLoS Biology</source>, <volume>20</volume>(<issue>2</issue>), <fpage>e3001493</fpage>.</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ozker</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Dugan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Doyle</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Friedman</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Devinsky</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Flinker</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Speech-induced suppression and vocal feedback sensitivity in human cortex</article-title>. <source>eLife</source>, <volume>13</volume>, <elocation-id>RP94198</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.94198</pub-id></mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Michel</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Thirion</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Grisel</surname>, <given-names>O.</given-names></string-name>, <etal>…</etal> <string-name><surname>Duchesnay</surname>, <given-names>É.</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Scikit- learn: Machine learning in Python</article-title>. <source>The Journal of machine Learning research</source>, <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Penny</surname>, <given-names>W. D.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Ashburner</surname>, <given-names>J. T.</given-names></string-name>, <string-name><surname>Kiebel</surname>, <given-names>S. J.</given-names></string-name>, &amp; <string-name><surname>Nichols</surname>, <given-names>T. E</given-names></string-name></person-group>. (Eds.). (<year>2011</year>). <source>Statistical parametric mapping: the analysis of functional brain images</source>. <publisher-name>Elsevier</publisher-name>.</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petkov</surname>, <given-names>C. I.</given-names></string-name>, <string-name><surname>Kang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Alho</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Bertrand</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Yund</surname>, <given-names>E. W.</given-names></string-name>, &amp; <string-name><surname>Woods</surname>, <given-names>D. L</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Attentional modulation of human auditory cortex</article-title>. <source>Nature neuroscience</source>, <volume>7</volume>(<issue>6</issue>), <fpage>658</fpage>–<lpage>663</lpage>.</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pickering</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Gambi</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Predicting while comprehending language: A theory and review</article-title>. <source>Psychological bulletin</source>, <volume>144</volume>(<issue>10</issue>), <fpage>1002</fpage>.</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pickering</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Garrod</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Toward a mechanistic psychology of dialogue</article-title>. <source>Behavioral and brain sciences</source>, <volume>27</volume>(<issue>2</issue>), <fpage>169</fpage>–<lpage>190</lpage>.</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Assaneo</surname>, <given-names>M. F</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Speech rhythms and their neural foundations</article-title>. <source>Nature reviews neuroscience</source>, <volume>21</volume>(<issue>6</issue>), <fpage>322</fpage>–<lpage>334</lpage>.</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rousseeuw</surname>, <given-names>P. J</given-names></string-name></person-group>. (<year>1987</year>). <article-title>Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</article-title>. <source>Journal of computational and applied mathematics</source>, <volume>20</volume>, <fpage>53</fpage>–<lpage>65</lpage>.</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schuhmann</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Schiller</surname>, <given-names>N. O.</given-names></string-name>, <string-name><surname>Goebel</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Sack</surname>, <given-names>A. T</given-names></string-name></person-group>. (<year>2009</year>). <article-title>The temporal characteristics of functional activation in Broca’s area during overt picture naming</article-title>. <source>cortex</source>, <volume>45</volume>(<issue>9</issue>), <fpage>1111</fpage>–<lpage>1116</lpage>.</mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Shahapure</surname>, <given-names>K. R.</given-names></string-name>, &amp; <string-name><surname>Nicholas</surname>, <given-names>C</given-names></string-name></person-group>. <year>2020</year>, <article-title>Cluster quality analysis using silhouette score</article-title>. In <conf-name>2020 IEEE 7th international conference on data science and advanced analytics (DSAA)</conf-name> (pp. <fpage>747</fpage>–<lpage>748</lpage>). <publisher-name>IEEE</publisher-name>.</mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steinschneider</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nourski</surname>, <given-names>K. V.</given-names></string-name>, <string-name><surname>Rhone</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Kawasaki</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Oya</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Howard</surname> <suffix>III</suffix>, <given-names>M. A</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Differential activation of human core, non-core and auditory-related cortex during speech categorization tasks as revealed by intracranial recordings</article-title>. <source>Frontiers in neuroscience</source>, <volume>8</volume>, <fpage>240</fpage>.</mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stuart</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kalinowski</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Rastatter</surname>, <given-names>M. P.</given-names></string-name>, &amp; <string-name><surname>Lynch</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Effect of delayed auditory feedback on normal speakers at two speech rates</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>111</volume>(<issue>5</issue>), <fpage>2237</fpage>–<lpage>2241</lpage>.</mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Takaso</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Eisner</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Wise</surname>, <given-names>R. J.</given-names></string-name>, &amp; <string-name><surname>Scott</surname>, <given-names>S. K.</given-names></string-name></person-group> (<year>2010</year>). <article-title>The effect of delayed auditory feedback on activity in the temporal lobe while speaking: a positron emission tomography study</article-title>. <source>Journal of speech, language, and hearing research</source>, <volume>53</volume>, <fpage>226</fpage>–<lpage>236</lpage></mixed-citation></ref>
<ref id="c81"><label>81.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tourville</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>Guenther</surname>, <given-names>F. H</given-names></string-name></person-group>. (<year>2011</year>). <article-title>The DIVA model: A neural theory of speech acquisition and production</article-title>. <source>Language and cognitive processes</source>, <volume>26</volume>(<issue>7</issue>), <fpage>952</fpage>–<lpage>981</lpage>.</mixed-citation></ref>
<ref id="c82"><label>82.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Villalon</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Paz</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Roehri</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Lagarde</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pizzo</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Colombet</surname>, <given-names>B.</given-names></string-name>, <etal>…</etal> <string-name><surname>Bénar</surname>, <given-names>C.</given-names></string-name> <string-name><surname>G</surname></string-name></person-group>. (<year>2018</year>). <article-title>EpiTools, A software suite for presurgical brain mapping in epilepsy: Intracerebral EEG</article-title>. <source>Journal of neuroscience methods</source>, <volume>303</volume>, <fpage>7</fpage>–<lpage>15</lpage>.</mixed-citation></ref>
<ref id="c83"><label>83.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamamoto</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Kawabata</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Adaptation to delayed auditory feedback induces the temporal recalibration effect in both speech perception and production</article-title>. <source>Experimental brain research</source>, <volume>232</volume>, <fpage>3707</fpage>–<lpage>3718</lpage>.</mixed-citation></ref>
<ref id="c84"><label>84.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zheng</surname>, <given-names>Z. Z.</given-names></string-name>, <string-name><surname>MacDonald</surname>, <given-names>E. N.</given-names></string-name>, <string-name><surname>Munhall</surname>, <given-names>K. G.</given-names></string-name>, &amp; <string-name><surname>Johnsrude</surname>, <given-names>I. S</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Perceiving a Stranger’s Voice as Being One’s Own: A ’Rubber Voice’ Illusion?</article-title> <source>PLOS One</source>, <volume>6</volume>(<issue>4</issue>), <fpage>e18655</fpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99547.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Obleser</surname>
<given-names>Jonas</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Lübeck</institution>
</institution-wrap>
<city>Lübeck</city>
<country>Germany</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This paper reports on an <bold>important</bold> study that aims to move beyond current experimental approaches in speech production by (1) investigating speech in the context of a fully interactive task and (2) employing advanced methodology to record intracranial brain activity. Together these allow for examination of the unfolding temporal dynamics of brain-behaviour relationships during interactive speech. This approach and the analyses presented in support of the authors' claims pose <bold>convincing</bold> evidence.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99547.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper reports an intracranial SEEG study of speech coordination, where participants synchronize their speech output with a virtual partner that is designed to vary its synchronization behavior. This allows the authors to identify electrodes throughout the left hemisphere of the brain that have activity (both power and phase) that correlates with the degree of synchronization behavior. They find that high-frequency activity in secondary auditory cortex (superior temporal gyrus) is correlated to synchronization, in contrast to primary auditory regions. Furthermore, activity in inferior frontal gyrus shows a significant phase-amplitude coupling relationship that is interpreted as compensation for deviation from synchronized behavior with the virtual partner.</p>
<p>Strengths:</p>
<p>
(1) The development of a virtual partner model trained for each individual participant, which can dynamically vary its synchronization to the participant's behavior in real time, is novel and exciting.</p>
<p>
(2) Understanding real-time temporal coordination for behaviors like speech is a critical and understudied area.</p>
<p>
(3) The use of SEEG provides the spatial and temporal resolution necessary to address the complex dynamics associated with the behavior.</p>
<p>
(4) The paper provides some results that suggest a role for regions like IFG and STG in the dynamic temporal coordination of behavior both within an individual speaker and across speakers performing a coordination task.</p>
<p>Weaknesses:</p>
<p>(1) The main weakness of the paper is that the results are presented in a largely descriptive and vague manner. For instance, while the interpretation about predictive coding and error correction is interesting, it is not clear how the experimental design or analyses specifically support such a model, or how they differentiate that model from the alternatives. It's possible that some greater specificity could be achieved by a more detailed examination of this rich dataset, for example by characterizing the specific phase relationships (e.g., positive vs negative lags) in areas that show correlations with synchronization behavior. However, as written, it is difficult to understand what these results tell us about how coordination behavior arises.</p>
<p>
(2) In the results section, there's a general lack of quantification. While some of the statistics reported in the figures are helpful, there are also claims that are stated without any statistical test. For example, in the paragraph starting on line 342, it is claimed that there is an inverse relationship between rho-value and frequency band, &quot;possibly due to the reversed desynchronization/synchronization process in low and high frequency bands&quot;. Based on Figure 3, the first part of this statement appears to be true qualitatively, but is not quantified, and is therefore impossible to assess in relation to the second part of the claim. Similarly, the next paragraph on line 348 describes optimal clustering, but statistics of the clustering algorithm and silhouette metric are not provided. More importantly, it's not entirely clear what is being clustered - is the point to identify activity patterns that are similar within/across brain regions? Or to interpret the meaning of the specific patterns? If the latter, this is not explained or explored in the paper.</p>
<p>
(3) Given the design of the stimuli, it would be useful to know more about how coordination relates to specific speech units. The authors focus on the syllabic level, which is understandable. But as far as the results relate to speech planning (an explicit point in the paper), the claims could be strengthened by determining whether the coordination signal (whether error correction or otherwise) is specifically timed to e.g., the consonant vs the vowel. If the mechanism is a phase reset, does it tend to occur on one part of the syllable?</p>
<p>
(4) In the discussion the results are related to a previously described speech-induced suppression effect. However, it's not clear what the current results have to do with SIS, since the speaker's own voice is present and predictable from the forward model on every trial. Statements such as &quot;Moreover, when the two speech signals come close enough in time, the patient possibly perceives them as its own voice&quot; are highly speculative and apparently not supported by the data.</p>
<p>
(5) There are some seemingly arbitrary decisions made in the design and analysis that, while likely justified, need to be explained. For example, how were the cutoffs for moderate coupling vs phase-shifted coupling (k ~0.09) determined? This is noted as &quot;rather weak&quot; (line 212), but it's not clear where this comes from. Similarly, the ROI-based analyses are only done on regions &quot;recorded in at least 7 patients&quot; - how was this number chosen? How many electrodes total does this correspond to? Is there heterogeneity within each ROI?</p>
<p>Comments on revisions:</p>
<p>The authors have generally responded to the critiques from the first round of review, and have provided additional details that help readers to understand what was done.</p>
<p>In my opinion, the paper still suffers from a lack of clarity about the interpretation, which is partly due to the fact that the results themselves are not straightforward. For example, the heterogeneity across individual electrodes that is obvious from Fig 3 makes it hard to justify the ROI-based approach. And even the electrode clustering, while more data-driven, does not substantially help the fact that the effects appear to be less spatially-organized than the authors may want to claim.</p>
<p>I recognize the value of introducing this new mutual adaptation paradigm, which is the main strength of the paper. However, the conclusions that can be drawn from the data presented here seem incomplete at best.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99547.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper investigates the neural underpinnings of an interactive speech task requiring verbal coordination with another speaker. To achieve this, the authors recorded intracranial brain activity from the left (and to a lesser extent, the right) hemisphere in a group of drug-resistant epilepsy patients while they synchronised their speech with a 'virtual partner'. Crucially, the authors were able to manipulate the degree of success of this synchronisation by programming the virtual partner to either actively synchronise or desynchronise their speech with the participant, or else to not vary its speech in response to the participant (making the synchronisation task purely one-way). Using such a paradigm, the authors identified different brain regions that were either more sensitive to the speech of the virtual partner (primary auditory cortex), or more sensitive to the degree of verbal coordination (i.e. synchronisation success) with the virtual partner (left secondary auditory cortex and bilateral IFG). Such sensitivity was measured by (1) calculating the correlation between the index of verbal coordination and mean power within a range of frequency bands across trials, and (2) calculating the phase-amplitude coupling between the behavioural and brain signals within single trials (using the power of high-frequency neural activity only). Overall, the findings help to elucidate some of the brain areas involved in interactive speaking behaviours, particularly highlighting high-frequency activity of the bilateral IFG as a potential candidate supporting verbal coordination.</p>
<p>Strengths:</p>
<p>This study provides the field with a convincing demonstration of how to investigate speaking behaviours in more complex situations that share many features with real-world speaking contexts e.g. simultaneous engagement of speech perception and production processes, the presence of an interlocutor and the need for inter-speaker coordination. The findings thus go beyond previous work that has typically studied solo speech production in isolation, and represent a significant advance in our understanding of speech as a social and communicative behaviour. It is further an impressive feat to develop a paradigm in which the degree of cooperativity of the synchronisation partner can be so tightly controlled; in this way, this study combines the benefits of using pre-recorded stimuli (namely, the high degree of experimental control) with the benefits of using a live synchronisation partner (allowing the task to be truly two-way interactive, an important criticism of other work using pre-recorded stimuli). A further key strength of the study lies in its employment of stereotactic EEG to measure brain responses with both high temporal and spatial resolution, an ideal method for studying the unfolding relationship between neural processing and this dynamic coordination behaviour.</p>
<p>Weaknesses:</p>
<p>One limitation of the current study is the relatively sparse coverage of the right hemisphere by the implanted electrodes (91 electrodes in the right compared to 145 in the left). Of course, electrode location is solely clinically motivated, and so the authors did not have control over this. In a previous version of this article, the authors therefore chose not to include data from the right hemisphere in reported analyses. However, after highlighting previous literature suggesting that the right hemisphere likely has high relevance to verbal coordination behaviours such as those under investigation here, the authors have now added analyses of the right hemisphere data to the results. These confirm an involvement of the right hemisphere in this task, largely replicating left hemisphere results. Some hemispheric differences were found in responses within the STG; however, interpretation should be tempered by an awareness of the relatively sparse coverage of the right hemisphere meaning that some regions have very few electrodes, resulting in reduced statistical power.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99547.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Schwab-Mohamed</surname>
<given-names>Isaïh</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Mercier</surname>
<given-names>Manuel R</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Trébuchon</surname>
<given-names>Agnès</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Morillon</surname>
<given-names>Benjamin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lancia</surname>
<given-names>Leonardo</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Schön</surname>
<given-names>Daniele</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>This paper reports an intracranial SEEG study of speech coordination, where participants synchronize their speech output with a virtual partner that is designed to vary its synchronization behavior. This allows the authors to identify electrodes throughout the left hemisphere of the brain that have activity (both power and phase) that correlates with the degree of synchronization behavior. They find that high-frequency activity in the secondary auditory cortex (superior temporal gyrus) is correlated to synchronization, in contrast to primary auditory regions. Furthermore, activity in the inferior frontal gyrus shows a significant phase-amplitude coupling relationship that is interpreted as compensation for deviation from synchronized behavior with the virtual partner.</p>
<p>Strengths:</p>
<p>(1) The development of a virtual partner model trained for each individual participant, which can dynamically vary its synchronization to the participant's behavior in real-time, is novel and exciting.</p>
<p>(2) Understanding real-time temporal coordination for behaviors like speech is a critical and understudied area.</p>
<p>(3) The use of SEEG provides the spatial and temporal resolution necessary to address the complex dynamics associated with the behavior.</p>
<p>(4) The paper provides some results that suggest a role for regions like IFG and STG in the dynamic temporal coordination of behavior both within an individual speaker and across speakers performing a coordination task.</p>
</disp-quote>
<p>We thank the Reviewer for their positive comments on our manuscript.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>(1) The main weakness of the paper is that the results are presented in a largely descriptive and vague manner. For instance, while the interpretation of predictive coding and error correction is interesting, it is not clear how the experimental design or analyses specifically support such a model, or how they differentiate that model from the alternatives. It's possible that some greater specificity could be achieved by a more detailed examination of this rich dataset, for example by characterizing the specific phase relationships (e.g., positive vs negative lags) in areas that show correlations with synchronization behavior. However, as written, it is difficult to understand what these results tell us about how coordination behavior arises.</p>
</disp-quote>
<p>We understand the reviewer’s comment. It is true that this work, being the first in the field using real-time adapting synchronous speech and intracerebral neural data, is a descriptive work, that hopefully will pave the way for further studies. We have now added more statistical analyses (see point 2) to go beyond a descriptive approach and we have also rewritten the discussion to clarify how this work can possibly contribute to disentangle different models of language interaction. Most importantly we have also run new analyses taking into account the specific phase relationship, as suggested.</p>
<p>We already had an analysis using instantaneous phase difference in the phase-amplitude coupling approach, that bridges phase of behaviour to neural responses (amplitude in the high-frequency range). However, this analysis, as the reviewer noted, does not distinguish between positive and negative lags, but rather uses the continuous fluctuations of coordinative behaviour. Following the reviewer’s suggestion, we have now run a new analysis estimating the average delay (between virtual partner speech and patient speech) in each trial, using a cross-correlation approach. This gives a distribution of delays across trials that can then be “binned” as positive or negative. We have thus rerun the phase-amplitude coupling analyses on positive and negative trials separately, to assess whether the phase amplitude relationship depends upon the anticipatory (negative lags) or compensatory (positive lags) behaviour. Our new analysis (now in the supplementary, see figure below) does not reveal significant differences between positive and negative lags. This lack of difference, although not easy to interpret, is nonetheless interesting because it seems to show that the IFG does not have a stronger coupling for anticipatory trials. Rather the IFG seems to be strongly involved in adjusting behaviour, minimizing the error, independently of whether this is early or late.</p>
<p>We have updated the “Coupling behavioural and neurophysiological data” section in Materials and methods as follows:</p>
<p>“In the third approach, we assessed whether the phase-amplitude relationship (or coupling) depends upon the anticipatory (negative delays) or compensatory (positive delays) behaviour between the VO and the patients’ speech. We computed the average delay in each trial using a cross-correlation approach on speech signals (between patient and VP) with the MATLAB function <italic>xcorr</italic>. A median split (patient-specific ; average median split = 0ms, average sd = 24ms) was applied to conserve a sufficient amount of data, classifying trials below the median as “anticipatory behaviour” and trials above the median as “compensatory behaviour”. Then we conducted the phase-amplitude coupling analyses on positive and negative trials separately.”</p>
<p>We also added a paragraph on this finding in the Discussion:</p>
<p>“Our results highlight the involvement of the inferior frontal gyrus (IFG) bilaterally, in particular the BA44 region, in speech coordination. First, trials with a weak verbal coordination (VCI) are accompanied by more prominent high frequency activity (HFa, Fig.4; Fig.S4). Second, when considering the within-trial time-resolved dynamics, the phase-amplitude coupling (PAC) reveals a tight relation between the low frequency behavioural dynamics (phase) and the modulation of high-frequency neural activity (amplitude, Fig.5B ; Fig.S5). This relation is strongest when considering the phase adjustments rather than the phase of speech of the VP per se : larger deviations in verbal coordination are accompanied by increase in HFa. Additionally, we also tested for potential effects of different asynchronies (i.e., temporal delay) between the participant's speech and that of the virtual partner but found no significant differences (Fig.S6). While lack of delay-effect does not permit to conclude about the sensitivity of BA44 to absolute timing of the partner’s speech, its neural dynamics are linked to the ongoing process of resolving phase deviations and maintaining synchrony.”</p>
<disp-quote content-type="editor-comment">
<p>(2) In the results section, there's a general lack of quantification. While some of the statistics reported in the figures are helpful, there are also claims that are stated without any statistical test. For example, in the paragraph starting on line 342, it is claimed that there is an inverse relationship between rho-value and frequency band, &quot;possibly due to the reversed desynchronization/synchronization process in low and high frequency bands&quot;. Based on Figure 3, the first part of this statement appears to be true qualitatively, but is not quantified, and is therefore impossible to assess in relation to the second part of the claim. Similarly, the next paragraph on line 348 describes optimal clustering, but statistics of the clustering algorithm and silhouette metric are not provided. More importantly, it's not entirely clear what is being clustered - is the point to identify activity patterns that are similar within/across brain regions? Or to interpret the meaning of the specific patterns? If the latter, this is not explained or explored in the paper.</p>
</disp-quote>
<p>The reviewer is right. We have now added statistical analyses showing that:</p>
<p>(1) the ratio between synchronization and desynchronization evolves across frequencies (as often reported in the literature).</p>
<p>(2) the sign of rho values also evolves across frequencies.</p>
<p>(3) the clustering does indeed differ when taking into account behaviour. We have also clarified the use of clustering and the reasoning behind it.</p>
<p>We have updated the Materials and methods section as follows:</p>
<p>“The statistical difference between spatial clustering in global effect and brain-behaviour correlation was estimated with linear model using the R function <italic>lm</italic> (stat package), post-hoc comparisons were corrected for multiple comparisons using the Tukey test (lsmeans R package ; Lenth, 2016). The statistical difference between clustering in global effect and behaviour correlation across the number of clusters was estimated using permutation tests (N=1000) by computing the silhouette score difference between the two conditions.” We have updated the Results section as follows:</p>
<p>(1) “This modulation between synchronization and desynchronization across frequencies was significant (F(5) = 6.42, p &lt; .001 ; estimated with linear model using the R function <italic>lm</italic>).”</p>
<p>(2) “The first observation is a gradual transition in the direction of correlations as we move up frequency bands, from positive correlations at low frequencies to negative ones at high frequencies (F(5) = 2.68, p = .02). This effect, present in both hemispheres, mimics the reversed desynchronization/synchronization process in low and high frequency bands reported above.”</p>
<p>(3) “Importantly, compared to the global activity (task vs rest, Fig 3A), the neural spatial profile of the behaviour-related activity (Fig 3B) is more clustered, in the left hemisphere. Indeed, silhouette scores are systematically higher for behaviour-related activity compared to global activity, indicating greater clustering consistency across frequency bands (t(106) = 7.79, p &lt; .001, see Figure S3). Moreover, silhouette scores are maximal, in particular for HFa, for five clusters (p &lt; .001), located in the IFG BA44, the IPL BA 40 and the STG BA 41/42 and BA22 (see Figure S3).”</p>
<disp-quote content-type="editor-comment">
<p>(3) Given the design of the stimuli, it would be useful to know more about how coordination relates to specific speech units. The authors focus on the syllabic level, which is understandable. But as far as the results relate to speech planning (an explicit point in the paper), the claims could be strengthened by determining whether the coordination signal (whether error correction or otherwise) is specifically timed to e.g., the consonant vs the vowel. If the mechanism is a phase reset, does it tend to occur on one part of the syllable?</p>
</disp-quote>
<p>Thank you for this thoughtful feedback. We agree that the relationship between speech coordination and specific speech units, such as consonants versus vowels, is an intriguing question. However, in our study, both interlocutors (the participant and the virtual partner) are adapting their speech production in real-time. This interactive coordination makes it difficult to isolate neural signatures corresponding to precise segments like consonants or vowels, as the adjustments occur in a continuous and dynamic context.</p>
<p>The VP's ability to adapt depends on its sensitivity to spectral cues, such as the transition from one phonetic element to another. This is likely influenced by the type of articulation, with certain transitions being more salient (e.g., between a stop consonant like &quot;p&quot; and a vowel like &quot;a&quot;) and others being less distinct (e.g., between nasal consonants like &quot;m&quot; and a vowel). Thus, the VP’s spectral adaptation tends to occur at these transitions, which are more prominent in some cases than in others.</p>
<p>For the participants, previous studies have shown a greater sensitivity during the production of stressed vowels (Oschkinat &amp; Hoole, 2022; Li &amp; Lancia, 2024), which may reflect a heightened attentional or motor adjustment to stressed syllables.</p>
<p>Here, we did not specifically address the question of coordination at the level of individual linguistic units. Moreover, even if we attempted to focus on this level, it would be challenging to relate neural dynamics directly to specific speech segments. The question of how synchronization at the level of individual linguistic units might relate to neural data is complex. The lack of clear, unit-specific predictions makes it difficult to parse out distinct neural signatures tied to individual segments, particularly when both interlocutors are continuously adjusting their speech in relation to one another.</p>
<p>Therefore, while we recognize the potential importance of examining synchronization at the level of individual phonetic elements, the design of our task and the nature of the coordination in this interactive context (realtime bidirection adaptation) led us to focus more broadly on the overall dynamics of speech synchronization at the syllabic level, rather than on specific linguistic units.</p>
<p>We now state at the end of the Discussion section:</p>
<p>“It is worth noting that the influence of specific speech units, such as consonants versus vowels, on speech coordination remains to be explored. In non-interactive contexts, participants show greater sensitivity during the production of stressed vowels, possibly reflecting heightened attentional or motor adjustments (Oschkinat &amp; Hoole, 2022; Li &amp; Lancia, 2024). In this study, the VP’s adaptation relies on sensitivity to spectral cues, particularly phonetic transitions, with some (e.g., formant transitions) being more salient than others. However, how these effects manifest in an interactive setting remains an open question, as both interlocutors continuously adjust their speech in real time. Future studies could investigate whether coordination signals, such as phase resets, preferentially align with specific parts of the syllable.” References cited:</p>
<p>– Oschkinat, M., &amp; Hoole, P. (2022). Reactive feedback control and adaptation to perturbed speech timing in stressed and unstressed syllables. Journal of Phonetics, 91, 101133.</p>
<p>– Li, J., &amp; Lancia, L. (2024). A multimodal approach to study the nature of coordinative patterns underlying speech rhythm. In Proc. Interspeech, 397-401.</p>
<disp-quote content-type="editor-comment">
<p>(4) In the discussion the results are related to a previously-described speech-induced suppression effect. However, it's not clear what the current results have to do with SIS, since the speaker's own voice is present and predictable from the forward model on every trial. Statements such as &quot;Moreover, when the two speech signals come close enough in time, the patient possibly perceives them as its own voice&quot; are highly speculative and apparently not supported by the data.</p>
</disp-quote>
<p>We thank the reviewer for raising thoughtful concerns about our interpretation of the observed neural suppression as related to speaker-induced suppression (SIS). We agree that our study lacks a passive listening condition, which limits direct comparisons to the original SIS effect, traditionally defined as the suppression of neural responses to self-produced speech compared to externally-generated speech (Meekings &amp; Scott, 2021).</p>
<p>In response, we have reconsidered our terminology and interpretation. In the revised Discussion section, we refer to our findings as a &quot;SIS-related phenomenon specific to the synchronous speech context&quot;. Unlike classic SIS paradigms, our interactive task involves simultaneous monitoring of self- and externally-generated speech, introducing additional attentional and coordinative demands.</p>
<p>The revised Discussion also incorporates findings by Ozker et al. (2022, 2024), which link SIS and speech monitoring, suggesting that suppressing responses to self-generated speech facilitates error detection. We propose that the decrease in high-frequency activity (HFa) as verbal coordination increases reflects reduced error signals due to closer alignment between perceived and produced speech. Conversely, HFa increases with reduced coordination may signify greater prediction error.</p>
<p>Additionally, we relate our findings to the &quot;rubber voice&quot; effect (Zheng et al., 2011; Lind et al., 2014; Franken et al., 2021), where temporally and phonetically congruent external speech can be perceived as self-generated. We speculate that this may occur in synchronous speech tasks when the participant's and VP's speech signals closely align. However, this interpretation remains speculative, as no subjective reports were collected to confirm this perception. Future studies could include participant questionnaires to validate this effect and relate subjective experience to neural measures of synchronization.</p>
<p>Overall, our findings extend the study of SIS to dynamic, interactive contexts and contribute to understanding internal forward models of speech production in more naturalistic scenarios.</p>
<p>We have now added these points to the discussion as follows:</p>
<p>“The observed negative correlation between verbal coordination and high-frequency activity (HFa) in STG BA22 suggests a suppression of neural responses as the degree of behavioural synchrony increases. This result is reminiscent of findings on speaker-induced suppression (SIS), where neural activity in auditory cortex decreases during self-generated speech compared to externally-generated speech (Meekings &amp; Scott, 2021; Niziolek et al., 2013). However, our paradigm differs from traditional SIS studies in two critical ways: (1) the speaker's own voice is always present and predictable from the forward model, and (2) no passive listening condition was included. Therefore, our findings cannot be directly equated with the original SIS effect.</p>
<p>Instead, we propose that the suppression observed here reflects a SIS-related phenomenon specific to the synchronous speech context. Synchronous speech requires simultaneous monitoring of self- and externallygenerated speech, a task that is both attentionally demanding and coordinative. This aligns with evidence from Ozker et al. (2024, 2022), showing that the same neural populations in STG exhibit SIS and heightened responses to feedback perturbations. These findings suggest that SIS and speech monitoring are related processes, where suppressing responses to self-generated speech facilitates error detection. In our study, suppression of HFa as coordination increases may reflect reduced prediction errors due to closer alignment between perceived and produced speech signals. Conversely, increased HFa during poor coordination may signify greater mismatch, consistent with prediction error theories (Houde &amp; Nagarajan, 2011; Friston et al., 2020). Furthermore, when self- and externally-generated speech signals are temporally and phonetically congruent, participants may perceive external speech as their own. This echoes the &quot;rubber voice&quot; effect, where external speech resembling self-produced feedback is perceived as self-generated (Zheng et al., 2011; Lind et al., 2014; Franken et al., 2021). While this interpretation remains speculative, future studies could incorporate subjective reports to investigate this phenomenon in more detail.” References cited:</p>
<p>– Franken, M. K., Hartsuiker, R. J., Johansson, P., Hall, L., &amp; Lind, A. (2021). Speaking With an Alien Voice: Flexible Sense of Agency During Vocal Production. Journal of Experimental Psychology-Human perception and performance, 47(4), 479-494. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/xhp0000799">https://doi.org/10.1037/xhp0000799</ext-link></p>
<p>– Houde, J. F., &amp; Nagarajan, S. S. (2011). Speech production as state feedback control. Frontiers in human neuroscience, 5, 82.</p>
<p>– Lind, A., Hall, L., Breidegard, B., Balkenius, C., &amp; Johansson, P. (2014). Speakers' acceptance of real-time speech exchange indicates that we use auditory feedback to specify the meaning of what we say. Psychological Science, 25(6), 1198-1205. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0956797614529797">https://doi.org/10.1177/0956797614529797</ext-link></p>
<p>– Meekings, S., &amp; Scott, S. K. (2021). Error in the Superior Temporal Gyrus? A Systematic Review and Activation Likelihood Estimation Meta-Analysis of Speech Production Studies. Journal of Cognitive Neuroscience, 33(3), 422-444. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_01661">https://doi.org/10.1162/jocn_a_01661</ext-link></p>
<p>– Niziolek C. A., Nagarajan S. S., Houde J. F (2013) What does motor efference copy represent? Evidence from speech production Journal of Neuroscience 33:16110–16116Ozker M., Doyle W., Devinsky O., Flinker A (2022) A cortical network processes auditory error signals during human speech production to maintain fluency PLoS Biology 20.</p>
<p>– Ozker, M., Yu, L., Dugan, P., Doyle, W., Friedman, D., Devinsky, O., &amp; Flinker, A. (2024). Speech-induced suppression and vocal feedback sensitivity in human cortex. eLife, 13, RP94198. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.94198">https://doi.org/10.7554/eLife.94198</ext-link></p>
<p>– Zheng, Z. Z., MacDonald, E. N., Munhall, K. G., &amp; Johnsrude, I. S. (2011). Perceiving a Stranger's Voice as Being One's Own: A 'Rubber Voice' Illusion? PLOS ONE, 6(4), e18655.</p>
<disp-quote content-type="editor-comment">
<p>(5) There are some seemingly arbitrary decisions made in the design and analysis that, while likely justified, need to be explained. For example, how were the cutoffs for moderate coupling vs phase-shifted coupling (k ~0.09) determined? This is noted as &quot;rather weak&quot; (line 212), but it's not clear where this comes from. Similarly, the ROI-based analyses are only done on regions &quot;recorded in at least 7 patients&quot; - how was this number chosen? How many electrodes total does this correspond to? Is there heterogeneity within each ROI?</p>
</disp-quote>
<p>The reviewer is correct, we apologize for this missing information. We now specify that the coupling values were empirically determined on the basis of a pilot experiment in order to induce more or less synchronization, but keeping the phase-shifted coupling at a rather implicit level.</p>
<p>Concerning the definition of coupling as weak, one should consider that, in the Kuramoto model, the strength of coupling (k) is relative to the spread of the natural frequencies (Δω) in the system. In our study, the natural frequencies of syllables range approximately from 2 Hz to 10Hz, resulting in a frequency spread of Δω = 8 Hz. For coupling to strongly synchronize oscillators across such a wide range, k must be comparable to or exceed Δω. Thus, since k = 0.1 is far much smaller than Δω, it is therefore classified as weak coupling.</p>
<p>We have now modified the Materials and methods section as follows:</p>
<p>“More precisely, for a third of the trials the VP had a neutral behaviour (close to zero coupling: k = +/- 0.01). For a third it had a moderate coupling, meaning that the VP synchronised more to the participant speech (k = -0.09). And for the last third of the trials the VP had a moderate coupling but with a phase shift of pi/2, meaning that it moderately aimed to speak in between the participant syllables (k = + 0.09). The coupling values were empirically determined on the basis of a pilot experiment in order to induce more or less synchronization but keeping the phase-shifted coupling at a rather implicit level. In other terms, while participants knew that the VP would adapt, they did not necessarily know in which direction the coupling went.”</p>
<p>Regarding the criterion of including regions recorded in at least 7 patients, our goal was to balance data completeness with statistical power. Given our total sample of 16 patients, this threshold ensures that each included region is represented in at least ~44% of the cohort, reducing the likelihood of spurious findings due to extremely small sample sizes. This choice also aligns with common neurophysiological analysis practices, where a minimum number of subjects (at least 2 in extreme cases) is required to achieve meaningful interindividual comparisons while avoiding excessive data exclusion. Additionally, this threshold maintains a reasonable tradeoff between maximizing patient inclusion and ensuring that statistical tests remain robust.</p>
<p>We have now added more information in the Results section “Spectral profiles in the language network are nuanced by behaviour” on this point as follows:</p>
<p>“To balance data completeness and statistical power, we included only brain regions recorded in at least 7 patients (~44% of the cohort) for the left hemisphere and at least 5 patients for the right hemisphere (~31% of the cohort), ensuring sufficient representation while minimizing biases due to sparse data.”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary:</p>
<p>This paper investigates the neural underpinnings of an interactive speech task requiring verbal coordination with another speaker. To achieve this, the authors recorded intracranial brain activity from the left hemisphere in a group of drug-resistant epilepsy patients while they synchronised their speech with a 'virtual partner'. Crucially, the authors were able to manipulate the degree of success of this synchronisation by programming the virtual partner to either actively synchronise or desynchronise their speech with the participant, or else to not vary its speech in response to the participant (making the synchronisation task purely one-way). Using such a paradigm, the authors identified different brain regions that were either more sensitive to the speech of the virtual partner (primary auditory cortex), or more sensitive to the degree of verbal coordination (i.e. synchronisation success) with the virtual partner (secondary auditory cortex and IFG). Such sensitivity was measured by (1) calculating the correlation between the index of verbal coordination and mean power within a range of frequency bands across trials, and (2) calculating the phase-amplitude coupling between the behavioural and brain signals within single trials (using the power of high-frequency neural activity only). Overall, the findings help to elucidate some of the left hemisphere brain areas involved in interactive speaking behaviours, particularly highlighting the highfrequency activity of the IFG as a potential candidate supporting verbal coordination.</p>
<p>Strengths:</p>
<p>This study provides the field with a convincing demonstration of how to investigate speaking behaviours in more complex situations that share many features with real-world speaking contexts e.g. simultaneous engagement of speech perception and production processes, the presence of an interlocutor, and the need for inter-speaker coordination. The findings thus go beyond previous work that has typically studied solo speech production in isolation, and represent a significant advance in our understanding of speech as a social and communicative behaviour. It is further an impressive feat to develop a paradigm in which the degree of cooperativity of the synchronisation partner can be so tightly controlled; in this way, this study combines the benefits of using prerecorded stimuli (namely, the high degree of experimental control) with the benefits of using a live synchronisation partner (allowing the task to be truly two-way interactive, an important criticism of other work using pre-recorded stimuli). A further key strength of the study lies in its employment of stereotactic EEG to measure brain responses with both high temporal and spatial resolution, an ideal method for studying the unfolding relationship between neural processing and this dynamic coordination behaviour.</p>
</disp-quote>
<p>We sincerely appreciate the Reviewer's thoughtful and positive feedback on our manuscript.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>One major limitation of the current study is the lack of coverage of the right hemisphere by the implanted electrodes. Of course, electrode location is solely clinically motivated, and so the authors did not have control over this. However, this means that the current study neglects the potentially important role of the right hemisphere in this task. The right hemisphere has previously been proposed to support feedback control for speech (likely a core process engaged by synchronous speech), as opposed to the left hemisphere which has been argued to underlie feedforward control (Tourville &amp; Guenther, 2011). Indeed, a previous fMRI study of synchronous speech reported the engagement of a network of right hemisphere regions, including STG, IPL, IFG, and the temporal pole (Jasmin et al., 2016). Further, the release from speech-induced suppression during a synchronous speech reported by Jasmin et al. was found in the right temporal pole, which may explain the discrepancy with the current finding of reduced leftward high-frequency activity with increasing verbal coordination (suggesting instead increased speech-induced suppression for successful synchronisation). The findings should therefore be interpreted with the caveat that they are limited to the left hemisphere, and are thus likely missing an important aspect of the neural processing underpinning verbal coordination behaviour.</p>
</disp-quote>
<p>We have now included, in the supplementary materials, data from the right hemisphere, although the coverage is a bit sparse (Figures S2, S4, S5, see our responses in the ‘Recommendation for the authors’ section, below). We have also revised the Discussion section to add the putative role of right temporal regions (see below as well).</p>
<disp-quote content-type="editor-comment">
<p>A further limitation of this study is that its findings are purely correlational in nature; that is, the results tell us how neural activity correlates with behaviour, but not whether it is instrumental in that behaviour. Elucidating the latter would require some form of intervention such as electrode stimulation, to disrupt activity in a brain area and measure the resulting effect on behaviour. Any claims therefore as to the specific role of brain areas in verbal coordination (e.g. the role of the IFG in supporting online coordinative adjustments to achieve synchronisation) are therefore speculative.</p>
</disp-quote>
<p>We appreciate the reviewer’s observation regarding the correlational nature of our findings and agree that this is a common limitation of neuroimaging studies. While elucidating causal relationships would indeed require intervention techniques such as electrical stimulation, our study leverages the unique advantages of intracerebral recordings, offering the best available spatial and temporal resolution alongside a high signal-tonoise ratio. These attributes ensure that our data accurately reflect neural activity and its temporal dynamics, providing a robust foundation for understanding the relationship between neural processes and behaviour. Therefore, while causal claims are beyond the scope of this study, the precision of our methodology allows us to make well-supported observations about the neural correlates of synchronous speech tasks.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewing Editor Comment:</bold></p>
<p>After joint consultation, we are seeing the potential for the report to be strengthened and the evidence here to be deemed ultimately at least 'solid': to us (editors and reviewers) it seems that this would require both (1) clarifying/acknowledging the limitations of not having right hemisphere data, and (2) running some of the additional analyses the reviewers suggest, which should allow for richer examination of the data e.g. phase relationships in areas that correlate with synchronisation.</p>
</disp-quote>
<p>We have now added data on the right hemisphere (RH) that we did not previously report due to a rather sparse sampling of the RH. These results are now reported in the Results section as well as in the Supplementary section, where we put all right hemisphere figures for all analyses (Figure S2, S4, S5). We have also run additional analyses digging into the phase relationship in areas that correlate with synchronisation (Figure S6). These additional analyses allowed us to improve the Discussion section as well.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>In some sections, the writing is a bit unclear, with both typos and vague statements that could be fixed with careful proofreading.</p>
</disp-quote>
<p>We thank the reviewer for pointing out areas where the writing could be improved. We carefully proofread the manuscript to address typos and clarify any vague statements. Specific sections identified as unclear have been rephrased for better precision and readability.</p>
<disp-quote content-type="editor-comment">
<p>In Figure 1, the colors repeat, making it impossible to tell patients apart.</p>
</disp-quote>
<p>We have now updated Figure 1 colormap to avoid redundancy and added the right hemisphere.</p>
<disp-quote content-type="editor-comment">
<p>Line 132: &quot;16 unilateral implantations (9 left, 7 bilateral implantations)&quot;. Should this say 7 right hemisphere? If so, the following sentence stating that there was &quot;insufficient cover [sic] of the right hemisphere&quot; is unclear, since the number of patients between LH and RH is similar.</p>
</disp-quote>
<p>The confusion was due to the fact that the lateralization refers to the presence/absence of electrodes in the Heschl’s gyrus (left : H’ ; right : H) exclusively.</p>
<p>We have thus changed this section as follows:</p>
<p>“16 patients (7 women, mean age 29.8 y, range 17 - 50 y) with pharmacoresistant epilepsy took part in the study. They were included if their implantation map covered at least partially the Heschl's gyrus and had sufficiently intact diction to support relatively sustained language production.” The relevant part (previously line 132) now states:</p>
<p>“Sixteen patients with a total of 236 electrodes (145 in the left hemisphere) and 2395 contacts (1459 in the left hemisphere, see Figure 1). While this gives a rather sparse coverage of the right hemisphere, we decided, due to the rarity of this type of data, to report results for both hemispheres, with figures for the left hemisphere in the main text and figures for the right hemisphere in the supplementary section.”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>(1) To address the concern regarding the absence of data from the right hemisphere, I would advise the authors to directly acknowledge this limitation in their Discussion section, citing relevant work suggesting that the right hemisphere has an important role to play in this task (e.g. Jasmin et al., 2016). You should also make this clear in your abstract e.g. you could rewrite the sentence in line 40 to be: &quot;Then, we recorded the intracranial brain activity of the left hemisphere in 16 patients with drug-resistant epilepsy...&quot;.</p>
</disp-quote>
<p>We are grateful to the reviewer for this comment that incited us to look into the right hemisphere data. We have now included results in the right hemisphere, although the coverage is a bit sparse. We have also revised the Discussion section to add the putative role of right temporal regions. Interestingly, our results show, as suggested by the reviewer, a clear involvement of the RH in this task.</p>
<p>First, the full brain analyses show a very similar implication of the RH as compared to the LH (see Figure below). We have now added in the Results section:</p>
<p>“As expected, the whole language network is strongly involved, including both dorsal and ventral pathways (Fig 3A). More precisely, in the left temporal lobe the superior, middle and inferior temporal gyri, in the left parietal lobe the inferior parietal lobule (IPL) and in the left frontal lobe the inferior frontal gyrus (IFG) and the middle frontal gyrus (MFG). Similar results are observed in the right hemisphere, neural responses being present across all six frequency bands with medium to large modulation in activity compared to baseline (Figure S2A) in the same regions. Desynchronizations are present in the theta, alpha and beta bands while the low gamma and HFa bands show power increases.”</p>
<p>As to compared to the left hemisphere, assessing brain-behaviour correlations in the right hemisphere does not provide the same statistical power, because some anatomical regions have very few electrodes. Nonetheless, we observe a strong correlation in the right IFG, similar to the one we previously reported in the left hemisphere, and we now report in the Results section:</p>
<p>“The decrease in HFa along the dorsal pathway is replicated in the right hemisphere (Figure S4). However, while both the right STG BA41/42 and STG BA22 present a power increase (compared to baseline) — with a stronger increase for the STG BA41/42 — neither shows a significant correlation with verbal coordination (t(45)=-1.65, p=.1 ; t(8)=-0.67, p=.5 ; Student’s T test, FDR correction). By contrast, results in the right IFG BA44 are similar to the one observed in the left hemisphere with a significant power increase associated with a negative brainbehaviour correlation (t(17) = -3.11, p = .01 ; Student’s T test, FDR correction).”</p>
<p>Interestingly, the phase-amplitude coupling analysis yields very similar results in both hemispheres (exception made for BA22). We have thus updated the Results section as follows:</p>
<p>“Notably, when comparing – within the regions of interest previously described – the PAC with the virtual partner speech and the PAC with the phase difference, the coupling relationship changes when moving along the dorsal pathway: a stronger coupling in the auditory regions with the speech input, no difference between speech and coordination dynamics in the IPL and a stronger coupling for the coordinative dynamics compared to speech signal in the IFG (Figure 5B ). When looking at the right hemisphere, we observe the same changes in the coupling relationship when moving along the dorsal pathway, except that no difference between speech and coordination dynamics is present in the right secondary auditory regions (STG BA22; Figure S5).”</p>
<p>We also included in the Discussion section the right hemisphere results also mentioning previous work of Guenther and the one of Jasmin. On the section “Left secondary auditory regions are more sensitive to coordinative behaviour” one can read:</p>
<p>“Furthermore, the absence of correlation in the right STG BA22 (Figure S4) seems in first stance to challenge influential speech production models (e.g. Guenther &amp; Hickok, 2016) that propose that the right hemisphere is involved in feedback control. However, one needs to consider the the task at stake heavily relied upon temporal mismatches and adjustments. In this context, the left-lateralized sensitivity to verbal coordination reminds of the works of Floegel and colleagues (2020, 2023) suggesting that both hemispheres are involved depending on the type of error: the right auditory association cortex monitoring preferentially spectral speech features and the left auditory association cortex monitoring preferentially temporal speech features. Nonetheless, the right temporal pole seems to be sensitive to speech coordinative behaviour, confirming previous findings using fMRI (Jasmin et al., 2016) and thus showing that the right hemisphere has an important role to play in this type of tasks (e.g. Jasmin et al., 2016).”</p>
<p>References cited:</p>
<p>– Floegel, M., Fuchs, S., &amp; Kell, C. A. (2020). Differential contributions of the two cerebral hemispheres to temporal and spectral speech feedback control. Nature Communications, 11(1), 2839.</p>
<p>– Floegel, M., Kasper, J., Perrier, P., &amp; Kell, C. A. (2023). How the conception of control influences our understanding of actions. Nature Reviews Neuroscience, 24(5), 313-329.</p>
<p>– Guenther, F. H., &amp; Hickok, G. (2016). Neural models of motor speech control. In Neurobiology of language (pp. 725-740). Academic Press.</p>
<disp-quote content-type="editor-comment">
<p>(2) When discussing previous work on alignment during synchronous speech, you may wish to include a recently published paper by Bradshaw et al (2024); this manipulated the acoustics of the accompanist's voice during a synchronous speech task to show interactions between speech motor adaptation and phonetic convergence/alignment.</p>
</disp-quote>
<p>We thank the reviewer for pointing to this recent and interesting paper. We added the article as reference as follows</p>
<p>“Furthermore, synchronous speech favors the emergence of alignment phenomena, for instance of the fundamental frequency or the syllable onset (Assaneo et al., 2019 ; Bradshaw &amp; McGettigan, 2021 ; Bradshaw et al., 2023; Bradshaw et al., 2024).”</p>
<disp-quote content-type="editor-comment">
<p>(3) Line 80: &quot;Synchronous speech resembles to a certain extent to delayed auditory feedback tasks&quot;- I think you mean &quot;altered auditory feedback tasks&quot; here.</p>
</disp-quote>
<p>In the case of synchronous speech it is more about timing than altered speech signals, that is why the comparison is done with delayed and not altered auditory feedback. Nonetheless, we understand the Reviewer’s point and we have now changed the sentence as follows:</p>
<p>“Synchronous speech resembles to a certain extent to delayed/altered auditory feedback tasks”</p>
<disp-quote content-type="editor-comment">
<p>(4) When discussing superior temporal responses during such altered feedback tasks, you may also want to cite a review paper by Meekings and Scott (2021).</p>
</disp-quote>
<p>We thank the reviewer for this suggestion, indeed this was a big oversight!</p>
<p>The paper is now quoted in the introduction as follows:</p>
<p>“Previous studies have revealed increased responses in the superior temporal regions compared to normal feedback conditions (Hirano et al., 1997 ; Hashimoto &amp; Sakai, 2003 ; Takaso et al., 2010 ; Ozerk et al., 2022 ; Floegel et al., 2020 ; see Meekings &amp; Scott, 2021 for a review of error-monitoring and feedback control in the STG during speech production).”</p>
<p>Furthermore, we updated the discussion part concerning the speaker-induced suppression phenomenon (see below our response to the point 10).</p>
<disp-quote content-type="editor-comment">
<p>(5) Line 125: &quot;The parameters and sound adjustment were set using an external low-latency sound card (RME Babyface Pro Fs)&quot;. Can you please report the total feedback loop latency in your set-up? Or at the least cite the following paper which reports low latencies with this audio device.</p>
<p>Kim, K. S., Wang, H., &amp; Max, L. (2020). It's About Time: Minimizing Hardware and Software Latencies in Speech Research With Real-Time Auditory Feedback. Journal of Speech, Language, and Hearing Research, 63(8), 25222534. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1044/2020_JSLHR-19-00419">https://doi.org/10.1044/2020_JSLHR-19-00419</ext-link></p>
</disp-quote>
<p>We now report the total feedback loop latency (~5ms) and also cite the relevant paper (Kim et al., 2020).</p>
<disp-quote content-type="editor-comment">
<p>(6) Line 127 &quot;A calibration was made to find a comfortable volume and an optimal balance for both the sound of the participant's own voice, which was fed back through the headphones, and the sound of the stimuli.&quot; What do you mean here by an 'optimal balance'? Was the participant's own voice always louder than the VP stimuli? Can you report roughly what you consider to be a comfortable volume in dB?</p>
</disp-quote>
<p>This point was indeed unlcear. We have now changed as follows:</p>
<p>“A calibration was made to find a comfortable volume and an optimal balance for both the sound of the participant's own voice, which was fed back through the headphones, and the sound of the stimuli. The aim of this procedure was that the patient would subjectively perceive their voice and the VP-voice in equal measure. VP voice was delivered at approximately 70dB.”</p>
<disp-quote content-type="editor-comment">
<p>(7) Relatedly, did you use any noise masking to mask the air-conducted feedback from their own voice (which would have been slightly out of phase with the feedback through the headphones, depending on your latency)?</p>
</disp-quote>
<p>Considering the low-latency condition allowed with the sound card (RME Babyface Pro Fs), we did not use noise masking to mask the air-conducted feedback from the self-voice of the patients.</p>
<disp-quote content-type="editor-comment">
<p>(8) Line 141: &quot;four short sentences were pre-recorded by a woman and a man.&quot; Did all participants synchronise with both the man and woman or was the VP gender matched to that of the participant/patient?</p>
</disp-quote>
<p>We thank the reviewer for this important missing detail. We know changed the text as follows:</p>
<p>“Four stimuli corresponding to four short sentences were pre-recorded by both a female and a male speaker. This allowed to adapt to the natural gender differences in fundamental frequency (i.e. so that the VP gender matched that of the patients). All stimuli were normalised in amplitude.”</p>
<disp-quote content-type="editor-comment">
<p>(9) Can you clarify what instructions participants were given regarding the VP? That is, were they told that this was a recording or a real live speaker? Were they naïve to the manipulation of the VP's coupling to the participant?</p>
</disp-quote>
<p>We have now added this information to the task description as follows:</p>
<p>“Participants, comfortably seated in a medical chair, were instructed that they would perform a real-time interactive synchronous speech task with an artificial agent (Virtual Partner, henceforth VP, see next section) that can modulate and adapt to the participant’s speech in real time.”</p>
<p>“The third step was the actual experiment. This was identical to the training but consisted of 24 trials (14s long, speech rate ~3Hz, yielding ~1000 syllables). Importantly, the VP varied its coupling behaviour to the participant. More precisely, for a third of the sequences the VP had a neutral behaviour (close to zero coupling : k = +/- 0.01). For a third it had a moderate coupling, meaning that the VP synchronised more to the participant speech (k = - 0.09). And for the last third of the sequences the VP had a moderate coupling but with a phase shift of pi/2, meaning that it moderately aimed to speak in between the participant syllables (k = + 0.09). The coupling values were empirically determined on the basis of a pilot experiment in order to induce more or less synchronization, but keeping the phase-shifted coupling at a rather implicit level. In other terms, while participants knew that the VP would adapt, they did not necessarily know in which direction the coupling went.”</p>
<disp-quote content-type="editor-comment">
<p>(10) The paragraph from line 438 entitled &quot;Secondary auditory regions are more sensitive to coordinative behaviour&quot; includes an interesting discussion of the relation of the current findings to the phenomenon of speech-induced suppression (SIS). However, the authors appear to equate the observed decrease in highfrequency activity as speech coordination increases with the phenomenon of SIS (in lines 456-457), which is quite a speculative leap. I would encourage the authors to temper this discussion by referring to SIS as a potentially related phenomenon, with a need for more experimental work to determine if this is indeed the same phenomenon as the decreases in high-frequency power observed here. I believe that the authors are arguing here for an interpretation of SIS as reflecting internal modelling of sensory input regardless of whether this is self-generated or other-generated; if this is indeed the case, I would ask the authors to be more explicit here that these ideas are not a standard part of the traditional account of SIS, which only includes internal modelling of self-produced sensory feedback.</p>
</disp-quote>
<p>As stated in the public review, we thank both reviewers for raising thoughtful concerns about our interpretation of the observed neural suppression as related to speaker-induced suppression (SIS). We agree that our study lacks a passive listening condition, which limits direct comparisons to the original SIS effect, traditionally defined as the suppression of neural responses to self-produced speech compared to externally-generated speech (Meekings &amp; Scott, 2021).</p>
<p>In response, we have reconsidered our terminology and interpretation. In the revised discussion, we refer to our findings as a &quot;SIS-related phenomenon specific to the synchronous speech context.&quot; Unlike classic SIS paradigms, our interactive task involves simultaneous monitoring of self- and externally-generated speech, introducing additional attentional and coordinative demands.</p>
<p>The revised discussion also incorporates findings by Ozker et al. (2024, 2022), which link SIS and speech monitoring, suggesting that suppressing responses to self-generated speech facilitates error detection. We propose that the decrease in high-frequency activity (HFa) as verbal coordination increases reflects reduced error signals due to closer alignment between perceived and produced speech. Conversely, HFa increases with reduced coordination may signify greater prediction error.</p>
<p>Additionally, we relate our findings to the &quot;rubber voice&quot; effect (Zheng et al., 2011; Lind et al., 2014; Franken et al., 2021), where temporally and phonetically congruent external speech can be perceived as self-generated. We speculate that this may occur in synchronous speech tasks when the participant's and VP's speech signals closely align. However, this interpretation remains speculative, as no subjective reports were collected to confirm this perception. Future studies could include participant questionnaires to validate this effect and relate subjective experience to neural measures of synchronization.</p>
<p>Overall, our findings extend the study of SIS to dynamic, interactive contexts and contribute to understanding internal forward models of speech production in more naturalistic scenarios.</p>
<p>We have now added these points to the discussion as follows:</p>
<p>“The observed negative correlation between verbal coordination and high-frequency activity (HFa) in STG BA22 suggests a suppression of neural responses as the degree of synchrony increases. This result aligns with findings on speaker-induced suppression (SIS), where neural activity in auditory cortex decreases during self-generated speech compared to externally-generated speech (Meekings &amp; Scott, 2021; Niziolek et al., 2013). However, our paradigm differs from traditional SIS studies in two critical ways: (1) the speaker's own voice is always present and predictable from the forward model, and (2) no passive listening condition was included. Therefore, our findings cannot be directly equated with the original SIS effect.</p>
<p>Instead, we propose that the suppression observed here reflects a SIS-related phenomenon specific to the synchronous speech context. Synchronous speech requires simultaneous monitoring of self- and externally generated speech, a task that is both attentionally demanding and coordinative. This aligns with evidence from Ozker et al. (2024, 2022), showing that the same neural populations in STG exhibit SIS and heightened responses to feedback perturbations. These findings suggest that SIS and speech monitoring are related processes, where suppressing responses to self-generated speech facilitates error detection.</p>
<p>In our study, suppression of HFa as coordination increases may reflect reduced prediction errors due to closer alignment between perceived and produced speech signals. Conversely, increased HFa during poor coordination may signify greater mismatch, consistent with prediction error theories (Houde &amp; Nagarajan, 2011; Friston et al., 2020).”</p>
<disp-quote content-type="editor-comment">
<p>(11) Within this section, you also speculate in line 460 that &quot;Moreover, when the two speech signals come close enough in time, the patient possibly perceives them as its own voice.&quot; I would recommend citing studies on the 'rubber voice' effect to back up this claim (e.g. Franken et al., 2021; Lind et al., 2014; Zheng et al., 2011).</p>
</disp-quote>
<p>We are grateful to the Reviewer for this interesting suggestion. Directly following the previous comment, the section now states:</p>
<p>“Furthermore, when self- and externally-generated speech signals are temporally and phonetically congruent, participants may perceive external speech as their own. This echoes the &quot;rubber voice&quot; effect, where external speech resembling self-produced feedback is perceived as self-generated (Zheng et al., 2011; Lind et al., 2014; Franken et al., 2021). While this interpretation remains speculative, future studies could incorporate subjective reports to investigate this phenomenon in more detail.”</p>
<disp-quote content-type="editor-comment">
<p>(12) As noted in my public review, since your methods are correlational, you need to be careful about inferring the causal role of any brain areas in supporting a specific aspect of functioning e.g. line 501-504: &quot;By contrast, in the inferior frontal gyrus, the coupling in the high-frequency activity is strongest with the input-output phase difference (input of the VP - output of the speaker), a metric that reflects the amount of error in the internal computation to reach optimal coordination, which indicates that this region optimises the predictive and coordinative behaviour required by the task.&quot; I would argue that the latter part of this sentence is a conclusion that, although consistent with, goes beyond the current data in this study, and thus needs tempering.</p>
</disp-quote>
<p>We agree with the Reviewer and changed the sentence as follows:</p>
<p>“By contrast, in the inferior frontal gyrus, the coupling in the high-frequency activity is strongest with the inputoutput phase difference (input of the VP - output of the speaker), a metric that could possibly reflect the amount of error in the internal computation to reach optimal coordination. This indicates that this region could have an implication in the optimisation of the predictive and coordinative behaviour required by the task.”</p>
</body>
</sub-article>
</article>