<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">106464</article-id>
<article-id pub-id-type="doi">10.7554/eLife.106464</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106464.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>A geometric shape regularity effect in the human brain</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0844-0775</contrib-id>
<name>
<surname>Sablé-Meyer</surname>
<given-names>Mathias</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>meyer@ucl.ac.uk</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9578-6039</contrib-id>
<name>
<surname>Benjamin</surname>
<given-names>Lucas</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6588-0614</contrib-id>
<name>
<surname>Watkins</surname>
<given-names>Cassandra Potier</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>He</surname>
<given-names>Chenxi</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Pajot</surname>
<given-names>Maxence</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Morfoisse</surname>
<given-names>Théo</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9590-080X</contrib-id>
<name>
<surname>Al Roumi</surname>
<given-names>Fosca</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7418-8275</contrib-id>
<name>
<surname>Dehaene</surname>
<given-names>Stanislas</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04ex24z53</institution-id><institution>Collège de France, Université Paris-Sciences-Lettres (PSL)</institution></institution-wrap>, <city>Paris</city>, <country country="FR">France</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xjwb503</institution-id><institution>Cognitive Neuroimaging Unit, CEA, INSERM, Université Paris-Saclay, NeuroSpin Center</institution></institution-wrap>, <city>Gif-sur-Yvette</city>, <country country="FR">France</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Sainsbury Wellcome Centre for Neural Circuits and Behaviour, University College London</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Groen</surname>
<given-names>Iris</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Amsterdam</institution>
</institution-wrap>
<city>Amsterdam</city>
<country country="NL">Netherlands</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Frank</surname>
<given-names>Michael J</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Brown University</institution>
</institution-wrap>
<city>Providence</city>
<country country="US">United States</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-05-30">
<day>30</day>
<month>05</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP106464</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-02-28">
<day>28</day>
<month>02</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-02-06">
<day>06</day>
<month>02</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.03.13.584141"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Sablé-Meyer et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Sablé-Meyer et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-106464-v1.pdf"/>
<abstract>
<title>Abstract</title><p>The perception and production of regular geometric shapes is a characteristic trait of human cultures since prehistory, whose neural mechanisms are unknown. Behavioral studies suggest that humans are attuned to discrete regularities such as symmetries and parallelism, and rely on their combinations to encode regular geometric shapes in a compressed form. To identify the relevant brain systems and their dynamics, we collected functional MRI and magnetoencephalography data in both adults and six-year-olds during the perception of simple shapes such as hexagons, triangles and quadrilaterals. The results revealed that geometric shapes, relative to other visual categories, induce a hypoactivation of ventral visual areas and an overactivation of the intraparietal and inferior temporal regions also involved in mathematical processing, whose activation is modulated by geometric regularity. While convolutional neural networks captured the early visual activity evoked by geometric shapes, they failed to account for subsequent dorsal parietal and prefrontal signals, which could only be captured by discrete geometric features or by more advanced transformer models of vision. We propose that the perception of abstract geometric regularities engages an additional symbolic mode of visual perception.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Reworked the overall structure and improved the discussion; added several additional models; added authors who helped with these new models.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>&quot;In geometry, the essential is invisible to the eyes, one sees well only with the mind.&quot; - Emmanuel Giroux (French blind mathematician)</p>
<p>Long before the invention of writing, the very first detectable graphic productions of prehistoric humans were highly regular non-pictorial geometric signs such as parallel lines, zig-zags, triangular or checkered patterns (<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>). Human cultures throughout the world compose complex figures using simple geometrical regularities such as parallelism and symmetry in their drawings, decorative arts, tools, buildings, graphics and maps (<xref ref-type="bibr" rid="c3">3</xref>). Cognitive anthropological studies suggest that, even in the absence of formal western education, humans possess intuitions of foundational geometric concepts such as points and lines and how they combine to form regular shapes (<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c5">5</xref>). The scarce data available to date suggests that other primates, including chimpanzees, may not share the same ability to perceive and produce regular geometric shapes (<xref ref-type="bibr" rid="c6">6</xref>–<xref ref-type="bibr" rid="c10">10</xref>), though unintentional-but-regular mark-marking behavior have been reported in macaques (<xref ref-type="bibr" rid="c11">11</xref>). Thus, studying the brain mechanisms that support the perception of geometric regularities may shed light on the origins of human compositionality and, ultimately, the mental language of mathematics. Here, we provide a first approach through the recording of functional MRI and magneto-encephalography signals evoked by simple classical geometric shapes such as triangles or squares. Our goal is to probe whether, over and above the pathways for processing the shapes of images such as faces, places or objects, the regularities of geometric shapes evoke additional activity.</p>
<p>The present brain-imaging research capitalizes on a recent series of studies of how humans perceive quadrilaterals (<xref ref-type="bibr" rid="c8">8</xref>). In this study, we created 11 tightly matched stimuli which were all simple, non-figurative, textureless four-sided shapes, yet varied in their geometric regularity. The most regular was the square, with four parallel sides of equal length and four identical right-angles. By progressively removing some of these features (parallelism, right-angles, equality of length, equality of angles), we created a hierarchy of quadrilaterals ranging from highly regular to completely irregular (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>). In a variety of tasks, geometric regularity had a large effect on human behavior. For instance, for equal objective amounts of deviation, human adults and children detected a deviant shape more easily among shapes of high regularity, such as squares or rectangles (&lt;5% errors), than among irregular quadrilaterals (&gt;40% errors). The effect appeared as a human universal, present in preschoolers, first-graders, and adults without access to formal western math education (the Himba from Namibia). However, when baboons were trained to perform the same task, they showed no such geometric regularity effect. Baboon behavior was accounted for by convolutional neural network (CNN) models of object recognition, but human behavior could only be explained by appealing to a representation of discrete geometric properties of parallelism, right angle and symmetry, in this and other tasks (<xref ref-type="bibr" rid="c12">12</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 1.</label>
<caption><title>Measuring and modeling the perceptual similarity of geometric shapes.</title>
<p>(<bold>A</bold>) The eleven quadrilaterals used throughout the experiments (colors are consistently used in all other figures). (<bold>B</bold>) Sample displays for the behavioral visual search task used to estimate the 11×11 shape similarity matrix. Participants had to locate the deviant shape. The right insert shows two trials from the behavioral visual task search task, used to estimate the 11×11 shape similarity matrix. Participants had to find the intruder within 9 shapes. (<bold>C</bold>) Multidimensional scaling of human dissimilarity judgments; the grey arrow indicates the projection on the MDS space of the number of geometric primitives in a shape. (<bold>D</bold>) The behavioral dissimilarity matrix (left) was better captured by a symbolic coding model (middle) than by a convolutional neural network (right). The graph at right shows the GLM coefficients for each participant.</p></caption>
<graphic xlink:href="584141v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We therefore formulated the hypothesis that, in the domain of geometry, humans deploy an additional cognitive process, specifically attuned to geometric regularities. On top of the circuits for object recognition, which are largely homologous in human and non-human primates (<xref ref-type="bibr" rid="c13">13</xref>–<xref ref-type="bibr" rid="c15">15</xref>), the human code for geometric shapes would involve a distinct “language of thought”, an encoding of discrete mathematical regularities and their combinations (<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c16">16</xref>–<xref ref-type="bibr" rid="c19">19</xref>).</p>
<p>This hypothesis predicts that the most elementary geometric shapes, such as a square, are not solely processed within the ventral and dorsal visual pathways, but may also evoke a later stage of geometrical feature encoding in brain areas that were previously shown to encode arithmetic, geometric and other mathematical properties, i.e. the bilateral intraparietal, inferotemporal and dorsal prefrontal areas (<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>). We hypothesized that (1) such cognitive processes encode shapes according to their discrete geometric properties including parallelism, right angles, equal lengths and equal angles; (2) the brain compress this information when those properties are more regularly organized, and thus exhibit activity proportional to minimal description length (<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c23">23</xref>); and (3) these computations occur downstream of other visual processes, since they rely on the initial output of visual processing pathways.</p>
<p>Here, we assessed these spatiotemporal predictions using two complementary neuroimaging techniques (functional MRI and magnetoencephalography). We presented the same 11 quadrilaterals as in our previous research and used representational similarity analysis (<xref ref-type="bibr" rid="c24">24</xref>) to contrast two models for their cerebral encoding, based either on classical CNN models or on exact geometric features. In the fMRI experiment, we also collected simpler images contrasting the category of geometric shapes to other classical categories such as faces, places or tools. Furthermore, to evaluate how early the brain networks for geometric shape perception arise, we collected those fMRI data in two age groups: adults, and children in 1st grade (6-years-old, this year was selected as it marks the first-year French students receive formal instruction in mathematics). If geometric shape perception involves elementary intuitions of geometric regularity common to all humans, then the corresponding brain networks should be detectable early on.</p>
</sec>
<sec id="s2">
<label>Experiment 1</label><title>Estimating the Representational Similarity of Quadrilaterals with Online Behavior</title>
<p>Our research focuses on the 11 quadrilaterals previous used in our behavioral research (<xref ref-type="bibr" rid="c8">8</xref>), which are tightly matched in average pairwise distance between vertices and length of the bottom side, yet differing broadly in geometric regularity (<xref rid="fig1" ref-type="fig">Fig.1A</xref>). The goal of our first experiment was to obtain a 11×11 matrix of behavioral dissimilarities between the 11 geometric shapes shown in <xref rid="fig1" ref-type="fig">Fig.1A</xref>, in order to compare it with predictions from classical visual models, embodied by CNNs, as well as geometric feature models of shape perception. To evaluate perceptual similarity in an objective manner, in experiment 1 we assessed the difficulty of visual search for one shape among rotated and scaled versions of the other (<xref rid="fig1" ref-type="fig">Fig.1B</xref>) (<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c26">26</xref>). Within a grid of 9 shapes, 8 are similar and 1 is different, and participant have to click on it. Intuitively, if two shapes are very dissimilar, we expect both the response time and the error rate of finding one exemplar of one shape amongst exemplars of the other shape to be low. Conversely, we expect both to be high if shapes are similar. This gives us an empirical measure of shape dissimilarity which we can compare to the distance predicted by different models.</p>
<sec id="s2a">
<title>Results</title>
<p>The 11×11 dissimilarity matrix, estimated by aggregating response time and errors from n=330 online participants is shown in <xref rid="fig1" ref-type="fig">Fig.1D</xref>. The distance estimated as the average success rate divided by the average response time; method section for details. To better understand its similarity structure, we performed 2-dimensional ordinal Multi-Dimensional Scaling (MDS) projection (<xref rid="fig1" ref-type="fig">Fig.1C</xref>) (<xref ref-type="bibr" rid="c27">27</xref>). The projection of the 11 shapes on the first dimension showed a strong geometric regularity, with the square and the rectangle landing at the far right, rhombus and parallelogram in the middle, and less regular shapes at the far left. Thus, human perceptual similarity seemed primarily driven by geometric properties. To quantify this resemblance using that 2D MDS projection, we examined the vector which corresponded to simply counting the number of geometric regularity (number of right angles, pairs of parallel lines, pairs of equal angles and pair of sides of equal length). This vector (shown in grey in <xref rid="fig1" ref-type="fig">Fig. 1C</xref>) had a projection that was significantly different from 0 for both principal axes (both p&lt;.01).</p>
<p>Most diagnostically, we compared the full human dissimilarity matrix to those generated by two competing models of shape processing (<xref ref-type="bibr" rid="c8">8</xref>). The geometric feature model proposes that each shape is encoded by a feature vector of its discrete geometric regularities and predicts dissimilarity by counting the number of features not in common: this makes squares and rectangles very similar, but squares and irregular quadrilaterals very dissimilar. On the other hand, we operationalize our visual model by propagating shapes through a feedforward CNN model of the ventral visual pathway (we use Cornet-S, but see <xref rid="figs1" ref-type="fig">Fig.S1</xref> in Supplementary On-line Materials for other CNNs and other layers of Cornet-S, with unchanged conclusions). Shape similarity was estimated as the crossnobis distance between activation vectors in late layers (<xref ref-type="bibr" rid="c28">28</xref>). Note that these two models are not significantly correlated (r<sup>2</sup>=.04, p&gt;.05).</p>
<p>Multiple regression of the human dissimilarity matrix with the predictions of those two visual and symbolic models (<xref rid="fig1" ref-type="fig">Fig.1E</xref>) showed that both contributed to explaining perceived similarity (p<italic>s</italic>&lt;.001), but that the weight of the geometric feature model was 3.6 times larger than that of the visual model, a significant difference (p&lt;.001). This finding supports the prior proposal that the two strategies contribute to human behavior, but that the symbolic one dominates, especially in educated humans (<xref ref-type="bibr" rid="c8">8</xref>). Additional models and comparisons are presented in <xref rid="figs5" ref-type="fig">Fig. S5</xref>: in particular, we have included two distance measures based on skeletal representations (<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>), both of which perform better than chance but significantly less than the symbolic model.</p>
</sec>
</sec>
<sec id="s3">
<label>Experiment 2</label><title>fMRI Geometric Shape Localizer</title>
<p>To understand the neural underpinning of the cognition of geometric shape using fMRI, we started with the simplest foray into geometric shape perception by including geometric shapes as an additional visual category in a standard visual localizer used in the lab (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). Across short mini-blocks, this fMRI run probed whole-brain responses to geometric shapes (triangles, squares, hexagons, etc.) and to a variety of matched visual categories (faces, objects, houses, arithmetic, words and Chinese characters). In 20 adults in functional MRI (9 females; 19-37 years old, mean age 24.6), we collected three localizer runs using a fast miniblock design. To maintain attention, participants were asked to detect a rare target, which could appear in any miniblock.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 2.</label>
<caption><title>Localizing the brain systems involved in geometric shape perception</title>
<p>(<bold>A</bold>) Visual categories used in the localizer. (<bold>B</bold>) Task: In each miniblock, among a series of 6 pictures from a given category, participants had to detect a rare target star. (<bold>C</bold>) Statistical map associated with the contrast “single geometric shape &gt; faces, houses and tools”, projected on an inflated brain (top: adults; bottom: children; clusters significant at cluster-corrected p&lt;.05 with nonparametric two-tailed bootstrap test as reported in the text). (<bold>D</bold>) BOLD response amplitude (regression weights, arbitrary units) within each significant cluster with subject-specific localization. Geometric shapes activate the intraparietal sulcus (IPS) and posterior inferior temporal gyrus (pITG), while causing reduced activation in broad bilateral ventral areas compared to other stimuli; see <xref rid="figs4" ref-type="fig">Fig.S4</xref> for analysis of subject-specific ventral subregions.</p></caption>
<graphic xlink:href="584141v2_fig2rev.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s3a">
<title>Results</title>
<sec id="s3a1">
<title>Reduced activity to geometric shapes in ventral visual cortex</title>
<p>Classical ventral visual category-specific responses, for instance to faces or words, were easily replicated (<xref rid="fig2" ref-type="fig">Fig.2</xref>; <xref rid="figs2" ref-type="fig">Fig.S2</xref>). However, when contrasting geometric shapes to faces, houses or tools, we observed a massive under-activation of bilateral ventral occipito-temporal areas (unless otherwise stated, all statistics are at voxelwise p&lt;.001, clusterwise p&lt;.05 permutation-test corrected for multiple comparisons across the whole brain). All of the regions specialized for words, faces, tools or houses showed this activity reduction when presented with geometric shapes (<xref rid="fig2" ref-type="fig">Fig.2C</xref>; <xref rid="tbls1" ref-type="table">Table S1</xref>).</p>
<p>This group analysis was further supported by subject-specific analyses of ROIs specialized for various categories of images (see Appendix and <xref rid="figs4" ref-type="fig">Fig.S4</xref>). First, unsurprisingly, the FFA, which is known for its strong category-selectivity, showed the lowest responses to geometric shapes in the fusiform face area. Second, in a subject-specific ROI analysis of the visual word form area (VWFA), identified by its stronger response to alphabetical stimuli than to face, tools and houses, no activity was evoked by single shapes, or strings of three shapes, above the level of other image categories such as objects or faces. This finding eliminates the possibility that geometric shapes might have been processed similarly to letters. Similarly, one could have thought that geometric shapes would be processed together with other complex man-made objects, which are often designed to be regular and symmetrical. However, geometric shapes again yielded a low activation in individual ventral visual voxels selective to tools, thus refuting this possibility. Finally, geometric shapes could have been encoded in the parahippocampal place area (PPA), which is known to encode the geometry of scenes, including abstract ones presented as Lego blocks (<xref ref-type="bibr" rid="c31">31</xref>). However, again, geometric shapes actually induced minimal activity in individually defined PPA voxels (see <xref rid="figs4" ref-type="fig">Fig.S4</xref> for a summary).</p>
</sec>
<sec id="s3a2">
<title>Increased activity to geometric shapes in intraparietal and inferior temporal cortices</title>
<p>While the activity of the ventral visual cortex thus seemed to be globally reduced during geometric shape perception, we observed, conversely, a superior activation to geometric shapes than to face, tools and houses in only two significant positive clusters, in the right anterior intraparietal sulcus (aIPS) and posterior inferior temporal gyrus (pITG) bordering on the lateral occipital sulcus. At a lower threshold (voxel p&lt;.001 uncorrected), the symmetrical aIPS in the left hemisphere was also activated (also see SOM for additional results concerning the “3-shapes” condition and with both shape conditions together).</p>
<p>Those areas are similar to those active during number perception, arithmetic, geometric sequences, and the processing of high-level math concepts (<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>). To test this idea formally, we used the localizer to identify ROIs activated by numbers more than words. This contrast identified a left IPS cluster (p&lt;.05), while the symmetrically identified cluster in right IPS did not reach significance at the whole brain level (p=.18). In both cases, however, those two ROIs were also significantly more activated by geometric shapes than other visual categories.</p>
<p>The observed overlap with number-related areas of the IPS is compatible with our hypothesis that geometric shapes are encoded as mental expressions that combines number, length, angle and other geometric features. However, it could also reflect an association with math knowledge acquired during education. To test whether the brain activity observed in adults reflects a basic intuition of geometry which is present early on in education, we replicated our fMRI study in 22 6-year-old first graders. When comparing the single shape condition and faces, houses and tools, we observed the same reduction in ventral visual activity. We also observed greater aIPS activity, now significant in both hemispheres. Furthermore, the right pITG voxels extracted from adults also reached significance in children for the shape versus face, houses and tools. Conversely, the left aIPS voxels extracted in children reached significance in adults. Indeed, the activation profiles were quite similar in both age groups (<xref rid="fig2" ref-type="fig">Fig.2D</xref><bold>; see </bold><xref rid="figs3" ref-type="fig">Fig.S3A</xref> and <xref rid="figs3" ref-type="fig">Fig.S3B</xref> for uncorrected statistical maps of adults and children, which are quite similar). In particular, aIPS activity was strongest to geometric shapes in children, while in adults strong responses to both geometric shapes and numbers were seen, indicating an overlap with previously observed areas involved in arithmetic (<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c32">32</xref>). Our findings in six-year-olds suggest that geometric shapes activate these regions even before they become responsive to symbolic arithmetic (<xref ref-type="bibr" rid="c33">33</xref>) (p&lt;.05 for “shape &gt; numbers”, two-tailed t-test, in each of the bilateral IPS ROIs identified for shapes in children; p&gt;.05 for both ROIs in adults).</p>
<p>In sum, geometric shapes led to reduced activity in ventral visual cortex, where other categories of visual images show strong category-specific activity. Instead, geometric shapes activated mathand number-related areas, particular the right aIPS.</p>
</sec>
</sec>
</sec>
<sec id="s4">
<label>Experiment 3</label><title>fMRI of the geometric intruder task</title>
<p>In a second fMRI experiment, we measured the detailed fMRI patterns evoked by our quadrilaterals, with the goal to submit them to a representational similarity analysis and test our hypothesis of a double dissociation between regions encoding visual (CNN) versus symbolic codes. Adults and children performed an intruder task similar to our previous behavioral study (<xref ref-type="bibr" rid="c8">8</xref>), with miniblocks allowing us to evaluate the activity pattern evoked by each quadrilateral shape. To render the task doable by 6-year-olds, we tested only 6 quadrilaterals, displayed as two half-circles of three items, and merely asked participants whether the intruder was on the left or the right of the screen (<xref rid="fig3" ref-type="fig">Fig.3</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 3.</label>
<caption><title>Dissociating two neural pathways for the perception of geometric shape.</title>
<p><bold>(A) fMRI intruder task.</bold> Participants indicated the location of a deviant shape via button clicks (left or right). Deviants were generated by moving a corner by a fixed amount in four different directions. <bold>(B)</bold> Performance inside the fMRI: both populations tested displayed an increasing error rate with geometric shape complexity, which significantly correlates with previous data collected online. <bold>(C)</bold> Whole brain correlation of the BOLD signal with geometric regularity in adults, as measured by the error rate in a previous online intruder detection task (<xref ref-type="bibr" rid="c8">8</xref>). Positive correlations are shown in red and negative ones in blue. Voxel threshold p&lt;.001, cluster-corrected by permutation at p&lt;.05. Side panels show the activation in two significant ROIs whose coordinates were identified in adults and where the correlation was also found in children (one-tailed test, corrected for the number of ROIs tested this way). <bold>(D)</bold> Whole-brain searchlight-based RSA analysis in adults (same statistical thresholds). Colors indicate the model which elicited the cluster: purple for CNN encoding, orange for the geometric feature model, green for their overlap.</p></caption>
<graphic xlink:href="584141v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s4a">
<title>Results</title>
<p>Behavioral performance inside the scanner replicated the geometric regularity effect in adults and children: performance was best for squares and rectangles and decreased linearly for figures with fewer geometric regularities (<xref rid="fig3" ref-type="fig">Fig.3B</xref>). Behavior in the fMRI scanner was significantly correlated with an empirical measure of geometric regularity measured with an online intruder detection task (<xref ref-type="bibr" rid="c8">8</xref>), which is used in all following analyses whenever we refer to geometric regularity. The neural bases of this effect were studied at the whole-brain level by searching for areas whose activation varied monotonically with geometrical regularity. In adults, this contrast identified a broad bilateral dorsal network including occipito-parietal, middle frontal, anterior insula and anterior cingulate cortices, with accompanying deactivation in posterior cingulate (<xref rid="fig3" ref-type="fig">Fig.3C</xref>). This broad network, however, probably encompassed both regions involved in geometric shape encoding and those involved in the decision about the intruder task, whose difficulty increased when geometric regularity decreased. To isolate the regions specifically involved in shape coding, we performed searchlight RSA analyses, which focused on the pattern rather than the level of activation. Within spheres spanning the entire cortex, we asked in which regions the similarity matrix between the activation patterns evoked by the six shapes was predicted by the CNN encoding model, the geometric feature model, or both (<xref rid="fig3" ref-type="fig">Fig.3D</xref>; <xref rid="tbls2" ref-type="table">Table S2</xref>). In adults, the CNN encoding model predicted neural similarity in bilateral lateral occipital clusters, while the geometric feature model yielded a much large set of clusters in occipito-parietal, superior prefrontal and right dorsolateral prefrontal cortex. Calcarine cortex was also engaged, possibly due to top-down feedback (<xref ref-type="bibr" rid="c34">34</xref>). Both CNN encoding and geometric feature models had overlapping voxels in anterior cingulate and right premotor cortex, possibly reflecting the pooling of both visual codes for decision-making.</p>
<p>In children, possibly because the task difficulty was too high, few results were obtained. The correlation of brain activity with regularity at the whole-brain level yielded a single significant cluster (see <xref rid="figs3" ref-type="fig">Figure S3C</xref> for an uncorrected statistical map (voxelwise p&lt;.001), with the single significant whole-brain, cluster corrected significant cluster (p=.012) in the ventromedial prefrontal cortex). When testing the ROIs from the adults in children, after correcting for multiple comparisons across the 14 ROIs, only the posterior cingulate was significant (p=.049), though two clusters were very close to significance, both with p=.06: one positive in the left precentral gyrus (shown in <xref rid="fig3" ref-type="fig">Fig.3C</xref>), and one negative in the dorsolateral prefrontal cortex. Testing the ROIs identified in the visual localizer showed that they all exhibited a positive geometric difficulty effect in adults (all p&lt;.05), but did not reach significance in children, possibly due to excessive noise (as indicated by the much higher error rate in the task inside the scanner). In the searchlight analysis, no cluster associated to the geometric feature model reached significance at the whole-brain level (<xref rid="figs3" ref-type="fig">Fig.S3D</xref>). However, a right lateral occipital cluster was significantly captured by the CNN encoding model in children (p=.019) and its symmetrical counterpart was close to the significance threshold (p=.062) (<xref rid="figs3" ref-type="fig">Fig.S3D</xref>). This result might indicate that geometric features are not well differentiated prior to schooling. It could also reflect that children weight the symbolic strategy less, as was found in previous work (<xref ref-type="bibr" rid="c8">8</xref>); combined with difficulty of obtaining precise subject-specific activation patterns in young children, this could make the symbolic strategy harder to localize.</p>
</sec>
</sec>
<sec id="s5">
<label>Experiment 4</label><title>oddball paradigm of geometric shapes in MEG</title>
<p>The temporal resolution of fMRI does not allow to track the dynamic of mental representations over time. Furthermore, the previous fMRI experiment suffered from two limitations: we studied six quadrilaterals only, and we used an explicit intruder detection which blended the activations due to shape identification and to task decision. To overcome those issues, we replicated the experiment in adult magnetoencephalography (MEG) with three important changes; (1) all 11 quadrilaterals were studied; (2) participants were simply asked to fixate and attend to every shape, without performing any explicit task; (3) shapes were presented serially, one at a time, in miniblocks with a fixed quadrilateral shape and with rare intruders with the bottom right corner shifted by a fixed amount (<xref ref-type="bibr" rid="c8">8</xref>). This design allowed us to study the neural mechanisms of the geometric regularity effect: would the shapes be automatically encoded according to their geometric regularities, even in a passive task? And would the intruders be detected more easily among geometric regular shapes than among irregular ones, as previously found behaviorally in an active task (<xref ref-type="bibr" rid="c8">8</xref>)?</p>
<sec id="s5a">
<title>Results</title>
<p>In spite of the passive design, MEG signals revealed an automatic detection of intruders, driven by geometric regularity. We trained logistic regression decoders to classify the MEG signals at each time point following a shape as arising from a reference shape or from an intruder (see <bold>SOM</bold> and <xref rid="fig4" ref-type="fig">Fig.4A</xref>). Overall, the decoder performed above chance level, reaching a peak at 428ms, indicating the presence of brain responses specific to intruders. Crucially, although trained on all shapes together, the decoder performed better with geometrically regular shapes than with irregular shapes, indicating that oddball shapes were more easily detected within blocks of regular shapes, as previously found behaviorally (<xref ref-type="bibr" rid="c8">8</xref>). Indeed, a regression of decoding performance on geometrical regularity yielded a significant spatio-temporal cluster of activity, which first became significant around ∼160ms and peaked at 432ms (here and after, temporal clusters are purposefully reported with approximate bounds following (<xref ref-type="bibr" rid="c35">35</xref>)). A geometrical regularity effect was also seen in the latency of the outlier response (See <xref rid="fig4" ref-type="fig">Fig.4A</xref>; correlation between geometric regularity and the latency when average decoding performance first exceeded 57% correct; one-tailed t-test of each participant’s regression slope against 0: t=-1.83, p=.041) indicating that oddballs yielded both a larger and a faster response when the shapes were geometrically more regular. The same effect was found when training separate outlier decoders for each shape, thus refuting an alternative hypothesis according to which all outliers evoke identical amounts of surprise, but in different directions of neural space (<xref rid="fig4" ref-type="fig">Fig.4B</xref>). Overall, the results fully replicate prior behavioral observations of the geometric regularity effect (<xref ref-type="bibr" rid="c8">8</xref>) and suggest that the computation of a geometric code and its deviations occurs under passive instructions and starts with a latency of about ∼150ms.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 4.</label>
<caption><title>Using MEG to time the two successive neural codes for geometric shapes</title>
<p>(<bold>A,B</bold>) Performance of a classifier using MEG signals to predict whether the stimulus is a regular shape or an oddball. Left: performance for each shape; middle: correlation with geometrical regularity (same x axis as in <xref rid="fig3" ref-type="fig">figure 3C</xref>); right: visualization of the average decoding performance over the cluster. In A, training of the classifier was performed on MEG signals from all 11 shapes; In B, eleven different classifiers were trained separately, one for each shape. (<bold>C</bold>) Sensor-level temporal RSA analysis. At each time point, the 11×11 dissimilarity matrix of MEG signals was modeled by the two model RDMs in <xref rid="fig1" ref-type="fig">Fig. 1D</xref>, and the graph shows the time course of the corresponding GLM beta weights. (<bold>D</bold>) Source-level temporal-spatial searchlight RSA. Same analysis as in C, but now after reconstruction of cortical source activity.</p></caption>
<graphic xlink:href="584141v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We next used temporal RSA to further probe the dynamics of the perception of the reference, non-intruder shapes. For each time point, we estimated the 11×11 neural dissimilarity matrix across all pairs of reference shapes using sensor data with crossnobis distances (<xref ref-type="bibr" rid="c28">28</xref>), and entered them in a multiple regression with those predicted by CNN encoding and geometric feature models. The coefficients associated to each predictor are shown in <xref rid="fig4" ref-type="fig">Fig. 4C</xref>. An early cluster (observed cluster extent at approximately 0 to 300ms, p&lt;.001) showed a significant correlation of the CNN encoding model on brain similarity. Its early onset in this analysis, at ∼0 ms, is likely due to the fact that (i) a given shape was repeatedly presented within a block at a constant rate, thus permitting it to be predicted; and (ii) to increase the signal to noise ratio, the data was smoothed with a window of 100ms, and indeed performing the same analysis with no smoothing yields a very similar significant cluster from 60ms to 320ms. It was followed by a significant cluster associated with the geometric feature model (∼150-450ms, p&lt;.001). Thus, those results are compatible with our hypothesis of two distinct stages in geometric shape perception, and suggest that a symbolic encoding arises around ∼200ms.</p>
<p>To understand which brain areas generated these distinct patterns of activations, and probe whether they fit with our previous fMRI results, we performed a source reconstruction of our data, then replicated the RSA analysis with searchlights sweeping across cortical patches of 2 cm geodesic radius (<xref rid="fig4" ref-type="fig">Fig. 4D</xref>). The CNN encoding model captured brain activity in a bilateral occipital and posterior parietal cluster, while the geometric model accounted for subsequent activity starting at ∼200 ms and spanning over broader dorsal occipito-parietal and intraparietal, prefrontal, anterior and posterior cingulate cortices. This double dissociation closely paralleled fMRI results (compare <xref rid="fig4" ref-type="fig">Fig. 4D</xref> and <xref rid="fig3" ref-type="fig">Fig. 3D</xref>), with greater spatial spread due to the unavoidable imprecision of MEG source reconstruction.</p>
</sec>
</sec>
<sec id="s6">
<title>Modelling of the results with alternative models of visual perception</title>
<p>In the above analyses, we contrasted two models for our data: a CNN versus a list of geometric properties. However, there are many other models of vision. In this section we model our data with two additional classes of models, based on the shape skeleton and principal axis theory, or vision transformers neural networks.</p>
<sec id="s6a">
<title>Skeleton models</title>
<p>Skeletal representations (<xref ref-type="bibr" rid="c36">36</xref>) offer biologically plausible and computationally tractable models of object representation in human cognition. Several methods to derive skeletal representation from object contour have been put forward, including Bayesian estimation that trade off accuracy for conciseness (<xref ref-type="bibr" rid="c37">37</xref>). More recently, distance estimators between shapes based on their skeletal structure have been used to model human behavior (<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c38">38</xref>–<xref ref-type="bibr" rid="c40">40</xref>), and hierarchical object decomposition have been rigorously described and tested (<xref ref-type="bibr" rid="c41">41</xref>). Remarkably, even when simply asked to tap a geometric shape on a touchscreen anywhere they want, human taps cluster along the shape skeleton (<xref ref-type="bibr" rid="c42">42</xref>).</p>
<p>To model our data, we needed a distance metric between two shape skeletons. We explored two metrics: one that measures the shortest distance between two skeletons after optimal alignment and rotation (<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c43">43</xref>), and one that measures the difference in angle and length of matched sub-parts of the shapes’ skeleton (<xref ref-type="bibr" rid="c30">30</xref>). Our results are summarized in <xref rid="figs5" ref-type="fig">Fig. S5A</xref>; the first observation is that over our 11 quadrilaterals, these two skeletal metrics were not significantly correlated, suggesting that they capture very different properties when applied to minimal geometric shapes and are possibly not best suited to characterize them. Additionally, neither of these metrics predicted the behavioral data better than the CNN models.</p>
<p>When tested on fMRI data, the first method based on optimal alignment and rotation elicited did not elicit any significant cluster in either population. In adults, but not in children, the second method based on the structure of the skeletons significantly correlated with bilateral clusters in the visual cortex, as well as a significant cluster in the right premotor cortex (see <xref rid="figs6" ref-type="fig">Fig.S6</xref>). These areas constitute a subset of the areas identified with the CNN encoding model.</p>
<p>When tested on MEG data, the first method similarly did not elicit any significant temporal cluster at the p&lt;.05 level. The second method yielded two significant temporal clusters, the first from ∼75ms to ∼260ms and the second from ∼285ms to ∼370ms. When looking at the reconstructed sources, the second temporal cluster did not yield and significant spatial cluster, and the first cluster elicited a bilateral cluster encompassing both early occipital area and the early dorsal visual stream, with no significant localization in the frontal cortex.</p>
<p>Overall, for our dataset, these models appear partially similar to the CNN model insofar as they capture subsets of the timings and localizations of the neural data captured by the CNN model, but they do not strongly predict the behavior data as the geometric feature model does, nor capture the broad cortical networks associated with the geometric feature model.</p>
</sec>
<sec id="s6b">
<title>Vision Transformers</title>
<p>While CNN models have well-known limits in capturing several aspects of human perception (<xref ref-type="bibr" rid="c43">43</xref>, <xref ref-type="bibr" rid="c44">44</xref>), connectionist models based on the transformer architecture may provide a closer match, especially when it comes to symbolic or mathematical regularities (<xref ref-type="bibr" rid="c45">45</xref>). To test this possibility, following a similar method as with CNNs, we modeled our empirical RDMs with distance measured between pair of shapes in the late layers of a visual transformer, DINOv2 (<xref ref-type="bibr" rid="c46">46</xref>). In <xref rid="figs5" ref-type="fig">Fig. S5B</xref>, we report the empirical RDMs, the DINO RDM, as well as the CNN RDM, together with a plot of each subject’s correlation with the CNN and DINO matrices in a GLM. The DINO predictor yielded a dissimilarity matrix quite similar to the human behavioral one; indeed, the distribution of coefficients for the DINO predictor was significantly greater than 0 (95% confidence interval [0.73, 0.75]; p&lt;.001) and significantly different from the CNN predictor (p&lt;.001). Note that in this GLM, the distribution of CNN predictors is significantly lower than zero (p&lt;.001), which was not the case when contrasted with the symbolic model. This suggests that the DINO predictor may involve a mixture of symbolic-like features and perceptual features. Indeed, its correlation with the symbolic model is .78 and the correlation with the cornet is .56. This may explain why human are best predicted with a mixture of these two models, but with different weights: to achieve the best fit, a model with both DINO and CNN puts a high weight on DINO and then negatively corrects the excess perceptual features by putting a negative weight on the CNN model.</p>
<p>This analysis was confirmed by a fit of the fMRI data (<xref rid="figs6" ref-type="fig">Fig.S6</xref>) and MEG data (<xref rid="figs5" ref-type="fig">Fig.S5C</xref>): in both cases, the RSA analysis with DINO last layer yielded results that were very similar to a mixture of the CNN and the symbolic model. In fMRI in adults, a distributed set of cortical areas were significantly correlated with the DINO model, and these areas overlapped with the union of the areas predicted by both of our original models. In children, DINOv2 elicited a significant cluster in the right visual cortex, in areas overlapping with the areas activated by the CNN model in children.</p>
<p>In MEG, a large and significant cluster was predicted by the DINO model. Its timing (significant cluster from 0ms to 456ms) overlapped with the spatiotemporal clusters found with both the CNN model and the symbolic model. Source analysis indicated that a correlation with DINO originated from widespread sources that, again, overlapped with both those found with the CNN model (early occipital areas) and the symbolic model (widespread regions included dorsal, anterior temporal, and frontal sources).</p>
<p>Taken together, these results suggest that the last layer of DINO is driven by both early visual and higher-level geometric features, and that both are needed to fit human data. A systematic investigation of the impact of network architecture, dataset size and dataset content is beyond the scope of the present paper, but the present data could serve as a reference dataset with which to understand which of these factors ultimately cause a neural network to display symbolic properties close to those found in the human brain.</p>
</sec>
</sec>
<sec id="s7">
<title>Discussion</title>
<p>Our previous behavioral work suggested that the human perception of geometric shapes cannot be solely explained by current CNN encoding models, and that humans also encode them using nested combinations of discrete geometric regularities (<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c12">12</xref>). Here, we provide direct human brain imaging evidence for the existence of two distinct cortical networks with distinct representational schemes, timing, and localization: an early occipitotemporal network well modeled by a CNN, and a dorso-frontal network sensitive to geometric regularity.</p>
<sec id="s7a">
<title>Behavioral signatures of geometric regularity</title>
<p>At the behavioral level, the new large-scale data that we report here (n=330 participants) fully replicated our prior finding that human perception of geometric shape dissimilarity, as evaluated by a visual search task, is best predicted by a model that relies on exact geometric features, rather by a CNN. These exact geometric feature representations can be thought of as a more abstract and compressed representations of shapes: they replace continuous variations along certain dimensions (such as angles or directions) with categorical features (right angle or not, parallel or not). Such a representation, which remains invariant over changes in size and planar rotation, turns a rich visual stimulus into a compressed internal representation.</p>
<p>Even in educated human adults, a small proportion of the variance in behavioral dissimilarity remained predicted by the CNN model, so that both CNN and geometry predictors were significant in a multiple regression of human judgments. Previously, we only found such a significant mixture of predictors in uneducated humans (whether French preschoolers or adults from the Himba community) (<xref ref-type="bibr" rid="c8">8</xref>). It is likely that the greater power afforded by the present experiment yielded a greater sensitivity. This finding comforts the hypothesis that two strategies for geometric shape perception are available to humans: one based on hierarchical visual processing (captured by the CNN), and the other based on an analysis of discrete geometric features – the latter dominating strongly in educated adults.</p>
</sec>
<sec id="s7b">
<title>fMRI results</title>
<p>Using fMRI, we found that the mere perception of geometric shapes, compared to various other visual categories, yields a reduced bilateral activation of the entire ventral visual pathway, together with a localized increased activation in the anterior IPS and in the posterior ITG. Furthermore, geometric regularity modulated fMRI activation in most of the bilateral occipito-parietal pathway, middle frontal gyrus and anterior insula. Finally, the most diagnostic result we found is that the representational similarity matrix in these regions was predicted by geometric similarity rather than by the CNN (<xref rid="fig3" ref-type="fig">figure 3</xref>).</p>
<p>The IPS areas activated by geometric shapes overlap with those active during the comprehension of elementary as well as advanced mathematical concepts (<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>). This finding agrees with their proposed involvement in an abstract, modality-independent symbolic encoding. Further support for this idea comes from the fact that these regions can be equally activated in sighted and in blind individuals when they perform mathematics (<xref ref-type="bibr" rid="c47">47</xref>, <xref ref-type="bibr" rid="c48">48</xref>) or just evaluate the shapes of manmade objects (<xref ref-type="bibr" rid="c49">49</xref>). Thus, the representation of shapes computed in these regions is more abstract and amodal than the hierarchy of visual filters that is thought to capture ventral visual shape identity processing.</p>
</sec>
<sec id="s7c">
<title>MEG results</title>
<p>Using MEG, we found that even during the passive perception of simple quadrilateral shapes, participants are sensitive to their geometric regularity: occasional oddball shapes elicited greater violation-of-expectation signals within blocks of regular shapes than within blocks of irregular shapes. Additionally, using RSA and source reconstruction, we observed a spatial and temporal dissociation in the processing of quadrilateral shapes: an early occipital response, well predicted by CNN models of object recognition, was followed by broadly distributed cortical activity that correlated with a model of exact geometric features. Despite the limited accuracy afforded by source reconstruction in MEG, there was considerable overlap between our RSA analysis in fMRI and in MEG.</p>
<p>This early response of occipital areas, followed by a later activation of a broad dorso-frontal network, may seem at odd with some results showing that the dorsal pathway responds faster than the ventral pathway (<xref ref-type="bibr" rid="c50">50</xref>, <xref ref-type="bibr" rid="c51">51</xref>). However, in this work, the dorsal network that we probed does not solely compute elementary features of location or motor affordance, but high-level geometrical and mathematical information of an abstract categorical nature. It seems logical that such high-level features rely on prior information processing within the occipito-temporal visual pathway, and therefore only activate thereafter.</p>
</sec>
<sec id="s7d">
<title>Development of geometry representations</title>
<p>Interestingly, the IPS and posterior ITG activations to geometric shapes were consistently observed at very similar locations in adult and 6-year-olds. Furthermore, the overlap with a math-responsive network was present in both age groups, with ROIs that respond to number more than words also responding to geometric shapes more than other visual categories. This finding fits with previous evidence of an early responsivity of the IPS to numbers and arithmetic in young children prior to formal schooling (<xref ref-type="bibr" rid="c52">52</xref>–<xref ref-type="bibr" rid="c54">54</xref>). In agreement with the existence of a behavioral geometric regularity effect in preschoolers (<xref ref-type="bibr" rid="c8">8</xref>), we hypothesize that an intuitive encoding of number and geometric features precede schooling and possibly guide the subsequent acquisition of formal mathematics (<xref ref-type="bibr" rid="c55">55</xref>).</p>
<p>On the other hand, the fMRI results from the intruder task were much less stable in children, and neither the geometric regularity effect nor the geometry-based RSA analysis yielded strong results at the whole-brain level (only one ventromedial cluster was associated with the regularity effect, see <xref rid="figs3" ref-type="fig">Figure S3</xref>). In previous work, we have shown that besides an overall decrease in performance between adults and first graders, the profile of behavior across different shapes is partially distinct in preschoolers: in them, both the CNN and the symbolic model are on par in predicting behavior, suggesting that unlike adults, children frequently mix the two strategies for identifying geometric shapes. Such a mixture of strategies could explain why the correlation with our empirical estimation of complexity, based on adult data, yielded a weaker and more noisy correlation in children. The RSA analysis is compatible with this: in the group analysis, while the CNN predictor was significant in the visual cortex of children, the symbolic geometry model did not reach significance anywhere. Future research with a larger number of participants could attempt to sort out children as a function of whether their behavior is dominated by geometric features or by the CNN model, and then examine how their brain activity profiles differ.</p>
</sec>
<sec id="s7e">
<title>Reduced activation of the ventral visual pathway</title>
<p>Strikingly, our fMRI data also evidenced a hypo-activation of the ventral visual pathways to geometric shapes relative to other pictures. A large bilateral cluster of reduced activity was found when comparing shapes versus other visual categories, and a similar reduced activity was found in each of four visual areas specialized for various visual categories (faces, words, tools, and places). This reduction is compatible with our hypothesis that geometric shapes are processed differently from other pictures, and with our empirical finding that CNNs, which usually provide a relatively good fit to inferotemporal cortex activity (<xref ref-type="bibr" rid="c56">56</xref>–<xref ref-type="bibr" rid="c59">59</xref>), did not provide a good model of geometric shape perception. Because our shapes consisted of a few straight lines, it is likely that the computations performed by the ventral pathways were minimally solicited, giving rise to a lower activation when compared to more complex visual stimuli such as pictures of faces or houses.</p>
</sec>
<sec id="s7f">
<title>Two Visual Pathways</title>
<p>Brain-imaging studies of the ventral visual pathway indicate that it is subdivided into patches responding to different visual categories such as faces, tools or houses, with a partial correspondence between human and non-human primates (<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c60">60</xref>–<xref ref-type="bibr" rid="c65">65</xref>). Converging evidence from behavior (<xref ref-type="bibr" rid="c66">66</xref>) and neuroimaging (<xref ref-type="bibr" rid="c67">67</xref>–<xref ref-type="bibr" rid="c70">70</xref>) has underlined the central role of the shape of objects, contour and texture in object recognition. A basic complementarity has been identified between the ventral visual pathway, crucial for visual identification, and the dorsal occipito-parietal route, extracting visuo-spatial information about orientation and motor affordances.</p>
<p>Recent work, however, challenges the idea that the global shape of objects is computed in the ventral pathway, and instead suggests that the dorsal pathway, and in particular the IPS, may compute global shape information and then propagate it to the ventral pathway for further processing (<xref ref-type="bibr" rid="c71">71</xref>). Our results are compatible with this new look at dorsal/ventral interactions: in our fMRI localizer, geometric shapes elicited reduced ventral and increased dorsal activation when compared to other visual categories. Geometric shapes could be considered as conveying very pure information about global shape, entirely driven by contour geometry, and fully devoid of other information such as texture, shading, or curvature. More research is needed to understand the dynamic collaboration between the ventral and dorsal streams during shape recognition, but the present results, as well as the arguments put forward in (<xref ref-type="bibr" rid="c71">71</xref>) concur to suggest that shape identification does not solely rely on the ventral visual pathway.</p>
</sec>
<sec id="s7g">
<title>Lateral Occipital Cortex</title>
<p>Of special relevance to this work is the role of the Lateral Occipital Cortex (LOC) in shape perception (<xref ref-type="bibr" rid="c72">72</xref>). The posterior ITG activation that we observed lied just anterior to the LOC and possibly overlapped with it. The LOC has been repeatedly associated with shape processing (<xref ref-type="bibr" rid="c73">73</xref>–<xref ref-type="bibr" rid="c76">76</xref>), and our work converges to suggest that this region and the cortex just anterior to it play a key role in the perception of simple shapes . However, unlike ours, previous work focused primarily on irregular potatoe-like shapes or objects and their contours. Furthermore, object representations in LOC were shown to be stable across depth changes while other visual properties that can be traced across depth do not rely on the LOC (<xref ref-type="bibr" rid="c72">72</xref>), and the LOC has been shown to be invariant to 2D and 3D rotations (<xref ref-type="bibr" rid="c77">77</xref>).</p>
<p>Object selectivity in the LOC is already present in early infancy, with a sensitivity to shape and not texture already observed at 6-months of age (<xref ref-type="bibr" rid="c78">78</xref>). At age 5-10, size invariance, but not viewpoint invariance, has been established (<xref ref-type="bibr" rid="c79">79</xref>). However, to our knowledge, no study has targeted geometrically regular shapes, and in particular how changes in regularity impacts LOC activity. While some of our quadrilateral stimuli can be constructed as 3D projections of one another (either by perspective or by orthogonal projection), this should actually make them more similar within LOC given its object viewpoint invariance. Further work should use within-subject LOC localizers such as objects minus scrambled-objects to establish the exact cortical relation between the present geometric regularity effect and the classical LO region. We speculate that the more anterior part of lateral occipito-temporal cortex may extract more abstract geometric descriptors of shapes, as also suggested by its sensitivity to mathematical training (<xref ref-type="bibr" rid="c20">20</xref>).</p>
</sec>
<sec id="s7h">
<title>Artificial Neural Networks: limits and promising directions</title>
<p>While CNNs are predictive of early ventral visual activity, the present work adds to a growing list of their limits as full models of visual perception (<xref ref-type="bibr" rid="c43">43</xref>, <xref ref-type="bibr" rid="c44">44</xref>). CNNs have been shown to fail to model many visual illusions (<xref ref-type="bibr" rid="c43">43</xref>, <xref ref-type="bibr" rid="c44">44</xref>), geometrically impossible objects (<xref ref-type="bibr" rid="c80">80</xref>), global shape, part-whole relations and other Gestalt properties (<xref ref-type="bibr" rid="c44">44</xref>). These limitations suggest the need to supplement current models not only by increasing their size (<xref ref-type="bibr" rid="c56">56</xref>) but also by changing their architecture in ways that better incorporate findings from psychology and neuroscience (<xref ref-type="bibr" rid="c81">81</xref>, <xref ref-type="bibr" rid="c82">82</xref>). Indeed, connectionist model based on the transformer architecture may begin to capture human geometric perception (<xref ref-type="bibr" rid="c45">45</xref>), and we found that the results generated by our symbolic geometry model could also be captured by the late layers of visual transformer DINOv2 (<xref ref-type="bibr" rid="c46">46</xref>) as shown in <xref rid="figs5" ref-type="fig">Fig. S5</xref>. And <xref rid="figs6" ref-type="fig">Fig. S6</xref>. This finding offers an exciting avenue to analyze a mechanistic implementation of symbol-like representations in connectionist architectures, in a paradigm which is simple enough to be tested in a wide array of models and species.</p>
</sec>
<sec id="s7i">
<title>Shape Skeleton and Principal Axis</title>
<p>Skeletal representations have been proposed as human-like representations of visual shapes, particularly appropriate for biological shapes such as animals or plants (<xref ref-type="bibr" rid="c36">36</xref>). Even for simple geometric shapes, skeletal representation may be automatically and unconsciously computed (<xref ref-type="bibr" rid="c42">42</xref>). However, in the present work, the two skeletal shape representations we tested did not model human data well (see <xref rid="figs5" ref-type="fig">Fig. S5</xref> and <xref rid="figs6" ref-type="fig">Fig. S6</xref>). For our quadrilaterals, which are visually similar to one another, all but the square have the same skeleton topological graph, with only variations in the length and angles of the skeletal segments. Thus, a skeletal representation is probably not the source of the large geometric regularity effect that we observed here and in past work (<xref ref-type="bibr" rid="c8">8</xref>).</p>
<p>As previously argued (Sablé-Meyer et al., 2022), geometric and medial axis theories need not be seen as incompatible. Rather, we conceive of them as complementary codes, best suited for distinct shape domains. Even after extraction of a figure’s skeleton, further compression could be achieved by encoding its geometric regularity, and this may be what humans do when they draw a snake, for instance, as a geometrically regular zigzag.</p>
</sec>
<sec id="s7j">
<title>Neurophysiological implementation of geometry</title>
<p>How could the postulated geometric code be implemented neurophysiologically? There are several challenges. First, some neurons should encode individual features such as lines and curves, or features formed by their relationships (e.g. parallelism, right-angle). Second, such codes should be discrete and categorical, forming sharp boundaries between, say, parallel versus non-parallel lines. Third, they should enter into compositional expressions describing how individual features combine into the whole shape (e.g. “a shape with 4 equal sides and 4 equal right-angles”).</p>
<p>The first point has been studied in both humans and monkeys in the context of research on the non-accidental properties (NAPs) of objects. NAPs are qualitative features of object shapes, such as straight versus curved, which remain invariant when an object is rotated. Metric properties (MPs), on the other hand, are quantitative properties such as amount of curvature that do not exhibit such invariance. Behavioral research has demonstrated that, equivalent amounts of pixel change in the image, changes in NAPs are more discriminable than changes in MPs, in educated and uneducated human adults, toddlers and even in infants (<xref ref-type="bibr" rid="c83">83</xref>–<xref ref-type="bibr" rid="c85">85</xref>). Furthermore, the firing of neurons in monkey infero-temporal cortex is more sensitive to changes in NAPs than in MPs, whether they are conveyed by 3D shapes (<xref ref-type="bibr" rid="c86">86</xref>) or 2D shapes (<xref ref-type="bibr" rid="c87">87</xref>, <xref ref-type="bibr" rid="c88">88</xref>). These findings occurred even in the absence of a task other than passive fixation, and without particular training for the stimuli. Further work with 2D shapes, including triangles and quadrilaterals partially overlapping with the present stimuli, showed that IT cortex neurons could also be sensitive to more global shape properties such as axis curvature or “taper” (the difference between a rectangle seen upfront or at a slanted axis) (<xref ref-type="bibr" rid="c87">87</xref>). Multidimensional scaling of neuronal population responses organized the tested shapes in a systematic “shape space” where, for instance, the rectangle occupies a extreme corner, thus making it distinct from its curved or tapered variants (<italic>87</italic>, figure 5).</p>
<p>Thus, monkey IT neurons may provide some of the basic features such as straight line or parallelism that are needed to account for the human encoding of geometric shapes. However, human shape similarity judgments are more discrete and categorical than predicted by either monkey IT neural populations or CNNs (<xref ref-type="bibr" rid="c88">88</xref>, <xref ref-type="bibr" rid="c89">89</xref>). The tuning of most IT neurons does not exhibit discrete changes, but is monotonic with continuous changes in shape space, with the strongest firing rates associated to either extreme of the continuum. Monotonic coding of shape variations is also true of other domains such as face perception (e.g. <italic>90</italic>). For instance, Suzuki and Tanaka (<xref ref-type="bibr" rid="c91">91</xref>) trained macaques for fine-grained object discrimination along small changes in continuous image dimensions (morphed animal pictures), and found neurons in IT that responded monotonically along these shape dimensions, with maximal firing rates at one extreme or the other. With categorical training, neurons in primate prefrontal, temporal and parietal cortices do become sharply tuned to categorical boundaries within a continuum of stimuli (<xref ref-type="bibr" rid="c92">92</xref>–<xref ref-type="bibr" rid="c94">94</xref>). Thus, prefrontal and parietal neurons may play a top-down role in imposing a task-dependent categorization to a continuum of shapes (<xref ref-type="bibr" rid="c95">95</xref>).</p>
<p>While these previous findings may provide a neurophysiological basis for the fMRI and MEG responses observed here, note that humans do not need to be extensively trained to understand the differences between geometric shapes (<xref ref-type="bibr" rid="c55">55</xref>, <xref ref-type="bibr" rid="c96">96</xref>), and spontaneously impose geometric categories such as “parallel” or “right angle” to a continuum of angles between lines (<xref ref-type="bibr" rid="c97">97</xref>). Several behavioral findings suggest that a distinct neural code for geometry may exist in humans. Non-human primates perform poorly on a broad variety of perceptual and production tasks with geometric shapes: baboons do not exhibit a human-like geometric complexity effect (<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>); chimpanzees do not transfer learning of visual categories from concrete pictures to geometric line drawings (<xref ref-type="bibr" rid="c6">6</xref>); and chimpanzees behave very differently from children in free-drawing experiments where they have to complete partial line-drawings of faces (<xref ref-type="bibr" rid="c9">9</xref>). These findings suggests that an understanding of the mechanisms that underlie the human coding of geometric shapes may ultimately shed light on the cognitive and neural singularity of the human brain (<xref ref-type="bibr" rid="c7">7</xref>).</p>
<p>In the future, replicating the present experiments with monkey fMRI and electrophysiology is therefore an important goal. The methodology could also be extended to the perception of a broader set of geometric patterns (circles, spirals, crosses, zigzags, plaids, etc) which recur since prehistory and in the drawings of children and adults of various cultures, thus testing whether they too originate in a minimal and universal “language of thought” (<xref ref-type="bibr" rid="c12">12</xref>).</p>
</sec>
</sec>
<sec id="s8">
<title>Method</title>
<sec id="s8a">
<title>Experiment 1</title>
<sec id="s8a1">
<title>Participants</title>
<p>330 participants (142 Females, 177 Males, 11 Others; mean age 51.1 years, SD=16.8) were recruited via a link provided on a New York Time article (available at <ext-link ext-link-type="uri" xlink:href="https://www.nytimes.com/2022/03/22/science/geometry-math-brain-primates.html">https://www.nytimes.com/2022/03/22/science/geometry-math-brain-primates.html</ext-link>) which reported previous research from the lab and featured a link to our new online experiment. When participants clicked the link, they landed on a page with our usual procedure for online experiments, including informed consent and demographic questions. No personal identity information was collected.</p>
</sec>
<sec id="s8a2">
<title>Task</title>
<p>On each trial, participants were shown a 3×3 square grid of shapes, eight of which were copies of the same shape up to rotation and scaling, and one of which was a different shape. Participants were asked to detect the intruder shape by clicking on it. Auditory feedback was provided in the form of tones of ascending or descending pitch, as well as coloring of the shapes (the intruder was always colored in green indicating what the right answer was, and in case of an erroneous choice, the chosen shape was colored in red indicating a wrong answer).</p>
</sec>
<sec id="s8a3">
<title>Stimuli</title>
<p>Shapes design followed previous work exactly (<xref ref-type="bibr" rid="c8">8</xref>): eleven quadrilaterals with a varying number of geometric features, matched for average pairwise distance of all vertices and length of the bottom side. Shapes were presented in pure screen white on pure screen black. Each shape was differently scaled and rotated by sampling nine values without replacement in the following scaling factors [0.85, 0.88, 0.92, 0.95, 0.98, 1.02, 1.05, 1.08, 1.12, 1.15] and rotation angles [−25°, −19.4°, −13.8°, −8.3°, −2.7°, 2.7°, 8.3°, 13.8°, 19.4°, 25°]. Note that the rotation angles were centered on 0° but excluded this value so that the sides of the shapes were never strictly vertical or horizontal. Participants performed 110 trials (11×10), one for each pair of different reference and intruder shapes. The order of trials was randomized, subject to the constraint that no two identical reference shapes were used on consecutive trials, and that the outlier of a trial was always different from the reference shape of the previous trial. Two examples of trials are shown in <xref rid="fig1" ref-type="fig">Fig.1B</xref>.</p>
</sec>
<sec id="s8a4">
<title>Procedure</title>
<p>The experimental procedure started with instructions, followed by a series of questions: device used (mouse or touch-screen), country of origin, gender, age, highest degree obtained. Participants then provided subjective self-evaluation assessments, with answers on a Likert scale from 1 to 10, for the following items: current skills in mathematics; current skills in first language. Finally, participants performed the task. The instructions text was the following: “The game is very simple: you will see sets of shapes on your screen. Apart from small rotation and scaling differences, they will be identical, except for one intruder. Your task is to respond as fast and accurately as you can about the location of the intruder by clicking on it. The difficulty will vary, but you always have to answer.”</p>
</sec>
<sec id="s8a5">
<title>Estimation of empirical representational dissimilarity</title>
<p>To estimate the representational dissimilarity across shapes, first we estimated the dissimilarity between two shapes as the average success rate divided by the average response time. Indeed, if two shapes are very dissimilar, we expect participants to make few mistakes (high success rate) and find the intruder fast (low response time), yielding a high value, and vice versa. Because we didn’t have predictions using the asymmetry in visual search (e.g. finding a square within rectangles versus a rectangle versus squares), we then averaged over these paired conditions, thereby turning the square dissimilarity matrix into a triangular dissimilarity matrix. Finally, we z-scored these dissimilarity estimates.</p>
<p>As participants performed a single trial per pair of shapes, the estimation of the dissimilarity is noisy at the single participant level (either 0 or 1/RT depending on whether they answered correctly). We kept this estimate for analyses at the single participant level or mixed-effect analysis; however, for analyses that required a single RDM estimate across participants, we pooled the data from participants to estimate the average success rates and response times, hence before estimating the empirical RDMS, rather than estimate one RDM per participant and then averaging.</p>
</sec>
<sec id="s8a6">
<title>Comparison with Model RDMs</title>
<p>To obtain theoretical representational dissimilarity matrices (RDMs) with which to compare the present behavioral data, as well as subsequent brain-imaging data, we proceeded as follows. For the CNN encoding model, we downloaded from GitHub the weights for several neural networks (CORnet (<xref ref-type="bibr" rid="c98">98</xref>), ResNet (<xref ref-type="bibr" rid="c99">99</xref>) and DenseNet (<xref ref-type="bibr" rid="c100">100</xref>) ; all high scoring on brain-score (<xref ref-type="bibr" rid="c58">58</xref>)), all pre-trained with ImageNet and not specifically trained for our task. We extracted the activation vectors in each hidden layer associated to each shape with the 6×6=36 different orientations and scaling used in the experiment. For each shape, we separated their 36 exemplars randomly into two group to have independent estimation of the representation vectors from the network, and used the cross-validated Mahalanobis distance between these two splits to estimate the distance between each pair of shapes. This provides us with an RDM that captures how dissimilar shapes are according to their internal representations in a CNN of object recognition. Unless specified otherwise (see supplementary text), we report correlation with CORnet layer IT following (<xref ref-type="bibr" rid="c8">8</xref>).</p>
<p>For the geometric feature model, we estimate a feature vector of geometric features for each shape. This feature vector includes information about (i) right angles (one for each angle, 4 features); (ii) angle equality (one for each pair of angles, 6 features); (iii) side length equality (one for each pair of sides, 6 features); and (iv) side parallelism (one for each pair of sides, 6 features); all of this was done up to a tolerance level (e.g. an angle slightly off a right angle still counts as a right angle), using the tolerance value of 12,5% which was fitted previously to independent behavioral data (<xref ref-type="bibr" rid="c8">8</xref>). The dissimilarity between each pair of shapes was the difference between the number of features that each shape possesses: because both the square and the rectangle share many features, they are similar. Two very irregular shapes also end up similar as well. Conversely, the square and an irregular shape end up very dissimilar.</p>
</sec>
</sec>
<sec id="s8b">
<title>Experiment 2</title>
<sec id="s8b1">
<title>Participants</title>
<p>Twenty healthy French adults (9 females; 19-37 years old, mean age = 24.6 years old, SD: 5.2 years old) and 25 French first graders (13 females; all 6 years old) participated in the study. Three children quit the experiment before any task began because they did not like the MRI noise or lying in the confined space for the scanner. One child completed the localizer task but not the other tasks. One child was missing a single run from the intruder task. All participants had normal hearing, normal or corrected-to-normal vision, and no known neurological deficit. All adults and guardians of children provided informed consent, and adult participants were compensated for their participation.</p>
</sec>
<sec id="s8b2">
<title>Task</title>
<p>In three localizer runs, children and adult participants were exposed to eight different image categories, such that single geometric shapes could be compared to matched displays of faces, houses and tools, and rows of three geometric shapes could be compared to matched displays of numbers, French words, and Chinese characters. To maintain attention, participants were asked to keep fixating on a green central fixation dot (radius=8pixels, RGB color=26, 167, 19, always shown on the screen), and to press a button with their right-hand whenever a star symbol was presented. The star spanned roughly the same visual angle as the stimuli from the eight categories, and appeared randomly once in one of the two blocks per category (8 target stars total), between the 3rd to the 6th stimuli within that block. As feedback, a 300 ms 650 Hz beep sound was provided after each button press.</p>
</sec>
<sec id="s8b3">
<title>Stimuli</title>
<p>In each miniblock, participants saw a series of 6 grayscale images, one per second, belonging to one of eight different categories: faces, houses, tools, numbers, French words, Chinese characters, single geometric shapes, and rows of three geometric shapes. Each category comprised 20 exemplars. All faces, 16 houses, and 18 tools had been used in previous localizer experiments (<xref ref-type="bibr" rid="c101">101</xref>). For face stimuli, front-view neutral faces (20 identities, 10 males) were selected from the Karolinska Directed Emotional Faces database (<xref ref-type="bibr" rid="c102">102</xref>). The stimuli were aligned by the eyes and the iris distances. A circular mask was applied to exclude the hair and clothing below the neck. House and tool stimuli were royalty-free images obtained from the internet. House stimuli were photos of 2 to 3-story residence houses.</p>
<p>Tool stimuli were photos of daily hand-held tools: half of the images were horizontally flipped, so that there were 10 images in a position graspable for the left and right hand respectively. For French word stimuli, 3-letter French words were selected which were known to first graders and had high occurrence frequencies (range=7-2146 occurrences per million, mean=302, SD=505, based on Lexique, <ext-link ext-link-type="uri" xlink:href="http://www.lexique.org/">http://www.lexique.org/</ext-link>). Chinese characters were selected from the school textbook of Chinese first graders. Chinese word frequency (range=11-1945 occurrences per million, mean=326, SD=451 (<xref ref-type="bibr" rid="c103">103</xref>)) was not significantly different from French words used here (t(38)=.2, p=.87). Single-digit formula stimuli were 3-character simple operations in the form of “x+y” or “x-y” with x greater than y, x ranging from 2 to 5, and y from 1 to 4. Single shapes consisted in a single, centered outline of a geometrical shape (diamond, hexagon, rhombus, parallelogram, rectangle, square, trapezoid, isosceles triangle, equilateral triangle and right triangle), and were matched in luminance, contrast, and visual angle to the faces/houses/tools/words/Chinese characters which also displayed single objects. A row of shapes consisted of three different shapes side by side, whose total width, size, and line width were matched with 3-letter French words and 3-character single-digit operations. To match the appearance of the monospaced font in previous work (<xref ref-type="bibr" rid="c104">104</xref>), the monospaced font Consolas was used for the French words and numbers, with identical font weight 900. The font for Chinese characters was Heiti, which looks similar to Consolas. Random font size (uniform in 35-55px font size) were repeatedly sampled until text stimuli achieved similar variability as with the other categories.</p>
<p>The stimuli were embedded in a gray circle (RGB color=157, 157, 157, radius=155 pixels), on the screen with a black background. Within the gray circle, the mean luminance and contrast of the 8 stimuli categories did not differ significantly (luminance: F(7,152)=.6, p=.749; contrast: F(7,152)=1.2, p=.317), see <xref rid="figs2" ref-type="fig">Fig.S2</xref></p>
</sec>
<sec id="s8b4">
<title>Procedure</title>
<p>The eight categories were presented in distinct blocks of 6s each, fully randomized for block presentation order, with the restriction that there were no consecutive blocks from the same category. Each miniblock comprised 6 stimuli, in random order. Each stimulus was presented for 1s, with no interval in between (<xref ref-type="bibr" rid="c105">105</xref>). The inter-block interval duration was jittered (4, 6, or 8s; mean = 6s). Each of the eight-block types appeared twice within each run. A 6s fixation period was included at both the beginning and the end of the run. Each run lasted for 3min 24s, and participants performed three such runs during the fMRI session.</p>
</sec>
</sec>
<sec id="s8c">
<title>Experiment 3</title>
<sec id="s8c1">
<title>Participants</title>
<p>Identical to experiment 2</p>
</sec>
<sec id="s8c2">
<title>Procedure and Stimuli</title>
<sec id="s8c2a">
<title>Task</title>
<p>We adapted the geometric intruder detection task (<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c106">106</xref>) to the fMRI scanner. On each trial, participants (children and adults) saw an array of 6 shapes around fixation (3 on the right, and 3 on the left; see <xref rid="fig3" ref-type="fig">Fig.3A</xref>). Five shapes were identical except for a small amount of random rotation and scaling, while one was a deviant shape. Because the pointing task used in (<xref ref-type="bibr" rid="c8">8</xref>) was not possible in the limited space of the fMRI scanner, participants were merely asked to click a button with their left or right hand, thereby indicating on which side they thought the deviant was. Note that participants responded on every trial, the side of the correct response was counterbalanced within each shape, and we verified that the average motor response side was unconfounded with geometric shape or complexity. After each answer, auditory feedback was provided with a tone of high, increasing pitch when the answer was correct, and a low-pitch tone otherwise.</p>
</sec>
<sec id="s8c2b">
<title>Stimuli</title>
<p>Geometric shapes were generated following the procedure described in previous work (<xref ref-type="bibr" rid="c8">8</xref>): to fit the experiment within the time constraints of children fMRI, a subset of shapes was used, comprising square, rectangle, isosceles trapezoid, rhombus, right hinge and irregular shapes to span the range of complexity found in (<xref ref-type="bibr" rid="c8">8</xref>). Following previous work (<xref ref-type="bibr" rid="c8">8</xref>), deviants were generated by displacing the bottom right corner by a constant distance in four possible positions (see <xref rid="fig3" ref-type="fig">Fig.3A</xref>). That distance was a fraction of the average distance between all pairs of points, which was standardized across shapes (45% change). On each trial, six gray-on-black shapes were shown (shape color rgb values: 127,127,127). Shapes were displayed along two semicircles: the positions were determined by positioning the three leftmost (resp. rightmost) shapes on the left side (resp. right side) of a circle of radius 120px, at angles 0, pi/2 and pi, and then shifting them 100px to the left (resp. right). The rotation and scaling of each shape were randomized so that no two shapes had the same scaling or rotation factor, and values were sampled in [0.875, 0.925, 0.975, 1.025, 1.075, 1.125] for scaling and [−25°, −15°, −5°, 5°, 15°, 25°] for rotations, avoiding 0° to prevent alignment of specific shapes with screen borders. One of the shapes was an outlier, whose position was sampled uniformly in all six possible positions such that no two consecutive trials featured outliers in the same position. Outliers were sampled uniformly from the four possible types of outliers, so that all outlier types occurred as often, but no two consecutive trials featured identical outlier types.</p>
</sec>
<sec id="s8c2c">
<title>Procedure</title>
<p>The six shapes were presented in miniblocks, in randomized order, with no two consecutive blocks with the same type of shape. Each block comprised 5 consecutive trials with an identical base shape, each with 2s of stimulus presentation and 2s of fixation. There was a 4s, 6s or 8s delay between blocks. A central green fixation cross was always on display, and it turned bold 600 ms before a block would start. Each run of the outlier detection task lasted 3m40s.</p>
</sec>
</sec>
<sec id="s8c3">
<title>fMRI methods common to experiments 2 and 3</title>
<sec id="s8c3a">
<title>MRI Acquisition Parameters</title>
<p>MRI acquisition was performed on a 3T scanner (Siemens, Tim Trio), equipped with a 64-channel head coil. Exactly 113 functional scans covering the whole brain were acquired for each localizer run, as well as on 179 functional scans covering the whole brain for each run of the geometry task. All functional scans were using a T2*-weighted gradient echo-planar imaging sequence (69 interleaved slices, TR = 1.81 s, TE = 30.4 ms, voxel size = 2×2×2mm, multiband factor = 3, flip angle = 71 degrees, phase encoding direction: posterior to anterior). To reconstruct accurate anatomical details, a 3D T1-weighted structural image was also acquired (TR = 2.30 s, TE = 2.98 ms, voxel size = 1×1×1mm, flip angle = 9 degrees). To estimate distortions, two spin-echo field maps with opposite phase encoding directions were acquired: one volume in the anterior-to-posterior direction (AP) and one volume in the other direction (PA). Each fMRI session lasted for around 50min for children including (by order) 3 runs of a task not discussed here, 3 Category localizer runs, T1 collection, and 2 Geometry runs. For adults, the session lasted for around 1h 20min because they took the same runs as for children as well as an additional harder version of the geometry task. This version, which involved smaller deviant distances and a stimulus presentation duration of only 200 ms, turned out to be too difficult. While the overall performance still shows a correlation with complexity (r²=.63, p&lt;.02), it was entirely driven by two shapes, the square and the rectangle: other shapes were equally hard, and although they were better than chance they did not correlate with complexity (r<sup>2</sup>=.35,p=.22) while the correlations remained for the simpler condition.</p>
</sec>
<sec id="s8c3b">
<title>Data analysis</title>
<p>Preprocessing was performed with the standard pipeline fMRIPrep. Results included in this manuscript come from preprocessing performed using fMRIPrep 20.0.5 (<xref ref-type="bibr" rid="c107">107</xref>, <xref ref-type="bibr" rid="c108">108</xref>), which is based on Nipype 1.4.2 (<xref ref-type="bibr" rid="c109">109</xref>, <xref ref-type="bibr" rid="c110">110</xref>), and generated the following detailed method description.</p>
</sec>
<sec id="s8c3c">
<title>Anatomical Data</title>
<p>The T1-weighted (T1w) image was corrected for intensity non-uniformity (INU) with N4BiasFieldCorrection (<xref ref-type="bibr" rid="c111">111</xref>), distributed with ANTs 2.2.0 (<xref ref-type="bibr" rid="c112">112</xref>), and used as T1w-reference throughout the workflow. The T1w-reference was then skull-stripped with a Nipype implementation of the antsBrainExtraction.sh workflow (from ANTs), using OASIS30ANTs as target template. Brain tissue segmentation of cerebrospinal fluid (CSF), white-matter (WM) and gray-matter (GM) was performed on the brain-extracted T1w using fast (<xref ref-type="bibr" rid="c113">113</xref>). Brain surfaces were reconstructed using recon-all (<xref ref-type="bibr" rid="c114">114</xref>), and the brain mask estimated previously was refined with a custom variation of the method to reconcile ANTs-derived and FreeSurfer-derived segmentations of the cortical gray-matter of Mindboggle (<xref ref-type="bibr" rid="c115">115</xref>). Volume-based spatial normalization to two standard spaces (MNI152NLin6Asym, MNI152Nlin2009cAsym) was performed through nonlinear registration with antsRegistration (ANTs 2.2.0), using brain-extracted versions of both T1w reference and the T1w template. the following templates were selected for spatial normalization: FSL’s MNI ICBM 152 non-linear 6<sup>th</sup> Generation Asymmetric Average Brain Stereotaxic Registration Model (<xref ref-type="bibr" rid="c116">116</xref>) and ICBM 152 Nonlinear Asymmetrical template version 2009c (<xref ref-type="bibr" rid="c117">117</xref>).</p>
</sec>
<sec id="s8c3d">
<title>Functional Data</title>
<p>For each of the 10 BOLD EPI runs found per subject (across all tasks and sessions), the following preprocessing was performed. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. Susceptibility distortion correction (SDC) was omitted. The BOLD reference was then co-registered to the T1w reference using bbregister (FreeSurfer) which implements boundary-based registration (<xref ref-type="bibr" rid="c118">118</xref>). Co-registration was configured with six degrees of freedom. Head-motion parameters with respect to the BOLD reference (transformation matrices, and six corresponding rotation and translation parameters) are estimated before any spatiotemporal filtering using mcflirt (<xref ref-type="bibr" rid="c119">119</xref>). BOLD runs were slice-time corrected using 3dTshift from AFNI 20160207 (<xref ref-type="bibr" rid="c120">120</xref>). The BOLD time-series (including slice-timing correction when applied) were resampled onto their original, native space by applying the transforms to correct for head-motion. These resampled BOLD time-series will be referred to as preprocessed BOLD in original space, or just preprocessed BOLD. The BOLD time-series were resampled into several standard spaces, correspondingly generating the following spatially-normalized, preprocessed BOLD runs: MNI152Nlin6Asym, MNI152Nlin2009cAsym. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. Several confounding time-series were calculated based on the preprocessed BOLD: framewise displacement (FD), DVARS and three region-wise global signals. FD and DVARS are calculated for each functional run, both using their implementations in Nipype (following the definitions by (<xref ref-type="bibr" rid="c121">121</xref>)). The three global signals are extracted within the CSF, the WM, and the whole-brain masks. Additionally, a set of physiological regressors were extracted to allow for component-based noise correction (<xref ref-type="bibr" rid="c122">122</xref>). Principal components are estimated after high-pass filtering the preprocessed BOLD time-series (using a discrete cosine filter with 128s cut-off) for the two CompCor variants: temporal (tCompCor) and anatomical (aCompCor). tCompCor components are then calculated from the top 5% variable voxels within a mask covering the subcortical regions. This subcortical mask was obtained by heavily eroding the brain mask, which ensures it does not include cortical GM regions. For aCompCor, components are calculated within the intersection of the aforementioned mask and the union of CSF and WM masks calculated in T1w space, after their projection to the native space of each functional run (using the inverse BOLD-to-T1w transformation). Components are also calculated separately within the WM and CSF masks. For each CompCor decomposition, the k components with the largest singular values are retained, such that the retained components’ time series are sufficient to explain 50 percent of variance across the nuisance mask (CSF, WM, combined, or temporal). The remaining components are dropped from consideration. The head-motion estimates calculated in the correction step were also placed within the corresponding confounds file. The confound time series derived from head motion estimates and global signals were expanded with the inclusion of temporal derivatives and quadratic terms for each(<xref ref-type="bibr" rid="c123">123</xref>). Frames that exceeded a threshold of 0.5 mm FD or 1.5 standardised DVARS were annotated as motion outliers. All resamplings can be performed with a single interpolation step by composing all the pertinent transformations (i.e. head-motion transform matrices, susceptibility distortion correction when available, and co-registrations to anatomical and output spaces). Gridded (volumetric) resamplings were performed using antsApplyTransforms (ANTs), configured with Lanczos interpolation to minimize the smoothing effects of other kernels(<xref ref-type="bibr" rid="c124">124</xref>). Non-gridded (surface) resamplings were performed using mri_vol2surf (FreeSurfer).</p>
<p>Many internal operations of fMRIPrep use Nilearn 0.6.2 (<xref ref-type="bibr" rid="c125">125</xref>), mostly within the functional processing workflow.</p>
</sec>
<sec id="s8c3e">
<title>fMRI GLM Models</title>
<p>fMRI first-level models (general linear models or GLM) were computed by convolving the experimental design matrix (specific to each task, see below) with SPM’s HRF model as implemented in Nilearn, with the following parameters: spatial smoothing using a full width at half maximum window (fwhm) of 4mm; a second-order autoregressive noise model; and signal standardized to percent signal change relative to whole-brain mean. The following confound regressors were added: polynomial drift models from constant to 5<sup>th</sup> order (6 regressors); estimated head translation and rotation on three axes (6 regressors) as well as the following confound regressors given by fmriprep: average cerebro-spinal fluid signal (1 regressor), average white matter signal (1 regressor), and the first five high-variance confounds estimates (<xref ref-type="bibr" rid="c122">122</xref>) (5 regressors).</p>
<p>In the visual category localizer, the design matrix contained distinct events for each visual stimulus, grouped by category, each with a duration of 1s, including a specific one for the target star, leading to 9 regressors (Chinese, face, house, tools, numbers, words, single shape, three shapes, and star). In the geometry task runs, each reference shape was associated to a regressor with trial duration set to 2s, thus leading to 6 regressors (square, rectangle, rhombus, iso-trapezoid, hinge and random). In both cases, button presses were not modeled as they were fully correlated with predictors of interest (either the star for the localizer, or every single trial for the geometry task).</p>
<p>Second-level models were estimated after additional smoothing with a full width at half maximum window of 8mm. Statistical significance of clusters was estimated with a bootstrap procedure as follows: given an uncorrected p-value of .001, clusters were identified by contiguity of voxels that had p-values below this threshold. Then, the p-value of a cluster was derived by comparing its statistical mass (the sum of its t-values) to the distribution of the maximum statistical mass obtained by performing the same contrast after randomly swapping the sign of each participant’s statistical map. We performed this swapping 10,000 times for each contrast we estimated, and computed the corrected p-value accordingly; for instance, a cluster whose mass was only outperformed by 3 random swaps out of the 10,000 was assigned a p-value of .0003.</p>
</sec>
<sec id="s8c3f">
<title>Searchlight RSA analyses</title>
<p>First, we estimated the Representational Dissimilarity Matrix (RDM) within spheres centered on each voxel. For this we performed a searchlight sweep across the whole brain. We extracted the GLM coefficients of the geometric shapes from each voxel and all the neighboring voxels in a 3-voxel radius (=6mm), for a total of 93 voxels per sphere. We discarded voxels where more than 50% of this sphere fell outside the participant’s brain. We extracted the betas of each shape and used a cross-validated Mahalanobis distance across runs (crossnobis, implemented in rsatoolbox) to estimate the dissimilarity between each pair of shapes. We attributed this distance to the center of the searchlight, thereby estimating an empirical RDM at each location.</p>
<p>Then we compared this empirical RDM with the two RDMs derived from our two models separately, using a whitened correlation metric. Both choices of metrics (crossnobis and whitened correlation) follow the recommendations from a previous methodological publication (<xref ref-type="bibr" rid="c126">126</xref>).</p>
<p>Finally, we computed group-level statistics by smoothing the resulting correlation map (fwhm=8mm) and performing statistical maps, cluster identification, and statistical inference at the cluster level as we did for the second-order level analysis, but with a one-tailed comparison only as we did not consider negative correlations.</p>
<p>The bar plot for ROIs in <xref rid="figs4" ref-type="fig">figure S4</xref> reflect a subject-specific voxel localization within ROIs. Within each ROI identified we find, for each subject, the 10% most responsive subject-specific voxels in the same contrast used to identify the cluster. To avoid double-dipping, we selected these voxels using the contrast from one run, then collected the fMRI responses (beta coefficients) from the other runs; we perform this procedure across all runs and average the responses. Error bars indicate the standard error of the mean across participants.</p>
</sec>
</sec>
</sec>
<sec id="s8d">
<title>Experiment 4</title>
<sec id="s8d1">
<title>Participants</title>
<p>Twenty healthy French adults (13 females; 21-42 years old, mean: 24.9 years old, SD: 8.1 years old) participated in the MEG study. All participants had normal hearing, normal or corrected-to-normal vision, and no neurological deficit. All adults provided informed consent and were compensated for their participation. For all but one participant, we had access to anatomical recordings in 3T MRI, either from prior, unrelated experiments in the lab, or because the MEG session was immediately followed by a recording. Because of one participant missing an anatomical recording, analyses that required source reconstruction were performed on nineteen subjects.</p>
</sec>
<sec id="s8d2">
<title>Task</title>
<p>During MEG, adult participants were merely exposed to shapes while maintaining fixation and attention. As in previous work (e.g. <italic>127</italic>, <italic>128</italic>), the goal was to examine the spontaneous encoding of stimuli and the presence or absence of a novelty response to occasional deviants.</p>
</sec>
<sec id="s8d3">
<title>Stimuli</title>
<p>All 11 geometric shapes in <xref rid="fig1" ref-type="fig">Fig.1A</xref> were presented in miniblocks of 30 shapes. Geometric shapes were presented centered on the screen, one shape every second, with shapes remaining onscreen for 800ms and a centered fixation cross present between shapes for 200ms. To make the shapes more attractive (and since the same shape were also used in an infant experiment, not reported here), during their 800ms presentation, the shapes slowly increased in size: in total, a scale factor of 1.2 was applied over the course of 800ms, with linear interpolation of the shape size during the duration of the presentation. Shapes were presented in miniblock, following an oddball paradigm: within a miniblock, all shapes were identical up to scaling (randomly sampled in [0.875, 0.925, 0.975, 1.025, 1.075, 1.125]) and rotation (sampled in [−25°, −15°, −5°, 5°, 15°, 25°]), except for occasional oddballs which were deviant version of the reference shape. Each miniblock comprised 30 shapes, 4 of which were oddballs that could replace any shape after the first six. Two oddballs never appeared in a row. There was no specific interval between miniblocks beyond the usual duration between shapes. A run was made of 11 miniblocks, one per shape in random order, and participants attended 8 runs except two participants who are missing a single run due to experimenters’ mistakes when setting up the MEG acquisition.</p>
</sec>
<sec id="s8d4">
<title>Procedure</title>
<p>After inclusion by the lab’s recruiting team, participants were prepared for the MEG with electrocardiogram (ECG) and electrooculogram (EOG) sensors, as well as four Head Position Indicator (HPI) coils, which were digitalized to track the head position throughout the experiment. Then we explained participants that the task was a replication of an experiment with infants, and therefore was purely passive: they would be shown shapes and were instructed to pay attention to each shape, while trying to avoid blinks as well as body and eye movements. They sat in the MEG and we checked the head position, ECG/EOG and MEG signal. From that point onward, we never opened the MEG door again to avoid resetting MEG signals and allow for optimal cross-run decoding and generalization. Participants took typically 8 runs consecutively, with small breaks with no stimuli between runs to rest their eyes. At the end of the experiment, participants took the intruder test from previous work(<xref ref-type="bibr" rid="c8">8</xref>) on a laptop computer outside of the MEG, and finally we spent some time debriefing with participants the goal of the experiment.</p>
<p>To ensure high accuracy of the timing in the MEG, each trial’s first frame contained a white square on the bottom of the screen, which was hidden from participants but recorded with a photodiode. The same area was black during the rest of the experiment. The ramping up of the photodiode was therefore synchronized with the screen update and the appearance of the stimulus, ensuring robust timing for analyses. Then each “screen update” event was linked to a recorded list of presented shapes.</p>
</sec>
<sec id="s8d5">
<title>MEG Acquisition Parameters</title>
<p>Participants were instructed to look at a screen while sitting inside an electromagnetically shielded room. The magnetic component of their brain activity was recorded with a 306-channel, whole-head MEG by Elekta Neuromag© (Helsinki, Finland). The MEG helmet is composed of 102 triplets of sensors, each comprising one magnetometer and two orthogonal planar gradiometers. The brain signals were acquired at a sampling rate of 1000 Hz with a hardware high-pass filter at 0.03 Hz.</p>
<p>Eye movements and heartbeats were monitored with vertical and horizontal electro-oculograms (EOGs) and electrocardiograms (ECGs). Head shape was digitized using various points on the scalp as well as the nasion, left and right pre-auricular points (FASTTRACK, Polhemus). Subjects’ head position inside the helmet was measured at the beginning of each run with an isotrack Polhemus Inc system from the location of four coils placed over frontal and mastoïdian skull areas.</p>
</sec>
<sec id="s8d6">
<title>Preprocessing of MEG signals</title>
<p>The preprocessing of the data was performed using MNE-BIDS-Pipeline, a streamlined implementation of the core ideas presented in the literature (<xref ref-type="bibr" rid="c129">129</xref>) and leveraging BIDS specifications (<xref ref-type="bibr" rid="c130">130</xref>, <xref ref-type="bibr" rid="c131">131</xref>). The pipeline performed automatic bad channel detection (both noisy and flat), then applied Maxwell filtering and Signal Space Separation on the raw data (<xref ref-type="bibr" rid="c132">132</xref>). The data was then filtered between .1 Hz and 40Hz, and resampled to 250 Hz. Extraction of epochs was performed for each shape, starting 150ms before stimulus onset and stopping 1150ms after, and the relevant metadata (event type, run, trial index, etc.) for each epoch was recovered from the stimulation procedure at this step. Artifacts in the data (e.g. blinks, and heartbeats) were repaired with signal-space projection (<xref ref-type="bibr" rid="c133">133</xref>), and thresholds derived with “autoreject global” (<xref ref-type="bibr" rid="c134">134</xref>). For source reconstruction, some preprocessing steps were performed by fmriprep (see below). Then, sources were positioned using the “oct5” spacing with 1026 sources per hemisphere, and we used the e(xact)LORETA method (following recommendations from the literature (<xref ref-type="bibr" rid="c135">135</xref>, <xref ref-type="bibr" rid="c136">136</xref>)) using empty-room recordings performed right before or right after the experiment to estimate the noise covariance matrix. Additionally, for source reconstruction anatomical MRI were preprocessed with fmriprep. T1-weighted (T1w) images were corrected for intensity non-uniformity (INU) with N4BiasFieldCorrection (<xref ref-type="bibr" rid="c111">111</xref>), distributed with ANTs 2.3.3 (<xref ref-type="bibr" rid="c112">112</xref>). The T1w-reference was then skull-stripped with a Nipype implementation of the antsBrainExtraction.sh workflow. Brain tissue segmentation of cerebrospinal fluid, white-matter and gray-matter was performed on the brain-extracted T1w using fast (<xref ref-type="bibr" rid="c113">113</xref>). Brain surfaces were reconstructed using recon-all (<xref ref-type="bibr" rid="c114">114</xref>) and the brain mask estimated previously was refined with a custom variation of the method to reconcile ANTs-derived and FreeSurfer-derived segmentations of the cortical gray-matter of Mindboggle (<xref ref-type="bibr" rid="c115">115</xref>).</p>
</sec>
<sec id="s8d7">
<title>Decoding</title>
<p>After epoching the data, for each timepoint within an epoch and each participant, we trained a logistic regression decoder to classify epochs as reference or oddball using samples from all shapes. For this analysis we discarded the 6 first trials at the beginning of each block, since (i) those could not be oddballs ever and (ii) there was no warning of transitions between blocks and so the first trials were also “oddballs” with respect to the previous block’s shape. Each epoch was normalized before training the classifier. To avoid decoders being biased by the overall signal’s autocor-relation across timescales, we used six folds of cross validation over runs with the following folds: even runs versus odd runs; first half versus second half; and runs 1, 2, 5, 6 versus 3, 4, 7 and 8. Folds were used in both directions, e.g. even for training and odd for testing; as well as odd for training and even for testing. The decoders were trained on data from all of the shapes conjointly. When testing its accuracy, we tested it separately on data from each shape (e.g. detecting oddballs within squares only, within rectangles only, etc.) – using runs independent from the training data. In order to estimate accuracy without being biased by the imbalanced number of epochs in the different classes (there are 4 oddballs for every 20 references), we report the ROC Area Under the Curve of the Receiver Operating Characteristic (ROC AUC) for each shape in <xref rid="fig4" ref-type="fig">Fig.4A</xref>, left subfigure. Then at each timepoint, we correlated the decoding performance with the number of geometric features present in each shape: the r correlation coefficient at each timepoint is plotted in the central subfigure, together with shading for a significant cluster identified with permutation tests across participants (implemented by mne, one tailed, 2<sup>13</sup> permutations, cluster forming threshold &lt;.05). Finally, we average the decoding performance in the identified cluster and plot each shape’s average decoding performance against online behavioral data: this is effectively visualizing the same data as the central column’s figure, and therefore no statistical test is reported. The analyses were performed both without any smoothing, and with a uniform averaging over a sliding window of 100ms; the results are identical but we chose the latter since plots using the smoothed version make the separation of the different shapes easier to see. The same holds true for the next analysis.</p>
<p>In <xref rid="fig4" ref-type="fig">Fig.4B</xref>, we display a similar sequence of plots, but now instead of training a single classifier to identify epochs as reference or oddball conjointly on all shapes, we train eleven separate such classifier, one for each shape. This produces very similar results from the previous analysis.</p>
</sec>
<sec id="s8d8">
<title>RSA analysis</title>
<p>For RSA analyses, data from the oddballs was discarded and we used data from the magnetometers only. The goal of this analysis was to pinpoint when and where the mental representation of shapes followed distances that matched either a neural network model or a geometric feature model. We used the same model RDMs as the one we used to analyze behavior and fMRI data, and provide below the details of how we derived the empirical RDMs for our analyses.</p>
<sec id="s8d8a">
<title>In sensor space</title>
<p>We estimated, at each timepoint, the dissimilarity between each pair of shape across sensors: we relied on rsatoolbox to compute the Mahalanobis distance, crossvalidated with a leave-one-out scheme over runs. This provided us with one empirical RDMs for each timepoint, with no spatial information as this was performed across all sensors. Then we compared this RDM with our two models: since our model RDMs are effectively orthogonal, we performed the comparisons with the empirical RDM separately. We used a whitened correlation metric to compare RDMs. This gave use one timeseries for each participant and each model, and we then performed permutation testing in the [0, 800]ms window to identify significant temporal clusters for each model separately. As shown in <xref rid="fig4" ref-type="fig">Fig.4C</xref>, this yields one significant cluster associated to the CNN encoding model, and one significant cluster associated to the geometric feature model. As in the decoding analysis, we performed the analyses with both no temporal smoothing, and with sliding averaging (savgol filter, 2<sup>nd</sup> order, 100ms window); the results are virtually identical but we chose to report the smoothed ones in <xref rid="fig4" ref-type="fig">Fig.4C</xref>.</p>
</sec>
<sec id="s8d8b">
<title>In source space</title>
<p>In order to understand not only the temporal dynamic of the mental representations, but also get an estimate of the localization of the various representations, we turned to RSA analysis in source space. We performed source reconstruction of each reference shape trial using eLORETA and empty-room recordings either just before or just after the scanning session to estimate the noise covariance. Then we averaged the data over the three identified temporal clusters from the sensor-space RSA analysis: [0, 328]ms and [136, 452]ms. We then performed RSA analysis at each source’s location using its neighboring sources (geodesic distance of 2cm on the reconstructed cortical surface). Finally, we compared the resulting RDMs to either the CNN encoding model or the geometric feature model. These steps were performed independently for each subject. Next, we projected the whitened correlation distance between empirical RDMs and model RDM onto a common cortical space, fsaverage. Finally, we performed a permutation spatial cluster test across participants, using adjacency matrices between sources (sources are adjacent if they were immediate neighbors on the reconstructed surface’s mesh). This resulted in two significant clusters associated to the CNN encoding model during the [0, 328]ms time window, located in bilateral occipital areas. Additionally, this resulted in two significant clusters associated to the geometric feature model during the [136, 452]ms, located in very broad bilateral networks encompassing dorsal and frontal areas.</p>
</sec>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="s12" sec-type="data-availability">
<title>Data and materials availability</title>
<p>scripts for all of the analyses are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/mathias-sm/AGeometricShapeRegularityEffectHumanBrain">https://github.com/mathias-sm/AGeometricShapeRegularityEffectHumanBrain</ext-link>; behavioral data and scripts to generate the models are also available at this url. Neuroimaging data provided upon request.</p>
</sec>
<ack>
<title>Acknowledgments</title>
<p>We are grateful to Ghislaine Dehaene-Lambertz, Leila Azizi, and the NeuroSpin support teams for help in data acquisition, and Lorenzo Ciccione, Christophe Pallier, Minye Zhan, Alexandre Gramfort and the MNE team for support in data processing.</p>
</ack>
<sec id="d1e2094" sec-type="additional-information">
<title>Additional information</title>
<sec id="s9">
<title>Funding</title>
<p>FYSSEN, INSERM, CEA, Collège de France, ERC MathBrain</p>
</sec>
<sec id="s10">
<title>Author contributions</title>
<p>Conceptualization: MSM, SD</p>
<p>Methodology: MSM, SD</p>
<p>Investigation: MSM, LB, CPW, CH, FAR</p>
<p>Visualization: MSM, LB, FAR, SD</p>
<p>Funding acquisition: SD</p>
<p>Project administration: MSM, SD</p>
<p>Supervision: SD</p>
<p>Writing – original draft: MSM, SD</p>
<p>Writing – review &amp; editing: all authors</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C. S.</given-names> <surname>Henshilwood</surname></string-name> <etal>et al.</etal></person-group>, <source>Nature</source> <volume>562</volume>, <fpage>115</fpage>–<lpage>118</lpage>, ISSN: <issn>0028-0836</issn>, <issn>1476-4687</issn> (Oct. <year>2018</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>B. L.</given-names> <surname>Van der Waerden</surname></string-name></person-group>, <source>Geometry and Algebra in Ancient Civilizations</source> (<publisher-name>Springer Science &amp; Business Media</publisher-name>, <year>2012</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B.</given-names> <surname>Tversky</surname></string-name></person-group>, <source>Topics in Cognitive Science</source> <volume>3</volume>, <fpage>499</fpage>–<lpage>535</lpage>, ISSN: <issn>1756-8757</issn>, <issn>1756-8765</issn> (July <year>2011</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Dehaene</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Izard</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Pica</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Spelke</surname></string-name></person-group>, <volume>311</volume>, <fpage>5</fpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>V.</given-names> <surname>Izard</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Pica</surname></string-name>, <string-name><given-names>E. S.</given-names> <surname>Spelke</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Dehaene</surname></string-name></person-group>, <source>Proceedings of the National Academy of Sciences</source> <volume>108</volume>, <fpage>9782</fpage>–<lpage>9787</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Close</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Call</surname></string-name></person-group>, <source>Animal Cognition</source> <volume>18</volume>, <fpage>437</fpage>–<lpage>449</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Dehaene</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Al Roumi</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Lakretz</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Planton</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Sablé-Meyer</surname></string-name></person-group>, <source>Trends in Cognitive Sciences</source> <volume>26</volume>, <fpage>751</fpage>–<lpage>766</lpage>, ISSN: <issn>1364-6613</issn> (Sept. 1, <year>2022</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Sablé-Meyer</surname></string-name> <etal>et al.</etal></person-group>, <source>Proceedings of the National Academy of Sciences</source> <volume>118</volume>, ISSN: <issn>0027-8424</issn>, <issn>1091-6490</issn>, (<year>2021</year>; <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/content/118/16/e2023123118">https://www.pnas.org/content/118/16/e2023123118</ext-link>) (Apr. 20, 2021).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Saito</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Hayashi</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Takeshita</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Matsuzawa</surname></string-name></person-group>, <source>Child Development</source>, <volume>n/a–n/a</volume>, <elocation-id>00093920</elocation-id>. <year>2014</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Tanaka</surname></string-name></person-group>, <source>Animal cognition</source> <volume>10</volume>, <fpage>169</fpage>–<lpage>179</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names> <surname>Sueur</surname></string-name></person-group>, <source>Primates</source>, <fpage>1</fpage>–<lpage>6</lpage> (<year>2025</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Sablé-Meyer</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Ellis</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Tenenbaum</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Dehaene</surname></string-name></person-group>, <source>Cognitive Psychology</source> <volume>139</volume>, <fpage>101527</fpage>, ISSN: <issn>0010-0285</issn> (Dec. 1, <year>2022</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>P.</given-names> <surname>Bao</surname></string-name>, <string-name><given-names>L.</given-names> <surname>She</surname></string-name>, <string-name><given-names>M.</given-names> <surname>McGill</surname></string-name>, <string-name><given-names>D. Y.</given-names> <surname>Tsao</surname></string-name></person-group>, <source>Nature</source>, <volume>1–6</volume>, ISSN: <issn>1476-4687</issn> (June 3, <year>2020</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Kriegeskorte</surname></string-name> <etal>et al.</etal></person-group>, <source>Neuron</source> <volume>60</volume>, <fpage>1126</fpage>–<lpage>1141</lpage>, ISSN: <issn>0896-6273</issn> (Dec. 26, <year>2008</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. Y.</given-names> <surname>Tsao</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Moeller</surname></string-name>, <string-name><given-names>W. A.</given-names> <surname>Freiwald</surname></string-name></person-group>, <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>105</volume>, <fpage>19514</fpage>–<lpage>19519</lpage>, ISSN: <issn>1091-6490</issn> (Dec. 9, <year>2008</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>P.</given-names> <surname>Cavanagh</surname></string-name></person-group>, <source>Perception</source> <volume>50</volume>, <fpage>195</fpage>–<lpage>215</lpage>, ISSN: <issn>0301-0066</issn>, <issn>1468-4233</issn> (Mar. <year>2021</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>J. A.</given-names> <surname>Fodor</surname></string-name></person-group>, <source>The Language of Thought</source> (<publisher-name>Harvard University Press</publisher-name>, <year>1975</year>), vol. <volume>5</volume>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E. L. J.</given-names> <surname>Leeuwenberg</surname></string-name></person-group>, <source>The American Journal of Psychology</source> <volume>84</volume>, <fpage>307</fpage>, ISSN: <issn>0002-9556</issn> (Sept. <year>1971</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Quilty-Dunn</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Porot</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Mandelbaum</surname></string-name></person-group>, <source>Behavioral and Brain Sciences</source>, <volume>1–55</volume>, ISSN: <issn>0140-525X</issn>, <issn>1469-1825</issn> (Dec. 6, <year>2022</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Amalric</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Dehaene</surname></string-name></person-group>, <source>Proceedings of the National Academy of Sciences</source>, <volume>201603205</volume>, ISSN: <issn>0027-8424</issn>, <issn>1091-6490</issn> (Apr. 11, <year>2016</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Amalric</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Dehaene</surname></string-name></person-group>, <source>NeuroImage</source> <volume>189</volume>, <fpage>19</fpage>–<lpage>31</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Chater</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Vitányi</surname></string-name></person-group>, <source>Trends in Cognitive Sciences</source> <volume>7</volume>, <fpage>19</fpage>–<lpage>22</lpage>, ISSN: <issn>1879-307X</issn> (Jan. <year>2003</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Feldman</surname></string-name></person-group>, <source>Current Directions in Psychological Science</source> (<publisher-name>Wiley-Blackwell</publisher-name>) <volume>12</volume>, <fpage>227</fpage>–<lpage>232</lpage>, ISSN: <issn>0963-7214</issn> (Dec. <year>2003</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Kriegeskorte</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Mur</surname></string-name>, <string-name><given-names>P. A.</given-names> <surname>Bandettini</surname></string-name></person-group>, <source>Frontiers in systems neuroscience</source> <volume>2</volume>, <fpage>4</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Agrawal</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Hari</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Arun</surname></string-name></person-group>, <source>Psychological Science</source> <volume>30</volume>, <fpage>1707</fpage>–<lpage>1723</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Agrawal</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Hari</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Arun</surname></string-name></person-group>, <source>eLife</source> <volume>9</volume>, <elocation-id>e54846</elocation-id>, ISSN: <issn>2050-084X</issn> (May 5, <year>2020</year>). <pub-id pub-id-type="doi">10.7554/eLife.54846</pub-id></mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>De Leeuw</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Mair</surname></string-name></person-group>, <source>Journal of statistical software</source> <volume>31</volume>, <fpage>1</fpage>–<lpage>30</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Walther</surname></string-name> <etal>et al.</etal></person-group>, <source>NeuroImage</source> <volume>137</volume>, <fpage>188</fpage>–<lpage>200</lpage>, ISSN: <issn>1053-8119</issn> (Aug. <year>2016</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>V.</given-names> <surname>Ayzenberg</surname></string-name>, <string-name><given-names>S. F.</given-names> <surname>Lourenco</surname></string-name></person-group>, <source>Scientific Reports</source> <volume>9</volume>, <fpage>9359</fpage>, ISSN: <issn>2045-2322</issn> (June 27, <year>2019</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T.</given-names> <surname>Morfoisse</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Izard</surname></string-name></person-group>, <source>Journal of Vision</source> <volume>21</volume>, <fpage>2621</fpage>–<lpage>2621</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.</given-names> <surname>Epstein</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Harris</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Stanley</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Kanwisher</surname></string-name></person-group>, <source>Neuron</source> (<year>1999</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>P.</given-names> <surname>Pinheiro-Chagas</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Daitch</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Parvizi</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Dehaene</surname></string-name></person-group>, <source>Journal of cognitive neuroscience</source> <volume>30</volume>, <fpage>1757</fpage>–<lpage>1772</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.</given-names> <surname>Ansari</surname></string-name></person-group>, <source>Nature reviews neuroscience</source> <volume>9</volume>, <fpage>278</fpage>–<lpage>291</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. A.</given-names> <surname>Williams</surname></string-name> <etal>et al.</etal></person-group>, <source>Nat Neurosci</source> <volume>11</volume>, <fpage>1439</fpage>–<lpage>1445</lpage>, ISSN: <issn>1546-1726</issn> (<year>2008</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Sassenhagen</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Draschkow</surname></string-name></person-group>, <source>Psychophysiology</source> <volume>56</volume>, <fpage>e13335</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H.</given-names> <surname>Blum</surname></string-name></person-group>, <source>Journal of Theoretical Biology</source> <volume>38</volume>, <fpage>205</fpage>–<lpage>287</lpage>, ISSN: <issn>0022-5193</issn> (Feb. <year>1973</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Feldman</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Singh</surname></string-name></person-group>, <source>Proceedings of the National Academy of Sciences</source> <volume>103</volume>, <fpage>18014</fpage>–<lpage>18019</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>V.</given-names> <surname>Ayzenberg</surname></string-name>, <string-name><given-names>F. S.</given-names> <surname>Kamps</surname></string-name>, <string-name><given-names>D. D.</given-names> <surname>Dilks</surname></string-name>, <string-name><given-names>S. F.</given-names> <surname>Lourenco</surname></string-name></person-group>, <source>Neuropsychologia</source> <volume>164</volume>, <fpage>108092</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K.</given-names> <surname>Denisova</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Feldman</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Su</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Singh</surname></string-name></person-group>, <source>Vision research</source> <volume>126</volume>, <fpage>347</fpage>–<lpage>361</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. S.</given-names> <surname>Lowet</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Firestone</surname></string-name>, <string-name><given-names>B. J.</given-names> <surname>Scholl</surname></string-name></person-group>, <source>Attention, Perception, &amp; Psychophysics</source> <volume>80</volume>, <fpage>1278</fpage>–<lpage>1289</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>V.</given-names> <surname>Froyen</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Feldman</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Singh</surname></string-name></person-group>, <source>Psychological Review</source> <volume>122</volume>, <fpage>575</fpage>–<lpage>597</lpage>, ISSN: <issn>1939-1471</issn>, <issn>0033-295X</issn> (Oct. <year>2015</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names> <surname>Firestone</surname></string-name>, <string-name><given-names>B. J.</given-names> <surname>Scholl</surname></string-name></person-group>, <source>Psychological science</source> <volume>25</volume>, <fpage>377</fpage>–<lpage>386</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Jacob</surname></string-name>, <string-name><given-names>R. T.</given-names> <surname>Pramod</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Katti</surname></string-name>, <string-name><given-names>S. P.</given-names> <surname>Arun</surname></string-name></person-group>, <source>Nature Communications</source> <volume>12</volume>, <fpage>1872</fpage>, ISSN: <issn>2041-1723</issn> (Mar. 25, <year>2021</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. S.</given-names> <surname>Bowers</surname></string-name> <etal>et al.</etal></person-group>, <source>Behavioral and Brain Sciences</source>, <fpage>1</fpage>–<lpage>74</lpage>, ISSN: <issn>0140-525X</issn>, <issn>1469-1825</issn> (Dec. 1, <year>2022</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>D. I.</given-names> <surname>Campbell</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Kumar</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Giallanza</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Cohen</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Griffiths</surname></string-name></person-group>, <conf-name>Proceedings of the Annual Meeting of the Cognitive Science Society</conf-name>, vol. <volume>46</volume>. <year>no date</year></mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Oquab</surname></string-name>, <etal>et al.</etal></person-group>, <source>arXiv</source> <elocation-id>2304.07193</elocation-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Amalric</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Denghien</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Dehaene</surname></string-name></person-group>, <source>Developmental Cognitive Neuroscience</source> <volume>30</volume>, <fpage>314</fpage>–<lpage>323</lpage>, ISSN: <issn>1878-9293</issn> (Apr. 1, <year>2018</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Kanjlia</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Lane</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Feigenson</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Bedny</surname></string-name></person-group>, <source>Proceedings of the National Academy of Sciences</source>, <fpage>201524982</fpage>, ISSN: <issn>0027-8424</issn>, <issn>1091-6490</issn> (Sept. 16, <year>2016</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Y.</given-names> <surname>Xu</surname></string-name> <etal>et al.</etal></person-group>, <source>PLOS Biology</source> <volume>21</volume>, <fpage>e3001930</fpage>, ISSN: <issn>1545-7885</issn> (July 25, <year>2023</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>V.</given-names> <surname>Ayzenberg</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Simmons</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Behrmann</surname></string-name></person-group>, <source>Cerebral Cortex Communications</source> <volume>4</volume>, <fpage>tgad003</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E.</given-names> <surname>Collins</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Freud</surname></string-name>, <string-name><given-names>J. M.</given-names> <surname>Kainerstorfer</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Cao</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Behrmann</surname></string-name></person-group>, <source>Journal of Cognitive Neuroscience</source> <volume>31</volume>, <fpage>821</fpage>–<lpage>836</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.</given-names> <surname>Ansari</surname></string-name></person-group>, <source>Nat Rev Neurosci</source> <volume>9</volume>, <fpage>278</fpage>–<lpage>91</lpage>, ISSN: <issn>1471-0048</issn> (<year>2008</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. F.</given-names> <surname>Cantlon</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Li</surname></string-name></person-group>, <source>PLoS biology</source> <volume>11</volume>, <fpage>e1001462</fpage>, ISSN: <issn>1545-7885</issn> (<year>2013</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>V.</given-names> <surname>Izard</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Dehaene-Lambertz</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Dehaene</surname></string-name></person-group>, <source>PLoS Biol</source> <volume>6</volume>, <fpage>275</fpage>–<lpage>285</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>V.</given-names> <surname>Izard</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Pica</surname></string-name>, <string-name><given-names>E. S.</given-names> <surname>Spelke</surname></string-name></person-group>, <source>Cognitive Psychology</source> <volume>136</volume>, <fpage>101494</fpage>, ISSN: <issn>0010-0285</issn> (Aug. 1, <year>2022</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>C.</given-names> <surname>Conwell</surname></string-name>, <string-name><given-names>J. S.</given-names> <surname>Prince</surname></string-name>, <string-name><given-names>G. A.</given-names> <surname>Alvarez</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Konkle</surname></string-name></person-group>, <conf-name>SVRHM 2021 Workshop@ NeurIPS</conf-name>. <year>no date</year></mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Kubilius</surname></string-name> <etal>et al.</etal></person-group>, <conf-name>Advances in Neural Information Processing Systems</conf-name>, vol. <volume>32</volume>, (<year>2024</year>; <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2019/hash/7813d1590d28a7dd372ad54b5d29d033-Abstract.html">https://proceedings.neurips.cc/paper/2019/hash/7813d1590d28a7dd372ad54b5d29d033-Abstract.html</ext-link>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Schrimpf</surname></string-name> <etal>et al.</etal></person-group>, “<source>Brain-Score: Which Artificial Neural Network for Object Recognition Is Most Brain-Like?</source>”, (<year>2020</year>; <ext-link ext-link-type="uri" xlink:href="http://biorxiv.org/lookup/doi/10.1101/407007">http://biorxiv.org/lookup/doi/10.1101/407007</ext-link>).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. L.</given-names> <surname>Yamins</surname></string-name> <etal>et al.</etal></person-group>, <source>Proceedings of the National Academy of Sciences</source> <volume>111</volume>, <fpage>8619</fpage>–<lpage>8624</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. J.</given-names> <surname>Arcaro</surname></string-name>, <string-name><given-names>P. F.</given-names> <surname>Schade</surname></string-name>, <string-name><given-names>J. L.</given-names> <surname>Vincent</surname></string-name>, <string-name><given-names>C. R.</given-names> <surname>Ponce</surname></string-name>, <string-name><given-names>M. S.</given-names> <surname>Livingstone</surname></string-name></person-group>, <source>Nature Neuroscience</source> <volume>20</volume>, <fpage>1404</fpage>–<lpage>1412</lpage>, ISSN: <issn>1097-6256</issn>, <issn>1546-1726</issn> (Oct. <year>2017</year>).</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E.</given-names> <surname>Margalit</surname></string-name> <etal>et al.</etal></person-group>, <source>The Journal of Neuroscience</source> <volume>40</volume>, <fpage>3008</fpage>–<lpage>3024</lpage>, ISSN: <issn>0270-6474</issn>, <issn>1529-2401</issn> (Apr. 8, <year>2020</year>).</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. M.</given-names> <surname>Rauschecker</surname></string-name>, <string-name><given-names>R. F.</given-names> <surname>Bowen</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Parvizi</surname></string-name>, <string-name><given-names>B. A.</given-names> <surname>Wandell</surname></string-name></person-group>, <source>Proceedings of the National Academy of Sciences</source> <volume>109</volume>, ISSN: <issn>0027-8424</issn>, <issn>1091-6490</issn>, (<year>2023</year>; <ext-link ext-link-type="uri" xlink:href="https://pnas.org/doi/full/10.1073/pnas.1121304109">https://pnas.org/doi/full/10.1073/pnas.1121304109</ext-link>) (June 12, 2012).</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. Y.</given-names> <surname>Tsao</surname></string-name>, <string-name><given-names>M. S.</given-names> <surname>Livingstone</surname></string-name></person-group>, <source>Annual Review of Neuroscience</source> <volume>31</volume>, <fpage>411</fpage>–<lpage>437</lpage>, ISSN: <issn>0147-006X</issn>, <issn>1545-4126</issn> (July 1, <year>2008</year>).</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Khosla</surname></string-name>, <string-name><given-names>N. A. R.</given-names> <surname>Murty</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Kanwisher</surname></string-name></person-group>, <source>Journal of Vision</source> <volume>22</volume>, <fpage>4184</fpage>–<lpage>4184</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>E. J.</given-names> <surname>Allen</surname></string-name> <etal>et al.</etal>,  publisher:  <collab>Group Subject_term: Cortex;Neural encoding;Object vision;Perception Subject_term_id: cortex;neural-encoding;objectvision;perception</collab></person-group><source>Nature Neuroscience, Bandiera_abtest: a Cg_type: Nature Research Journals Primary_atype: Research</source> publisher: <publisher-name>Nature Publishing</publisher-name> , <fpage>1</fpage>–<lpage>11</lpage>, ISSN: <issn>1546-1726</issn> (Dec. 16, <year>2021</year>).</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>I.</given-names> <surname>Biederman</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Ju</surname></string-name></person-group>, <source>Cognitive Psychology</source> <volume>20</volume>, <fpage>38</fpage>–<lpage>64</lpage>, ISSN: <issn>0010-0285</issn> (Jan. <year>1988</year>).</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>V.</given-names> <surname>Ayzenberg</surname></string-name>, <string-name><given-names>F. S.</given-names> <surname>Kamps</surname></string-name>, <string-name><given-names>D. D.</given-names> <surname>Dilks</surname></string-name>, <string-name><given-names>S. F.</given-names> <surname>Lourenco</surname></string-name></person-group>, <source>Neuropsychologia</source> <volume>164</volume>, <fpage>108092</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. D.</given-names> <surname>Lescroart</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Biederman</surname></string-name></person-group>, <source>Cerebral cortex</source> <volume>23</volume>, <fpage>629</fpage>–<lpage>637</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>P.</given-names> <surname>Papale</surname></string-name> <etal>et al.</etal></person-group>, <source>Scientific Reports</source> <volume>9</volume>, <fpage>7601</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>P.</given-names> <surname>Papale</surname></string-name> <etal>et al.</etal></person-group>, <source>Journal of Neurophysiology</source> <volume>124</volume>, <fpage>1560</fpage>–<lpage>1570</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>V.</given-names> <surname>Ayzenberg</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Behrmann</surname></string-name></person-group>, <source>Trends in Cognitive Sciences</source> <volume>26</volume>, <fpage>1119</fpage>–<lpage>1132</lpage>, ISSN: <issn>1364-6613</issn> (Dec. <year>2022</year>).</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K.</given-names> <surname>Grill-Spector</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Kourtzi</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Kanwisher</surname></string-name></person-group>, <source>Vision research</source> <volume>41</volume>, <fpage>1409</fpage>–<lpage>1422</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K.</given-names> <surname>Grill-Spector</surname></string-name> <etal>et al.</etal></person-group>, <source>Human brain mapping</source> <volume>6</volume>, <fpage>316</fpage>–<lpage>328</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Kanwisher</surname></string-name>, <string-name><given-names>M. M.</given-names> <surname>Chun</surname></string-name>, <string-name><given-names>J.</given-names> <surname>McDermott</surname></string-name>, <string-name><given-names>P. J.</given-names> <surname>Ledden</surname></string-name></person-group>, <source>Cognitive Brain Research</source> <volume>5</volume>, <fpage>55</fpage>–<lpage>67</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Z.</given-names> <surname>Kourtzi</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Kanwisher</surname></string-name></person-group>, <source>Journal of Neuroscience</source> <volume>20</volume>, <fpage>3310</fpage>–<lpage>3318</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.</given-names> <surname>Malach</surname></string-name> <etal>et al.</etal></person-group>, <source>Proceedings of the National Academy of Sciences</source> <volume>92</volume>, <fpage>8135</fpage>–<lpage>8139</lpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Z.</given-names> <surname>Kourtzi</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Erb</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Grodd</surname></string-name>, <string-name><given-names>H. H.</given-names> <surname>Bülthoff</surname></string-name></person-group>, <source>Cerebral cortex</source> <volume>13</volume>, <fpage>911</fpage>–<lpage>920</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L. L.</given-names> <surname>Emberson</surname></string-name>, <string-name><given-names>S. L.</given-names> <surname>Crosswhite</surname></string-name>, <string-name><given-names>J. E.</given-names> <surname>Richards</surname></string-name>, <string-name><given-names>R. N.</given-names> <surname>Aslin</surname></string-name></person-group>, <source>Journal of neuroscience</source> <volume>37</volume>, <fpage>3698</fpage>–<lpage>3703</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Nishimura</surname></string-name>, <string-name><given-names>K. S.</given-names> <surname>Scherf</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Zachariou</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Tarr</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Behrmann</surname></string-name></person-group>, <source>Journal of cognitive neuroscience</source> <volume>27</volume>, <fpage>474</fpage>–<lpage>491</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.</given-names> <surname>Heinke</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Wachman</surname></string-name>, <string-name><given-names>W.</given-names> <surname>van Zoest</surname></string-name>, <string-name><given-names>E. C.</given-names> <surname>Leek</surname></string-name></person-group>, <source>Vision Research</source> <volume>189</volume>, <fpage>81</fpage>–<lpage>92</lpage>, <issn>0042-6989</issn> <year>2021</year>.</mixed-citation></ref>
<ref id="c81"><label>81.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>J. A.</given-names> <surname>Thompson</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Sheahan</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Summerfield</surname></string-name>  <etal>et al.</etal></person-group><conf-name>Proceedings of The 1st Gaze Meets ML workshop</conf-name> , vol. <volume>210</volume>, pp. <fpage>199</fpage>–<lpage>218</lpage>, (<ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v210/thompson23a.html">https://proceedings.mlr.press/v210/thompson23a.html</ext-link>). <year>no date</year></mixed-citation></ref>
<ref id="c82"><label>82.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>I.</given-names> <surname>Biederman</surname></string-name></person-group>, <source>Psychological review</source> <volume>94</volume>, <fpage>115</fpage> (<year>1987</year>).</mixed-citation></ref>
<ref id="c83"><label>83.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>O.</given-names> <surname>Amir</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Biederman</surname></string-name>, <string-name><given-names>K. J.</given-names> <surname>Hayworth</surname></string-name></person-group>, <source>Vision Research</source> <volume>62</volume>, <fpage>35</fpage>–<lpage>43</lpage>, ISSN: <issn>0042-6989</issn> (June 1, <year>2012</year>).</mixed-citation></ref>
<ref id="c84"><label>84.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>I.</given-names> <surname>Biederman</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Yue</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Davidoff</surname></string-name></person-group>, <source>Psychological Science</source> <volume>20</volume>, <fpage>1437</fpage>–<lpage>1442</lpage>, ISSN: <issn>0956-7976</issn>, <issn>1467-9280</issn> (Dec. <year>2009</year>).</mixed-citation></ref>
<ref id="c85"><label>85.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Kayaert</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Wagemans</surname></string-name></person-group>, <source>i-Perception</source> <volume>1</volume>, <fpage>149</fpage>–<lpage>158</lpage>, ISSN: <issn>2041-6695</issn>, <issn>2041-6695</issn> (Dec. <year>2010</year>).</mixed-citation></ref>
<ref id="c86"><label>86.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Kayaert</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Biederman</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Vogels</surname></string-name></person-group>, <source>Journal of Neuroscience</source> <volume>23</volume>, <fpage>3016</fpage>–<lpage>3027</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c87"><label>87.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Kayaert</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Biederman</surname></string-name>, <string-name><given-names>H. P.</given-names> <surname>Op de Beeck</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Vogels</surname></string-name></person-group>, <source>European Journal of Neuroscience</source> <volume>22</volume>, <fpage>212</fpage>–<lpage>224</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c88"><label>88.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Kayaert</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Biederman</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Vogels</surname></string-name></person-group>, <source>Cerebral Cortex</source> <volume>15</volume>, <fpage>1308</fpage>–<lpage>1321</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c89"><label>89.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>I.</given-names> <surname>Kalfas</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Vinken</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Vogels</surname></string-name></person-group>, <source>PLOS Computational Biology</source> <volume>14</volume>, <fpage>e1006557</fpage>, ISSN: <issn>1553-7358</issn> (Oct. 26, <year>2018</year>).</mixed-citation></ref>
<ref id="c90"><label>90.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L.</given-names> <surname>Chang</surname></string-name>, <string-name><given-names>D. Y.</given-names> <surname>Tsao</surname></string-name></person-group>, <source>Cell</source> <volume>169</volume>, <fpage>1013</fpage>–<lpage>1028.e14,</lpage> ISSN: <issn>1097-4172</issn> (June 1, <year>2017</year>).</mixed-citation></ref>
<ref id="c91"><label>91.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>W.</given-names> <surname>Suzuki</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Tanaka</surname></string-name></person-group>, <source>European Journal of Neuroscience</source> <volume>33</volume>, <fpage>748</fpage>–<lpage>757</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c92"><label>92.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. J.</given-names> <surname>Freedman</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Riesenhuber</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Poggio</surname></string-name>, <string-name><given-names>E. K.</given-names> <surname>Miller</surname></string-name></person-group>, <source>Science</source> <volume>291</volume>, <fpage>312</fpage>–<lpage>6</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c93"><label>93.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. J.</given-names> <surname>Freedman</surname></string-name>, <string-name><given-names>J. A.</given-names> <surname>Assad</surname></string-name></person-group>, <source>Nature</source> <volume>443</volume>, <fpage>85</fpage>–<lpage>8</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c94"><label>94.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. J.</given-names> <surname>Freedman</surname></string-name>, <string-name><given-names>E. K.</given-names> <surname>Miller</surname></string-name></person-group>, <source>Neuroscience &amp; Biobehavioral Reviews, The Cognitive Neuroscience of Category Learning</source> <volume>32</volume>, <fpage>311</fpage>–<lpage>329</lpage>, ISSN: <issn>0149-7634</issn> (Jan. 1, <year>2008</year>).</mixed-citation></ref>
<ref id="c95"><label>95.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. L.</given-names> <surname>McKee</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Riesenhuber</surname></string-name>, <string-name><given-names>E. K.</given-names> <surname>Miller</surname></string-name>, <string-name><given-names>D. J.</given-names> <surname>Freedman</surname></string-name></person-group>, <source>The Journal of Neuroscience</source> <volume>34</volume>, <fpage>16065</fpage>–<lpage>16075</lpage>, ISSN: <issn>0270-6474</issn>, <issn>1529-2401</issn> (Nov. 26, <year>2014</year>).</mixed-citation></ref>
<ref id="c96"><label>96.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>V.</given-names> <surname>Izard</surname></string-name>, <string-name><given-names>E. S.</given-names> <surname>Spelke</surname></string-name></person-group>, <source>Human evolution</source> <volume>23</volume>, <fpage>213</fpage>–<lpage>248</lpage>, ISSN<issn>0393-9375</issn> <year>2009</year>.</mixed-citation></ref>
<ref id="c97"><label>97.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. R.</given-names> <surname>Dillon</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Duyck</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Dehaene</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Amalric</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Izard</surname></string-name></person-group>, <source>Journal of Experimental Psychology: Human Perception and Performance</source> <volume>45</volume>, <fpage>1236</fpage>–<lpage>1247</lpage>, ISSN: <issn>1939-1277</issn> (<year>2019</year>).</mixed-citation></ref>
<ref id="c98"><label>98.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Kubilius</surname></string-name> <etal>et al.</etal></person-group>, “<source>CORnet: Modeling the Neural Mechanisms of Core Object Recognition</source>”, (<year>2020</year>; <ext-link ext-link-type="uri" xlink:href="http://biorxiv.org/lookup/doi/10.1101/408385">http://biorxiv.org/lookup/doi/10.1101/408385</ext-link>).</mixed-citation></ref>
<ref id="c99"><label>99.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>K.</given-names> <surname>He</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Ren</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Sun</surname></string-name></person-group>, <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>, pp. <fpage>770</fpage>–<lpage>778</lpage>. <year>no date</year></mixed-citation></ref>
<ref id="c100"><label>100.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>L.</given-names> <surname>van der Maaten</surname></string-name>, <string-name><given-names>K. Q.</given-names> <surname>Weinberger</surname></string-name></person-group>, <source>arXiv</source> <elocation-id>1608.06993</elocation-id>, (<year>2020</year>; <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1608.06993">http://arxiv.org/abs/1608.06993</ext-link>).</mixed-citation></ref>
<ref id="c101"><label>101.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Zhan</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Goebel</surname></string-name>, <string-name><given-names>B.</given-names> <surname>de Gelder</surname></string-name></person-group>, <source>eneuro</source> <volume>5</volume>, <elocation-id>ENEURO.0285–17.2017</elocation-id>, ISSN: <issn>2373-2822</issn> (Jan. <year>2018</year>).</mixed-citation></ref>
<ref id="c102"><label>102.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.</given-names> <surname>Lundqvist</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Flykt</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Öhman</surname></string-name></person-group>, <source>Cognition and Emotion</source> (<year>1998</year>).</mixed-citation></ref>
<ref id="c103"><label>103.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Q.</given-names> <surname>Cai</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Brysbaert</surname></string-name></person-group>, <source>PloS one</source> <volume>5</volume>, <fpage>e10729</fpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c104"><label>104.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>F.</given-names> <surname>Vinckier</surname></string-name> <etal>et al.</etal></person-group>, <source>Neuron</source> <volume>55</volume>, <fpage>143</fpage>–<lpage>156</lpage>, ISSN: <issn>0896-6273</issn> (July <year>2007</year>).</mixed-citation></ref>
<ref id="c105"><label>105.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Dehaene-Lambertz</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Monzalvo</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Dehaene</surname></string-name></person-group>, <source>PLOS Biology</source> <volume>16</volume>, <elocation-id>e2004103</elocation-id>, ISSN: <issn>1545-7885</issn> (Mar. 6, <year>2018</year>).</mixed-citation></ref>
<ref id="c106"><label>106.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Dehaene</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Izard</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Pica</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Spelke</surname></string-name></person-group>, <source>Science</source> <volume>311</volume>, <fpage>381</fpage>–<lpage>384</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c107"><label>107.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>O.</given-names> <surname>Esteban</surname></string-name> <etal>et al.</etal></person-group>, <source>Software</source> (<year>2018</year>).</mixed-citation></ref>
<ref id="c108"><label>108.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>O.</given-names> <surname>Esteban</surname></string-name> <etal>et al.</etal></person-group>, <source>Nature Methods</source> (<year>2018</year>).</mixed-citation></ref>
<ref id="c109"><label>109.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K.</given-names> <surname>Gorgolewski</surname></string-name> <etal>et al.</etal></person-group>, <source>Frontiers in Neuroinformatics</source> <volume>5</volume>, <fpage>13</fpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c110"><label>110.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K. J.</given-names> <surname>Gorgolewski</surname></string-name> <etal>et al.</etal></person-group>, <source>Software</source> (<year>2018</year>).</mixed-citation></ref>
<ref id="c111"><label>111.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N. J.</given-names> <surname>Tustison</surname></string-name> <etal>et al.</etal></person-group>, <source>IEEE Transactions on Medical Imaging</source> <volume>29</volume>, <fpage>1310</fpage>–<lpage>1320</lpage>, ISSN: <issn>0278-0062</issn> (<year>2010</year>).</mixed-citation></ref>
<ref id="c112"><label>112.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B.</given-names> <surname>Avants</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Epstein</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Grossman</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Gee</surname></string-name></person-group>, <source>Medical Image Analysis</source> <volume>12</volume>, <fpage>26</fpage>–<lpage>41</lpage>, ISSN: <issn>1361-8415</issn> (<year>2008</year>).</mixed-citation></ref>
<ref id="c113"><label>113.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Y.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Brady</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Smith</surname></string-name></person-group>, <source>IEEE Transactions on Medical Imaging</source> <volume>20</volume>, <fpage>45</fpage>–<lpage>57</lpage>, ISSN: <issn>0278-0062</issn> (<year>2001</year>).</mixed-citation></ref>
<ref id="c114"><label>114.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. M.</given-names> <surname>Dale</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Fischl</surname></string-name>, <string-name><given-names>M. I.</given-names> <surname>Sereno</surname></string-name></person-group>, <source>NeuroImage</source> <volume>9</volume>, <fpage>179</fpage>–<lpage>194</lpage>, ISSN: <issn>1053-8119</issn> (<year>1999</year>).</mixed-citation></ref>
<ref id="c115"><label>115.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Klein</surname></string-name> <etal>et al.</etal></person-group>, <source>PLOS Computational Biology</source> <volume>13</volume>, <fpage>e1005350</fpage>, ISSN: <issn>1553-7358</issn> (<year>2017</year>).</mixed-citation></ref>
<ref id="c116"><label>116.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Evans</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Janke</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Collins</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Baillet</surname></string-name></person-group>, <source>NeuroImage</source> <volume>62</volume>, <fpage>911</fpage>–<lpage>922</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c117"><label>117.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>V.</given-names> <surname>Fonov</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Evans</surname></string-name>, <string-name><given-names>R.</given-names> <surname>McKinstry</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Almli</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Collins</surname></string-name></person-group>, <source>NeuroImage</source> <volume>47</volume>, <issue>Supplement 1</issue>, <fpage>S102</fpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c118"><label>118.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. N.</given-names> <surname>Greve</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Fischl</surname></string-name></person-group>, <source>NeuroImage</source> <volume>48</volume>, <fpage>63</fpage>–<lpage>72</lpage>, ISSN: <issn>1095-9572</issn> (<year>2009</year>).</mixed-citation></ref>
<ref id="c119"><label>119.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Jenkinson</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Bannister</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Brady</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Smith</surname></string-name></person-group>, <source>NeuroImage</source> <volume>17</volume>, <fpage>825</fpage>–<lpage>841</lpage>, ISSN: <issn>1053-8119</issn> (<year>2002</year>).</mixed-citation></ref>
<ref id="c120"><label>120.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. W.</given-names> <surname>Cox</surname></string-name>, <string-name><given-names>J. S.</given-names> <surname>Hyde</surname></string-name></person-group>, <source>NMR in Biomedicine</source> <volume>10</volume>, <fpage>171</fpage>–<lpage>178</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c121"><label>121.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. D.</given-names> <surname>Power</surname></string-name> <etal>et al.</etal></person-group>, <source>NeuroImage</source> <volume>84</volume>, <fpage>320</fpage>–<lpage>341</lpage>, ISSN: <issn>1053-8119</issn> (<year>2014</year>).</mixed-citation></ref>
<ref id="c122"><label>122.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Y.</given-names> <surname>Behzadi</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Restom</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Liau</surname></string-name>, <string-name><given-names>T. T.</given-names> <surname>Liu</surname></string-name></person-group>, <source>NeuroImage</source> <volume>37</volume>, <fpage>90</fpage>–<lpage>101</lpage>, ISSN: <issn>1053-8119</issn> (<year>2007</year>).</mixed-citation></ref>
<ref id="c123"><label>123.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T. D.</given-names> <surname>Satterthwaite</surname></string-name> <etal>et al.</etal></person-group>, <source>NeuroImage</source> <volume>64</volume>, <fpage>240</fpage>–<lpage>256</lpage>, ISSN: <issn>1053-8119</issn> (<year>2013</year>).</mixed-citation></ref>
<ref id="c124"><label>124.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names> <surname>Lanczos</surname></string-name></person-group>, <source>Journal of the Society for Industrial and Applied Mathematics Series B Numerical Analysis</source> <volume>1</volume>, <fpage>76</fpage>–<lpage>85</lpage>, ISSN: <issn>0887-459X</issn> (<year>1964</year>).</mixed-citation></ref>
<ref id="c125"><label>125.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Abraham</surname></string-name> <etal>et al.</etal></person-group>, <source>Frontiers in Neuroinformatics</source> <volume>8</volume>, ISSN: <issn>1662-5196</issn>, (<ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fninf.2014.00014/full">https://www.frontiersin.org/articles/10.3389/fninf.2014.00014/full</ext-link>) (<year>2014</year>).</mixed-citation></ref>
<ref id="c126"><label>126.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Diedrichsen</surname></string-name> <etal>et al.</etal></person-group>, <source>Comparing Representational Geometries Using Whitened Unbiased-Distance-Matrix Similarity</source>, Aug. 5, 2021, (<year>2022</year>; <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2007.02789">http://arxiv.org/abs/2007.02789</ext-link>).</mixed-citation></ref>
<ref id="c127"><label>127.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>F.</given-names> <surname>Al Roumi</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Planton</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Dehaene</surname></string-name></person-group>, <source>eLife</source> in press, 2022.10.15.512361, (2022; <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.10.15.512361v1">https://www.biorxiv.org/content/10.1101/2022.10.15.512361v1</ext-link>) (<year>2023</year>).</mixed-citation></ref>
<ref id="c128"><label>128.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L.</given-names> <surname>Benjamin</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Sable-Meyer</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Flo</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Dehaene-Lambertz</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Al Roumi</surname></string-name></person-group>, <source>Journal of Neuroscience</source> (<year>2024</year>).</mixed-citation></ref>
<ref id="c129"><label>129.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Jas</surname></string-name> <etal>et al.</etal></person-group>, <source>Frontiers in Neuroscience</source> <volume>12</volume>, <fpage>530</fpage>, ISSN: <issn>1662-453X</issn> (Aug. 6, <year>2018</year>).</mixed-citation></ref>
<ref id="c130"><label>130.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Niso</surname></string-name> <etal>et al.</etal></person-group>, <source>Scientific Data</source> <volume>5</volume>, <fpage>180110</fpage>, ISSN: <issn>2052-4463</issn> (Dec. <year>2018</year>).</mixed-citation></ref>
<ref id="c131"><label>131.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C. R.</given-names> <surname>Pernet</surname></string-name> <etal>et al.</etal></person-group>, <source>Scientific Data</source> <volume>6</volume>, <fpage>103</fpage>, ISSN: <issn>2052-4463</issn> (Dec. <year>2019</year>).</mixed-citation></ref>
<ref id="c132"><label>132.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Taulu</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Kajola</surname></string-name></person-group>, <source>Journal of Applied Physics</source> <volume>97</volume>, <fpage>124905</fpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c133"><label>133.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. A.</given-names> <surname>Uusitalo</surname></string-name>, <string-name><given-names>R. J.</given-names> <surname>Ilmoniemi</surname></string-name></person-group>, <source>Medical and biological engineering and computing</source> <volume>35</volume>, <fpage>135</fpage>–<lpage>140</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c134"><label>134.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Jas</surname></string-name>, <string-name><given-names>D. A.</given-names> <surname>Engemann</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Bekhti</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Raimondo</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Gramfort</surname></string-name></person-group>, <source>NeuroImage</source> <volume>159</volume>, <fpage>417</fpage>–<lpage>429</lpage>, ISSN: <issn>1053-8119</issn> (Oct. <year>2017</year>).</mixed-citation></ref>
<ref id="c135"><label>135.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. A.</given-names> <surname>Jatoi</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Kamel</surname></string-name>, <string-name><given-names>A. S.</given-names> <surname>Malik</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Faye</surname></string-name></person-group>, <source>Australasian physical &amp; engineering sciences in medicine</source> <volume>37</volume>, <fpage>713</fpage>–<lpage>721</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c136"><label>136.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>R. D.</given-names> <surname>Pascual-Marqui</surname></string-name> <etal>et al.</etal></person-group>, <source>BioRxiv</source>, <elocation-id>269753</elocation-id> (<year>2018</year>).</mixed-citation></ref>
<ref id="c137"><label>137.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. M.</given-names> <surname>Cichy</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Oliva</surname></string-name></person-group>, <source>Neuron</source> <volume>0</volume>, ISSN: <issn>0896-6273</issn>, (<year>2020</year>; <ext-link ext-link-type="uri" xlink:href="https://www.cell.com/neuron/abstract/S0896-6273(20)30518-3">https://www.cell.com/neuron/abstract/S0896-6273(20)30518-3</ext-link>) (July 27, 2020).</mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1">
<title>Appendix</title>
<sec id="s13a">
<title>Materials and Methods</title>
<sec id="s13a1" sec-type="ethics-statement">
<title>Ethics</title>
<p>All studies were conducted in accordance with the Declaration of Helsinki and French bioethics laws. On-line collection of behavioral data was approved by the Paris-Saclay University Committee for Ethical Research (CER CER-Paris-Saclay-2019-063). Behavioral and brain-imaging studies in adults and 6-year-old children (MEG and fMRI) were approved by the CEA ethics boards and by a nationally approved ethics committee (MEG: CPP_100049; fMRI in children: CPP 100027 and CPP 100053; in adults CPP 100050)</p>
</sec>
</sec>
<sec id="s13b">
<title>Supplementary text</title>
<sec id="s13b1">
<title>Behavioral intruder task and generation of behavioral similarity matrix</title>
<sec id="s13b1a">
<title>Additional Results in the Behavior task: comparison with other CNNs</title>
<p>In <xref rid="figs1" ref-type="fig">Fig.S1</xref> we report additional results from correlating human behavior with various models (top; DenseNet and ResNet) and various layers of a single model (bottom; many layers of CORnet). In all cases the CNN encoding model is a much worse predictor of the human behavior than the geometric feature model (all p&lt;.001). The late layers of CORnet, DenseNet and ResNet all capture some of the variance of participants behaviors to comparable; while ResNet is the highest scoring in this analysis, we can see in <xref rid="figs1" ref-type="fig">Fig.S1A</xref> that they are all highly correlated, and in order to stay close to previous work we used CORnet’s layer IT throughout the article.</p>
<p>Additionally, the fit of the successive layers of CORnet (V1, V2, V4: small effect sizes, only significant for V1; IT, flatten: much higher effect sizes, increasing between IT and flatten) indicates that the feed-forward processing of the visual input by the CNN encoding yields internal representations that are increasingly closer to humans’. This suggest that even the visual model goes beyond local properties that V1 would capture – but in all cases, the fit is much worse than the geometric feature model.</p>
</sec>
</sec>
<sec id="s13b2">
<title>Intruder task during fMRI</title>
<sec id="s13b2a">
<title>Intruder task</title>
<p>Overall, both age groups performed better than chance (all p&lt;.001) at detecting the intruder, with an average response time of 1.03s in adults, 1.36s in children, for the easy condition, and 0.79s in adults in the hard condition. Both error rates and response times are modulated by the reference shape (all p &lt;.001), and both are correlated with error rate data from outside the scanner (all p&lt;.05).</p>
</sec>
</sec>
<sec id="s13b3">
<title>Intruder task in MEG Participants</title>
<p>Participants recruited for the MEG studied performed no behavioral task inside the MEG, as we relied on a passive presentation oddball paradigm. After the scanner session, participants took one run of the intruder detection task previously used online and described in (<xref ref-type="bibr" rid="c8">8</xref>) – we presented them with the task after the scanning session to avoid biasing participants toward actively looking for intruders in the oddball paradigm. At the group level, the data fully replicated the geometric regularity effect (<xref ref-type="bibr" rid="c8">8</xref>) (correlation of error rate with that of previous participants, regression over 11 shapes: r<sup>2</sup>=.90, p&lt;.001). A mixed effect model correlating the error rates of the two groups, with both the slope and the intercept as separate random effects, yielded an intercept not significantly different from 0 (p=.42), and a slope significantly different from 0 (p&lt;1e-10) and not significantly different from 1 (p=.19) suggesting that the data was similar in the two groups. We also correlated each participant’s average error rate per shape to the group data from our previously dataset (<xref ref-type="bibr" rid="c8">8</xref>). A one-tailed test for a positive slope indicated that 19 out of 20 participants displayed a significant geometric regularity effect. We still included the data from the participant that did not display a significant effect, as we had not decided on such a rejection criterion beforehand.</p>
<sec id="s13b3a">
<title>fMRI contrasts for geometric shape in the visual localizer</title>
<p>In order to isolate the brain responses to geometric shapes, we focused on the simplest possible contrast, i.e. greater activation to the presentation of a single geometric shape versus all of its single-image controls (face, house, tool). This contrast is presented in <xref rid="fig2" ref-type="fig">Figure 2C</xref>. Note that we excluded Chinese characters from this comparison because they often include geometric features (e.g. parallel lines), but including them gave virtually identical results (compare <xref rid="figs3" ref-type="fig">Fig.S3A</xref> and <xref rid="figs3" ref-type="fig">Fig.S3B</xref>, identical list of cluster-level corrected clusters at the p&lt;.05 level in both age groups). We also included in the design rows of three distinct geometric shapes (e.g. square, triangle, circle). Our logic here was that this condition, although somewhat artificial from the geometric viewpoint, could be very tightly matched with two other conditions, namely a string of three letters (“words” condition, e.g. BON) or a small 3-symbol mathematical operation (“numbers” condition, e.g. 3+1). However, the corresponding contrast (3 geometric shapes &gt; words and numbers) did not give any positive activation: no positive cluster was significant at the whole brain cluster-level corrected p&lt;.05 level in either age group; additionally, none of the ROIs identified in the single shape contrast was significant for this contrast: aIPS left, p=.975 for adults and p=.389 for children; aIPS right, p=.09 in both adults and children; pITG right, p=.13 in adults and p=.362 in children). We reasoned, however, that numbers should be excluded from this contrast because, by our very hypothesis, geometric shapes should activate a number-based program-like representations (e.g. square = repeat(4){line, turn}) (<xref ref-type="bibr" rid="c12">12</xref>). When restricting the contrast to “3 geometric shapes &gt; words”, at the whole brain level, no cluster reached significance at the p&lt;.05 level (cluster-level correction). However, in this case, testing the ROIs identified in the single shape condition revealed that while the left aIPS still did not reach significance at the p&lt;.05 level (p=.71 in adults, p=.30 in children), both the right aIPS and the right pITG reached significance (aIPS right: p&lt;.001 in adults and p=.014 in chidren; pITG, p=.008 in adults and p=.046 in children).</p>
</sec>
</sec>
<sec id="s13b4">
<title>Additional ROI analysis of the ventral pathway in fMRI (<xref rid="figs4" ref-type="fig">Fig.S4</xref>)</title>
<p>We used individually defined regions of interest (ROIs) to probe the fMRI response to geometric shapes in four cortical regions defined by their preferential responses to four other visual categories known to be selectively processed in the ventral visual pathway: faces, houses, tools and words. To this aim, we first identified, at the group level, clusters of voxels associated to each visual category. Such clusters were identified using a non-parametric permutation test across participants at the whole-brain level using a contrast for the target category against the mean of the other three (voxel p&lt;.001 then clusters p&lt;.05 except for ffa in children, p=.09, and the absence of a well-identified vwfa in children). Significant clusters which intersect the MNI coordinate z=-14 are shown in <xref rid="figs4" ref-type="fig">Fig.S4</xref>; in the case of the fusiform face area (FFA) and the visual word form area (VWFA), we restricted ourselves to clusters at MNI coordinates typically found in the literature, respectively (−45, −57, −14) and (40, −55, −14).</p>
<p>Then, within each such ROI identified in adults, we identified for each subject, including children, the 10% most responsive subject-specific voxels in the same contrast used to identify the cluster. To avoid double-dipping, we selected the voxels using the contrast from a single run, then collected the fMRI responses (beta coefficients) to all categories from the other runs, and then replicated this procedure across all runs while averaging the responses to a given category. The average coefficients within each such individually-defined cortical ROI are shown in <xref rid="figs4" ref-type="fig">Fig.S4</xref>, separately for children and adults. Several observations are in order, and detailed below for each visual category.</p>
<p>In the left-hemispheric VWFA, we can see that voxels are indeed responsive to written words in the participants’ language (French), more than to an unknown language (Chinese), in both adults and children (paired t-tests, p&lt;.001 in adults, p=.003 in children). VWFA voxels also responded to the symbolic display of numbers entering into small computations (e.g. 3+1) in adults, but this response did not appear to be developed yet in children. VWFA voxels also showed a response to tools, particularly in young children, as already been reported (<xref ref-type="bibr" rid="c105">105</xref>). In the opposite direction, they were particularly under-activated by houses in adults. Finally, and most crucially, geometric shapes, whether presented alone or in a string of 3, did not elicit a strong response, indeed no stronger than non-preferred categories such as Chinese characters or faces (p=.37 in adults, p=.057 in children on a one-tailed paired t-test).</p>
<p>In the right-hemispheric FFA, we only saw a purely selective response to faces in both adults and children. All the other visual categories yielded an equally low level of activity in this area. In particular, the responses to geometric shapes were not significantly different from those to other visual categories.</p>
<p>In the bilateral ROIs responsive to tools, a very similar result was found: apart from their strong response to tools and their slightly increased activation to houses and reduced activations to written words and numbers in the right hemisphere, these voxels elicit activations that were essentially equal across all non-preferred visual categories, with geometric shapes being no exception.</p>
<p>Finally, bilateral house-responsive ROIs corresponding to the parahippocampal place area (PPA) were also mildly responsive to tools in both populations and both hemispheres. However, geometric shapes did not activate these clusters more than other visual categories such as faces (both hemispheres, both age groups have p&gt;.05 in one-tailed paired t-tests).</p>
</sec>
<sec id="s13b5">
<title>Additional analysis: Evoked Reponse Potentials (ERP)</title>
<p>Different participants can in principle end up with very different decoder weights, since the decoders are trained independently for each participant. Several of our analyses are based on the decoders’ performance only, and therefore the decoder’s weights associated to each sensor are not considered. To stay closer to the MEG data, we replicated the previous analysis directly from evoked potential data in the gradiometers. Using spatiotemporal permutation testing, we identified a set of sensors ant timepoints across participants where the reference and the oddball epochs were significantly different. We identified three significant clusters: [268, 844]ms, [440, 896]ms and [492, 896]ms.</p>
<p>Then we computed the average difference between the reference and the oddball trials across these sensors separately for each shape. Finally, we correlated the differences with the number of geometric features in the reference shape. The first cluster, from 268 ms to 844 ms, did not elicit any significant correlation with the geometric regularity; however, the two others yielded significant clusters at the p&lt;.05 level, although with later timing than the decoding analysis, at around ∼600ms.</p>
</sec>
<sec id="s13b6">
<title>MEG: Joint MEG-fMRI RSA</title>
<p>Representational similarity analysis also offers a way to directly compared similarity matrices measured in MEG and fMRI, thus allowing for fusion of those two modalities and tentatively assigning a “time stamp” to distinct MRI clusters (<xref ref-type="bibr" rid="c137">137</xref>). However, we did not attempt such an analysis here for several reasons. First, distinct tasks and block structures were used in MEG and fMRI. Second, a smaller list of shapes was used in fMRI, as imposed by the slower modality of acquisition. Third, our study was designed as an attempt to sort out between two models of geometric shape recognition. We therefore focused all analyses on this goal, which could not have been achieved by direct MEG-fMRI fusion, but required correlation with independently obtained model predictions.</p>
</sec>
</sec>
<sec id="s13c">
<title>Supplementary Figures</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S1.</label>
<caption><title>Additional CNN encoding models of human behavior.</title> <p><bold>A</bold>, Correlation matrix between all pairs of RDMs generated by each CNN and layer considered. <bold>B,</bold> Replication of <xref rid="fig1" ref-type="fig">Fig.1D</xref> with different CNNs. Stars indicate a significant difference between the geometric feature model and the respective CNN encoding model (p&lt;0.001). t and p values also indicate whether the CNN encoding model is a significant predictor of participant’s behavior (the geometric feature model is always highly significant). <bold>C</bold> Replication of <bold>B</bold> using different layers of CORnet, organized from early to late layers, from left to right. Note that the late layers are much more significant predictors of human behavior than the early ones – although still far inferior to the geometric feature model.</p></caption>
<graphic xlink:href="584141v2_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S2.</label>
<caption><title>Overview of the stimuli used for the category localizer.</title> <p><bold>A</bold>, Average pixel value (left) and average standard deviation across pixels (right) for stimuli within each category (y axis). An ANOVA indicated no significant effect of the stimulus category on either the average or the standard deviation across pixels. <bold>B</bold>, Average (top) and max (bottom) pixel value at each location across the eight possible visual categories used in the localizer.</p></caption>
<graphic xlink:href="584141v2_figs2rev.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S3.</label>
<caption><title>Details of the fMRI results in children (complement to <xref rid="fig2" ref-type="fig">Fig.2</xref> and <xref rid="fig3" ref-type="fig">Fig.3</xref> in the main text).</title> <p><bold>A:</bold> Statistical map associated with the contrast “single geometric shape &gt; faces, houses and tools”, projected on an inflated brain (top: adults; bottom: children; for illustration purpose we display the uncorrected statistical map at the p&lt;.01 level). Notice how similar the activations are in both age groups. <bold>B:</bold> Same as A, but for the contrast “single geometric shape &gt; all single-object visual categories (face, house, tools, Chinese characters)”. The activation maps are very similar to the previous contrast, and very similar across age groups. <bold>C</bold>: Whole brain correlation of the BOLD signal with geometric regularity in children, as measured by the error rate in a previous online intruder detection task (<xref ref-type="bibr" rid="c8">8</xref>). Positive correlations are shown in red and negative ones in blue. Voxel threshold p&lt;.001, no correction for multiple comparisons, but the p-value indicates the only cluster that was significant at the cluster-level corrected p&lt;.05 threshold. <bold>D</bold>, Results of RSA analysis in children. No cluster was significant at the p&lt;.05 level for the geometric feature models; one right-lateralized occipital cluster reached significance for the CNN encoding model (cluster-level corrected p=.019), and its symmetrical counterpart was close to the significance threshold (cluster-level corrected p=.062)</p></caption>
<graphic xlink:href="584141v2_figs3rev.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S4.</label>
<caption><title>fMRI response of subject-specific voxels in the ventral visual pathway to geometric shapes and other visual stimuli.</title> <p>The brain slices show the group-level clusters associated to various contrasts known to elicit a selective response in the ventral visual pathway, in both age groups: VWFA (words &gt; others; green), FFA (faces &gt; others; purple), tool-selective ROIs (tools &gt; others; red) and PPA (houses &gt; others; light blue). Within each ROI, plots show the mean beta coefficients for the BOLD effect within a subject-specific selection of the 10% best voxels, using independent runs for selection and plotting to avoid circularity and “double-dipping”.</p></caption>
<graphic xlink:href="584141v2_figs4rev.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S5.</label>
<caption><title>Additional Models: Behavior and MEG.</title> <p><bold>A.</bold> Correlation between several models and the average RDM across participants. In particular, we have added the last two layers of DINOv2 (<xref ref-type="bibr" rid="c46">46</xref>) as well as two different implementation of distances in skeletal spaces (<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>) <bold>B.</bold> <xref rid="fig1" ref-type="fig">Fig. 1D</xref> with the symbolic model replaced with the empirical RDM obtained from the last layer of DINOv2 using Euclidean distance. <bold>C.</bold> <xref rid="fig4" ref-type="fig">Fig. 4C</xref> with the same replacement of the symbolic model with the last layer of DINOv2. <bold>D.</bold> <xref rid="fig4" ref-type="fig">Fig. 4D</xref> with the same replacement of the symbolic model with the last layer of DINOv2.</p></caption>
<graphic xlink:href="584141v2_figs5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S6.</label>
<caption><title>Additional Models: fMRI. t-values inside significant clusters at the p&lt;.05 level for four models: geometric features (top left), CNN encoding (top right), DINOv2 last layer (bottom left) and skeletal representations from (<xref ref-type="bibr" rid="c30">30</xref>) (bottom right).</title> <p>Skeletal representations from (<xref ref-type="bibr" rid="c29">29</xref>) did not yield any significant clusters in adults. In children (bottom), only the DINOv2 elicited any significant cluster.</p></caption>
<graphic xlink:href="584141v2_figs6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Table S1.</label>
<caption><title>Coordinates and characteristics of significant fMRI clusters responding to geometric shapes in localizer runs</title></caption>
<graphic xlink:href="584141v2_tbls1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls2" orientation="portrait" position="float">
<label>Table S2.</label>
<caption><title>Coordinates and characteristics of significant fMRI clusters in the RSA analysis</title></caption>
<graphic xlink:href="584141v2_tbls2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106464.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Groen</surname>
<given-names>Iris</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Amsterdam</institution>
</institution-wrap>
<city>Amsterdam</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study provides converging results from complementary neuroimaging and behavioral experiments to identify human brain regions involved in representing regular geometric shapes. Geometric shape concepts are universally present across diverse human cultures and possibly essential for unique human capabilities such as numerical cognition and symbolic reasoning, and identifying the brain networks involved in geometric shape representation is of broad interest to researchers studying human visual perception, reasoning, and cognitive development. The provided experimental evidence regarding the presence of geometric shape regularity representation in dorsal parietal and prefrontal cortex is <bold>solid</bold>, but the claimed link with mathematical reasoning, the influence of experimental tasks, and the role of experience in driving geometric shape representation in both humans and artificial vision models require further elucidation.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106464.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper examines how geometric regularities in abstract shapes (e.g., parallelograms, kites) are perceived and processed in the human brain. The manuscript contains multimodal data (behavior, fMRI, MEG) from adults and additional fMRI data from 6-year-old children. The key findings show that (1) processing geometric shapes lead to reduced activity in ventral areas in comparison to complex stimuli and increased activity in intraparietal and inferior temporal regions, (2) the degree of geometric regularity modulates activity in intraparietal and inferior temporal regions, (3) similarity in neural representation of geometric shapes can be captured early by using CNN models and later by models of geometric regularity. In addition to these novel findings, the paper also includes a replication of behavioral data, showing that the perceptual similarity structure amongst the geometric stimuli used can be explained by a combination of visual similarities (as indexed by a feedforward CNN model of the ventral visual pathway) and geometric features.</p>
<p>Strengths:</p>
<p>(1) The study incorporates multi-modal data that uses more than one task and different populations of participants (adults and children).</p>
<p>(2) It replicates behavioral findings of an earlier study in a larger cohort.</p>
<p>(3) The paper comes with openly accessible code in a well-documented GitHub repository, and the data will be published with the paper on OpenNeuro.</p>
<p>Weaknesses:</p>
<p>I wonder how task difficulty and linguistic labels interact with the current findings. Based on the behavioral data, shapes with more geometric regularities are easier to detect when surrounded by other shapes. Do shape labels that are readily available (e.g., &quot;square&quot;) help in making accurate and speedy decisions? Can the sensitivity to geometric regularity in intraparietal and inferior temporal regions be attributed to differences in task difficulty? Similarly, are the MEG oddball detection effects that are modulated by geometric regularity also affected by task difficulty?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106464.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The current study seeks to understand the neural mechanisms underlying geometric reasoning. Using fMRI with both children and adults, the authors found that contrasting simple geometric shapes with naturalistic images (faces, tools, houses) led to responses in the dorsal visual stream, rather than ventral regions that are generally thought to represent shape properties. The authors followed up on this result using computational modeling and MEG to show that geometric properties explain distinct variance in the neural response beyond what is captured by a CNN.</p>
<p>Strengths:</p>
<p>These findings contribute much-needed neural and developmental data to the ongoing debate regarding shape processing in the brain and offer additional insights into why CNNs may have difficulty with shape processing. The motivation and discussion for the study are appropriately measured, and I appreciate the authors' use of multiple populations, neuroimaging modalities, and computational models to explore this question.</p>
<p>Weaknesses:</p>
<p>Given that the primary take away from this study is that geometric shape information is found in the dorsal stream, rather than the ventral stream there is very little there is very little discussion of prior work in this area (for reviews, see Freud et al., 2016; Orban, 2011; Xu, 2018). Indeed, there is extensive evidence of shape processing in the dorsal pathway in human adults (Freud, Culham, et al., 2017; Konen &amp; Kastner, 2008; Romei et al., 2011), children (Freud et al., 2019), patients (Freud, Ganel, et al., 2017), and monkeys (Janssen et al., 2008; Sereno &amp; Maunsell, 1998; Van Dromme et al., 2016), as well as the similarity between models and dorsal shape representations (Ayzenberg &amp; Behrmann, 2022; Han &amp; Sereno, 2022).</p>
<p>The presence of activation in aIPS led the authors to interpret their results to mean that geometric reasoning draws on the same processes as mathematical thinking. However, there is not enough evidence in the current study to support this claim.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106464.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors report converging evidence from several brain-imaging techniques that geometric figures, notably quadrilaterals, are processed differently in visual (lower activation) and spatial (greater) areas of the human brain than representative figures. Comparison of mathematical models to fit activity for geometric figures shows the best fit for abstract geometric features like parallelism and symmetry. The brain areas active for geometric figures are also active in processing mathematical concepts, even in blind mathematicians, linking geometric shapes to abstract math concepts. The effects are stronger in adults than in 6-year-old Western children. Similar phenomena do not appear in great apes, suggesting that this is uniquely human and developmental.</p>
<p>Strengths:</p>
<p>Multiple converging techniques of brain imaging and testing of mathematical models. Careful reasoning at every step of research and presentation of research, anticipating and addressing possible reservations. Connecting these findings to other findings, brain, behavior, and historical/anthropological, to suggest broad and important fundamental connections between abstract visual-spatial forms and mathematical reasoning, further suggesting visual-spatial origins of mathematical reasoning.</p>
<p>Weaknesses:</p>
<p>Perhaps the manuscript could emphasize that the areas recruited by geometric figures but not objects are spatial, with reduced processing in visual areas. It also seems important to say that the images of real objects are interpreted as representations of 3D objects, as they activate the same visual areas as real objects. By contrast, the images of geometric forms are not interpreted as representations of real objects but rather perhaps as 2D abstractions. The authors use the term &quot;symbolic.&quot; That use of that term could usefully be expanded here.</p>
<p>Pigeons have remarkable visual systems. According to my fallible memory, Herrnstein investigated visual categories in pigeons. They can recognize individual people from fragments of photos, among other feats. I believe pigeons failed at geometric figures and also at cartoon drawings of things they could recognize in photos. This suggests they did not interpret line drawings of objects as representations of objects.</p>
<p>Categories are established in part by contrast categories; are quadrilaterals, triangles, and circles different categories?</p>
<p>It would be instructive to investigate stimuli that are on a continuum from representational to geometric, e.g., table tops or cartons under various projections, or balls or buildings that are rectangular or triangular. Building parts, inside and out. like corners. Objects differ from geometric forms in many ways: 3D rather than 2D, more complicated shapes, and internal texture. The geometric figures used are flat, 2-D, but much geometry is 3-D (e. g. cubes) with similar abstract features. The feature space of geometry is more than parallelism and symmetry; angles are important, for example. Listing and testing features would be fascinating. Similarly, looking at younger or preferably non-Western children, as Western children are exposed to shapes in play at early ages.</p>
<p>What in human experience but not the experience of close primates would drive the abstraction of these geometric properties? It's easy to make a case for elaborate brain processes for recognizing and distinguishing things in the world, shared by many species, but the case for brain areas sensitive to processing geometric figures is harder. The fact that these areas are active in blind mathematicians and that they are parietal areas suggests that what is important is spatial far more than visual. Could these geometric figures and their abstract properties be connected in some way to behavior, perhaps with fabrication and construction as well as use? Or with other interactions with complex objects and environments where symmetry and parallelism (and angles and curvature--and weight and size) would be important? Manual dexterity and fabrication also distinguish humans from great apes (quantitatively, not qualitatively), and action drives both visual and spatial representations of objects and spaces in the brain. I certainly wouldn't expect the authors to add research to this already packed paper, but raising some of the conceptual issues would contribute to the significance of the paper.</p>
</body>
</sub-article>
</article>