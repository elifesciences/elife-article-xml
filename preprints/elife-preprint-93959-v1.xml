<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">93959</article-id>
<article-id pub-id-type="doi">10.7554/eLife.93959</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.93959.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The asymmetric transfers of visual perceptual learning determined by the stability of geometrical invariants</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0003-4429-8443</contrib-id>
<name>
<surname>Yang</surname>
<given-names>Yan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhuo</surname>
<given-names>Yan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5909-3884</contrib-id>
<name>
<surname>Zuo</surname>
<given-names>Zhentao</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0005-1005-5106</contrib-id>
<name>
<surname>Zhuo</surname>
<given-names>Tiangang</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chen</surname>
<given-names>Lin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<aff id="a1"><label>1</label><institution>State Key Laboratory of Brain and Cognitive Science, Institute of Biophysics, Chinese Academy of Sciences</institution>, Beijing 100101, <country>China</country></aff>
<aff id="a2"><label>2</label><institution>Hefei Comprehensive National Science Center, Institute of Artificial Intelligence</institution>, Hefei 230088, <country>China</country></aff>
<aff id="a3"><label>3</label><institution>University of Chinese Academy of Sciences, Chinese Academy of Sciences</institution>, Beijing 100049, <country>China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>SP</surname>
<given-names>Arun</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label><bold>For correspondence:</bold> <email>tgzhou@ibp.ac.cn</email> (T. Zhou); <email>zuozt@ibp.ac.cn</email> (Z. Zuo)</corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-02-28">
<day>28</day>
<month>02</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP93959</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-11-28">
<day>28</day>
<month>11</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-01-03">
<day>03</day>
<month>01</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.01.02.573923"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Yang et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Yang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-93959-v1.pdf"/>
<abstract>
<title>Abstract</title><p>We could recognize the dynamic world quickly and accurately benefiting from extracting invariance from highly variable scenes, and this process can be continuously optimized through visual perceptual learning. It is widely accepted that more stable invariants are prior to be perceived in the visual system. But how the structural stability of invariants affects the process of perceptual learning remains largely unknown. We designed three geometrical invariants with varying levels of stability for perceptual learning: projective (e.g., collinearity), affine (e.g., parallelism), and Euclidean (e.g., orientation) invariants, following the Klein’s Erlangen program. We found that the learning effects of low-stability invariants could transfer to those with higher stability, but not vice versa. To uncover the mechanism of the asymmetric transfers, we used deep neural networks to simulate the learning procedure and further discovered that more stable invariants were learned faster. Additionally, the analysis of the network’s weight changes across layers revealed that training on less stable invariants induced more changes in lower layers. These findings suggest that the process of perceptual learning in extracting different invariants is consistent with the Klein hierarchy of geometries and the relative stability of the invariants plays a crucial role in the mode of learning and generalization.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>In order to adapt to a continually evolving and changing environment, the organism must identify information that remains consistent and stable in dynamic changes (<bold><italic><xref ref-type="bibr" rid="c18">Gold and Stocker, 2017</xref></italic></bold>). Through the process of visual perceptual learning (VPL), the visual system acquires an increased ability to extract meaningful and structured information from the environment to guide decisions and actions adaptively as the result of experience <bold><italic>(<xref ref-type="bibr" rid="c2">Adolph and Kretch, 2015</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c14">Gibson and Pick, 2000</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c12">Gibson, 1969</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c19">Gold and Watanabe, 2010</xref></italic></bold>). The “information” mentioned above can be understood as invariance preserved under transformation in perception <bold><italic>(<xref ref-type="bibr" rid="c17">Gibson, 1979</xref></italic></bold>).</p>
<p>It is widely accepted that different kinds of invariant properties hold distinct ecological significance and possess different levels of utility in perception <bold><italic>(<xref ref-type="bibr" rid="c6">Buccella, 2021</xref></italic></bold>). According to Klein’s Erlangen Program <bold><italic>(<xref ref-type="bibr" rid="c24">Klein, 1893</xref></italic></bold>), a geometrical property is considered as an invariant preserved over a corresponding shape-changing transformation, the more general a transformation group, the more fundamental and stable the geometrical invariants over this transformation group. Stratify geometrical invariants in ascending order of stability: Euclidean geometry, affine geometry, and projective geometry. A fairly large set of experimental results collected within a variety of paradigms have converged at the conclusion that the relative perceptual salience and priority of different attributes of an object may be systematically related to their structural stability under change in a manner that is similar to the Klein hierarchy of geometries: the more stable the attributes, the higher their perceptual salience and priority <bold><italic>(<xref ref-type="bibr" rid="c9">Chen, 2005</xref></italic></bold>, <bold><italic>1985</italic></bold>, <bold><italic>1982</italic></bold>; <bold><italic><xref ref-type="bibr" rid="c39">Todd et al., 2014</xref></italic></bold>, <bold><italic>1998</italic></bold>). However, it is remains unknown that whether the learning of invariants with different stability follows a certain pattern, and this is the major focus of our research. To accomplish this, we must first grasp the characteristics of VPL.</p>
<p>According to Gibson’s differentiation view <bold><italic>(<xref ref-type="bibr" rid="c16">Gibson and Gibson, 1955</xref></italic></bold>), perceptual learning is a process of “differentiating previously vague impressions” and whereby perceptual information becomes increasingly specific to the stimuli in the world. They suggested that learning as differentiation is the discovery and selective processing of the information most relevant to a task, and this includes discovery of “higher-order invariants” that govern some classification and filter out irrelevant information.</p>
<p>A hallmark of VPL is its specificity for the basic attributes of the trained stimulus and task in the past for a long time <bold><italic>(<xref ref-type="bibr" rid="c10">Crist et al., 1997</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c11">Fiorentini and Berardi, 1981</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c21">Hua et al., 2010</xref></italic></bold>). Recent studies have challenged the specificity of learned improvements and demonstrated transfer effects between stimuli <bold><italic>(<xref ref-type="bibr" rid="c27">Liu and Weinshall, 2000</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c36">Sowden et al., 2002</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c44">Zhang et al., 2010</xref></italic></bold>), location <bold><italic>(<xref ref-type="bibr" rid="c22">Hung and Seitz, 2014</xref></italic></bold>) and substantially different tasks <bold><italic>(<xref ref-type="bibr" rid="c31">McGovern et al., 2012</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c38">Szpiro and Carrasco, 2015</xref></italic></bold>). To be of practical utility, the generalization of learning effects should be a research focus, and understanding the determinants of specificity and transfer remains one of the large outstanding questions in field. Ahissar and Hochstein uncovered the relationship between task difficulty and transfer effects <bold><italic>(<xref ref-type="bibr" rid="c3">Ahissar and Hochstein, 1997</xref></italic></bold>), leading to the formulation of the reverse hierarchy theory (RHT) which suggests that VPL is a top-down process that originates from the top of the cortical hierarchy and gradually progresses downstream to recruit the most informative neurons to encode the stimulus <bold><italic>(<xref ref-type="bibr" rid="c4">Ahissar and Hochstein, 2004</xref></italic></bold>). More specifically, the level at which learning occurs is related to the difficulty of the task, easier tasks were learned at higher levels and showed more transfer, than did harder tasks. Extending this work, Jeter and colleagues <bold><italic>(<xref ref-type="bibr" rid="c23">Jeter et al., 2009</xref></italic></bold>) demonstrated that the precision of the trained stimuli, and not task difficulty per se, was the critical factor determining transfer, and that learning in fact transferred to low-precision tasks but not to high-precision tasks.</p>
<p>The research described in the present article concerns the very nature of form perception, trying to explore whether VPL of geometrical invariants with various stability also exhibit hierarchical relationships and what a priori rule defines the mode of learning and generalization. Our research focuses on the VPL of different geometrical properties in the Klein hierarchy of geometries: projective property (e.g., collinearity), affine property (e.g., parallelism), and Euclidean property (e.g., orientation). We developed two psychophysical experiments assessing how the structural stability of geometrical properties affect the learning effect, meanwhile investigating the transfer effect between different levels of geometrical invariants. To explore the relationship between behavioral learning and plasticity across the visual hierarchy during VPL, we perform an experiment in a deep neural network (DNN) that recapitulates several known VPL phenomena <bold><italic>(<xref ref-type="bibr" rid="c42">Wenliang and Seitz, 2018</xref></italic></bold>). The network reproduced the behavior results and further unveiled the learning speeds and layer changes associated with the stability of invariants. We will then interpret the results based on the Klein hierarchy of geometries and RHT.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Asymmetric transfer effect: The learning effect consistently transferred from low-stability to high-stability invariants</title>
<p>The paradigm of “configural superiority effects” with reaction time measures Forty-four right-handed healthy subjects participated in Experiment 1. We randomly assigned subjects into three groups trained with one invariant discrimination task: the collinearity (colli.) training group (n = 15), the parallelism (para.) training group (n = 15) and the orientation (ori.) training group (n = 14). The paradigm of “configural superiority effects” <bold><italic>(<xref ref-type="bibr" rid="c9">Chen, 2005</xref></italic></bold>) (faster and more accurate detection/discrimination of a composite display than of any of its constituent parts) was adapted to measure the short-term perceptual learning effects of different levels of invariants. As illustrated in <xref ref-type="fig" rid="fig1">Figure 1A</xref>, subjects performed the odd-quadrant discrimination task in which they were asked to report which quadrant differs from the other three as fast as possible on the premise of accuracy. During the test phases before and after training (Pre-test and Post-test, <bold><italic><xref rid="fig1" ref-type="fig">Figure 1B</xref></italic></bold>), subjects performed the three invariant discrimination tasks (colli., para., ori.) at three blocks respectively, the response times (RTs) were recorded to measure the learning effects. To distinguish VPL from programmed learning due to motor learning, a color discrimination task, served as baseline training, were performed before the main experiment <bold><italic>(<xref rid="fig1" ref-type="fig">Figure 1B</xref></italic></bold>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Examples of stimulus arrays for each task and the procedure in Experiment 1. (A) Stimulus examples of the collinearity discrimination task (left), the parallelism discrimination task (middle), the orientation discrimination task (right). (B) The procedure of Experiment 1.</p></caption>
<graphic xlink:href="573923v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>First of all, to investigate if there was any speed-accuracy trade-off, one-way repeated measures analysis of variance (ANOVA) with task as within-subject factors was conducted on the accuracies collected in the Pre-test phase. A significant main effect of the task was found (F(2, 129) = 7.977, p = 0.0005, <inline-formula><inline-graphic xlink:href="573923v1_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> = 0.110). We performed further post-hoc analysis carrying out paired t-test with FDR correction to examine the differences of accuracies between each pair of tasks. As a result, the accuracies of the collinearity task were significantly higher than that in the parallelism task (t(43) = 5.443, p &lt; 0.0001, Hedge’s g = 0.917), and that in the orientation task (t(43) = 4.351, p = 0.0001, Hedge’s g = 0.574). There was no difference in accuracy between the parallelism and orientation task (t(43) = 1.535, p = 0.132, Hedge’s g = 0.214). Then the same analysis was applied to the RTs in the three tasks prior to training. A significant main effect of the task was found in ANOVA (F(2, 129) = 59.557, p &lt; 0.0001, <inline-formula><inline-graphic xlink:href="573923v1_inline1a.gif" mimetype="image" mime-subtype="gif"/></inline-formula> = 0.480). As shown from the post-hoc test, the RTs of collinearity task were significantly faster than that in the parallelism task (t(43) = 13.374, p &lt; 0.0001, Hedge’s g = 1.945), and that in the orientation task (t(43) = 13.333, p &lt; 0.0001, Hedge’s g = 2.295). The RT of the parallelism task was faster than that of orientation task (t(43) = 4.179, p &lt; 0.0001, Hedge’s g = 0.416). Taken together, the collinearity task has the highest accuracy as well as the faster RT among the three tasks, showing no speed-accuracy trade-off in Experiment 1. What’s more, the results before training were in line with the prediction from the Klein hierarchy of geometries, which suggest that the more stable invariants possessed higher detectability, resulting in better task performance. The statistical results of the accuracies in Post-test didn’t differ from that in Pre-test <bold><italic>(<xref rid="fig1" ref-type="fig">Figure 2— figure Supplement 1</xref></italic></bold>). In the following analysis, only correct trials were used.</p>
<p>The second analysis conducted was to assess whether there were learning effects of the trained tasks and transfer effects to the untrained tasks. This was assessed by examining the RTs of Pre- test and Post-test for each geometrical property discrimination task. One-tailed, paired sample t-test was performed to do this. For the collinearity training group, significant learning effect was found (t(14) = 3.911, p = 0.0008, Cohen’s d = 0.457), but there was no transfer to the other two untrained task <bold><italic>(<xref ref-type="fig" rid="fig2">Figure 2A</xref></italic></bold>). The parallelism training group show significant learning effect (t(14) = 5.169, p &lt; 0.0001, Cohen’s d = 1.095), and also show substantial improvement in the collinearity task (t(14) = 2.753, p = 0.008, Cohen’s d = 0.609) <bold><italic>(<xref ref-type="fig" rid="fig2">Figure 2B</xref></italic></bold>). Moreover, performances of all three task were improved after training on orientation discrimination task: the collinearity task (t(13) = 4.033, p = 0.0007, Cohen’s d = 0.800), the parallelism task (t(13) = 3.482, p = 0.002, Cohen’s d = 0.631), and the orientation task (t(13) = 4.693, p = 0.0002, Cohen’s d = 1.048) <bold><italic>(<xref ref-type="fig" rid="fig2">Figure 2C</xref></italic></bold>). This particular pattern of transfer is interesting given the hierarchical relationship of the three different stimulus configurations. For instance, the performance improvement obtained on the parallelism task transferred to the collinearity task which is more stable, whereas not transferred to the orientation task which is less stable. Similarity, learned improvement in orientation discrimination transferred to more stable tasks, with RTs of both collinearity and parallelism tasks showing significant improvements. However, training on collinearity discrimination which is the most stable among the three tasks exhibited task specificity. These findings indicate that perceptual improvements derived from training on a relatively low-stability form invariant can transfer to those invariants with higher stability, but not vice versa.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Results of Experiment 1, RTs of each discrimination tasks measured at Pre-test and Post-test were compared by one-tailed, paired sample t-test. (A) Results from the group trained on the collinearity task (n=15). Performances of the collinearity task were improved after training (p = 0.0008). (B) Results from the group trained on the parallelism task (n=15). Performances of the collinearity (p = 0.008) and parallelism (p &lt; 0.0001) task were improved after training. (C) Results from the group trained on the orientation task (n=14). Performances of the collinearity (p = 0.0007), parallelism (p = 0.002) and orientation task (p = 0.0002) were improved after training. (***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05). Error bars denote 1 SEM across subjects.</p><p><xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. Accuracies for the three discrimination tasks measured at Pre-test and Post-test.</p><p><xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2.</xref> The learning indexes of the three geometrical invariants in Experiment 1.</p><p><bold>Figure 2—source data 1.</bold> RTs and accuracies at Pre-test and Post-test, and learning indexes in the course of training for each participant.</p></caption>
<graphic xlink:href="573923v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Finally, we assessed whether learning effect differ in different form invariants. To this end, we computed the “ learning index ” (LI) <bold><italic>(<xref ref-type="bibr" rid="c35">Petrov et al., 2011</xref></italic></bold>), which quantifies learning relative to the baseline performance. ANOVA analysis did not find a significant difference in the LIs among the three tasks (F(2, 41) = 2.246, p = 0.119, <inline-formula><inline-graphic xlink:href="573923v1_inline1b.gif" mimetype="image" mime-subtype="gif"/></inline-formula> = 0.100) <bold><italic>(<xref ref-type="fig" rid="fig2">Figure 2—figure Supplement 2</xref></italic></bold>).</p>
<p>What needs to be cautious is that, Experiment 1 with RT measures has limitations that make it difficult to truly compare the time required for processing different invariants. Specifically, our interest lies in understanding how learning affects the process of extracting geometrical invariants. However, in order to make a response, participants also need to locate the differing quadrant. The strength of the grouping effect of the shapes among the four quadrants can affect the speed of the localization process <bold><italic>(<xref ref-type="bibr" rid="c33">Orsten-Hooge et al., 2011</xref></italic></bold>). Additionally, the strength of the grouping effect may vary under different conditions, leading to differences in reaction times that may reflect differences in the extraction time of geometrical invariants as well as the strength of the group effect among the quadrants.</p>
<p>The paradigm of discrimination with threshold measures To overcome the shortcomings of the RT measures, VPL is indexed by the improvements in thresholds of discrimination tasks after training in Experiment 2. We employed the adaptive staircase procedure QUEST <bold><italic>(<xref ref-type="bibr" rid="c41">Watson and Pelli, 1983</xref></italic></bold>) to assess the thresholds. QUEST is a kind of Bayesian adaptive methods which typically produces estimates of psychometric function parameters and converges to a threshold more quickly than conventional staircase procedures. Forty-five healthy subjects participated in Experiment 2, and they were randomly assigned into three groups: the collinearity training group (n = 15), the parallelism training group (n = 15) and the orientation training group (n = 15). The experiment paradigm was adapted from a classical 2-alternative forced choice (2AFC) task, in which subjects were required to judge which of two simultaneously presented stimuli was the “target” <bold><italic>(<xref ref-type="fig" rid="fig3">Figure 3</xref></italic></bold>, <bold><italic><xref rid="fig2" ref-type="fig">Figure 4</xref></italic></bold> and <bold><italic><xref rid="fig3s1" ref-type="fig">Figure 3—figure Supplement 1</xref></italic></bold>). The &quot;target&quot; referred to the pair of non-collinear lines for the colli. task, the pair of unparallel lines for the para. task, and the more clockwise line for the ori. task. The trials not involved presentation of a “target” were set as catch trials <bold><italic>(<xref ref-type="fig" rid="fig3">Figure 3</xref></italic></bold>). The procedure of Experiment 2 is similar to that of Experiment 1 except for no involvement of the baseline training. For each block, a QUEST staircase was used to adaptively adjust the angle separation (𝜃) of discrimination task within all trials but the catch trials, and provided an estimate of each subject’s 50% correct discrimination threshold.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label><caption><p>Examples of the layout of a stimulus frame. The top line demonstrates trials with a “target” (surrounded by orange dashed box), and the bottom line demonstrates the catch trials without “target”. The blue dashed lines represent the “base” orientation for each stimulus, and 𝜃 is the angle separation of the discrimination task. (A) Stimulus examples of the collinearity (colli.) task, the upper example shows a “target” (a pair of non-collinear lines) located at the lower right quadrant. (B) Stimulus examples of the parallelism (para.) task, the upper example shows a “target” (a pair of unparallel lines) located at the lower right quadrant. (C) Stimulus examples of the orientation (ori.) task, the upper example shows a “target” (the more clockwise line) located at the upper right quadrant.</p><p><xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. Examples of stimuli in Experiment 2.</p></caption>
<graphic xlink:href="573923v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Examples of stimulus arrays for each task and the procedure in Experiment 1. (A) Stimulus examples of the collinearity discrimination task (left), the parallelism discrimination task (middle), the orientation discrimination task (right). (B) The procedure of Experiment 1</p></caption>
<graphic xlink:href="573923v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>One-way repeated measures ANOVA and post-hoc t-test were conducted on the thresholds collected from all three training groups prior to training. A significant main effect of the task was found in ANOVA analysis (F(2, 132) = 25.619, p &lt; 0.0001, <inline-formula><inline-graphic xlink:href="573923v1_inline1c.gif" mimetype="image" mime-subtype="gif"/></inline-formula> = 0.280). As revealed by post-hoc tests, the initial performances of the three tasks were consistent with the relative stability of the invariant they involved, the discrimination threshold of the collinearity task was significantly lower than that of the parallelism task (t(44) = 3.247, p = 0.002, Hedge’s g = 0.595), and that of the orientation task (t(44) = 6.662, p &lt; 0.0001, Hedge’s g = 1.285). The threshold of the parallelism task was lower than that of the orientation task (t(44) = 4.570, p = 0.0001, Hedge’s g = 0.916). Moreover, the accuracies of the three tasks in Pre-test were also submitted to a one-way repeated measures ANOVA and no significant effect was found (F(2, 132) = 0.046, p = 0.955, <inline-formula><inline-graphic xlink:href="573923v1_inline1d.gif" mimetype="image" mime-subtype="gif"/></inline-formula> = 0.001), suggesting no difference in difficulty among the three tasks before training.</p>
<p>One-tailed, paired sample t-test was performed to compare the threshold in Pre-test and Posttest. After orientation discrimination training, the orientation discrimination threshold at Post-test was significantly lower than that at Pre-test (t(14) = 5.527, p &lt; 0.0001, Cohen’s d = 1.516), and the same applied to the collinearity discrimination task, t(14) = 2.740, p = 0.008, Cohen’s d = 0.752, and the parallelism discrimination task, t(14) = 1.949, p = 0.036, Cohen’s d = 0.654 <bold><italic>(<xref ref-type="fig" rid="fig5">Figure 5C</xref></italic></bold>). After parallelism discrimination training, significant improvements were found in the collinearity discrimination task (t(14) = 2.775, p = 0.007, Cohen’s d = 1.013) and the parallelism discrimination task (t(14) = 3.259, p = 0.003, Cohen’s d = 1.192) <bold><italic>(<xref ref-type="fig" rid="fig5">Figure 5B</xref></italic></bold>). And collinearity discrimination training only produced an improvement on its own performance (t(14) = 2.128, p = 0.026, Cohen’s d = 0.759, <bold><italic><xref ref-type="fig" rid="fig5">Figure 5A</xref></italic></bold>). In summary, the pattern of generalization is identical to what was found in Experiment 1, where training of low-stability invariants optimized the perception of high-stability invariants but not vice versa. Just the same as in Experiment 1, no differences were found between the LIs of the three tasks in Experiment 2 (F(2, 42) = 2.875, p = 0.068, <inline-formula><inline-graphic xlink:href="573923v1_inline1e.gif" mimetype="image" mime-subtype="gif"/></inline-formula> = 0.120, <bold><italic><xref rid="fig1" ref-type="fig">Figure 5—figure Supplement 1</xref></italic></bold>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label><caption><p>Results of Experiment 2, Thresholds of each discrimination task measured at Pre-test and Post-test were compared by one-tailed, paired sample t-test. (A) Results from the group trained on the collinearity task (n = 15). Performances of the collinearity task were improved after training (p = 0.026). (B) Results from the group trained on the parallelism task (n = 15). Performances of the collinearity (p = 0.007) and parallelism (p = 0.003) task were improved after training. (C) Results from the group trained on the orientation task (n = 15). Performances of the collinearity (p = 0.008), parallelism (p = 0.036) and orientation task (p &lt; 0.0001) were improved after training. (***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05). Error bars denote 1 SEM across subjects.</p><p><xref ref-type="fig" rid="fig5">Figure 5—figure supplement 1</xref>. The learning indexes of the three geometrical invariants in Experiment 2.</p></caption>
<graphic xlink:href="573923v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Previous studies claimed that transfer of VPL is controlled by the difficulty or precision of the training task <bold><italic>(<xref ref-type="bibr" rid="c3">Ahissar and Hochstein, 1997</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c23">Jeter et al., 2009</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c42">Wenliang and Seitz, 2018</xref></italic></bold>). In this experiment, task difficulty is related to the accuracy, and task precision which is related to the angle separation can be indexed by the threshold. As stated above, prior to training, there was not significant difference among the difficulties (accuracies) of the three tasks, and tasks with higher stability had lower threshold values, resulting in higher precision during training. We showed that the relative stability of invariants determined the transfer effects between tasks even when task difficulty was held constant between tasks, and the particular transfer pattern found in our study (learning from tasks with lower stability and lower precision transferred to the tasks with higher stability and precision) is contrary to the precision-dependent explanation for the generalization of VPL which proposed that training on higher precision can improve performance on lower precision tasks but the reverse is not true <bold><italic>(<xref ref-type="bibr" rid="c23">Jeter et al., 2009</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c42">Wenliang and Seitz, 2018</xref></italic></bold>).</p>
</sec>
<sec id="s2b">
<title>Deep neural network simulations of learning and transfer effects</title>
<sec id="s2b1">
<title>Behavioral results</title>
<p>To gain further insight into the neural basis underlying the asymmetric transfers found in the two psychophysical experiments, in Experiment 3, we repeated Experiment 2 in a DNN for modeling VPL <bold><italic>(<xref ref-type="bibr" rid="c42">Wenliang and Seitz, 2018</xref></italic></bold>). This network, which is derived from the general AlexNet architecture, fulfilled predictions of existing theories (e.g. the RHT) regarding specificity and plasticity and reproduced findings of tuning changes in neurons of the primate visual areas <bold><italic>(<xref ref-type="bibr" rid="c28">Manenti et al., 2023</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c42">Wenliang and Seitz, 2018</xref></italic></bold>). Three networks were trained on the three tasks (colli., para., ori.) respectively, repeated in 12 conditions with varying stimulus parameters.</p>
<p>The performance trajectories of the networks trained with collinearity, parallelism, and orientation discrimination were averaged across the 12 stimulus conditions, and are shown in <bold><italic><xref ref-type="fig" rid="fig6">Figure 6A</xref></italic></bold> respectively. Each network was also tested on the two untrained tasks in the same stimulus condition during the training phase, and the transfer accuracies are presented in <bold><italic><xref ref-type="fig" rid="fig6">Figure 6A</xref></italic></bold> as well. As shown from the final accuracies (the numbers located at the end of each curve), the networks trained on ori. showed greatest transfer effects to the untrained tasks, and the networks trained on colli. showed worst transfer effects.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label><caption><p>Performance of the model when trained under different discrimination tasks. (A) Accuracy trajectories against training iterations from the models trained on collinearity (left), parallelism (middle), and orientation task (right), with the error bar representing 1 SEM. 𝑡<sub>95</sub> is the iteration where the fully plastic network reached 95% accuracy, depicted by green dashed lines. The numbers located at the end of each curve are the final accuracies of the last iteration. (B) The learning speed which was indexed by 𝑡<sub>95</sub> of the three tasks. The learning speed of the collinearity task was faster than the parallelism (p = 0.018) and orientation task (p &lt; 0.0001). The learning speed of the parallelism task was faster than the orientation task (p &lt; 0.0001). Statistical significance was calculated by paired t-test with FDR correction. (***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05). Error bars denote 1 SEM across subjects. (C) Final mean accuracies when the network was trained and tested on all combinations of tasks</p><p><xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. Model structure and stimulus examples in Experiment 3.</p></caption>
<graphic xlink:href="573923v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We then investigate the speeds of learning of the three tasks, we calculated 𝑡<sub>95</sub> <bold><italic>(<xref ref-type="bibr" rid="c42">Wenliang and Seitz, 2018</xref></italic></bold>), the iteration where the fully plastic network reached 95% accuracy, for each task. We found a significant main effect of the training task on 𝑡<sub>95</sub> using the one-way repeated measures ANOVA (F(2, 33) = 144.636, p &lt; 0.0001, <inline-formula><inline-graphic xlink:href="573923v1_inline1f.gif" mimetype="image" mime-subtype="gif"/></inline-formula> = 0.898). Further post-hoc analysis with paired t-test showed that the performance of collinearity discrimination started to asymptote earlier than the parallelism (t(11) = 2.567, p = 0.018, Cohen’s d = 1.012) and the orientation task (t(11) = 13.172, p &lt; 0.0001, Cohen’s d = 5.192). The orientation discrimination had slowest speed of learning, it reached saturation slower than the parallelism task (t(11) = 11.626, p &lt; 0.0001, Cohen’s d = 4.583) <bold><italic>(<xref ref-type="fig" rid="fig6">Figure 6B</xref></italic></bold>). That is to say, learning speed was superior for the invariants with higher stability.</p>
<p><bold><italic><xref ref-type="fig" rid="fig6">Figure 6C</xref></italic></bold> shows the final learning and transfer performance on all combinations of training and test task. A linear regression on the final accuracies showed a significant positive main effect of the stability of test task on the performance (𝛽 = 0.055, t(105) = 2.467, p = 0.015, 𝑅<sup>2</sup> = 0.043), shown as increasing color gradient from top to bottom. We also found that the stability of training task had a significant negative effect (𝛽 = -0.057, t(105) = -2.591, p = 0.011, 𝑅<sup>2</sup> = 0.048), shown as decreasing color gradient from right to left. Overall, consistent with the results in the two psychophysical experiments, these results suggested that transfer is more pronounced from less stable geometrical invariants to more stable invariants than vice versa, shown as higher accuracy on lower-right quadrants compared with top-left quadrants.</p>
</sec>
</sec>
<sec id="s2c">
<title>Distribution of learning across layers</title>
<p>To demonstrate the distribution of learning over different levels of hierarchy, we next examined the time course of learning across the layers, the weight changes for each layer were shown in <bold><italic><xref rid="fig3" ref-type="fig">Figure 7A</xref></italic></bold>. Overall, training on lower-stability geometrical invariants produced greater overall changes. Due to this mismatch of weight initialization, we focus on layers 1–5 with weights initialized from the pre-trained AlexNet.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><p>Layer change under different training tasks. (A) Layer change trajectories during learning. (B) Iteration at which the rate of change peaked (PSI) in layers 1-5. (C) Final layer change in layers 1-5. The error bar representing 1 SEM.</p></caption>
<graphic xlink:href="573923v1_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To characterize learning across layers, we studied when and how much each layer changed during training. First, to quantify when significant learning happened in each layer, we estimated the iteration at which the gradient of a trajectory reached its peak (peak speed iteration, PSI; shown in <xref rid="fig3" ref-type="fig">Figure 7B</xref>). As the result of a linear regression analysis, in layers 1–5, we observed significant negative main effects of the stability of training task (𝛽 = -27.315, t(176) = -6.623, p &lt; 0.0001, 𝑅<sup>2</sup> = 0.072), layer number (𝛽 = -17.327, t(176) = -6.450, p &lt; 0.0001, 𝑅<sup>2</sup> = 0.065) and a positive interaction of the two on PSI (𝛽 = 6.579, t(176) = 5.290, p &lt; 0.0001, 𝑅<sup>2</sup> = 0.115), suggesting that layer change started to asymptote later for lower layers and less stable invariants. For individual tasks, a linear regression analysis showed a significant negative effect of layer number on PSI only in the least stable task, that is the orientation discrimination task (𝛽 = -12.833, t(58) = -4.470, p &lt; 0.0001, 𝑅<sup>2</sup> = 0.243). Therefore, for the discrimination of the least stable invariants, the order of change across layers is consistent with the RHT prediction that higher visual areas change before lower ones <bold><italic>(<xref ref-type="bibr" rid="c4">Ahissar and Hochstein, 2004</xref></italic></bold>, <bold><italic>1997</italic></bold>).</p>
<p>The final layer changes at the end of training for the networks trained on the three tasks are shown in <xref rid="fig3" ref-type="fig">Figure 7C</xref> respectively. A linear regression analysis on the final changes in layers 1–5 revealed significant negative main effects of the stability of training task (𝛽 = -0.037, t(176) = -16.409, p &lt; 0.0001, 𝑅<sup>2</sup> = 0.689), layer number (𝛽 = -0.011, t(176) = -7.655, p &lt; 0.001, 𝑅<sup>2</sup> = 0.0001) and a positive interaction of the two (𝛽 = 0.005, t(176) = 7.329, p &lt; 0.0001, 𝑅<sup>2</sup> = 0.075). However, for individual tasks, a significant negative linear effect of layer number was only found in the orientation discrimination task (𝛽 = -0.008, t(58) = -12.763, p &lt; 0.0001, 𝑅<sup>2</sup> = 0.733). On the contrary, significant positive effect of layer number was found in the parallelism (𝛽 = 0.003, t(58) = 3.701, p = 0.0005, 𝑅<sup>2</sup> = 0.177) and collinearity task (𝛽 = 0.002, t(58) = 2.538, p = 0.014, 𝑅<sup>2</sup> = 0.084). We then calculated the centroid, which is equal to the weighted average of the layer numbers using the corresponding mean final change as the weight. The centroids for the networks trained on colli., para. and ori. are 3.11, 3.14 and 2.77, respectively. These centroids together with the results from linear regression collectively indicate that the least stable task induces more change lower in the hierarchy whereas the two more stable tasks induce change higher in the hierarchy.</p>
<p>Wenliang and Seitz proposed that high-precision training transfers more broadly to untrained and coarse discriminations than low-precision training <bold><italic>(<xref ref-type="bibr" rid="c42">Wenliang and Seitz, 2018</xref></italic></bold>). However, in each stimulus condition of Experiment 3, the angle separations (precisions) in the collinearity and parallelism task were always the same, and were half of the angle separations in the orientation task. So, the pattern of transfer and layer change cannot be explained based on the relative precision of the training and test tasks that was suggested by Wenliang and Seitz. Rather, the relative stability of invariants involved in the tasks provides consistent and reasonable explanation for the asymmetric transfers found in our study.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We find a consistent pattern underlying learning and generalization across the three geometrical invariants in the Klein hierarchy: the less stable invariants have slower learning speeds (revealed by the DNN experiment), and transfer is more pronounced from less stable invariants to more stable invariants than vice versa.</p>
<p>We explain results on the basis of the Klein hierarchy of geometries. First of all, because more stable invariants possess higher perceptual salience and greater ecological significance <bold><italic>(<xref ref-type="bibr" rid="c32">Meng et al., 2019</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c39">Todd et al., 2014</xref></italic></bold>), they are not only perceived earlier but also learned earlier, leading to faster learning speeds. We further propose the hypothesis that learning high-stability invariants must precede the learning of low-stability invariants, so the learners have to go through learning of high-stability invariant discrimination to get to the point where learning of low-stability invariant discrimination is accessible. What’s more, according to the inclusive relationship between the invariants in terms of their structural stability <bold><italic>(<xref ref-type="bibr" rid="c25">Klein, 1941</xref></italic></bold>), the change of a high-stability invariant includes changes of invariants which are less stable than it, thus the improved ability to discriminate low-stability invariants obtained from training could assist in the discrimination of more stable ones. On the other hand, form invariants with high stability are represented in a holistic manner during learning, their individual components are hard to encoded solely. When performing a form discrimination task, the learners extract the most stable invariants without necessarily extracting their embedded invariants which are less stable. In other words, the less stable invariants are ignored and suppressed when they are embedded into more stable configurations, leading to limited generalization to the less stable invariants via training on more stable invariants. Taken together, the generalization of learning is more substantial from low-stability to high-stability invariants.</p>
<p>According to the distribution of learning across layers found in the DNN experiment, for the orientation task with lowest stability, the order of change across layers is consistent with the RHT prediction that higher visual areas change before earlier ones <bold><italic>(<xref ref-type="bibr" rid="c4">Ahissar and Hochstein, 2004</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c20">Hochstein and Ahissar, 2002</xref></italic></bold>), meanwhile, low-stability invariant training induces more change lower in the hierarchy and high-stability training induces more change higher in the hierarchy. Based on these findings, the asymmetric transfers and the distribution of learning across layers can be both explained from the perspective of RHT accompanied by the relative stability of different invariants: VPL is a process that occurs from high-to-low-level visual areas, and from high-to-low-stability invariants. The VPL of high-stability invariants occurs earlier and relies more on higher-level cortical areas, while the learning of low-stability invariants leads to a greater reliance on lower-level cortical areas that have higher resolution for finer and more difficult discriminations but not involved during the learning of high-stability invariants. On account of the feedforward anatomical hierarchy of the visual system <bold><italic>(<xref ref-type="bibr" rid="c29">Markov et al., 2013</xref></italic></bold>), the modifications of lower areas caused by training on low-stability invariants will also affect higher-level visual areas, thus influencing the discrimination of high-stability invariants and resulting in transfer effects. Taken together, discriminating invariants with higher-stability can benefits from VPL of discriminating invariants with lower-stability, but not vice versa.</p>
<p>Returning to Gibson’s theory of perceptual learning, we make a reasonable inference that the Klein hierarchy of geometrics belong to what Gibson referred to as “structure” and “invariant” <bold><italic>(<xref ref-type="bibr" rid="c13">Gibson, 1970</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c37">Szokolszky et al., 2019</xref></italic></bold>). More importantly, the invariants with higher structural stability are equivalent to “higher-order invariants” mentioned by Gibson <bold><italic>(<xref ref-type="bibr" rid="c15">Gibson, 1971</xref></italic></bold>), and they are extracted earlier in both processes of perception and perceptual learning. Learning is a process of differentiation, starting with the extraction of global, more stable invariants and gradually involving local, less stable invariants. During this process, the perceptual system becomes more differentiated, more specific, and better at distinguishing local details. Moreover, such a perceptual system can also utilize local information to improve the performance of discriminating high-stability invariants, leading to the asymmetric transfers observed in our research.</p>
<p>We do not deny that several attributes of task including difficulty and precision could play some roles in generalization and the locus of learning, as mentioned in the Introduction. Broadly speaking, there are consistencies between these attributes and the Klein hierarchy of geometries. For example, the more stable form invariant should be more coarse and easier to discriminate than the less stable one in the same presentation condition. However, our study found consistent learning and transfer pattern with the Klein hierarchy of geometries even after controlling for difficulty and precision in Experiment 2 and 3. So, it seems that the relative stability of form invariants is a more essential and determinant factor underlying the transfer of learning effects in our research.</p>
<p>Due to the employment of short-term perceptual learning paradigms in both psychophysical experiments in our study, it has been challenging to accurately track the temporal processes of learning <bold><italic>(<xref ref-type="bibr" rid="c43">Yang et al., 2022</xref></italic></bold>).It is also possible that the lack of observed differences in learning effects between tasks could be due to insufficient learning in some tasks, considering the differences in learning speed among the three tasks, as well as the possibility that different training tasks may involve different short-term and long-term learning processes <bold><italic>(<xref ref-type="bibr" rid="c1">Aberg et al., 2009</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c30">Mascetti et al., 2013</xref></italic></bold>).</p>
<p>In future research, it may be beneficial to consider employing long-term perceptual learning to investigate the learning rates of discriminating geometrical invariants with variable stability. Additionally, various brain imaging and neurophysiological techniques should be utilized to study the perception and learning of these form invariants, in order to explore their underlying neural mechanisms. This may contribute to our understanding of object recognition, conscious perception and perceptual development.</p>
</sec>
<sec id="s4">
<title>Methods and Materials</title>
<sec id="s4a">
<title>Participants and apparatus</title>
<p>A total of 89 right-handed healthy subjects participated in this study: 44 in Experiment 1 (24 female, mean age 23.70 ± 3.46 years), and 45 in Experiment 2 (26 female, mean age 23.02 ± 3.40 years). All subjects were naïve to the experiment with normal or corrected-to-normal vision. Subjects provided written informed consent, and were paid to compensate for their time. Sample size was determined based on power calculations following a pilot study showing significant learning effect of the collinearity task for effect size of Cohen’s d = 0.728 at 80% power. In each experiment, subjects were randomly assigned to one of three training groups (colli., para., and ori. trainging group). The study was approved by the ethics committee of the Institute of Biophysics at the Chinese Academy of Sciences, Beijing.</p>
<p>Stimuli were displayed on a 24-inch computer monitor (AOC VG248) with a resolution of 1920 × 1080 pixels and a refresh rate of 100 Hz. The experiment was programmed and run in MATLAB (The Mathwork corp, Orien, USA) with the Psychotoolbox-3 extensions <bold><italic>(<xref ref-type="bibr" rid="c5">Brainard, 1997</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c34">Pelli, 1997</xref></italic></bold>). Subjects were stabilized using a chin and head rest with visual distance of 70 cm in a dim ambient light room.</p>
</sec>
<sec id="s4b">
<title>Stimuli and tasks for psychophysics experiments</title>
<p>In Experiment 1, we applied the paradigm of “configural superiority effects” (CSEs) to measure the learning effects. CSEs refer to the findings that configural relations between simple components rather than the components themselves may play a basic role in visual processing and were originally revealed by an odd quadrant task, as illustrated in <xref rid="fig1" ref-type="fig">Figure 1A</xref>. This paradigm was also adapted to measure the relative salience of different levels of invariants <bold><italic>(<xref ref-type="bibr" rid="c9">Chen, 2005</xref></italic></bold>). There were three discrimination tasks: discriminations based on a difference in collinearity (colli., a kind of projective property, shown in <xref rid="fig1" ref-type="fig">Figure 1A</xref>, left), a difference in parallelism (para., a kind of affine property, shown in <xref rid="fig1" ref-type="fig">Figure 1A</xref>, middle), a difference in orientation of angles (ori., a kind of Euclidean property, shown in <xref rid="fig1" ref-type="fig">Figure 1A</xref>, right). The stimuli are composed of white line segments with luminance of 38.83 cd/𝑚<sup>2</sup> presented on black background with luminance of 0.13 cd/𝑚<sup>2</sup>. The stimulus array is consisted of four quadrants (visual angle 2.6° × 2.6° for each quadrant) and presented in the center region (visual angle, 6° × 6°). The subjects need to identify which quadrant is different from the others. Either of the two states of an invariant may serve as a target. For example, in the Euclidean invariant condition (<xref rid="fig1" ref-type="fig">Figure 1A</xref>, right), both upward and downward arrows could be the target. A green central fixation point (RGB [0,130,0], 0.15°) was presented throughout the entire block. Subjects were instructed to perform the actual task while maintaining central fixation. Each trial began immediately after the Space key was pressed. The stimulus array was presented until the subject indicated the location of the odd quadrant (&quot;target&quot;) via a manual button press as fast as possible on the premise of accuracy. The response time (RT) in each trial was calculated from the onset of the stimulus array. A negative feedback tone was given if the response was wrong.</p>
<p>The sample stimuli and the layout of a stimulus frame in Experiment 2 are illustrated in <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref rid="fig3s1" ref-type="fig">Figure 3—figure Supplement 1</xref>, respectively. All stimuli are white (38.83 cd/𝑚<sup>2</sup>) presented on black background (0.13 cd/𝑚<sup>2</sup>). Each stimulus is composed of a group of line(s): a pair of lines which are collinear or non-collinear in colli. task, a pair of lines which are parallel or unparallel in para. task, and a single long line in ori. Task (<xref rid="fig3s1" ref-type="fig">Figure 3—figure Supplement 1</xref>). The stimuli for the three tasks were made up of exactly the same line-segments. Thus, line-segments as well as all local features based on these line-segments, such as luminous flux, and spatial frequency components, were well controlled. The length of line (𝑙) in colli. and para. task is 80 arc min, which is half of the length of line ori. task. The width of line is 2 arc min. The distance (𝑑) between the pair of lines in the parallelism task is 40 arc min. The “base” orientation (the dashed line in <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref rid="fig3s1" ref-type="fig">Figure 3—figure Supplement 1</xref>) was randomly selected from 0-180°. For colli. and para., the “base” orientation for each stimulus on a stimulus frame was selected independently. Individual stimulus could occur in one of four quadrants, approximately 250 arc min of visual angle from fixation (𝑅); two stimuli were presented at two diagonally opposite quadrants on each trial (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p>
<p>Schematic description of a trial in Experiment 2 is shown in <xref rid="fig2" ref-type="fig">Figure 4</xref>, to make the first-order choice, subjects were instructed to press the J key if they thought there was a “target” as defined by each task, and they should press the F key if the contrary is the case (that is, there was no “target” in this trial). A second-order choice needed to be made if subjects have pressed the J key in the first-order choice: they should select which one was the “target” and report its position by pressing the corresponding key. The response of a trial was regarded as correct only if both choices were correct. This type of experimental design with two-stage choice options together with relatively higher proportion of catch trials (one third of all trials) wound help reduce false alarms and response bias.</p>
</sec>
<sec id="s4c">
<title>Procedure for psychophysics experiments</title>
<p>The overall procedure of Experiment 1 is show in <xref rid="fig1" ref-type="fig">Figure 1B</xref>. The main VPL procedure consisted of three phases: pre-training test (Pre-test), discrimination training (Training), and post-training test (Post-test). During the test phases, the three form invariant discrimination tasks were performed counterbalance across subjects at three blocks, respectively. During the training phase, all subjects were required to finish 10 blocks of the training task which is determined by their group. Each block contained 40 trials. Before all of the phases, subjects practiced 5 trials per task to make sure that they fully understood the tasks.</p>
<p>The procedure of Experiment 2 is similar to that of Experiment 1 except for no involvement of the baseline training. During each block, subject’s threshold was measured for each of the three tasks using a QUEST staircase of 40 trials augmented with 20 catch trials, leading to overall 60 trials per block. During the test phases, the tests for the three tasks were counterbalanced across subjects. The training phase contained 8 blocks of the training task corresponding with the group of subjects. Each subject practiced one block per task with large angle difference to make sure that they fully understood the tasks.</p>
</sec>
<sec id="s4d">
<title>Deep neural network simulations</title>
<p>The deep learning model used in this paper was adopted from Wenliang and Seitz <bold><italic>(<xref ref-type="bibr" rid="c42">Wenliang and Seitz, 2018</xref></italic></bold>). The model was implemented in PyTorch (version 2.0.0) and consists of two parallel streams, each encompassing the first five convolutional layers of AlexNet <bold><italic>(<xref ref-type="bibr" rid="c26">Krizhevsky et al., 2017</xref></italic></bold>) plus one fully connected layer which gives out a single scalar value. The network performed a two-interval two-alternative forced choice (2I-2AFC) task. One stream accepted one standard stimulus and the other stream accepted one comparison stimulus. The comparison stimulus was then compared to the standard stimulus. After the fully connected layers, the outputs of the two parallel streams – two scalar values – were entered to a sigmoid layer to give out one binary value indicating which stimulus was noncolinear, unparallel, or more clockwise, equivalent to the “target” in Experiment 2. Weights at each layer are shared between the two streams so that the representations of the two images are generated by the same parameters.</p>
<p>For the stimulus images, 12 equally spaced “base” orientations were chosen (0-165°, in steps of 15°), and the “base” orientation of the standard and comparison was chosen independently except for the orientation discrimination task. We trained the network on all combinations of the 3 parameters: (1) angle separation between standard and comparison —— ① 5° for colli. &amp; para. and 10° for ori., ② 10° for colli. &amp; para. and 20° for ori., ③ 20° for colli. &amp; para. and 40° for ori.; (2) distance between the pair of lines for para. —— ① 30 pixels, ② 40 pixels; (3) the location of the gap on line for colli. —— ① the midpoint; ② the front one-third. So there were overall 12 (3 × 2 × 2) stimulus conditions. It should be noted that the angle separations in the orientation task were always twice the angle separations in the other two tasks in each condition, this proportion was based on the initial threshold observed in the behavioral experiment. Here are the other stimulus parameters: length of line in para. (100 pixels); length of line in colli. and ori. (200 pixels); width of line (3 pixels); radius of gap for colli. (5 pixels). Each stimulus was centered on an 8-bit 227 × 227 pixel image with black background.</p>
<p>Three networks were trained on the three tasks (colli., para., ori.) respectively, repeated in 12 stimulus conditions. Each network was trained for 1000 iterations of 60-image batches with batch size of 20 pairs, and meanwhile was tested on the other two untrained tasks. Learning and transfer performances were measured at 30 approximately logarithmically spaced iterations from 1 to 1000. We used the same feature maps and kernel size as the original paper. Network weights were initialized such that the last fully connected layer was initialized by zero and weights in the five convolutional layers were copied from an AlexNet trained on object recognition (downloaded from <ext-link ext-link-type="uri" xlink:href="http://dl.caffe.berkeleyvision.org/">http://dl.caffe.berkeleyvision.org/</ext-link> bvlc_reference_caffenet.caffemodel) to mimic a (pretrained) adult brain. Training parameters were set as follow: learning rate = 0.0001, momentum = 0.9. The cross-entropy loss function was used as an objective function and optimized via stochastic gradient descent.</p>
</sec>
<sec id="s4e">
<title>Data analysis</title>
<p>All data analyses were carried out in Matlab (The Mathwork corp, Orien, USA) and Python (Python Software Foundation, v3.9.13). No data were excluded.</p>
<p>Human behavioral data were analyzed using analysis of variance (ANOVA), post-hoc test, and paired sample t-test. For ANOVAs and post-hoc tests, we computed <inline-formula><inline-graphic xlink:href="573923v1_inline1g.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and Hedge’s g as effect sizes. For paired sample t-tests, we computed Cohen’s d as effect size. To quantify learning effect, we computed the Learning Index (LI) <bold><italic>(<xref ref-type="bibr" rid="c35">Petrov et al., 2011</xref></italic></bold>) as follows:
<disp-formula id="eqn1">
<graphic xlink:href="573923v1_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where 𝑝𝑒𝑟𝑓 𝑜𝑟𝑚𝑎𝑛𝑐𝑒 refers to the RT or threshold obtained at the test phases.</p>
<p>In DNN experiment, Ordinary Least Squares (OLS) method of linear regression was implemented to analyze the transfer effects between different tasks. The following equation describes the model’s specification:
<disp-formula id="eqn2">
<graphic xlink:href="573923v1_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where 𝑦 represents the dependent variable (final accuracy), 𝑥<sub>1</sub> and 𝑥<sub>2</sub> represent the two features (training and test task) respectively. 𝛽<sub>0</sub> is the intercept, 𝛽<sub>1</sub>, 𝛽<sub>2</sub> are coefficients of the linear model, and 𝜖 is the error term (unexplained by the model).</p>
<p>To estimate the learning effects in layers of DNN, the differences in weights before and after training at each layer were measured. Specifically, for a particular layer with 𝑁 total connections to its lower layer, we denote the original 𝑁-dimensional weight vector trained on object classification as 𝑤 (𝑁 and 𝑤 are specified in AlexNet), the change in this vector after perceptual learning as 𝛿𝑤, and define the layer change as follows:
<disp-formula id="eqn3">
<graphic xlink:href="573923v1_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where 𝑖 indexes each element in the weight vector. Under this measure, scaling the weight vector by a constant gives the same change regardless of dimensionality, reducing the effect of unequal weight dimensionalities on the magnitude of weight change. For the weights in the final readout layer that were initialized with zeros, the denominator in <xref rid="eqn1" ref-type="disp-formula">Equation 1</xref> was set to 𝑁, effectively measuring the average change per connection in this layer. Due to the convolutional nature of the layers 1–5, 𝑑<sub>𝑟𝑒𝑙</sub> is equal to the change in filters that are shared across location in those layers. When comparing weight change across layers, we focus on the first five layers unless otherwise stated.</p>
<p>OLS method of linear regression with interaction terms was implemented to analyze the learning effects across layers. The following equation describes the model’s specification:
<disp-formula id="eqn4">
<graphic xlink:href="573923v1_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where 𝑦 represents the dependent variable (PSI or final layer change), 𝑥<sub>1</sub> and 𝑥<sub>2</sub> represent the two features (training task and layer number) respectively. 𝛽<sub>0</sub> is the intercept, 𝛽<sub>1</sub>, 𝛽<sub>2</sub> and 𝛽<sub>3</sub> are coefficients of the linear model, and 𝜖 is the error term (unexplained by the model).</p>
</sec>
<sec id="s4f">
<title>Code availability</title>
<p>The deep neural network is available from the original authors on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/kevin-w-li/DNN_for_VPL">https://github.com/kevin-w-li/DNN_for_VPL</ext-link>).</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>The study was funded by grants from Ministry of Science and Technology of China grant, the University Synergy Innovation Program of Anhui Province, and the Chinese Academy of Sciences grants.</p>
</ack>
<sec id="s5">
<title>Additional information</title>
<sec id="s5a">
<title>Funding</title>
<p>Ministry of Science and Technology of China grant (2020AAA0105601), Tiangang Zhou; Ministry of Science and Technology of China grant (2019YFA0707103 and 2022ZD0209500), Zhentao Zuo; University Synergy Innovation Program of Anhui Province (GXXT-2021-002, GXXT-2022-09), Zhentao Zuo; Chinese Academy of Sciences grants (ZDBS-LY-SM028, 2021091 and YSBR-068), Zhentao Zuo. The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</p>
</sec>
<sec id="s5b">
<title>Author contributions</title>
<p>Yan Yang, Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review &amp; editing; Zhentao Zuo, Tiangang Zhou, Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Project administration, Writing – review &amp; editing; Yan Zhuo, Conceptualization, Resources, Supervision, Funding acquisition; Lin Chen, Conceptualization, Resources, Funding acquisition, Methodology.</p>
</sec>
<sec id="s5c">
<title>Ethics</title>
<p>Human subjects: Informed consent, and consent to publish was obtained from each observer before testing. The study was approved by the ethics committee of the Institute of Biophysics at the Chinese Academy of Sciences, Beijin (reference number:2017-IRB-004).</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Aberg</surname> <given-names>KC</given-names></string-name>, <string-name><surname>Tartaglia</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Herzog</surname> <given-names>MH</given-names></string-name>. <article-title>Perceptual learning with Chevrons requires a minimal number of trials, transfers to untrained directions, but does not require sleep</article-title>. <source>Vision Research</source>. <year>2009</year>; <volume>49</volume>:<fpage>2087</fpage>–<lpage>2094</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.visres.2009.05.020</pub-id>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Adolph</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Kretch</surname> <given-names>KS</given-names></string-name>. <article-title>Gibson’s Theory of Perceptual Learning</article-title>. <source>International Encyclopedia of the Social &amp; Behavioral Sciences</source>. <year>2015</year> <volume>12</volume>; <issue>10</issue>:<fpage>127</fpage>–<lpage>134</lpage>. doi: <pub-id pub-id-type="doi">10.1016/B978-0-08-097086-8.23096-1</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Ahissar</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hochstein</surname> <given-names>S</given-names></string-name>. <article-title>Task difficulty and the specificity of perceptual learning</article-title>. <source>Nature</source>. <year>1997</year> 5; <volume>387</volume>(<issue>6631</issue>):<fpage>401</fpage>–<lpage>406</lpage>. doi: <pub-id pub-id-type="doi">10.1038/387401a0</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Ahissar</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hochstein</surname> <given-names>S</given-names></string-name>. <article-title>The reverse hierarchy theory of visual perceptual learning</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2004</year> 10; <volume>8</volume>(<issue>10</issue>):<fpage>457</fpage>–<lpage>464</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tics.2004.08.011</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Brainard</surname> <given-names>DH.</given-names></string-name> <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial Vision</source>. <year>1997</year>; <volume>10</volume>(<issue>4</issue>):<fpage>433</fpage>–<lpage>436</lpage>. doi: <pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id>, pMID: <pub-id pub-id-type="pmid">9176952</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Buccella</surname> <given-names>A</given-names></string-name>. <article-title>The problem of perceptual invariance</article-title>. <source>Synthese</source>. <year>2021</year> 12; <volume>199</volume>(<issue>5-6</issue>):<fpage>13883</fpage>–<lpage>13905</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s11229-021-03402-2</pub-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Chen</surname> <given-names>L</given-names></string-name>. <article-title>Topological Structure in Visual Perception</article-title>. <source>Science</source>. <year>1982</year> 11; <volume>218</volume>(<issue>4573</issue>):<fpage>699</fpage>–<lpage>700</lpage>. doi: <pub-id pub-id-type="doi">10.1126/science.7134969</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Chen</surname> <given-names>L</given-names></string-name>. <article-title>Topological structure in the perception of apparent motion</article-title>. <source>Perception</source>. <year>1985</year>; <volume>14</volume>(<issue>2</issue>):<fpage>197</fpage>–<lpage>208</lpage>. doi: <pub-id pub-id-type="doi">10.1068/p140197</pub-id>, pMID: <pub-id pub-id-type="pmid">4069950</pub-id>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Chen</surname> <given-names>L</given-names></string-name>. <article-title>The topological approach to perceptual organization</article-title>. <source>Visual Cognition</source>. <year>2005</year> 5; <volume>12</volume>(<issue>4</issue>):<fpage>553</fpage>–<lpage>637</lpage>. doi: <pub-id pub-id-type="doi">10.1080/13506280444000256</pub-id>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Crist</surname> <given-names>RE</given-names></string-name>, <string-name><surname>Kapadia</surname> <given-names>MK</given-names></string-name>, <string-name><surname>Westheimer</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gilbert</surname> <given-names>CD</given-names></string-name>. <article-title>Perceptual Learning of Spatial Localization: Specificity for Orientation, Position, and Context</article-title>. <source>Journal of Neurophysiology</source>. <year>1997</year> 12; <volume>78</volume>(<issue>6</issue>):<fpage>2889</fpage>–<lpage>2894</lpage>. doi: <pub-id pub-id-type="doi">10.1152/jn.1997.78.6.2889</pub-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Fiorentini</surname> <given-names>A</given-names></string-name>, <string-name><surname>Berardi</surname> <given-names>N</given-names></string-name>. <article-title>Learning in grating waveform discrimination: Specificity for orientation and spatial frequency</article-title>. <source>Vision Research</source>. <year>1981</year> 1; <volume>21</volume>(<issue>7</issue>):<fpage>1149</fpage>–<lpage>1158</lpage>. doi: <pub-id pub-id-type="doi">10.1016/0042-6989(81)90017-1</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="book"><string-name><surname>Gibson</surname> <given-names>EJ.</given-names></string-name> <chapter-title>Principles of perceptual learning and development</chapter-title>. <publisher-name>East Norwalk, CT, US: Appleton-Century-Crofts</publisher-name>; <year>1969</year>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Gibson</surname> <given-names>EJ</given-names></string-name>. <article-title>The development of perception as an adaptive process</article-title>. <source>American scientist</source>. <year>1970</year>; <volume>58</volume>(<issue>1</issue>):<fpage>98</fpage>–<lpage>107</lpage>. PMID: <pub-id pub-id-type="pmid">5001578</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="book"><string-name><surname>Gibson</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>Pick</surname> <given-names>A</given-names></string-name>. <source>An Ecological Approach to Perceptual Learning and Development</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2000</year>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Gibson</surname> <given-names>EJ</given-names></string-name>. <article-title>Perceptual learning and the theory of word perception</article-title>. <source>Cognitive Psychology</source>. <year>1971</year>; <volume>2</volume>(<issue>4</issue>):<fpage>351</fpage>–<lpage>368</lpage>. doi: <pub-id pub-id-type="doi">10.1016/0010-0285(71)90020-X</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Gibson</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Gibson</surname> <given-names>EJ</given-names></string-name>. <article-title>Perceptual learning: Differentiation or enrichment?</article-title> <source>Psychological Review</source>. <year>1955</year>; <volume>62</volume>(<issue>1</issue>):<fpage>32</fpage>–<lpage>41</lpage>. doi: <pub-id pub-id-type="doi">10.1037/h0048826</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="book"><string-name><surname>Gibson</surname> <given-names>JJ.</given-names></string-name> <chapter-title>The ecological approach to visual perception</chapter-title>. <publisher-name>Boston, MA, US: Houghton, Miffin and Company</publisher-name>; <year>1979</year>. Page: xiv, 332.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Gold</surname> <given-names>JI</given-names></string-name>, <string-name><surname>Stocker</surname> <given-names>AA</given-names></string-name>. <article-title>Visual Decision-Making in an Uncertain and Dynamic World</article-title>. <source>Annual Review of Vision Science</source>. <year>2017</year> <volume>9</volume>; <issue>3</issue>:<fpage>227</fpage>–<lpage>250</lpage>. doi: <pub-id pub-id-type="doi">10.1146/annurev-vision-111815-114511</pub-id>, pMID: <pub-id pub-id-type="pmid">28715956</pub-id>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Gold</surname> <given-names>JI</given-names></string-name>, <string-name><surname>Watanabe</surname> <given-names>T</given-names></string-name>. <article-title>Perceptual learning</article-title>. <source>Current biology : CB</source>. <year>2010</year> 1; <volume>20</volume>(<issue>2</issue>):10.1016/j.cub.2009.10.066. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2009.10.066</pub-id>, pMID: <pub-id pub-id-type="pmid">20129034</pub-id> PMCID: <pub-id pub-id-type="pmcid">PMC3821996</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Hochstein</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ahissar</surname> <given-names>M</given-names></string-name>. <article-title>View from the Top Hierarchies and Reverse Hierarchies in the Visual System</article-title>. <source>Neuron</source>. <year>2002</year>; <volume>36</volume>:<fpage>791</fpage>–<lpage>804</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S0896-6273(02)01091-7</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Hua</surname> <given-names>T</given-names></string-name>, <string-name><surname>Bao</surname> <given-names>P</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>CB</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>ZL</given-names></string-name>. <article-title>Perceptual learning improves contrast sensitivity of V1 neurons in cats</article-title>. <source>Current biology: CB</source>. <year>2010</year> 5; <volume>20</volume>(<issue>10</issue>):<fpage>887</fpage>–<lpage>894</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2010.03.066</pub-id>, pMID: <pub-id pub-id-type="pmid">20451388</pub-id> PMCID: <pub-id pub-id-type="pmcid">PMC2877770</pub-id>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Hung</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Seitz</surname> <given-names>AR</given-names></string-name>. <article-title>Prolonged training at threshold promotes robust retinotopic specificity in perceptual learning</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>. <year>2014</year> 6; <volume>34</volume>(<issue>25</issue>):<fpage>8423</fpage>– <lpage>8431</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0745-14.2014</pub-id>, pMID: <pub-id pub-id-type="pmid">24948798</pub-id> PMCID: <pub-id pub-id-type="pmcid">PMC4061387</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Jeter</surname> <given-names>PE</given-names></string-name>, <string-name><surname>Dosher</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Petrov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>ZL</given-names></string-name>. <article-title>Task precision at transfer determines specificity of perceptual learning</article-title>. <source>Journal of Vision</source>. <year>2009</year> 3; <volume>9</volume>(<issue>3</issue>):<fpage>1</fpage>–<lpage>1</lpage>. doi: <pub-id pub-id-type="doi">10.1167/9.3.1</pub-id>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Klein</surname> <given-names>F</given-names></string-name>. <article-title>Vergleichende Betrachtungen über neuere geometrische Forschungen</article-title>. <source>Mathematische Annalen</source>. <year>1893</year> 3; <volume>43</volume>(<issue>1</issue>):<fpage>63</fpage>–<lpage>100</lpage>. doi: <pub-id pub-id-type="doi">10.1007/BF01446615</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Klein</surname> <given-names>F</given-names></string-name>. <article-title>Elementary Mathematics from an Advanced Standpoint: Geometry. Washington</article-title>, <source>D.C.: National Mathematics Magazine</source>; <year>1941</year>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Krizhevsky</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sutskever</surname> <given-names>I</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>GE</given-names></string-name>. <article-title>ImageNet classification with deep convolutional neural networks</article-title>. <source>Communications of the ACM</source>. <year>2017</year> 5; <volume>60</volume>(<issue>6</issue>):<fpage>84</fpage>–<lpage>90</lpage>. doi: <pub-id pub-id-type="doi">10.1145/3065386</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Liu</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Weinshall</surname> <given-names>D</given-names></string-name>. <article-title>Mechanisms of generalization in perceptual learning</article-title>. <source>Vision Research</source>. <year>2000</year> 1; <volume>40</volume>(<issue>1</issue>):<fpage>97</fpage>–<lpage>109</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S0042-6989(99)00140-6</pub-id>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Manenti</surname> <given-names>GL</given-names></string-name>, <string-name><surname>Dizaji</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Schwiedrzik</surname> <given-names>CM</given-names></string-name>. <article-title>Variability in training unlocks generalization in visual perceptual learning through invariant representations</article-title>. <source>Current Biology</source>. <year>2023</year> <volume>1</volume>; p. <fpage>S0960982223000118</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2023.01.011</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Markov</surname> <given-names>NT</given-names></string-name>, <string-name><surname>Ercsey-Ravasz</surname> <given-names>M</given-names></string-name>, <string-name><surname>Essen</surname> <given-names>DCV</given-names></string-name>, <string-name><surname>Knoblauch</surname> <given-names>K</given-names></string-name>, <string-name><surname>Toroczkai</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Kennedy</surname> <given-names>H</given-names></string-name>. <article-title>Cortical High-Density Counterstream Architectures</article-title>. <source>Science</source>. <year>2013</year>; <volume>342</volume>. doi: <pub-id pub-id-type="doi">10.1126/science.123840</pub-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Mascetti</surname> <given-names>L</given-names></string-name>, <string-name><surname>Muto</surname> <given-names>V</given-names></string-name>, <string-name><surname>Matarazzo</surname> <given-names>L</given-names></string-name>, <string-name><surname>Foret</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ziegler</surname> <given-names>E</given-names></string-name>, <string-name><surname>Albouy</surname> <given-names>G</given-names></string-name>, <string-name><surname>Sterpenich</surname> <given-names>V</given-names></string-name>, <string-name><surname>Schmidt</surname> <given-names>C</given-names></string-name>, <string-name><surname>Degueldre</surname> <given-names>C</given-names></string-name>, <string-name><surname>Leclercq</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Phillips</surname> <given-names>C</given-names></string-name>, <string-name><surname>Luxen</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vandewalle</surname> <given-names>G</given-names></string-name>, <string-name><surname>Vogels</surname> <given-names>R</given-names></string-name>, <string-name><surname>Maquet</surname> <given-names>P</given-names></string-name>, <string-name><surname>Balteau</surname> <given-names>E</given-names></string-name>. <article-title>The Impact of Visual Perceptual Learning on Sleep and Local Slow-Wave Initiation</article-title>. <source>The Journal of Neuroscience</source>. <year>2013</year>; <volume>33</volume>:<fpage>3323</fpage> – <lpage>3331</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0763-12.2013</pub-id>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>McGovern</surname> <given-names>DP</given-names></string-name>, <string-name><surname>Webb</surname> <given-names>BS</given-names></string-name>, <string-name><surname>Peirce</surname> <given-names>JW</given-names></string-name>. <article-title>Transfer of perceptual learning between different visual tasks</article-title>. <source>Journal of Vision</source>. <year>2012</year> 10; <volume>12</volume>(<issue>11</issue>):<fpage>4</fpage>. doi: <pub-id pub-id-type="doi">10.1167/12.11.4</pub-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Meng</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>B</given-names></string-name>, <string-name><surname>Cui</surname> <given-names>D</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>N</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>YB</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>Y</given-names></string-name>. <article-title>Age-related changes in local and global visual perception</article-title>. <source>Journal of vision</source>. <year>2019</year>; <volume>19</volume>(<issue>1</issue>):<fpage>10</fpage>. doi: <pub-id pub-id-type="doi">10.1167/19.1.10</pub-id>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Orsten-Hooge</surname> <given-names>K</given-names></string-name>, <string-name><surname>Portillo</surname> <given-names>M</given-names></string-name>, <string-name><surname>Pomerantz</surname> <given-names>J</given-names></string-name>. <article-title>False Pop Out in Visual Search</article-title>. <source>Journal of Vision</source>. <year>2011</year> <volume>9</volume>; <issue>11</issue>:<fpage>1314</fpage>– <lpage>1314</lpage>. doi: <pub-id pub-id-type="doi">10.1167/11.11.1314</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Pelli</surname> <given-names>DG</given-names></string-name>. <article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title>. <source>Spatial Vision</source>. <year>1997</year>; <volume>10</volume>(<issue>4</issue>):<fpage>437</fpage>–<lpage>442</lpage>. doi: <pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id>, pMID: <pub-id pub-id-type="pmid">9176953</pub-id>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Petrov</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Van Horn</surname> <given-names>NM</given-names></string-name>, <string-name><surname>Ratcliff</surname> <given-names>R</given-names></string-name>. <article-title>Dissociable perceptual-learning mechanisms revealed by diffusion-model analysis</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>2011</year> 6; <volume>18</volume>(<issue>3</issue>):<fpage>490</fpage>–<lpage>497</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13423-011-0079-8</pub-id>, pMID: <pub-id pub-id-type="pmid">21394547</pub-id>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Sowden</surname> <given-names>PT</given-names></string-name>, <string-name><surname>Rose</surname> <given-names>D</given-names></string-name>, <string-name><surname>Davies</surname> <given-names>IRL</given-names></string-name>. <article-title>Perceptual learning of luminance contrast detection: specific for spatial frequency and retinal location but not orientation</article-title>. <source>Vision Research</source>. <year>2002</year> 5; <volume>42</volume>(<issue>10</issue>):<fpage>1249</fpage>–<lpage>1258</lpage>. doi: <pub-id pub-id-type="doi">10.1016/s0042-6989(02)00019-6</pub-id>, pMID: <pub-id pub-id-type="pmid">12044757</pub-id>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Szokolszky</surname> <given-names>A</given-names></string-name>, <string-name><surname>Read</surname> <given-names>CY</given-names></string-name>, <string-name><surname>Palatinus</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Palatinus</surname> <given-names>K</given-names></string-name>. <article-title>Ecological approaches to perceptual learning: learning to perceive and perceiving as learning</article-title>. <source>Adaptive Behavior</source>. <year>2019</year>; <volume>27</volume>:<fpage>363</fpage> – <lpage>388</lpage>. doi: <pub-id pub-id-type="doi">10.1177/1059712319854687</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Szpiro</surname> <given-names>SFA</given-names></string-name>, <string-name><surname>Carrasco</surname> <given-names>M</given-names></string-name>. <article-title>Exogenous Attention Enables Perceptual Learning</article-title>. <source>Psychological Science</source>. <year>2015</year>; <volume>26</volume>(<issue>12</issue>):<fpage>1854</fpage>–<lpage>1862</lpage>. doi: <pub-id pub-id-type="doi">10.1177/0956797615598976</pub-id>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Todd</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Weismantel</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kallie</surname> <given-names>CS</given-names></string-name>. <article-title>On the relative detectability of configural properties</article-title>. <source>Journal of Vision</source>. <year>2014</year> 1; <volume>14</volume>(<issue>1</issue>):<fpage>18</fpage>–<lpage>18</lpage>. doi: <pub-id pub-id-type="doi">10.1167/14.1.18</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Todd</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Norman</surname> <given-names>JF</given-names></string-name>. <article-title>On the Relative Salience of Euclidean, Affine, and Topological Structure for 3-D Form Discrimination</article-title>. <source>Perception</source>. <year>1998</year> 3; <volume>27</volume>(<issue>3</issue>):<fpage>273</fpage>–<lpage>282</lpage>. doi: <pub-id pub-id-type="doi">10.1068/p270273</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Watson</surname> <given-names>AB</given-names></string-name>, <string-name><surname>Pelli</surname> <given-names>DG</given-names></string-name>. <article-title>Quest: A Bayesian adaptive psychometric method</article-title>. <source>Perception &amp; Psychophysics</source>. <year>1983</year> 3; <volume>33</volume>(<issue>2</issue>):<fpage>113</fpage>–<lpage>120</lpage>. doi: <pub-id pub-id-type="doi">10.3758/BF03202828</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Wenliang</surname> <given-names>LK</given-names></string-name>, <string-name><surname>Seitz</surname> <given-names>AR</given-names></string-name>. <article-title>Deep Neural Networks for Modeling Visual Perceptual Learning</article-title>. <source>The Journal of Neuroscience</source>. <year>2018</year> 7; <volume>38</volume>(<issue>27</issue>):<fpage>6028</fpage>–<lpage>6044</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1620-17.2018</pub-id>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Yang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Yan</surname> <given-names>F</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Fan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>YF</given-names></string-name>, <string-name><surname>Jiang</surname> <given-names>L</given-names></string-name>, <string-name><surname>Xi</surname> <given-names>J</given-names></string-name>, <string-name><surname>lei Zhao</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>ZL</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>CB.</given-names></string-name> <article-title>Identifying Long- and Short-Term Processes in Perceptual Learning</article-title>. <source>Psychological Science</source>. <year>2022</year>; <volume>33</volume>:<fpage>830</fpage> – <lpage>843</lpage>. doi: <pub-id pub-id-type="doi">10.1177/09567976211056620</pub-id>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname> <given-names>JY</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>GL</given-names></string-name>, <string-name><surname>Xiao</surname> <given-names>LQ</given-names></string-name>, <string-name><surname>Klein</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Levi</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>C</given-names></string-name>. <article-title>Rule-Based Learning Explains Visual Perceptual Learning and Its Specificity and Transfer</article-title>. <source>Journal of Neuroscience</source>. <year>2010</year> 9; <volume>30</volume>(<issue>37</issue>):<fpage>12323</fpage>–<lpage>12328</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0704-10.2010</pub-id>.</mixed-citation></ref>
</ref-list>
<sec><p><fig id="fig2s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2—figure supplement 1.</label>
<caption><p>Accuracies for the three discrimination task measured at Pre-test and Post-test. Accuracy was defined as the average percentage correct per block. At Pre-test, the accuracies of the collinearity task were significantly higher than that in the parallelism (p &lt; 0.0001) and orientation task (p = 0.0001). At Post-test, the accuracies of the collinearity task were still significantly higher than the other two tasks (p &lt; 0.0001 for the parallelism task, p = 0.0001 for the orientation task). Statistical significance was calculated by paired t-test with FDR correction. (***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05). Error bars denote 1 SEM across subjects.</p></caption>
<graphic xlink:href="573923v1_fig2s1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig2s2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2—figure supplement 2.</label>
<caption><p>The learning indexes of the three geometrical invariants in Experiment 1. Error bars denote 1 SEM across subjects.</p></caption>
<graphic xlink:href="573923v1_fig2s2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig3s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3—figure supplement 1.</label>
<caption><p>Examples of stimuli in Experiment 2. Sample stimuli in the collinearity (left), parallelism (middle) and orientation (right) discrimination task. The blue dashed lines represent the &quot;base&quot; orientation for each stimulus, and 𝜃 is the angle separation of the discrimination task.</p></caption>
<graphic xlink:href="573923v1_fig3s1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5—figure supplement 1.</label>
<caption><p>The learning indexes of the three geometrical invariants in Experiment 2. Error bars denote 1 SEM across subjects.</p></caption>
<graphic xlink:href="573923v1_fig5s1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig6s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6—figure supplement 1.</label>
<caption><p>Stimulus examples in Experiment 3. Examples of the pairs of stimulus images for the three discrimination tasks in Experiment 3. The examples here are selected from the stimulus condition with the following parameters: angle separation (10° for colli. &amp; para. and 20° for ori.), distance for para. (40 pixels), location of gap for colli. (the front one-third).</p></caption>
<graphic xlink:href="573923v1_fig6s1.tif" mimetype="image" mime-subtype="tiff"/>
</fig></p></sec></back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93959.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>SP</surname>
<given-names>Arun</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study proposes a framework to understand and predict generalization in visual perceptual learning in humans based on form invariants. Using behavioral experiments in humans and by training deep networks, the authors offer evidence that the presence of stable invariants in a task leads to faster learning. However, this interpretation is promising but <bold>incomplete</bold>. It can be strengthened through clearer theoretical justification, additional experiments, and by rejecting alternate explanations.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93959.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
Visual Perceptual Learning (VPL) results in varying degrees of generalization to tasks or stimuli not seen during training. The question of which stimulus or task features predict whether learning will transfer to a different perceptual task has long been central in the field of perceptual learning, with numerous theories proposed to address it. This paper introduces a novel framework for understanding generalization in VPL, focusing on the form invariants of the training stimulus. Contrary to a previously proposed theory that task difficulty predicts the extent of generalization - suggesting that more challenging tasks yield less transfer to other tasks or stimuli - this paper offers an alternative perspective. It introduces the concept of task invariants and investigates how the structural stability of these invariants affects VPL and its generalization. The study finds that tasks with high-stability invariants are learned more quickly. However, training with low-stability invariants leads to greater generalization to tasks with higher stability, but not the reverse. This indicates that, at least based on the experiments in this paper, an easier training task results in less generalization, challenging previous theories that focus on task difficulty (or precision). Instead, this paper posits that the structural stability of stimulus or task invariants is the key factor in explaining VPL generalization across different tasks</p>
<p>Strengths:</p>
<p>
- The paper effectively demonstrates that the difficulty of a perceptual task does not necessarily correlate with its learning generalization to other tasks, challenging previous theories in the field of Visual Perceptual Learning. Instead, it proposes a significant and novel approach, suggesting that the form invariants of training stimuli are more reliable predictors of learning generalization. The results consistently bolster this theory, underlining the role of invariant stability in forecasting the extent of VPL generalization across different tasks.</p>
<p>- The experiments conducted in the study are thoughtfully designed and provide robust support for the central claim about the significance of form invariants in VPL generalization.</p>
<p>Weaknesses:</p>
<p>
- The paper assumes a considerable familiarity with the Erlangen program and the definitions of invariants and their structural stability, potentially alienating readers who are not versed in these concepts. This assumption may hinder the understanding of the paper's theoretical rationale and the selection of stimuli for the experiments, particularly for those unfamiliar with the Erlangen program's application in psychophysics. A brief introduction to these key concepts would greatly enhance the paper's accessibility. The justification for the chosen stimuli and the design of the three experiments could be more thoroughly articulated.</p>
<p>- The paper does not clearly articulate how its proposed theory can be integrated with existing observations in the field of VPL. While it acknowledges previous theories on VPL generalization, the paper falls short in explaining how its framework might apply to classical tasks and stimuli that have been widely used in the VPL literature, such as orientation or motion discrimination with Gabors, vernier acuity, etc. It also does not provide insight into the application of this framework to more naturalistic tasks or stimuli. If the stability of invariants is a key factor in predicting a task's generalization potential, the paper should elucidate how to define the stability of new stimuli or tasks. This issue ties back to the earlier mentioned weakness: namely, the absence of a clear explanation of the Erlangen program and its relevant concepts.</p>
<p>- The paper does not convincingly establish the necessity of its introduced concept of invariant stability for interpreting the presented data. For instance, consider an alternative explanation: performing in the collinearity task requires orientation invariance. Therefore, it's straightforward that learning the collinearity task doesn't aid in performing the other two tasks (parallelism and orientation), which do require orientation estimation. Interestingly, orientation invariance is more characteristic of higher visual areas, which, consistent with the Reverse Hierarchy Theory, are engaged more rapidly in learning compared to lower visual areas. This simpler explanation, grounded in established concepts of VPL and the tuning properties of neurons across the visual cortex, can account for the observed effects, at least in one scenario. This approach has previously been used/proposed to explain VPL generalization, as seen in (Chowdhury and DeAngelis, Neuron, 2008), (Liu and Pack, Neuron, 2017), and (Bakhtiari et al., JoV, 2020). The question then is: how does the concept of invariant stability provide additional insights beyond this simpler explanation?</p>
<p>- While the paper discusses the transfer of learning between tasks with varying levels of invariant stability, the mechanism of this transfer within each invariant condition remains unclear. A more detailed analysis would involve keeping the invariant's stability constant while altering a feature of the stimulus in the test condition. For example, in the VPL literature, one of the primary methods for testing generalization is examining transfer to a new stimulus location. The paper does not address the expected outcomes of location transfer in relation to the stability of the invariant. Moreover, in the affine and Euclidean conditions one could maintain consistent orientations for the distractors and targets during training, then switch them in the testing phase to assess transfer within the same level of invariant structural stability.</p>
<p>- In the section detailing the modeling experiment using deep neural networks (DNN), the takeaway was unclear. While it was interesting to observe that the DNN exhibited a generalization pattern across conditions similar to that seen in the human experiments, the claim made in the abstract and introduction that the model provides a 'mechanistic' explanation for the phenomenon seems overstated. The pattern of weight changes across layers, as depicted in Figure 7, does not conclusively explain the observed variability in generalizations. Furthermore, the substantial weight change observed in the first two layers during the orientation discrimination task is somewhat counterintuitive. Given that neurons in early layers typically have smaller receptive fields and narrower tunings, one would expect this to result in less transfer, not more.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93959.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The strengths of this paper are clear: The authors are asking a novel question about geometric representation that would be relevant to a broad audience. Their question has a clear grounding in pre-existing mathematical concepts, that, to my knowledge, have been only minimally explored in cognitive science. Moreover, the data themselves are quite striking, such that my only concern would be that the data seem almost *too* clean. It is hard to know what to make of that, however. From one perspective, this is even more reason the results should be publicly available. Yet I am of the (perhaps unorthodox) opinion that reviewers should voice these gut reactions, even if it does not influence the evaluation otherwise. Below I offer some more concrete comments:</p>
<p>(1) The justification for the designs is not well explained. The authors simply tell the audience in a single sentence that they test projective, affine, and Euclidean geometry. But despite my familiarity with these terms -- familiarity that many readers may not have -- I still had to pause for a very long time to make sense of how these considerations led to the stimuli that were created. I think the authors must, for a point that is so central to the paper, thoroughly explain exactly why the stimuli were designed the way that they were and how these designs map onto the theoretical constructs being tested.</p>
<p>(2) I wondered if the design in Experiment 1 was flawed in one small but critical way. The goal of the parallelism stimuli, I gathered, was to have a set of items that is not parallel to the other set of items. But in doing that, isn't the manipulation effectively the same as the manipulation in the orientation stimuli? Both functionally involve just rotating one set by a fixed amount. (Note: This does not seem to be a problem in Experiment 2, in which the conditions are more clearly delineated.)</p>
<p>(3) I wondered if the results would hold up for stimuli that were more diverse. It seems that a determined experimenter could easily design an &quot;adversarial&quot; version of these experiments for which the results would be unlikely to replicate. For instance: In the orientation group in Experiment 1, what if the odd-one-out was rotated 90 degrees instead of 180 degrees? Intuitively, it seems like this trial type would now be much easier, and the pattern observed here would not hold up. If it did hold up, that would provide stronger support for the authors' theory.</p>
<p>It is not enough, in my opinion, to simply have some confirmatory evidence of this theory. One would have to have thoroughly tested many possible ways that theory could fail. I'm unsure that enough has been done here to convince me that these ideas would hold up across a more diverse set of stimuli.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93959.1.sa3</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Yang</surname>
<given-names>Yan</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0003-4429-8443</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Zhuo</surname>
<given-names>Yan</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zuo</surname>
<given-names>Zhentao</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5909-3884</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Zhuo</surname>
<given-names>Tiangang</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0005-1005-5106</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Chen</surname>
<given-names>Lin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>Visual Perceptual Learning (VPL) results in varying degrees of generalization to tasks or stimuli not seen during training. The question of which stimulus or task features predict whether learning will transfer to a different perceptual task has long been central in the field of perceptual learning, with numerous theories proposed to address it. This paper introduces a novel framework for understanding generalization in VPL, focusing on the form invariants of the training stimulus. Contrary to a previously proposed theory that task difficulty predicts the extent of generalization - suggesting that more challenging tasks yield less transfer to other tasks or stimuli - this paper offers an alternative perspective. It introduces the concept of task invariants and investigates how the structural stability of these invariants affects VPL and its generalization. The study finds that tasks with high-stability invariants are learned more quickly. However, training with low-stability invariants leads to greater generalization to tasks with higher stability, but not the reverse. This indicates that, at least based on the experiments in this paper, an easier training task results in less generalization, challenging previous theories that focus on task difficulty (or precision). Instead, this paper posits that the structural stability of stimulus or task invariants is the key factor in explaining VPL generalization across different tasks</p>
<p>Strengths:</p>
<list list-type="bullet">
<list-item><p>The paper effectively demonstrates that the difficulty of a perceptual task does not necessarily correlate with its learning generalization to other tasks, challenging previous theories in the field of Visual Perceptual Learning. Instead, it proposes a significant and novel approach, suggesting that the form invariants of training stimuli are more reliable predictors of learning generalization. The results consistently bolster this theory, underlining the role of invariant stability in forecasting the extent of VPL generalization across different tasks.</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>The experiments conducted in the study are thoughtfully designed and provide robust support for the central claim about the significance of form invariants in VPL generalization.</p>
</list-item></list>
<p>Weaknesses:</p>
<list list-type="bullet">
<list-item><p>The paper assumes a considerable familiarity with the Erlangen program and the definitions of invariants and their structural stability, potentially alienating readers who are not versed in these concepts. This assumption may hinder the understanding of the paper's theoretical rationale and the selection of stimuli for the experiments, particularly for those unfamiliar with the Erlangen program's application in psychophysics. A brief introduction to these key concepts would greatly enhance the paper's accessibility. The justification for the chosen stimuli and the design of the three experiments could be more thoroughly articulated.</p>
</list-item></list>
</disp-quote>
<p>Response: We appreciate the reviewer's feedback regarding the accessibility of our paper. In response to this feedback, we plan to enhance the introduction section of our paper to provide a concise yet comprehensive overview of the key concepts of Erlangen program. Additionally, we will provide a more thorough justification for the selection of stimuli and the experimental design in our revised version, ensuring that readers understand the rationale behind our choices.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The paper does not clearly articulate how its proposed theory can be integrated with existing observations in the field of VPL. While it acknowledges previous theories on VPL generalization, the paper falls short in explaining how its framework might apply to classical tasks and stimuli that have been widely used in the VPL literature, such as orientation or motion discrimination with Gabors, vernier acuity, etc. It also does not provide insight into the application of this framework to more naturalistic tasks or stimuli. If the stability of invariants is a key factor in predicting a task's generalization potential, the paper should elucidate how to define the stability of new stimuli or tasks. This issue ties back to the earlier mentioned weakness: namely, the absence of a clear explanation of the Erlangen program and its relevant concepts.</p>
</list-item></list>
</disp-quote>
<p>Response: Thanks for highlighting the need for better integration of our proposed theory with existing observations in the field of VPL. Unfortunately, the theoretical framework proposed in our study is based on the Klein’s Erlangen program and is only applicable to geometric shape stimuli. For VPL studies using stimuli and paradigms that are completely unrelated to geometric transformations (such as motion discrimination with Gabors or random dots, vernier acuity, spatial frequency discrimination, contrast detection or discrimination, etc.), our proposed theory does not apply. Some stimuli employed by VPL studies can be classified into certain geometric invariants. For instance, orientation discrimination with Gabors (Dosher &amp; Lu, 2005) and texture discrimination task (F. Wang et al., 2016) both belong to tasks involving Euclidean invariants, and circle versus square discrimination (Kraft et al., 2010) belongs to tasks involving affine invariance. However, these studies do not simultaneously involve multiple geometric invariants of varying levels stability, and thus cannot be directly compared with our research. It is worth noting that while the Klein’s hierarchy of geometries, which our study focuses on, is rarely mentioned in the field of VPL, it does have connections with concepts such as 'global/local', 'coarse/fine', 'easy/difficulty', 'complex/simple': more stable invariants are closer to 'global', 'coarse', 'easy', 'complex', while less stable invariants are closer to 'local', 'fine', 'difficulty', 'simple'. Importantly, several VPL studies have found ‘fine-to-coarse’ or ‘local-to-global’ asymmetric transfer (Chang et al., 2014; N. Chen et al., 2016; Dosher &amp; Lu, 2005), which seems consistent with the results of our study.</p>
<p>In the introduction section of our revised version and subsequent full author response, we will provide a clear explanation of the Erlangen program and elucidate how to define the stability of new stimuli or tasks. In the discussion section of our revised version, we will compare our results to other studies concerned with the generalization of perceptual learning and speculate on how our proposed theory fit with existing observations in the field of VPL.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The paper does not convincingly establish the necessity of its introduced concept of invariant stability for interpreting the presented data. For instance, consider an alternative explanation: performing in the collinearity task requires orientation invariance. Therefore, it's straightforward that learning the collinearity task doesn't aid in performing the other two tasks (parallelism and orientation), which do require orientation estimation. Interestingly, orientation invariance is more characteristic of higher visual areas, which, consistent with the Reverse Hierarchy Theory, are engaged more rapidly in learning compared to lower visual areas. This simpler explanation, grounded in established concepts of VPL and the tuning properties of neurons across the visual cortex, can account for the observed effects, at least in one scenario. This approach has previously been used/proposed to explain VPL generalization, as seen in (Chowdhury and DeAngelis, Neuron, 2008), (Liu and Pack, Neuron, 2017), and (Bakhtiari et al., JoV, 2020). The question then is: how does the concept of invariant stability provide additional insights beyond this simpler explanation?</p>
</list-item></list>
</disp-quote>
<p>Response: We appreciate the alternative explanation proposed by the reviewer and agree that it presents a valid perspective grounded in established concepts of VPL and neural tuning properties. However, performing in the collinearity and parallelism tasks both require orientation invariance. While utilizing the orientation invariance, as proposed by the reviewer, can explain the lack of transfer from collinearity or parallelism to orientation task, it cannot explain why collinearity does not transfer to parallelism.</p>
<p>As stated in the response to the previous review, in the revised discussion section, we will compare our study with other studies (including the three papers mentioned by the reviewer), aiming to clarify the necessity of the concept of invariant stability for interpreting the observed data and understanding the mechanisms underlying VPL generalization.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>While the paper discusses the transfer of learning between tasks with varying levels of invariant stability, the mechanism of this transfer within each invariant condition remains unclear. A more detailed analysis would involve keeping the invariant's stability constant while altering a feature of the stimulus in the test condition. For example, in the VPL literature, one of the primary methods for testing generalization is examining transfer to a new stimulus location. The paper does not address the expected outcomes of location transfer in relation to the stability of the invariant. Moreover, in the affine and Euclidean conditions one could maintain consistent orientations for the distractors and targets during training, then switch them in the testing phase to assess transfer within the same level of invariant structural stability.</p>
</list-item></list>
</disp-quote>
<p>Response: Thanks for raising the issue regarding the mechanism of transfer within each invariant conditions. We plan to design an additional experiment that is similar in paradigm to Experiment 2, aiming to examine how VPL generalizes to a new test location within a single invariant stability level.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>In the section detailing the modeling experiment using deep neural networks (DNN), the takeaway was unclear. While it was interesting to observe that the DNN exhibited a generalization pattern across conditions similar to that seen in the human experiments, the claim made in the abstract and introduction that the model provides a 'mechanistic' explanation for the phenomenon seems overstated. The pattern of weight changes across layers, as depicted in Figure 7, does not conclusively explain the observed variability in generalizations. Furthermore, the substantial weight change observed in the first two layers during the orientation discrimination task is somewhat counterintuitive. Given that neurons in early layers typically have smaller receptive fields and narrower tunings, one would expect this to result in less transfer, not more.</p>
</list-item></list>
</disp-quote>
<p>Response: We appreciate the reviewer's feedback regarding the clarity of our DNN modeling experiment. We acknowledge that while DNNs have been demonstrated to serve as models for visual systems as well as VPL, the claim that the model provides a ‘mechanistic’ explanation for the phenomenon still overstated. In our revised version,</p>
<p>We will attempt a more detailed analysis of the DNN model while providing a more explicit explanation of the findings from the DNN modeling experiment, emphasizing its implications for understanding the observed variability in generalizations.</p>
<p>Additionally, the substantial weight change observed in the first two layers during the orientation discrimination task is not contradictory to the theoretical framework we proposed, instead, it aligns with our speculation regarding the neural mechanisms of VPL for geometric invariants. Specifically, it suggests that invariants with lower stability rely more on the plasticity of lower-level brain areas, thus exhibiting poorer generalization performance to new locations or stimulus features within each invariant conditions. However, it does not imply that their learning effects cannot transfer to invariants with higher stability.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>The strengths of this paper are clear: The authors are asking a novel question about geometric representation that would be relevant to a broad audience. Their question has a clear grounding in pre-existing mathematical concepts, that, to my knowledge, have been only minimally explored in cognitive science. Moreover, the data themselves are quite striking, such that my only concern would be that the data seem almost <italic>too</italic> clean. It is hard to know what to make of that, however. From one perspective, this is even more reason the results should be publicly available. Yet I am of the (perhaps unorthodox) opinion that reviewers should voice these gut reactions, even if it does not influence the evaluation otherwise. Below I offer some more concrete comments:</p>
<p>(1) The justification for the designs is not well explained. The authors simply tell the audience in a single sentence that they test projective, affine, and Euclidean geometry. But despite my familiarity with these terms -- familiarity that many readers may not have -- I still had to pause for a very long time to make sense of how these considerations led to the stimuli that were created. I think the authors must, for a point that is so central to the paper, thoroughly explain exactly why the stimuli were designed the way that they were and how these designs map onto the theoretical constructs being tested.</p>
<p>(2) I wondered if the design in Experiment 1 was flawed in one small but critical way. The goal of the parallelism stimuli, I gathered, was to have a set of items that is not parallel to the other set of items. But in doing that, isn't the manipulation effectively the same as the manipulation in the orientation stimuli? Both functionally involve just rotating one set by a fixed amount. (Note: This does not seem to be a problem in Experiment 2, in which the conditions are more clearly delineated.)</p>
<p>(3) I wondered if the results would hold up for stimuli that were more diverse. It seems that a determined experimenter could easily design an &quot;adversarial&quot; version of these experiments for which the results would be unlikely to replicate. For instance: In the orientation group in Experiment 1, what if the odd-one-out was rotated 90 degrees instead of 180 degrees? Intuitively, it seems like this trial type would now be much easier, and the pattern observed here would not hold up. If it did hold up, that would provide stronger support for the authors' theory.</p>
<p>It is not enough, in my opinion, to simply have some confirmatory evidence of this theory. One would have to have thoroughly tested many possible ways that theory could fail. I'm unsure that enough has been done here to convince me that these ideas would hold up across a more diverse set of stimuli.</p>
</disp-quote>
<p>Response: (1) We appreciate the reviewer’s feedback regarding the justification for our experimental designs. We recognize the importance of thoroughly explaining how our stimuli were designed and how these designs correspond to the theoretical constructs being tested. In our revised version, we will enhance the introduction of Erlangen program and provide a more detailed explanation of the rationale behind our stimulus designs, aiming to enhance the clarity and transparency of our experimental approach for readers who may not be familiar with these concepts.</p>
<p>(2) We appreciate the reviewer’s insight into the design of Experiment 1 and the concern regarding the potential similarity between the parallelism and orientation stimuli manipulations.</p>
<p>The parallelism and orientation stimuli in Experiment 1 were first used by Olson &amp; Attneave (1970) to support line-based models of shape coding and then adapted to measure the relative salience of different geometric properties (Chen, 1986). In the parallelism stimuli, the odd quadrant differs from the rest in line slope, while in the orientation stimuli, in contrast, the odd quadrant contains exactly the same line segments as the rest but differs in direction pointed by the angles. The result, that the odd quadrant was detected much faster in the parallelism stimuli than in the orientation stimuli, can serve as evidence for line-based models of shape coding. However, according to Chen (1986, 2005), the idea of invariants over transformations suggests a new analysis of the data: in the parallelism stimuli, the fact that line segments share the same slope essentially implies that they are parallel, and the discrimination may be actually based on parallelism. Thus, the faster discrimination of the parallelism stimuli than that of the orientation stimuli may be explained in terms of relative superiority of parallelism over orientation of angles—a Euclidean property.</p>
<p>The group of stimuli in Experiment 1 has been employed by several studies to investigate scientific questions related to the Klein’s hierarchy of geometries (L. Chen, 2005; Meng et al., 2019; B. Wang et al., n.d.). Due to historical inheritance, we adopted this set of stimuli and corresponding paradigm, despite their imperfect design.</p>
<p>(3) Thanks for raising the important issue of stimulus diversity and the potential for &quot;adversarial&quot; versions of the experiments to challenge our findings. We acknowledge the validity of your concern and recognize the need to demonstrate the robustness of our results across a range of stimuli. We plan to design additional experiments to investigate the potential implications of varying stimulus characteristics, such as different rotation angles proposed by the reviewer, on the observed patterns of performance.</p>
</body>
</sub-article>
</article>